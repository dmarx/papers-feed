'1503.02531':
  abstract: A very simple way to improve the performance of almost any machine learning
    algorithm is to train many different models on the same data and then to average
    their predictions. Unfortunately, making predictions using a whole ensemble of
    models is cumbersome and may be too computationally expensive to allow deployment
    to a large number of users, especially if the individual models are large neural
    nets. Caruana and his collaborators have shown that it is possible to compress
    the knowledge in an ensemble into a single model which is much easier to deploy
    and we develop this approach further using a different compression technique.
    We achieve some surprising results on MNIST and we show that we can significantly
    improve the acoustic model of a heavily used commercial system by distilling the
    knowledge in an ensemble of models into a single model. We also introduce a new
    type of ensemble composed of one or more full models and many specialist models
    which learn to distinguish fine-grained classes that the full models confuse.
    Unlike a mixture of experts, these specialist models can be trained rapidly and
    in parallel.
  arxivId: '1503.02531'
  authors: Geoffrey Hinton, Oriol Vinyals, Jeff Dean
  created_at: '2024-12-21T16:05:51Z'
  issue_number: 93
  issue_url: https://github.com/dmarx/arxiv-archive/issues/93
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Distilling the Knowledge in a Neural Network
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1503.02531
'1711.11586':
  abstract: Many image-to-image translation problems are ambiguous, as a single input
    image may correspond to multiple possible outputs. In this work, we aim to model
    a \emph{distribution} of possible outputs in a conditional generative modeling
    setting. The ambiguity of the mapping is distilled in a low-dimensional latent
    vector, which can be randomly sampled at test time. A generator learns to map
    the given input, combined with this latent code, to the output. We explicitly
    encourage the connection between output and the latent code to be invertible.
    This helps prevent a many-to-one mapping from the latent code to the output during
    training, also known as the problem of mode collapse, and produces more diverse
    results. We explore several variants of this approach by employing different training
    objectives, network architectures, and methods of injecting the latent code. Our
    proposed method encourages bijective consistency between the latent encoding and
    output modes. We present a systematic comparison of our method and other variants
    on both perceptual realism and diversity.
  arxivId: '1711.11586'
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-15T07:21:47Z'
  issue_number: 14
  issue_url: https://github.com/dmarx/arxiv-archive/issues/14
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1711.11586
'1904.08779':
  abstract: We present SpecAugment, a simple data augmentation method for speech recognition.
    SpecAugment is applied directly to the feature inputs of a neural network (i.e.,
    filter bank coefficients). The augmentation policy consists of warping the features,
    masking blocks of frequency channels, and masking blocks of time steps. We apply
    SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition
    tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard
    300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER
    on test-other without the use of a language model, and 5.8% WER with shallow fusion
    with a language model. This compares to the previous state-of-the-art hybrid system
    of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome
    portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1%
    with shallow fusion, which compares to the previous state-of-the-art hybrid system
    at 8.3%/17.3% WER.
  arxivId: '1904.08779'
  authors: Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
    Ekin D. Cubuk, Quoc V. Le
  created_at: '2024-12-17T14:41:06Z'
  issue_number: 60
  issue_url: https://github.com/dmarx/arxiv-archive/issues/60
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/1904.08779
'2009.10195':
  abstract: Models that perform well on a training domain often fail to generalize
    to out-of-domain (OOD) examples. Data augmentation is a common method used to
    prevent overfitting and improve OOD generalization. However, in natural language,
    it is difficult to generate new examples that stay on the underlying data manifold.
    We introduce SSMBA, a data augmentation method for generating synthetic training
    examples by using a pair of corruption and reconstruction functions to move randomly
    on a data manifold. We investigate the use of SSMBA in the natural language domain,
    leveraging the manifold assumption to reconstruct corrupted text with masked language
    models. In experiments on robustness benchmarks across 3 tasks and 9 datasets,
    SSMBA consistently outperforms existing data augmentation methods and baseline
    models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD
    Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.
  arxivId: '2009.10195'
  authors: Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi
  created_at: '2024-12-21T18:19:38Z'
  issue_number: 97
  issue_url: https://github.com/dmarx/arxiv-archive/issues/97
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving\n\
    \  Out-of-Domain Robustness"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2009.10195
'2203.15556':
  abstract: 'We investigate the optimal model size and number of tokens for training
    a transformer language model under a given compute budget. We find that current
    large language models are significantly undertrained, a consequence of the recent
    focus on scaling language models whilst keeping the amount of training data constant.
    By training over 400 language models ranging from 70 million to over 16 billion
    parameters on 5 to 500 billion tokens, we find that for compute-optimal training,
    the model size and the number of training tokens should be scaled equally: for
    every doubling of model size the number of training tokens should also be doubled.
    We test this hypothesis by training a predicted compute-optimal model, Chinchilla,
    that uses the same compute budget as Gopher but with 70B parameters and 4$\times$
    more more data. Chinchilla uniformly and significantly outperforms Gopher (280B),
    GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range
    of downstream evaluation tasks. This also means that Chinchilla uses substantially
    less compute for fine-tuning and inference, greatly facilitating downstream usage.
    As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
    on the MMLU benchmark, greater than a 7% improvement over Gopher.'
  arxivId: '2203.15556'
  authors: Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den
    Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen,
    Jack W. Rae, Oriol Vinyals, Laurent Sifre
  created_at: '2024-12-18T22:03:26Z'
  issue_number: 67
  issue_url: https://github.com/dmarx/arxiv-archive/issues/67
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Training Compute-Optimal Large Language Models
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2203.15556
'2209.02740':
  abstract: Networks of weakly coupled oscillators had a profound impact on our understanding
    of complex systems. Studies on model reconstruction from data have shown prevalent
    contributions from hypernetworks with triplet and higher interactions among oscillators,
    in spite that such models were originally defined as oscillator networks with
    pairwise interactions. Here, we show that hypernetworks can spontaneously emerge
    even in the presence of pairwise albeit nonlinear coupling given certain triplet
    frequency resonance conditions. The results are demonstrated in experiments with
    electrochemical oscillators and in simulations with integrate-and-fire neurons.
    By developing a comprehensive theory, we uncover the mechanism for emergent hypernetworks
    by identifying appearing and forbidden frequency resonant conditions. Furthermore,
    it is shown that microscopic linear (difference) coupling among units results
    in coupled mean fields, which have sufficient nonlinearity to facilitate hypernetworks.
    Our findings shed light on the apparent abundance of hypernetworks and provide
    a constructive way to predict and engineer their emergence.
  arxivId: '2209.02740'
  authors: Eddie Nijholt, Jorge Luis Ocampo-Espindola, Deniz Eroglu, István Z. Kiss,
    Tiago Pereira
  created_at: '2024-12-22T05:30:04Z'
  issue_number: 103
  issue_url: https://github.com/dmarx/arxiv-archive/issues/103
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Emergent hypernetworks in weakly coupled oscillators
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2209.02740
'2210.14891':
  abstract: We present a smoothly broken power law functional form (that we refer
    to as a Broken Neural Scaling Law (BNSL)) that accurately models &amp; extrapolates
    the scaling behaviors of deep neural networks (i.e. how the evaluation metric
    of interest varies as amount of compute used for training (or inference), number
    of model parameters, training dataset size, model input size, number of training
    steps, or upstream performance varies) for various architectures &amp; for each
    of various tasks within a large &amp; diverse set of upstream &amp; downstream
    tasks, in zero-shot, prompted, &amp; finetuned settings. This set includes large-scale
    vision, language, audio, video, diffusion, generative modeling, multimodal learning,
    contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution
    (OOD) generalization, continual learning, transfer learning, uncertainty estimation
    / calibration, OOD detection, adversarial robustness, distillation, sparsity,
    retrieval, quantization, pruning, fairness, molecules, computer programming/coding,
    math word problems, "emergent phase transitions", arithmetic, supervised learning,
    unsupervised/self-supervised learning, &amp; reinforcement learning (single agent
    &amp; multi-agent). When compared to other functional forms for neural scaling,
    this functional form yields extrapolations of scaling behavior that are considerably
    more accurate on this set. Moreover, this functional form accurately models &amp;
    extrapolates scaling behavior that other functional forms are incapable of expressing
    such as the nonmonotonic transitions present in the scaling behavior of phenomena
    such as double descent &amp; the delayed, sharp inflection points present in the
    scaling behavior of tasks such as arithmetic. Lastly, we use this functional form
    to glean insights about the limit of the predictability of scaling behavior. Code
    is available at https://github.com/ethancaballero/broken_neural_scaling_laws
  arxivId: '2210.14891'
  authors: Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger
  created_at: '2024-12-21T05:51:09Z'
  issue_number: 85
  issue_url: https://github.com/dmarx/arxiv-archive/issues/85
  labels:
  - paper
  - rating:downvote
  last_read: null
  state: open
  title: Broken Neural Scaling Laws
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2210.14891
'2212.07677':
  abstract: At present, the mechanisms of in-context learning in Transformers are
    not well understood and remain mostly an intuition. In this paper, we suggest
    that training Transformers on auto-regressive objectives is closely related to
    gradient-based meta-learning formulations. We start by providing a simple weight
    construction that shows the equivalence of data transformations induced by 1)
    a single linear self-attention layer and by 2) gradient-descent (GD) on a regression
    loss. Motivated by that construction, we show empirically that when training self-attention-only
    Transformers on simple regression tasks either the models learned by GD and Transformers
    show great similarity or, remarkably, the weights found by optimization match
    the construction. Thus we show how trained Transformers become mesa-optimizers
    i.e. learn models by gradient descent in their forward pass. This allows us, at
    least in the domain of regression problems, to mechanistically understand the
    inner workings of in-context learning in optimized Transformers. Building on this
    insight, we furthermore identify how Transformers surpass the performance of plain
    gradient descent by learning an iterative curvature correction and learn linear
    models on deep data representations to solve non-linear regression tasks. Finally,
    we discuss intriguing parallels to a mechanism identified to be crucial for in-context
    learning termed induction-head (Olsson et al., 2022) and show how it could be
    understood as a specific case of in-context learning by gradient descent learning
    within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
    .
  arxivId: '2212.07677'
  authors: Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento,
    Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov
  created_at: '2024-12-21T08:19:06Z'
  issue_number: 91
  issue_url: https://github.com/dmarx/arxiv-archive/issues/91
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Transformers learn in-context by gradient descent
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2212.07677
'2304.15004':
  abstract: 'Recent work claims that large language models display emergent abilities,
    abilities not present in smaller-scale models that are present in larger-scale
    models. What makes emergent abilities intriguing is two-fold: their sharpness,
    transitioning seemingly instantaneously from not present to present, and their
    unpredictability, appearing at seemingly unforeseeable model scales. Here, we
    present an alternative explanation for emergent abilities: that for a particular
    task and model family, when analyzing fixed model outputs, emergent abilities
    appear due to the researcher''s choice of metric rather than due to fundamental
    changes in model behavior with scale. Specifically, nonlinear or discontinuous
    metrics produce apparent emergent abilities, whereas linear or continuous metrics
    produce smooth, continuous predictable changes in model performance. We present
    our alternative explanation in a simple mathematical model, then test it in three
    complementary ways: we (1) make, test and confirm three predictions on the effect
    of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent
    abilities; (2) make, test and confirm two predictions about metric choices in
    a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics
    to produce never-before-seen seemingly emergent abilities in multiple vision tasks
    across diverse deep networks. Via all three analyses, we provide evidence that
    alleged emergent abilities evaporate with different metrics or with better statistics,
    and may not be a fundamental property of scaling AI models.'
  arxivId: '2304.15004'
  authors: Rylan Schaeffer, Brando Miranda, Sanmi Koyejo
  created_at: '2024-12-21T06:06:11Z'
  issue_number: 89
  issue_url: https://github.com/dmarx/arxiv-archive/issues/89
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Are Emergent Abilities of Large Language Models a Mirage?
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2304.15004
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    for generating high-dimensional data. In this paper, we show that the dynamics
    of these models exhibit a spontaneous symmetry breaking that divides the generative
    dynamics into two distinct phases: 1) A linear steady-state dynamics around a
    central fixed-point and 2) an attractor dynamics directed towards the data manifold.
    These two "phases" are separated by the change in stability of the central fixed-point,
    with the resulting window of instability being responsible for the diversity of
    the generated samples. Using both theoretical and empirical evidence, we show
    that an accurate simulation of the early dynamics does not significantly contribute
    to the final generation, since early fluctuations are reverted to the central
    fixed point. To leverage this insight, we propose a Gaussian late initialization
    scheme, which significantly improves model performance, achieving up to 3x FID
    improvements on fast samplers, while also increasing sample diversity (e.g., racial
    composition of generated CelebA images). Our work offers a new way to understand
    the generative dynamics of diffusion models that has the potential to bring about
    higher performance and less biased fast-samplers.'
  arxivId: '2305.19693'
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-15T09:58:39Z'
  issue_number: 24
  issue_url: https://github.com/dmarx/arxiv-archive/issues/24
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2305.19693
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and rendering
    times by orders of magnitude. While these methods demonstrate state-of-the-art
    quality and speed, they are designed for photogrammetry of static scenes and do
    not generalize well to freely moving humans in the environment. In this work,
    we introduce Human Gaussian Splats (HUGS) that represents an animatable human
    together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only
    a monocular video with a small number of (50-100) frames, and it automatically
    learns to disentangle the static scene and a fully animatable human avatar within
    30 minutes. We utilize the SMPL body model to initialize the human Gaussians.
    To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow
    the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians
    for animated humans brings new challenges, including the artifacts created when
    articulating the Gaussians. We propose to jointly optimize the linear blend skinning
    weights to coordinate the movements of individual Gaussians during animation.
    Our approach enables novel-pose synthesis of human and novel view synthesis of
    both the human and the scene. We achieve state-of-the-art rendering quality with
    a rendering speed of 60 FPS while being ~100x faster to train over previous work.
    Our code will be announced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-15T08:20:13Z'
  issue_number: 18
  issue_url: https://github.com/dmarx/arxiv-archive/issues/18
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'HUGS: Human Gaussian Splats'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2311.17910
'2401.17671':
  abstract: Recent advancements in artificial intelligence have sparked interest in
    the parallels between large language models (LLMs) and human neural processing,
    particularly in language comprehension. While prior research has established similarities
    in the representation of LLMs and the brain, the underlying computational principles
    that cause this convergence, especially in the context of evolving LLMs, remain
    elusive. Here, we examined a diverse selection of high-performance LLMs with similar
    parameter sizes to investigate the factors contributing to their alignment with
    the brain's language processing mechanisms. We find that as LLMs achieve higher
    performance on benchmark tasks, they not only become more brain-like as measured
    by higher performance when predicting neural responses from LLM embeddings, but
    also their hierarchical feature extraction pathways map more closely onto the
    brain's while using fewer layers to do the same encoding. We also compare the
    feature extraction pathways of the LLMs to each other and identify new ways in
    which high-performing models have converged toward similar hierarchical processing
    mechanisms. Finally, we show the importance of contextual information in improving
    model performance and brain similarity. Our findings reveal the converging aspects
    of language processing in the brain and LLMs and offer new directions for developing
    models that align more closely with human cognitive processing.
  arxivId: '2401.17671'
  authors: Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, Nima
    Mesgarani
  created_at: '2024-12-19T11:12:50Z'
  issue_number: 70
  issue_url: https://github.com/dmarx/arxiv-archive/issues/70
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Contextual Feature Extraction Hierarchies Converge in Large Language\n \
    \ Models and the Brain"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2401.17671
'2402.03239':
  abstract: 'Bluesky is a new social network built upon the AT Protocol, a decentralized
    foundation for public social media. It was launched in private beta in February
    2023, and has grown to over 10 million registered users by October 2024. In this
    paper we introduce the architecture of Bluesky and the AT Protocol, and explain
    how the technical design of Bluesky is informed by our goals: to enable decentralization
    by having multiple interoperable providers for every part of the system; to make
    it easy for users to switch providers; to give users agency over the content they
    see; and to provide a simple user experience that does not burden users with complexity
    arising from the system''s decentralized nature. The system''s openness allows
    anybody to contribute to content moderation and community management, and we invite
    the research community to use Bluesky as a dataset and testing ground for new
    approaches in social media moderation.'
  arxivId: '2402.03239'
  authors: Martin Kleppmann, Paul Frazee, Jake Gold, Jay Graber, Daniel Holmgren,
    Devin Ivy, Jeromy Johnson, Bryan Newbold, Jaz Volpert
  created_at: '2024-12-22T05:41:39Z'
  issue_number: 105
  issue_url: https://github.com/dmarx/arxiv-archive/issues/105
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'Bluesky and the AT Protocol: Usable Decentralized Social Media'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2402.03239
'2402.14903':
  abstract: Tokenization, the division of input text into input tokens, is an often
    overlooked aspect of the large language model (LLM) pipeline and could be the
    source of useful or harmful inductive biases. Historically, LLMs have relied on
    byte pair encoding, without care to specific input domains. With the increased
    use of LLMs for reasoning, various number-specific tokenization schemes have been
    adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization
    while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers.
    In this work, we study the effect this choice has on numerical reasoning through
    the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization
    for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma
    separating numbers at inference time) leads to largely improved performance. Furthermore,
    we find that model errors when using standard left-to-right tokenization follow
    stereotyped error patterns, suggesting that model computations are systematic
    rather than approximate. We show that the model is able to convert between tokenizations
    easily, thus allowing chain-of-thought-inspired approaches to recover performance
    on left-to-right tokenized inputs. We also find the gap between tokenization directions
    decreases when models are scaled, possibly indicating that larger models are better
    able to override this tokenization-dependent inductive bias. In summary, our work
    performs the first study of how number tokenization choices lead to differences
    in model performance on arithmetic tasks, accompanied by a thorough analysis of
    error patterns. We hope this work inspires practitioners to more carefully ablate
    number tokenization-related choices when working towards general models of numerical
    reasoning.
  arxivId: '2402.14903'
  authors: Aaditya K. Singh, DJ Strouse
  created_at: '2024-12-19T22:43:04Z'
  issue_number: 78
  issue_url: https://github.com/dmarx/arxiv-archive/issues/78
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Tokenization counts: the impact of tokenization on arithmetic in\n  frontier\
    \ LLMs"
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2402.14903
'2405.17399':
  abstract: The poor performance of transformers on arithmetic tasks seems to stem
    in large part from their inability to keep track of the exact position of each
    digit inside of a large span of digits. We mend this problem by adding an embedding
    to each digit that encodes its position relative to the start of the number. In
    addition to the boost these embeddings provide on their own, we show that this
    fix enables architectural modifications such as input injection and recurrent
    layers to improve performance even further.   With positions resolved, we can
    study the logical extrapolation ability of transformers. Can they solve arithmetic
    problems that are larger and more complex than those in their training data? We
    find that training on only 20 digit numbers with a single GPU for one day, we
    can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit
    addition problems. Finally, we show that these gains in numeracy also unlock improvements
    on other multi-step reasoning tasks including sorting and multiplication.
  arxivId: '2405.17399'
  authors: Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian
    R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild,
    Tom Goldstein
  created_at: '2024-12-19T22:43:45Z'
  issue_number: 79
  issue_url: https://github.com/dmarx/arxiv-archive/issues/79
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Transformers Can Do Arithmetic with the Right Embeddings
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2405.17399v1
'2407.17465':
  abstract: 'The Maximal Update Parametrization ($\mu$P) aims to make the optimal
    hyperparameters (HPs) of a model independent of its size, allowing them to be
    swept using a cheap proxy model rather than the full-size target model. We present
    a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with Unit Scaling,
    a method for designing models that makes them easy to train in low-precision.
    The two techniques have a natural affinity: $\mu$P ensures that the scale of activations
    is independent of model size, and Unit Scaling ensures that activations, weights
    and gradients begin training with a scale of one. This synthesis opens the door
    to a simpler scheme, whose default values are near-optimal. This in turn facilitates
    a more efficient sweeping strategy, with u-$\mu$P models reaching a loss that
    is equal to or lower than comparable $\mu$P models and working out-of-the-box
    in FP8.'
  arxivId: '2407.17465'
  authors: Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y.
    Prince, Björn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach,
    Douglas Orr
  created_at: '2024-12-19T18:35:47Z'
  issue_number: 72
  issue_url: https://github.com/dmarx/arxiv-archive/issues/72
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'u-$μ$P: The Unit-Scaled Maximal Update Parametrization'
  total_reading_time_minutes: 0
  url: https://arxiv.org/html/2407.17465v2
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is a critical step towards building generally self-improving agents that can operate
    on open-ended natural language. In this paper, we study the scaling of inference-time
    computation in LLMs, with a focus on answering the question: if an LLM is allowed
    to use a fixed but non-trivial amount of inference-time compute, how much can
    it improve its performance on a challenging prompt? Answering this question has
    implications not only on the achievable performance of LLMs, but also on the future
    of LLM pretraining and how one should tradeoff inference-time and pre-training
    compute. Despite its importance, little research attempted to understand the scaling
    behaviors of various test-time inference methods. Moreover, current work largely
    provides negative results for a number of these strategies. In this work, we analyze
    two primary mechanisms to scale test-time computation: (1) searching against dense,
    process-based verifier reward models; and (2) updating the model''s distribution
    over a response adaptively, given the prompt at test time. We find that in both
    cases, the effectiveness of different approaches to scaling test-time compute
    critically varies depending on the difficulty of the prompt. This observation
    motivates applying a "compute-optimal" scaling strategy, which acts to most effectively
    allocate test-time compute adaptively per prompt. Using this compute-optimal strategy,
    we can improve the efficiency of test-time compute scaling by more than 4x compared
    to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find
    that on problems where a smaller base model attains somewhat non-trivial success
    rates, test-time compute can be used to outperform a 14x larger model.'
  arxivId: '2408.03314'
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-17T13:48:03Z'
  issue_number: 57
  issue_url: https://github.com/dmarx/arxiv-archive/issues/57
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2408.03314
'2410.15468':
  abstract: 'We consider emergence from the perspective of dynamics: states of a system
    evolving with time. We focus on the role of a decomposition of wholes into parts,
    and attempt to characterize relationships between levels without reference to
    whether higher-level properties are "novel" or "unexpected." We offer a classification
    of different varieties of emergence, with and without new ontological elements
    at higher levels.'
  arxivId: '2410.15468'
  authors: Sean M. Carroll, Achyuth Parola
  created_at: '2024-12-22T05:48:56Z'
  issue_number: 107
  issue_url: https://github.com/dmarx/arxiv-archive/issues/107
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: What Emergence Can Possibly Mean
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.15468
'2410.24054':
  abstract: We develop EigenVI, an eigenvalue-based approach for black-box variational
    inference (BBVI). EigenVI constructs its variational approximations from orthogonal
    function expansions. For distributions over $\mathbb{R}^D$, the lowest order term
    in these expansions provides a Gaussian variational approximation, while higher-order
    terms provide a systematic way to model non-Gaussianity. These approximations
    are flexible enough to model complex distributions (multimodal, asymmetric), but
    they are simple enough that one can calculate their low-order moments and draw
    samples from them. EigenVI can also model other types of random variables (e.g.,
    nonnegative, bounded) by constructing variational approximations from different
    families of orthogonal functions. Within these families, EigenVI computes the
    variational approximation that best matches the score function of the target distribution
    by minimizing a stochastic estimate of the Fisher divergence. Notably, this optimization
    reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps
    the iterative gradient-based optimizations that are required for many other BBVI
    algorithms. (Gradient-based methods can be sensitive to learning rates, termination
    criteria, and other tunable hyperparameters.) We use EigenVI to approximate a
    variety of target distributions, including a benchmark suite of Bayesian models
    from posteriordb. On these distributions, we find that EigenVI is more accurate
    than existing methods for Gaussian BBVI.
  arxivId: '2410.24054'
  authors: Diana Cai, Chirag Modi, Charles C. Margossian, Robert M. Gower, David M.
    Blei, Lawrence K. Saul
  created_at: '2024-12-22T05:52:47Z'
  issue_number: 109
  issue_url: https://github.com/dmarx/arxiv-archive/issues/109
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: "EigenVI: score-based variational inference with orthogonal function\n  expansions"
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2410.24054
'2411.19722':
  abstract: Removing modeling constraints and unifying architectures across domains
    has been a key driver of the recent progress in training large multimodal models.
    However, most of these models still rely on many separately trained components
    such as modality-specific encoders and decoders. In this work, we further streamline
    joint generative modeling of images and text. We propose an autoregressive decoder-only
    transformer - JetFormer - which is trained to directly maximize the likelihood
    of raw data, without relying on any separately pretrained components, and can
    understand and generate both text and images. Specifically, we leverage a normalizing
    flow model to obtain a soft-token image representation that is jointly trained
    with an autoregressive multimodal transformer. The normalizing flow model serves
    as both an image encoder for perception tasks and an image decoder for image generation
    tasks during inference. JetFormer achieves text-to-image generation quality competitive
    with recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained
    image autoencoders, which are trained with a complex mixture of losses, including
    perceptual ones. At the same time, JetFormer demonstrates robust image understanding
    capabilities. To the best of our knowledge, JetFormer is the first model that
    is capable of generating high-fidelity images and producing strong log-likelihood
    bounds.
  arxivId: '2411.19722'
  authors: Michael Tschannen, André Susano Pinto, Alexander Kolesnikov
  created_at: '2024-12-16T17:41:00Z'
  issue_number: 54
  issue_url: https://github.com/dmarx/arxiv-archive/issues/54
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
  total_reading_time_minutes: 0
  url: https://arxiv.org/abs/2411.19722
'2412.09621':
  abstract: 'Learning to understand dynamic 3D scenes from imagery is crucial for
    applications ranging from robotics to scene reconstruction. Yet, unlike other
    problems where large-scale supervised training has enabled rapid progress, directly
    supervising methods for recovering 3D motion remains challenging due to the fundamental
    difficulty of obtaining ground truth annotations. We present a system for mining
    high-quality 4D reconstructions from internet stereoscopic, wide-angle videos.
    Our system fuses and filters the outputs of camera pose estimation, stereo depth
    estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions.
    We use this method to generate large-scale data in the form of world-consistent,
    pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate
    the utility of this data by training a variant of DUSt3R to predict structure
    and 3D motion from real-world image pairs, showing that training on our reconstructed
    data enables generalization to diverse real-world scenes. Project page: https://stereo4d.github.io'
  arxivId: '2412.09621'
  authors: Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander
    Holynski
  created_at: '2024-12-16T08:59:30Z'
  issue_number: 53
  issue_url: https://github.com/dmarx/arxiv-archive/issues/53
  labels:
  - reading-session
  last_read: '2024-12-16T08:59:30.170Z'
  state: open
  title: 'Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos'
  total_reading_time_minutes: 1
  url: https://arxiv.org/abs/2412.09621v1
'2412.13145':
  abstract: 'Could an AI have conscious experiences? Any answer to this question should
    conform to Evidentialism - that is, it should be based not on intuition, dogma
    or speculation but on solid scientific evidence. I argue that such evidence is
    hard to come by and that the only justifiable stance on the prospects of artificial
    consciousness is agnosticism. In the current debate, the main division is between
    biological views that are sceptical of artificial consciousness and functional
    views that are sympathetic to it. I argue that both camps make the same mistake
    of over-estimating what the evidence tells us. Scientific insights into consciousness
    have been achieved through the study of conscious organisms. Although this has
    enabled cautious assessments of consciousness in various creatures, extending
    this to AI faces serious obstacles. AI thus presents consciousness researchers
    with a dilemma: either reach a verdict on artificial consciousness but violate
    Evidentialism; or respect Evidentialism but offer no verdict on the prospects
    of artificial consciousness. The dominant trend in the literature has been to
    take the first option while purporting to follow the scientific evidence. I argue
    that if we truly follow the evidence, we must take the second option and adopt
    agnosticism.'
  arxivId: '2412.13145'
  authors: Tom McClelland
  created_at: '2024-12-22T05:22:42Z'
  issue_number: 102
  issue_url: https://github.com/dmarx/arxiv-archive/issues/102
  labels:
  - paper
  - rating:novote
  last_read: null
  state: open
  title: Agnosticism About Artificial Consciousness
  total_reading_time_minutes: 0
  url: https://arxiv.org/pdf/2412.13145
