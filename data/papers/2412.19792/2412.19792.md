---
abstract: |
  Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the *win rate* of a sample from the aligned model over a sample from the reference model is high, subject to a KL divergence constraint. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. However, the alignment objective does not capture such inference-time decoding procedures. We show that the existing alignment framework is sub-optimal in view of such inference-time methods. We then modify the alignment objective and propose a framework for inference-aware alignment (`InfAlign`). We prove that for any inference-time decoding algorithm, the optimal solution that optimizes the *inference-time win rate* of the aligned policy against the reference policy is the solution to the typical RLHF problem with a *transformation of the reward*. This motivates us to provide the KL-regularized calibrate-and-transform RL (`CTRL`) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. We particularize our study to two important inference-time strategies: best-of-$N$ sampling and best-of-$N$ jailbreaking, where $N$ responses are sampled from the model and the one with the highest or lowest reward is selected. We propose specific transformations for these inference-time strategies and demonstrate that our framework offers significant improvements over existing state-of-the-art methods for language model alignment. Empirically, we outperform state-of-the-art methods that are designed without taking inference-time decoding into consideration by 8-12% and 4-9% on inference-time win rates over the Anthropic helpfulness and harmlessness dialog benchmark datasets.
author:
- |
  Ananth Balashankar[^1]$^1$ Ziteng Sun$^{\@fnsymbol{1} 2}$ Jonathan Berant$^{1}$Jacob Eisenstein$^{1}$   
  **Michael Collins$^1$ Adrian Hutter$^1$ Jong Lee$^1$ Chirag Nagpal$^2$   
  ** Flavien Prost$^1$ Aradhana Sinha$^1$ Ananda Theertha Suresh$^{2}$   
  ** Ahmad Beirami$^{1}$  
     
  $^1$Google DeepMind $^2$Google Research******
bibliography:
- main.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "InfAlign: Inference-aware language model alignment"
---





# Introduction

Aligning generative language models (LMs) through KL-regularized reinforcement learning (KL-RL) is a widely adopted framework for finetuning a generative language model to improve a reward (e.g., safety or quality). Solving KL-RL generally entails training a reward model, and then using an RL solver . Other ways of solving KL-RL include variants of direct preference optimization , reward model distillation , and Best-of-$N$ distillation . The success of the KL-RL framework is typically measured through the win rate of the aligned model over the reference model for a given task, which captures how often a sample drawn from the aligned model wins against a sample drawn from the base model using a judge for that task.

However, rarely is the aligned model used as is at inference time; instead an *inference-time procedure* is typically used to accomplish a task. For example, it is customary to perform one or more of the following simple or complex procedures at decoding: best-of-$N$ sampling  , best-of-$N$ jailbreaking , chain-of-thought reasoning , and self consistency  (see for more). This creates a discrepancy between the inference-time decoding procedure and the training KL-RL objective, which is to maximize the expectation of a function of reward (e.g., win rate) of a sample from the aligned model against that of the base model.

In this paper, we address the following question: Given a known inference-time procedure, can we align a model to optimize for the *inference-time win rate* against the reference model, where inference-time win rate entails obtaining a response from each model through the *inference-time procedure* and counting which sample wins? While directly optimizing the inference-time win rate seems intractable, we show that the optimal solution is captured via a family of optimization objectives, which we call the *inference-aware alignment* (`InfAlign`) framework. We further prove that for a $\delta$-bound language model, where the likelihood of all outcomes are upper-bounded by $\delta,$ as $\delta \to 0,$ the solution could be obtained by solving KL-RL with a specific transformation of the reward (). Therefore, the challenge of optimizing for inference-time win rate can be captured by designing a reward transformation that is suited to the specific inference-time procedure, and solving KL-RL using existing optimization algorithms like PPO .

We particularize the study of `InfAlign` to two simple yet popular inference-time strategies: Best-of-$N$ (`BoN`) sampling  and Best-of-$N$ jailbreaking , which we call Worst-of-$N$ (`WoN`) since the defender may assume that the attacker is choosing the worst outcome of $N$ samples. Despite their simplicity, `BoN` is extremely effective, and even known to be an effective procedure for inference-time alignment . `WoN` is a popular safety evaluation strategy often adopted to evaluate the ability to jailbreak foundation models . Hence, there is both theoretical and practical motivation for training to optimize inference-time `BoN` and `WoN` win rates. For these procedures, we solve `InfAlign` for different values of $N$ in the limit of infinitely expressive language models. We also show that exponential reward transformation is almost optimal for these strategies. We further show that the win rate optimal solution to KL-RL without reward transformation leads to suboptimal policies for inference-time win rate.

Motivated by our theoretical findings, we propose a practical solver for `InfAlign`, called Calibrate-and-Transform Reinforcement Learning (`CTRL`), which adopts a three-step approach: (1) we first *calibrate* the scores of the reward model with respect to responses sampled per-prompt by the reference model; (2) we then adopt a suitable *transformation* of the calibrated scores for a given inference-time procedure; and (3) we solve the KL-RL problem (e.g., using PPO).

We apply `CTRL` to the Anthropic helpfulness and harmlessness dataset  for optimizing `BoN` helpfulness and `WoN` harmlessness with various values of $N.$ We show that the (close-to-optimal) exponential reward transformations that were derived theoretically on idealized distributions transfer to real-world tasks – outperforming various KL-RL solvers at inference-time win rate. Finally, we also show `CTRL`(without any reward transformation) is a strong baseline for optimizing the standard win rate vs KL trade-off against various methods.

Our contributions are as follows:

- We propose a framework for inference-aware language model alignment (`InfAlign`) that captures the optimal solution to optimizing win rate in view of any well-behaving inference-time procedure. Theoretically, we show that this framework satisfies a coupled-transformed reward/policy optimization objective which lends itself to iterative optimization for a large class of inference-time procedures (see Section ).

- Motivated by our theoretical findings, we propose a practical solver, calibrate-and-transform RL (`CTRL`) for `InfAlign`, which consists of an offline reward calibration and transformation step prior to applying the regular KL-RL framework. For two popular inference-time procedures, namely optimizing Best-of-$N$ (`BoN`) sampling and preventing Best-of-$N$ jailbreaking (`WoN`), we derive the conditions for optimal theoretical solutions to alignment in a toy setting. Somewhat surprisingly, for `BoN` and `WoN`, the optimal solution is independent of the base policy and the reward model, which leads to an iterative optimization approach that can be computed offline (see and ).

- Empirically, we show on Anthropic dialog helpfulness and harmlessness datasets that `CTRL` with identity reward transformation achieves competitive performance compared to a variety of SOTA methods for optimizing standard win rate , and produces better inference-time win rate vs KL tradeoffs by 8-12% for `BoN` and 4-9% for `WoN` inference-time procedures respectively (see ).

# Problem setup

We consider a generative language model that produces a response conditioned on an input prompt. Given a prompt ${\bm{x}}$, e.g., ${\bm{x}}= \textit{What is a large language model?},$ a generative language model ${\pi_{\rm ref}}$ returns a response ${\bm{y}}$, which is sampled according to the distribution ${\pi_{\rm ref}}(\cdot \mid {\bm{x}})$. We use ${\bm{\mathcal{X}}}$ and ${\bm{\mathcal{Y}}}$ to denote the space of possible inputs and outputs, respectively. Throughout the paper, we refer to the conditional distribution $\pi(\cdot \mid {\bm{x}})$ introduced by the language model as a policy, and assume ${\pi_{\rm ref}}$ is a fixed base policy, e.g., obtained from supervised finetuning. We will often use the notation $\pi({\bm{y}}\mid {\bm{x}}) \propto f({\bm{y}})$ for some $f: {\bm{\mathcal{Y}}}\rightarrow \mathbb{R}_+$ to denote the conditional distribution $\pi({\bm{y}}\mid {\bm{x}}) = f({\bm{y}})/(\sum_{{\bm{y}}}f({\bm{y}}){\rm d}{\bm{y}})$ obtained after normalization.

#### Alignment of language models.

Let $r: {\bm{\mathcal{X}}}\times {\bm{\mathcal{Y}}}\rightarrow \mathbb{R}$ be a reward function that assigns a scalar value to any (prompt, response) pair, e.g., a model trained side-by-side on human preferences. The reward determines the goodness of response ${\bm{y}}$ in context ${\bm{x}}$. The goal of language model alignment is to construct an aligned distribution $\pi$ that improves the reward of the response while being close to ${\pi_{\rm ref}}$. Below we introduce -regularized reinforcement learning (RL), a popular training-time alignment framework in the literature.

<div class="definition">

**Definition 1** (-regularized RL framework).  * Let $\beta > 0$ be a regularization parameter, the KL-regularized RL problem aims to maximize the pointwise expected reward with a KL regularizer below: $\forall {\bm{x}},$ $$%
   \pi^*_{r, \beta}(\cdot \mid {\bm{x}}) = \arg \max_{\pi} \left\{ \mathbb{E}_{{\bm{y}}\sim \pi(\cdot \mid {\bm{x}})} \{r({\bm{x}}, {\bm{y}})\} - \beta D_{\text{KL}}(\pi(\cdot \mid {\bm{x}}) \| {\pi_{\rm ref}}(\cdot \mid {\bm{x}}))\right\}.\footnote{%
    The solution to this optimization problem is unique~\citep{korbak2022rl,rafailov2024direct,yang2024asymptotics}. %
    }$$*

</div>

When evaluating an aligned policy, a common measure to use is the win rate over the base policy ${\pi_{\rm ref}}$. Let $$w_r({\bm{y}}, {\bm{z}}\mid {\bm{x}}) := \mathbbm{1} \left\{ r({\bm{x}}, {\bm{y}}) > r({\bm{x}}, {\bm{z}})\right\} +  \frac{1}{2}\mathbbm{1} \left\{ r({\bm{x}}, {\bm{y}}) = r({\bm{x}}, {\bm{z}})\right\}$$ be the win random variable under reward $r$.

<div class="definition">

**Definition 2** (Calibrated reward). *Let $\mathcal{C}_{r, \pi}( {\bm{x}}, {\bm{y}})$ be the *calibrated reward* [^2] under policy $\pi$ defined below $$\mathcal{C}_{r, \pi}({\bm{x}}, {\bm{y}}) :=  \mathbb{E}_{{\bm{z}}\sim \pi(\cdot \mid {\bm{x}})} w_r({\bm{y}}, {\bm{z}}\mid {\bm{x}}). \label{eqn:def_quantile}$$ *

</div>

The win rate is defined as following:

<div class="definition">

**Definition 3** (Win rate).  * For any policy $\pi_1$ and $\pi_2$, we define win rate of policy $\pi_1$ over policy $\pi_2$ given prompt ${\bm{x}}$, as measured by reward $r$ as follows: $$W_r(\pi_1 \succ \pi_2 \mid {\bm{x}}) := \mathbb{E}_{{\bm{y}}\sim \pi_1(\cdot\mid {\bm{x}})} \mathcal{C}_{r, \pi_2}({\bm{x}}, {\bm{y}}).$$*

</div>

In this paper, for simplicity of the presentation and analysis, we assume that the language model is continuous, whose outcomes have infinitesimally small probabilities. In other words, we assume ${\bm{\mathcal{Y}}}$ is a continuous set, which can be mapped to $[0, 1]$ through a CDF inverse transformation , with the ordering determined by the reward of the outcomes from the smallest reward to the highest reward. We also assume that $r$ assigns distinct rewards to different ${\bm{y}}$’s for a given ${\bm{x}}$. These assumptions have been made in the past implicitly by  to estimate the KL divergence of Best-of-$n$ policy, and by to estimate its win rate and characterize the KL divergence and win rate of optimal policy for win rate vs KL divergence tradeoffs. While these assumptions lead to approximations when analyzing real-world distributions, the results derived under them are reasonably tight when the actual likelihood of the language model outcomes are small . Note that we don’t make any such assumptions when providing our algorithmic developments, and the experimental results. Under these assumptions, we define the quantile mapping, which is the reverse of the calibrated reward mapping $\mathcal{C}^{-1}$, satisfying $\forall u \in [0,1]$, $$\mathcal{C}^{-1}_{r, \pi, {\bm{x}}} (u) = {\bm{y}}_{u, {\bm{x}}} \quad \text{where} \quad \mathcal{C}_{r, \pi} ({\bm{y}}_{u, {\bm{x}}} \mid {\bm{x}}) = u.$$

#### Inference-time processing.

As mentioned earlier, in many cases we do not simply just sample from the language model. When decoding is done through an inference-time procedure, the obtained sample follows a transformed distribution that depends on the aligned policy and the inference-time procedure. For example, when the inference-time procedure is `BoN` from the base policy $\pi$, it can be shown that the final obtained sample is distributed proportional to $\pi({\bm{y}}\mid {\bm{x}}) \mathcal{C}_{r, \pi}({\bm{x}}, {\bm{y}})^{N-1}$ . In this work, we model inference-time processing as a procedure $T$ that maps $\pi$ to a distribution $T(\pi)$, $$\pi \stackrel{T}{\longrightarrow} T(\pi).$$ In these cases, it is customary to compare the models by considering the following inference-time win rate of the aligned policy $\pi$.

<div class="definition">

**Definition 4** (Inference-time win rate).  * Under inference-time processing $T$, the inference-time win rate of policy $\pi_1$ over $\pi_2$ is defined as $$W^T_r(\pi_1 \succ \pi_2 \mid {\bm{x}}) := \mathbb{E}_{{\bm{y}}\sim T(\pi_1)(\cdot \mid {\bm{x}}), {\bm{z}}\sim T(\pi_2)(\cdot \mid {\bm{x}})} \left\{w_r({\bm{y}}, {\bm{z}}\mid {\bm{x}})\right\}.$$*

</div>

With the above definitions at hand, the goal of the paper is solve the following KL-regularized inference-time win rate maximization problem.

<div class="definition">

**Definition 5** (Inference-time win rate KL-regularized RL problem). *Let $T$ be a given inference-time procedure and $\beta> 0.$ Then, the optimization problem for maximizing inference-time win rate is posed as $$\begin{aligned}
 \label{eqn:kl-constrained-ppwr}
  \max_\pi\left\{ W^T_r(\pi\succ {\pi_{\rm ref}}\mid {\bm{x}}) -\beta D_{\text{KL}}(\pi(\cdot \mid {\bm{x}}) \| {\pi_{\rm ref}}(\cdot | {\bm{x}}))\right\}.
\end{aligned}$$ *

</div>

Note that this formulation reduces to the IPO objective when $T$ is the identity transformation and extends it otherwise for an arbitrary inference-time procedure.[^3]

# Reinforcement learning with reward transformation

In this section, we propose a general framework for solving the language model alignment problem defined in . Our approach is based on designing a new reward function $\mathcal{R}$ based on the reward model $r$, the inference-time procedure $T$, and the base policy ${\pi_{\rm ref}}$, such that solving the KL-regularized RL problem () with the transformed reward $\mathcal{R}$ leads to an almost optimal solution to . More precisely, the aligned policy is the solution to the following optimization problem: $$\label{eqn:kl-rl-tr}
    \max_\pi \mathbb{E}_{{\bm{x}}\sim \mu, {\bm{y}}\sim \pi(\cdot \mid {\bm{x}})} \{\mathcal{R}_{r, {\pi_{\rm ref}}, T}({\bm{x}}, {\bm{y}}) - \beta D_{\text{KL}}(\pi(\cdot \mid {\bm{x}}) \| {\pi_{\rm ref}}(\cdot \mid {\bm{x}}))\},$$ where $\mathcal{R}_{r, {\pi_{\rm ref}}, T}({\bm{x}}, {\bm{y}})$ is a transformed reward function. At first, it might not be immediately clear how might help solve the problem in , however, we will show that for any inference-time procedure $T$, there exists a transformed reward $\mathcal{R}$, which solves .

<div class="lemma">

**Lemma 1**.  * For any base policy ${\pi_{\rm ref}}$, reward model $r$, inference-time procedure $T$, and $\beta> 0$, there exists a reward function $\mathcal{R}_{r, {\pi_{\rm ref}}, T}$ such that the solution to solves the optimization problem in ().*

</div>

In general, such optimal reward transformation will depend on the base policy ${\pi_{\rm ref}}$, the post-hoc procedure $T$, and the reward function $r$. In the lemma below, we list the property that the reward transformation and the resulting optimal aligned policy must satisfy.

<div class="theorem">

**Theorem 1** (Characterization of `InfAlign` solution).  * Assuming that $T$ is such that $\partial T(\pi)({\bm{y}}_1\mid{\bm{x}})/ \partial \pi ({\bm{y}}_2\mid{\bm{x}})$ exists for all ${\bm{x}}, {\bm{y}}_1, {\bm{y}}_2$, then we have the optimal transformed reward $\mathcal{R}$ and the optimal policy $\pi^*$ in must satisfy the following coupled equations $\forall {\bm{x}}, {\bm{y}}$ $$\begin{aligned}
\pi^*({\bm{y}}|{\bm{x}}) & \propto {\pi_{\rm ref}}({\bm{y}}\mid {\bm{x}}) e^{\frac{1}{\beta} \mathcal{R}({\bm{x}},{\bm{y}})} \label{eqn:pi_update}\\
    \mathcal{R}({\bm{x}},{\bm{y}}) &=  \frac{\partial  }{\partial \pi({\bm{y}}\mid {\bm{x}})}W^T_r(\pi \succ {\pi_{\rm ref}}\mid {\bm{x}}) \label{eqn:reward_update}
    \\ & 
    = \sum_{{\bm{z}}} \mathcal{C}_{r, T({\pi_{\rm ref}})}({\bm{x}}, {\bm{z}}) \frac{\partial  }{\partial \pi({\bm{y}}\mid {\bm{x}})}T(\pi)({\bm{z}}\mid{\bm{x}}), \label{eqn:reward_expand}
\end{aligned}$$ where $\mathcal{C}_{r, T({\pi_{\rm ref}})}({\bm{x}}, {\bm{z}})$ is the calibrated reward under the inference-time transformed policy.*

</div>

Missing proofs are presented in . naturally leads to an iterative EM-style algorithm that (I) updates $\pi$ with $\mathcal{R}$ fixed based on and (II) updates $\mathcal{R}$ with $\pi$ fixed based on until convergence. However, such algorithm suffers from two drawbacks: first, for general language models, it is inefficient/intractable to evaluate since it involves evaluating the policy on a large, or even infinite output space; second, it is unclear whether such an algorithm could lead to the optimal solution.

To find more efficient ways to design reward transformations, we examine the case when no inference-time procedure is performed. In this case, $T(\pi) = \pi$ and $$\frac{\partial  }{\partial \pi({\bm{y}}\mid {\bm{x}})}T(\pi)({\bm{z}}\mid{\bm{x}}) = \mathbbm{1}\left\{{\bm{z}}= {\bm{y}}\right\}.$$ will reduce to $\mathcal{R}({\bm{x}},{\bm{y}}) =  \mathcal{C}_{r, {\pi_{\rm ref}}}({\bm{x}}, {\bm{y}})$, the CDF or calibrated reward under ${\pi_{\rm ref}}$.

<div class="corollary">

**Corollary 1**.  * When no inference-time procedure is performed, i.e.$\forall \pi, T(\pi) = \pi$, the solution to with $\mathcal{R}({\bm{x}},{\bm{y}}) =  \mathcal{C}_{r, {\pi_{\rm ref}}}({\bm{x}}, {\bm{y}})$ is the solution to .*

</div>

Note that the above corollary is also observed in . Hence can be viewed as a generalization of these results with general inference-time procedures. The observation motivates us to consider a family of reward transformations based on this calibrated reward, described in the next section. As we will see, for the class of *calibrated* inference-time procedures (), different transformations in such family could be efficiently evaluated through a toy language model, which enables the search for good or even optimal transformations.

# Towards solving `InfAlign`

We first state a few properties of reward calibration in . Then, in , we demonstrate how this approach enables efficient evaluations of different transformations for calibrated inference-time procedures. We will then use best-of-$N$ and worst-of-$N$ as examples of post-hoc procedures to demonstrate the effectiveness of such an approach in . Missing proofs in the section are presented in .

## Reward calibration

Recall the definition of the calibrated reward in . In this section, we assume that $\mathcal{C}_{r, {\pi_{\rm ref}}}$ is already obtained and discuss methods to approximate $\mathcal{C}_{r, {\pi_{\rm ref}}}$ in . We state a few properties of $\mathcal{C}_{r, {\pi_{\rm ref}}}$ below. The first property states that reward calibration preserves the ordering of the reward. This implies that the win rate evaluated under the calibrated reward stays unchanged.

<div class="lemma">

**Lemma 2** (Calibration is a bounded monotone increasing transformation of reward).  * We have $\mathcal{C}_{r, {\pi_{\rm ref}}}({\bm{x}}, {\bm{y}}) \in [0,1].$ Furthermore, we have for any ${\bm{y}}$ and ${\bm{z}}$ $$r({\bm{x}}, {\bm{y}}) \geq r({\bm{x}}, {\bm{z}}) \implies \mathcal{C}_{r, {\pi_{\rm ref}}}({\bm{x}}, {\bm{y}}) \geq \mathcal{C}_{r, {\pi_{\rm ref}}}({\bm{x}}, {\bm{z}})$$*

</div>

Moreover, we have that $\mathcal{C}_{r, {\pi_{\rm ref}}}$ is a canonical transformation of reward, and is invariant under all monotone increasing transformations of the reward function, stated below.

<div class="lemma">

**Lemma 3** (The calibrated reward is invariant under monotone increasing transformations of reward). *Let $m: \mathbb{R} \to \mathbb{R}$ be any monotonic increasing function. Then, $$\mathcal{C}_{m(r), {\pi_{\rm ref}}} = \mathcal{C}_{r, {\pi_{\rm ref}}}.$$ In particular, this also immediately implies that $\mathcal{C}_{\mathcal{C}_{r, {\pi_{\rm ref}}}, {\pi_{\rm ref}}} = \mathcal{C}_{r, {\pi_{\rm ref}}}.$ *

</div>

This property is useful since as long as the learned reward model $r$ can capture relative human preference between pairs, the calibration of $r$ will be the same. Hence $\mathcal{C}$ is more robust to the learning process of $r$.

The next property shows that the calibration operation allows us to transform the distribution of the reward under the base policy to a uniform distribution over $[0, 1]$ regardless of the base policy ${\pi_{\rm ref}}$ and the reward model $r$.

<div class="lemma">

**Lemma 4**.  * If $\pi$ is a continuous language model, let ${\bm{y}}$ be sampled from ${\pi_{\rm ref}}(\cdot \mid {\bm{x}})$, then we have $\forall {\bm{x}}$, $$\mathcal{C}_{r, {\pi_{\rm ref}}}({\bm{x}}, {\bm{y}}) \sim {\rm Unif}([0, 1]).$$*

</div>

The lemma provides us a simple, unified view of the output from a language model through the space of calibrated reward, which is independent from the base policy, and the reward model.

## KL-regularized RL with the calibrated reward

Next we discuss how the calibrated reward can be used in KL-regularized reinforcement learning. As suggests, after calibration, the reward distribution of the outputs from the base policy is independent from the reward model and the base policy itself. This allows us to design a transformation function $\Phi$, focusing only on the inference-time procedure $T$, to be applied on top of the calibrated reward function, independent of ${\pi_{\rm ref}}$ and $r$.

More precisely, let $\Phi: [0, 1] \rightarrow \mathbb{R}$ be a transformation function, we propose the following reward function $$\label{eqn:transformed_reward}
   \mathcal{R}_\Phi({\bm{x}}, {\bm{y}}) = \Phi(\mathcal{C}_{r, {\pi_{\rm ref}}}({\bm{y}}\mid {\bm{x}})),$$ and we would like the aligned policy to be the solution to the KL-regularized RL problem defined in with reward $\mathcal{R}_\Phi({\bm{x}}, {\bm{y}})$, $$\label{eqn:transformed_klrl}
    \pi^*_{\mathcal{R}_\Phi, \beta}(\cdot \mid {\bm{x}}) = \arg \max_\pi\mathbb{E}_{{\bm{y}}\sim \pi(\cdot \mid {\bm{x}})} \{\mathcal{R}_\Phi({\bm{x}}, {\bm{y}})\} - \beta D_{\text{KL}}(\pi(\cdot \mid {\bm{x}}) \| {\pi_{\rm ref}}(\cdot \mid {\bm{x}})).$$

#### Inference-aware reward transformation.

For a given inference-time procedure $T$, our goal is to derive or design a suitable transformation $\Phi$, such that the solution leads to a good or even optimal trade-off between the inference-time win rate $W^T$ and the KL divergence from the base policy.

#### Standard win rate (no inference-time procedure).

When no inference-time procedure is employed (i.e., $T$ is the identity mapping), $W^T$ reduces to the standard win rate. Setting $\Phi$ to be the identity transformation leads to the optimal win rate vs KL trade-off curve by noting that $$\mathbb{E}_{{\bm{y}}\sim \pi(\cdot \mid {\bm{x}})} \{\mathcal{C}_{r, {\pi_{\rm ref}}}({\bm{x}}, {\bm{y}})\} 
    %
    = W_r(\pi \succ {\pi_{\rm ref}}\mid {\bm{x}}).$$ In this case, will be the same as the alignment objective of . Moreover, it can also be shown that when $\Phi(\cdot) = \log(\cdot)$, the solution $\pi^*_{\mathcal{R}_\Phi, \beta}$ recovers the popular best-of-$N$ distillation objective, which has been studied in a recent line of works , and shown to be nearly win rate optimal . We also note that while these methods lead to similar optimization objectives, the algorithmic approaches to solve the problems are different. In , we will discuss a unified algorithm to solve for general $\Phi$. In , we compare the results for $\Phi$ set to the identity mapping with the abovementioned baseline approaches.

We consider a family of inference-time procedures that only depend on the calibrated reward of the outputs, which we term *calibrated procedures*, and discuss how to design a suitable $\Phi$ for this family of transformations. We first define *calibrated procedures* below.

<div class="definition">

**Definition 6** (Calibrated inference-time procedure).  * An inference-time procedure $T$ is called a *calibrated procedure* if there exists a mapping function $g_{T}: [0, 1] \rightarrow \mathbb{R}$ such that for any $\pi$, $r$, and ${\bm{x}}, {\bm{y}}$, we have $$T(\pi)({\bm{y}}\mid {\bm{x}}) \propto \pi({\bm{y}}\mid {\bm{x}}) \cdot g_{T}(\mathcal{C}_{r, \pi} ({\bm{x}}, {\bm{y}})).$$*

</div>

Our next result shows that for calibrated inference-time procedures, the aligned policy from solving has a win rate and KL divergence independent of the base policy and reward function.

<div class="theorem">

**Theorem 2** (Model-agnostic property of calibrated inference-time procedures, informal version of ).  * If $T$ is a calibrated inference-time procedure, for any continuous language model $\pi$, $\beta> 0$ and reward transformation function $\Phi$, we have that both $W^T_r(\pi^*_{\mathcal{R}_{\Phi}, \beta} \succ {\pi_{\rm ref}}\mid {\bm{x}})$ and $D_{\text{KL}}(\pi^*_{\mathcal{R}_{\Phi}, \beta} \| {\pi_{\rm ref}})$ are independent of $r$ and ${\pi_{\rm ref}}$.*

</div>

The above theorem allows us to evaluate a transformation $\Phi$ by focusing on simple continuous language models that are easy to compute and simulate. In the next section, we will use two popular inference-time procedures, best-of-$N$ and worst-of-$N$, as examples to demonstrate how the theorem enables us to efficiently evaluate the inference-time win rate vs KL divergence tradeoff curve for different $\Phi$ functions, which could be used to find a suitable transformation $\Phi$ in practical scenarios.

## Finding better transformations for `BoN` and `WoN`

In this section, we focus on the following two inference-time procedures.

**Best-of-$N$ inference-time procedure (`BoN`)**. During inference, $N$ i.i.d. responses from a policy $\pi$ are generated, i.e., ${\bm{y}}_1, \ldots {\bm{y}}_N \sim \pi$. The final output is the one with the highest reward, i.e., $${\bm{y}}_{\texttt{BoN}} = \arg \max_{{\bm{y}}\in  \{ {\bm{y}}_1, \ldots {\bm{y}}_N\}}
 r({\bm{x}}, {\bm{y}})    .$$

**Worst-of-$N$ inference-time procedure (`WoN`).** Let ${\bm{y}}_1, \ldots {\bm{y}}_N$ be $N$ i.i.d. draws from policy $\pi$. The final output is the one with the lowest reward. i.e., $${\bm{y}}_{\texttt{WoN}} = \arg \min_{{\bm{y}}\in  \{ {\bm{y}}_1, \ldots {\bm{y}}_N\}}
 r({\bm{x}}, {\bm{y}})    .$$ The lemma below presents the distribution of outputs after the inference-time procedure is performed.

<div class="lemma">

**Lemma 5**.  * For any $N$ and continuous language model $\pi$, $$\texttt{BoN}(\pi)({\bm{y}}\mid {\bm{x}}) \propto \pi({\bm{y}}\mid {\bm{x}}) \cdot \mathcal{C}_{r, \pi}({\bm{x}}, {\bm{y}})^{N-1}.$$ $$\texttt{WoN}(\pi)({\bm{y}}\mid {\bm{x}}) \propto \pi({\bm{y}}\mid {\bm{x}})   \cdot  (1 - \mathcal{C}_{r, \pi}({\bm{x}}, {\bm{y}}))^{N-1}.$$*

</div>

Note that the results for `BoN` have already been derived previously . The lemma shows that these two inference-time procedures are calibrated procedures so that as claimed in , for the aligned policy, the inference-time win rate and KL divergence deviation from the base policy are independent of the base policy and reward model. Below we present the precise formula for these two procedures.

<div class="theorem">

**Theorem 3** (Properties of `BoN` and `WoN` procedures).  * For any transformation function $\Phi$, the solution $\pi^*_{\mathcal{R}_{\Phi}, \beta}$ to the KL-regularized RL problem defined in satisfies the followings: Let $F_{\Phi, \beta}(u)= \frac{\int_{0}^u e^{\Phi(u')/\beta} {\rm d}u'}{\int_{0}^1 e^{\Phi(u')/\beta} {\rm d}u'}$. Then the following hold true:*

- *The best-of-$N$ win rate over the base policy satisfies for any ${\bm{x}}$, $$W^{\texttt{BoN}}_r(\pi^*_{\mathcal{R}_{\Phi}, \beta} \succ {\pi_{\rm ref}}\mid {\bm{x}}) = 1 - N \int_{0}^1 F_{\Phi, \beta}(u)^N u^{N-1} {\rm d}u,$$*

- *The worst-of-$N$ win rate over the base policy satisfies for any ${\bm{x}}$, $$W^{\texttt{WoN}}_r(\pi^*_{\mathcal{R}_{\Phi}, \beta} \succ {\pi_{\rm ref}}\mid {\bm{x}}) =  N \int_{0}^1 \left(1 - F_{\Phi, \beta}(u) \right)^N (1-u)^{N-1} {\rm d}u,$$*

- *The KL divergence between $\pi^*_{r_{\Phi}, \beta}$ and ${\pi_{\rm ref}}$ satisfies $$\label{eqn:kl_div_2}
      D_{\text{KL}}(\pi^*_{\mathcal{R}_{\Phi}, \beta} \| {\pi_{\rm ref}}) =
  \frac{1}{\beta}  \frac{ \int_{0}^1 \Phi(u) e^{\Phi(u)/\beta} {\rm d}u }{\int_{0}^1 e^{\Phi(u)/\beta} {\rm d}u} - \log \left(\int_{0}^1 e^{\Phi(u)/\beta} {\rm d}u\right).$$*

</div>

By varying $\beta$ in , we can obtain an alignment curve plotting the inference-time win rate and KL divergence deviation for different aligned policies. This allows us to compare the performance of different transformation functions $\Phi$.

In the rest of the section, we will consider different types of transformations, and analytically compute the alignment curves using by varying $\beta$, i.e., the plot of $(D_{\text{KL}}(\pi^*_{r_{\Phi}, \beta} \| {\pi_{\rm ref}}), W^{\rm T}_r(\pi^*_{r_{\Phi}, \beta}) \succ {\pi_{\rm ref}})$ for different $\beta$’s. The transformations we consider include optimal transformations for standard win rate, exponential functions, and optimization-based transformations, described below.

#### Optimal reward transformations for standard win rate.

The identity mapping proposed by and the logarithmic mapping as used by `BoN` distillation are shown to be (almost) optimal for the standard win rate. We would like to understand whether they are still good candidates when inference-time procedures are considered.

#### Deriving an optimized reward transformation function.

For calibrated inference-time procedures like `BoN`, and `WoN`, due to , we have that the optimal reward transformation $\Phi$ is independent of the base policy. One can optimize for good $\Phi$’s using simple toy language models, and we have the following corollary.

<div class="corollary">

**Corollary 2**.  * For any $\beta> 0$, the $\Phi$ that achieves the optimal `BoN` win-rate vs. KL tradeoff must satisfy the following pair of equations: $$\begin{gathered}
    \Phi_{\texttt{BoN}}(u) = -N^2 \int_u^1 F_{\pi}(v)^{N-1} v^{N-1} {\rm d}v ,\\
    f(u) \propto e^\frac{\Phi_{\texttt{BoN}}(u)}{\beta},
\end{gathered}$$ and for `WoN`, it satisfies that $$\begin{gathered}
    \Phi_{\texttt{WoN}}(u)  =  -N^2 \int_u^1 (1-v)^{N-1} (1 - F_{\pi}(v))^{N-1} {\rm d}v,\\
    f(u) \propto e^\frac{\Phi_{\texttt{WoN}}(u)}{\beta}.
\end{gathered}$$*

</div>

Hence one can find a transformation function based on finding the fixed point of the coupled equations in through iterative updates.

<figure id="fig:simulation">
<p><span class="image placeholder" data-original-image-src="win-rate-vs-KL.png" data-original-image-title="" width=".48\textwidth">image</span><br />
<span class="image placeholder" data-original-image-src="Best-of-2.png" data-original-image-title="" width=".48\textwidth">image</span>  <span class="image placeholder" data-original-image-src="Best-of-4.png" data-original-image-title="" width=".48\textwidth">image</span> <span class="image placeholder" data-original-image-src="Worst-of-2.png" data-original-image-title="" width=".48\textwidth">image</span>  <span class="image placeholder" data-original-image-src="Worst-of-4.png" data-original-image-title="" width=".48\textwidth">image</span></p>
<figcaption>Best-of-<span class="math inline">\(N\)</span> and worst-of-<span class="math inline">\(N\)</span> win rate vs KL tradeoff curves for <span class="math inline">\(N = 2, 4\)</span> with different transformation functions. </figcaption>
</figure>

#### Exponential tilting for reward transformation.

In addition to deriving the optimized transformation, motivated by the exponential tilting of loss functions , we consider the following exponential transformation: $$\Phi_t(u) = \text{sign}(t) \cdot e^{t u},
\label{eqn:exp_transform}$$ where $\text{sign}(t) = 1$ for $t\geq 0$ and $\text{sign}(t) = -1$ for $t<0.$ These exponential transformations are essentially helping to optimize different quantiles of the reward for different values of $t$ . For a positive value of $t$, the exponential tilting transformation focuses on optimizing the high quantiles of the objective (calibrated reward) and indeed recovers the max function for a large positive $t$. Hence, we expect that positive values of $t$ help with `BoN` inference-time procedure. On the other hand, for a negative value of $t$, the transformation is akin to optimizing the lower quantiles of the objective (calibrated reward), which makes it a suitable transformation for the `WoN` inference-time procedure.

#### Results.

In , we consider an ideal continuous language model and compare different alignment objectives on inference-time win rate vs KL divergence for three inference-time procedures: {standard, `BoN`, `WoN`}. The tradeoff curves are obtained by varying the strength of the regularizer, $\beta$. The different objectives correspond to different transformations of the calibrated reward, which include (1) identity mapping $\Phi =I(\cdot)$ (IPO); (2) logarithmic mapping $\Phi = \log$ (best-of-$N$ distillation); (3) exponential tilting $\Phi_t(\cdot)$ as defined in ; (4) optimized transformation function by solving for fixed points in .

In the first plot, we consider standard win rate. In this case, it is known that the IPO objective is win rate optimal. As can be seen, the logarithmic transformation (i.e., best-of-$N$ distillation) also achieves a nearly optimal win rate, which was already observed by . All other transformations are sub-optimal for standard win rate.

Next, we consider Best-of-$2$ and Best-of-$4$ win rate. Here, additionally we include `bon_opt`, which is obtained by deriving the fixed point of . As can be seen, the identity transformation is no longer optimal. The best tradeoffs are given by `bon_opt`. We also observe that $\exp(5x)$ and $\exp(10x)$ are almost as good as `bon_opt` for Best-of-$2$ and Best-of-$4,$ respectively. Moreover,the identity transformation and logarithmic transformation are sub-optimal in these cases, which shows that considering standard win rate as the only metric is not optimal when inference-time procedure is concerned. We also observe that the behavior of identity transformation and logarithmic transformation is different in that the identity transformation gives better tradeoffs.

Finally, we consider Worst-of-$2$ and Worst-of-$4$ win rate. Again, it can be observed that `bon_opt` gives the best tradeoffs for this inference-time procedure. Here, $\exp(-5x)$ and $\exp(-10x)$ are almost as good for Worst-of-$2$ and Worst-of-$4,$ respectively. We also observe that identity transformation and logarithmic transformation are sub-optimal in these cases and the logarithmic transformation gives better tradeoffs for `WoN` compared to the identity transformation.

The above results demonstrate the importance of considering the inference-time procedure when performing alignment. We find that exponential transformation with different $t$’s are good for different inference-time procedures, which will be our focus in practical experiments. Next, we will examine whether a good transformation that we found on the idealized continuous language model generalizes in the wild to real-world scenarios. Before moving on to the experiments, we will have to offer a practical algorithm for solving the inference-time KL-regularized RL optimization problem, which is the subject of the next section.

# `CTRL`: Calibrate-and-Transform Reinforcement Learning

In this section, we propose the `CTRL` method, which is our proposed solver for the inference-time win rate optimization problem. Recall from the previous section that the proposed solution could be decomposed into three stages: *reward calibration* and *reward transformation* followed by the standard KL-RL solver. In the previous section, we highlighted the reward transformation. In the rest of this section, we focus on approximate empirical calibration. Combining it with reward transformation yields the final $\texttt{CTRL}$ algorithm, given in Algorithm .

Consider *empirical calibration*, where we draw $K$ samples ${\bm{z}}_1,{\bm{z}}_2, ..., {\bm{z}}_K$ from the reference model ${\pi_{\rm ref}}$ for each prompt ${\bm{x}}$ in the RL training data. We then sort the rewards to all the responses $\{r({\bm{x}}, {\bm{z}}_1), r({\bm{x}}, {\bm{z}}_2),... r({\bm{x}}, {\bm{z}}_K)\}$, and assign *empirical calibrated reward* scores during RLHF training for the prompt, response pair $({\bm{x}}, {\bm{y}})$ as $$\begin{aligned}
    \widetilde{\mathcal{C}}_{r, {\pi_{\rm ref}}}({\bm{x}}, {\bm{y}}) = \frac{1}{K} \sum_{i=1, z_i \sim {\pi_{\rm ref}}}^K  \mathbbm{1} [r({\bm{x}}, {\bm{y}}) \geq r({\bm{x}}, {\bm{z}}_i) ].
\end{aligned}$$ Ideally, as $K \rightarrow \infty$, the empirical calibrated reward would converge to the true calibrated reward, and may be used in the RL training objective through PPO . However, this could be costly as the exact computation of this calibrated reward requires us to sample and store $K$ reward scores per-prompt and per roll-out in the KL-RL solvers.

Instead, we propose to *approximate* the calibration curve by scaling it with a step-wise function in the logarithmic domain. We do this by choosing $p$ anchor points $q_1, q_2,\ldots$, where at each of the quantiles $q_i \in (0,1)$ we achieve zero calibration error. The algorithm for the simpler case ($p=1$, median) is given in Algorithm . For larger values of $p$, the algorithm is given in Algorithm  ().

# Experiment Results

## Evaluation setup

#### Datasets.

To train the reward models, we use the Anthropic Helpfulness and Harmlessness datasets which involve multi-turn dialogues between a human and a digital assistant. The preference datasets consist of two responses for one context, and a label for the human preference for the response. We use the train split of the two datasets (44K examples for helpfulness and 42K for harmlessness) to train the uncalibrated and calibrated reward models – separate reward models for each objective.

#### Model.

The uncalibrated reward model is trained based on the Bradley-Terry pairwise objective , and the calibration is done on the training-split of the RL training procedure by drawing samples from the reference model. The underlying model for both these rewards is the PaLM-2 S model . The base reference policy model is a PaLM-2 S model that is fine-tuned (SFT) on the Anthropic dialog preferred responses. We then train the aligned policy model through KL-regularized Reinforcement Learning . We compare against `uncalibrated` (a model trained with KL-RL using PPO and no further processing), `BoNBoN` , `Best-of-N` , `BonD` , and `IPO`  as baselines.

#### True rewards.

As evaluating using ground truth rewards in a pointwise manner based on human annotations can be expensive, we follow and perform automated evaluation using a larger PaLM-2 M model to compute *true rewards*.

#### Metrics.

To measure improvement due to post-RL training, we report both the win rate and the `BoN` and `WoN` win rates, calculated after applying the respective inference-time procedures on the responses generated by the RL model against the base SFT model, along with the corresponding KL-divergence of the RL model with the SFT model. For each of the runs, we experiment with different KL-regularizer strengths ($\beta \in \{0.01, 0.02, \ldots , 0.09\}$) and obtain the Pareto-curve of the KL divergence vs {standard, `BoN`, `WoN`} win rate curves.

## Reward models are typically miscalibrated

We first validate our hypothesis that reward models used on real-world tasks are miscalibrated. We measure the miscalibration of the reward model trained on Anthropic helpfulness preference dataset by computing the scores of $100$ reference-policy responses for $10$ random prompts from training split. We then sort the scores and compute the ranks corresponding to each of the responses and plot these values as a scatter plot in Figure  (left). If the model were perfectly calibrated, the points for each prompt would lie on the line $y=x$. However, observe that for most prompts, the scatter plot deviates significantly from the $y=x$ line, and the extent of this deviation varies depending on the prompt.

We then measure the Absolute Error (AE) between the reward scores and their corresponding ranks and plot the cumulative distribution function (CDF) of the AE of the various calibration approximations in Figure  (right). If the model is well-calibrated AE is zero always and hence the CDF reaches one at zero AE. We find that the reward scores (see legend named ‘identity’) are not calibrated (mean AE: $0.22$) and using fixed reward polynomial transformation functions like square-root, cube, square - do not reduce the calibration error (mean AE $> 0.15$). However, using a per-prompt quantile-based reward calibration, see legend named ‘quantile’) significantly reduces the calibration error (mean AE: $0.02$).

<figure id="fig:1">
<figure>

</figure>
<figure>

</figure>
<figcaption>Results on reward models trained on the Anthropic helpfulness preference dataset. (left) Scatter plot of reward scores and best-of-n ranks on a random sample of 10 prompts in the Anthropic helpfulness dataset. Note that the model shows miscalibration on most prompts, with the degree of miscalibration varying by prompt. (right) Plot of CDF of absolute error (AE). Observe that per-prompt quantile based methods have low AE with high probability, where as prompt-agnostic transformations have high AE typically. <span></span> </figcaption>
</figure>

## Calibrated rewards improve standard win rate

As a sanity check of `InfAlign`, we first measure the performance when there is no inference-time procedure applied. Without any inference-time procedure, the optimization objective is *standard win rate*, and we compare the performance of `CTRL` (using an identity transform) against other relevant reward optimization baselines that are known to be (almost) win rate optimal, such as IPO and Best-of-N distillation . Specifically, we calibrated the reward model for helpfulness and harmlessness of the base model fine-tuned on preferred responses using Algorithm  with $p=1$. In Figure , we find that compared to IPO and BoNBoN, the calibrated reward optimization achieves better win rate-KL trade-offs. We attribute this gain to a more efficient computation of win rate on training data using $m$ samples from the base model, as opposed to relying of existing pairwise comparison data during KL-RL. Further, we find that the reward transformations applied on the calibrated reward perform similar to each other, thus validating the theoretical results of . We also find that reducing the calibration error with more anchors or by varying $m$, leads to better win rate vs KL tradeoffs (see ).

<figure id="fig:winrate">
<figure>

</figure>
<figure>

</figure>
<figcaption>Calibrating the helpfulness and harmlessness reward improves average win rate (identity transformation).</figcaption>
</figure>

## `CTRL` improves `BoN`

For the helpfulness objective in the Anthropic dialog dataset, we aim to optimize the Best-of-$N$ performance of the aligned model through the exponential transformation of the calibrated rewards. We measure the win rate against the Best-of-$N$ of the base policy model ($N$=4). In Figure we see that calibration based on the median rewards per-prompt achieves $8-12\%$ higher Best-of-$N$ win rates as compared to the uncalibrated model on helpfulness objective. The exponential transformation of the calibrated reward outperforms the rest of the models. We find that the exponential factor of $t=10$ works best as simulated by our framework on a toy-setting (see Section ). Further, we show that these gains hold for varying values of $N (2, 32)$ (see Appendix ).

## `CTRL` improves `WoN`(`BoN` jailbreaks)

For the harmlessness objective in the Anthropic dialog dataset, we aim to improve the Worst-of-n performance of the aligned policy model to improve safety against adversarial actors . Here, we use the negative exponential transformation $t<0$. In Figure  we see that calibration based on the median rewards per-prompt achieves $4-9\%$ higher Worst-of-$N$ win rates as compared to the uncalibrated model. The negative transformation of the calibrated reward outperforms the rest of the models, with $t=-10$ performing the best: again identified as the optimal value per our simulation in a toy setting (see Section ).

<figure id="fig:2">
<figure>

</figure>
<figure>

</figure>
<figcaption>Calibration improves Best-of-<span class="math inline">\(N\)</span> and Worst-of-<span class="math inline">\(N\)</span> win rates for helpfulness (left) and harmlessness (right) objectives in the Anthropic dialog dataset respectively. We report win rate against the Best-of-<span class="math inline">\(N\)</span> and Worst-of-<span class="math inline">\(N\)</span> rewards of the base SFT model responses on the test split as measured by the PaLM-2 M reward model trained on the helpfulness and harmlessness data respectively.</figcaption>
</figure>

# Related work

**Inference-time compute.** Test-time compute has been leveraged in recent work to achieve better win rate vs KL tradeoffs from the aligned models including controlled decoding , Monte Carlo tree search , iterative jailbreak query refinement , and model-chaining within agentic frameworks . Best-of-N (`BoN`) is also used as an evaluation metric in code and natural language generation benchmarks . Further, Worst-of-N (`WoN`) is a popular jailbreaking strategy for adversarial actors to elicit unsafe text from large language models . Prior work has largely focused on approximating inference-time solutions during training time through sampling , distillation , and decoding . Our work is orthogonal to this body of work as they assume that no inference-time procedure is applied, but rather attempt to approximate it during training. We show that our theoretical framework generalizes IPO  and best-of-$N$ distillation  as special cases.

We are motivated by recent work that apply meta-generation procedures at inference-time such as chaining prompted models , problem decomposition through chain-of-thought , Best-of-N reranking applied on reasoning traces . Our `InfAlign` framework was also motivated by complex inference-time strategies that involve transformation techniques such as refinement , majority voting , or using the generator as input to other search algorithms , that have outperformed other models for harder tasks. In this spirit, our framework allows to get additional gains in aligning models with such inference-time procedures deployed in the future.

**Reward miscalibration.** Reward miscalibration or hacking has been studied extensively in recent work . The hypotheses behind reward hacking can be broadly categorized into 3 themes: (1) reward underspecification, (2) training-serving skew between pairwise and pointwise reward models, (3) dominant reward due to adhoc transformations. Reward models suffer from *underspecification* due to under-specified training data by capturing spurious correlations in the data . Methods to mitigate this often include training on non-overlapping splits during reward model fine-tuning and ensembling . Our `CTRL` method can be easily augmented with such data interventions in reward learning.

**KL-RL solvers.** Training reward models on pairwise preference data, and then using it as pointwise scorers during reinforcement learning poses problems of transitive inconsistency. To mitigate this problem, optimization techniques that directly incorporate the pairwise preference data during offline reinforcement learning have been proposed . Further, calibrating model probabilities to reflect rank-order generated sequences by quality metrics have been proposed . We share the motivation behind these methods, while additionally recognizing the need to calibrate the rewards against the base policy on which we are aligning.

When aligning language models for multiple objectives, aggregating the rewards via a weighted sum is known to result in reward hacking of one of the dominant rewards. Thresholding the effect of individual rewards or changing the weights of the training data , however requires costly hyper-parameter fine-tuning and retraining without the ability to reason about the hyperparameters and their effects on the reward-tradeoffs. Reward transformation techniques that calibrate against a reference reward is effective at mitigating domination of one reward , but implicitly assumes that the reward aggregation function is a logical "AND" of all rewards, heavily penalizing under-performance on any of the rewards. Motivated by the success of exponential tilting for focusing on high/low quantiles , we also show that `CTRL` with exponential reward transformation achieves near-optimal inference-time win rate vs KL divergence tradeoffs, surpassing the performance of methods such as IPO  that target to optimize standard win rate vs KL divergence tradeoffs. In this paper, we show that calibration as a first-step can help ground reward transformations based on the final inference-time procedure applied. Further, we build on recent work that show the theoretical guarantees of Best-of-$N$ sampling over most reinforcement learning optimization techniques to ground our calibration and transformation method.

# Concluding Remarks

In this paper, we have shown that existing win rate optimal alignment procedures are sub-optimal when inference-time procedures are applied on aligned language models. As models equipped with inference-time procedures such as reasoning, best-of-$N$, majority voting continue to outperform models which do not use such procedures, we study the question of how to learn inference-aware optimally aligned language models. While learning optimal solutions for general inference-time procedures is intractable, we propose `InfAlign` — a framework that optimizes for inference-time win rate, and provide theoretical guarantees of finding an optimal inference-aware aligned model. Our framework generalizes prior work on win rate optimal solutions . We further show that, for any inference time procedure, such an optimal model can be learned through KL-RL optimization using reward transformation. For a class of transformations that rely only on the rank of rewards, calibration of said rewards into a uniform distribution allows us to search efficiently for the optimal reward transformation through empirical simulation.

We demonstrate the efficacy of this framework, by transferring findings from empirical simulation to real-world tasks and propose `CTRL` — a calibrate-and-transform reinforcement learning solver for ranking based inference-time procedures — and particularize it to Best-of-$N$ sampling (`BoN`) and jailbreaking (`WoN`). Empirically, we demonstrate on Anthropic dialog helpfulness and harmlessness datasets that, (1) in the standard setting when no inference-time procedure is applied, `CTRL` with identity reward transformation achieves competitive or slightly better performance compared to a variety of SOTA methods for optimizing standard win rate (2) when inference-time procedures are applied, we outperform inference-time win rate vs KL tradeoffs compared to existing preference optimization methods by 8-12% for `BoN` and 4-9% for `WoN` inference-time procedures respectively.

Future work includes finding efficient solvers for complex inference-time procedures based on `InfAlign` that do not rely on the rank of rewards assigned to samples from the base policy model, e.g., reasoning . Further, practical solvers for reward aggregation of multiple competing objectives at inference-time could also be explored. From a theoretical standpoint, the generalizability of our inference-aware alignment approach to upstream tasks such as supervised fine-tuning or pre-training needs to be studied.

# Missing proofs

# The role of KL divergence in model alignment

One question that arises is the role of the KL divergence regularizer in . In this section, we argue that the regularizer essentially enables multi-tasking between the SFT task and the RL task.

Let’s consider a log-linear model such that $$\pi_\theta(y|x) = e^{\theta^T g(x, y) - A(\theta; x)} ,$$ where $g(x,y)$ is a fixed encoding of $(x, y)$, and $A(\theta;x)$ is the partition function normalizing the distribution.

#### Supervised finetuning (SFT).

Let $D_\text{sft}(x, y) = \mu(x) \times p_\text{sft}(y|x)$ be the SFT data distribution. Then, the SFT task is $${\theta}^*_\text{sft}= \arg\min_\theta \mathcal{L}_\text{sft}(\theta) \quad \quad \text{where} \quad \quad \mathcal{L}_\text{sft}(\theta) := E_{(x, y) \sim D_\text{sft}} \{ A(\theta; x) - \theta^\top  g(x, y)\},$$ We further call ${p}= \pi_{\theta^*_\text{sft}}$.

<div class="lemma">

**Lemma 6**. *The SFT solution satisfies $$E_{x\sim \mu} \{\nabla_\theta A(\theta^*_\text{sft})\} = E_{(x,y) \sim D_\text{sft}} g(x,y).$$ *

</div>

<div class="proof">

*Proof.* This is a known property of exponential families. The proof follows by noticing $\nabla_\theta \mathcal{L}_\text{sft}(\theta^*_\text{sft}) = 0.$ ◻

</div>

#### KL-regularized reward optimization (RO).

Let $r$ be a reward function that determines the reward for each $(x,y).$ Let $\mathcal{L}_\text{ro}(\theta) :=  E_{x \sim \mu} E_{y \sim \pi_\theta} r(x, y)$. Then, $$\theta^*_{\text{bilevel}, \beta} = \arg\min_\theta  \mathcal{L}_{\text{bilevel}, \beta}(\theta) \quad\quad \text{where} \quad\quad \mathcal{L}_\text{bilevel}(\theta ) :=  D_{\text{KL}}(\pi_\theta \| {p}) + \frac{1}{\beta}\mathcal{L}_\text{ro}(\theta),$$ where $D_{\text{KL}}(\pi_\theta \| {p}) = E_{x \sim \mu} D_{\text{KL}}(\pi_\theta(\cdot|x) \| {p}(\cdot|x))$.

#### Multi-tasking SFT and RO.

Now consider the following tasks $$\theta^*_{\text{multi-task}, \beta} = \arg\min_\theta \mathcal{L}_{\text{multi-task}, \beta}(\theta) \quad \quad \text{where} \quad \quad \mathcal{L}_\text{multi-task}(\theta) := \mathcal{L}_\text{sft}(\theta) + \frac{1}{\beta} \mathcal{L}_\text{ro}(\theta) .$$

<div class="theorem">

**Theorem 4**. *For all $\beta \in \mathbb{R},$ we have $\theta^*_{\text{bilevel}, \beta} = \theta^*_{\text{multi-task}, \beta}.$ *

</div>

<div class="proof">

*Proof.* Notice that $$\begin{aligned}
    \mathcal{L}_{\text{bilevel},\beta}(\theta) &= D_{\text{KL}}(\pi_\theta \| {p}) + \frac{1}{\beta} \mathcal{L}_\text{ro}(\theta) \\
    &= E_{x \sim \mu}  \{A(\theta; x) - A(\theta^*_\text{sft}; x) - (\theta- \theta^*_\text{sft})^\top \nabla_\theta A(\theta^*_\text{sft}; x) \}  +\frac{1}{\beta}\mathcal{L}_\text{ro}(\theta) \label{eq:bregman}\\
    & = E_{x \sim \mu} \{A(\theta; x) - A(\theta^*_\text{sft}; x)\} - (\theta - \theta^*_\text{sft})^\top E_{(x,y) \sim D_\text{sft}} g(x,y) + \frac{1}{\beta} \mathcal{L}_\text{ro}(\theta) \label{eq:lemma1}\\
    &= \mathcal{L}_{\text{multi-task}, \beta}(\theta) + \mathcal{L}_\text{sft}(\theta^*_\text{sft}), \label{eq:last_ineq}
\end{aligned}$$ where follows by noticing that KL divergence is a Bregman divergence in this setup, follows from Lemma , and follows from the definition of $\mathcal{L}_\text{sft}(\theta)$ applied to $\theta$ and $\theta^*_\text{sft}$. Hence, the minimizers of the two objectives are the same given that $$\mathcal{L}_{\text{bilevel},\beta}(\theta) = \mathcal{L}_{\text{multi-task}, \beta}(\theta)+ C,$$ completing the proof. ◻

</div>

Thus, effectively this proves that the KL-RL objective enables multi-tasking between the SFT stage and the reward optimization RL objective. One may wonder why we did not pose the KL divergence regularizer on the transformed distributions through $D_{\text{KL}}(T(\pi)(\cdot \mid {\bm{x}}) \| T({\pi_{\rm ref}})(\cdot | {\bm{x}}))$ instead. Consider the Best-of-$N$ jailbreaking for example. While the adversary may be using the model to generate $N$ responses and choose the least safe one for jailbreaking, the model should possess the core capabilities for other types of inference-time usage for other tasks that is different from that of jailbreaking (e.g., through chain-of-thought). Therefore, changing the KL divergence regularizer does not capture the fact that the model should remain suitable for all other tasks, and not just for the one for which it is getting aligned. We also note that if we used $D_{\text{KL}}(T(\pi)(\cdot \mid {\bm{x}}) \| T({\pi_{\rm ref}})(\cdot | {\bm{x}}))$ instead, the problem would actually simplify to the standard alignment problem through a simple change of variables.

# Zero-shot prompt

We also evaluate the win rate using a prompted PaLM-XL model with the following instructions:

# The effect of anchor points on calibration

In this section, we analyze the effect of using more anchor points on results. First, we present the full algorithm for more than one anchor point in .

## Better calibration leads to better gains

The effectiveness of calibration can be better understood when we approximate the calibration with fewer number of rollouts: $n$. When $n=1$, we take one other rollout from the base policy model and use that as a reference reward, similar to , and with $n=3,5,7$, we approximate the median with this limited sample, and do the reward calibration. In Fig (a), we find that calibration using more number of samples improves the win rate, with $n=1$ providing no gains in the KL vs win rate tradeoff. Additionally, in Fig (b), when we calibrate with more anchor points, we find that the KL vs win rate tradeoffs improves as compared to the median-based calibration. This is further supported by the fact that with more anchor points, the calibration error is significantly reduced as shown in Fig (b). Hence, we find that as the calibration error is reduced, we achieve better KL vs win rate tradeoffs, thus able to better implicitly approximate the best-of-n model.

<figure id="fig:3">
<figure>

</figure>
<figure>

</figure>
<figcaption>(a) Calibration improves KL vs win rate tradeoff even when we approximate calibration of median approximated with n=7 samples, whereas with n=1,3,5, the gains in win rate for a fixed KL-budget is lower, but still outperforms the uncalibrated model at higher KL values. Using n=100 samples is (calibrate_median_pareto) provides the best tradeoff (b) Calibration with two or more anchor points per-prompt improves KL-winrate tradeoff, as compared to median-based approximation - however there is diminishing gains with ten anchor points as compared to using two anchor points. The win rate is measured using the PaLM-2 M reward model trained on the Anthropic helpfulness preference dataset.</figcaption>
</figure>

## Calibration Reduces Reward Hacking

We demonstrate that calibrated reward models are less susceptible to reward hacking, a phenomenon where models exploit spurious correlations in training data to optimize for reward signals instead of true task objectives.

To induce reward hacking, we injected specific phrases into the start of preferred responses of our preference datasets: “Sorry, I can’t help you with that” for Harmlessness and “Sure” for Helpfulness. We then evaluated the model’s accuracy on a poisoned evaluation set where these phrases were inverted (added to the *unpreferred* responses). A significant drop in accuracy on this poisoned set would indicate reward hacking: a reliance on the spurious correlation.

Figure shows that calibrated reward models are far more robust to these manipulated correlations, maintaining higher accuracy compared to uncalibrated models. Note that the calibration mechanism used here is different from the median-based calibration, and instead uses a pointwise loss.

<figure id="fig:reward_hacking">
<span class="image placeholder" data-original-image-src="reward_hacking_fig.png" data-original-image-title="" width="0.65\columnwidth"></span>
<figcaption>Calibrated reward models demonstrate robustness against reward hacking: We poisoned the training data by adding phrases to the preferred response to induce spurious correlations. When we evaluated against a test set where the correlations are inverted (phrase added to unpreferred models), calibrated models maintained higher accuracy than uncalibrated ones, demonstrating their reduced reliance on spurious correlations.</figcaption>
</figure>

## Gains are higher with more inference-time compute available

As can be seen, in , for $N=32$, we see better gains as compared to $N=2$ in both `BoN` and `WoN` win-rates on Anthropic helpfulness and harmlessness datasets. Thus, as more inference-time compute becomes available in the future, we find that `InfAlign` can scale and further augment the performance gains.

<figure id="fig:varying_n_winrate">
<figure>

</figure>
<figure>

</figure>
<figure>

</figure>
<figure>

</figure>
<figcaption>Higher N in <code>BoN</code> and <code>WoN</code> gives better gains in inference-time win rate-KL tradeoffs. The first row indicate the gains in <code>BoN</code> helpfulness, and second on <code>WoN</code> harmlessness for N=2,32.</figcaption>
</figure>

[^1]: Equal contribution.

[^2]: *The definition is similar to the cumulative density function of the reward $r({\bm{x}}, {\bm{y}})$ under policy $\pi$ except for how ties are decided.*

[^3]: One question that arises is the role of the KL divergence regularizer in . We argue that the regularizer essentially enables multi-tasking between the SFT task and the RL task, which we formally prove for log-linear models in . In other words, the KL divergence regularizer enables to preserve the core capabilities of the model while acquiring a new one through the KL-RL process.
