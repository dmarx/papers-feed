- **Objective of InfAlign**: Optimize inference-time win rate of aligned model against reference model using KL-regularized RL framework.
  
- **Key Definitions**:
  - **KL-regularized RL Problem**: 
    \[
    \pi^*_{r,\beta}(• | x) = \arg \max_\pi E_{y \sim \pi(•|x)} \{r(x, y)\} - \beta D_{KL}(\pi(• | x) \| \pi_{ref}(• | x))
    \]
  - **Inference-time Win Rate**:
    \[
    W^T_r(\pi_1 \succ \pi_2 | x) = E_{y \sim T(\pi_1)(•|x), z \sim T(\pi_2)(•|x)} \{w_r(y, z | x)\}
    \]

- **Framework Overview**: InfAlign modifies the alignment objective to account for inference-time decoding methods, ensuring that the model is optimized for practical use cases.

- **CTRL Algorithm**: 
  1. **Calibrate** the reward model scores based on responses from the reference model.
  2. **Transform** the calibrated scores according to the inference-time procedure.
  3. **Solve** the KL-RL problem using existing optimization algorithms (e.g., PPO).

- **Inference-time Procedures**:
  - **Best-of-N (BoN) Sampling**: Selects the response with the highest reward from N samples.
  - **Best-of-N Jailbreaking (WoN)**: Selects the response with the lowest reward, often used for safety evaluations.

- **Theoretical Findings**:
  - Optimal reward transformation is independent of the base policy and reward model for BoN and WoN.
  - Exponential reward transformation is nearly optimal for these strategies.

- **Empirical Results**: CTRL outperforms state-of-the-art methods by 8-12% for BoN and 4-9% for WoN on the Anthropic helpfulness and harmlessness datasets.

- **Key Equations**:
  - **Transformed Reward Function**:
    \[
    R_{r,\pi_{ref},T}(x, y) = E_{x \sim \mu, y \sim \pi(•|x)} \{R(x, y) - \beta D_{KL}(\pi(• | x) \| \pi_{ref}(• | x))\}
    \]
  - **Optimal Policy Relation**:
    \[
    \pi^*(y|x) \propto \pi_{ref}(y | x)e^{\frac{1}{\beta} R(x,y)}
    \]

- **Win Rate Calculation**:
  - Win rate is calculated using the calibrated reward:
    \[
    C_{r,T}(\pi_{ref})(x, z) = E_{z \sim \pi(•|x)} w_r(y, z | x)
    \]

- **Conclusion**: InfAlign provides a robust framework for aligning language models with practical inference-time procedures, enhancing their performance in real-world applications.