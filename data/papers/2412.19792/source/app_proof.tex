\subsection{Proof of \cref{thm:exisitence_informal}}
We start by stating the policy obtained by KL-regularized RL problem,  which can be obtained by standard arguments in the literature (\eg, \citep{korbak2022rl, rafailov2024direct, yang2024asymptotics}).

\begin{lemma}\label{lem:kl_rl}
The solution to the optimization problem in  \cref{def:kl_rl} satisfies
\[
    \pi_{r, \reg}^*(\by \mid \bx) \propto \bp(\by \mid \bx) \exp\Paren{\frac{r(\bx, \by)}{\reg}}.
\]
\end{lemma}

\begin{proof}[Proof of \cref{thm:exisitence_informal}]
Let $\pi_T^*$ be a solution to the optimization problem in \cref{eqn:kl-constrained-ppwr}. Note that for all $\by$ such that $\pi_T^*(\by \mid \bx) > 0$, we must have $\bp(\by \mid \bx) >0$ since otherwise the KL divergence would become infinite.

Then setting
%
$$\tr(\bx, \by)  =  \reg \log(\pi_T^*(\by \mid \bx) / \bp(\by \mid \bx))$$ in \cref{eqn:kl-rl-tr} leads to the solution $\pi_T^*$ being its optimal solution as shown in \cref{lem:kl_rl}.
\end{proof}


\subsection{Proof of \cref{lem:coupled}}
We first prove the following auxiliary lemma that we need in the proof.
\begin{lemma}\label{lem:pp_win_rate_decompose}
For any inference-time procedure $\pp$, we have
\[
    W^\pp_r(\pi_1 \succ \pi_2 \mid \bx) = \sum_{\bz} \qt_{r, T(\pi_2)}(\bx, \bz) T(\pi_1)(\bz \mid\bx).
\]
\end{lemma}
\begin{proof}
The proof follows from the following identities:
    \begin{align*}
     W^\pp_r(\pi_1 \succ \pi_2 \mid \bx)&  = \E_{\bz \sim \pp(\pi_1)(\cdot \mid \bx), \by \sim \pp(\pi_2)(\cdot \mid \bx)} \left\{w_r(\bz, \by \mid \bx)\right\} \\
     & = \E_{\bz \sim \pp(\pi_1)(\cdot \mid \bx)} \left\{\E_{\by \sim \pp(\pi_2)(\cdot \mid \bx)}\left\{w_r(\bz, \by \mid \bx)\right\} \right\} \\
     & =  \E_{\bz \sim \pp(\pi_1)(\cdot \mid \bx)} \left\{ \qt_{r, T(\pi_2)}(\bx, \bz) \right\} \\
     & = \sum_{\bz} \qt_{r, T(\pi_2)}(\bx, \bz) T(\pi_1)(\bz \mid\bx).
    \end{align*}
\end{proof}

\begin{proof}[Proof of \cref{lem:coupled}]
Note that \cref{eqn:kl-constrained-ppwr} has an implicit constraint that $\pi(\cdot \mid \bx)$ must be a valid distribution, \ie
\[
\sum_{\by} \pi(\by \mid \bx) = 1.
\]

Hence adding the Lagrangian multiplier, we get the following Lagrangian form
\[
    \cL(\pi(\cdot \mid \bx), \alpha) = W^\pp_r(\op \succ \bp \mid \bx) -\reg \KL(\op(\cdot \mid \bx) \| \bp(\cdot | \bx)) + \alpha \Paren{\sum_{\by} \pi(\by \mid \bx) - 1}.
\]

By method of Lagrange multipliers, we have that the solution to \cref{eqn:kl-constrained-ppwr} must be a stationary point of $\cL(\pi(\cdot \mid \bx), \alpha)$. And hence
\[
\mathbf{0} = \frac{\partial \cL(\pi(\cdot \mid \bx), \alpha)}{\partial   \pi(\by\mid\bx)} = \frac{\partial W^\pp_r(\op \succ \bp \mid \bx)}{\partial   \pi(\by\mid\bx)} - \beta\Paren{\log\frac{\op(\by \mid \bx)}{\bp(\by \mid \bx)} + 1} + \alpha.
\]

Setting 
\[
    \tr(\bx, \by) = \frac{\partial W^\pp_r(\op \succ \bp \mid \bx)}{\partial   \pi(\by\mid\bx)},
\]
we get 
\[
\log\frac{\op(\by \mid \bx)}{\bp(\by \mid \bx)}  = \frac{ \tr(\bx, \by) + \alpha }{\beta} - 1.
\]
And hence
\[
    \op(\by \mid \bx) \propto \bp(\by \mid \bx) \exp\Paren{\frac{\tr(\bx, \by)}{\beta}}.
\]

It remains to prove \cref{eqn:reward_expand}. 
Plugging in $\pi_1 = \pi$, $\pi_2 = \bp$ to \cref{lem:pp_win_rate_decompose}, and taking partial derivative with respect to $\pi(\by \mid \bx)$ on the right hand side completes the proof.
\end{proof}

\subsection{Proof of \cref{lem:monotone}}
If $r(\bx, \by) \ge r(\bx, \bz)$, we have $\forall \by'$,
\[
w_r(\by, \by' \mid \bx) \ge w_r(\bz, \by' \mid \bx).
\]

Hence 
\begin{align*}
    \qt_{r, \bp}(\bx, \by) =  \mathbb{E}_{\by' \sim \pi(\cdot \mid \bx)} w_r(\by, \by' \mid \bx) \ge  \mathbb{E}_{\by' \sim \pi(\cdot \mid \bx)} w_r(\bz, \by' \mid \bx) = \qt_{r, \bp}(\bx, \bz). 
\end{align*}


\subsection{Proof of \cref{lem:canonical}}
The proof follows from the fact that for all $\bx$ and $\by$
\begin{align}
      \car_{g(r), \bp} (\bx, \by) &= 
    \E_{\bz \sim \bp} \left\{ \mathbf{1}[g(r(\bx,\by)) > g(r(\bx,\bz))] + \frac{1}{2}  \mathbf{1}[g(r(\bx,\by)) = g(r(\bx,\bz))]\right\} \nonumber \\
    & =\E_{\bz \sim \bp} \left\{ \mathbf{1}[r(\bx,\by) > r(\bx,\bz)] + \frac{1}{2}  \mathbf{1}[r(\bx,\by) = r(\bx,\bz)]\right\} \label{eq:step-monotone}\\
      & = \car_{r, \bp} (\bx, \by), \nonumber
\end{align}
where~\eqref{eq:step-monotone} follows from monotone increasing property of $g.$

\subsection{Proof of \cref{lem:reward_uniform}}

To show that $\car_{r, \bp}(\bx, \by) \sim {\rm Unif}([0, 1]),$ it would be enough to show $\forall u \in [0, 1]$,
\[
    \text{Pr}_{\by \sim \bp}\Paren{\car_{r, \bp}(\bx, \by) \le u} = u. 
\]
\begin{align*}
     \text{Pr}_{\by \sim \bp}\Paren{\car_{r, \bp}(\bx, \by) \le u} & =  \text{Pr}_{\by \sim \bp}{r(\bx, \by) \le r(\bx, \by_{u, \bx})}  \\
     & = \car_{r, \bp}(\bx, \by_{u, \bx}) \\
     & = u,
\end{align*}
completing the proof.


\subsection{Proof of \cref{thm:calibrated_procedure}}

In this section, we will prove an extended version of \cref{thm:calibrated_procedure} below.

\begin{theorem}[Extended version of \cref{thm:calibrated_procedure}] \label{thm:calibrated_procedure_extended}
If $T$ is a calibrated inference-time procedure with mapping function $\calp$, for any continuous language model $\pi$, $\reg > 0$ and reward transformation function $\Phi$,  we have that 
\[W^T_r(\op^*_{\cR_{\Phi}, \reg} \succ \bp \mid \bx) = \frac{\int_{0}^1 \exp \Paren{\Phi(u)/\reg}g_T( F_{\Phi, \reg}(u) )\int_{0}^{u} g_T (u') \dd u' \dd u}{\int_{0}^1 \exp \Paren{\Phi(u)/\reg} g_T( F_{\Phi, \reg}(u) ) \dd u \int_{0}^1 \calp(u)\dd u} 
\] where $F_{\Phi, \reg}(u)= \frac{\int_{0}^u e^{\Phi(u')/\reg} \dd u'}{\int_{0}^1 e^{\Phi(u')/\reg} \dd u'}$, and \[\KL(\op^*_{\cR_{\Phi}, \reg} \| \bp) = 
\frac{1}{\reg}  \frac{ \int_{0}^1 \Phi(u) e^{\Phi(u)/\reg} \dd u }{\int_{0}^1 e^{\Phi(u)/\reg} \dd u} - \log \Paren{\int_{0}^1 e^{\Phi(u)/\reg} \dd u}.
\] 
And hence they are independent of $r$ and $\bp$.
\end{theorem}
\begin{proof}

In the proof, we consider the two language models $\op^*_{\cR_{\Phi}, \reg}$ and $\bp$ in the space of calibrated reward against the base policy $\bp$.  For any policy $\pi$, let $\qt_{r, \bp} \circ \pi(\cdot \mid \bx)$ be a distribution over $[0, 1]$ that outputs the calibrated reward $\qt_{r, \bp}(\bx, \by)$ of the sample $\by$ sampled from $\pi(\cdot \mid \bx)$. Then by \cref{lem:reward_uniform},
\[
    \qt_{r, \bp} \circ \pi(\cdot \mid \bx) \sim {\rm Unif}([0, 1]) .
\]

Similarly, using \cref{lem:kl_rl}, it can be shown that $\qt_{r, \bp} \circ \op^*_{\cR_{\Phi}, \reg}$ follows the distribution with density $\forall u \in [0, 1]$,
\[
    \qt_{r, \bp} \circ \op^*_{\cR_{\Phi}, \reg} (u) = \frac{ e^{\Phi(u)/\reg} }{\int_{0}^1 e^{\Phi(u')/\reg} \dd u'}.
\]

Note that since $r$ assigns distinct rewards to different $\by$'s and $\qt_{r, \bp}$ is a monotone transformation of $r$, we
have that

\begin{align*}
   \KL(\op^*_{\cR_{\Phi}, \reg} \| \bp) & =\KL( \qt_{r, \bp} \circ \op^*_{\cR_{\Phi}, \reg} \| \qt_{r, \bp} \circ \pi ) \\
   & = \int_{u = 0}^1 \frac{ e^{\Phi(u)/\reg} }{\int_{0}^1 e^{\Phi(u')/\reg} \dd u'} \log \frac{ e^{\Phi(u)/\reg} }{\int_{0}^1 e^{\Phi(u')/\reg} \dd u'} \dd u \\
   & = \frac{1}{\reg}  \frac{ \int_{u = 0}^1 \Phi(u) e^{\Phi(u)/\reg} \dd u }{\int_{0}^1 e^{\Phi(u)/\reg} \dd u} - \log \Paren{\int_{0}^1 e^{\Phi(u)/\reg} \dd u}.
\end{align*}

After the inference-time procedure $T$ is applied, we have that inference-time base policy satisfies
\[
    \qt_{r, \bp} \circ T( \bp) (u) = \frac{ g_T( u )}{\int_{0}^1 g_T( u' ) \dd u' }.
\]

For the inference-time aligned policy, we have that:
\[
    \text{Pr}_{\by \sim \op^*_{\cR_{\Phi}, \reg}(\cdot \mid \bx)}  \Paren{\qt_{r, \bp}(\bx, \by) \le u} = \frac{\int_{0}^u e^{\Phi(u')/\reg} \dd u'}{\int_{0}^1 e^{\Phi(u')/\reg} \dd u'},
\]
which is defined as $F_{\Phi, \reg}(u)$. And hence we have
\[
     \qt_{r, \bp} \circ T( \op^*_{\cR_{\Phi}, \reg}) (u) = \frac{ \exp \Paren{\Phi(u)/\reg}g_T( F_{\Phi, \reg}(u) )}{\int_{0}^1 \exp \Paren{\Phi(u)/\reg} g_T( F_{\Phi, \reg}(u) ) \dd u }.
\]
Thus, the inference-time win rate satisfies\footnote{Below we add an additional $\frac{1}{2}\idc{r(\bx, \by) = r(\bx, \bz)}$ to the win r.v for simplicity, which won't affect the result for continuous LMs.}
\begin{align*}
     W^T_r(\op^*_{\cR_{\Phi}, \reg} \succ \bp \mid \bx) 
    & = \E_{\by \sim \pp(\op^*_{\cR_{\Phi}, \reg})(\cdot \mid \bx), \bz \sim \pp(\bp)(\cdot \mid \bx)} \left\{\idc{r(\bx, \by) \ge r(\bx, \bz)} \right\}\\
    & = \E_{\by \sim \pp(\op^*_{\cR_{\Phi}, \reg})(\cdot \mid \bx), \bz \sim \pp(\bp)(\cdot \mid \bx)} \left\{\idc{\qt_{r, \bp}(\bx, \by) \ge \qt_{r, \bp}(\bx, \bz)} \right\}\\
    & = \text{Pr}_{u \sim  \qt_{r, \bp} \circ T( \op^*_{\cR_{\Phi}, \reg}), u' \sim  \qt_{r, \bp} \circ T( \bp)} \Paren{ u' \le u} \\
    & = \frac{\int_{0}^1 \exp \Paren{\Phi(u)/\reg}g_T( F_{\Phi, \reg}(u) )\int_{0}^{u} g_T (u') \dd u' \dd u}{\int_{0}^1 \exp \Paren{\Phi(u)/\reg}g_T( F_{\Phi, \reg}(u) ) \dd u \int_{0}^1 \calp(u)\dd u},
\end{align*}
completing the proof.
\end{proof}
%
%
%
%
%

%
%
%


\subsection{Proof of \cref{thm:wr_kl_bon_won}}
The KL divergence is the same as the KL divergence in \cref{thm:calibrated_procedure_extended}. The win rate can be obtained by plugging $g_{\bofn}(u) = u^{N-1}$ and $g_{\wofn}(u) = (1 - u)^{N-1}$ (as shown in \cref{lem:bofn_wofn}) into \cref{thm:calibrated_procedure_extended}.

%
\subsection{Proof of \cref{cor:coupled_won}}
We will show that \cref{cor:coupled_won} is a special case of \cref{lem:coupled} with a simple continuous language model. And by \cref{thm:calibrated_procedure}, we have the $\Phi$ can be generalized to arbitrary continuous language models.

Let $\bcY = [0,1]$. We assume the LMs and reward models are context-independent. We use $u \in [0, 1]$ to denote $\by$ and set the reward model to be $r(u) = u$. The base policy is a simple uniform distribution over $[0, 1]$, $\bp = \text{Unif}([0, 1])$. Let $F_{\pi}(u)$ be the CDF of $\pi$, then we have that the \bofn win rate is
\[
    W^{\bofn}_r(\pi \succ \bp \mid \bx) = 1 - N \int_{0}^1 F_{\pi}(u)^N u^{N-1} \dd u,
    \]
and \wofn win rate is
    \[
    W^{\wofn}_r(\pi \succ \bp \mid \bx) =  N \int_{0}^1 \Paren{1 - F_{\pi}(u) }^N (1-u)^{N-1} \dd u.
    \]
Plugging these into \cref{lem:coupled}, we have for \bofn, 
\begin{align*}
    \tr(u) & = \frac{\partial W^{\bofn}_r(\pi \succ \bp \mid \bx)}{\partial \pi(u)} \\
    & =  - N \int_{0}^1  v^{N-1} \frac{\partial F_{\pi}(v)^N}{\partial \pi(u)}\dd v \\
     & = - N \int_{0}^1  v^{N-1} F_{\pi}(v)^{N-1}\frac{\partial F_{\pi}(v)}{\partial \pi(u)}\dd v \\
    & = -N^2 \int_0^1 F_{\pi}(v)^{N-1} \idc{v \ge u} v^{N-1} \dd v \\
    & = -N^2 \int_u^1 F_{\pi}(v)^{N-1} v^{N-1} \dd v.
\end{align*}

For \wofn, we have
\begin{align*}
    \tr(u) & = \frac{\partial W^{\wofn}_r(\pi \succ \bp \mid \bx)}{\partial \pi(u)} \\
    & =  N \int_{0}^1  (1 - v)^{N-1} \frac{\partial \Paren{1 - F_{\pi}(v)}^N}{\partial \pi(u)}\dd v \\
     & = - N^2 \int_{0}^1  (1-v)^{N-1} (1 - F_{\pi}(v))^{N-1}\frac{\partial F_{\pi}(v)}{\partial \pi(u)}\dd v \\
    & = -N^2 \int_0^1 (1-v)^{N-1} (1 - F_{\pi}(v))^{N-1} \idc{v \ge u}\dd v \\
    & = -N^2 \int_u^1 (1-v)^{N-1} (1 - F_{\pi}(v))^{N-1} \dd v.
\end{align*}