\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amini et~al.(2024)Amini, Vieira, and Cotterell]{amini2024variationalbestofnalignment}
Afra Amini, Tim Vieira, and Ryan Cotterell.
\newblock Variational best-of-n alignment, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.06057}.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and Man{\'e}]{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man{\'e}.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023PaLM}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Azar et~al.(2023)Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos]{azar2023general}
Mohammad~Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R{\'e}mi Munos.
\newblock A general theoretical paradigm to understand learning from human preferences.
\newblock \emph{arXiv preprint arXiv:2310.12036}, 2023.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022.

\bibitem[Beirami et~al.(2024)Beirami, Agarwal, Berant, D'Amour, Eisenstein, Nagpal, and Suresh]{beirami2024theoretical}
Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, and Ananda~Theertha Suresh.
\newblock Theoretical guarantees on the best-of-n alignment policy.
\newblock \emph{arXiv preprint arXiv:2401.01879}, 2024.

\bibitem[Brown et~al.(2024)Brown, Juravsky, Ehrlich, Clark, Le, R{\'e}, and Mirhoseini]{brown2024large}
Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc~V Le, Christopher R{\'e}, and Azalia Mirhoseini.
\newblock Large language monkeys: Scaling inference compute with repeated sampling.
\newblock \emph{arXiv preprint arXiv:2407.21787}, 2024.

\bibitem[Chaffin et~al.(2022)Chaffin, Claveau, and Kijak]{chaffin-etal-2022-ppl}
Antoine Chaffin, Vincent Claveau, and Ewa Kijak.
\newblock {PPL-MCTS}: {C}onstrained textual generation through discriminator-guided {MCTS} decoding.
\newblock In Marine Carpuat, Marie-Catherine de~Marneffe, and Ivan~Vladimir Meza~Ruiz (eds.), \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  2953--2967, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.215}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.215}.

\bibitem[Chakraborty et~al.(2024)Chakraborty, Ghosal, Yin, Manocha, Wang, Bedi, and Huang]{chakraborty2024transfer}
Souradip Chakraborty, Soumya~Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit~Singh Bedi, and Furong Huang.
\newblock Transfer q star: Principled decoding for llm alignment.
\newblock \emph{arXiv preprint arXiv:2405.20495}, 2024.

\bibitem[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{pair}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock \emph{arXiv preprint arXiv:2310.08419}, 2023.

\bibitem[Charniak \& Johnson(2005)Charniak and Johnson]{charniak-johnson-2005-coarse}
Eugene Charniak and Mark Johnson.
\newblock Coarse-to-fine n-best parsing and {M}ax{E}nt discriminative reranking.
\newblock In Kevin Knight, Hwee~Tou Ng, and Kemal Oflazer (eds.), \emph{Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)}, pp.\  173--180, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.
\newblock \doi{10.3115/1219840.1219862}.
\newblock URL \url{https://aclanthology.org/P05-1022}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Collins \& Koo(2005)Collins and Koo]{collins-koo-2005-discriminative}
Michael Collins and Terry Koo.
\newblock Discriminative reranking for natural language parsing.
\newblock \emph{Computational Linguistics}, 31\penalty0 (1):\penalty0 25--70, 2005.
\newblock \doi{10.1162/0891201053630273}.
\newblock URL \url{https://aclanthology.org/J05-1003}.

\bibitem[Coste et~al.(2023)Coste, Anwar, Kirk, and Krueger]{coste2023reward}
Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger.
\newblock Reward model ensembles help mitigate overoptimization.
\newblock \emph{arXiv preprint arXiv:2310.02743}, 2023.

\bibitem[Eisenstein et~al.(2024)Eisenstein, Nagpal, Agarwal, Beirami, D'Amour, Dvijotham, Fisch, Heller, Pfohl, Ramachandran, Shaw, and Berant]{eisenstein2023helping}
Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ~Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, and Jonathan Berant.
\newblock Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking, 2024.
\newblock URL \url{https://arxiv.org/abs/2312.09244}.

\bibitem[Fisch et~al.(2024)Fisch, Eisenstein, Zayats, Agarwal, Beirami, Nagpal, Shaw, and Berant]{fisch2024robust}
Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, and Jonathan Berant.
\newblock Robust preference optimization through reward model distillation.
\newblock \emph{arXiv preprint arXiv:2405.19316}, 2024.

\bibitem[Gao et~al.(2023)Gao, Schulman, and Hilton]{gao2023scaling}
Leo Gao, John Schulman, and Jacob Hilton.
\newblock Scaling laws for reward model overoptimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10835--10866. PMLR, 2023.

\bibitem[Gui et~al.(2024)Gui, Gârbacea, and Veitch]{gui2024bonbonalignmentlargelanguage}
Lin Gui, Cristina Gârbacea, and Victor Veitch.
\newblock Bonbon alignment for large language models and the sweetness of best-of-n sampling, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.00832}.

\bibitem[Gur et~al.(2024)Gur, Furuta, Huang, Safdari, Matsuo, Eck, and Faust]{gurreal2024}
Izzeddin Gur, Hiroki Furuta, Austin~V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust.
\newblock A real-world webagent with planning, long context understanding, and program synthesis.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Hilton \& Gao(2022)Hilton and Gao]{hilton2022measuring}
Jacob Hilton and Leo Gao.
\newblock Measuring {Goodhart’s} law, April 2022.
\newblock URL \url{https://openai.com/research/measuring-goodharts-law}.
\newblock Accessed: 2024-01-03.

\bibitem[Hughes et~al.(2024{\natexlab{a}})Hughes, Price, Lynch, Schaeffer, Barez, Koyejo, Sleight, Jones, Perez, and Sharma]{hughes2024best}
John Hughes, Sara Price, Aengus Lynch, Rylan Schaeffer, Fazl Barez, Sanmi Koyejo, Henry Sleight, Erik Jones, Ethan Perez, and Mrinank Sharma.
\newblock Best-of-n jailbreaking.
\newblock \emph{arXiv preprint arXiv:2412.03556}, 2024{\natexlab{a}}.

\bibitem[Hughes et~al.(2024{\natexlab{b}})Hughes, Price, Lynch, Schaeffer, Barez, Koyejo, Sleight, Jones, Perez, and Sharma]{hughes2024bestofnjailbreaking}
John Hughes, Sara Price, Aengus Lynch, Rylan Schaeffer, Fazl Barez, Sanmi Koyejo, Henry Sleight, Erik Jones, Ethan Perez, and Mrinank Sharma.
\newblock Best-of-n jailbreaking, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2412.03556}.

\bibitem[Korbak et~al.(2022)Korbak, Perez, and Buckley]{korbak2022rl}
Tomasz Korbak, Ethan Perez, and Christopher~L Buckley.
\newblock {RL with KL penalties is better viewed as Bayesian inference}.
\newblock \emph{arXiv preprint arXiv:2205.11275}, 2022.

\bibitem[Li et~al.(2021)Li, Beirami, Sanjabi, and Smith]{li2021tilted}
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith.
\newblock Tilted empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Li et~al.(2023)Li, Beirami, Sanjabi, and Smith]{li2023tilted}
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith.
\newblock On tilted losses in machine learning: Theory and applications.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (142):\penalty0 1--79, 2023.

\bibitem[Madaan et~al.(2024)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2024self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Moskovitz et~al.(2023)Moskovitz, Singh, Strouse, Sandholm, Salakhutdinov, Dragan, and McAleer]{moskovitz2023confronting}
Ted Moskovitz, Aaditya~K Singh, DJ~Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca~D Dragan, and Stephen McAleer.
\newblock Confronting reward model overoptimization with constrained rlhf.
\newblock \emph{arXiv preprint arXiv:2310.04373}, 2023.

\bibitem[Mudgal et~al.(2024)Mudgal, Lee, Ganapathy, Li, Wang, Huang, Chen, Cheng, Collins, Strohman, Chen, Beutel, and Beirami]{mudgal2023controlled}
Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, and Ahmad Beirami.
\newblock Controlled decoding from language models.
\newblock \emph{International Conference on Machine Learning}, 2024.

\bibitem[Nakano et~al.(2022)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse, Jain, Kosaraju, Saunders, Jiang, Cobbe, Eloundou, Krueger, Button, Knight, Chess, and Schulman]{nakano2022webgptbrowserassistedquestionansweringhuman}
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu~Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.
\newblock Webgpt: Browser-assisted question-answering with human feedback, 2022.
\newblock URL \url{https://arxiv.org/abs/2112.09332}.

\bibitem[OpenAI(2024)]{o1}
OpenAI.
\newblock Learning to reason with llms.
\newblock 2024.
\newblock URL \url{https://openai.com/index/ learning-to-reason-with-llms/}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pan et~al.(2022)Pan, Bhatia, and Steinhardt]{pan2022effects}
Alexander Pan, Kush Bhatia, and Jacob Steinhardt.
\newblock The effects of reward misspecification: Mapping and mitigating misaligned models.
\newblock \emph{arXiv preprint arXiv:2201.03544}, 2022.

\bibitem[Pang et~al.(2022)Pang, Padmakumar, Sellam, Parikh, and He]{pang2022reward}
Richard~Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur~P Parikh, and He~He.
\newblock Reward gaming in conditional text generation.
\newblock \emph{arXiv preprint arXiv:2211.08714}, 2022.

\bibitem[Pauls \& Klein(2009)Pauls and Klein]{pauls-klein-2009-k}
Adam Pauls and Dan Klein.
\newblock K-best {A}* parsing.
\newblock In Keh-Yih Su, Jian Su, Janyce Wiebe, and Haizhou Li (eds.), \emph{Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}}, pp.\  958--966, Suntec, Singapore, August 2009. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/P09-1108}.

\bibitem[Qiu et~al.(2024)Qiu, Lu, Zeng, Guo, Geng, Wang, Huang, Wu, and Wang]{qiu2024treebon}
Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang.
\newblock Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling.
\newblock \emph{arXiv preprint arXiv:2410.16033}, 2024.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rosenblatt(1952)]{Rosenblatt1952}
Murray Rosenblatt.
\newblock Remarks on a multivariate transformation.
\newblock \emph{The Annals of Mathematical Statistics}, 23\penalty0 (3):\penalty0 470--472, 1952.
\newblock ISSN 00034851.
\newblock URL \url{http://www.jstor.org/stable/2236692}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Scialom et~al.(2021)Scialom, Dray, Staiano, Lamprier, and Piwowarski]{scialom2021beam}
Thomas Scialom, Paul-Alexis Dray, Jacopo Staiano, Sylvain Lamprier, and Benjamin Piwowarski.
\newblock To beam or not to beam: That is a question of cooperation for language gans.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 26585--26597, 2021.

\bibitem[Sessa et~al.(2024)Sessa, Dadashi, Hussenot, Ferret, Vieillard, Ramé, Shariari, Perrin, Friesen, Cideron, Girgin, Stanczyk, Michi, Sinopalnikov, Ramos, Héliou, Severyn, Hoffman, Momchev, and Bachem]{sessa2024bondaligningllmsbestofn}
Pier~Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos, Amélie Héliou, Aliaksei Severyn, Matt Hoffman, Nikola Momchev, and Olivier Bachem.
\newblock Bond: Aligning llms with best-of-n distillation, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.14622}.

\bibitem[Skalse et~al.(2022)Skalse, Howe, Krasheninnikov, and Krueger]{skalse2022defining}
Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger.
\newblock Defining and characterizing reward gaming.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 9460--9471, 2022.

\bibitem[Snell et~al.(2024)Snell, Lee, Xu, and Kumar]{snell2024scaling}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
\newblock Scaling {LLM} test-time compute optimally can be more effective than scaling model parameters.
\newblock \emph{arXiv preprint arXiv:2408.03314}, 2024.

\bibitem[Souly et~al.(2024)Souly, Lu, Bowen, Trinh, Hsieh, Pandey, Abbeel, Svegliato, Emmons, Watkins, et~al.]{souly2024strongreject}
Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu~Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, et~al.
\newblock A strongreject for empty jailbreaks.
\newblock \emph{arXiv preprint arXiv:2402.10260}, 2024.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3008--3021, 2020.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{self-consistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2024)Wang, Nagpal, Berant, Eisenstein, D'Amour, Koyejo, and Veitch]{wang2024transforming}
Zihao Wang, Chirag Nagpal, Jonathan Berant, Jacob Eisenstein, Alex D'Amour, Sanmi Koyejo, and Victor Veitch.
\newblock Transforming and combining rewards for aligning large language models.
\newblock \emph{International Conference on Machine Learning}, 2024.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Welleck et~al.(2024)Welleck, Bertsch, Finlayson, Schoelkopf, Xie, Neubig, Kulikov, and Harchaoui]{welleck2024decoding}
Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui.
\newblock From decoding to meta-generation: Inference-time algorithms for large language models.
\newblock \emph{arXiv preprint arXiv:2406.16838}, 2024.

\bibitem[Wu et~al.(2024{\natexlab{a}})Wu, Sun, Li, Welleck, and Yang]{wu2024empirical}
Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang.
\newblock An empirical analysis of compute-optimal inference for problem-solving with language models.
\newblock \emph{arXiv preprint arXiv:2408.00724}, 2024{\natexlab{a}}.

\bibitem[Wu et~al.(2024{\natexlab{b}})Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu, Smith, Ostendorf, and Hajishirzi]{wu2024fine}
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah~A Smith, Mari Ostendorf, and Hannaneh Hajishirzi.
\newblock Fine-grained human feedback gives better rewards for language model training.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Xie et~al.(2024)Xie, Guo, Yu, and Li]{xie2024calibrating}
Zhihui Xie, Jizhou Guo, Tong Yu, and Shuai Li.
\newblock Calibrating reasoning in language models with internal consistency.
\newblock \emph{arXiv preprint arXiv:2405.18711}, 2024.

\bibitem[Yang et~al.(2024)Yang, Salamatian, Sun, Suresh, and Beirami]{yang2024asymptotics}
Joy~Qiping Yang, Salman Salamatian, Ziteng Sun, Ananda~Theertha Suresh, and Ahmad Beirami.
\newblock Asymptotics of language model alignment.
\newblock \emph{arXiv preprint arXiv:2404.01730}, 2024.

\bibitem[Yao et~al.(2024)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2024tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yohsua et~al.(2024)Yohsua, Daniel, Tamay, Rishi, Stephen, Yejin, Danielle, Hoda, Leila, Shayne, et~al.]{yohsua2024international}
Bengio Yohsua, Privitera Daniel, Besiroglu Tamay, Bommasani Rishi, Casper Stephen, Choi Yejin, Goldfarb Danielle, Heidari Hoda, Khalatbari Leila, Longpre Shayne, et~al.
\newblock \emph{International Scientific Report on the Safety of Advanced AI}.
\newblock PhD thesis, Department for Science, Innovation and Technology, 2024.

\bibitem[Zhao et~al.(2024)Zhao, Brekelmans, Makhzani, and Grosse]{zhaoprobabilistic2024}
Stephen Zhao, Rob Brekelmans, Alireza Makhzani, and Roger~Baker Grosse.
\newblock Probabilistic inference in language models via twisted sequential monte carlo.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Zhao et~al.(2022)Zhao, Khalman, Joshi, Narayan, Saleh, and Liu]{zhao2022calibrating}
Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter~J Liu.
\newblock Calibrating sequence likelihood improves conditional language generation.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\end{thebibliography}
