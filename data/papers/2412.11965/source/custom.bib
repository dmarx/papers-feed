@inproceedings{andrew2007scalable,
 author = {Andrew, Galen and Gao, Jianfeng},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 pages = {33--40},
 title = {Scalable training of {L1}-regularized log-linear models},
 year = {2007}
}

@book{Gusfield:97,
 address = {Cambridge, UK},
 author = {Dan Gusfield},
 publisher = {Cambridge University Press},
 title = {Algorithms on Strings, Trees and Sequences},
 year = {1997}
}

@article{rasooli-tetrault-2015,
 author = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
 journal = {ArXiv preprint},
 title = {Yara Parser: {A} Fast and Accurate Dependency Parser},
 url = {https://arxiv.org/abs/1503.06733},
 volume = {abs/1503.06733},
 year = {2015}
}

@article{Ando2005,
 acmid = {1194905},
 author = {Ando, Rie Kubota and Zhang, Tong},
 issn = {1532-4435},
 issue_date = {12/1/2005},
 journal = {Journal of Machine Learning Research},
 numpages = {37},
 pages = {1817--1853},
 publisher = {JMLR.org},
 title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
 volume = {6},
 year = {2005}
}

@incollection{Bengio+chapter2007,
 author = {Bengio, Yoshua and LeCun, Yann},
 booktitle = {Large Scale Kernel Machines},
 publisher = {MIT Press},
 title = {Scaling Learning Algorithms Towards {AI}},
 year = {2007}
}

@article{Hinton06,
 author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
 journal = {Neural Computation},
 pages = {1527--1554},
 title = {A Fast Learning Algorithm for Deep Belief Nets},
 volume = {18},
 year = {2006}
}

@inproceedings{goodfellow2016deep,
 author = {Ruslan Salakhutdinov},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/kdd/Salakhutdinov14.bib},
 booktitle = {The 20th {ACM} {SIGKDD} International Conference on Knowledge Discovery
and Data Mining, {KDD} '14, New York, NY, {USA} - August 24 - 27,
2014},
 doi = {10.1145/2623330.2630809},
 editor = {Sofus A. Macskassy and
Claudia Perlich and
Jure Leskovec and
Wei Wang and
Rayid Ghani},
 pages = {1973},
 publisher = {{ACM}},
 timestamp = {Tue, 06 Nov 2018 00:00:00 +0100},
 title = {Deep learning},
 url = {https://doi.org/10.1145/2623330.2630809},
 year = {2014}
}

@article{elhage2021mathematical,
 author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
 journal = {Transformer Circuits Thread},
 number = {1},
 pages = {12},
 title = {A mathematical framework for transformer circuits},
 volume = {1},
 year = {2021}
}

@inproceedings{Vaswani2017AttentionIA,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{Dar2022AnalyzingTI,
 address = {Toronto, Canada},
 author = {Dar, Guy  and
Geva, Mor  and
Gupta, Ankit  and
Berant, Jonathan},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.893},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 pages = {16124--16170},
 publisher = {Association for Computational Linguistics},
 title = {Analyzing Transformers in Embedding Space},
 url = {https://aclanthology.org/2023.acl-long.893},
 year = {2023}
}

@misc{logitlens,
 author = {nostalgebraist},
 title = {Interpreting GPT: the Logit Lens},
 url = {https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
 year = {2020}
}

@inproceedings{wang2022interpretability,
 author = {Kevin Ro Wang and
Alexandre Variengien and
Arthur Conmy and
Buck Shlegeris and
Jacob Steinhardt},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/WangVCSS23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Interpretability in the Wild: a Circuit for Indirect Object Identification
in {GPT-2} Small},
 url = {https://openreview.net/pdf?id=NpsVSN6o4ul},
 year = {2023}
}

@inproceedings{
gould2024successor,
title={Successor Heads: Recurring, Interpretable Attention Heads In The Wild},
author={Rhys Gould and Euan Ong and George Ogden and Arthur Conmy},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=kvcbV8KQsi}
}

@inproceedings{geva-etal-2022-transformer,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Geva, Mor  and
Caciularu, Avi  and
Wang, Kevin  and
Goldberg, Yoav},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2022.emnlp-main.3},
 editor = {Goldberg, Yoav  and
Kozareva, Zornitsa  and
Zhang, Yue},
 pages = {30--45},
 publisher = {Association for Computational Linguistics},
 title = {Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},
 url = {https://aclanthology.org/2022.emnlp-main.3},
 year = {2022}
}

@article{10.1145/2629489,
 abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
 address = {New York, NY, USA},
 author = {Vrande\v{c}i\'{c}, Denny and Kr\"{o}tzsch, Markus},
 doi = {10.1145/2629489},
 issn = {0001-0782},
 issue_date = {October 2014},
 journal = {Commun. ACM},
 number = {10},
 numpages = {8},
 pages = {78–85},
 publisher = {Association for Computing Machinery},
 title = {Wikidata: a free collaborative knowledgebase},
 url = {https://doi.org/10.1145/2629489},
 volume = {57},
 year = {2014}
}

@inproceedings{
hernandez2024linearity,
title={Linearity of Relation Decoding in Transformer Language Models},
author={Evan Hernandez and Arnab Sen Sharma and Tal Haklay and Kevin Meng and Martin Wattenberg and Jacob Andreas and Yonatan Belinkov and David Bau},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=w7LU2s14kE}
}

@inproceedings{meng2022locating,
 author = {Kevin Meng and
David Bau and
Alex Andonian and
Yonatan Belinkov},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/MengBAB22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Locating and Editing Factual Associations in {GPT}},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{bird-loper-2004-nltk,
 address = {Philadelphia, Pennsylvania, USA},
 author = {Loper, Edward  and
Bird, Steven},
 booktitle = {Proceedings of the {ACL}-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics},
 doi = {10.3115/1118108.1118117},
 pages = {63--70},
 publisher = {Association for Computational Linguistics},
 title = {{NLTK}: The Natural Language Toolkit},
 url = {https://aclanthology.org/W02-0109},
 year = {2002}
}

@inproceedings{geva-etal-2021-transformer,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Geva, Mor  and
Schuster, Roei  and
Berant, Jonathan  and
Levy, Omer},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.446},
 editor = {Moens, Marie-Francine  and
Huang, Xuanjing  and
Specia, Lucia  and
Yih, Scott Wen-tau},
 pages = {5484--5495},
 publisher = {Association for Computational Linguistics},
 title = {Transformer Feed-Forward Layers Are Key-Value Memories},
 url = {https://aclanthology.org/2021.emnlp-main.446},
 year = {2021}
}

@inproceedings{katz-etal-2024-backward,
    title = "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space",
    author = "Katz, Shahar  and
      Belinkov, Yonatan  and
      Geva, Mor  and
      Wolf, Lior",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.142",
    doi = "10.18653/v1/2024.emnlp-main.142",
    pages = "2390--2422",
    abstract = "Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models{'} vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs{'} backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes{'} inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs{'} neurons.",
}

@inproceedings{geva2023dissecting,
 address = {Singapore},
 author = {Geva, Mor  and
Bastings, Jasmijn  and
Filippova, Katja  and
Globerson, Amir},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.751},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {12216--12235},
 publisher = {Association for Computational Linguistics},
 title = {Dissecting Recall of Factual Associations in Auto-Regressive Language Models},
 url = {https://aclanthology.org/2023.emnlp-main.751},
 year = {2023}
}

@inproceedings{mcdougall-etal-2024-copy,
    title = "Copy Suppression: Comprehensively Understanding a Motif in Language Model Attention Heads",
    author = "McDougall, Callum Stuart  and
      Conmy, Arthur  and
      Rushing, Cody  and
      McGrath, Thomas  and
      Nanda, Neel",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.22",
    doi = "10.18653/v1/2024.blackboxnlp-1.22",
    pages = "337--363",
    abstract = "We present the copy suppression motif: an algorithm implemented by attention heads in large language models that reduces loss.If i) language model components in earlier layers predict a certain token, ii) this token appears earlier in the context and iii) later attention heads in the model suppress prediction of the token, then this is copy suppression. To show the importance of copy suppression, we focus on reverse-engineering attention head 10.7 (L10H7) in GPT-2 Small. This head suppresses naive copying behavior which improves overall model calibration, which explains why multiple prior works studying certain narrow tasks found negative heads that systematically favored the wrong answer. We uncover the mechanism that the negative heads use for copy suppression with weights-based evidence and are able to explain 76.9{\%} of the impact of L10H7 in GPT-2 Small, by this motif alone.To the best of our knowledge, this is the most comprehensive description of the complete role of a component in a language model to date. One major effect of copy suppression is its role in self-repair. Self-repair refers to how ablating crucial model components results in downstream neural network parts compensating for this ablation. Copy suppression leads to self-repair: if an initial overconfident copier is ablated, then there is nothing to suppress. We show that self-repair is implemented by several mechanisms, one of which is copy suppression, which explains 39{\%} of the behavior in a narrow task. Interactive visualizations of the copy suppression phenomena may be seen at our web app https://copy-suppression.streamlit.app/.",
}

@article{zheng2024attention,
 author = {Zheng, Zifan and Wang, Yezhaohui and Huang, Yuxin and Song, Shichao and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
 journal = {ArXiv preprint},
 title = {Attention Heads of Large Language Models: A Survey},
 url = {https://arxiv.org/abs/2409.03752},
 volume = {abs/2409.03752},
 year = {2024}
}

@article{olsson2022context,
 author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
 journal = {ArXiv preprint},
 title = {In-context learning and induction heads},
 url = {https://arxiv.org/abs/2209.11895},
 volume = {abs/2209.11895},
 year = {2022}
}

@article{ferrando2024primer,
 author = {Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R},
 journal = {ArXiv preprint},
 title = {A primer on the inner workings of transformer-based language models},
 url = {https://arxiv.org/abs/2405.00208},
 volume = {abs/2405.00208},
 year = {2024}
}

@article{kim2024mechanistic,
 author = {Kim, Geonhee and Valentino, Marco and Freitas, Andr{\'e}},
 journal = {ArXiv preprint},
 title = {A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models},
 url = {https://arxiv.org/abs/2408.08590},
 volume = {abs/2408.08590},
 year = {2024}
}

@inproceedings{garcia2024does,
 author = {Jorge Garc{\'{\i}}a{-}Carrasco and
Alejandro Mat{\'{e}} and
Juan C. Trujillo},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aistats/Garcia-Carrasco24.bib},
 booktitle = {International Conference on Artificial Intelligence and Statistics,
2-4 May 2024, Palau de Congressos, Valencia, Spain},
 editor = {Sanjoy Dasgupta and
Stephan Mandt and
Yingzhen Li},
 pages = {3322--3330},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 13 May 2024 01:00:00 +0200},
 title = {How does {GPT-2} Predict Acronyms? Extracting and Understanding a
Circuit via Mechanistic Interpretability},
 url = {https://proceedings.mlr.press/v238/garcia-carrasco24a.html},
 volume = {238},
 year = {2024}
}

@misc{svd-interpretable,
 author = {Beren Millidge and Sid Black},
 title = {The Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretable},
 url = {https://www.alignmentforum.org/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight},
 year = {2022}
}

@inproceedings{
merullo2024circuit,
title={Circuit Component Reuse Across Tasks in Transformer Language Models},
author={Jack Merullo and Carsten Eickhoff and Ellie Pavlick},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=fpoAYV6Wsk}
}

@misc{nanda2022transformerlens,
 author = {Neel Nanda and Joseph Bloom},
 howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
 title = {TransformerLens},
 year = {2022}
}

@inproceedings{wolf2019transformers,
 address = {Online},
 author = {Wolf, Thomas  and
Debut, Lysandre  and
Sanh, Victor  and
Chaumond, Julien  and
Delangue, Clement  and
Moi, Anthony  and
Cistac, Pierric  and
Rault, Tim  and
Louf, Remi  and
Funtowicz, Morgan  and
Davison, Joe  and
Shleifer, Sam  and
von Platen, Patrick  and
Ma, Clara  and
Jernite, Yacine  and
Plu, Julien  and
Xu, Canwen  and
Le Scao, Teven  and
Gugger, Sylvain  and
Drame, Mariama  and
Lhoest, Quentin  and
Rush, Alexander},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
 doi = {10.18653/v1/2020.emnlp-demos.6},
 editor = {Liu, Qun  and
Schlangen, David},
 pages = {38--45},
 publisher = {Association for Computational Linguistics},
 title = {Transformers: State-of-the-Art Natural Language Processing},
 url = {https://aclanthology.org/2020.emnlp-demos.6},
 year = {2020}
}

@inproceedings{
merullo2024talking,
title={Talking Heads: Understanding Inter-Layer Communication in Transformer Language Models},
author={Jack Merullo and Carsten Eickhoff and Ellie Pavlick},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=LUsx0chTsL}
}

@article{zhou2024role,
 author = {Zhou, Zhenhong and Yu, Haiyang and Zhang, Xinghua and Xu, Rongwu and Huang, Fei and Wang, Kun and Liu, Yang and Fang, Junfeng and Li, Yongbin},
 journal = {ArXiv preprint},
 title = {On the Role of Attention Heads in Large Language Model Safety},
 url = {https://arxiv.org/abs/2410.13708},
 volume = {abs/2410.13708},
 year = {2024}
}

@article{bolukbasi2021interpretability,
 author = {Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
 journal = {ArXiv preprint},
 title = {An interpretability illusion for bert},
 url = {https://arxiv.org/abs/2104.07143},
 volume = {abs/2104.07143},
 year = {2021}
}

@article{gao2024scaling,
 author = {Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
 journal = {ArXiv preprint},
 title = {Scaling and evaluating sparse autoencoders},
 url = {https://arxiv.org/abs/2406.04093},
 volume = {abs/2406.04093},
 year = {2024}
}

@inproceedings{
kissane2024interpreting,
title={Interpreting Attention Layer Outputs with Sparse Autoencoders},
author={Connor Kissane and Robert Krzyzanowski and Joseph Isaac Bloom and Arthur Conmy and Neel Nanda},
booktitle={ICML 2024 Workshop on Mechanistic Interpretability},
year={2024},
url={https://openreview.net/forum?id=fewUBDwjji}
}

@article{dubey2024llama,
 author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
 journal = {ArXiv preprint},
 title = {The llama 3 herd of models},
 url = {https://arxiv.org/abs/2407.21783},
 volume = {abs/2407.21783},
 year = {2024}
}

@inproceedings{biderman2023pythia,
 author = {Stella Biderman and
Hailey Schoelkopf and
Quentin Gregory Anthony and
Herbie Bradley and
Kyle O'Brien and
Eric Hallahan and
Mohammad Aflah Khan and
Shivanshu Purohit and
USVSN Sai Prashanth and
Edward Raff and
Aviya Skowron and
Lintang Sutawika and
Oskar van der Wal},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/BidermanSABOHKP23.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
2023, Honolulu, Hawaii, {USA}},
 editor = {Andreas Krause and
Emma Brunskill and
Kyunghyun Cho and
Barbara Engelhardt and
Sivan Sabato and
Jonathan Scarlett},
 pages = {2397--2430},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 28 Aug 2023 01:00:00 +0200},
 title = {Pythia: {A} Suite for Analyzing Large Language Models Across Training
and Scaling},
 url = {https://proceedings.mlr.press/v202/biderman23a.html},
 volume = {202},
 year = {2023}
}

@article{radford2019language,
 author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
 journal = {OpenAI blog},
 number = {8},
 pages = {9},
 title = {Language models are unsupervised multitask learners},
 volume = {1},
 year = {2019}
}

@misc{phi2,
 author = { Mojan Javaheripi and Sébastien Bubeck},
 title = {Phi-2: The surprising power of small language models},
 url = {https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/},
 year = {2023}
}

@article{hurst2024gpt,
 author = {Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
 journal = {ArXiv preprint},
 title = {Gpt-4o system card},
 url = {https://arxiv.org/abs/2410.21276},
 volume = {abs/2410.21276},
 year = {2024}
}

@inproceedings{voita2019analyzing,
 address = {Florence, Italy},
 author = {Voita, Elena  and
Talbot, David  and
Moiseev, Fedor  and
Sennrich, Rico  and
Titov, Ivan},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1580},
 editor = {Korhonen, Anna  and
Traum, David  and
M{\`a}rquez, Llu{\'\i}s},
 pages = {5797--5808},
 publisher = {Association for Computational Linguistics},
 title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
 url = {https://aclanthology.org/P19-1580},
 year = {2019}
}

@inproceedings{xiao2024efficient,
 author = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {Efficient Streaming Language Models with Attention Sinks},
 url = {https://openreview.net/forum?id=NG7sS51zVF},
 year = {2024}
}

@article{Schober2018CorrelationCA,
 author = {Patrick Schober and Christa Boer and Lothar A. Schwarte},
 journal = {Anesthesia \& Analgesia},
 pages = {1763–1768},
 title = {Correlation Coefficients: Appropriate Use and Interpretation},
 url = {https://api.semanticscholar.org/CorpusID:13354506},
 volume = {126},
 year = {2018}
}

@inproceedings{Ainslie2023GQATG,
 address = {Singapore},
 author = {Ainslie, Joshua  and
Lee-Thorp, James  and
de Jong, Michiel  and
Zemlyanskiy, Yury  and
Lebron, Federico  and
Sanghai, Sumit},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.298},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {4895--4901},
 publisher = {Association for Computational Linguistics},
 title = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
 url = {https://aclanthology.org/2023.emnlp-main.298},
 year = {2023}
}


@InProceedings{pmlr-v44-li15convergent,
  title = 	 {Convergent Learning: Do different neural networks learn the same representations?},
  author = 	 {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
  booktitle = 	 {Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015},
  pages = 	 {196--212},
  year = 	 {2015},
  editor = 	 {Storcheus, Dmitry and Rostamizadeh, Afshin and Kumar, Sanjiv},
  volume = 	 {44},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Montreal, Canada},
  month = 	 {11 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v44/li15convergent.pdf},
  url = 	 {https://proceedings.mlr.press/v44/li15convergent.html}
}

@article{arditi2024refusal,
  title={Refusal in language models is mediated by a single direction},
  author={Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11717},
  year={2024}
}

@inproceedings{
ghandeharioun2024patchscopes,
title={Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models},
author={Asma Ghandeharioun and Avi Caciularu and Adam Pearce and Lucas Dixon and Mor Geva},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=5uwBzcn885}
}

@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Belinkov, Yonatan  and
      Hupkes, Dieuwke",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286"
}

@inproceedings{vig-belinkov-2019-analyzing,
    title = "Analyzing the Structure of Attention in a Transformer Language Model",
    author = "Vig, Jesse  and
      Belinkov, Yonatan",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Belinkov, Yonatan  and
      Hupkes, Dieuwke",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4808",
    doi = "10.18653/v1/W19-4808",
    pages = "63--76"
}

@inproceedings{yom-din-etal-2024-jump,
    title = "Jump to Conclusions: Short-Cutting Transformers with Linear Transformations",
    author = "Yom Din, Alexander  and
      Karidi, Taelin  and
      Choshen, Leshem  and
      Geva, Mor",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.840",
    pages = "9615--9625"
}

@inproceedings{
tigges2024llm,
title={{LLM} Circuit Analyses Are Consistent Across Training and Scale},
author={Curt Tigges and Michael Hanna and Qinan Yu and Stella Biderman},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=3Ds5vNudIE}
}

@inproceedings{ethayarajh-2019-contextual,
    title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
    author = "Ethayarajh, Kawin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1006",
    doi = "10.18653/v1/D19-1006",
    pages = "55--65",
    abstract = "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5{\%} of the variance in a word{'}s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.",
}

@inproceedings{godey-etal-2024-anisotropy,
    title = "Anisotropy Is Inherent to Self-Attention in Transformers",
    author = "Godey, Nathan  and
      Clergerie, {\'E}ric  and
      Sagot, Beno{\^\i}t",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.3",
    pages = "35--48",
    abstract = "The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.",
}
