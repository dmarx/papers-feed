\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Bengio(2013)}]{bengio2013deep}
Bengio, Y. 2013.
\newblock Deep learning of representations: Looking forward.
\newblock In \emph{International conference on statistical language and speech
  processing}, 1--37. Springer.

\bibitem[{Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le}]{cubuk2020randaugment}
Cubuk, E.~D.; Zoph, B.; Shlens, J.; and Le, Q.~V. 2020.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, 702--703.

\bibitem[{Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser}]{dehghani2018universal}
Dehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and Kaiser, {\L}. 2018.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}.

\bibitem[{Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei}]{deng2009imagenet}
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, 248--255. Ieee.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT}: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, 4171--4186. Minneapolis,
  Minnesota: Association for Computational Linguistics.

\bibitem[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly
  et~al.}]{dosovitskiy2020image}
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.;
  Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et~al.
  2020.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}.

\bibitem[{Fedus, Zoph, and Shazeer(2021)}]{fedus2021switch}
Fedus, W.; Zoph, B.; and Shazeer, N. 2021.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{arXiv preprint arXiv:2101.03961}.

\bibitem[{Huang et~al.(2018)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu et~al.}]{huang2018gpipe}
Huang, Y.; Cheng, Y.; Bapna, A.; Firat, O.; Chen, M.~X.; Chen, D.; Lee, H.;
  Ngiam, J.; Le, Q.~V.; Wu, Y.; et~al. 2018.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock \emph{arXiv preprint arXiv:1811.06965}.

\bibitem[{Krizhevsky, Hinton et~al.(2009)}]{krizhevsky2009learning}
Krizhevsky, A.; Hinton, G.; et~al. 2009.
\newblock Learning multiple layers of features from tiny images.

\bibitem[{Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut}]{lan2019albert}
Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Soricut, R. 2019.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}.

\bibitem[{Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen}]{lepikhin2020gshard}
Lepikhin, D.; Lee, H.; Xu, Y.; Chen, D.; Firat, O.; Huang, Y.; Krikun, M.;
  Shazeer, N.; and Chen, Z. 2020.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock \emph{arXiv preprint arXiv:2006.16668}.

\bibitem[{Li et~al.(2021)Li, Xue, Li, and You}]{li2021sequence}
Li, S.; Xue, F.; Li, Y.; and You, Y. 2021.
\newblock Sequence Parallelism: Making 4D Parallelism Possible.
\newblock \emph{arXiv preprint arXiv:2105.13120}.

\bibitem[{Loshchilov and Hutter(2017)}]{loshchilov2017decoupled}
Loshchilov, I.; and Hutter, F. 2017.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}.

\bibitem[{Qu et~al.(2019)Qu, Yang, Qiu, Croft, Zhang, and Iyyer}]{qu2019bert}
Qu, C.; Yang, L.; Qiu, M.; Croft, W.~B.; Zhang, Y.; and Iyyer, M. 2019.
\newblock BERT with history answer embedding for conversational question
  answering.
\newblock In \emph{Proceedings of the 42nd International ACM SIGIR Conference
  on Research and Development in Information Retrieval}, 1133--1136.

\bibitem[{Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2019exploring}
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou,
  Y.; Li, W.; and Liu, P.~J. 2019.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}.

\bibitem[{Rajpurkar, Jia, and Liang(2018)}]{rajpurkar-etal-2018-know}
Rajpurkar, P.; Jia, R.; and Liang, P. 2018.
\newblock Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, 784--789. Melbourne,
  Australia: Association for Computational Linguistics.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{rajpurkar-etal-2016-squad}
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
\newblock {SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, 2383--2392. Austin, Texas: Association for
  Computational Linguistics.

\bibitem[{Riquelme et~al.(2021)Riquelme, Puigcerver, Mustafa, Neumann,
  Jenatton, Pinto, Keysers, and Houlsby}]{riquelme2021scaling}
Riquelme, C.; Puigcerver, J.; Mustafa, B.; Neumann, M.; Jenatton, R.; Pinto,
  A.~S.; Keysers, D.; and Houlsby, N. 2021.
\newblock Scaling Vision with Sparse Mixture of Experts.
\newblock \emph{arXiv preprint arXiv:2106.05974}.

\bibitem[{Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean}]{shazeer2017outrageously}
Shazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.; Hinton, G.; and
  Dean, J. 2017.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}.

\bibitem[{Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro}]{shoeybi2019megatron}
Shoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper, J.; and Catanzaro,
  B. 2019.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}.

\bibitem[{Sun et~al.(2017)Sun, Shrivastava, Singh, and
  Gupta}]{sun2017revisiting}
Sun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, 843--852.

\bibitem[{Sun et~al.(2019)Sun, Cheng, Gan, and Liu}]{sun-etal-2019-patient}
Sun, S.; Cheng, Y.; Gan, Z.; and Liu, J. 2019.
\newblock Patient Knowledge Distillation for {BERT} Model Compression.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, 4323--4332. Hong Kong, China:
  Association for Computational Linguistics.

\bibitem[{Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna}]{szegedy2016rethinking}
Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2016.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2818--2826.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang-etal-2018-glue}
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2018.
\newblock {GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural
  Language Understanding.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
  Analyzing and Interpreting Neural Networks for {NLP}}, 353--355. Brussels,
  Belgium: Association for Computational Linguistics.

\bibitem[{Xu et~al.(2020)Xu, Zhou, Ge, Wei, and Zhou}]{xu-etal-2020-bert}
Xu, C.; Zhou, W.; Ge, T.; Wei, F.; and Zhou, M. 2020.
\newblock {BERT}-of-Theseus: Compressing {BERT} by Progressive Module
  Replacing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, 7859--7869. Online: Association for
  Computational Linguistics.

\bibitem[{Xue et~al.(2020{\natexlab{a}})Xue, Sun, Zhang, and
  Chng}]{xue2020embarrassingly}
Xue, F.; Sun, A.; Zhang, H.; and Chng, E.~S. 2020{\natexlab{a}}.
\newblock An Embarrassingly Simple Model for Dialogue Relation Extraction.
\newblock \emph{arXiv preprint arXiv:2012.13873}.

\bibitem[{Xue et~al.(2020{\natexlab{b}})Xue, Sun, Zhang, and
  Chng}]{xue2020gdpnet}
Xue, F.; Sun, A.; Zhang, H.; and Chng, E.~S. 2020{\natexlab{b}}.
\newblock GDPNet: Refining Latent Multi-View Graph for Relation Extraction.
\newblock \emph{arXiv preprint arXiv:2012.06780}.

\bibitem[{Yang et~al.(2021)Yang, Lin, Men, Zhou, Jiang, Jia, Wang, Zhang, Wang,
  Li et~al.}]{yang2021exploring}
Yang, A.; Lin, J.; Men, R.; Zhou, C.; Jiang, L.; Jia, X.; Wang, A.; Zhang, J.;
  Wang, J.; Li, Y.; et~al. 2021.
\newblock Exploring Sparse Expert Models and Beyond.
\newblock \emph{arXiv preprint arXiv:2105.15082}.

\bibitem[{Yang et~al.(2020)Yang, Garcia, Chu, Otani, Nakashima, and
  Takemura}]{yang2020bert}
Yang, Z.; Garcia, N.; Chu, C.; Otani, M.; Nakashima, Y.; and Takemura, H. 2020.
\newblock Bert representations for video question answering.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, 1556--1565.

\bibitem[{You et~al.(2019{\natexlab{a}})You, Li, Hseu, Song, Demmel, and
  Hsieh}]{you2019reducing}
You, Y.; Li, J.; Hseu, J.; Song, X.; Demmel, J.; and Hsieh, C.-J.
  2019{\natexlab{a}}.
\newblock Reducing BERT pre-training time from 3 days to 76 minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}.

\bibitem[{You et~al.(2019{\natexlab{b}})You, Li, Reddi, Hseu, Kumar,
  Bhojanapalli, Song, Demmel, Keutzer, and Hsieh}]{you2019large}
You, Y.; Li, J.; Reddi, S.; Hseu, J.; Kumar, S.; Bhojanapalli, S.; Song, X.;
  Demmel, J.; Keutzer, K.; and Hsieh, C.-J. 2019{\natexlab{b}}.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}.

\bibitem[{Yuan et~al.(2020)Yuan, Tay, Li, Wang, and Feng}]{yuan2020revisiting}
Yuan, L.; Tay, F.~E.; Li, G.; Wang, T.; and Feng, J. 2020.
\newblock Revisiting knowledge distillation via label smoothing regularization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 3903--3911.

\bibitem[{Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and
  Beyer}]{zhai2021scaling}
Zhai, X.; Kolesnikov, A.; Houlsby, N.; and Beyer, L. 2021.
\newblock Scaling vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.04560}.

\bibitem[{Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz}]{zhang2017mixup}
Zhang, H.; Cisse, M.; Dauphin, Y.~N.; and Lopez-Paz, D. 2017.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}.

\bibitem[{Zhou et~al.(2020)Zhou, Huang, Ma, and Huang}]{zhou2020document}
Zhou, W.; Huang, K.; Ma, T.; and Huang, J. 2020.
\newblock Document-Level Relation Extraction with Adaptive Thresholding and
  Localized Context Pooling.
\newblock \emph{arXiv preprint arXiv:2010.11304}.

\bibitem[{Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler}]{zhu2015aligning}
Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.;
  and Fidler, S. 2015.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, 19--27.

\end{thebibliography}
