\pdfoutput=1
\documentclass{article}

\usepackage[left=1.5in,right=1.5in,top=1in,bottom=1in]{geometry}
\parindent 0pt
\parskip .5pc

\usepackage[colorlinks=true]{hyperref}
\usepackage{url}

\usepackage{natbib}
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}

\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amsthm,amssymb}

\newcommand{\supp}{\mbox{supp}}
\newcommand{\R}{\mathbb{R}}
\newtheorem{theorem}{Theorem}[section]
\newcommand{\p}{\mathbb{P}}
\newcommand{\pz}{\mathbb{P}_{z}}
\newcommand{\pxx}{\mathbb{P}_{\tilde{x}}}
\newcommand{\px}{\mathbb{P}_{x}}
\newcommand{\pg}{\mathbb{P}_{g}}
\newcommand{\pxi}{{\mathbb{P}_{x}}_i}
\newcommand{\pwx}{\mathbb{P}_{W^Tx}}
\newcommand{\pwix}{\mathbb{P}_{W_k^Tx}}
\newcommand{\pwig}{\mathbb{P}_{W_k^Tg}}
\newcommand{\expec}{\mathbb{E}}
\newcommand{\ptx}{\mathbb{\tilde{P}}_{x}}
\newcommand{\ptg}{\mathbb{\tilde{P}}_{g}}
\newcommand{\ptwix}{\mathbb{\tilde{P}}_{W_k^Tx}}
\newcommand{\ptwig}{\mathbb{\tilde{P}}_{W_k^Tg}}


\title{\vspace{-3em}\bf Stabilizing GAN Training with\\Multiple Random Projections\\~}
\date{}

\author{%
  \parbox[t]{15em}{\centering%
    \textbf{Behnam Neyshabur}\\
    School of Mathematics\\
    Institute for Advanced Study\\
    Princeton, NJ 90210, USA \\
    \texttt{bneyshabur@ias.edu}
  }~\parbox[t]{19em}{\centering%
      \textbf{Srinadh Bhojanapalli}\\
      Toyota Technological Institute at Chicago\\
      Chicago, IL 60637, USA.\\
      \texttt{srinadh@ttic.edu}
  }\vspace{2em}\\~\parbox[c]{\textwidth}{\centering%
    \textbf{Ayan Chakrabarti}\\
    Computer Science \& Engineering\\
    Washington University in St. Louis\\
    Saint Louis, MO 63130, USA\\
    \texttt{ayan@wustl.edu}
  }
}
  

%\author{Behnam Neyshabur~~~~Srinadh Bhojanapalli~~~~Ayan Chakrabarti\\%
%Toyota Technological Institute at Chicago\\%
%6045 S. Kenwood Ave., Chicago, IL 60637\\%
%\texttt{\{bneyshabur,srinadh,ayanc\}@ttic.edu}%
%}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document} 

\maketitle

\begin{abstract} 
  Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.  Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.
\end{abstract} 

\section{Introduction}
\label{sec:intro}

Generative adversarial networks (GANs), introduced by \cite{goodfellow2014generative}, endow neural networks with the ability to express distributional outputs. The framework includes a generator network that is tasked with producing samples from some target distribution, given as input a (typically low dimensional) noise vector drawn from a simple known distribution, and possibly conditional side information. The generator learns to generate such samples, not by directly looking at the data, but through adversarial training with a discriminator network that seeks to differentiate real data from those generated by the generator. To satisfy the objective of ``fooling'' the discriminator, the generator eventually learns to produce samples with statistics that match those of real data. 

In regression tasks where the true output is ambiguous, GANs provide a means to simply produce an output that is plausible (with a single sample), or to explicitly model that ambiguity (through multiple samples). In the latter case, they provide an attractive alternative to fitting distributions to parametric forms during training, and employing expensive sampling techniques at the test time. In particular, conditional variants of GANs have shown to be useful for tasks such as in-painting~\citep{inpgan}, and super-resolution~\citep{srgan}. Recently, \citet{pix2pix} demonstrated that GANs can be used to produce plausible mappings between a variety of domains---including sketches and photographs, maps and aerial views, segmentation masks and images, \emph{etc}. GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks~\citep{inpgan,chen2016infogan,radford2015unsupervised}.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\columnwidth]{Figs/Fig1.pdf}%
\caption{Overview of our approach. We train a single generator against an array of discriminators, each of which receives lower-dimensional projections---chosen randomly prior to training---as input. Individually, these discriminators are unable to perfectly separate real and generated samples, and thus provide stable gradients to the generator throughout training. In turn, by trying to fool all the discriminators simultaneously, the generator learns to match the true full data distribution.}
  \label{fig:teaser}
\end{figure}

Despite their success, training GANs to generate high-dimensional data (such as large images) is challenging. Adversarial training between the generator and discriminator involves optimizing a min-max objective. This is typically carried out by gradient-based updates to both networks, and the generator is prone to divergence and mode-collapse as the discriminator begins to successfully distinguish real data from generated samples with high confidence. Researchers have tried to address this instability and train better generators through several techniques. \citet{denton2015deep} proposed generating an image by explicitly factorizing the task into a sequence of conditional generations of levels of a Laplacian pyramid, while \citet{radford2015unsupervised} demonstrated that specific architecture choices and parameter settings led to higher-quality samples.  Other techniques include providing additional supervision~\citep{salimans2016improved}, adding noise to the discriminator input~\citep{arjovsky2017towards}, as well as modifying or adding regularization to the training objective functions~\citep{nowozin2016f,arjovsky2017wasserstein,zhao2016energy}.

%In this work, we seek to provide a framework that addresses a key source of instability~\citep{arjovsky2017towards} when training GANs---when the true distribution of high-dimensional data is concentrated in a small fraction of the ambient space. During training in such settings, the generator's samples have very little overlap with the true distribution. This allows the discriminator to easily separate real and generated data perfectly. The generator then no longer receives any significant gradients from the discriminator to update its model, irrespective of its distance from the real data distribution.

We propose a different approach to address this instability, where  the generator is trained against an array of discriminators, each of which looks at a different, randomly-chosen, low-dimensional projection of the data. Each discriminator is unable to perfectly separate real and generated samples since it only gets a partial view of these samples. At the same time, to satisfy its objective of fooling \emph{all} discriminators, the generator learns to match the true full data distribution. We describe a realization of this approach for training image generators, and discuss the intuition behind why we expect this training to be stable---i.e., the gradients to the generator to be meaningful throughout training---and consistent---i.e., for the generator to learn to match the full real data distribution.  Despite its simplicity, we find this approach to be surprisingly effective in practice. We demonstrate this efficacy by using it to train generators on standard image datasets, and find that these produce higher-quality samples than generators trained against a single discriminator.

\subsection{Related Work}\label{sec:related}

Researchers have explored several approaches to improve the stability of GANs for training on higher dimensional images. Instead of optimizing Jensen-Shannon divergence as suggested in the original GAN framework, Energy based GAN ~\citep{zhao2016energy} and Wasserstein GAN~\citep{arjovsky2017wasserstein} show improvement in the stability by optimizing total variation distance and Earth mover distance respectively, together with regularizing the discriminator to limit the discriminator capacity. \citet{nowozin2016f} further extended GAN training to any choice of $f$-divergence as objective. \citet{salimans2016improved} proposed several heuristics to improve the stability of training. These include modifying the objective function, virtual batch-normalization, historical averaging of parameters, semi-supervised learning, \emph{etc.}. All of these methods are designed to improve the quality of gradients, and provide additional supervision to the generator.  They are therefore complementary to, and can likely be used in combination with, our approach.

Note that prior methods have involved  training ensembles of GANs~\citep{wang2016ensembles}, or ensembles of discriminators~\citep{durugkar2016generative}. However, their goal is different from ours. In our framework, each discriminator is shown only a low-dimensional projection of the data, with the goal of preventing it from being able to perfectly reject generated samples. Crucially, we do not combine the outputs of discriminators directly, but rather compute losses on individual discriminators and average these losses.

Indeed, the idea of combining multiple simple classifiers, \emph{e.g.,} with boosting~\citep{freund1996experiments}, or mixture-of-experts~\cite{moe}, has a rich history in machine learning, including recent work where the simple classifiers act on random lower-dimensional projections of their inputs~\citep{cannings2017random} (additionally benefiting from the regularization effect of such projections~\citep{durrant2015random}). While these combinations have been aimed at achieving high classification accuracy (i.e., achieve accurate discrimination), our objective is to maintain a flow of gradients from individual discriminators to the generator. It is also worth noting here the work of \citet{bonneel2015sliced}, who also use low-dimensional projections to deal with high-dimensional probability distributions. They use such projections to efficiently compute Wasserstein distances and optimal transport between two distributions, while we seek to enable stable GAN training in high dimensions.

\section{Training  with Multiple Random Projections}
\label{sec:method}

GANs~\citep{goodfellow2014generative} traditionally comprise of a  generator $G$ that learns to generate samples from a data distribution $\px$, through adversarial training against a single discriminator $D$ as:
\begin{align}
  \label{eq:trad}
\underset{G}{\min} ~~  \underset{  D }{\max} ~~  V(D, G) =  \expec_{x \sim \px} [\log D(x)] + \expec_{z \sim \pz} [\log(1- D(G(z)))],
\end{align}
for $x\in\R^d$, $\px: \R^d\rightarrow\R_+, \int \px = 1$, and with $\pz$ a fixed distribution (typically uniform or Gaussian) for noise vectors $z\in\R^{d'}, d' \ll d$. The optimization in \eqref{eq:trad} is carried out using stochastic gradient descent (SGD), with alternating updates to the generator and the discriminator.  While this approach works surprisingly well, instability is common during training, especially when generating high-dimensional data.

Theoretically, the optimal stationary point for \eqref{eq:trad} is one where the generator matches the data distribution perfectly, and the discriminator is forced to always output 0.5~\citep{goodfellow2014generative}. But usually in practice, the discriminator tends to ``win'' and the cost in \eqref{eq:trad} saturates at a high value. While the loss keeps increasing, the gradients received by the  generator are informative during the early stages of training. However, once the loss reaches a high enough value, the gradients are dominated by noise, at which point generator quality stops improving and in fact begins to deteriorate. Training must therefore be stopped early by manually inspecting sample quality, and this caps the number of iterations over which the generator is able to improve.

\citet{arjovsky2017towards} discuss one possible source of this instability, suggesting that it is because natural data distributions $\px$ often have very limited support in the ambient domain of $x$ and $G(z)$. Then, the generator $G$ is unable to learn quickly enough to generate samples from distributions that have significant overlap with this support. This in turn makes it easier for the discriminator $D$ to perfectly separate the generator's samples, at which point the latter is left without useful gradients for further training.

We propose to ameliorate this problem by training a \emph{single} generator against an \emph{array} of discriminators. Each discriminator operates on a different low-dimensional linear projection, that is set randomly prior to training. Formally, we train a generator $G$ against multiple discriminators $\{D_k\}_{k=1}^K$ as:
\begin{align}\label{eq:mmult}
\underset{G}{\min} ~~  \underset{ \{ D_k \} }{\max} ~~  \sum_{i=k}^K V(D_k, G) =  \sum_{i=k}^K \expec_{x \sim \px} [\log D_k(W_k^T x)] + \expec_{z \sim \pz} [\log(1- D_k(W_k^T G(z)))],
\end{align}
where $W_k, k \in \{1, \cdots, K\}$ is a randomly chosen matrix in $\mathbb{R}^{d \times m}$ with $m < d$. Therefore, instead of a single discriminator looking at the full input (be it real or fake) in \eqref{eq:trad}, each of the $K$ discriminators in \eqref{eq:mmult} sees a different low-dimensional projection. Each discriminator tries to maximize its accuracy at detecting generated samples from its own projected version, while the generator tries to ensure that each sample it generates simultaneously fools all discriminators for all projected versions of that sample.

When the true data $X$ and generated samples $G(z)$ are images, prior work~\citep{radford2015unsupervised} has shown that it is key that both the generator and discriminator have convolutional architectures. While individual discriminators in our framework see projected inputs that are lower-dimensional than the full image, their dimensionality is still large enough (a very small $m$ would require a large number of discriminators) to make it hard to train discriminators with only fully-connected layers. Therefore, it is desirable to employ convolutional architectures for each discriminator. To do so, the projection matrices $W_k^T$ must be chosen to produce ``image-like'' data. Accordingly, we use strided convolutions with random filters to embody the projections $W_k^T$ (as illustrated in Fig.~\ref{fig:teaser}). The elements of these convolution filters are drawn i.i.d.~from a Gaussian distribution, and the filters are then scaled to have unit $\ell_2$ norm.  To promote more ``mixing'' of the input coordinates, we choose filter sizes that are larger than the stride   (\emph{e.g.,} we use $8\times 8$ filters when using a stride of 2). While still random, this essentially imposes a block-Toeplitz structure on $W_k^T$.

\section{Motivation}
\label{sec:theory}

In this section, we  motivate our approach, and provide the reader with some intuition for why we \emph{expect} our approach to improve the stability of training while maintaining consistency.

\newcommand{\vol}{\mbox{Vol}}

\textbf{Stability.}  Each discriminators $D_k$ in our framework in \eqref{eq:mmult} works with low dimensional projections of the data and can do no better than the traditional full discriminator $D$ in \eqref{eq:trad}. Let $y \in \R^d$ and $l \in \{0,1\}$ denote the input and ideal output of the original discriminator $D$, where $l=1$ if $y$ is real, and $0$ if generated. However, each $D_k$ is provided only a lower dimensional projection $W_k^Ty$. Since $W_k^Ty$ is a deterministic and non-invertible function of $y$, it can contain no more information regarding $l$ than $Y$ itself, i.e., $I(l;Y) \geq I(l;W^T_kY)$, where $I(x;y)$ denotes the mutual information between the random variables $x$ and $y$. In other words, taking a low-dimensional projection introduces an information bottleneck which interferes with the ability of the discriminator to determine the real/fake label $l$.

While strictly possible, it is intuitively unlikely that all the information required for classification is present in the $W_k^Ty$ (for all, or even most of the different $W_k^T$), especially in the adversarial setting. We already know that GAN training is far more stable in lower dimensions, and we expect that each of the discriminators $D_k$ will perform similarly to a traditional discriminator training on lower  (i.e., $m-$) dimensional data. Moreover, \citet{arjovsky2017towards} suggest that instability in higher-dimensions is caused by the true distribution $\px$ being concentrated in a small fraction of the ambient space---i.e., the support of $\px$ occupies a low volume relative to the range of possible values of $x$. Again, we can intuitively expect this to be ameliorated by performing a random lower-dimensional projection (and provide analysis for a simplistic $\px$ in the supplementary).

When the discriminators $D_k$ are not perfectly saturated, $D_k(W^T_kG(z))$ will have some variation in the neighborhood of a generated sample $G(z)$, which can provide meaningful gradients to the generator. But note that gradients from each individual discriminator $D_k$  to $G(z)$ will lie entirely in the lower dimensional sub-space spanned by $W_k^T$.

\textbf{Consistency.} While impeding the discriminator's ability benefits stability, it could also have trivially been achieved by other means---\emph{e.g.,} by severely limiting its architecture or excessive regularization. However, these would also limit the ability of the discriminator to encode the true data distribution and pass it on to the generator. We begin by considering the following modified version of  Theorem 1 in~\cite{goodfellow2014generative}:
\begin{theorem}\label{thm:const1}
  Let $\pg$ denote the distribution of the generator outputs $G(z)$, where $z \sim \pz$, and let $\pwig$ be the marginals of $\pg$ along $W_k$. For fixed $G$, the optimal $\{D_k\}$ are given by
  \begin{equation}
    D_k(y) = \pwix(y) / \left(\pwix(y) + \pwig(y)\right),
  \end{equation}
  for all $k \in \{1, \cdots, K\}$. The optimal $G$ under \eqref{eq:mmult} is achieved iff $\pwix =\pwig$, $\forall k$.
\end{theorem}
This  result, proved in the supplementary, implies that under ideal conditions %(infinite capacity of both discriminator and generator networks)
the generator will produce samples from a distribution whose \emph{marginals} along each of the projections $W_k$ match those of the true distribution. Thus, each discriminator adds an additional constraint on the generator, forcing it to match $\px$ along a different marginal. As we show in the supplementary, matching along a sufficiently high number of such marginals---under smoothness assumptions on the true and generator distributions $\px$ and $\pg$---guarantees that the full joint distributions will be close (note it isn't sufficient for the set of projection matrices $\{W_k\}$ to simply span $\R^d$ for this). Therefore, even though viewing the data through random projections limits the ability of individual discriminators, with enough discriminators acting in concert, the generator learns to match the full joint distribution of real data in $\R^d$.

\section{Experimental Results}
\label{sec:experiments}

We now evaluate our approach with experiments comparing it to generators trained against a single traditional discriminator, and demonstrate that it leads to higher stability during training, and ultimately yields generators that produce higher quality samples.

\begin{figure}[!t]
  \centering
\includegraphics[width=\textwidth]{Figs/Fig2a.pdf}%  
\caption{Training Stability.  We plot the evolution of the ``generator loss'' across training---against a traditional single discriminator (DC-GAN), and the average and individual losses against multiple discriminators ($K=48$) in our setting. For the traditional single discriminator, this loss rises quickly to high value, indicating that the discriminator saturates to rejecting generated samples with very high confidence. In contrast, the loss in our case remains lower, allowing our discriminators to provide meaningful gradients to the generator.}
\label{fig:trevol}
\end{figure}
\begin{figure}[!t]
  \centering
\includegraphics[width=0.96\textwidth]{Figs/Fig2b.pdf}%  
\caption{Evolution of sample quality across training iterations. With our approach, the generator improves the visual quality of its samples quickly and throughout training. Meanwhile, the generator trained in the traditional setting with a single discriminator shows slower improvement, and indeed quality begins to deteriorate after a point.}% The proposed generator also appears to learn the data distribution faster, producing higher quality samples even in early iterations.}
\label{fig:trevol2}
\end{figure}

\textbf{Dataset and Architectures.} For evaluation, we primarily use the dataset of celebrity faces collected by \citet{liu2015faceattributes}---we use the cropped and aligned $64\times 64$-size version of the images---and the DC-GAN~\citep{radford2015unsupervised} architectures for the generator and discriminator. We make two minor modifications to the DC-GAN implementation that we find empirically to yield improved results (for both the standard single discriminator setting, as well as our multiple discriminator setting). First, we use different batches of generated images to update the discriminator and generator---this increases computation per iteration, but yields better quality results. Second, we employ batch normalization in the generator but \emph{not} in the discriminator (the original implementation normalized real and generated batches separately, which we found yielded poorer generators).

\begin{figure}[!t]
  \centering
    \includegraphics[width=0.88\textwidth]{Figs/Fig3.pdf}
\caption{Random sets of generated samples from traditional DC-GAN and the proposed framework. For DC-GAN, we show results from the model both at 40k iterations (when the samples are qualitatively the best) and at the end of training (100k iterations). For our setting, we show samples from the end of training for generator models trained with $K=12$, $24$, $48$ projections. Our generator produces qualitatively better samples, with finer detail and fewer distortions. Quality is worse with subtle high-frequency noise when $K$ is smaller, but these decrease with increasing $K$ to 24 and 48.} %We find that our generator learns to produce qualitatively better face images, with finer detail and fewer distortions. We also note that the quality is slightly worse when $K$ only equals the projection ratio $d/m=12$, with subtle high-frequency noise apparent in the samples. However, these artifacts decrease with increasing $K$, and setting $K=24$ and $48$ leads to similar, high-quality samples.}
  \label{fig:thefaces}
\end{figure}

For our approach, we train a generator against $K$ discriminators, each operating on a different single-channel $32\times 32$ projected version of the input, \emph{i.e.,}~$d/m=12$. The projected images are generated through convolution with $8\times 8$ filters and a stride of two. The filters are generated randomly and kept constant throughout training. We compare this to the standard DC-GAN setting of a single discriminator that looks at the full-resolution $64\times 64$ color image. We use identical generator architectures in both settings---that map a $100$ dimensional uniformly distributed noise vector to a full resolution image. The discriminators also have similar architectures---but each of the discriminator in our setting has one less layer as it operates on a lower resolution input (we map the number of channels in the first layer in our setting to those of the second layer of the full-resolution single-discriminator, thus matching the size of the final feature vector used for classification). As is standard practice, we compute generator gradients in both settings with respect to minimizing the ``incorrect'' classification loss of the discriminator---in our setting, this is given by $-\frac{1}{K}\sum_k \log D_k(G(z))$. As suggested in~\citep{radford2015unsupervised}, we use Adam~\citep{adam} with learning rate $2\times10^{-4}$, $\beta_1=0.5$, and a batch size of $64$.

\begin{figure}[!t]
  \centering
    \includegraphics[width=0.95\textwidth]{Figs/Fig4.pdf}%
\caption{Interpolating in latent space. For selected pairs of generated faces (with $K=48$), we generate samples using different convex combinations of their corresponding noise vectors. Every combination generates a plausible face, and these appear to smoothly interpolate between various facial attributes---age, gender, expression, hair, \emph{etc.} Note that the $\alpha=0.5$ sample always corresponds to an individual clearly distinct from the original pair.}
  \label{fig:interp}
\end{figure}

\textbf{Stability.} We begin by analyzing the evolution of generators in both settings through training. Figure~\ref{fig:trevol} shows the generator training loss for traditional DC-GAN with a single discriminator, and compares it the proposed framework with $K=48$ discriminators. In both settings, the generator losses increase through much of training after decreasing in the initial iterations (\emph{i.e.,} the discriminators eventually ``win''). However, DC-GAN's generator loss rises quickly and remains higher than ours in absolute terms throughout training. Figure~\ref{fig:trevol2} includes examples of generated samples from both generators across iterations (from the same noise vectors). We observe that DC-GAN's samples improve mostly in the initial iterations while the training loss is still low, in line with our intuition that generator gradients become less informative as discriminators get stronger. Indeed, the quality of samples from traditional DC-GAN actually begins to deteriorate after around 40k iterations. In contrast, the generator trained in our framework improves continually  throughout training.

\textbf{Consistency.} Beyond stability, Fig.~\ref{fig:trevol2} also demonstrates the consistency of our framework. While the average loss in our framework is lower, we see this does not impede our generator's ability to learn the data distribution quickly as it collates feedback from multiple discriminators. Indeed, our generator produces higher-quality samples than traditional DC-GAN even in early iterations. Figure~\ref{fig:thefaces} includes a larger number of (random) samples from generators trained with traditional DC-GAN and our setting. For DC-GAN, we include samples from both roughly the end (100k iterations) of training, as well as from roughly the middle (40k iterations) where the sample quality are approximately the best. For our approach, we show results from training with different numbers of discriminators with $K=12, 24,$ and $48$---selecting the generator models from the end of training for all. We see that our generators produce face images with higher-quality detail and far fewer distortions than traditional DC-GAN. We also note the effect of the number of discriminators on sample quality. Specifically, we find that setting $K$ to be equal only to the projection ratio $d/m=12$ leads to subtle high-frequency noise in the generator samples, suggesting these many projections do not sufficiently constrain the generator to learn the full data distribution. Increasing $K$ diminishes these artifacts, and $K=24$ and $48$ both yield similar, high-quality samples.

\textbf{Training Time.} Note that the improved generator comes at the expense of increased computation during training. Traditional DC-GAN with a single discriminator takes only $0.6s$ per training iteration (on an NVIDIA Titan X), but this goes up to $3.2s$ for $K=12$, $5.8s$ for $K=24$, and $11.2s$ for $K=48$ in our framework. Note that once trained, all generators produce samples at the same speed as they have identical architectures.

\textbf{Latent Embedding.} Next, we explore the quality of the embedding induced by our generator ($K=48$) in the latent space of noise vectors $z$. We consider selected pairs of randomly generated faces, and show  samples generated by linear interpolation between their corresponding noise vectors in Fig.~\ref{fig:interp}. Each of these generated samples is also a plausible face image, which confirms that our generator is not simply memorizing training samples, and that it is densely packing the latent space with face images. We also find that the generated samples smoothly interpolate between semantically meaningful face attributes---gender, age, hair-style, expression, and so on. Note that in all rows, the sample for $\alpha=0.5$ appears to be a clearly different individual than the ones represented by $\alpha=0$ and $\alpha=1$.


\begin{figure}[!t]
  \centering
    \includegraphics[width=\textwidth]{Figs/dsel.png}
\caption{Examples from training on canine images from Imagenet. We show manually selected examples of $128\times 128$ images produced by a generator trained on various canine classes from Imagenet. Although not globally plausible, the samples contain realistic low-level textures, and reproduce rough high-level composition.}
  \label{fig:highres}
\end{figure}

\textbf{Results on Imagenet-Canines.} Finally, we show results on training a generator on a subset of the Imagenet-1K database~\citep{deng2009imagenet}. We use $128\times 128$  crops (formed by scaling the smaller side to 128, and taking a random crop along the other dimension) of 160k images from a subset of Imagenet classes (ids 152 to 281) of different canines. We use similar settings as for faces, but feed a higher 200-dimensional noise vector to the generator, which also begins by mapping this to a feature vector that is twice as large (2048), and which has an extra transpose-convolution layer to go upto the $128\times 128$ resolution. We again use $8\times 8$ convolutional filters with stride two to form $\{W_k^T\}$---in this case, these produce $64\times 64$ single channel images. We use only $K=12$ discriminators, each of which has an additional layer because of the higher-resolution---beginning with fewer channels in the first layer so that the final feature vector used for classification is the same length as for faces. Figure~\ref{fig:highres} shows manually selected samples after 100k iterations of training (see supplementary material for a larger random set). We see that since it is trained on a more diverse and unaligned image content, the generated images are not globally plausible photographs. Nevertheless, we find that the produced images are sharp, and that generator learns to reproduce realistic low-level textures as well as some high-level composition.

\section{Conclusion}
\label{sec:conc}

In this paper, we proposed a new framework to training GANs for high-dimensional outputs. Our approach employs multiple discriminators on random low-dimensional projections of the data to stabilize training, with enough projections to ensure that the generator learns the true data distribution. Experimental results demonstrate that this approaches leads to more stable training, with generators continuing to improve for longer to ultimately produce higher-quality samples. Source code and trained models for our implementation is available at the project page  \url{http://www.cse.wustl.edu/~ayan/rpgan/}.

In our current framework, the number of discriminators is limited by computational cost. In future work, we plan to investigate training with a much larger set of discriminators, employing only a small subset of them at each iteration, or every set of iterations. We are also interested in using multiple discriminators with modified and regularized objectives (\emph{e.g.}, \citep{nowozin2016f,arjovsky2017wasserstein,zhao2016energy}). Such modifications are complementary to our approach, and deploying them together will likely be beneficial.

\paragraph{Acknowledgments.} AC was supported by the National Science Foundation under award no.~IIS:1820693, and also thanks NVIDIA corporation for the donation of a Titan X GPU that was used in this research.
\clearpage

\input{rpgan.bbl}

\clearpage
\appendix

\begin{center}
  {\LARGE \bf Supplementary Material\\}~\\~
\end{center}
\section{Theory}
In this section we provide additional theoretical results showing the benefits of random projections, for some particular choice of distributions, for both improving stability and guaranteeing consistency of GAN training.

\subsection*{Stability}

Here, we consider several simplifying assumptions on the distribution of $x$, to gain intuition as to how a random low-dimensional projection affects the ``relative'' volume of the support of $\px$. Let us assume that the range of $x$ is the $d$-dimensional ball $B^d$ of radius 1 centered at 0. We define the support $\supp(\px)\subset B^d$ of a distribution $\px$ to be the set where the density is greater than some small threshold $\epsilon$. Assume that the projection $W\in\R^{d\times m}$ is entry-wise random Gaussian (rather than corresponding to a random convolution). Denote as $B^d_W$ the projection of the range on $W$, and $\pwx$ as the marginal of $\px$ along $W$.

\begin{theorem}\label{thm:volume}
Assume $\px = \sum_j \tau_j \mathcal{N}(x|\mu_j,\Sigma_j)$ is a mixture of Gaussians, such that the individual components are sufficiently well separated (in the sense that there is no overlap between their supports or the projections thereof, see \citep{sanjoy}). If $\supp(\px) \subset B^d$ and $\vol(\supp(\px))>0$, then  $\vol(\supp(\pwx))/\vol(B^d_W) > \vol(\supp(\px))/\vol(B^d)$  with high probability.
\end{theorem}
This  result implies that the projection of the support of $\px$, under this simplified assumptions, occupies a higher fraction of the volume of the projection of the range of $x$. This aids in stability because it makes it more likely that a larger fraction of the generator's samples (which also lie within the range of $x$) will, after projection, overlap with the projected support $\supp(\px)$, and can not be rejected absolutely by the discriminator.

\paragraph{Proof of Theorem~\ref{thm:volume}.} We first show that we can assume that the columns of the projection $W$ are orthonormal. Since $W\in\R^{d\times m}$ is entry-wise Gaussian distributed, it has rank $m$ with high probability. Then, there exists a square invertible matrix $A$ such that $W'=AW$ where $W'$ is orthonormal. In that case, $\vol(\supp(\pwx))/\vol(B^d_W) = \vol(\supp(\mathbb{P}_{{W'}^T_x}))/\vol(B^d_{W'})$ because the numerator and denominator terms for both can be related by $\det(A)$ for the change of variables, which cancels out. Note that under this orthonormal assumption, $B^d_W=B^m$.

Next, we consider the case of an individual Gaussian distribution $\px = \mathcal{N}(x|\mu,\Sigma)$, and prove that the ratio of supports (defined with respect to a threshold $\epsilon$) does not decrease with the projection. The expression for these ratios is given by:
\begin{align}
  \vol(\supp(\px)) &= \vol(B^d) \times \det(\Sigma) \times \left[\log \frac{1}{\epsilon^2} - d\log 2\pi - \log \det(\Sigma) \right]\notag\\
  \Rightarrow \frac{\vol(\supp(\px))}{\vol(B^d)} &= \det(\Sigma) \times \left[\log \frac{1}{\epsilon^2} - d\log 2\pi - \log \det(\Sigma) \right].\\
  \frac{\vol(\supp(\pwx))}{\vol(B^d_W)} &= \det(W^T\Sigma W) \times \left[\log \frac{1}{\epsilon^2} - m\log 2\pi - \log \det(W^T\Sigma W) \right].
\end{align}
For sufficiently small $\epsilon$, the volume ratio of a single Gaussian will increase with projection if $\det(W^T\Sigma W) > \det(\Sigma)$. Note that all eigenvalues of $\Sigma \leq 1$, with at-least one eigenvalue strictly $< 1$ (since $\supp(\px) \subset B^d$). First, we consider the case when $\Sigma$ is not strictly positive definite and one of the eigenvalues is $0$. Then, $\vol(\supp(\px))=0$ and $\vol(\supp(\pwx)) \geq 0$, \emph{i.e.,} the volume ratio either stays the same or increases.

For the case when all eigenvalues are strictly positive, consider a co-ordinate transform where the first $m$ co-ordinates of $x$ correspond to the column vectors of $W$, such that
\begin{equation}
  \Sigma = \left[\begin{array}{cc} \Sigma_W & \Sigma_{WW'}\\\Sigma_{WW'}^T & \Sigma_{W'}\end{array}\right],
\end{equation}
where $\Sigma_W = W^T\Sigma W$. Then,
\begin{align}
  \det(\Sigma) &= \det(\Sigma_W)\det(\Sigma_{W'}-\Sigma_{WW'}^T\Sigma_W^{-1}\Sigma_{WW'})\notag\\
               &\leq \det(\Sigma_W)\det(\Sigma_{W'}),\notag\\
  \Rightarrow \det(\Sigma_W) &\geq \det(\Sigma) / \det(\Sigma_{W'}).
\end{align}
Note that $\det(\Sigma_{W'}) \leq 1$, since all eigenvalues of $\Sigma$ are $\leq 1$, with equality only when $W$ is completely orthogonal to the single eigenvector whose eigenvalue is strictly $< 1$, which has probability zero under the distribution for $W$. So, we have that $\det(\Sigma_{W'}) < 1$, and
\begin{equation}
  \label{eq:detinc}
  \det(W^T \Sigma W) = \det(\Sigma_w) > \det(\Sigma).
\end{equation}

The above result shows that the volume ratio of individual components never decrease, and \emph{always} increase when their co-variance matrices are full rank (no zero eigenvalue). Now, we consider the case of the Gaussian mixture. Note that the volume ratio of the mixture equals the sum of the ratios of individual components, since the denominator $\vol(B^m)$ is the same, where the support volume in these ratios for component $j$ is defined with respect to a threshold $\epsilon / \tau_j$. Also, note that since mixture distribution has non-zero volume, at least one of the Gaussian components must have all non-zero eigenvalues. So, the volume ratios of $\px$ and $\pwx$ are both sums of individual Gaussian component terms, and each term for $\pwx$ is greater than or equal to the corresponding term for $\px$, and at least one term is strictly greater. Thus, the support volume ratio of $\pwx$ is strictly greater than that of $\px$.

\subsection*{Consistency}

\paragraph{Proof of Theorem~\ref{thm:const1}.} The proof  follows along the same steps as that of Theorem 1 in~\cite{goodfellow2014generative}.
\begin{align}&V(D_k, G) \notag\\&=\expec_{x \sim \px} [\log D_k(W_k^T x)] + \expec_{x \sim \pg} [\log(1- D_k(W_k^T x) )]\notag\\
&= \expec_{Y \sim \pwix} [\log D_k(y)] + \expec_{y \sim \pwig} [\log(1- D_k(y) )].
\end{align} 
For any point $y \in \supp(\pwix) \cup \supp(\pwig)$, differentiating $V(D_k, G)$ w.r.t.~$D_k$ and setting to $0$ gives us:
\begin{equation}
D_k(y) =  \frac{\pwix(y)}{\pwix(y) + \pwig(y)}.  
\end{equation}
Notice we can rewrite $V(D_k, G)$ as
\begin{multline}
V(D_k,G)=-2\log(2)+KL \left(\pwix|| \frac{\pwix +\pwig}{2}\right)\\+KL \left(\pwig|| \frac{\pwix +\pwig}{2}\right).
\end{multline}
Here KL is the Kullback Leibler divergence, and it is easy to see that the above expression achieves the minimum value when $\pwix =\pwig$.\\~



Next, we present a result that shows that given enough random projections $K$, the full distribution $\pg$ of generated samples will closely match the full true distribution $\px$.
\paragraph{Def:} A function $f: \R^d \to \R_+$ is $L$-Lipschitz, if $\forall~x_1,x_2 \in \R^d$, $\lvert f(x_1) - f(x_2) \rvert \leq L \cdot d(x_1, x_2)$.
\begin{theorem}\label{thm:const2}
Let $\px$ and $\pg$ be two compact distributions with support of radius $B$, such that $\pwix = \pwig, \forall  k \in \{1, \cdots, K\}$. Let $\{ W_k\}$ be entrywise random Gaussian matrices in $\R^{d \times m}$. Let $R =\px -\pg$ be the residual function of the  difference of densities. Then,  with high probability, for any $x \in \R^d$, $$ \lvert R(x) \rvert = \lvert \px(x) -\pg(x)\rvert \leq O\left(\frac{BL_R}{K^{\frac{1}{d-m}}} \right), $$ where $L_R$ is the Lipschitz constant of $R$. 
\end{theorem}
This theorem captures how much two probability densities can differ if they match along $K$ marginals of dimension $m$, and how the error decays with increasing number of discriminators $K$. %To get an error of $\epsilon$ we need $K = O( \left(\frac{B L_R}{\epsilon} \right)^{d-m} )$.
In particular, if we have a smooth residual function $R$ (with small Lipschitz constant $L_R$), then at any point $\in \R^d$, it is constrained to have small values---smaller with increasing number of discriminators $K$, and higher dimension $m$. The dependence on $L_R$ suggests that the residual can take larger values if it is not smooth---which can result from either the true density $\px$ or the generator density $\pg$, or both, being not smooth.

Again, this result gives us rough intuition about how we may expect the error between the true and generator distribution to change with changes to the values of $K$ and $m$. In practice, the empirical performance will depend on the nature of the true data distribution, the parametric form / network architecture for the generator, and the ability of the optimization algorithm to get near the true optimum in Theorem \ref{thm:const1}.

\paragraph{Proof of Theorem~\ref{thm:const2}.}

Let $R = \px -\pg$, be the residual function that captures the difference between the two distributions.  $R$ satisfies the following properties: \begin{equation}  \int_x R(x) dx =  \int_x (\px(x) -\pg(x)) dx = 1 -1 =0, \end{equation} and for any set $S$, \begin{equation}  \int_{x \in S} R(x) dx \leq \int_{x \in S} \px(x) dx \leq 1. \end{equation} Further, since both the distributions have same marginals along $k$ different directions $\{ W_k\}$, we have, for any $x$, \begin{equation*}{R}_{W_k^Ty}(x)= \int_{y | x= W_k^T y} R(y) =0. \end{equation*} 
 
We first prove this result for discrete distributions supported on a compact set $\mathcal{S}$ with $\gamma$ points along each dimension. Let $\mathbb{\tilde{P}}$ denote such a discretization of a distribution $\mathbb{P}$ and $\tilde{R} = \ptx -\ptg$.

Each of the marginal equation $\ptwix =\ptwig$ ($\tilde{R}_{W_k^Ty} =0 $) is equivalent to $ \gamma^m$ linear equations of the distribution $\ptx$ of the form, $\sum_{x: W_k^Tx=y}~ \ptx(x) =\ptwig(y)$. Note that we have $\gamma^d$ choices for $x$ and $\gamma^m$ choices for $y$. Let $A_k \in \R^{\gamma^m \times \gamma^d}$ denote the coefficient matrix $A_k \ptx =\ptwig$, such that $A_k(i, j) = 1$ if $W_k^T x_i =y_j$, and 0 otherwise.

The rows of $A_k$ for different values of $y_j$ are clearly orthogonal. Further, since different $W_k$ are independent Gaussian matrices, rows of $A_k$ corresponding to different $W_k$ are linearly independent. In particular let $A \in \R^{\gamma^m K \times \gamma^d}$ denote the vertical concatenation of $A_k$. Then, $A$ has full row rank of $\gamma^m\cdot K$ with probability $\geq 1 -c\cdot m\cdot e^{-d}$~\citep{vershynin2010introduction}, with $c$ some arbitrary positive constant. Since $\ptx$ is a $\gamma^d$ dimensional vector, $\gamma^d$ linearly independent equations determine it uniquely. Hence $\gamma^m\cdot K \geq \gamma^d$, guarantees that $\ptx =\ptg$ or $\tilde{R} =0$.

Now we extend the results to the continuous setting. Without loss of generality, let the compact support $\mathcal{S}$ of the distributions be contained in a sphere of radius $B$. Let $\mathcal{N}_{\frac{\epsilon}{L}}$ be an $\frac{\epsilon}{L_R}$ net of $\mathcal{S}$, with $\gamma^d$ points (see Lemma 5.2 in \cite{vershynin2010introduction}), where $\gamma=2B\cdot L_R/\epsilon$. Then for every point $x_1 \in \mathcal{S}$, there exists a $x_2 \in \mathcal{N}_{\frac{\epsilon}{L_R}}$ such that, $d(x_1,x_2) \leq \frac{\epsilon}{L_R}$.

Further for any $x_1, x_2$ with $d(x_1, x_2) \leq  \frac{\epsilon}{L_R}$, if $R$ is $L_R$ Lipschitz we know that, \begin{equation}\label{eq:thm2}\lvert R(x_1) -R(x_2)\rvert \leq L_R \cdot \frac{\epsilon}{L_R} =\epsilon.\end{equation}

Finally, notice that the marginal constraints do not guarantee that the distributions $\ptwix$ and $\ptwig$ match exactly on the $\epsilon$-net (or that $R$ is 0), but only that they are equal upto an additive factor of $\epsilon$. Hence, combining this with equation~\ref{eq:thm2} we get,
$|R(x)| = \lvert \px(x) -\pg(x)\rvert \leq O(\epsilon),$ for any $x$ with probability $\geq 1- c \cdot m \cdot K \cdot e^{-d}$.  Since we have $\gamma^{d-m} = \left(\frac{BL}{\epsilon}\right)^{d-m} =O(\frac{1}{K})$, we get  $\epsilon = O(\frac{BL}{K^{\frac{1}{d-m}}})$. 

\clearpage
\section{Additional Experimental Results}

\textbf{Face Images: Proposed Method ($K=48$)}\\
\includegraphics[width=1.0\textwidth]{Figs/sup/fk48.jpg}\\
\clearpage
\textbf{Face Images: Proposed Method ($K=24$)}\\
\includegraphics[width=1.0\textwidth]{Figs/sup/fk24.jpg}\\
\clearpage
\textbf{Face Images: Proposed Method ($K=12$)}\\
\includegraphics[width=1.0\textwidth]{Figs/sup/fk12.jpg}\\
\clearpage
\textbf{Face Images: Traditional DC-GAN (Iter. 40k)}\\
\includegraphics[width=1.0\textwidth]{Figs/sup/dcg40k.jpg}\\
\clearpage
\textbf{Face Images: Traditional DC-GAN (Iter. 100k)}\\
\includegraphics[width=1.0\textwidth]{Figs/sup/dcg100k.jpg}\\
\clearpage
\textbf{Random Imagenet-Canine Images: Proposed Method}\\
\includegraphics[width=1.0\textwidth]{Figs/sup/dogs.jpg}\\

\end{document} 


 
