
\section{Related Work} \label{sec:related_work}


\paragraph{Text-to-Image Diffusion Models.} Text-to-image diffusion models have made significant strides in producing high-quality and diverse images. These models depend on robust text encoders to interpret intricate image descriptions. Several models, such as GLIDE\cite{nichol2021glide}, LDM\cite{rombach2022high}, DALL-E 2\cite{ramesh2022hierarchical}, and Stable Diffusion\cite{rombach2022high,podell2023sdxl}, leverage the pre-trained CLIP\cite{radford2021learning} model to generate text embeddings. Other models like Imagen\cite{saharia2022photorealistic}, Pixart-$\alpha$\cite{chen2023pixart}, ELLA\cite{hu2024ella}, and DALL-E 3\cite{betker2023improving} employ large pre-trained language models, such as T5\cite{raffel2020exploring}, to enhance their understanding of text. Some models, including eDiff-I\cite{balaji2022ediffi} and EMU\cite{dai2023emu}, use a combination of both CLIP and T5 embeddings to improve their capabilities. ParaDiffusion\cite{wu2023paradiffusion} proposes fine-tuning the LLaMA-2\cite{touvron2023llama} model during diffusion model training and utilizing the fine-tuned language model text features as a condition. To further enhance the prompt following ability, we integrate large language models (LLM\cite{raffel2020exploring,touvron2023llama,zhang2024tinyllama}) with pre-trained CLIP-based models, using techniques such as TSC (Textual Style Control).


\paragraph{Subject-driven Image Generation.} 
This category includes studies focused on enhancing personalization and subject specificity in image generation through innovative techniques and architectures. Subject-Diffusion~\cite{ma2023subject} integrates text and image semantics for personalized generation without test-time fine-tuning. ELITE~\cite{wei2023elite} and FastComposer~\cite{xiao2023fastcomposer} reduce the need for fine-tuning by employing efficient encoding and attention mechanisms for personalized image generation.
BLIP-Diffusion~\cite{li2024blip} and Kosmos-G~\cite{pan2023kosmos} utilize pre-trained models for quick and effective personalized image generation. Unified Multi-Modal Latent Diffusion~\cite{ma2023unified} and IP-Adapter~\cite{ye2023ip} enhance image quality by integrating multimodal inputs to align images with textual descriptions. FaceStudio~\cite{yan2023facestudio}, InstantID~\cite{wang2024instantid}, and PhotoMaker~\cite{li2023photomaker} address the high resource demands of previous models and include features for identity preservation, critical for high-fidelity tasks like artistic portrait generation. 
The MoA (Mixture-of-Attention)~\cite{ostashev2024moa} uses a novel mechanism to separate subject and context for better image quality. 
BootPIG~\cite{purushwalkam2024bootpig} uses the reference net to introduce low-level information and achieves pixel-level control over generated images. 
The most recent and related work is SSR-Encoder~\cite{zhang2023ssr}, which uses cross-attention to inject image information into text features and supports selective feature extraction.

\paragraph{Optimization-based subject-driven image generation.} The paper \cite{gal2022image} introduces a method to personalize text-to-image generation through unique embeddings derived from user-provided images, enhancing the creation of unique concepts. Dreambooth \cite{ruiz2023dreambooth} describes a technique for fine-tuning text-to-image models to produce novel, contextualized images of a specific subject using a unique identifier. The paper \cite{liu2023cones} explores the concept of neurons in diffusion models that facilitate customized generation and efficient storage. A subsequent study \cite{liu2023cones} addresses synthesizing images with multiple subjects using text embeddings and spatial layouts to improve the quality and control of the synthesis.