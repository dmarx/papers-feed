---
author:
- |
  **Yucheng Han$^{1,2}$[^1] Rui Wang$^{2\ast}$[^2] Chi Zhang$^{2\ast}$ [^3] Juntao Hu$^{2}$**  
  **Pei Cheng$^2$ Bin Fu$^2$ Hanwang Zhang$^1$**   
  $^1$Nanyang Technological University $^2$ Tencent  
  $^1$ {yucheng002, hanwangzhang}@ntu.edu.sg  
  $^2$ {raywwang, johnczhang, jetthu, brianfu}@tencent.com  
  <https://tencentqqgylab.github.io/EMMA>
bibliography:
- citefile.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts "
---





<figure id="fig:teaser">
<span class="image placeholder" data-original-image-src="images/emma_teaser_v2.pdf" data-original-image-title="" width=".95\textwidth"></span>
<figcaption>EMMA could compose multiple multi-modal conditions (on the top left branch) without further finetuning, while still maintaining strong text control over the generated results (bottom branch). Furthermore, EMMA could combine various existing diffusion models in communities without training.</figcaption>
</figure>

[^1]: Equal contributions. Work was done when Yucheng Han was a Research Intern at Tencent.

[^2]: Project Leader.

[^3]: Corresponding Author.
