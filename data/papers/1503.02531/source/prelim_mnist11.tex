\section{Preliminary experiments on MNIST}

To see how well distillation works, we trained a single large neural
net with two hidden layers of 1200 rectified linear hidden units on
all 60,000 training cases. The net was strongly regularized using
dropout and weight-constraints as described in \cite{dropout}. Dropout
can be viewed as a way of training an exponentially large ensemble of
models that share weights. In addition, the input images were jittered
by up to two pixels in any direction.  This net achieved 67 test
errors whereas a smaller net with two hidden layers of 800 rectified
linear hidden units and no regularization achieved 146 errors. But if
the smaller net was regularized solely by adding the additional task
of matching the soft targets produced by the large net at a
temperature of 20, it achieved 74 test errors. This shows that soft
targets can transfer a great deal of knowledge to the distilled model,
including the knowledge about how to generalize that is learned from
translated training data even though the transfer set does not contain
any translations.

When the distilled net had 300 or more units in each of its two hidden
layers, all temperatures above 8 gave fairly similar results.  But
when this was radically reduced to 30 units per layer, temperatures in the range
2.5 to 4 worked significantly better than higher or lower
temperatures.

We then tried omitting all examples of the digit 3 from the transfer set.  So from the perspective of the distilled
model, 3 is a mythical digit that it has never seen. Despite this, the distilled model only makes 206 test errors of
which 133 are on the 1010 threes in the test set.  Most of the errors are caused by the fact that the learned bias for
the 3 class is much too low. If this bias is increased by 3.5 (which optimizes overall performance on the test set), the
distilled model makes 109 errors of which 14 are on 3s.  So with the right bias, the distilled model gets 98.6\% of
the test 3s correct despite never having seen a 3 during training. If the transfer set contains {\it only} the 7s and 8s from
the training set, the distilled model makes 47.3\% test errors, but when the biases for 7 and 8 are reduced by 7.6 to
optimize test performance, this falls to 13.2\% test errors.



