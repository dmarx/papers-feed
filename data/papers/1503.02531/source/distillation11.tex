\section{Distillation}\label{sec:distillation}

Neural networks typically produce class probabilities by using a ``softmax'' output layer that converts the logit,
$z_i$, computed for each class into a probability, $q_i$, by comparing $z_i$ with the other logits.
\begin{equation}
q_i = \frac{exp(z_i/T)}{\sum_j exp(z_j/T)}
\end{equation}
where $T$ is a temperature that is normally set to $1$.  Using a higher value for $T$ produces a softer probability
distribution over classes.

In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer set
and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model
with a high temperature in its softmax.  The same high temperature is used when training the distilled model, but after
it has been trained it uses a temperature of 1.

When the correct labels are known for all or some of the transfer set,
this method can be significantly improved by also training the
distilled model to produce the correct labels. One way to do this is
to use the correct labels to modify the soft targets, but we found
that a better way is to simply use a weighted average of two different
objective functions.  The first objective function is the cross
entropy with the soft targets and this cross entropy is computed using
the same high temperature in the softmax of the distilled model as was
used for generating the soft targets from the cumbersome model. The
second objective function is the cross entropy with the correct
labels. This is computed using exactly the same logits in softmax of
the distilled model but at a temperature of 1.  We found that the best
results were generally obtained by using a condiderably lower weight
on the second objective function. Since the magnitudes of the
gradients produced by the soft targets scale as $1/T^2$ it is
important to multiply them by $T^2$ when using both hard and soft
targets. This ensures that the relative contributions of the hard and
soft targets remain roughly unchanged if the temperature used for
distillation is changed while experimenting with meta-parameters.

\subsection{Matching logits is a special case of distillation}

Each case in the transfer set contributes a cross-entropy gradient, $dC/dz_i$, with respect to each logit, $z_i$ of the
distilled model.  If the cumbersome model has logits $v_i$ which produce soft target probabilities $p_i$ and the transfer training is done at a temperature of $T$,
this gradient is given by:
\begin{equation}
\frac{\partial C}{\partial z_i} = \frac{1}{T}\left(q_i - p_i\right) = \frac{1}{T}\left(\frac{e^{z_i/T}}{\sum_j e^{z_j/T}} -\frac{e^{v_i/T}}{\sum_j e^{v_j/T}}\right)
\label{deriv} 
\end{equation}

If the temperature is high compared with the magnitude of the logits, we can approximate:
\begin{equation}
\frac{\partial C}{\partial z_i} \approx \frac{1}{T}\left( \frac{1+z_i/T}{N + \sum_j z_j/T} - \frac{1+v_i/T}{N + \sum_j v_j/T} \right)
\label{mess}
\end{equation}

If we now assume that the logits have been zero-meaned separately for each transfer case so that $\sum_j z_j = \sum_j v_j = 0$ Eq. \ref{mess} simplifies to:
\begin{equation}
\frac{\partial C}{\partial z_i} \approx \frac{1}{NT^2}\left( z_i - v_i \right)
\label{nice}
\end{equation}
So in the high temperature limit, distillation is equivalent to
minimizing ${1/2}(z_i-v_i)^2$, provided the logits are zero-meaned
separately for each transfer case. At lower temperatures, distillation
pays much less attention to matching logits that are much more
negative than the average. This is potentially advantageous because
these logits are almost completely unconstrained by the cost function
used for training the cumbersome model so they could be very noisy.
On the other hand, the very negative logits may convey useful
information about the knowledge acquired by the cumbersome
model. Which of these effects dominates is an empirical question. We
show that when the distilled model is much too small to capture all of
the knowledege in the cumbersome model, intermediate temperatures work
best which strongly suggests that ignoring the large negative logits
can be helpful.


