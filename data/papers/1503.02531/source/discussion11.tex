\section{Discussion}

We have shown that distilling works very well for transferring
knowledge from an ensemble or from a large highly regularized model
into a smaller, distilled model. On MNIST distillation works remarkably
well even when the transfer set that is used to train the distilled
model lacks any examples of one or more of the classes. For a deep
acoustic model that is version of the one used by Android voice search, we have shown
that nearly all of the improvement that is achieved by training an
ensemble of deep neural nets can be distilled into a single
neural net of the same size which is far easier to deploy.

For really big neural networks, it can be infeasible even to train a
full ensemble, but we have shown that the performance of a single really big
net that has been trained for a very long time can be significantly
improved by learning a large number of specialist nets, each of which
learns to discriminate between the classes in a highly confusable
cluster. We have not yet shown that we can distill the knowledge in
the specialists back into the single large net.  





