\section{Relationship to Mixtures of Experts}

The use of specialists that are trained on subsets of the data has
some resemblance to mixtures of experts \cite{Jacobs} which use a
gating network to compute the probability of assigning each example to
each expert. At the same time as the experts are learning to deal with
the examples assigned to them, the gating network is learning to
choose which experts to assign each example to based on the relative
discriminative performance of the experts for that example.  Using the discriminative
performance of the experts to determine the learned assignments is much
better than simply clustering the input vectors and assigning an
expert to each cluster, but it makes the training hard to parallelize: First, the
weighted training set for each expert keeps changing in a way that
depends on all the other experts and second, the gating network needs
to compare the performance of different experts on the same example to
know how to revise its assignment probabilities.  These difficulties
have meant that mixtures of experts are rarely used in the regime
where they might be most beneficial: tasks with huge datasets that
contain distinctly different subsets.

It is much easier to parallelize the training of multiple specialists.
We first train a generalist model and then use the confusion matrix to
define the subsets that the specialists are trained on.  Once these
subsets have been defined the specialists can be trained entirely
independently.  At test time we can use the predictions from the
generalist model to decide which specialists are relevant and only
these specialists need to be run.


