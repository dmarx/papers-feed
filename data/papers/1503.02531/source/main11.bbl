\begin{thebibliography}{1}

\bibitem{Caruana}
C.~Bucilu\v{a}, R.~Caruana, and A.~Niculescu-Mizil.
\newblock Model compression.
\newblock In {\em Proceedings of the 12th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, KDD '06, pages 535--541, New York,
  NY, USA, 2006. ACM.

\bibitem{brain-stuff}
J.~Dean, G.~S. Corrado, R.~Monga, K.~Chen, M.~Devin, Q.~V. Le, M.~Z. Mao,
  M.~Ranzato, A.~Senior, P.~Tucker, K.~Yang, and A.~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In {\em NIPS}, 2012.

\bibitem{Dietterich2000}
T.~G. Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In {\em Multiple classifier systems}, pages 1--15. Springer, 2000.

\bibitem{SPM}
G.~E. Hinton, L.~Deng, D.~Yu, G.~E. Dahl, A.~Mohamed, N.~Jaitly, A.~Senior,
  V.~Vanhoucke, P.~Nguyen, T.~N Sainath, and B.~Kingsbury.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock {\em Signal Processing Magazine, IEEE}, 29(6):82--97, 2012.

\bibitem{dropout}
G.~E. Hinton, N.~Srivastava, A.~Krizhevsky, I.~Sutskever, and R.~R.
  Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock {\em arXiv preprint arXiv:1207.0580}, 2012.

\bibitem{Jacobs}
R.~A. Jacobs, M.~I. Jordan, S.~J. Nowlan, and G.~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock {\em Neural computation}, 3(1):79--87, 1991.

\bibitem{Kriz}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1097--1105, 2012.

\bibitem{LiZhaoHuangGong}
J.~Li, R.~Zhao, J.~Huang, and Y.~Gong.
\newblock Learning small-size dnn with output-distribution-based criteria.
\newblock In {\em Proceedings Interspeech 2014}, pages 1910--1914, 2014.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.E. Hinton, A.~Krizhevsky, I.~Sutskever, and R.~R.
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em The Journal of Machine Learning Research}, 15(1):1929--1958,
  2014.

\end{thebibliography}
