\section{Soft Targets as Regularizers}

One of our main claims about using soft targets instead of hard targets is that a lot of helpful information can be
carried in soft targets that could not possibly be encoded with a single hard target. In this section we demonstrate
that this is a very large effect by using far less data to fit the 85M
parameters of the baseline speech
model described earlier.  Table~\ref{tab:small} shows that with only 3\% of the data (about 20M examples), training the baseline model with
hard targets leads to severe overfitting (we did early stopping, as the accuracy drops sharply after reaching 44.5\%),
whereas the same model trained with soft targets is able to recover almost all the information in the full training set
(about 2\% shy). It is even more remarkable to note that we did not have to do early stopping: the system with soft
targets simply ``converged'' to 57\%. This shows that soft targets are a very effective way of communicating the
regularities discovered by a model trained on all of the data to another model.

\begin{table}
\centering
\small
\begin{tabular}{|l|c|c|}
\hline
System \& training set & Train Frame Accuracy & Test Frame Accuracy \\
\hline
Baseline (100\% of training set) & 63.4\% & 58.9\% \\
Baseline (3\% of training set) & 67.3\%& 44.5\% \\
Soft Targets (3\% of training set) & 65.4\%& 57.0\%\\
\hline
\end{tabular}
\caption{Soft targets allow a new model to generalize well from only 3\% of the
  training set. The soft targets are obtained by training on the full training set.}\label{tab:small}
\end{table}


\subsection{Using soft targets to prevent specialists from
  overfitting}

The specialists that we used in our experiments on the JFT dataset
collapsed all of their non-specialist classes into a single dustbin
class. If we allow specialists to have a full softmax over all
classes, there may be a much better way to prevent them overfitting than using
early stopping. A specialist is trained on data that is highly
enriched in its special classes.  This means that the effective size
of its training set is much smaller and it has a strong tendency to
overfit on its special classes. This problem cannot be solved by
making the specialist a lot smaller because then we lose the very
helpful transfer effects we get from modeling all of the
non-specialist classes. 

Our experiment using 3\% of the speech data strongly suggests that if
a specialist is initialized with the weights of the generalist, we can
make it retain nearly all of its knowledge about the non-special
classes by training it with soft targets for the non-special classes
in addition to training it with hard targets. The soft targets can
be provided by the generalist.  We are currently exploring this
approach.






