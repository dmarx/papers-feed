- **Distillation Concept**: Transfer knowledge from a cumbersome model (ensemble or large model) to a smaller model for efficient deployment.
  
- **Soft Targets**: Use class probabilities from the cumbersome model as soft targets for training the distilled model, enhancing generalization.

- **Temperature in Softmax**: Adjust temperature \( T \) in softmax to produce softer probability distributions:
  \[
  q_i = \frac{e^{z_i/T}}{\sum_j e^{z_j/T}}
  \]

- **Training Objective**: Combine cross-entropy loss with soft targets and true labels:
  \[
  L = \alpha \cdot L_{soft} + (1 - \alpha) \cdot L_{hard}
  \]
  where \( L_{soft} \) is computed at temperature \( T \) and \( L_{hard} \) at \( T = 1 \).

- **Gradient Calculation**: Gradient for logits during training:
  \[
  \frac{\partial C}{\partial z_i} = \frac{1}{T} (q_i - p_i)
  \]

- **Logit Matching**: Distillation can be viewed as minimizing the squared difference between logits of the cumbersome and distilled models:
  \[
  \text{Minimize } \frac{1}{2} (z_i - v_i)^2
  \]

- **Empirical Findings**: 
  - Distillation improves performance on MNIST; smaller models can achieve lower error rates when trained with soft targets.
  - Optimal temperature range for distilled models varies; for example, 2.5 to 4 for models with fewer units.

- **Application in Speech Recognition**: Distillation enhances performance of DNN acoustic models in ASR, allowing a single model to outperform an ensemble of models.

- **DNN Training Objective**: For ASR, the model is trained to maximize the probability of the correct HMM state:
  \[
  \theta = \arg \max_{\theta'} P(h_t | s_t; \theta')
  \]

- **Diversity in Ensembles**: Random initialization and varied training data contribute to model diversity, improving ensemble performance.

- **Preliminary Results on MNIST**: 
  - Large model with dropout achieved 67 errors; distilled model with soft targets achieved 74 errors.
  - Distilled model can generalize well even with missing classes in the training set.

- **Specialist Models**: Introduce specialist models to distinguish fine-grained classes that full models confuse, trained rapidly and in parallel.

- **Regularization Techniques**: Use dropout and weight constraints to regularize large models, simulating an ensemble effect during training.