\begin{thebibliography}{110}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ailon et~al.(2021)Ailon, Leibovitch, and Nair]{ailon2021sparse}
Ailon, N., Leibovitch, O., and Nair, V.
\newblock Sparse linear networks with a fixed butterfly structure: theory and
  practice.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1174--1184.
  PMLR, 2021.

\bibitem[Akema et~al.(2020)Akema, Yamagishi, and Yamada]{akema2020approximate}
Akema, R., Yamagishi, M., and Yamada, I.
\newblock Approximate simultaneous diagonalization of matrices via structured
  low-rank approximation.
\newblock \emph{arXiv preprint arXiv:2010.06305}, 2020.

\bibitem[Bailey(1990)]{bailey1990ffts}
Bailey, D.~H.
\newblock {FFT}s in external or hierarchical memory.
\newblock \emph{The journal of Supercomputing}, 4\penalty0 (1):\penalty0
  23--35, 1990.

\bibitem[Bunse-Gerstner et~al.(1993)Bunse-Gerstner, Byers, and
  Mehrmann]{gerstner1993numerical}
Bunse-Gerstner, A., Byers, R., and Mehrmann, V.
\newblock Numerical methods for simultaneous diagonalization.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 1993.

\bibitem[Chaudhari et~al.(2020)Chaudhari, Sandino, Cole, Larson, Gold,
  Vasanawala, Lungren, Hargreaves, and Langlotz]{chaudhari2020prospective}
Chaudhari, A.~S., Sandino, C.~M., Cole, E.~K., Larson, D.~B., Gold, G.~E.,
  Vasanawala, S.~S., Lungren, M.~P., Hargreaves, B.~A., and Langlotz, C.~P.
\newblock Prospective deployment of deep learning in {MRI}: A framework for
  important considerations, challenges, and recommendations for best practices.
\newblock \emph{Journal of Magnetic Resonance Imaging}, 2020.

\bibitem[Chen et~al.(2022)Chen, Dao, Liang, Yang, Song, Rudra, and
  R{\'e}]{chen2021pixelated}
Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., and R{\'e}, C.
\newblock Pixelated butterfly: Simple and efficient sparse training for neural
  network models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2019)Choromanski, Rowland, Chen, and
  Weller]{choromanski2019unifying}
Choromanski, K., Rowland, M., Chen, W., and Weller, A.
\newblock Unifying orthogonal {M}onte {C}arlo methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1203--1212, 2019.

\bibitem[Cole et~al.(2020)Cole, Pauly, Vasanawala, and
  Ong]{cole2020unsupervised}
Cole, E.~K., Pauly, J.~M., Vasanawala, S.~S., and Ong, F.
\newblock Unsupervised {MRI} reconstruction with generative adversarial
  networks.
\newblock \emph{arXiv preprint arXiv:2008.13065}, 2020.

\bibitem[Conrad()]{Conrad_theminimal}
Conrad, K.
\newblock The minimal polynomial and some applications.

\bibitem[Cooley \& Tukey(1965)Cooley and Tukey]{cooley1965algorithm}
Cooley, J.~W. and Tukey, J.~W.
\newblock An algorithm for the machine calculation of complex fourier series.
\newblock \emph{Mathematics of computation}, 19\penalty0 (90):\penalty0
  297--301, 1965.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and R{\'e}]{dao2019learning}
Dao, T., Gu, A., Eichhorn, M., Rudra, A., and R{\'e}, C.
\newblock Learning fast algorithms for linear transforms using butterfly
  factorizations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Dao et~al.(2020)Dao, Sohoni, Gu, Eichhorn, Blonder, Leszczynski,
  Rudra, and R{\'e}]{dao2020kaleidoscope}
Dao, T., Sohoni, N., Gu, A., Eichhorn, M., Blonder, A., Leszczynski, M., Rudra,
  A., and R{\'e}, C.
\newblock Kaleidoscope: An efficient, learnable representation for all
  structured linear maps.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Darestani \& Heckel(2021)Darestani and
  Heckel]{darestani2021accelerated}
Darestani, M.~Z. and Heckel, R.
\newblock Accelerated {MRI} with un-trained neural networks.
\newblock \emph{IEEE Transactions on Computational Imaging}, 7:\penalty0
  724--733, 2021.

\bibitem[Darestani et~al.(2021)Darestani, Chaudhari, and
  Heckel]{darestani2021measuring}
Darestani, M.~Z., Chaudhari, A., and Heckel, R.
\newblock Measuring robustness in deep learning based compressive sensing.
\newblock \emph{arXiv preprint arXiv:2102.06103}, 2021.

\bibitem[De~Sa et~al.(2018)De~Sa, Gu, Puttagunta, R{\'e}, and
  Rudra]{desa2018two}
De~Sa, C., Gu, A., Puttagunta, R., R{\'e}, C., and Rudra, A.
\newblock A two-pronged progress in structured dense matrix vector
  multiplication.
\newblock In \emph{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pp.\  1060--1079. SIAM, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Desai et~al.(2021{\natexlab{a}})Desai, Gunel, Ozturkler, Beg,
  Vasanawala, Hargreaves, R{\'e}, Pauly, and Chaudhari]{desai2021vortex}
Desai, A.~D., Gunel, B., Ozturkler, B.~M., Beg, H., Vasanawala, S., Hargreaves,
  B.~A., R{\'e}, C., Pauly, J.~M., and Chaudhari, A.~S.
\newblock Vortex: Physics-driven data augmentations for consistency training
  for robust accelerated {MRI} reconstruction.
\newblock \emph{arXiv preprint arXiv:2111.02549}, 2021{\natexlab{a}}.

\bibitem[Desai et~al.(2021{\natexlab{b}})Desai, Ozturkler, Sandino, Vasanawala,
  Hargreaves, Re, Pauly, and Chaudhari]{desai2021noise2recon}
Desai, A.~D., Ozturkler, B.~M., Sandino, C.~M., Vasanawala, S., Hargreaves,
  B.~A., Re, C.~M., Pauly, J.~M., and Chaudhari, A.~S.
\newblock Noise2recon: A semi-supervised framework for joint {MRI}
  reconstruction and denoising.
\newblock \emph{arXiv preprint arXiv:2110.00075}, 2021{\natexlab{b}}.

\bibitem[Desai et~al.(2021{\natexlab{c}})Desai, Schmidt, Rubin, Sandino, Black,
  Mazzoli, Stevens, Boutin, Re, Gold, et~al.]{desai2021skm}
Desai, A.~D., Schmidt, A.~M., Rubin, E.~B., Sandino, C.~M., Black, M.~S.,
  Mazzoli, V., Stevens, K.~J., Boutin, R., Re, C., Gold, G.~E., et~al.
\newblock {SKM-TEA}: A dataset for accelerated {MRI} reconstruction with dense
  image labels for quantitative clinical evaluation.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}, 2021{\natexlab{c}}.

\bibitem[Dettmers \& Zettlemoyer(2019)Dettmers and
  Zettlemoyer]{dettmers2019sparse}
Dettmers, T. and Zettlemoyer, L.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock \emph{arXiv preprint arXiv:1907.04840}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Dong, X., Chen, S., and Pan, S.~J.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock \emph{arXiv preprint arXiv:1705.07565}, 2017.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Driscoll et~al.(1997)Driscoll, Healy~Jr, and
  Rockmore]{driscoll1997fast}
Driscoll, J.~R., Healy~Jr, D.~M., and Rockmore, D.~N.
\newblock Fast discrete polynomial transforms with applications to data
  analysis for distance transitive graphs.
\newblock \emph{SIAM Journal on Computing}, 26\penalty0 (4):\penalty0
  1066--1099, 1997.

\bibitem[Eckart \& Young(1936)Eckart and Young]{eckart1936approximation}
Eckart, C. and Young, G.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, 1936.

\bibitem[Eidelman \& Gohberg(1999)Eidelman and Gohberg]{eidelman1999new}
Eidelman, Y. and Gohberg, I.
\newblock On a new class of structured matrices.
\newblock \emph{Integral Equations and Operator Theory}, 34\penalty0
  (3):\penalty0 293--324, 1999.

\bibitem[Evci et~al.(2019)Evci, Pedregosa, Gomez, and
  Elsen]{evci2019difficulty}
Evci, U., Pedregosa, F., Gomez, A., and Elsen, E.
\newblock The difficulty of training sparse neural networks.
\newblock \emph{arXiv preprint arXiv:1906.10732}, 2019.

\bibitem[Fan et~al.(2020)Fan, Xu, Pathak, and Darve]{fan2020solving}
Fan, T., Xu, K., Pathak, J., and Darve, E.
\newblock Solving inverse problems in steady-state navier-stokes equations
  using deep neural networks.
\newblock \emph{arXiv preprint arXiv:2008.13074}, 2020.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2019stabilizing}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Stabilizing the lottery ticket hypothesis.
\newblock \emph{arXiv preprint arXiv:1903.01611}, 2019.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Frankle, J., Dziugaite, G.~K., Roy, D., and Carbin, M.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3259--3269. PMLR, 2020.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and
  Zou]{eval-harness}
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L.,
  Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E.,
  Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, September 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Geva et~al.(2020)Geva, Schuster, Berant, and
  Levy]{geva2020transformer}
Geva, M., Schuster, R., Berant, J., and Levy, O.
\newblock Transformer feed-forward layers are key-value memories.
\newblock \emph{arXiv preprint arXiv:2012.14913}, 2020.

\bibitem[Gokaslan et~al.(2019)Gokaslan, Cohen, Ellie, and
  Tellex]{Gokaslan2019OpenWeb}
Gokaslan, A., Cohen, V., Ellie, P., and Tellex, S.
\newblock Openwebtext corpus, 2019.

\bibitem[Gray(2006)]{gray2006toeplitz}
Gray, R.~M.
\newblock Toeplitz and circulant matrices: A review.
\newblock \emph{Foundations and Trends{\textregistered} in Communications and
  Information Theory}, 2\penalty0 (3):\penalty0 155--239, 2006.

\bibitem[Gray et~al.(2017)Gray, Radford, and Kingma]{gray2017gpu}
Gray, S., Radford, A., and Kingma, D.~P.
\newblock {GPU} kernels for block-sparse weights.
\newblock \emph{arXiv preprint arXiv:1711.09224}, 3, 2017.

\bibitem[Griswold et~al.(2002)Griswold, Jakob, Heidemann, Nittka, Jellus, Wang,
  Kiefer, and Haase]{griswold2002generalized}
Griswold, M.~A., Jakob, P.~M., Heidemann, R.~M., Nittka, M., Jellus, V., Wang,
  J., Kiefer, B., and Haase, A.
\newblock Generalized autocalibrating partially parallel acquisitions (grappa).
\newblock \emph{Magnetic Resonance in Medicine: An Official Journal of the
  International Society for Magnetic Resonance in Medicine}, 47\penalty0
  (6):\penalty0 1202--1210, 2002.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020hippo}
Gu, A., Dao, T., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2020.

\bibitem[Guo et~al.(2020)Guo, Hsueh, Leng, Qiu, Guan, Wang, Jia, Li, Guo, and
  Zhu]{guo2020accelerating}
Guo, C., Hsueh, B.~Y., Leng, J., Qiu, Y., Guan, Y., Wang, Z., Jia, X., Li, X.,
  Guo, M., and Zhu, Y.
\newblock Accelerating sparse dnn models without hardware-support via tile-wise
  sparsity.
\newblock In \emph{SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pp.\  1--15. IEEE, 2020.

\bibitem[Haldar(2013)]{haldar2013low}
Haldar, J.~P.
\newblock Low-rank modeling of local $ k $-space neighborhoods (loraks) for
  constrained {MRI}.
\newblock \emph{IEEE transactions on medical imaging}, 33\penalty0
  (3):\penalty0 668--681, 2013.

\bibitem[Hammernik et~al.(2018)Hammernik, Klatzer, Kobler, Recht, Sodickson,
  Pock, and Knoll]{hammernik2018learning}
Hammernik, K., Klatzer, T., Kobler, E., Recht, M.~P., Sodickson, D.~K., Pock,
  T., and Knoll, F.
\newblock Learning a variational network for reconstruction of accelerated
  {MRI} data.
\newblock \emph{Magnetic resonance in medicine}, 79\penalty0 (6):\penalty0
  3055--3071, 2018.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015{\natexlab{a}}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.~J.
\newblock Learning both weights and connections for efficient neural networks.
\newblock \emph{arXiv preprint arXiv:1506.02626}, 2015{\natexlab{b}}.

\bibitem[Han et~al.(2016)Han, Pool, Narang, Mao, Gong, Tang, Elsen, Vajda,
  Paluri, Tran, et~al.]{han2016dsd}
Han, S., Pool, J., Narang, S., Mao, H., Gong, E., Tang, S., Elsen, E., Vajda,
  P., Paluri, M., Tran, J., et~al.
\newblock Dsd: Dense-sparse-dense training for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1607.04381}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hooker(2020)]{hooker2020hardware}
Hooker, S.
\newblock The hardware lottery.
\newblock \emph{arXiv preprint arXiv:2009.06489}, 2020.

\bibitem[Hsieh(2003)]{hsieh2003computed}
Hsieh, J.
\newblock \emph{Computed tomography: principles, design, artifacts, and recent
  advances}, volume 114.
\newblock SPIE press, 2003.

\bibitem[Jayakumar et~al.(2021)Jayakumar, Pascanu, Rae, Osindero, and
  Elsen]{jayakumar2021top}
Jayakumar, S.~M., Pascanu, R., Rae, J.~W., Osindero, S., and Elsen, E.
\newblock Top-{KAST}: Top-{K} always sparse training.
\newblock \emph{arXiv preprint arXiv:2106.03517}, 2021.

\bibitem[Jolicoeur-Martineau et~al.(2021)Jolicoeur-Martineau, Li,
  Pich{\'e}-Taillefer, Kachman, and Mitliagkas]{jolicoeur2021gotta}
Jolicoeur-Martineau, A., Li, K., Pich{\'e}-Taillefer, R., Kachman, T., and
  Mitliagkas, I.
\newblock Gotta go fast when generating data with score-based models.
\newblock \emph{arXiv preprint arXiv:2105.14080}, 2021.

\bibitem[Jurafsky \& Martin(2014)Jurafsky and Martin]{jurafsky2014speech}
Jurafsky, D. and Martin, J.~H.
\newblock \emph{Speech and language processing}, volume~3.
\newblock Pearson London, 2014.

\bibitem[Kailath et~al.(1979)Kailath, Kung, and Morf]{kailath1979displacement}
Kailath, T., Kung, S.-Y., and Morf, M.
\newblock Displacement ranks of matrices and linear equations.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 68\penalty0
  (2):\penalty0 395--407, 1979.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Khalitov et~al.(2021)Khalitov, Yu, Cheng, and
  Yang]{khalitov2021sparse}
Khalitov, R., Yu, T., Cheng, L., and Yang, Z.
\newblock Sparse factorization of large square matrices.
\newblock \emph{arXiv preprint arXiv:2109.08184}, 2021.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and
  Lyons]{kidger2020neural}
Kidger, P., Morrill, J., Foster, J., and Lyons, T.
\newblock Neural controlled differential equations for irregular time series.
\newblock \emph{arXiv preprint arXiv:2005.08926}, 2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Knoll et~al.(2020)Knoll, Hammernik, Zhang, Moeller, Pock, Sodickson,
  and Akcakaya]{knoll2020deep}
Knoll, F., Hammernik, K., Zhang, C., Moeller, S., Pock, T., Sodickson, D.~K.,
  and Akcakaya, M.
\newblock Deep-learning methods for parallel magnetic resonance imaging
  reconstruction: A survey of the current approaches, trends, and issues.
\newblock \emph{IEEE signal processing magazine}, 37\penalty0 (1):\penalty0
  128--140, 2020.

\bibitem[Kochkov et~al.(2021)Kochkov, Smith, Alieva, Wang, Brenner, and
  Hoyer]{kochkov2021machine}
Kochkov, D., Smith, J.~A., Alieva, A., Wang, Q., Brenner, M.~P., and Hoyer, S.
\newblock Machine learning--accelerated computational fluid dynamics.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (21), 2021.

\bibitem[Lagunas et~al.(2021)Lagunas, Charlaix, Sanh, and
  Rush]{lagunas2021block}
Lagunas, F., Charlaix, E., Sanh, V., and Rush, A.~M.
\newblock Block pruning for faster transformers.
\newblock \emph{arXiv preprint arXiv:2109.04838}, 2021.

\bibitem[Lahiri et~al.(2021)Lahiri, Wang, Ravishankar, and
  Fessler]{lahiri2021blind}
Lahiri, A., Wang, G., Ravishankar, S., and Fessler, J.~A.
\newblock Blind primed supervised (blips) learning for mr image reconstruction.
\newblock \emph{arXiv preprint arXiv:2104.05028}, 2021.

\bibitem[Le et~al.(2013)Le, Sarl{\'o}s, and Smola]{le2013fastfood}
Le, Q., Sarl{\'o}s, T., and Smola, A.
\newblock Fastfood-computing hilbert space expansions in loglinear time.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  244--252, 2013.

\bibitem[Le~Magoarou \& Gribonval(2016)Le~Magoarou and
  Gribonval]{le2016flexible}
Le~Magoarou, L. and Gribonval, R.
\newblock Flexible multilayer sparse approximations of matrices and
  applications.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  10\penalty0 (4):\penalty0 688--700, 2016.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Li et~al.(2020)Li, Kovachki, Azizzadenesheli, Bhattacharya, Stuart,
  Anandkumar, et~al.]{li2020fourier}
Li, Z., Kovachki, N.~B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A.,
  Anandkumar, A., et~al.
\newblock Fourier neural operator for parametric partial differential
  equations.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lin et~al.(2017)Lin, Rao, Lu, and Zhou]{NIPS2017_a51fb975}
Lin, J., Rao, Y., Lu, J., and Zhou, J.
\newblock Runtime neural pruning.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Lin et~al.(2021)Lin, Ran, Chiu, Chesi, and Wong]{lin2021deformable}
Lin, R., Ran, J., Chiu, K.~H., Chesi, G., and Wong, N.
\newblock Deformable butterfly: A highly structured and sparse linear
  transform.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Liu \& Zenke(2020)Liu and Zenke]{liu2020finding}
Liu, T. and Zenke, F.
\newblock Finding trainable sparse networks through neural tangent transfer.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6336--6347. PMLR, 2020.

\bibitem[Lustig et~al.(2007)Lustig, Donoho, and Pauly]{lustig2007sparse}
Lustig, M., Donoho, D., and Pauly, J.~M.
\newblock Sparse {MRI}: The application of compressed sensing for rapid mr
  imaging.
\newblock \emph{Magnetic Resonance in Medicine: An Official Journal of the
  International Society for Magnetic Resonance in Medicine}, 58\penalty0
  (6):\penalty0 1182--1195, 2007.

\bibitem[Mardani et~al.(2018)Mardani, Gong, Cheng, Vasanawala, Zaharchuk, Xing,
  and Pauly]{mardani2018deep}
Mardani, M., Gong, E., Cheng, J.~Y., Vasanawala, S.~S., Zaharchuk, G., Xing,
  L., and Pauly, J.~M.
\newblock Deep generative adversarial neural networks for compressive sensing
  {MRI}.
\newblock \emph{IEEE transactions on medical imaging}, 38\penalty0
  (1):\penalty0 167--179, 2018.

\bibitem[Massaroli et~al.(2021)Massaroli, Poli, Sonoda, Suzuki, Park,
  Yamashita, and Asama]{massaroli2021differentiable}
Massaroli, S., Poli, M., Sonoda, S., Suzuki, T., Park, J., Yamashita, A., and
  Asama, H.
\newblock Differentiable multiple shooting layers.
\newblock \emph{arXiv preprint arXiv:2106.03885}, 2021.

\bibitem[Mattson et~al.(2020)Mattson, Cheng, Diamos, Coleman, Micikevicius,
  Patterson, Tang, Wei, Bailis, Bittorf, et~al.]{mattson2020mlperf}
Mattson, P., Cheng, C., Diamos, G., Coleman, C., Micikevicius, P., Patterson,
  D., Tang, H., Wei, G.-Y., Bailis, P., Bittorf, V., et~al.
\newblock Mlperf training benchmark.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  336--349, 2020.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Moczulski et~al.(2016)Moczulski, Denil, Appleyard, and
  de~Freitas]{moczulski2015acdc}
Moczulski, M., Denil, M., Appleyard, J., and de~Freitas, N.
\newblock {ACDC: a structured efficient linear layer}.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Morcos et~al.(2019)Morcos, Yu, Paganini, and Tian]{morcos2019one}
Morcos, A.~S., Yu, H., Paganini, M., and Tian, Y.
\newblock One ticket to win them all: generalizing lottery ticket
  initializations across datasets and optimizers.
\newblock \emph{arXiv preprint arXiv:1906.02773}, 2019.

\bibitem[Munkhoeva et~al.(2018)Munkhoeva, Kapushev, Burnaev, and
  Oseledets]{munkhoeva2018quadrature}
Munkhoeva, M., Kapushev, Y., Burnaev, E., and Oseledets, I.
\newblock Quadrature-based features for kernel approximation.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  9165--9174. Curran Associates,
  Inc., 2018.

\bibitem[Ong \& Lustig(2016)Ong and Lustig]{ong2016beyond}
Ong, F. and Lustig, M.
\newblock Beyond low rank+ sparse: Multiscale low rank matrix decomposition.
\newblock \emph{IEEE journal of selected topics in signal processing},
  10\penalty0 (4):\penalty0 672--687, 2016.

\bibitem[Orseau et~al.(2020)Orseau, Hutter, and
  Rivasplata]{orseau2020logarithmic}
Orseau, L., Hutter, M., and Rivasplata, O.
\newblock Logarithmic pruning is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Pan(2012)]{pan2012structured}
Pan, V.~Y.
\newblock \emph{Structured matrices and polynomials: unified superfast
  algorithms}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Parker(1995)]{parker1995random}
Parker, D.~S.
\newblock Random butterfly transformations with applications in computational
  linear algebra.
\newblock 1995.

\bibitem[Pensia et~al.(2020)Pensia, Rajput, Nagle, Vishwakarma, and
  Papailiopoulos]{pensia2020optimal}
Pensia, A., Rajput, S., Nagle, A., Vishwakarma, H., and Papailiopoulos, D.
\newblock Optimal lottery tickets via subsetsum: Logarithmic
  over-parameterization is sufficient.
\newblock \emph{arXiv preprint arXiv:2006.07990}, 2020.

\bibitem[Peste et~al.(2021)Peste, Iofinova, Vladu, and Alistarh]{peste2021ac}
Peste, A., Iofinova, E., Vladu, A., and Alistarh, D.
\newblock Ac/dc: Alternating compressed/decompressed training of deep neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Poli et~al.(2020)Poli, Massaroli, Yamashita, Asama, Park,
  et~al.]{poli2020hypersolvers}
Poli, M., Massaroli, S., Yamashita, A., Asama, H., Park, J., et~al.
\newblock Hypersolvers: Toward fast continuous-depth models.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Pruessmann et~al.(1999)Pruessmann, Weiger, Scheidegger, and
  Boesiger]{pruessmann1999sense}
Pruessmann, K.~P., Weiger, M., Scheidegger, M.~B., and Boesiger, P.
\newblock Sense: sensitivity encoding for fast {MRI}.
\newblock \emph{Magnetic Resonance in Medicine: An Official Journal of the
  International Society for Magnetic Resonance in Medicine}, 42\penalty0
  (5):\penalty0 952--962, 1999.

\bibitem[Rackauckas et~al.(2020)Rackauckas, Ma, Martensen, Warner, Zubov,
  Supekar, Skinner, Ramadhan, and Edelman]{rackauckas2020universal}
Rackauckas, C., Ma, Y., Martensen, J., Warner, C., Zubov, K., Supekar, R.,
  Skinner, D., Ramadhan, A., and Edelman, A.
\newblock Universal differential equations for scientific machine learning.
\newblock \emph{arXiv preprint arXiv:2001.04385}, 2020.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raissi et~al.(2019)Raissi, Perdikaris, and
  Karniadakis]{raissi2019physics}
Raissi, M., Perdikaris, P., and Karniadakis, G.~E.
\newblock Physics-informed neural networks: A deep learning framework for
  solving forward and inverse problems involving nonlinear partial differential
  equations.
\newblock \emph{Journal of Computational Physics}, 378:\penalty0 686--707,
  2019.

\bibitem[Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and
  He]{rasley2020deepspeed}
Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y.
\newblock Deepspeed: System optimizations enable training deep learning models
  with over 100 billion parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  3505--3506, 2020.

\bibitem[Ravishankar et~al.(2017)Ravishankar, Moore, Nadakuditi, and
  Fessler]{ravishankar2017low}
Ravishankar, S., Moore, B.~E., Nadakuditi, R.~R., and Fessler, J.~A.
\newblock Low-rank and adaptive sparse signal (lassi) models for highly
  accelerated dynamic imaging.
\newblock \emph{IEEE transactions on medical imaging}, 36\penalty0
  (5):\penalty0 1116--1128, 2017.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, pp.\  234--241. Springer, 2015.

\bibitem[Sandino et~al.(2020)Sandino, Cheng, Chen, Mardani, Pauly, and
  Vasanawala]{sandino2020compressed}
Sandino, C.~M., Cheng, J.~Y., Chen, F., Mardani, M., Pauly, J.~M., and
  Vasanawala, S.~S.
\newblock Compressed sensing: From research to clinical practice with deep
  neural networks: Shortening scan times for magnetic resonance imaging.
\newblock \emph{IEEE signal processing magazine}, 37\penalty0 (1):\penalty0
  117--127, 2020.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{sanh2020movement}
Sanh, V., Wolf, T., and Rush, A.~M.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock \emph{arXiv preprint arXiv:2005.07683}, 2020.

\bibitem[Sch{\"o}nemann(1966)]{schonemann1966generalized}
Sch{\"o}nemann, P.~H.
\newblock A generalized solution of the orthogonal procrustes problem.
\newblock \emph{Psychometrika}, 31\penalty0 (1):\penalty0 1--10, 1966.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-{LM}: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Sindhwani et~al.(2015)Sindhwani, Sainath, and
  Kumar]{sindhwani2015structured}
Sindhwani, V., Sainath, T., and Kumar, S.
\newblock Structured transforms for small-footprint deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3088--3096, 2015.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and
  Ganguli]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock \emph{arXiv preprint arXiv:2006.05467}, 2020.

\bibitem[Tewarson(1973)]{tewarson1973sparse}
Tewarson, R.~P.
\newblock \emph{Sparse matrices}, volume~69.
\newblock Academic Press New York, 1973.

\bibitem[Thomas et~al.(2018)Thomas, Gu, Dao, Rudra, and
  R{\'e}]{thomas2018learning}
Thomas, A., Gu, A., Dao, T., Rudra, A., and R{\'e}, C.
\newblock Learning compressed transforms with low displacement rank.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  9052--9060, 2018.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Keysers, Uszkoreit, Lucic, et~al.]{tolstikhin2021mlp}
Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner,
  T., Yung, J., Keysers, D., Uszkoreit, J., Lucic, M., et~al.
\newblock Mlp-{M}ixer: An all-mlp architecture for vision.
\newblock \emph{arXiv preprint arXiv:2105.01601}, 2021.

\bibitem[Trefethen(2000)]{trefethen2000spectral}
Trefethen, L.~N.
\newblock \emph{Spectral methods in MATLAB}.
\newblock SIAM, 2000.

\bibitem[Vahid et~al.(2020)Vahid, Prabhu, Farhadi, and
  Rastegari]{vahid2020butterfly}
Vahid, K.~A., Prabhu, A., Farhadi, A., and Rastegari, M.
\newblock Butterfly transform: An efficient fft based neural architecture
  design.
\newblock In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  12021--12030. IEEE, 2020.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Zhang, and
  Grosse]{wang2020picking}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock \emph{arXiv preprint arXiv:2002.07376}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Kashinath, Mustafa, Albert, and
  Yu]{wang2020towards}
Wang, R., Kashinath, K., Mustafa, M., Albert, A., and Yu, R.
\newblock Towards physics-informed deep learning for turbulent flow prediction.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  1457--1466, 2020{\natexlab{b}}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
  P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.~L., Gugger, S., Drame, M.,
  Lhoest, Q., and Rush, A.~M.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Yaman et~al.(2020)Yaman, Hosseini, Moeller, Ellermann, U{\u{g}}urbil,
  and Ak{\c{c}}akaya]{yaman2020self}
Yaman, B., Hosseini, S. A.~H., Moeller, S., Ellermann, J., U{\u{g}}urbil, K.,
  and Ak{\c{c}}akaya, M.
\newblock Self-supervised physics-based deep learning {MRI} reconstruction
  without fully-sampled data.
\newblock In \emph{2020 IEEE 17th International Symposium on Biomedical Imaging
  (ISBI)}, pp.\  921--925. IEEE, 2020.

\bibitem[Yu et~al.(2016)Yu, Suresh, Choromanski, Holtmann-Rice, and
  Kumar]{yu2016orthogonal}
Yu, F.~X., Suresh, A.~T., Choromanski, K.~M., Holtmann-Rice, D.~N., and Kumar,
  S.
\newblock Orthogonal random features.
\newblock In Lee, D.~D., Sugiyama, M., Luxburg, U.~V., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\
  1975--1983. Curran Associates, Inc., 2016.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Wang, Yu, Shi, Tay, Feng, and
  Yan]{yuan2021tokens}
Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F.~E., Feng, J., and Yan,
  S.
\newblock Tokens-to-token {V}i{T}: Training vision transformers from scratch on
  imagenet.
\newblock \emph{arXiv preprint arXiv:2101.11986}, 2021.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh]{zhao2021calibrate}
Zhao, T.~Z., Wallace, E., Feng, S., Klein, D., and Singh, S.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock \emph{arXiv preprint arXiv:2102.09690}, 2021.

\bibitem[Zhu \& Gupta(2017)Zhu and Gupta]{zhu2017prune}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{arXiv preprint arXiv:1710.01878}, 2017.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{zhu2015aligning}
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
  and Fidler, S.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  19--27, 2015.

\end{thebibliography}
