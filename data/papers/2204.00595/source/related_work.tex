\section{Related Work and Background}
\label{sec:related_work}

\subsection{Related Work}

\textbf{Sparse Training.}
Sparse training is an active research topic. There has been inspiring work along the line of compressing models such as neural network pruning and lottery tickets~\citep{han2015deep,han2015learning, frankle2018lottery}. Pruning methods usually eliminate neurons and connections through iterative retraining~\citep{han2015deep,han2015learning,sanh2020movement} or at runtime~\citep{NIPS2017_a51fb975,dong2017learning}. 
Although both Monarch and pruning methods aim to produce sparse models, we differ in our emphasis on \emph{overall} efficiency, whereas pruning mostly focuses on inference efficiency and disregards the cost of finding the smaller model. Lottery tickets \citep{frankle2018lottery,frankle2019stabilizing,frankle2020linear} are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks in convergence speed and potentially in generalization. Monarch can be roughly seen as a class of manually constructed lottery tickets.

\textbf{Structured Matrices.}
Structured matrices are those with subquadratic ($o(n^2)$ for dimension $n \times n$) number of parameters and runtime.
Examples include sparse and low-rank matrices, and fast transforms (Fourier, Chebyshev, sine/cosine, orthogonal polynomials).
They are commonly used to replace the dense weight matrices of deep learning models, thus reducing the number of parameters and training/inference FLOPs.
Large classes of structured matrices (e.g., Toeplitz-like~\citep{sindhwani2015structured}, low-displacement rank~\citep{kailath1979displacement}, quasi-separable~\citep{eidelman1999new}) have been shown to be able to represent many commonly used fast transforms.
For example, \citet{desa2018two} show that a simple divide-and-conquer scheme leads to a fast algorithm for a large class of structured matrices.
Our work builds on butterfly matrices~\citep{parker1995random, dao2019learning},
which have been shown to be expressive but remain hardware-inefficient.
Pixelated butterfly~\citep{chen2021pixelated} has attempted to make butterfly matrices more hardware-friendly, but at the cost of reduced expressiveness.
Furthermore, it is not known if one can directly decompose a dense pretrained model to a model with butterfly weight matrices without retraining.


\subsection{Butterfly Matrices}
\label{sec:butterfly}
Our work builds on recent work on \emph{butterfly matrices}. \citet{dao2019learning} introduced the notion of a {butterfly matrix} as a certain product of permuted block-diagonal matrices, inspired by the Cooley-Tukey fast Fourier transform algorithm~\citep{cooley1965algorithm}.
They encode the divide-and-conquer structure of many fast multiplication algorithms.
\citet{dao2020kaleidoscope} showed that all structured matrices can be written as products of such butterfly matrices, and this representation has optimal memory and runtime complexity up to polylogarithmic factors. We now review these definitions (following \citep{dao2020kaleidoscope}).

    A \textbf{butterfly factor} of size $k$ (where $k$ is even) is a matrix of the form
    \(
        \begin{bmatrix}
            \vD_1 & \vD_2 \\ \vD_3 & \vD_4
        \end{bmatrix}
    \)
    where each $\vD_i$ is a $\frac{k}{2} \times \frac{k}{2}$ diagonal matrix. We call this class of matrices $\BF\ind{k,k}$.

 A \textbf{butterfly factor matrix} of size $n$ and block size $k$ is a block diagonal matrix of $\frac{n}{k}$ butterfly factors of size $k$:
     \[
        \mathrm{diag}\left(\vB_1, \vB_2, \hdots, \vB_\frac{n}{k} \right),
     \]
      where $\vB_i \in \BF\ind{k,k}$. We call this class of matrices $\BF\ind{n,k}$.

    Finally, a \textbf{butterfly matrix} of size $n = 2^s$ is a matrix $\vM$ that can be expressed as a product of butterfly factor matrices:
    \[
        \vM = \vB_n \vB_{n/2} \hdots \vB_2,
    \]
    where each $\vB_i \in \BF\ind{n, i}$. We denote the set of size-$n$ butterfly matrices by $\B\ind{n}$.
    Equivalently, $\vM$ can be written in the following form:
    \[
         \vM = \vB_n \begin{bmatrix}
            \vM_1 & 0 \\
            0 & \vM_2
         \end{bmatrix},
    \]
    where $\vB_n \in \BF\ind{n,n}$ and $\vM_1, \vM_2 \in \B\ind{\frac{n}{2}}$.

    \citet{dao2020kaleidoscope} further introduce the \emph{kaleidoscope matrix hierarchy}: the class $\B\B^{*(n)}$ is the set of matrices of the form $\vM_1\vM_2^*$ for $\vM_1,\vM_2 \in \B\ind{n}$, and the class $(\B\B^{*(n)})^w_e$ is the set of all matrices of the form $\lt \prod\limits_{i=1}^{w} \vM_i \rt[1{:}n, 1{:}n]$ where each $\vM_i \in \B\B^{*(e\cdot n)}$. ($\vA^*$ denotes the conjugate transpose of $\vA$.)
    When the size $n$ is clear from context, we will omit the superscript $\ind{n}$ (i.e., just write $\B,\BBS$, etc.).
As shown by Theorem 1 of \citet{dao2020kaleidoscope}, the kaleidoscope hierarchy can represent any structured matrix with nearly-optimal parameters and runtime: if $\vM$ is an $n \times n$ matrix such that multiplying any vector $v$ by $\vM$ can be represented as a linear arithmetic circuit with depth $d$ and $s$ total gates, then $\vM \in (\B\B^{*(n)})^{O(d)}_{O(s/n)}$. \nimit{this is probably too much detail.}
    

