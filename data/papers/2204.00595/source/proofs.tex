\section{Theory}
\label{sec:proofs}

\subsection{Expressiveness of $\M$}
\label{subsec:expressiveness_proof}

\begin{proof}[Proof of \cref{thm:Monarch_expressiveness}]
  As~\citet[Appendix J]{dao2020kaleidoscope} show, the matrix class $\BBS$ can
  represent convolution, Hadamard transform, Toeplitz matrices, and AFDF.
  Since the Monarch class $\MMS$ contains the butterfly class $\BBS$ (which follows from \cref{thm:b_contained}), it follows
  that $\MMS$ can also represent those transforms / matrices.

  Note that the Hadamard transform is actually in $\B$ \citep{dao2020kaleidoscope}, so it is in $\M$ as well.

  \citet[Appendix J]{dao2020kaleidoscope} also show that the matrix class $(\BBS)^2$ can
  represent the Fourier, discrete sine/cosine transforms, the $(HD)^3$ class,
  Fastfood, and ACDC matrices.
  By the same argument, as the Monarch class $(\MMS)^2$ contains the butterfly
  class $(\BBS)^2$, $(\MMS)^2$ can thus also represent these transforms / matrices.
\end{proof}

\subsection{Projection onto $\M$}

In \cref{alg:project}, we provide pseudocode for the algorithm outlined in
\cref{subsec:projection}.
We now prove \cref{thm:Monarch_projection}.
Note that the rectangular matrix case generalizes naturally from the square matrix case, by replacing square blocks with rectangular blocks.

\begin{proof}[Proof of \cref{thm:Monarch_projection}]
  As shown in \cref{subsec:projection}, after reshaping the Monarch matrix $\vM$ as a 4D tensor $M_{\ell  jki}$ and writing the two block-diagonal matrices $\vL$ and $\vR$ as 3D tensors $L_{j \ell  k}$ and $R_{k j i}$, we obtain:
  \begin{equation*}
    M_{\ell  j k i} = L_{j \ell  k} R_{k j i}, \quad \text{for } \ell, j, k, i = 1, \dots, m.
  \end{equation*}
  We can similarly reshape the given matrix $A$ into a 4D tensor $A_{\ell j k i}$ with size $m \times m \times m \times m$.

  Since the squared Frobenius norm objective $\norm{A - M}_F^2$ (\cref{eq:projection_objective}) only depends on the entries of $A$ and $M$ and not their shape,
  we can rewrite the objective after reshaping:
  \begin{align*}
    \norm{A - M}_F^2
    &= \sum_{\ell  j k i} \left(A_{\ell  j k i} - M_{\ell  j k i}\right)^2 \\
    &= \sum_{\ell  j k i} \left( A_{\ell  j k i} - L_{j \ell  k} R_{k j i} \right)^2 \\
    &= \sum_{j k} \sum_{\ell i} \left( A_{\ell  j k i} - L_{j \ell  k} R_{k j i} \right)^2.
  \end{align*}
  We see that the objective decomposes into $m \times m$ independent terms (indexed by $j$ and $k$).
  For each value of $j$ and $k$, the objective is exactly the rank-1 approximation objective for the corresponding slice $\vA_{:, j, k, :}$.

  Let $\vu_{jk} \vv_{jk}^\top$ be the best rank-1 approximation of $\vA_{:, j, k, :}$ (which we can compute using the SVD, by the Eckart--Young theorem~\citep{eckart1936approximation} for Frobenius norm).
  Let $\vR$ be the 3D tensor of size $m \times m \times m$ where $\vR_{kji} = (\vv_{jk})_i$, and let $\vL$ be the 3D tensor of size $m \times m \times m$ where $\vL_{j \ell k} = (\vu_{jk})_\ell$.
  Then each of the terms in the objective is minimized, and thus the overall objective is minimized.

  We see that the algorithm requires $m \cdot m$ SVD's, each of size $m \times m$.
  Each SVD takes $O(m^3)$ time~\citep{trefethen2000spectral}, so the overall time complexity is $O(m^5) = O(n^{5/2})$.
\end{proof}


\subsection{Monarch Factorizations for Matrices in $\MMS$}
In this section, we describe the algorithm for factorizing matrices in $\MMS$ previously outlined in \cref{subsec:recovery} (\cref{alg:mm_recovery}). Again, \cref{alg:mm_recovery} handles the general case where the block sizes of $L$ and $R$ can be different. We then prove \cref{thm:Monarch_recovery_full}, which has \cref{thm:Monarch_recovery} as an immediate corollary.


Our goal is thus to compute the matrices $\vL_1,\vR,\vL_2$ in the factorization of $\vM$.
In order to compute this factorization, we require the following assumption on $\vM$:
\begin{assumption}
\label{assump:a1_appendix}
Assume that (1) $\vM \in \M\M^{*(b,n)}$ is invertible and (2) $\vM$ can be written as $(\vP_{(b,n)}^\top \vL_1 \vP_{(b,n)}) \vR (\vP_{(b,n)}^\top\vL_2\vP_{(b,n)})$, where $\vL_1,\vL_2 \in \BD\ind{\frac{n}{b},n},\vR \in \BD\ind{b,n}$, and $\vR$ has no nonzero entries in its diagonal blocks.
(Note that by \cref{prop:mm-eqv-def}, we can write any $\vM \in \M\M^{*(b,n)}$ as $(\vP_{(b,n)}^\top \vL_1 \vP_{(b,n)}) \vR (\vP_{(b,n)}^\top\vL_2\vP_{(b,n)})$; thus, (2) is merely the assumption that $\vR$ has no zero entries in its blocks.)
\end{assumption}%

This is analogous to \cref{assump:a1}, except applicable to the more general block size $b$.
We now present \cref{alg:mm_recovery} to find factors $\vL_1,\vR,\vL_2$ of matrices satisfying \cref{assump:a1_appendix}.

First, observe that if we define $\MM = \vP_{(b,n)} \vM \vP_{(b,n)}^\top$, we have $\MM = \vL_1 (\vP_{(b,n)} \vR \vP_{(b,n)}^\top) \vL_2$. By \cref{thm:lr_permutation}, the matrix $\vP_{(b,n)} \vR \vP_{(b,n)}^\top$ is in $\DB\ind{\frac{n}{b}, n}$, i.e., is a block matrix with blocks of size $\frac{n}{b} \times \frac{n}{b}$ where each block is a diagonal matrix.
Thus, we can write:

\begin{equation*}
\lt \begin{array}{cccc} \MM_{11} & \MM_{12} & \dots & \MM_{1b} \\ \MM_{21} & \MM_{22} & \dots & \MM_{2b} \\ \ddots & \ddots & \ddots & \ddots  \\ \MM_{b1} & \MM_{b2} & \dots & \MM_{bb} \end{array}\rt
= \lt \begin{array}{ccccc} \vA_1 \\ & \vA_2 \\ & & \ddots \\ & & & \vA_{b} \end{array}\rt
\lt \begin{array}{cccc} \vD_{11} & \vD_{12} & \dots & \vD_{1b} \\ \vD_{21} & \vD_{22} & \dots & \vD_{2b} \\ \ddots & \ddots & \ddots & \ddots  \\ \vD_{b1} & \vD_{b2} & \dots & \vD_{bb} \end{array}\rt
\lt \begin{array}{ccccc} \vC_1 \\ & \vC_2 \\ & & \ddots \\ & & & \vC_{b} \end{array}\rt,
\end{equation*}

where $\vA_1,\dots,\vA_b$ are $\frac{n}{b} \times \frac{n}{b}$ matrices that are the diagonal blocks of $\vL_1$; $\vC_1,\dots,\vC_b$ are $\frac{n}{b} \times \frac{n}{b}$ matrices that are the diagonal blocks of $\vL_2$; $\vD_{11},\dots,\vD_{1b},\vD_{21},\dots,\vD_{2b},\dots,\vD_{b1},\dots,\vD_{bb}$ are $\frac{n}{b} \times \frac{n}{b}$ \emph{diagonal} matrices that are the blocks of $\vP_{(b,n)} \vR \vP_{(b,n)}^\top$; and
$\MM_{11},\dots,\MM_{1b},\MM_{21},\dots,\MM_{2b},\dots,\MM_{b1},\dots,\MM_{bb}$ are $\frac{n}{b} \times \frac{n}{b}$ matrices that are the blocks of $\MM = \vP_{(b,n)} \vM \vP_{(b,n)}^\top$.

Thus, we have the set of matrix equations $\vA_i \vD_{ij} \vC_j = \MM_{ij}$, for $1 \le i, j \le b$. Notice that the assumption that the $\vR$ has no nonzero entries in its blocks (\cref{assump:a1_appendix}) is equivalent to assuming that none of the diagonal entries of any matrix $\vD_{ij}$ is equal to zero. Also, the assumption that $\vM$ is invertible implies that $\vL_1, \vL_2$ are invertible (since the product of square singular matrices is singular), which in turn implies that each block matrix $\vA_i$ and each block matrix $\vC_j$ is invertible (since a square block-diagonal matrix where one of the blocks is singular is itself singular). Taken together, this means that each matrix $\MM_{ij}$ is invertible, since $\MM_{ij} = \vA_i \vD_{ij} \vC_j$ and each of the matrices on the RHS of the equation is invertible.

Observe that given a solution to the set of equations $\vA_i \vD_{ij} \vC_j = \MM_{ij}$, if we rescale and permute the matrices $\vA_i, \vD_{ij}, \vC_j$ appropriately, the result is still a solution to the equations. Specifically, let $\vP$ be any permutation matrix and $\{\vS_i\}_{i=1}^b, \{\vS_j'\}_{j=1}^b$ be any invertible diagonal matrices (i.e., diagonal matrices without any zeros on the diagonal). 
Define $\vD_{ij}' = \vS_i \vP^\top {\vD}_{ij} \vP \vS_j'$ for all $i, j$. Notice that $\vP^\top \vD_{ij} \vP = \vP^{-1} \vD_{ij} \vP$ is diagonal because $\vD_{ij}$ is diagonal. Thus, $\vD_{ij}'$ is diagonal (and invertible) since the product of diagonal matrices is diagonal. Define $\vA_i' = \vA_i \vP \vS_i^{-1}$ and $\vC_j' = \vP^\top \vS_j'^{-1} \vC_j$ for all $i, j$.
Thus, we have that $\MM_{ij} = \vA_i \vD_{ij} \vC_j = (\vA_i \vP \vS_i^{-1} ) \vD_{ij}' (\vP^\top \vS_j'^{-1} \vC_j) = {\vA}_i' \vD_{ij}' \vC_j'$ for all $i, j$: in other words, we can scale the $\vA_i$'s on the right by any invertible diagonal matrix, the $\vC_j$'s on the left by any invertible diagonal matrix, and apply a matching permutation to the rows of the $\vC_j$'s and the columns of the $\vA_i$'s, and apply matching transformations to the $\vD_{ij}$'s and the result will still be a valid factorization. This implies that as long as we recover a ``correct'' $\hat{\vC}_1$ up to a permutation and scaling of its rows, we can set the $\hat{\vD}_{i1}$'s and $\hat{\vD}_{1j}$'s to the identity matrix, and then compute the remaining $\hat{\vA}_i$'s and $\hat{\vC}_j$'s via the equations $\hat{\vA}_i = \MM_{i1}\hat{\vC}_1^{-1}$ and $\hat{\vC}_j = \hat{\vA}_1^{-1}\MM_{1j}$.


To understand how we can compute such a matrix $\hat{\vC}_1$, define $\vF(i, j) = \MM_{i1}^{-1} \MM_{ij} \MM_{1j}^{-1} \MM_{11}$ and observe that
\begin{align*}
\vF(i, j) &= \MM_{i1}^{-1} \MM_{ij} \MM_{1j}^{-1} \MM_{11} \\ &=
(\vC_1^{-1} \vD_{i1}^{-1} \vA_i^{-1}) (\vA_i \vD_{ij} \vC_j) (\vC_j^{-1} \vD_{1j}^{-1} \vA_1^{-1}) (\vA_1 \vD_{11} \vC_1) \\
&= \vC_1^{-1} (\vD_{i1}^{-1} \vD_{ij} \vD_{1j}^{-1} \vD_{11}) \vC_1
\end{align*} for all $1 \le i, j \le b$.
Note that $\vD_{i1}^{-1} \vD_{ij} \vD_{1j}^{-1} \vD_{11}$ is a diagonal matrix;
thus, $\vC_1 \vF(i, j) \vC_1^{-1}$ is diagonal for all $i, j$, i.e., 
$\vC_1$ simultaneously diagonalizes all the matrices $\vF(i, j)$.
(Note: In this paper, we say that a matrix $\vQ$ ``simultaneously diagonalizes'' a set of matrices $\vG_1, \dots, \vG_k$ if $\vQ \vG_i \vQ^{-1}$ is a diagonal matrix for all $1 \le i \le k$. Note that sometimes the opposite convention [i.e., $\vQ^{-1} \vG_i \vQ$ must be diagonal] is used in the literature; we adopt the former for notational convenience.)
Indeed, if \emph{any} matrix simultaneously diagonalizes all these matrices, then it leads to a valid factorization, which we show in the proof of \cref{thm:Monarch_recovery_full}. Therefore, we compute some matrix that simultaneously diagonalizes all these matrices, and set $\hat{\vC}_1$ to that matrix.

These ideas form the basis of \cref{alg:mm_recovery}, which is presented formally below. \cref{alg:mm_recovery} uses simultaneous diagonalization as a subroutine; we discuss how to solve simultaneous diagonalization problems below.

\begin{algorithm}[H]
\caption{$\MMS$ Factorization}
\label{alg:mm_recovery}
\begin{algorithmic}[1]
\REQUIRE Block size $b$; matrix $\vM \in \M\M^{*(b,n)}$ satisfying \cref{assump:a1_appendix}
\item Define $\MM_{ij}$ (of size $\frac{n}{b} \times \frac{n}{b}$) as the $i,j$ block of $\vP_{(b,n)} \vM \vP_{(b,n)}^\top$
\FOR{$1 \le i, j \le b$}
\item Compute $\vF(i,j) := \MM_{i1}^{-1}\MM_{ij}\MM_{1j}^{-1}\MM_{11}$
\ENDFOR
\item $\hat{\vC}_1 \leftarrow \ \textsc{SIMULTANEOUS\_DIAG}\lt \{\vF(i,j)\}_{i,j=1,1}^{b,b}\rt $
\FOR{$1 \le i \le b$}
\item $\hat{\vA}_i \leftarrow \MM_{i1} \hat{\vC}_1^{-1}$
\ENDFOR
\FOR{$2 \le j \le b$}
\item $\hat{\vC}_j \leftarrow \hat{\vA}_1^{-1} \MM_{1j}$
\ENDFOR
\FOR{$1 \le i, j \le b$}
\item $\hat{\vD}_{ij} \leftarrow \hat{\vA}_i^{-1} \MM_{ij}\hat{\vC}_j^{-1}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{theorem}\label{thm:Monarch_recovery_full}
Given an $n \times n$ matrix $\vM \in \M\M^{*(b,n)}$ satisfying Assumption \ref{assump:a1}, \cref{alg:mm_recovery} finds its Monarch factors $\vL_1, \vR, \vL_2$ in  time $O\lt \frac{n^3}{b} \rt$.
\end{theorem}

Notice that by setting $b = \sqrt{n}$, we immediately recover \cref{thm:Monarch_recovery}.
Note also that by \cref{prop:mstarm}, \cref{thm:Monarch_recovery_full} implies that given an $\vM \in \M^*\M^{(\frac{n}{b},n)}$, we can find its Monarch factorization in time $O(\frac{n^3}{b})$ as well (e.g., simply permute it to a matrix in $\M\M^{*(b,n)}$ and then run \cref{alg:mm_recovery}). 
We now prove \cref{thm:Monarch_recovery_full}.

\begin{proof}
We first show that the factorization returned by \cref{alg:mm_recovery} is valid, which reduces to showing that (1) $\MM_{ij} = \hat{\vA}_i \hat{\vD}_{ij} \hat{\vC}_j$ and (2) $\hat{\vD}_{ij}$ is diagonal, for all $1 \le i, j \le b$ as argued above.

As argued above, since $\MM$ satisfies \cref{assump:a1_appendix}, then there exists a matrix ($\vC_1$) that simultaneously diagonalizes all the $\vF(i,j)$'s. Thus, we can always compute some matrix that simultaneously diagonalizes these matrices (i.e., line 2 of \cref{alg:mm_recovery} will always return a valid solution); we discuss how to actually do this below. By definition of simultaneous diagonalization, this matrix (which we set $\hat{\vC}_1$ to) is invertible.

So, $\hat{\vA}_i = \MM_{i1}\hat{\vC}_1^{-1}$ is invertible for all $i$. Thus $\hat{\vC}_j = \hat{\vA}_1^{-1} \MM_{1j}$ is invertible for all $j$ as well. (Note that the equation $\hat{\vC}_j = \hat{\vA}_1^{-1} \MM_{1j}$ holds by construction of $\hat{\vC}_j$ for $j \ge 2$, and by construction of $\hat{\vA}_1$ when $j = 1$.) As $\hat{\vD}_{ij} = \hat{\vA}_i^{-1} \MM_{ij}\hat{\vC}_j^{-1}$ by definition, we thus have that $\MM_{ij} = \hat{\vA}_i \hat{\vD}_{ij} \hat{\vC}_j$ for all $i, j$.

It remains to show that $\hat{\vD}_{ij}$ is diagonal.
\begin{align*}
\hat{\vD}_{ij} &= \hat{\vA}_i^{-1} \MM_{ij}\hat{\vC}_j^{-1} \\
&= (\MM_{i1} \hat{\vC}_1^{-1})^{-1} \MM_{ij} (\hat{\vA}_1^{-1} \MM_{1j})^{-1} \\
&= \hat{\vC}_1 \MM_{i1}^{-1} \MM_{ij} \MM_{1j}^{-1} \hat{\vA}_1  \\
&= \hat{\vC}_1 (\MM_{i1}^{-1} \MM_{ij} \MM_{1j}^{-1} \MM_{11}) \hat{\vC}_1^{-1} \\
&= \hat{\vC}_1 \vF(i, j) \hat{\vC}_1^{-1}%
\end{align*}

But $\hat{\vC}_1 \vF(i, j) \hat{\vC}_1^{-1}$ is diagonal for all $i,j$ by \emph{definition} of $\hat{\vC}_1$ as a matrix that simultaneously diagonalizes the $\vF(i,j)$'s.

As for $\vL_1,\vR,\vL_2$, recall that we can simply set $\vL_1 = \diag(\hat{\vA}_1, \dots, \hat{\vA}_b)$, $\vL_2 = \diag(\hat{\vC}_1, \dots, \hat{\vC}_b)$, and $\vR = \vP_{(b,n)}^\top \lt \begin{array}{cccc} \hat{\vD}_{11} & \hat{\vD}_{12} & \dots & \hat{\vD}_{1b} \\ \hat{\vD}_{21} & \hat{\vD}_{22} & \dots & \hat{\vD}_{2b} \\ \ddots & \ddots & \ddots & \ddots  \\ \hat{\vD}_{b1} & \hat{\vD}_{b2} & \dots & \hat{\vD}_{bb} \end{array}\rt
\vP_{(b,n)}$, and we have $\vM = (\vP_{(b,n)}^\top \vL_1 \vP_{(b,n)}) \vR (\vP_{(b,n)}^\top\vL_2\vP_{(b,n)})$ with $\vL_1, \vL_2 \in \BD\ind{\frac{n}{b}, n}$ and $\vR \in \BD\ind{b,n}$ as argued above. This completes the proof of correctness.


Now, we analyze the runtime. There are $b^2$ matrices $\F(i,j)$ to compute, and computing each one takes $O(\frac{n^3}{b^3})$ time. Once we've found $\hat{\vC}_1$, there are $b$ matrices $\hat{\vA}_i$ to compute, each one taking $O(\frac{n^3}{b^3})$ time, and $b-1$ matrices $\hat{\vC}_j$ (for $j \ge 2$) to compute, each one taking $O(\frac{n^3}{b^3})$ time, and then $b^2$ matrices $\hat{\vD}_{ij}$ to compute, each taking $O(\frac{n^3}{b^3})$ time. (Note that we can compute each of these faster using fast matrix multiplication / inversion; however, it turns out not to matter as the simultaneous diagonalization is the bottleneck.)

Finally, we analyze the simultaneous diagonalization runtime. Simultaneous diagonalization of a set of matrices $\{\vG_1, \dots, \vG_k\}$ is equivalent to finding a mutual eigenbasis for the matrices, since if $\vD_i$ is a diagonal matrix and $\vQ \vG_i \vQ^{-1} = \vD_i$, then the $j^{th}$ column of $\vQ$ is an eigenvector of $\vG_i$ with eigenvalue equal to the $j^{th}$ entry of $\vD_i$.

A simple algorithm for simultaneous diagonalizing a set of matrices, assuming that they are in fact simultaneously diagonalizable (which implies that each matrix is individually diagonalizable), is as follows (e.g. see \cite{Conrad_theminimal, gerstner1993numerical}): first, set $i = 1$ and diagonalize the first matrix $\vG_i = \vG_1$ (i.e., find an eigenbasis), and set $\vQ$ to be the diagonalizing matrix (i.e., the matrix of eigenvectors). So, $\vQ \vG_1 \vQ^{-1}$ is diagonal.
By the assumption that the matrices are in fact simultaneously diagonalizable, $\vQ \vG_j \vQ^{-1}$ will be permuted block diagonal for all $j \ne i$ as well: the size of each block corresponds to the multiplicity of the corresponding eigenvalue of $\vG_1$. (Note that if $\vG_1$'s has unique eigenvalues, then the eigenbasis is unique (up to permutation and nonzero scaling), and thus in this case $\vG_1$ uniquely determines the simultaneously diagonalizing matrix, up to arbitrary permutation and nonzero scaling of the rows. In other words, the block size will be 1 in this case, meaning that $\vQ \vG_j \vQ^{-1}$ will be diagonal for all $j$, and we are done.)

So now, we repeat the following for all $i$ up to $k$. Increment $i$ and compute $\vQ \vG_i \vQ^{-1}$. If it is already diagonal, move on. Otherwise, first permute $\vQ \leftarrow \vP \vQ \vP^\top$ so that it is block diagonal (observe that this maintains the property that $\vQ \vG_j \vQ^{-1}$ is diagonal for all $j < i$, since $\vP \vD \vP^\top$ is diagonal for any permutation $\vP$ and diagonal matrix $\vD$). Then for each block of size $> 1$, compute a matrix that diagonalizes that block; denoting the number of blocks (including size-1 blocks) by $b$, let $\vQ_1', \dots, \vQ_b'$ denote the corresponding diagonalizing transformations, or the scalar 1 when the block is of size 1. Finally set $\vQ' \leftarrow \diag(\vQ_1', \dots, \vQ_b') $ and $\vQ \leftarrow \vQ'^{-1} \vQ \vQ'$. By construction, $\vQ \vG_i \vQ^{-1}$ will now be diagonal; also, $\vQ \vG_j \vQ^{-1}$ is still diagonal for all $j < i$, because any linear combination of a set of eigenvectors of a diagonalizable matrix corresponding to a repeated eigenvalue $\lambda$ is itself an eigenvector of that matrix with eigenvalue $\lambda$.

Thus, once we've processed all $k$ of the $\vG_i$'s, $\vQ$ is a matrix that simultaneously diagonalizes all of them. At each step $i$, we compute diagonalizing transformations for square block matrices whose sizes $s_1, \dots, s_k$ sum to $n$. As eigendecomposition (for a fixed desired precision) takes $O(n^3)$ time for an $n \times n$ matrix, this means the total runtime of step $i$ is $O\lt \sum_{j=1}^{k} s_i^3 \rt \le O(n^3)$. Thus the total runtime of the entire simultaneous diagonalization procedure is $O(kn^3)$, where $k$ is the number of matrices. (Note that iterative methods for simultaneous diagonalization also exist \citep{gerstner1993numerical,akema2020approximate} and could be used to speed up this step in practice.)

Applying this to our problem, we have $b^2$ matrices to simultaneously diagonalize, each of size $\frac{n}{b} \times \frac{n}{b}$. This leads to a total runtime of $O\lt b^2 \cdot (\frac{n}{b})^3\rt = O\lt \frac{n^3}{b}\rt$ for the entire simultaneous diagonalization procedure, and thus the runtime of \cref{alg:mm_recovery} is also $O\lt \frac{n^3}{b}\rt$, as desired.

(Note: As can be seen from the above analysis, we don't actually need $\vM$ itself to be invertible---we simply need all its blocks $\MM_{ij}$ to be, so that all the $\vA_i$'s and $\vC_j$'s are, which is a weaker assumption that invertibility of $\vM$ given that we already assumed the $\vD_{ij}$'s are invertible due to the nonzero assumption on the blocks of $\vR$.)


\end{proof}



























