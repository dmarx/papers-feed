\documentclass[nohyperref]{article}

\usepackage{etoolbox}
\newtoggle{arxiv}
\toggletrue{arxiv}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{xfrac}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\iftoggle{arxiv}{
  \usepackage[numbers]{natbib}
}
{
\usepackage{icml2022}
}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{relsize}
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{subfigure}

\usepackage[capitalize]{cleveref}

\usepackage[inline]{enumitem}


\usepackage[textsize=tiny]{todonotes}

\input{math_commands}

\newtoggle{comment}
\toggletrue{comment}
\togglefalse{comment} %

\iftoggle{comment}{
\newcommand{\Beidi}[1]{{\color{orange} [Beidi: {#1}]}}
\newcommand{\Tri}[1]{{\color{cyan} [Tri: {#1}]}}
\newcommand{\nimit}[1]{{\color{red} [Nimit: {#1}]}}
\newcommand{\micp}[1]{{\color{blue!70} [Michael: {#1}]}}
\newcommand{\arjun}[1]{{\color{green} [Arjun: {#1}]}}
}{
\newcommand{\Beidi}[1]{}
\newcommand{\Tri}[1]{}
\newcommand{\nimit}[1]{}
\newcommand{\micp}[1]{}
\newcommand{\arjun}[1]{}
}

\iftoggle{arxiv}{
  \setlength{\textwidth}{6.5in}
  \setlength{\textheight}{9in}
  \setlength{\oddsidemargin}{0in}
  \setlength{\evensidemargin}{0in}
  \setlength{\topmargin}{-0.5in}
  \newlength{\defbaselineskip}
  \setlength{\defbaselineskip}{\baselineskip}
  \setlength{\marginparwidth}{0.8in}
}{
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*1.0}{*0}
\titlespacing{\subsection}{0pt}{*0}{*0}
\titlespacing{\subsubsection}{0pt}{*0}{*0}

\usepackage[subtle, mathdisplays=tight, charwidths=tight, leading=normal]{savetrees}


\def\setstretch#1{\renewcommand{\baselinestretch}{#1}}
\setstretch{0.99}
\addtolength{\parskip}{-0.3pt}
}


\iftoggle{arxiv}{
\title{Monarch: Expressive Structured Matrices for Efficient and Accurate Training}
\usepackage{authblk}
\author[1]{Tri Dao}
\author[1]{Beidi Chen}
\author[1]{Nimit Sohoni}
\author[1]{Arjun Desai}
\author[1]{Michael Poli}
\author[2]{Jessica Grogan}
\author[3]{Alexander Liu}
\author[3]{Aniruddh Rao}
\author[2]{Atri Rudra}
\author[1]{Christopher R{\'e}}
\affil[1]{Stanford University}
\affil[2]{University at Buffalo, SUNY}
\affil[2]{University of Michigan}
\affil[ ]{\texttt{\{trid,beidic,nims,arjundd,poli\}@stanford.edu}, \texttt{\{jrgrogan,atri\}@buffalo.edu}, \texttt{\{avliu,anrao\}@umich.edu}, \texttt{chrismre@cs.stanford.edu}}
}{
\icmltitlerunning{Monarch}
}

\begin{document}


\iftoggle{arxiv}{
  \maketitle
}{
\twocolumn[
\icmltitle{Monarch: Expressive Structured Matrices for Efficient and Accurate Training}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %
}

\begin{abstract}
  Large neural networks excel in many domains, but they are expensive to train and fine-tune.
  A popular approach to reduce their compute/memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform).
  These methods have not seen widespread adoption (1) in end-to-end training due to
  unfavorable efficiency--quality tradeoffs, and
  (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to
  approximate a given dense weight matrix.
  To address these issues, we propose a class of matrices (Monarch) that is \emph{hardware-efficient} (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and \emph{expressive} (they can represent many commonly used transforms).
  Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution.
  These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models.
  We empirically validate that Monarch can achieve favorable accuracyâ€“efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2$\times$ with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40\%.
  In sparse-to-dense training, with a simple technique called ``reverse sparsification,'' Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2$\times$ without quality drop.
  The same technique brings 23\% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record.
  In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7$\times$ with comparable accuracy.
\end{abstract}

\input{intro}

\input{related_work}

\input{theory}

\input{method}

\input{experiments}

\input{conclusion}

\section*{Acknowledgments}

We thank Laurel Orr, Xun Huang, Trevor Gale, Jian Zhang, Victor Bittorf, Sarah Hooper, Neel Guha, and Michael Zhang for their helpful discussions and feedback on early drafts of the paper.

We gratefully acknowledge the support of NIH under No.\ U54EB020405 (Mobilize), NSF under Nos.\ CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No.\ W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No.\ N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program,  the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S.\ Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S.\ Government.

\bibliography{ref}
\bibliographystyle{icml2022}


\newpage
\appendix
\onecolumn

\input{extended_related_work}
\input{notation}

\input{permutations_new}

\input{proofs}

\input{experiment_details}

\end{document}
