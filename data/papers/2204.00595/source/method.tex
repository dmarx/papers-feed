\section{Using Monarch Matrices in Model Training}
\label{sec:method}

We can use our class of Monarch matrices to parameterize weight
matrices of deep learning models in several settings.

\begin{itemize}[leftmargin=*,nosep,nolistsep]
  \item In the \textbf{E2E sparse training} setting, we replace the dense weight
  matrices of a baseline model with Monarch matrices with the same dimension,
  initialize them randomly, and train as usual.
  Most of our baseline models are Transformers, and we replace the projection
  matrices in the attention blocks, along with the weights of the feed-forward
  network (FFN) blocks, with Monarch matrices.
  The Monarch parameterization is differentiable, and we rely on
  autodifferentiation to train with first-order methods such as Adam~\citep{kingma2014adam}.

  \item In the \textbf{S2D training} setting, we first replace the dense weight
  matrices of a baseline model with Monarch matrices, then train the sparse
  model for about 90\% of the usual number of iterations.
  We then convert the Monarch matrices to dense matrices (by simply multiplying
  the factors $L$ and $R$ along with permutations), and continue training for
  the remaining 10\% of the iterations.
  Compared to dense end-to-end training, we train for the same number of
  iterations, but the first 90\% of the iterations are faster due to the
  hardware efficiency of Monarch matrices.

  \item In the \textbf{D2S fine-tuning} setting, we start with a dense
  pretrained model (e.g., BERT), and project the dense weight matrices (e.g., in
  the attention blocks and FFN blocks) on the set of Monarch matrices using the
  algorithm in \cref{subsec:projection}.
  We then fine-tune the resulting model on downstream tasks (e.g., GLUE), using
  first-order methods.
\end{itemize}
We typically set the number of blocks in the block-diagonal matrices to be between 2 and 4 based on the parameter budgets (25\% -- 50\% of the dense model).

