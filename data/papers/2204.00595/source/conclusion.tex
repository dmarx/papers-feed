\section{Conclusion}
\label{sec:conclusion}
We propose Monarch, a novel matrix parameterization that inherits the expressiveness of butterfly matrices and thus can represent many fast transforms.
Our parameterization leverages optimized batch matrix multiply routines on GPUs, yielding up to 2$\times$ speedup compared to dense matrix multiply.
We derive an efficient algorithm for projecting an arbitrary dense matrix on the set of Monarch factors.
Our algorithm allows us to easily fine-tune a pretrained model into a model with Monarch weight matrices.
As a result, Monarch matrices unlock new ways for faster end-to-end training, sparse-to-dense training, and dense-to-sparse fine-tuning of large neural networks.
By making structured matrices practical, our work is a first step towards unlocking tremendous performance improvements in applying sparse models to wide-ranging ML applications (including science and medicine).
We anticipate this work can inspire more future work on advancing machine learning models for interdisciplinary research with limited computational resources.
