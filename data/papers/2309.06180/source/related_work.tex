\section{Related Work}
\label{sec:related_work}

\heading{General model serving systems.}
Model serving has been an active area of research in recent years, with numerous systems proposed to tackle diverse aspects of deep learning model deployment.
Clipper~\cite{crankshaw2017clipper}, TensorFlow Serving~\cite{olston2017tensorflow}, Nexus~\cite{shen2019nexus}, InferLine~\cite{crankshaw2020inferline}, and
Clockwork~\cite{gujarati2020serving} are some earlier general model serving systems. They study batching, caching, placement, and scheduling for serving single or multiple models.
More recently, DVABatch~\cite{cui2022dvabatch} introduces multi-entry multi-exit batching. REEF~\cite{han2022microsecond} and  Shepherd~\cite{zhang2023shepherd} propose preemption for serving. AlpaServe~\cite{li2023alpaserve} utilizes model parallelism for statistical multiplexing.
However, these general systems fail to take into account the auto-regressive property and token state of LLM inference, resulting in missed opportunities for optimization.

\heading{Specialized serving systems for transformers.}
Due to the significance of the transformer architecture, numerous specialized serving systems for it have been developed. These systems utilize GPU kernel optimizations~\cite{wang2021lightseq,aminabadi2022deepspeed,nvidiaft,ma2020rammer},
advanced batching mechanisms~\cite{fang2021turbotransformers,yu2022orca},
model parallelism~\cite{pope2022efficiently,yu2022orca,aminabadi2022deepspeed}, and parameter sharing~\cite{zhou2022pets} for efficient serving.
Among them, Orca~\cite{yu2022orca} is most relevant to our approach.

\heading{Comparison to Orca.}
The iteration-level scheduling in Orca~\cite{yu2022orca} and \tech in \sys are complementary techniques: While both systems aim to increase the GPU utilization and hence the throughput of LLM serving, Orca achieves it by scheduling and interleaving the requests so that more requests can be processed in parallel, while \sys is doing so by increasing memory utilization so that the working sets of more requests fit into memory.
By reducing memory fragmentation and enabling sharing, \sys runs more requests in a batch in parallel and achieves a 2-4$\times$ speedup compared to Orca.
Indeed, the fine-grained scheduling and interleaving of the requests like in Orca makes memory management more challenging, making the techniques proposed in \sys even more crucial.

\heading{Memory optimizations.}
The widening gap between the compute capability and memory capacity of accelerators has caused memory to become a bottleneck for both training and inference.
Swapping~\cite{huang2020swapadvisor,wang2018superneurons,ren2021zero}, recomputation~\cite{chen2016training,jain2020checkmate} and their combination~\cite{patil2022poet} have been utilized to reduce the peak memory of training.
Notably, FlexGen~\cite{sheng2023high} studies how to swap weights and token states for LLM inference with limited GPU memory, but it does not target the online serving settings.
OLLA~\cite{steiner2022olla} optimizes the lifetime and location of tensors to reduce fragmentation, but it does not do fine-grained block-level management or online serving.
FlashAttention~\cite{dao2022flashattention} applies tiling and kernel optimizations to reduce the peak memory of attention computation and reduce I/O costs.
This paper introduces a new idea of block-level memory management in the context of online serving.
