@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)},
  pages={561--577},
  year={2018}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}



@misc{openaiapi-pricing, url={https://openai.com/pricing}, journal={OpenAI API Pricing}, author={OpenAI}, year={2023}} 

@misc{chat-cost, url={https://www.reuters.com/technology/tech-giants-ai-like-bing-bard-poses-billion-dollar-search-problem-2023-02-22/}, author={Reuters}, year={2023}} 

@misc{openaiapi, url={https://openai.com/blog/openai-api}, journal={OpenAI API}, author={OpenAI}, year={2020}} 

@misc{chatgpt, url={https://openai.com/blog/chatgpt}, journal={OpenAI ChatGPT}, author={OpenAI}, year={2022}} 

@misc{chatgptuserprompt, url={https://openai.com/blog/custom-instructions-for-chatgpt}, journal={Custom instructions for ChatGPT}, author={OpenAI}, year={2023}} 

@misc{copilot, url={https://github.com/features/copilot}, journal={Github Copilot}, author={Github}, year={2022}}

@misc{bard, url={https://bard.google.com/}, journal={Google Bard}, author={Google}, year={2023}} 

@misc{amazonbedrock, url={https://aws.amazon.com/bedrock/}, journal={Amazon Bedrock}, author={Amazon Web Services}, year={2023}} 

@article{wiseman2016sequence,
  title={Sequence-to-sequence learning as beam-search optimization},
  author={Wiseman, Sam and Rush, Alexander M},
  journal={arXiv preprint arXiv:1606.02960},
  year={2016}
}

@inproceedings{yu2022orca,
  title={Orca: A Distributed Serving System for $\{$Transformer-Based$\}$ Generative Models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{kilburn1962one,
  title={One-level storage system},
  author={Kilburn, Tom and Edwards, David BG and Lanigan, Michael J and Sumner, Frank H},
  journal={IRE Transactions on Electronic Computers},
  number={2},
  pages={223--235},
  year={1962},
  publisher={IEEE}
}

@article{li2023alpaserve,
  title={AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
  author={Li, Zhuohan and Zheng, Lianmin and Zhong, Yinmin and Liu, Vincent and Sheng, Ying and Jin, Xin and Huang, Yanping and Chen, Zhifeng and Zhang, Hao and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2302.11665},
  year={2023}
}

@inproceedings{crankshaw2017clipper,
  title={Clipper: A Low-Latency Online Prediction Serving System},
  author={Crankshaw, Daniel and Wang, Xin and Zhou, Guilio and Franklin, Michael J and Gonzalez, Joseph E and Stoica, Ion},
  booktitle={14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)},
  pages={613--627},
  year={2017}
}

@inproceedings{crankshaw2020inferline,
  title={InferLine: latency-aware provisioning and scaling for prediction serving pipelines},
  author={Crankshaw, Daniel and Sela, Gur-Eyal and Mo, Xiangxi and Zumar, Corey and Stoica, Ion and Gonzalez, Joseph and Tumanov, Alexey},
  booktitle={Proceedings of the 11th ACM Symposium on Cloud Computing},
  pages={477--491},
  year={2020}
}

@inproceedings{cui2022dvabatch,
  title={DVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs},
  author={Cui, Weihao and Zhao, Han and Chen, Quan and Wei, Hao and Li, Zirui and Zeng, Deze and Li, Chao and Guo, Minyi},
  booktitle={2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages={183--198},
  year={2022}
}

@inproceedings{zhou2022pets,
  title={PetS: A Unified Framework for Parameter-Efficient Transformers Serving},
  author={Zhou, Zhe and Wei, Xuechao and Zhang, Jiejing and Sun, Guangyu},
  booktitle={2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages={489--504},
  year={2022}
}

@inproceedings{fang2021turbotransformers,
  title={TurboTransformers: an efficient GPU serving system for transformer models},
  author={Fang, Jiarui and Yu, Yang and Zhao, Chengduo and Zhou, Jie},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={389--402},
  year={2021}
}

@article{olston2017tensorflow,
  title={Tensorflow-serving: Flexible, high-performance ml serving},
  author={Olston, Christopher and Fiedel, Noah and Gorovoy, Kiril and Harmsen, Jeremiah and Lao, Li and Li, Fangwei and Rajashekhar, Vinu and Ramesh, Sukriti and Soyke, Jordan},
  journal={arXiv preprint arXiv:1712.06139},
  year={2017}
}

@misc{nvidiatriton,
  author="NVIDIA",
  title={Triton Inference Server},
  howpublished={\url{https://developer.nvidia.com/nvidia-triton-inference-server}}
}

@inproceedings{shen2019nexus,
  title={Nexus: A GPU cluster engine for accelerating DNN-based video analysis},
  author={Shen, Haichen and Chen, Lequn and Jin, Yuchen and Zhao, Liangyu and Kong, Bingyu and Philipose, Matthai and Krishnamurthy, Arvind and Sundaram, Ravi},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={322--337},
  year={2019}
}

@inproceedings{gujarati2020serving,
  title={Serving $\{$DNNs$\}$ like Clockwork: Performance Predictability from the Bottom Up},
  author={Gujarati, Arpan and Karimi, Reza and Alzayat, Safya and Hao, Wei and Kaufmann, Antoine and Vigfusson, Ymir and Mace, Jonathan},
  booktitle={14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  pages={443--462},
  year={2020}
}

@inproceedings{wang2021lightseq,
  title={LightSeq: A High Performance Inference Library for Transformers},
  author={Wang, Xiaohui and Xiong, Ying and Wei, Yang and Wang, Mingxuan and Li, Lei},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers},
  pages={113--120},
  year={2021}
}

@article{pope2022efficiently,
  title={Efficiently Scaling Transformer Inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={arXiv preprint arXiv:2211.05102},
  year={2022}
}

@article{aminabadi2022deepspeed,
  title={DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Zhang, Minjia and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Rasley, Jeff and Smith, Shaden and Ruwase, Olatunji and others},
  journal={arXiv preprint arXiv:2207.00032},
  year={2022}
}

@article{sheng2023high,
  title={High-throughput Generative Inference of Large Language Models with a Single GPU},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2303.06865},
  year={2023}
}

@article{frantar2023massive,
  title={Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2301.00774},
  year={2023}
}

@inproceedings{dettmers2022gptint,
    title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
    author={Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
}

@article{xiao2022smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2022}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@inproceedings{yao2022zeroquant,
    title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
    author={Zhewei Yao and Reza Yazdani Aminabadi and Minjia Zhang and Xiaoxia Wu and Conglong Li and Yuxiong He},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
}


@article{kwon2022alphatuning,
  title={AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models},
  author={Kwon, Se Jung and Kim, Jeonghoon and Bae, Jeongin and Yoo, Kang Min and Kim, Jin-Hwa and Park, Baeseong and Kim, Byeongwook and Ha, Jung-Woo and Sung, Nako and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2210.03858},
  year={2022}
}


@article{park2022nuqmm,
  title={nuqmm: Quantized matmul for efficient inference of large-scale generative language models},
  author={Park, Gunho and Park, Baeseong and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2206.09557},
  year={2022}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@inproceedings{ren2021zero,
  title={ZeRO-Offload: Democratizing Billion-Scale Model Training.},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={USENIX Annual Technical Conference},
  pages={551--564},
  year={2021}
}

@inproceedings{patil2022poet,
  title={POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging},
  author={Patil, Shishir G and Jain, Paras and Dutta, Prabal and Stoica, Ion and Gonzalez, Joseph},
  booktitle={International Conference on Machine Learning},
  pages={17573--17583},
  year={2022},
  organization={PMLR}
}

@inproceedings{han2022microsecond,
  title={Microsecond-scale Preemption for Concurrent $\{$GPU-accelerated$\}$$\{$DNN$\}$ Inferences},
  author={Han, Mingcong and Zhang, Hanze and Chen, Rong and Chen, Haibo},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={539--558},
  year={2022}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{jiao2023chatgpt,
  title={Is ChatGPT a good translator? A preliminary study},
  author={Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and Wang, Xing and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2301.08745},
  year={2023}
}

@inproceedings{zheng2022alpa,
  title={Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={559--578},
  year={2022}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@inproceedings{huang2020swapadvisor,
  title={Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping},
  author={Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={1341--1355},
  year={2020}
}

@inproceedings{wang2018superneurons,
  title={Superneurons: Dynamic GPU memory management for training deep neural networks},
  author={Wang, Linnan and Ye, Jinmian and Zhao, Yiyang and Wu, Wei and Li, Ang and Song, Shuaiwen Leon and Xu, Zenglin and Kraska, Tim},
  booktitle={Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming},
  pages={41--53},
  year={2018}
}

@misc{nvidiaft,
  author={NVIDIA},
  title={FasterTransformer},
  howpublished={\url{https://github.com/NVIDIA/FasterTransformer}},
  year={2023}
}

@misc{lmsysweek8,
    author={LMSYS ORG},
    title={Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B},
    howpublished={https://lmsys.org/blog/2023-06-22-leaderboard/},
    year={2023},
}

@misc{nccl,
  author={NVIDIA},
  title={NCCL: The NVIDIA Collective Communication Library},
  howpublished={\url{https://developer.nvidia.com/nccl}},
  year={2023}
}

@misc{fastapi,
  author="FastAPI",
  title={FastAPI},
  howpublished={\url{https://github.com/tiangolo/fastapi}},
  year={2023}
}


@misc{fairseq,
  author="Meta Inc.",
  title={FasterTransformer},
  howpublished={\url{https://github.com/facebookresearch/fairseq}},
}



@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{jain2020checkmate,
  title={Checkmate: Breaking the memory wall with optimal tensor rematerialization},
  author={Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={497--511},
  year={2020}
}

@article{steiner2022olla,
  title={OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks},
  author={Steiner, Benoit and Elhoushi, Mostafa and Kahn, Jacob and Hegarty, James},
  doi = {10.48550/arXiv.2210.12924},
  year={2022}
}

@article{rabe2021self,
  title={Self-attention Does Not Need $ O (n\^{} 2) $ Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
}

@inproceedings{zhang2023shepherd,
author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica},
title = {{SHEPHERD}: Serving {DNNs} in the Wild},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {787--808},
url = {https://www.usenix.org/conference/nsdi23/presentation/zhang-hong},
publisher = {USENIX Association},
month = apr,
}

@inproceedings{krizhevsky2012imagenet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}


@misc{sharegpt,
url={https://sharegpt.com/},
author={ShareGPT Team},
year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@inproceedings{wang2022pacman,
  title={Pacman: An Efficient Compaction Approach for $\{$Log-Structured$\}$$\{$Key-Value$\}$ Store on Persistent Memory},
  author={Wang, Jing and Lu, Youyou and Wang, Qing and Xie, Minhui and Huang, Keji and Shu, Jiwu},
  booktitle={2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages={773--788},
  year={2022}
}

@InProceedings{bojar-EtAl:2016:WMT1,
  author    = {Bojar, Ond
{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and  Jimeno Yepes, Antonio  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Neveol, Aurelie  and  Neves, Mariana  and  Popel, Martin  and  Post, Matt  and  Rubino, Raphael  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco  and  Verspoor, Karin  and  Zampieri, Marcos},
  title     = {Findings of the 2016 Conference on Machine Translation},
  booktitle = {Proceedings of the First Conference on Machine Translation},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {131--198},
  url       = {http://www.aclweb.org/anthology/W/W16/W16-2301}
}

@inproceedings{ma2020rammer,
  title={Rammer: Enabling holistic deep learning compiler optimizations with rtasks},
  author={Ma, Lingxiao and Xie, Zhiqiang and Yang, Zhi and Xue, Jilong and Miao, Youshan and Cui, Wei and Hu, Wenxiang and Yang, Fan and Zhang, Lintao and Zhou, Lidong},
  booktitle={Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
  pages={881--897},
  year={2020}
}

@inproceedings{gao2018low,
  title={Low latency rnn inference with cellular batching},
  author={Gao, Pin and Yu, Lingfan and Wu, Yongwei and Li, Jinyang},
  booktitle={Proceedings of the Thirteenth EuroSys Conference},
  pages={1--15},
  year={2018}
}

@article{gholami2021ai,
  title={Ai and memory wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Mahoney, Michael W and Keutzer, Kurt},
  journal={RiseLab Medium Post},
  volume={1},
  pages={6},
  year={2021}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}