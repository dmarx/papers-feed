\section{Discussion}

\heading{Applying the virtual memory and paging technique to other GPU workloads.}
The idea of virtual memory and paging is effective for managing the KV cache in LLM serving because the workload requires dynamic memory allocation (since the output length is not known a priori) and its performance is bound by the GPU memory capacity.
However, this does not generally hold for every GPU workload.
For example, in DNN training, the tensor shapes are typically static, and thus memory allocation can be optimized ahead of time.
For another example, in serving DNNs that are not LLMs, an increase in memory efficiency may not result in any performance improvement since the performance is primarily compute-bound.
In such scenarios, introducing the \sys's techniques may rather degrade the performance due to the extra overhead of memory indirection and non-contiguous block memory.
However, we would be excited to see \sys's techniques being applied to other workloads with similar properties to LLM serving.

\heading{LLM-specific optimizations in applying virtual memory and paging.}
\sys re-interprets and augments the idea of virtual memory and paging by leveraging the application-specific semantics.
One example is \sys's all-or-nothing swap-out policy, which exploits the fact that processing a request requires all of its corresponding token states to be stored in GPU memory.
Another example is the recomputation method to recover the evicted blocks, which is not feasible in OS.
Besides, \sys mitigates the overhead of memory indirection in paging by fusing the GPU kernels for memory access operations with those for other operations such as attention.
