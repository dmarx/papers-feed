\section{Conclusion}
This paper proposes \tech, a new attention algorithm that allows attention keys and values to be stored in non-contiguous paged memory, and presents \sys, a high-throughput LLM serving system with efficient memory management enabled by \tech.
Inspired by operating systems, we demonstrate how established techniques, such as virtual memory and copy-on-write, can be adapted to efficiently manage KV cache and handle various decoding algorithms in LLM serving.
Our experiments show that \sys achieves 2-4$\times$ throughput improvements over the state-of-the-art systems.
