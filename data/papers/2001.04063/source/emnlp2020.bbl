\begin{thebibliography}{41}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Cao et~al.(2018)Cao, Li, Li, and Wei}]{cao2018retrieve}
Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2018.
\newblock Retrieve, rerank and rewrite: Soft template based neural
  summarization.
\newblock In \emph{ACL}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and
  Toutanova}]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}.

\bibitem[{Dong et~al.(2019)Dong, Yang, Wang, Wei, Liu, Wang, Gao, Zhou, and
  Hon}]{dong2019unified}
Li~Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu~Wang, Jianfeng Gao,
  Ming Zhou, and Hsiao-Wuen Hon. 2019.
\newblock Unified language model pre-training for natural language
  understanding and generation.
\newblock In \emph{NeurIPS}.

\bibitem[{Du and Cardie(2018)}]{du2018harvesting}
Xinya Du and Claire Cardie. 2018.
\newblock Harvesting paragraph-level question-answer pairs from wikipedia.
\newblock In \emph{ACL}.

\bibitem[{Du et~al.(2017)Du, Shao, and Cardie}]{du2017learning}
Xinya Du, Junru Shao, and Claire Cardie. 2017.
\newblock Learning to ask: Neural question generation for reading
  comprehension.
\newblock \emph{arXiv preprint arXiv:1705.00106}.

\bibitem[{Edunov et~al.(2019)Edunov, Baevski, and Auli}]{edunov2019pre}
Sergey Edunov, Alexei Baevski, and Michael Auli. 2019.
\newblock Pre-trained language model representations for language generation.
\newblock \emph{arXiv preprint arXiv:1903.09722}.

\bibitem[{Fan et~al.(2017)Fan, Grangier, and Auli}]{fan2017controllable}
Angela Fan, David Grangier, and Michael Auli. 2017.
\newblock Controllable abstractive summarization.
\newblock \emph{arXiv preprint arXiv:1711.05217}.

\bibitem[{Gehrmann et~al.(2018)Gehrmann, Deng, and Rush}]{gehrmann2018bottom}
Sebastian Gehrmann, Yuntian Deng, and Alexander~M Rush. 2018.
\newblock Bottom-up abstractive summarization.
\newblock In \emph{EMNLP}.

\bibitem[{Gulcehre et~al.(2017)Gulcehre, Dutil, Trischler, and
  Bengio}]{gulcehre2017plan}
Caglar Gulcehre, Francis Dutil, Adam Trischler, and Yoshua Bengio. 2017.
\newblock Plan, attend, generate: Planning for sequence-to-sequence models.
\newblock In \emph{NIPS}.

\bibitem[{Joshi et~al.(2019)Joshi, Chen, Liu, Weld, Zettlemoyer, and
  Levy}]{joshi2019spanbert}
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel~S Weld, Luke Zettlemoyer, and Omer
  Levy. 2019.
\newblock Spanbert: Improving pre-training by representing and predicting
  spans.
\newblock \emph{arXiv preprint arXiv:1907.10529}.

\bibitem[{Kingma and Ba(2015)}]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba. 2015.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}.

\bibitem[{Klein et~al.(2017)Klein, Kim, Deng, Senellart, and
  Rush}]{klein2017opennmt}
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander~M Rush.
  2017.
\newblock Opennmt: Open-source toolkit for neural machine translation.
\newblock In \emph{ACL}.

\bibitem[{Krueger et~al.(2016)Krueger, Maharaj, Kram{\'a}r, Pezeshki, Ballas,
  Ke, Goyal, Bengio, Courville, and Pal}]{krueger2016zoneout}
David Krueger, Tegan Maharaj, J{\'a}nos Kram{\'a}r, Mohammad Pezeshki, Nicolas
  Ballas, Nan~Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and
  Chris Pal. 2016.
\newblock Zoneout: Regularizing rnns by randomly preserving hidden activations.
\newblock \emph{arXiv preprint arXiv:1606.01305}.

\bibitem[{Lawrence et~al.(2019)Lawrence, Kotnis, and
  Niepert}]{lawrence2019attending}
Carolin Lawrence, Bhushan Kotnis, and Mathias Niepert. 2019.
\newblock Attending to future tokens for bidirectional sequence generation.
\newblock \emph{arXiv preprint arXiv:1908.05915}.

\bibitem[{Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}.

\bibitem[{Li et~al.(2017)Li, Monroe, and Jurafsky}]{li2017learning}
Jiwei Li, Will Monroe, and Dan Jurafsky. 2017.
\newblock Learning to decode for future success.
\newblock \emph{arXiv preprint arXiv:1701.06549}.

\bibitem[{Lin(2004)}]{lin2004rouge}
Chin-Yew Lin. 2004.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}.

\bibitem[{Liu and Lapata(2019)}]{liu2019text}
Yang Liu and Mirella Lapata. 2019.
\newblock Text summarization with pretrained encoders.
\newblock \emph{arXiv preprint arXiv:1908.08345}.

\bibitem[{Merity et~al.(2017)Merity, Keskar, and
  Socher}]{merity2017regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher. 2017.
\newblock Regularizing and optimizing lstm language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}.

\bibitem[{Nallapati et~al.(2017)Nallapati, Zhai, and
  Zhou}]{nallapati2017summarunner}
Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
\newblock Summarunner: A recurrent neural network based sequence model for
  extractive summarization of documents.
\newblock In \emph{AAAI}.

\bibitem[{Nallapati et~al.(2016)Nallapati, Zhou, Gulcehre, Xiang
  et~al.}]{nallapati2016abstractive}
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et~al. 2016.
\newblock Abstractive text summarization using sequence-to-sequence rnns and
  beyond.
\newblock \emph{arXiv preprint arXiv:1602.06023}.

\bibitem[{Oord et~al.(2018)Oord, Li, and Vinyals}]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals. 2018.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}.

\bibitem[{Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio}]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{ICML}.

\bibitem[{Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer}]{peters2018deep}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer. 2018.
\newblock Deep contextualized word representations.
\newblock In \emph{NAACL}.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever}]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{URL https://s3-us-west-2. amazonaws.
  com/openai-assets/research-covers/languageunsupervised/language understanding
  paper. pdf}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1(8).

\bibitem[{Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2019.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In \emph{EMNLP}.

\bibitem[{Rush et~al.(2015)Rush, Chopra, and Weston}]{rush2015neural}
Alexander~M Rush, Sumit Chopra, and Jason Weston. 2015.
\newblock A neural attention model for abstractive sentence summarization.
\newblock \emph{arXiv preprint arXiv:1509.00685}.

\bibitem[{See et~al.(2017)See, Liu, and Manning}]{see2017get}
Abigail See, Peter~J Liu, and Christopher~D Manning. 2017.
\newblock Get to the point: Summarization with pointer-generator networks.
\newblock In \emph{ACL}.

\bibitem[{Serdyuk et~al.(2018)Serdyuk, Ke, Sordoni, Trischler, Pal, and
  Bengio}]{serdyuk2017twin}
Dmitriy Serdyuk, Nan~Rosemary Ke, Alessandro Sordoni, Adam Trischler, Chris
  Pal, and Yoshua Bengio. 2018.
\newblock Twin networks: Matching the future for sequence generation.
\newblock In \emph{ICLR}.

\bibitem[{Song et~al.(2019)Song, Tan, Qin, Lu, and Liu}]{song2019mass}
Kaitao Song, Xu~Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019.
\newblock Mass: Masked sequence to sequence pre-training for language
  generation.
\newblock \emph{arXiv preprint arXiv:1905.02450}.

\bibitem[{Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le}]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le. 2014.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{NIPS}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{NIPS}.

\bibitem[{Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le}]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
  and Quoc~V Le. 2019.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1906.08237}.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Rashkin, Bisk, Farhadi,
  Roesner, and Choi}]{zellers2019defending}
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
  Franziska Roesner, and Yejin Choi. 2019.
\newblock Defending against neural fake news.
\newblock \emph{arXiv preprint arXiv:1905.12616}.

\bibitem[{Zhang et~al.(2019)Zhang, Zhao, Saleh, and Liu}]{zhang2019pegasus}
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter~J Liu. 2019.
\newblock Pegasus: Pre-training with extracted gap-sentences for abstractive
  summarization.
\newblock \emph{arXiv preprint arXiv:1912.08777}.

\bibitem[{Zhang and Bansal(2019)}]{zhang2019addressing}
Shiyue Zhang and Mohit Bansal. 2019.
\newblock Addressing semantic drift in question generation for semi-supervised
  question answering.
\newblock \emph{arXiv preprint arXiv:1909.06356}.

\bibitem[{Zhao et~al.(2018)Zhao, Ni, Ding, and Ke}]{zhao2018paragraph}
Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. 2018.
\newblock Paragraph-level neural question generation with maxout pointer and
  gated self-attention networks.
\newblock In \emph{EMNLP}.

\bibitem[{Zhou et~al.(2017)Zhou, Yang, Wei, Tan, Bao, and
  Zhou}]{zhou2017neural}
Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. 2017.
\newblock Neural question generation from text: A preliminary study.
\newblock In \emph{National CCF Conference on Natural Language Processing and
  Chinese Computing}, pages 662--671.

\bibitem[{Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler}]{zhu2015aligning}
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler. 2015.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 19--27.

\end{thebibliography}
