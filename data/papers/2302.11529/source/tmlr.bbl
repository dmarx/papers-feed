\begin{thebibliography}{360}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Gupta, and Zettlemoyer]{aghajanyan-etal-2021-intrinsic}
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer.
\newblock Intrinsic dimensionality explains the effectiveness of language model fine-tuning.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  7319--7328, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.568}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.568}.

\bibitem[Ahn et~al.(2019)Ahn, Kim, and Oh]{ahn2019deep}
Chanho Ahn, Eunwoo Kim, and Songhwai Oh.
\newblock Deep elastic networks with model selection for multi-task learning.
\newblock In \emph{2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2019, Seoul, Korea (South), October 27 - November 2, 2019}, pp.\  6528--6537. {IEEE}, 2019.
\newblock \doi{10.1109/ICCV.2019.00663}.
\newblock URL \url{https://doi.org/10.1109/ICCV.2019.00663}.

\bibitem[Ainsworth et~al.(2022)Ainsworth, Hayase, and Srinivasa]{Ainsworth2022GitReBasin}
Samuel~K. Ainsworth, Jonathan Hayase, and Siddhartha~S. Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock \emph{CoRR}, abs/2209.04836, 2022.
\newblock \doi{10.48550/arXiv.2209.04836}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2209.04836}.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei, Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski, Barreira, Vinyals, Zisserman, and Simonyan]{alayrac2022flamingo}
Jean{-}Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{CoRR}, abs/2204.14198, 2022.
\newblock \doi{10.48550/arXiv.2204.14198}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2204.14198}.

\bibitem[Alet et~al.(2018)Alet, Lozano{-}P{\'{e}}rez, and Kaelbling]{alet2018modular}
Ferran Alet, Tom{\'{a}}s Lozano{-}P{\'{e}}rez, and Leslie~Pack Kaelbling.
\newblock Modular meta-learning.
\newblock In \emph{2nd Annual Conference on Robot Learning, CoRL 2018, Z{\"{u}}rich, Switzerland, 29-31 October 2018, Proceedings}, volume~87 of \emph{Proceedings of Machine Learning Research}, pp.\  856--868. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v87/alet18a.html}.

\bibitem[Aljundi et~al.(2017)Aljundi, Chakravarty, and Tuytelaars]{Aljundi2017Expert}
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars.
\newblock Expert gate: Lifelong learning with a network of experts.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pp.\  7120--7129. {IEEE} Computer Society, 2017.
\newblock \doi{10.1109/CVPR.2017.753}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2017.753}.

\bibitem[Andor et~al.(2019)Andor, He, Lee, and Pitler]{andor-etal-2019-giving}
Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler.
\newblock Giving {BERT} a calculator: Finding operations and arguments with reading comprehension.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  5947--5952, Hong Kong, China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1609}.
\newblock URL \url{https://aclanthology.org/D19-1609}.

\bibitem[Andreas et~al.(2016{\natexlab{a}})Andreas, Rohrbach, Darrell, and Klein]{andreas2016learning}
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
\newblock Learning to compose neural networks for question answering.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1545--1554, San Diego, California, June 2016{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N16-1181}.
\newblock URL \url{https://aclanthology.org/N16-1181}.

\bibitem[Andreas et~al.(2016{\natexlab{b}})Andreas, Rohrbach, Darrell, and Klein]{andreas2016nmn}
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
\newblock Neural module networks.
\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pp.\  39--48. {IEEE} Computer Society, 2016{\natexlab{b}}.
\newblock \doi{10.1109/CVPR.2016.12}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2016.12}.

\bibitem[Andreas et~al.(2017)Andreas, Klein, and Levine]{andreas2017modular}
Jacob Andreas, Dan Klein, and Sergey Levine.
\newblock Modular multitask reinforcement learning with policy sketches.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, volume~70 of \emph{Proceedings of Machine Learning Research}, pp.\  166--175. {PMLR}, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/andreas17a.html}.

\bibitem[Ansell et~al.(2021)Ansell, Ponti, Pfeiffer, Ruder, Glava{\v{s}}, Vuli{\'c}, and Korhonen]{Ansell2021MADG}
Alan Ansell, Edoardo~Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glava{\v{s}}, Ivan Vuli{\'c}, and Anna Korhonen.
\newblock {MAD}-{G}: {M}ultilingual adapter generation for efficient cross-lingual transfer.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pp.\  4762--4781, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-emnlp.410}.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.410}.

\bibitem[Ansell et~al.(2022)Ansell, Ponti, Korhonen, and Vuli{\'c}]{ansell2021composable}
Alan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli{\'c}.
\newblock Composable sparse fine-tuning for cross-lingual transfer.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1778--1796, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.125}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.125}.

\bibitem[Anwar et~al.(2017)Anwar, Hwang, and Sung]{anwar2017structured}
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung.
\newblock Structured pruning of deep convolutional neural networks.
\newblock \emph{ACM Journal on Emerging Technologies in Computing Systems (JETC)}, 13\penalty0 (3):\penalty0 32:1--32:18, 2017.
\newblock \doi{10.1145/3005348}.
\newblock URL \url{https://doi.org/10.1145/3005348}.

\bibitem[Artetxe et~al.(2022)Artetxe, Bhosale, Goyal, Mihaylov, Ott, Shleifer, Lin, Du, Iyer, Pasunuru, Anantharaman, Li, Chen, Akin, Baines, Martin, Zhou, Koura, O{'}Horo, Wang, Zettlemoyer, Diab, Kozareva, and Stoyanov]{artetxe2021efficient}
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi~Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit~Singh Koura, Brian O{'}Horo, Jeffrey Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Veselin Stoyanov.
\newblock Efficient large scale language modeling with mixtures of experts.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  11699--11732, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.804}.

\bibitem[Asai et~al.(2022)Asai, Salehi, Peters, and Hajishirzi]{asai-etal-2022-attempt}
Akari Asai, Mohammadreza Salehi, Matthew Peters, and Hannaneh Hajishirzi.
\newblock {ATTEMPT}: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  6655--6672, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.446}.

\bibitem[Baars(2005)]{baars2005global}
Bernard~J. Baars.
\newblock Global workspace theory of consciousness: toward a cognitive neuroscience of human experience.
\newblock \emph{Progress in Brain Research}, 150:\penalty0 45--53, 2005.
\newblock \doi{10.1016/S0079-6123(05)50004-9}.
\newblock URL \url{https://10.1016/S0079-6123(05)50004-9}.

\bibitem[Babu et~al.(2022)Babu, Wang, Tjandra, Lakhotia, Xu, Goyal, Singh, von Platen, Saraf, Pino, Baevski, Conneau, and Auli]{Babu:2022xlsr}
Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli.
\newblock {XLS-R:} self-supervised cross-lingual speech representation learning at scale.
\newblock In \emph{Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18-22 September 2022}, pp.\  2278--2282. {ISCA}, 2022.
\newblock \doi{10.21437/Interspeech.2022-143}.
\newblock URL \url{https://doi.org/10.21437/Interspeech.2022-143}.

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{bacon2017option}
Pierre{-}Luc Bacon, Jean Harb, and Doina Precup.
\newblock The option-critic architecture.
\newblock In Satinder Singh and Shaul Markovitch (eds.), \emph{Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, {USA}}, pp.\  1726--1734. {AAAI} Press, 2017.
\newblock URL \url{http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14858}.

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and Auli]{Baevski:2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: {A} framework for self-supervised learning of speech representations.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html}.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{Bahdanau2015NMT}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and translate.
\newblock In Yoshua Bengio and Yann LeCun (eds.), \emph{3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1409.0473}.

\bibitem[Bai et~al.(2022)Bai, Li, Han, Ni, Xu, Zhang, Yi, and Wang]{Bai:2022speech}
Ye~Bai, Jie Li, Wenjing Han, Hao Ni, Kaituo Xu, Zhuo Zhang, Cheng Yi, and Xiaorui Wang.
\newblock Parameter-efficient conformers via sharing sparsely-gated experts for end-to-end speech recognition.
\newblock In \emph{Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18-22 September 2022}, pp.\  1676--1680. {ISCA}, 2022.
\newblock \doi{10.21437/Interspeech.2022-709}.
\newblock URL \url{https://doi.org/10.21437/Interspeech.2022-709}.

\bibitem[Baktashmotlagh et~al.(2013)Baktashmotlagh, Harandi, Lovell, and Salzmann]{baktashmotlagh2013unsupervised}
Mahsa Baktashmotlagh, Mehrtash~Tafazzoli Harandi, Brian~C. Lovell, and Mathieu Salzmann.
\newblock Unsupervised domain adaptation by domain invariant projection.
\newblock In \emph{{IEEE} International Conference on Computer Vision, {ICCV} 2013, Sydney, Australia, December 1-8, 2013}, pp.\  769--776. {IEEE} Computer Society, 2013.
\newblock \doi{10.1109/ICCV.2013.100}.
\newblock URL \url{https://doi.org/10.1109/ICCV.2013.100}.

\bibitem[Baldwin \& Clark(2000)Baldwin and Clark]{baldwin2000design}
Carliss~Young Baldwin and Kim~B. Clark.
\newblock \emph{Design rules: The power of modularity}.
\newblock MIT Press, 2000.
\newblock URL \url{https://doi.org/10.7551/mitpress/2366.001.0001}.

\bibitem[Ballard(1986)]{ballard1986cortical}
Dana~H Ballard.
\newblock Cortical connections and parallel processing: Structure and function.
\newblock \emph{Behavioral and Brain Sciences}, 9\penalty0 (1):\penalty0 67--90, 1986.
\newblock URL \url{https://psycnet.apa.org/doi/10.1017/S0140525X00021555}.

\bibitem[Bapna \& Firat(2019)Bapna and Firat]{Bapna2019Adapters}
Ankur Bapna and Orhan Firat.
\newblock Simple, scalable adaptation for neural machine translation.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  1538--1548, Hong Kong, China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1165}.
\newblock URL \url{https://aclanthology.org/D19-1165}.

\bibitem[Baziotis et~al.(2022)Baziotis, Artetxe, Cross, and Bhosale]{Baziotis2022MultilingualMTHyperAdapter}
Christos Baziotis, Mikel Artetxe, James Cross, and Shruti Bhosale.
\newblock Multilingual machine translation with hyper-adapters.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  1170--1185, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.77}.

\bibitem[Ben~Zaken et~al.(2022)Ben~Zaken, Goldberg, and Ravfogel]{ben-zaken-etal-2022-bitfit}
Elad Ben~Zaken, Yoav Goldberg, and Shauli Ravfogel.
\newblock {B}it{F}it: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  1--9, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-short.1}.
\newblock URL \url{https://aclanthology.org/2022.acl-short.1}.

\bibitem[Bengio et~al.(2015)Bengio, Bacon, Pineau, and Precup]{bengio2015conditional}
Emmanuel Bengio, Pierre{-}Luc Bacon, Joelle Pineau, and Doina Precup.
\newblock Conditional computation in neural networks for faster models.
\newblock \emph{CoRR}, abs/1511.06297, 2015.
\newblock URL \url{http://arxiv.org/abs/1511.06297}.

\bibitem[Bengio et~al.(2013)Bengio, L{\'{e}}onard, and Courville]{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'{e}}onard, and Aaron~C. Courville.
\newblock Estimating or propagating gradients through stochastic neurons for conditional computation.
\newblock \emph{CoRR}, abs/1308.3432, 2013.
\newblock URL \url{http://arxiv.org/abs/1308.3432}.

\bibitem[Bengio et~al.(2020)Bengio, Deleu, Rahaman, Ke, Lachapelle, Bilaniuk, Goyal, and Pal]{Bengio2020meta}
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan~Rosemary Ke, S{\'{e}}bastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher~J. Pal.
\newblock A meta-transfer objective for learning to disentangle causal mechanisms.
\newblock In \emph{8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=ryxWIgBFPS}.

\bibitem[Berriel et~al.(2019)Berriel, Lathuili{\`{e}}re, Nabi, Klein, Oliveira{-}Santos, Sebe, and Ricci]{Berriel:2019iccv}
Rodrigo~Ferreira Berriel, St{\'{e}}phane Lathuili{\`{e}}re, Moin Nabi, Tassilo Klein, Thiago Oliveira{-}Santos, Nicu Sebe, and Elisa Ricci.
\newblock Budget-aware adapters for multi-domain learning.
\newblock In \emph{2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2019, Seoul, Korea (South), October 27 - November 2, 2019}, pp.\  382--391, 2019.
\newblock \doi{10.1109/ICCV.2019.00047}.
\newblock URL \url{https://doi.org/10.1109/ICCV.2019.00047}.

\bibitem[Bertinetto et~al.(2016)Bertinetto, Henriques, Valmadre, Torr, and Vedaldi]{Bertinetto2016Learning}
Luca Bertinetto, Jo{\~{a}}o~F. Henriques, Jack Valmadre, Philip H.~S. Torr, and Andrea Vedaldi.
\newblock Learning feed-forward one-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain}, pp.\  523--531, 2016.
\newblock URL \url{https://proceedings.neurips.cc/paper/2016/hash/839ab46820b524afda05122893c2fe8e-Abstract.html}.

\bibitem[Biadsy et~al.(2022)Biadsy, Chen, Zhang, Rybakov, Rosenberg, and Moreno]{Biadsy:2022speech}
Fadi Biadsy, Youzheng Chen, Xia Zhang, Oleg Rybakov, Andrew Rosenberg, and Pedro~J. Moreno.
\newblock A scalable model specialization framework for training and inference using submodels and its application to speech model personalization.
\newblock In \emph{Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18-22 September 2022}, pp.\  5125--5129. {ISCA}, 2022.
\newblock \doi{10.21437/Interspeech.2022-10613}.
\newblock URL \url{https://doi.org/10.21437/Interspeech.2022-10613}.

\bibitem[Bilen \& Vedaldi(2017)Bilen and Vedaldi]{Bilen2017Universal}
Hakan Bilen and Andrea Vedaldi.
\newblock Universal representations: The missing link between faces, text, planktons, and cat breeds.
\newblock \emph{CoRR}, abs/1701.07275, 2017.
\newblock URL \url{http://arxiv.org/abs/1701.07275}.

\bibitem[Bodenreider(2004)]{bodenreider2004unified}
Olivier Bodenreider.
\newblock The unified medical language system ({UMLS}): integrating biomedical terminology.
\newblock \emph{Nucleic Acids Research}, 32\penalty0 (suppl\_1):\penalty0 D267--D270, 2004.
\newblock URL \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC308795/}.

\bibitem[Booch et~al.(2008)Booch, Maksimchuk, Engle, Young, Conallen, and Houston]{booch2008object}
Grady Booch, Robert~A. Maksimchuk, Michael~W. Engle, Bobbi~J. Young, Jim Conallen, and Kelli~A. Houston.
\newblock Object-oriented analysis and design with applications, third edition.
\newblock \emph{{ACM} {SIGSOFT} Software Engineering Notes}, 33\penalty0 (5), 2008.
\newblock \doi{10.1145/1402521.1413138}.
\newblock URL \url{https://doi.org/10.1145/1402521.1413138}.

\bibitem[Bousmalis et~al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan]{Bousmalis2016}
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
\newblock Domain separation networks.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), \emph{Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain}, pp.\  343--351, 2016.
\newblock URL \url{https://proceedings.neurips.cc/paper/2016/hash/45fbc6d3e05ebd93369ce542e8f2322d-Abstract.html}.

\bibitem[Bragman et~al.(2019)Bragman, Tanno, Ourselin, Alexander, and Cardoso]{bragman2019stochastic}
Felix J.~S. Bragman, Ryutaro Tanno, S{\'{e}}bastien Ourselin, Daniel~C. Alexander, and Manuel~Jorge Cardoso.
\newblock Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels.
\newblock In \emph{2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2019, Seoul, Korea (South), October 27 - November 2, 2019}, pp.\  1385--1394. {IEEE}, 2019.
\newblock \doi{10.1109/ICCV.2019.00147}.
\newblock URL \url{https://doi.org/10.1109/ICCV.2019.00147}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Bugliarello et~al.(2022)Bugliarello, Liu, Pfeiffer, Reddy, Elliott, Ponti, and Vuli\'{c}]{bugliarello2022iglue}
Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo~Maria Ponti, and Ivan Vuli\'{c}.
\newblock {IGLUE:} {A} benchmark for transfer learning across modalities, tasks, and languages.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  2370--2392. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/bugliarello22a.html}.

\bibitem[Caccia et~al.(2022)Caccia, Ponti, Liu, Pereira, Roux, and Sordoni]{caccia2022multihead}
Lucas Caccia, Edoardo Ponti, Lucas Liu, Matheus Pereira, Nicolas~Le Roux, and Alessandro Sordoni.
\newblock Multi-head adapter routing for data-efficient fine-tuning, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.03831}.

\bibitem[Cai et~al.(2020)Cai, Gan, Zhu, and Han]{Cai2020tinytl}
Han Cai, Chuang Gan, Ligeng Zhu, and Song Han.
\newblock Tinytl: Reduce memory, not parameters for efficient on-device learning.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html}.

\bibitem[Caruana(1997)]{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine Learning}, 28\penalty0 (1):\penalty0 41--75, 1997.
\newblock \doi{10.1023/A:1007379606734}.
\newblock URL \url{https://doi.org/10.1023/A:1007379606734}.

\bibitem[Cases et~al.(2019)Cases, Rosenbaum, Riemer, Geiger, Klinger, Tamkin, Li, Agarwal, Greene, Jurafsky, Potts, and Karttunen]{Cases2019Recursive}
Ignacio Cases, Clemens Rosenbaum, Matthew Riemer, Atticus Geiger, Tim Klinger, Alex Tamkin, Olivia Li, Sandhini Agarwal, Joshua~D. Greene, Dan Jurafsky, Christopher Potts, and Lauri Karttunen.
\newblock Recursive routing networks: Learning to compose modules for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  3631--3648, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1365}.
\newblock URL \url{https://aclanthology.org/N19-1365}.

\bibitem[Casper et~al.(2022)Casper, Hod, Filan, Wild, Critch, and Russell]{casper2022graphical}
Stephen Casper, Shlomi Hod, Daniel Filan, Cody Wild, Andrew Critch, and Stuart Russell.
\newblock Graphical clusterability and local specialization in deep neural networks.
\newblock In \emph{ICLR 2022 Workshop on PAIR$^2$Struct}, 2022.
\newblock URL \url{https://openreview.net/pdf?id=HreeeJvkue9}.

\bibitem[Chang et~al.(2019)Chang, Gupta, Levine, and Griffiths]{chang2018automatically}
Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas~L. Griffiths.
\newblock Automatically composing representation transformations as a means for generalization.
\newblock In \emph{7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1ffQnRcKX}.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and Carbin]{chen2020lottery}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin.
\newblock The lottery ticket hypothesis for pre-trained {BERT} networks.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/b6af2c9703f203a2794be03d443af2e3-Abstract.html}.

\bibitem[Chen et~al.(2021)Chen, Cheng, Gan, Yuan, Zhang, and Wang]{Chen:2021neurips}
Tianlong Chen, Yu~Cheng, Zhe Gan, Lu~Yuan, Lei Zhang, and Zhangyang Wang.
\newblock Chasing sparsity in vision transformers: An end-to-end exploration.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  19974--19988, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/a61f27ab2165df0e18cc9433bd7f27c5-Abstract.html}.

\bibitem[Chen et~al.(2019)Chen, Lu\v{c}i\'{c}, Houlsby, and Gelly]{Chen2019}
Ting Chen, Mario Lu\v{c}i\'{c}, Neil Houlsby, and Sylvain Gelly.
\newblock On self modulation for generative adversarial networks.
\newblock In \emph{7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=Hkl5aoR5tm}.

\bibitem[Chen \& Cardie(2018)Chen and Cardie]{chen-cardie-2018-multinomial}
Xilun Chen and Claire Cardie.
\newblock Multinomial adversarial networks for multi-domain text classification.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pp.\  1226--1240, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-1111}.
\newblock URL \url{https://aclanthology.org/N18-1111}.

\bibitem[Chen et~al.(2023)Chen, Fu, Liu, Li, and Lee]{Chen:2022speech}
Zih{-}Ching Chen, Chin{-}Lun Fu, Chih{-}Ying Liu, Shang{-}Wen~(Daniel) Li, and Hung{-}yi Lee.
\newblock Exploring efficient-tuning methods in self-supervised speech models.
\newblock In \emph{{IEEE} Spoken Language Technology Workshop, {SLT} 2022, Doha, Qatar, January 9-12, 2023}, pp.\  1120--1127. {IEEE}, 2023.
\newblock \doi{10.1109/SLT54892.2023.10023274}.
\newblock URL \url{https://doi.org/10.1109/SLT54892.2023.10023274}.

\bibitem[Cheng et~al.(2023)Cheng, Lin, Chen, Zhao, and Yan]{cheng-etal-2023-decouple}
Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan.
\newblock Decouple knowledge from paramters for plug-and-play language modeling.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pp.\  14288--14308, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.901}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.901}.

\bibitem[Chi et~al.(2022)Chi, Dong, Huang, Dai, Ma, Patra, Singhal, Bajaj, Song, Mao, Huang, and Wei]{chi2022representation}
Zewen Chi, Li~Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei.
\newblock On the representation collapse of sparse mixture of experts, 2022.

\bibitem[Choenni et~al.(2022)Choenni, Garrette, and Shutova]{Choenni:2022arxiv}
Rochelle Choenni, Dan Garrette, and Ekaterina Shutova.
\newblock Data-efficient cross-lingual transfer with language-specific subnetworks.
\newblock \emph{CoRR}, abs/2211.00106, 2022.
\newblock \doi{10.48550/arXiv.2211.00106}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2211.00106}.

\bibitem[Choshen et~al.(2022)Choshen, Venezian, Slonim, and Katz]{choshen2022fusing}
Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz.
\newblock Fusing finetuned models for better pretraining, 2022.

\bibitem[Chronopoulou et~al.(2022{\natexlab{a}})Chronopoulou, Peters, and Dodge]{Chronopoulou2022EfficientHierarchical}
Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge.
\newblock Efficient hierarchical domain adaptation for pretrained language models.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1336--1351, Seattle, United States, July 2022{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.96}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.96}.

\bibitem[Chronopoulou et~al.(2022{\natexlab{b}})Chronopoulou, Stojanovski, and Fraser]{Chronopolou:2022arxiv}
Alexandra Chronopoulou, Dario Stojanovski, and Alexander Fraser.
\newblock Language-family adapters for multilingual neural machine translation.
\newblock \emph{CoRR}, abs/2209.15236, 2022{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2209.15236}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2209.15236}.

\bibitem[Clark et~al.(2022)Clark, de~Las~Casas, Guy, Mensch, Paganini, Hoffmann, Damoc, Hechtman, Cai, Borgeaud, van~den Driessche, Rutherford, Hennigan, Johnson, Cassirer, Jones, Buchatskaya, Budden, Sifre, Osindero, Vinyals, Ranzato, Rae, Elsen, Kavukcuoglu, and Simonyan]{Clark2022UnifiedScaling}
Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake~A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van~den Driessche, Eliza Rutherford, Tom Hennigan, Matthew~J. Johnson, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Marc'Aurelio Ranzato, Jack~W. Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan.
\newblock Unified scaling laws for routed language models.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  4057--4086. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/clark22a.html}.

\bibitem[Conneau et~al.(2020)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek, Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{conneau-etal-2020-unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  8440--8451, Online, July 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.747}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.747}.

\bibitem[Cooper~Stickland et~al.(2021)Cooper~Stickland, Berard, and Nikoulina]{cooper-stickland-etal-2021-multilingual}
Asa Cooper~Stickland, Alexandre Berard, and Vassilina Nikoulina.
\newblock Multilingual domain adaptation for {NMT}: Decoupling language and domain information with adapters.
\newblock In \emph{Proceedings of the Sixth Conference on Machine Translation}, pp.\  578--598, Online, November 2021. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2021.wmt-1.64}.

\bibitem[Costa{-}juss{\`{a}} et~al.(2022)Costa{-}juss{\`{a}}, Cross, {\c{C}}elebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard, Sun, Wang, Wenzek, Youngblood, Akula, Barrault, Gonzalez, Hansanti, Hoffman, Jarrett, Sadagopan, Rowe, Spruit, Tran, Andrews, Ayan, Bhosale, Edunov, Fan, Gao, Goswami, Guzm{\'{a}}n, Koehn, Mourachko, Ropers, Saleem, Schwenk, and Wang]{Costa:2022nllb}
Marta~R. Costa{-}juss{\`{a}}, James Cross, Onur {\c{C}}elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al~Youngblood, Bapi Akula, Lo{\"{\i}}c Barrault, Gabriel~Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik~Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip~Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm{\'{a}}n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
\newblock No language left behind: Scaling human-centered machine translation.
\newblock \emph{CoRR}, abs/2207.04672, 2022.
\newblock \doi{10.48550/arXiv.2207.04672}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2207.04672}.

\bibitem[Crawshaw(2020)]{crawshaw2020multi}
Michael Crawshaw.
\newblock Multi-task learning with deep neural networks: {A} survey.
\newblock \emph{CoRR}, abs/2009.09796, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.09796}.

\bibitem[Csord{\'{a}}s et~al.(2021)Csord{\'{a}}s, van Steenkiste, and Schmidhuber]{csordas2021are}
R{\'{o}}bert Csord{\'{a}}s, Sjoerd van Steenkiste, and J{\"{u}}rgen Schmidhuber.
\newblock Are neural nets modular? inspecting functional modularity through differentiable weight masks.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=7uVcpu-gMD}.

\bibitem[Das et~al.(2018)Das, Gkioxari, Lee, Parikh, and Batra]{das2018neural}
Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra.
\newblock Neural modular control for embodied question answering.
\newblock In \emph{2nd Annual Conference on Robot Learning, CoRL 2018, Z{\"{u}}rich, Switzerland, 29-31 October 2018, Proceedings}, volume~87 of \emph{Proceedings of Machine Learning Research}, pp.\  53--62. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v87/das18a.html}.

\bibitem[Dayan \& Hinton(1992)Dayan and Hinton]{dayan1992feudal}
Peter Dayan and Geoffrey~E. Hinton.
\newblock Feudal reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 5, {[NIPS} Conference, Denver, Colorado, USA, November 30 - December 3, 1992]}, pp.\  271--278. Morgan Kaufmann, 1992.
\newblock URL \url{http://papers.nips.cc/paper/714-feudal-reinforcement-learning}.

\bibitem[de~Vries et~al.(2017)de~Vries, Strub, Mary, Larochelle, Pietquin, and Courville]{de2017modulating}
Harm de~Vries, Florian Strub, J{\'{e}}r{\'{e}}mie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron~C. Courville.
\newblock Modulating early visual processing by language.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\  6594--6604, 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/hash/6fab6e3aa34248ec1e34a4aeedecddc8-Abstract.html}.

\bibitem[Devin et~al.(2017)Devin, Gupta, Darrell, Abbeel, and Levine]{devin2017learning}
Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine.
\newblock Learning modular neural network policies for multi-task and multi-robot transfer.
\newblock In \emph{2017 {IEEE} International Conference on Robotics and Automation, {ICRA} 2017, Singapore, Singapore, May 29 - June 3, 2017}, pp.\  2169--2176. {IEEE}, 2017.
\newblock \doi{10.1109/ICRA.2017.7989250}.
\newblock URL \url{https://doi.org/10.1109/ICRA.2017.7989250}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Devlin:2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Didolkar et~al.(2021)Didolkar, Goyal, Ke, Blundell, Beaudoin, Heess, Mozer, and Bengio]{goyal2021nps}
Aniket Didolkar, Anirudh Goyal, Nan~Rosemary Ke, Charles Blundell, Philippe Beaudoin, Nicolas Heess, Michael Mozer, and Yoshua Bengio.
\newblock Neural production systems.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  25673--25687, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/d785bf9067f8af9e078b93cf26de2b54-Abstract.html}.

\bibitem[Dietterich(2000)]{dietterich2000hierarchical}
Thomas~G. Dietterich.
\newblock Hierarchical reinforcement learning with the {MAXQ} value function decomposition.
\newblock \emph{Journal of Artificial Intelligence Research}, 13:\penalty0 227--303, 2000.
\newblock \doi{10.1613/jair.639}.
\newblock URL \url{https://doi.org/10.1613/jair.639}.

\bibitem[Dobs et~al.(2022)Dobs, Martinez, Kell, and Kanwisher]{dobs2022brain}
Katharina Dobs, Julio Martinez, Alexander~JE Kell, and Nancy Kanwisher.
\newblock Brain-like functional specialization emerges spontaneously in deep neural networks.
\newblock \emph{Science Advances}, 8\penalty0 (11):\penalty0 eabl8913, 2022.
\newblock URL \url{https://pubmed.ncbi.nlm.nih.gov/35294241/}.

\bibitem[Don{-}Yehiya et~al.(2022)Don{-}Yehiya, Venezian, Raffel, Slonim, Katz, and Choshen]{Shachar2022ColD}
Shachar Don{-}Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.
\newblock Cold fusion: Collaborative descent for distributed multitask finetuning.
\newblock \emph{arXiv prerint}, 2022.
\newblock \doi{10.48550/arXiv.2212.01378}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2212.01378}.

\bibitem[Donahue et~al.(2014)Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, and Darrell]{donahue2014decaf}
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.
\newblock Decaf: {A} deep convolutional activation feature for generic visual recognition.
\newblock In \emph{Proceedings of the 31th International Conference on Machine Learning, {ICML} 2014, Beijing, China, 21-26 June 2014}, volume~32 of \emph{{JMLR} Workshop and Conference Proceedings}, pp.\  647--655. JMLR.org, 2014.
\newblock URL \url{http://proceedings.mlr.press/v32/donahue14.html}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and Hamprecht]{pmlr-v80-draxler18a}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred~A. Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  1308--1317. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/draxler18a.html}.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier{-}Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{Du2022Glam}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten~P. Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier{-}Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  5547--5569. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/du22c.html}.

\bibitem[Dua et~al.(2022)Dua, Bhosale, Goswami, Cross, Lewis, and Fan]{dua-etal-2022-tricks}
Dheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, and Angela Fan.
\newblock Tricks for training sparse translation models.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  3340--3345, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.244}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.244}.

\bibitem[Eeckt \& Van~hamme(2022)Eeckt and Van~hamme]{Eeckt:2022speech}
Steven~Vander Eeckt and Hugo Van~hamme.
\newblock Using adapters to overcome catastrophic forgetting in end-to-end automatic speech recognition.
\newblock \emph{CoRR}, abs/203.16082, 2022.
\newblock \doi{10.48550/arXiv.2203.16082}.
\newblock URL \url{https://arxiv.org/abs/2203.16082}.

\bibitem[Eigen et~al.(2014)Eigen, Ranzato, and Sutskever]{Eigen2013LearningFactored}
David Eigen, Marc'Aurelio Ranzato, and Ilya Sutskever.
\newblock Learning factored representations in a deep mixture of experts.
\newblock In \emph{2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.4314}.

\bibitem[Faisal \& Anastasopoulos(2022)Faisal and Anastasopoulos]{Faisal:2022arxiv}
Fahim Faisal and Antonios Anastasopoulos.
\newblock Phylogeny-inspired adaptation of multilingual models to new languages.
\newblock In \emph{Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing}, pp.\  434--452, Online only, November 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.aacl-main.34}.

\bibitem[Fan et~al.(2021)Fan, Bhosale, Schwenk, Ma, El{-}Kishky, Goyal, Baines, Celebi, Wenzek, Chaudhary, Goyal, Birch, Liptchinsky, Edunov, Auli, and Joulin]{Fan2021Beyond}
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El{-}Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin.
\newblock Beyond english-centric multilingual machine translation.
\newblock \emph{Journal of Machine Learning Research}, 22:\penalty0 107:1--107:48, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-1307.html}.

\bibitem[Fan \& Alwan(2022)Fan and Alwan]{Fan:2022speech}
Ruchao Fan and Abeer Alwan.
\newblock {DRAFT:} {A} novel framework to reduce domain shifting in self-supervised learning and its application to children's {ASR}.
\newblock In Hanseok Ko and John H.~L. Hansen (eds.), \emph{Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18-22 September 2022}, pp.\  4900--4904. {ISCA}, 2022.
\newblock \doi{10.21437/Interspeech.2022-11128}.
\newblock URL \url{https://doi.org/10.21437/Interspeech.2022-11128}.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{CoRR}, abs/2101.03961, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.03961}.

\bibitem[Fedus et~al.(2022)Fedus, Dean, and Zoph]{fedus2022review}
William Fedus, Jeff Dean, and Barret Zoph.
\newblock A review of sparse expert models in deep learning.
\newblock \emph{CoRR}, abs/2209.01667, 2022.
\newblock \doi{10.48550/arXiv.2209.01667}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2209.01667}.

\bibitem[Fernando et~al.(2017)Fernando, Banarse, Blundell, Zwols, Ha, Rusu, Pritzel, and Wierstra]{fernando2017pathnet}
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei~A. Rusu, Alexander Pritzel, and Daan Wierstra.
\newblock Pathnet: Evolution channels gradient descent in super neural networks.
\newblock \emph{CoRR}, abs/1701.08734, 2017.
\newblock URL \url{http://arxiv.org/abs/1701.08734}.

\bibitem[Florensa et~al.(2017)Florensa, Duan, and Abbeel]{florensa2017stochastic}
Carlos Florensa, Yan Duan, and Pieter Abbeel.
\newblock Stochastic neural networks for hierarchical reinforcement learning.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1oK8aoxe}.

\bibitem[Fodor(1983)]{fodor1983modularity}
Jerry~A. Fodor.
\newblock \emph{The modularity of Mind}.
\newblock MIT Press, 1983.
\newblock URL \url{https://mitpress.mit.edu/9780262560252/the-modularity-of-mind/}.

\bibitem[Foroutan et~al.(2022)Foroutan, Banaei, Lebret, Bosselut, and Aberer]{Foroutan2022Discovering}
Negar Foroutan, Mohammadreza Banaei, R{\'e}mi Lebret, Antoine Bosselut, and Karl Aberer.
\newblock Discovering language-neutral sub-networks in multilingual language models.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  7560--7575, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.513}.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{Frankle2019}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock In \emph{7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and Carbin]{Frankle2020LinearModeConnect}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M. Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  3259--3269. {PMLR}, 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/frankle20a.html}.

\bibitem[Freeman \& Bruna(2017)Freeman and Bruna]{Freeman2017Topology}
C.~Daniel Freeman and Joan Bruna.
\newblock Topology and geometry of half-rectified network optimization.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=Bk0FWVcgx}.

\bibitem[French(1999)]{french1999catastrophic}
Robert~M. French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in Cognitive Sciences}, 3\penalty0 (4):\penalty0 128--135, 1999.
\newblock \doi{https://doi.org/10.1016/S1364-6613(99)01294-2}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S1364661399012942}.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor~S. Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{Journal of Machine Learning Research}, 17:\penalty0 59:1--59:35, 2016.
\newblock URL \url{http://jmlr.org/papers/v17/15-239.html}.

\bibitem[Gao et~al.(2021{\natexlab{a}})Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and Qiao]{Gao:2021modal}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu~Qiao.
\newblock {CLIP-Adapter: B}etter vision-language models with feature adapters.
\newblock \emph{CoRR}, abs/2110.04544, 2021{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2110.04544}.

\bibitem[Gao et~al.(2021{\natexlab{b}})Gao, Fisch, and Chen]{gao-etal-2021-making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  3816--3830, Online, August 2021{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.295}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.295}.

\bibitem[Gao et~al.(2019)Gao, Ma, Zhao, Liu, and Yuille]{gao2019nddr}
Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan~L. Yuille.
\newblock {NDDR-CNN:} layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}, pp.\  3205--3214. Computer Vision Foundation / {IEEE}, 2019.
\newblock \doi{10.1109/CVPR.2019.00332}.
\newblock URL \url{http://openaccess.thecvf.com/content\_CVPR\_2019/html/Gao\_NDDR-CNN\_Layerwise\_Feature\_Fusing\_in\_Multi-Task\_CNNs\_by\_Neural\_Discriminative\_CVPR\_2019\_paper.html}.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson]{Garipov2018LossSurface}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P. Vetrov, and Andrew~Gordon Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada}, pp.\  8803--8812, 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper/2018/hash/be3087e74e9100d4bc4c6268cdbe8456-Abstract.html}.

\bibitem[Gaur et~al.(2021)Gaur, Farris, Haghani, Leal, Moreno, Prasad, Ramabhadran, and Zhu]{Gaur:2021speechmoe}
Neeraj Gaur, Brian Farris, Parisa Haghani, Isabel Leal, Pedro~J. Moreno, Manasa Prasad, Bhuvana Ramabhadran, and Yun Zhu.
\newblock Mixture of informed experts for multilingual speech recognition.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021}, pp.\  6234--6238. {IEEE}, 2021.
\newblock URL \url{https://doi.org/10.1109/ICASSP39728.2021.9414379}.

\bibitem[Geffner et~al.(2022)Geffner, Antoran, Foster, Gong, Ma, Kiciman, Sharma, Lamb, Kukla, Pawlowski, Allamanis, and Zhang]{geffner2022deep}
Tomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, Miltiadis Allamanis, and Cheng Zhang.
\newblock Deep end-to-end causal inference.
\newblock In \emph{NeurIPS 2022 Workshop on Causality for Real-world Impact}, 2022.
\newblock URL \url{https://openreview.net/forum?id=6DPVXzjnbDK}.

\bibitem[Gesmundo(2022)]{Gesmundo2022Multiagent}
Andrea Gesmundo.
\newblock A multi-agent framework for the asynchronous and collaborative extension of multitask {ML} systems.
\newblock \emph{CoRR}, abs/2209.14745, 2022.
\newblock \doi{10.48550/arXiv.2209.14745}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2209.14745}.

\bibitem[Gesmundo \& Dean(2022{\natexlab{a}})Gesmundo and Dean]{Gesmundo2022Evolutionary}
Andrea Gesmundo and Jeff Dean.
\newblock An evolutionary approach to dynamic introduction of tasks in large-scale multitask learning systems.
\newblock \emph{CoRR}, abs/2205.12755, 2022{\natexlab{a}}.
\newblock \doi{10.48550/arXiv.2205.12755}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.12755}.

\bibitem[Gesmundo \& Dean(2022{\natexlab{b}})Gesmundo and Dean]{Gesmundo2022munet}
Andrea Gesmundo and Jeff Dean.
\newblock munet: Evolving pretrained deep neural networks into scalable auto-tuning multitask systems.
\newblock \emph{CoRR}, abs/2205.10937, 2022{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2205.10937}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.10937}.

\bibitem[Goyal et~al.(2021)Goyal, Lamb, Hoffmann, Sodhani, Levine, Bengio, and Sch{\"{o}}lkopf]{goyal2019recurrent}
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Sch{\"{o}}lkopf.
\newblock Recurrent independent mechanisms.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=mLcmdlEUxy-}.

\bibitem[Graves et~al.(2006)Graves, Fern{\'{a}}ndez, Gomez, and Schmidhuber]{Graves:2006ctc}
Alex Graves, Santiago Fern{\'{a}}ndez, Faustino~J. Gomez, and J{\"{u}}rgen Schmidhuber.
\newblock Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.
\newblock In William~W. Cohen and Andrew~W. Moore (eds.), \emph{Machine Learning, Proceedings of the Twenty-Third International Conference {(ICML} 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006}, volume 148 of \emph{{ACM} International Conference Proceeding Series}, pp.\  369--376. {ACM}, 2006.
\newblock \doi{10.1145/1143844.1143891}.
\newblock URL \url{https://doi.org/10.1145/1143844.1143891}.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock \emph{CoRR}, abs/1410.5401, 2014.
\newblock URL \url{http://arxiv.org/abs/1410.5401}.

\bibitem[Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka, Grabska{-}Barwinska, Colmenarejo, Grefenstette, Ramalho, Agapiou, Badia, Hermann, Zwols, Ostrovski, Cain, King, Summerfield, Blunsom, Kavukcuoglu, and Hassabis]{graves2016hybrid}
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska{-}Barwinska, Sergio~Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John~P. Agapiou, Adri{\`{a}}~Puigdom{\`{e}}nech Badia, Karl~Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 538\penalty0 (7626):\penalty0 471--476, 2016.
\newblock \doi{10.1038/nature20101}.
\newblock URL \url{https://doi.org/10.1038/nature20101}.

\bibitem[Greff et~al.(2020)Greff, van Steenkiste, and Schmidhuber]{greff2020binding}
Klaus Greff, Sjoerd van Steenkiste, and J{\"{u}}rgen Schmidhuber.
\newblock On the binding problem in artificial neural networks.
\newblock \emph{CoRR}, abs/2012.05208, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.05208}.

\bibitem[Griffiths \& Ghahramani(2011)Griffiths and Ghahramani]{griffiths2011indian}
Thomas~L Griffiths and Zoubin Ghahramani.
\newblock The indian buffet process: An introduction and review.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (4), 2011.
\newblock URL \url{https://www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf}.

\bibitem[Gueta et~al.(2023)Gueta, Venezian, Raffel, Slonim, Katz, and Choshen]{Gueta2023Knowledge}
Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.
\newblock Knowledge is a region in weight space for fine-tuned language models.
\newblock \emph{CoRR}, abs/2302.04863, 2023.
\newblock \doi{10.48550/arXiv.2302.04863}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.04863}.

\bibitem[Guo et~al.(2021)Guo, Rush, and Kim]{guo-etal-2021-parameter}
Demi Guo, Alexander Rush, and Yoon Kim.
\newblock Parameter-efficient transfer learning with diff pruning.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  4884--4896, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.378}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.378}.

\bibitem[Guo et~al.(2018)Guo, Shah, and Barzilay]{Guo:2018emnlp}
Jiang Guo, Darsh Shah, and Regina Barzilay.
\newblock Multi-source domain adaptation with mixture of experts.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pp.\  4694--4703, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1498}.
\newblock URL \url{https://aclanthology.org/D18-1498}.

\bibitem[Gupta et~al.(2022)Gupta, Mukherjee, Subudhi, Gonzalez, Jose, Awadallah, and Gao]{Gupta2022Sparsely}
Shashank Gupta, Subhabrata Mukherjee, Krishan Subudhi, Eduardo Gonzalez, Damien Jose, Ahmed~Hassan Awadallah, and Jianfeng Gao.
\newblock Sparsely activated mixture-of-experts are robust multi-task learners.
\newblock \emph{CoRR}, abs/2204.07689, 2022.
\newblock \doi{10.48550/arXiv.2204.07689}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2204.07689}.

\bibitem[Gupta et~al.(2020)Gupta, Serrano, and DeCoste]{Gupta2020Stochastic}
Vipul Gupta, Santiago~Akle Serrano, and Dennis DeCoste.
\newblock Stochastic weight averaging in parallel: Large-batch training that generalizes well.
\newblock \emph{CoRR}, abs/2001.02312, 2020.
\newblock URL \url{http://arxiv.org/abs/2001.02312}.

\bibitem[Gururangan et~al.(2022)Gururangan, Lewis, Holtzman, Smith, and Zettlemoyer]{Gururangan2022Demix}
Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah~A. Smith, and Luke Zettlemoyer.
\newblock {DEM}ix layers: Disentangling domains for modular language modeling.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  5557--5576, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.407}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.407}.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020retrieval}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
\newblock Retrieval augmented language model pre-training.
\newblock In \emph{International conference on machine learning}, pp.\  3929--3938. PMLR, 2020.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{Ha2017HyperNetworks}
David Ha, Andrew~M. Dai, and Quoc~V. Le.
\newblock Hypernetworks.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=rkpACe1lx}.

\bibitem[Hambardzumyan et~al.(2021)Hambardzumyan, Khachatrian, and May]{hambardzumyan-etal-2021-warp}
Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May.
\newblock {WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  4921--4933, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.381}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.381}.

\bibitem[Hampshire \& Waibel(1992)Hampshire and Waibel]{hampshire1992meta}
John~B. Hampshire and Alex Waibel.
\newblock The meta-pi network: Building distributed knowledge representations for robust multisource pattern recognition.
\newblock \emph{{IEEE} Transactions on Pattern Analysis and Machine Intelligence}, 14\penalty0 (7):\penalty0 751--769, 1992.
\newblock \doi{10.1109/34.142911}.
\newblock URL \url{https://doi.org/10.1109/34.142911}.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William~J. Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada}, pp.\  1135--1143, 2015.
\newblock URL \url{https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html}.

\bibitem[Han et~al.(2017)Han, Pool, Narang, Mao, Gong, Tang, Elsen, Vajda, Paluri, Tran, Catanzaro, and Dally]{han2016dsd}
Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong, Shijian Tang, Erich Elsen, Peter Vajda, Manohar Paluri, John Tran, Bryan Catanzaro, and William~J. Dally.
\newblock {DSD:} dense-sparse-dense training for deep neural networks.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=HyoST\_9xl}.

\bibitem[Han et~al.(2021)Han, Pang, and Wu]{han-etal-2021-robust}
Wenjuan Han, Bo~Pang, and Ying~Nian Wu.
\newblock Robust transfer learning with pretrained language models through adapters.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)}, pp.\  854--861, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-short.108}.
\newblock URL \url{https://aclanthology.org/2021.acl-short.108}.

\bibitem[Hazimeh et~al.(2021)Hazimeh, Zhao, Chowdhery, Sathiamoorthy, Chen, Mazumder, Hong, and Chi]{hazimeh2021dselect}
Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, and Ed~H. Chi.
\newblock Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  29335--29347, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/f5ac21cd0ef1b88e9848571aeb53551a-Abstract.html}.

\bibitem[He et~al.(2022{\natexlab{a}})He, Zhou, Ma, Berg{-}Kirkpatrick, and Neubig]{He2021UnifiedAdapters}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg{-}Kirkpatrick, and Graham Neubig.
\newblock Towards a unified view of parameter-efficient transfer learning.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=0RDcd5Axok}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part {IV}}, volume 9908 of \emph{Lecture Notes in Computer Science}, pp.\  630--645. Springer, 2016.
\newblock \doi{10.1007/978-3-319-46493-0\_38}.
\newblock URL \url{https://doi.org/10.1007/978-3-319-46493-0\_38}.

\bibitem[He et~al.(2021)He, Liu, Ye, Tan, Ding, Cheng, Low, Bing, and Si]{he-etal-2021-effectiveness}
Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, and Luo Si.
\newblock On the effectiveness of adapter-based tuning for pretrained language model adaptation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  2208--2222, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.172}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.172}.

\bibitem[He et~al.(2022{\natexlab{b}})He, Li, Zhang, Yang, and Wang]{He:2022arxiv}
Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin~Eric Wang.
\newblock Parameter-efficient fine-tuning for vision transformers.
\newblock \emph{CoRR}, abs/2203.16329, 2022{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2203.16329}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2203.16329}.

\bibitem[He et~al.(2022{\natexlab{c}})He, Zheng, Tay, Gupta, Du, Aribandi, Zhao, Li, Chen, Metzler, Cheng, and Chi]{he2022hyperprompt}
Yun He, Huaixiu~Steven Zheng, Yi~Tay, Jai~Prakash Gupta, Yu~Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, Heng{-}Tze Cheng, and Ed~H. Chi.
\newblock Hyperprompt: Prompt-based task-conditioning of transformers.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  8678--8690. {PMLR}, 2022{\natexlab{c}}.
\newblock URL \url{https://proceedings.mlr.press/v162/he22f.html}.

\bibitem[Heess et~al.(2016)Heess, Wayne, Tassa, Lillicrap, Riedmiller, and Silver]{heess2016learning}
Nicolas Heess, Gregory Wayne, Yuval Tassa, Timothy~P. Lillicrap, Martin~A. Riedmiller, and David Silver.
\newblock Learning and transfer of modulated locomotor controllers.
\newblock \emph{CoRR}, abs/1610.05182, 2016.
\newblock URL \url{http://arxiv.org/abs/1610.05182}.

\bibitem[Hou et~al.(2022)Hou, Zhu, Wang, Wang, Qin, Xu, and Shinozaki]{Hou:2022speech}
Wenxin Hou, Han Zhu, Yidong Wang, Jindong Wang, Tao Qin, Renjun Xu, and Takahiro Shinozaki.
\newblock Exploiting adapters for cross-lingual low-resource speech recognition.
\newblock \emph{{IEEE} {ACM} Transactions on Audio Speech Language Processing}, 30:\penalty0 317--329, 2022.
\newblock \doi{10.1109/TASLP.2021.3138674}.
\newblock URL \url{https://doi.org/10.1109/TASLP.2021.3138674}.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, de~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  2790--2799. {PMLR}, 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/houlsby19a.html}.

\bibitem[Howard \& Ruder(2018)Howard and Ruder]{Howard2018ulmfit}
Jeremy Howard and Sebastian Ruder.
\newblock Universal language model fine-tuning for text classification.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  328--339, Melbourne, Australia, 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P18-1031}.
\newblock URL \url{https://aclanthology.org/P18-1031}.

\bibitem[Hsieh et~al.(2022)Hsieh, Ghosh, and Ginsburg]{Hsieh:2022tts}
Cheng{-}Ping Hsieh, Subhankar Ghosh, and Boris Ginsburg.
\newblock Adapter-based extension of multi-speaker text-to-speech model for new speakers.
\newblock \emph{CoRR}, abs/2211.00585, 2022.
\newblock \doi{10.48550/arXiv.2211.00585}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2211.00585}.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Hu et~al.(2020)Hu, Ruder, Siddhant, Neubig, Firat, and Johnson]{Hu:2020xtreme}
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson.
\newblock {XTREME:} {A} massively multilingual multi-task benchmark for evaluating cross-lingual generalisation.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, pp.\  4411--4421, 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/hu20b.html}.

\bibitem[Hu et~al.(2017)Hu, Andreas, Rohrbach, Darrell, and Saenko]{Hu_2017_ICCV}
Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko.
\newblock Learning to reason: End-to-end module networks for visual question answering.
\newblock In \emph{{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice, Italy, October 22-29, 2017}, pp.\  804--813. {IEEE} Computer Society, 2017.
\newblock \doi{10.1109/ICCV.2017.93}.
\newblock URL \url{https://doi.org/10.1109/ICCV.2017.93}.

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and Mordatch]{huang2022language}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  9118--9147. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/huang22a.html}.

\bibitem[Hupkes et~al.(2020)Hupkes, Dankers, Mul, and Bruni]{hupkes2020compositionality}
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni.
\newblock Compositionality decomposed: How do neural networks generalise?
\newblock \emph{Journal of Artificial Intelligence Research}, 67:\penalty0 757--795, 2020.
\newblock \doi{10.1613/jair.1.11674}.
\newblock URL \url{https://doi.org/10.1613/jair.1.11674}.

\bibitem[Ilharco et~al.(2022)Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt, Hajishirzi, and Farhadi]{Ilharco2022EditingModelsTaskArith}
Gabriel Ilharco, Marco~T{\'{u}}lio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic.
\newblock \emph{CoRR}, abs/2212.04089, 2022.
\newblock \doi{10.48550/arXiv.2212.04089}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2212.04089}.

\bibitem[Jacobs et~al.(1991{\natexlab{a}})Jacobs, Jordan, and Barto]{jacobs1991task}
Robert~A. Jacobs, Michael~I. Jordan, and Andrew~G. Barto.
\newblock Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks.
\newblock \emph{Cognitive Science}, 15\penalty0 (2):\penalty0 219--250, 1991{\natexlab{a}}.
\newblock \doi{10.1207/s15516709cog1502\_2}.
\newblock URL \url{https://doi.org/10.1207/s15516709cog1502\_2}.

\bibitem[Jacobs et~al.(1991{\natexlab{b}})Jacobs, Jordan, Nowlan, and Hinton]{jacobs1991adaptive}
Robert~A. Jacobs, Michael~I. Jordan, Steven~J. Nowlan, and Geoffrey~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural Computation}, 3\penalty0 (1):\penalty0 79--87, 1991{\natexlab{b}}.
\newblock \doi{10.1162/neco.1991.3.1.79}.
\newblock URL \url{https://doi.org/10.1162/neco.1991.3.1.79}.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{Jang2017Gumbel}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=rkE3y85ee}.

\bibitem[Jang et~al.(2023)Jang, Kim, Ye, Kim, Logeswaran, Lee, Lee, and Seo]{jang2023exploring}
Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo.
\newblock Exploring the benefits of training expert language models over instruction tuning, 2023.

\bibitem[Javaloy \& Valera(2022)Javaloy and Valera]{Javaloy2022Rotograd}
Adri{\'{a}}n Javaloy and Isabel Valera.
\newblock Rotograd: Gradient homogenization in multitask learning.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=T8wHz4rnuGL}.

\bibitem[Jiang et~al.(2019)Jiang, Gu, Murphy, and Finn]{jiang2019language}
Yiding Jiang, Shixiang Gu, Kevin Murphy, and Chelsea Finn.
\newblock Language as an abstraction for hierarchical deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  9414--9426, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/0af787945872196b42c9f73ead2565c8-Abstract.html}.

\bibitem[Jin et~al.(2023)Jin, Ren, Preotiuc-Pietro, and Cheng]{jin2023dataless}
Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng.
\newblock Dataless knowledge fusion by merging weights of language models, 2023.

\bibitem[Jordan \& Jacobs(1994)Jordan and Jacobs]{Jordan1994Hierarchical}
Michael~I. Jordan and Robert~A. Jacobs.
\newblock Hierarchical mixtures of experts and the {EM} algorithm.
\newblock \emph{Neural Computation}, 6\penalty0 (2):\penalty0 181--214, 1994.
\newblock \doi{10.1162/neco.1994.6.2.181}.
\newblock URL \url{https://doi.org/10.1162/neco.1994.6.2.181}.

\bibitem[Kandpal et~al.(2023)Kandpal, Lester, Muqeeth, Mascarenhas, Evans, Baskaran, Huang, Liu, and Raffel]{kandpal2023gittheta}
Nikhil Kandpal, Brian Lester, Mohammed Muqeeth, Anisha Mascarenhas, Monty Evans, Vishal Baskaran, Tenghao Huang, Haokun Liu, and Colin Raffel.
\newblock Git-theta: A git extension for collaborative development of machine learning models, 2023.

\bibitem[Kannan et~al.(2019)Kannan, Datta, Sainath, Weinstein, Ramabhadran, Wu, Bapna, Chen, and Lee]{Kannan:2019interspeech}
Anjuli Kannan, Arindrima Datta, Tara~N. Sainath, Eugene Weinstein, Bhuvana Ramabhadran, Yonghui Wu, Ankur Bapna, Zhifeng Chen, and Seungji Lee.
\newblock Large-scale multilingual speech recognition with a streaming end-to-end model.
\newblock In \emph{Proceedings of Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019}, pp.\  2130--2134, 2019.
\newblock \doi{10.21437/Interspeech.2019-2858}.
\newblock URL \url{https://doi.org/10.21437/Interspeech.2019-2858}.

\bibitem[Kashtan \& Alon(2005)Kashtan and Alon]{kashtan2005spontaneous}
Nadav Kashtan and Uri Alon.
\newblock Spontaneous evolution of modularity and network motifs.
\newblock \emph{Proceedings of the National Academy of Sciences (PNAS)}, 102\penalty0 (39):\penalty0 13773--13778, 2005.
\newblock \doi{https://doi.org/10.1073/pnas.0503610102}.
\newblock URL \url{https://doi.org/10.1073/pnas.0503610102}.

\bibitem[Ke et~al.(2019)Ke, Bilaniuk, Goyal, Bauer, Larochelle, Pal, and Bengio]{ke2019learning}
Nan~Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Chris Pal, and Yoshua Bengio.
\newblock Learning neural causal models from unknown interventions.
\newblock \emph{CoRR}, abs/1910.01075, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.01075}.

\bibitem[Ke et~al.(2021{\natexlab{a}})Ke, Didolkar, Mittal, Goyal, Lajoie, Bauer, Rezende, Mozer, Bengio, and Pal]{ke2021systematic}
Nan~Rosemary Ke, Aniket Didolkar, Sarthak Mittal, Anirudh Goyal, Guillaume Lajoie, Stefan Bauer, Danilo~Jimenez Rezende, Michael Mozer, Yoshua Bengio, and Chris Pal.
\newblock Systematic evaluation of causal discovery in visual model based reinforcement learning.
\newblock In Joaquin Vanschoren and Sai{-}Kit Yeung (eds.), \emph{Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual}, 2021{\natexlab{a}}.
\newblock URL \url{https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/8f121ce07d74717e0b1f21d122e04521-Abstract-round2.html}.

\bibitem[Ke et~al.(2021{\natexlab{b}})Ke, Xu, and Liu]{ke-etal-2021-adapting}
Zixuan Ke, Hu~Xu, and Bing Liu.
\newblock Adapting {BERT} for continual learning of a sequence of aspect sentiment classification tasks.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  4746--4755, Online, June 2021{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.378}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.378}.

\bibitem[Khanuja et~al.(2021)Khanuja, Johnson, and Talukdar]{Khanuja2021}
Simran Khanuja, Melvin Johnson, and Partha Talukdar.
\newblock {M}erge{D}istill: {M}erging language models using pre-trained distillation.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pp.\  2874--2887, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-acl.254}.
\newblock URL \url{https://aclanthology.org/2021.findings-acl.254}.

\bibitem[Kim et~al.(2017)Kim, Stratos, and Kim]{kim-etal-2017-adversarial}
Young-Bum Kim, Karl Stratos, and Dongchan Kim.
\newblock Adversarial adaptation of synthetic or stale data.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1297--1307, Vancouver, Canada, July 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1119}.
\newblock URL \url{https://aclanthology.org/P17-1119}.

\bibitem[Kirkpatrick et~al.(1983)Kirkpatrick, Gelatt~Jr, and Vecchi]{kirkpatrick1983optimization}
Scott Kirkpatrick, C.~Daniel Gelatt~Jr, and Mario~P. Vecchi.
\newblock Optimization by simulated annealing.
\newblock \emph{Science}, 220\penalty0 (4598):\penalty0 671--680, 1983.
\newblock \doi{https://doi.org/10.1126/science.220.4598.671}.
\newblock URL \url{https://doi.org/10.1126/science.220.4598.671}.

\bibitem[Kirsch et~al.(2018)Kirsch, Kunze, and Barber]{kirsch2018modular}
Louis Kirsch, Julius Kunze, and David Barber.
\newblock Modular networks: Learning to decompose neural computation.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada}, pp.\  2414--2423, 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper/2018/hash/310ce61c90f3a46e340ee8257bc70e93-Abstract.html}.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 22199--22213, 2022.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and Hinton]{Kornblith2019SimilairtyofNN}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey~E. Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  3519--3529. {PMLR}, 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/kornblith19a.html}.

\bibitem[Kudugunta et~al.(2021)Kudugunta, Huang, Bapna, Krikun, Lepikhin, Luong, and Firat]{kudugunta2021beyond}
Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat.
\newblock Beyond distillation: Task-level mixture-of-experts for efficient inference.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pp.\  3577--3599, 2021.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.304}.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and Tenenbaum]{kulkarni2016hierarchical}
Tejas~D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.
\newblock Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain}, pp.\  3675--3683, 2016.
\newblock URL \url{https://proceedings.neurips.cc/paper/2016/hash/f442d33fa06832082290ad8544a8da27-Abstract.html}.

\bibitem[Kumatani et~al.(2021)Kumatani, Gmyr, Salinas, Liu, Zuo, Patel, Sun, and Shi]{Kumatani:2021arxiv}
Ken'ichi Kumatani, Robert Gmyr, Felipe~Cruz Salinas, Linquan Liu, Wei Zuo, Devang Patel, Eric Sun, and Yu~Shi.
\newblock Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition.
\newblock \emph{CoRR}, abs/2112.05820, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.05820}.

\bibitem[Lake \& Baroni(2018)Lake and Baroni]{lake2018generalization}
Brenden~M. Lake and Marco Baroni.
\newblock Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  2879--2888. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/lake18a.html}.

\bibitem[Lancucki(2021)]{Lancucki:2021fastpitch}
Adrian Lancucki.
\newblock Fastpitch: Parallel text-to-speech with pitch prediction.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021}, pp.\  6588--6592. {IEEE}, 2021.
\newblock \doi{10.1109/ICASSP39728.2021.9413889}.
\newblock URL \url{https://doi.org/10.1109/ICASSP39728.2021.9413889}.

\bibitem[Lange et~al.(2022)Lange, Rolnick, and Kording]{lange2022clustering}
Richard~D. Lange, David~S. Rolnick, and Konrad~P. Kording.
\newblock Clustering units in neural networks: upstream vs downstream information.
\newblock \emph{CoRR}, abs/2203.11815, 2022.
\newblock \doi{10.48550/arXiv.2203.11815}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2203.11815}.

\bibitem[Lauscher et~al.(2020)Lauscher, Majewska, Ribeiro, Gurevych, Rozanov, and Glava{\v{s}}]{lauscher-etal-2020-common}
Anne Lauscher, Olga Majewska, Leonardo F.~R. Ribeiro, Iryna Gurevych, Nikolai Rozanov, and Goran Glava{\v{s}}.
\newblock Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers.
\newblock In \emph{Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures}, pp.\  43--49, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.deelio-1.5}.
\newblock URL \url{https://aclanthology.org/2020.deelio-1.5}.

\bibitem[Lauscher et~al.(2021)Lauscher, Lueken, and Glava{\v{s}}]{lauscher-etal-2021-sustainable-modular}
Anne Lauscher, Tobias Lueken, and Goran Glava{\v{s}}.
\newblock Sustainable modular debiasing of language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pp.\  4782--4797, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-emnlp.411}.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.411}.

\bibitem[Le et~al.(2021)Le, Pino, Wang, Gu, Schwab, and Besacier]{le-etal-2021-lightweight}
Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier Schwab, and Laurent Besacier.
\newblock Lightweight adapter tuning for multilingual speech translation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)}, pp.\  817--824, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-short.103}.
\newblock URL \url{https://aclanthology.org/2021.acl-short.103}.

\bibitem[Le et~al.(2014)Le, Sarl{\'{o}}s, and Smola]{le2013fastfood}
Quoc~Viet Le, Tam{\'{a}}s Sarl{\'{o}}s, and Alexander~Johannes Smola.
\newblock Fastfood: Approximate kernel expansions in loglinear time.
\newblock \emph{CoRR}, abs/1408.3060, 2014.
\newblock URL \url{http://arxiv.org/abs/1408.3060}.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{Lepikhin2021GShard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and automatic sharding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=qrwe7XHTmYb}.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{Lester2021prompttuning}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  3045--3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.243}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.243}.

\bibitem[Lewis et~al.(2021)Lewis, Bhosale, Dettmers, Goyal, and Zettlemoyer]{lewis2021base}
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.
\newblock {BASE} layers: Simplifying training of large, sparse models.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  6265--6274. {PMLR}, 2021.
\newblock URL \url{http://proceedings.mlr.press/v139/lewis21a.html}.

\bibitem[Lewis et~al.(2020{\natexlab{a}})Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9459--9474, 2020{\natexlab{a}}.

\bibitem[Lewis et~al.(2020{\natexlab{b}})Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"{u}}ttler, Lewis, Yih, Rockt{\"{a}}schel, Riedel, and Kiela]{Lewis:2020rag}
Patrick S.~H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"{u}}ttler, Mike Lewis, Wen{-}tau Yih, Tim Rockt{\"{a}}schel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020{\natexlab{b}}.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html}.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Yu, Khabsa, Zettlemoyer, Halevy, and Andreas]{li-etal-2022-quantifying}
Belinda Li, Jane Yu, Madian Khabsa, Luke Zettlemoyer, Alon Halevy, and Jacob Andreas.
\newblock Quantifying adaptability in pre-trained language models with 500 tasks.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  4696--4715, Seattle, United States, July 2022{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.346}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.346}.

\bibitem[Li et~al.(2018)Li, Farkhoor, Liu, and Yosinski]{Li2018intrinsic}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock In \emph{6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=ryup8-WCW}.

\bibitem[Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and Hoi]{li2021align}
Junnan Li, Ramprasaath~R. Selvaraju, Akhilesh Gotmare, Shafiq~R. Joty, Caiming Xiong, and Steven~Chu{-}Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with momentum distillation.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  9694--9705, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/505259756244493872b7709a8a01b536-Abstract.html}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Gururangan, Dettmers, Lewis, Althoff, Smith, and Zettlemoyer]{Li2022BranchTrainMerge}
Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah~A. Smith, and Luke Zettlemoyer.
\newblock Branch-train-merge: Embarrassingly parallel training of expert language models.
\newblock \emph{CoRR}, abs/2208.03306, 2022{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2208.03306}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2208.03306}.

\bibitem[Li \& Liang(2021)Li and Liang]{Li2020PrefixTuning}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  4582--4597, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.353}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.353}.

\bibitem[Liang et~al.(2021)Liang, Zhao, Wang, Qiu, and Li]{Liang:2020aaai}
Jianze Liang, Chengqi Zhao, Mingxuan Wang, Xipeng Qiu, and Lei Li.
\newblock Finding sparse structures for domain specific neural machine translation.
\newblock In \emph{Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9, 2021}, pp.\  13333--13342. {AAAI} Press, 2021.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/17574}.

\bibitem[Lin et~al.(2021)Lin, Wu, Wang, and Li]{lin-etal-2021-learning}
Zehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li.
\newblock Learning language specific sub-network for multilingual machine translation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  293--305, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.25}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.25}.

\bibitem[Litschko et~al.(2022)Litschko, Vuli{\'c}, and Glava{\v{s}}]{litschko-etal-2022-parameter}
Robert Litschko, Ivan Vuli{\'c}, and Goran Glava{\v{s}}.
\newblock Parameter-efficient neural reranking for cross-lingual and multilingual retrieval.
\newblock In \emph{Proceedings of the 29th International Conference on Computational Linguistics}, pp.\  1071--1082, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.coling-1.90}.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Pfeiffer, Korhonen, Vuli\'{c}, and Gurevych]{Liu:2022delving}
Chen Liu, Jonas Pfeiffer, Anna Korhonen, Ivan Vuli\'{c}, and Iryna Gurevych.
\newblock Delving deeper into cross-lingual visual question answering.
\newblock \emph{CoRR}, abs/2202.07630, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2202.07630}.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Bugliarello, Ponti, Reddy, Collier, and Elliott]{liu-etal-2021-visually}
Fangyu Liu, Emanuele Bugliarello, Edoardo~Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott.
\newblock Visually grounded reasoning across languages and cultures.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  10467--10485, Online and Punta Cana, Dominican Republic, November 2021{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.818}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.818}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Tam, Muqeeth, Mohta, Huang, Bansal, and Raffel]{Liu2022IA3}
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel.
\newblock Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.
\newblock \emph{CoRR}, abs/2205.05638, 2022{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2205.05638}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.05638}.

\bibitem[Liu et~al.(2023)Liu, Yuan, Fu, Jiang, Hayashi, and Neubig]{Liu:2021survey}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
\newblock Pre-train, prompt, and predict: {A} systematic survey of prompting methods in natural language processing.
\newblock \emph{{ACM} Computing Surveys}, 55\penalty0 (9):\penalty0 195:1--195:35, 2023.
\newblock \doi{10.1145/3560815}.
\newblock URL \url{https://doi.org/10.1145/3560815}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Zheng, Du, Ding, Qian, Yang, and Tang]{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang.
\newblock {GPT} understands, too.
\newblock \emph{CoRR}, abs/2103.10385, 2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2103.10385}.

\bibitem[Liu et~al.(2022{\natexlab{c}})Liu, Ji, Fu, Tam, Du, Yang, and Tang]{liu-etal-2022-p}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.
\newblock {P}-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  61--68, Dublin, Ireland, May 2022{\natexlab{c}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-short.8}.
\newblock URL \url{https://aclanthology.org/2022.acl-short.8}.

\bibitem[Liu et~al.(2019)Liu, He, Chen, and Gao]{liu2019multi}
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pp.\  4487--4496, Florence, Italy, July 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1441}.
\newblock URL \url{https://aclanthology.org/P19-1441}.

\bibitem[Liu et~al.(2020)Liu, Gu, Goyal, Li, Edunov, Ghazvininejad, Lewis, and Zettlemoyer]{liu-etal-2020-multilingual-denoising}
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.
\newblock Multilingual denoising pre-training for neural machine translation.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:\penalty0 726--742, 2020.
\newblock \doi{10.1162/tacl_a_00343}.
\newblock URL \url{https://aclanthology.org/2020.tacl-1.47}.

\bibitem[Locatello et~al.(2020)Locatello, Weissenborn, Unterthiner, Mahendran, Heigold, Uszkoreit, Dosovitskiy, and Kipf]{NEURIPS2020_8511df98}
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf.
\newblock Object-centric learning with slot attention.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html}.

\bibitem[Lu et~al.(2021)Lu, Dou, and Nguyen]{Lu:2021knowledge}
Qiuhao Lu, Dejing Dou, and Thien~Huu Nguyen.
\newblock Parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pp.\  3855--3865, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-emnlp.325}.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.325}.

\bibitem[Lu et~al.(2022)Lu, Bartolo, Moore, Riedel, and Stenetorp]{lu-etal-2022-fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  8086--8098, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.556}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.556}.

\bibitem[Lu et~al.(2017)Lu, Kumar, Zhai, Cheng, Javidi, and Feris]{lu2017fully}
Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu~Cheng, Tara Javidi, and Rog{\'{e}}rio~Schmidt Feris.
\newblock Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pp.\  1131--1140. {IEEE} Computer Society, 2017.
\newblock \doi{10.1109/CVPR.2017.126}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2017.126}.

\bibitem[Ma et~al.(2018)Ma, Zhao, Yi, Chen, Hong, and Chi]{Ma2018Modeling}
Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed~H. Chi.
\newblock Modeling task relationships in multi-task learning with multi-gate mixture-of-experts.
\newblock In \emph{Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery {\&} Data Mining, {KDD} 2018, London, UK, August 19-23, 2018}, pp.\  1930--1939. {ACM}, 2018.
\newblock \doi{10.1145/3219819.3220007}.
\newblock URL \url{https://doi.org/10.1145/3219819.3220007}.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{maddison2017the}
Chris~J. Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: {A} continuous relaxation of discrete random variables.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=S1jE5L5gl}.

\bibitem[Mahabadi et~al.(2021{\natexlab{a}})Mahabadi, Henderson, and Ruder]{Mahabadi2021Compacter}
Rabeeh~Karimi Mahabadi, James Henderson, and Sebastian Ruder.
\newblock Compacter: Efficient low-rank hypercomplex adapter layers.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  1022--1035, 2021{\natexlab{a}}.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/081be9fdff07f3bc808f935906ef70c0-Abstract.html}.

\bibitem[Mahabadi et~al.(2021{\natexlab{b}})Mahabadi, Ruder, Dehghani, and Henderson]{mahabadi2021parameter}
Rabeeh~Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson.
\newblock Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  565--576, Online, August 2021{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.47}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.47}.

\bibitem[Maharana et~al.(2022)Maharana, Hannan, and Bansal]{Maharana:2022modal}
Adyasha Maharana, Darryl Hannan, and Mohit Bansal.
\newblock Storydall-e: Adapting pretrained text-to-image transformers for story continuation.
\newblock In \emph{Computer Vision - {ECCV} 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part {XXXVII}}, volume 13697 of \emph{Lecture Notes in Computer Science}, pp.\  70--87. Springer, 2022.
\newblock \doi{10.1007/978-3-031-19836-6\_5}.
\newblock URL \url{https://doi.org/10.1007/978-3-031-19836-6\_5}.

\bibitem[Majewska et~al.(2021)Majewska, Vuli{\'c}, Glava{\v{s}}, Ponti, and Korhonen]{majewska-etal-2021-verb}
Olga Majewska, Ivan Vuli{\'c}, Goran Glava{\v{s}}, Edoardo~Maria Ponti, and Anna Korhonen.
\newblock Verb knowledge injection for multilingual event processing.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  6952--6969, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.541}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.541}.

\bibitem[Mallya \& Lazebnik(2018)Mallya and Lazebnik]{Mallya2018packnet}
Arun Mallya and Svetlana Lazebnik.
\newblock Packnet: Adding multiple tasks to a single network by iterative pruning.
\newblock In \emph{2018 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018}, pp.\  7765--7773. Computer Vision Foundation / {IEEE} Computer Society, 2018.
\newblock \doi{10.1109/CVPR.2018.00810}.
\newblock URL \url{http://openaccess.thecvf.com/content\_cvpr\_2018/html/Mallya\_PackNet\_Adding\_Multiple\_CVPR\_2018\_paper.html}.

\bibitem[Mallya et~al.(2018)Mallya, Davis, and Lazebnik]{mallya2018piggyback}
Arun Mallya, Dillon Davis, and Svetlana Lazebnik.
\newblock Piggyback: Adapting a single network to multiple tasks by learning to mask weights.
\newblock In \emph{Computer Vision - {ECCV} 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part {IV}}, volume 11208 of \emph{Lecture Notes in Computer Science}, pp.\  72--88. Springer, 2018.
\newblock \doi{10.1007/978-3-030-01225-0\_5}.
\newblock URL \url{https://doi.org/10.1007/978-3-030-01225-0\_5}.

\bibitem[Matena \& Raffel(2021)Matena and Raffel]{Matena2022}
Michael Matena and Colin Raffel.
\newblock Merging models with fisher-weighted averaging.
\newblock \emph{CoRR}, abs/2111.09832, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.09832}.

\bibitem[McCloskey \& Cohen(1989)McCloskey and Cohen]{mccloskey1989catastrophicinterference}
Michael McCloskey and Neal~J. Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential learning problem.
\newblock In \emph{Psychology of Learning and Motivation}, volume~24, pp.\  109--165. Elsevier, 1989.
\newblock \doi{https://doi.org/10.1016/S0079-7421(08)60536-8}.
\newblock URL \url{https://doi.org/10.1016/S0079-7421(08)60536-8}.

\bibitem[Mehta(2019)]{Mehta2019}
Rahul Mehta.
\newblock Sparse transfer learning via winning lottery tickets.
\newblock \emph{CoRR}, abs/1905.07785, 2019.
\newblock URL \url{http://arxiv.org/abs/1905.07785}.

\bibitem[Meyerson \& Miikkulainen(2018)Meyerson and Miikkulainen]{Meyerson2018}
Elliot Meyerson and Risto Miikkulainen.
\newblock {Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering}.
\newblock In \emph{Proceedings of ICLR 2018}, 2018.

\bibitem[Meyes et~al.(2020)Meyes, de~Puiseau, Posada{-}Moreno, and Meisen]{meyes2020under}
Richard Meyes, Constantin~Waubert de~Puiseau, Andres Posada{-}Moreno, and Tobias Meisen.
\newblock Under the hood of neural networks: Characterizing learned representations by functional neuron populations and network ablations.
\newblock \emph{CoRR}, abs/2004.01254, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.01254}.

\bibitem[Mialon et~al.(2023)Mialon, Dess{\`{\i}}, Lomeli, Nalmpantis, Pasunuru, Raileanu, Rozi{\`{e}}re, Schick, Dwivedi{-}Yu, Celikyilmaz, Grave, LeCun, and Scialom]{auglms:2023}
Gr{\'{e}}goire Mialon, Roberto Dess{\`{\i}}, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozi{\`{e}}re, Timo Schick, Jane Dwivedi{-}Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom.
\newblock Augmented language models: A survey.
\newblock \emph{CoRR}, abs/2302.07842, 2023.
\newblock \doi{10.48550/arXiv.2302.07842}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.07842}.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  14014--14024, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html}.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and Dean]{Mikolov2013DistributedRepresentations}
Tom{\'{a}}s Mikolov, Ilya Sutskever, Kai Chen, Gregory~S. Corrado, and Jeffrey Dean.
\newblock Distributed representations of words and phrases and their compositionality.
\newblock In \emph{Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States}, pp.\  3111--3119, 2013.
\newblock URL \url{https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html}.

\bibitem[Mishra et~al.(2022)Mishra, Khashabi, Baral, and Hajishirzi]{mishra-etal-2022-cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing instructions.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3470--3487, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.244}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.244}.

\bibitem[Misra et~al.(2016)Misra, Shrivastava, Gupta, and Hebert]{Misra2016}
Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.
\newblock Cross-stitch networks for multi-task learning.
\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pp.\  3994--4003. {IEEE} Computer Society, 2016.
\newblock \doi{10.1109/CVPR.2016.433}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2016.433}.

\bibitem[Mittal et~al.(2020)Mittal, Lamb, Goyal, Voleti, Shanahan, Lajoie, Mozer, and Bengio]{pmlr-v119-mittal20a}
Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie, Michael Mozer, and Yoshua Bengio.
\newblock Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules.
\newblock In Hal~Daumé III and Aarti Singh (eds.), \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  6972--6986. PMLR, 13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/mittal20a.html}.

\bibitem[Mittal et~al.(2022)Mittal, Bengio, and Lajoie]{mittal2022is}
Sarthak Mittal, Yoshua Bengio, and Guillaume Lajoie.
\newblock Is a modular architecture enough?
\newblock \emph{CoRR}, abs/2206.02713, 2022.
\newblock \doi{10.48550/arXiv.2206.02713}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2206.02713}.

\bibitem[Molchanov et~al.(2017)Molchanov, Tyree, Karras, Aila, and Kautz]{Molchanov2017}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient inference.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=SJGCiw5gl}.

\bibitem[Morioka et~al.(2022)Morioka, Zen, Chen, Zhang, and Ding]{Morioka:2022tts}
Nobuyuki Morioka, Heiga Zen, Nanxin Chen, Yu~Zhang, and Yifan Ding.
\newblock Residual adapters for few-shot text-to-speech speaker adaptation.
\newblock \emph{CoRR}, abs/2210.15868, 2022.
\newblock URL \url{https://doi.org/10.48550/arXiv.2210.15868}.

\bibitem[Muqeeth et~al.(2022)Muqeeth, Liu, and Raffel]{Muqeeth2022Models}
Mohammed Muqeeth, Haokun Liu, and Colin Raffel.
\newblock {Models with Conditional Computation Learn Suboptimal Solutions}.
\newblock \emph{arXiv preprint}, 2022.
\newblock URL \url{https://colinraffel.com/publications/icbinb2022models.pdf}.

\bibitem[Muqeeth et~al.(2023)Muqeeth, Liu, and Raffel]{muqeeth2023soft}
Mohammed Muqeeth, Haokun Liu, and Colin Raffel.
\newblock Soft merging of experts with adaptive routing, 2023.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{nachum2018data}
Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada}, pp.\  3307--3317, 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper/2018/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html}.

\bibitem[Nagarajan \& Kolter(2019)Nagarajan and Kolter]{Nagarajan2019UniformConvergence}
Vaishnavh Nagarajan and J.~Zico Kolter.
\newblock Uniform convergence may be unable to explain generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  11611--11622, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/05e97c207235d63ceb1db43c60db7bbb-Abstract.html}.

\bibitem[Nayak et~al.(2022)Nayak, Yu, and Bach]{nayak2022learning}
Nihal~V Nayak, Peilin Yu, and Stephen~H Bach.
\newblock Learning to compose soft prompts for compositional zero-shot learning.
\newblock \emph{arXiv preprint arXiv:2204.03574}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2204.03574.pdf}.

\bibitem[Negrinho et~al.(2019)Negrinho, Gormley, Gordon, Patil, Le, and Ferreira]{negrinho2019towards}
Renato Negrinho, Matthew Gormley, Geoffrey~J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira.
\newblock Towards modular and programmable architecture search.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Newell et~al.(2019)Newell, Jiang, Wang, Li, and Deng]{newell2019feature}
Alejandro Newell, Lu~Jiang, Chong Wang, Li{-}Jia Li, and Jia Deng.
\newblock Feature partitioning for efficient multi-task architectures.
\newblock \emph{CoRR}, abs/1908.04339, 2019.
\newblock URL \url{http://arxiv.org/abs/1908.04339}.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and Zhang]{Neyshabur2020WhatTransfered}
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.
\newblock What is being transferred in transfer learning?
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html}.

\bibitem[Ostapenko et~al.(2021)Ostapenko, Rodr{\'{\i}}guez, Caccia, and Charlin]{ostapenko2021continual}
Oleksiy Ostapenko, Pau Rodr{\'{\i}}guez, Massimo Caccia, and Laurent Charlin.
\newblock Continual learning via local module composition.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  30298--30312, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html}.

\bibitem[Pan et~al.(2022)Pan, Lin, Zhu, Shao, and Li]{Pan:2022modal}
Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li.
\newblock {ST-Adapter: P}arameter-efficient image-to-video transfer learning for action recognition.
\newblock \emph{CoRR}, abs/2206.13559, 2022.
\newblock URL \url{https://doi.org/10.48550/arXiv.2206.13559}.

\bibitem[Papalampidi \& Lapata(2022)Papalampidi and Lapata]{Papalampidi:2022modal}
Pinelopi Papalampidi and Mirella Lapata.
\newblock Hierarchical3d adapters for long video-to-text summarization.
\newblock \emph{CoRR}, abs/2210.04829, 2022.
\newblock \doi{10.48550/arXiv.2210.04829}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2210.04829}.

\bibitem[Parascandolo et~al.(2018)Parascandolo, Kilbertus, Rojas{-}Carulla, and Sch{\"{o}}lkopf]{parascandolo2018learning}
Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas{-}Carulla, and Bernhard Sch{\"{o}}lkopf.
\newblock Learning independent causal mechanisms.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  4033--4041. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/parascandolo18a.html}.

\bibitem[Parovi{\'c} et~al.(2022)Parovi{\'c}, Glava{\v{s}}, Vuli{\'c}, and Korhonen]{Parovic2022BADX}
Marinela Parovi{\'c}, Goran Glava{\v{s}}, Ivan Vuli{\'c}, and Anna Korhonen.
\newblock {BAD}-{X}: Bilingual adapters improve zero-shot cross-lingual transfer.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1791--1799, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.130}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.130}.

\bibitem[Pearl(2009)]{pearl2009causality}
Judea Pearl.
\newblock \emph{Causality}.
\newblock Cambridge University Press, 2009.
\newblock URL \url{https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B}.

\bibitem[Perez et~al.(2018)Perez, Strub, de~Vries, Dumoulin, and Courville]{Perez2018}
Ethan Perez, Florian Strub, Harm de~Vries, Vincent Dumoulin, and Aaron~C. Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018}, pp.\  3942--3951. {AAAI} Press, 2018.
\newblock URL \url{https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16528}.

\bibitem[Pfeiffer et~al.(2020{\natexlab{a}})Pfeiffer, R{\"u}ckl{\'e}, Poth, Kamath, Vuli{\'c}, Ruder, Cho, and Gurevych]{pfeiffer-etal-2020-adapterhub}
Jonas Pfeiffer, Andreas R{\"u}ckl{\'e}, Clifton Poth, Aishwarya Kamath, Ivan Vuli{\'c}, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych.
\newblock {A}dapter{H}ub: A framework for adapting transformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pp.\  46--54, Online, October 2020{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-demos.7}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.7}.

\bibitem[Pfeiffer et~al.(2020{\natexlab{b}})Pfeiffer, Vuli{\'c}, Gurevych, and Ruder]{pfeiffer-etal-2020-mad}
Jonas Pfeiffer, Ivan Vuli{\'c}, Iryna Gurevych, and Sebastian Ruder.
\newblock {MAD-X}: {A}n {A}dapter-{B}ased {F}ramework for {M}ulti-{T}ask {C}ross-{L}ingual {T}ransfer.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  7654--7673, Online, November 2020{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.617}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.617}.

\bibitem[Pfeiffer et~al.(2021{\natexlab{a}})Pfeiffer, Kamath, R{\"u}ckl{\'e}, Cho, and Gurevych]{pfeiffer2020adapterfusion}
Jonas Pfeiffer, Aishwarya Kamath, Andreas R{\"u}ckl{\'e}, Kyunghyun Cho, and Iryna Gurevych.
\newblock {A}dapter{F}usion: Non-destructive task composition for transfer learning.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pp.\  487--503, Online, April 2021{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.eacl-main.39}.
\newblock URL \url{https://aclanthology.org/2021.eacl-main.39}.

\bibitem[Pfeiffer et~al.(2021{\natexlab{b}})Pfeiffer, Vuli{\'c}, Gurevych, and Ruder]{Pfeiffer2021UNKs}
Jonas Pfeiffer, Ivan Vuli{\'c}, Iryna Gurevych, and Sebastian Ruder.
\newblock {UNK}s everywhere: {A}dapting multilingual language models to new scripts.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  10186--10203, Online and Punta Cana, Dominican Republic, November 2021{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.800}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.800}.

\bibitem[Pfeiffer et~al.(2022{\natexlab{a}})Pfeiffer, Geigle, Kamath, Steitz, Roth, Vuli{\'c}, and Gurevych]{pfeiffer-etal-2022-xgqa}
Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-Martin Steitz, Stefan Roth, Ivan Vuli{\'c}, and Iryna Gurevych.
\newblock x{GQA}: Cross-lingual visual question answering.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pp.\  2497--2511, Dublin, Ireland, May 2022{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.findings-acl.196}.
\newblock URL \url{https://aclanthology.org/2022.findings-acl.196}.

\bibitem[Pfeiffer et~al.(2022{\natexlab{b}})Pfeiffer, Goyal, Lin, Li, Cross, Riedel, and Artetxe]{Pfeiffer2022Lifting}
Jonas Pfeiffer, Naman Goyal, Xi~Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.
\newblock Lifting the curse of multilinguality by pre-training modular transformers.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  3479--3495, Seattle, United States, July 2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.255}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.255}.

\bibitem[Pham et~al.(2021)Pham, Crego, and Yvon]{pham-etal-2021-revisiting}
MinhQuang Pham, Josep~Maria Crego, and Fran{\c{c}}ois Yvon.
\newblock Revisiting multi-domain machine translation.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 17--35, 2021.
\newblock \doi{10.1162/tacl_a_00351}.
\newblock URL \url{https://aclanthology.org/2021.tacl-1.2}.

\bibitem[Philip et~al.(2020)Philip, Berard, Gall{\'e}, and Besacier]{philip-etal-2020-monolingual}
Jerin Philip, Alexandre Berard, Matthias Gall{\'e}, and Laurent Besacier.
\newblock Monolingual adapters for zero-shot neural machine translation.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  4465--4470, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.361}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.361}.

\bibitem[Pierrot et~al.(2019)Pierrot, Ligner, Reed, Sigaud, Perrin, Laterre, Kas, Beguir, and de~Freitas]{Pierrot2019alphanpi}
Thomas Pierrot, Guillaume Ligner, Scott~E. Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, and Nando de~Freitas.
\newblock Learning compositional neural programs with recursive tree search and planning.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  14646--14656, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/95b431e51fc53692913da5263c214162-Abstract.html}.

\bibitem[Pilault et~al.(2021)Pilault, Elhattami, and Pal]{pilault2021conditionally}
Jonathan Pilault, Amine Elhattami, and Christopher~J. Pal.
\newblock Conditionally adaptive multi-task learning: Improving transfer learning in {NLP} using fewer parameters {\&} less data.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=de11dbHzAMF}.

\bibitem[Platanios et~al.(2018)Platanios, Sachan, Neubig, and Mitchell]{platanios-etal-2018-contextual}
Emmanouil~Antonios Platanios, Mrinmaya Sachan, Graham Neubig, and Tom Mitchell.
\newblock Contextual parameter generation for universal neural machine translation.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pp.\  425--435, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1039}.
\newblock URL \url{https://aclanthology.org/D18-1039}.

\bibitem[Ponti(2021)]{ponti2021inductive}
Edoardo~M. Ponti.
\newblock \emph{Inductive Bias and Modular Design for Sample-Efficient Neural Language Learning}.
\newblock PhD thesis, University of Cambridge, 2021.
\newblock URL \url{https://doi.org/10.17863/CAM.66424}.

\bibitem[Ponti et~al.(2021)Ponti, Vuli{\'c}, Cotterell, Parovic, Reichart, and Korhonen]{ponti-etal-2021-parameter}
Edoardo~M. Ponti, Ivan Vuli{\'c}, Ryan Cotterell, Marinela Parovic, Roi Reichart, and Anna Korhonen.
\newblock Parameter space factorization for zero-shot learning across tasks and languages.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 410--428, 2021.
\newblock \doi{10.1162/tacl_a_00374}.
\newblock URL \url{https://aclanthology.org/2021.tacl-1.25}.

\bibitem[Ponti et~al.(2022)Ponti, Sordoni, and Reddy]{ponti2022combining}
Edoardo~M. Ponti, Alessandro Sordoni, and Siva Reddy.
\newblock Combining modular skills in multitask learning.
\newblock \emph{CoRR}, abs/2202.13914, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.13914}.

\bibitem[Ponti et~al.(2020)Ponti, Glava{\v{s}}, Majewska, Liu, Vuli{\'c}, and Korhonen]{ponti-etal-2020-xcopa}
Edoardo~Maria Ponti, Goran Glava{\v{s}}, Olga Majewska, Qianchu Liu, Ivan Vuli{\'c}, and Anna Korhonen.
\newblock {XCOPA}: A multilingual dataset for causal commonsense reasoning.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  2362--2376, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.185}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.185}.

\bibitem[Prasanna et~al.(2020)Prasanna, Rogers, and Rumshisky]{prasanna-etal-2020-bert}
Sai Prasanna, Anna Rogers, and Anna Rumshisky.
\newblock {W}hen {BERT} {P}lays the {L}ottery, {A}ll {T}ickets {A}re {W}inning.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  3208--3229, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.259}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.259}.

\bibitem[Precup(2000)]{precup2000temporal}
Doina Precup.
\newblock \emph{Temporal Abstraction in Reinforcement Learning}.
\newblock PhD thesis, University of Massachusetts Amherst, 2000.
\newblock URL \url{https://scholarworks.umass.edu/dissertations/AAI9978540}.

\bibitem[Puigcerver et~al.(2023)Puigcerver, Riquelme, Mustafa, and Houlsby]{puigcerver2023sparse}
Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby.
\newblock From sparse to soft mixtures of experts, 2023.

\bibitem[Rahaman et~al.(2021)Rahaman, Gondal, Joshi, Gehler, Bengio, Locatello, and Sch{\"{o}}lkopf]{rahaman2021dynamic}
Nasim Rahaman, Muhammad~Waleed Gondal, Shruti Joshi, Peter~V. Gehler, Yoshua Bengio, Francesco Locatello, and Bernhard Sch{\"{o}}lkopf.
\newblock Dynamic inference with neural interpreters.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  10985--10998, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/5b4e9aa703d0bfa11041debaa2d1b633-Abstract.html}.

\bibitem[Rajbhandari et~al.(2022)Rajbhandari, Li, Yao, Zhang, Aminabadi, Awan, Rasley, and He]{rajbhandari2022deepspeed}
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza~Yazdani Aminabadi, Ammar~Ahmad Awan, Jeff Rasley, and Yuxiong He.
\newblock Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation {AI} scale.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  18332--18346. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/rajbhandari22a.html}.

\bibitem[Rajendran et~al.(2017)Rajendran, Lakshminarayanan, Khapra, Prasanna, and Ravindran]{rajendran2017adaapt}
Janarthanan Rajendran, Aravind~S. Lakshminarayanan, Mitesh~M. Khapra, P.~Prasanna, and Balaraman Ravindran.
\newblock Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=Sy6iJDqlx}.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{Rebuffi2017Adapters1}
Sylvestre{-}Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\  506--516, 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html}.

\bibitem[Rebuffi et~al.(2018)Rebuffi, Bilen, and Vedaldi]{Rebuffi2018Adapters2}
Sylvestre{-}Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Efficient parametrization of multi-domain deep neural networks.
\newblock In \emph{2018 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018}, pp.\  8119--8127. Computer Vision Foundation / {IEEE} Computer Society, 2018.
\newblock \doi{10.1109/CVPR.2018.00847}.
\newblock URL \url{http://openaccess.thecvf.com/content\_cvpr\_2018/html/Rebuffi\_Efficient\_Parametrization\_of\_CVPR\_2018\_paper.html}.

\bibitem[Reed \& de~Freitas(2016)Reed and de~Freitas]{Reed2016npi}
Scott~E. Reed and Nando de~Freitas.
\newblock Neural programmer-interpreters.
\newblock In \emph{4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings}, 2016.
\newblock URL \url{http://arxiv.org/abs/1511.06279}.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov, Barth{-}Maron, Gimenez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi, Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and de~Freitas]{reed2022generalist}
Scott~E. Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander Novikov, Gabriel Barth{-}Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost~Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de~Freitas.
\newblock A generalist agent.
\newblock \emph{CoRR}, abs/2205.06175, 2022.
\newblock \doi{10.48550/arXiv.2205.06175}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.06175}.

\bibitem[Riquelme et~al.(2021)Riquelme, Puigcerver, Mustafa, Neumann, Jenatton, Pinto, Keysers, and Houlsby]{riquelme2021scaling}
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr{\'{e}}~Susano Pinto, Daniel Keysers, and Neil Houlsby.
\newblock Scaling vision with sparse mixture of experts.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  8583--8595, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html}.

\bibitem[Roller et~al.(2021)Roller, Sukhbaatar, Szlam, and Weston]{Roller2021Hash}
Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston.
\newblock Hash layers for large sparse models.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  17555--17566, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html}.

\bibitem[Rosenbaum et~al.(2018)Rosenbaum, Klinger, and Riemer]{rosenbaum2017routing}
Clemens Rosenbaum, Tim Klinger, and Matthew Riemer.
\newblock Routing networks: Adaptive selection of non-linear functions for multi-task learning.
\newblock In \emph{6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=ry8dvM-R-}.

\bibitem[Rosenbaum et~al.(2019)Rosenbaum, Cases, Riemer, and Klinger]{rosenbaum2019routing}
Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger.
\newblock Routing networks and the challenges of modular and compositional computation.
\newblock \emph{CoRR}, abs/1904.12774, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.12774}.

\bibitem[R{\"u}ckl{\'e} et~al.(2021)R{\"u}ckl{\'e}, Geigle, Glockner, Beck, Pfeiffer, Reimers, and Gurevych]{Rueckle2021AdapterDrop}
Andreas R{\"u}ckl{\'e}, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych.
\newblock {AdapterDrop}: {O}n the efficiency of adapters in transformers.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  7930--7946, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.626}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.626}.

\bibitem[Ruder(2017)]{ruder2017overview}
Sebastian Ruder.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{CoRR}, abs/1706.05098, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.05098}.

\bibitem[Ruder et~al.(2019{\natexlab{a}})Ruder, Bingel, Augenstein, and S{\o}gaard]{ruder2019latent}
Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S{\o}gaard.
\newblock Latent multi-task architecture learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pp.\  4822--4829, 2019{\natexlab{a}}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/4410/4288}.

\bibitem[Ruder et~al.(2019{\natexlab{b}})Ruder, Peters, Swayamdipta, and Wolf]{ruder2019transfer}
Sebastian Ruder, Matthew~E. Peters, Swabha Swayamdipta, and Thomas Wolf.
\newblock Transfer learning in natural language processing.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorials}, pp.\  15--18, Minneapolis, Minnesota, June 2019{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-5004}.
\newblock URL \url{https://aclanthology.org/N19-5004}.

\bibitem[Ruder et~al.(2021)Ruder, Constant, Botha, Siddhant, Firat, Fu, Liu, Hu, Garrette, Neubig, and Johnson]{Ruder:2021xtremer}
Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson.
\newblock {XTREME}-{R}: Towards more challenging and nuanced multilingual evaluation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  10215--10245, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.802}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.802}.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell]{Rusu2016Progressive}
Andrei~A. Rusu, Neil~C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock \emph{CoRR}, abs/1606.04671, 2016.
\newblock URL \url{http://arxiv.org/abs/1606.04671}.

\bibitem[Sanh et~al.(2019)Sanh, Wolf, and Ruder]{sanh2019hierarchical}
Victor Sanh, Thomas Wolf, and Sebastian Ruder.
\newblock A hierarchical multi-task approach for learning embeddings from semantic tasks.
\newblock In \emph{The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI} 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019}, pp.\  6949--6956. {AAAI} Press, 2019.
\newblock \doi{10.1609/aaai.v33i01.33016949}.
\newblock URL \url{https://doi.org/10.1609/aaai.v33i01.33016949}.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{sanh2020movement}
Victor Sanh, Thomas Wolf, and Alexander~M. Rush.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/eae15aabaa768ae4a5993a8a4f4fa6e4-Abstract.html}.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey, Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'{e}}vry, Fries, Teehan, Biderman, Gao, Bers, Wolf, and Rush]{sanh2022multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, Manan Dey, M.~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti Datta, Jonathan Chang, Mike~Tian{-}Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'{e}}vry, Jason~Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{The Tenth International Conference on Learning Representations}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2110.08207.pdf}.

\bibitem[Sathyendra et~al.(2022)Sathyendra, Muniyappa, Chang, Liu, Su, Strimel, Mouchtaris, and Kunzmann]{Sathyendra:2022speech}
Kanthashree~Mysore Sathyendra, Thejaswi Muniyappa, Feng{-}Ju Chang, Jing Liu, Jinru Su, Grant~P. Strimel, Athanasios Mouchtaris, and Siegfried Kunzmann.
\newblock Contextual adapters for personalized speech recognition in neural transducers.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022}, pp.\  8537--8541, 2022.
\newblock URL \url{https://doi.org/10.1109/ICASSP43922.2022.9746126}.

\bibitem[Saunders(2022)]{Saunders:2022survey}
Danielle Saunders.
\newblock Domain adaptation and multi-domain adaptation for neural machine translation: {A} survey.
\newblock \emph{Journal of Artificial Intelligence Research}, 75:\penalty0 351--424, 2022.
\newblock \doi{10.1613/jair.1.13566}.
\newblock URL \url{https://doi.org/10.1613/jair.1.13566}.

\bibitem[Schick \& Sch{\"u}tze(2021{\natexlab{a}})Schick and Sch{\"u}tze]{Schick2021Exploiting}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock Exploiting cloze-questions for few-shot text classification and natural language inference.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pp.\  255--269, Online, April 2021{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.eacl-main.20}.
\newblock URL \url{https://aclanthology.org/2021.eacl-main.20}.

\bibitem[Schick \& Sch{\"u}tze(2021{\natexlab{b}})Schick and Sch{\"u}tze]{Schick2021ItsNotJustSize}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock It{'}s not just size that matters: Small language models are also few-shot learners.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  2339--2352, Online, June 2021{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.185}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.185}.

\bibitem[Schick et~al.(2021)Schick, Udupa, and Sch{\"{u}}tze]{Schick2021SelfDiagnosis}
Timo Schick, Sahana Udupa, and Hinrich Sch{\"{u}}tze.
\newblock Self-diagnosis and self-debiasing: {A} proposal for reducing corpus-based bias in {NLP}.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 9:\penalty0 1408--1424, 2021.
\newblock \doi{10.1162/tacl\_a\_00434}.
\newblock URL \url{https://doi.org/10.1162/tacl\_a\_00434}.

\bibitem[Sch{\"{o}}lkopf et~al.(2012)Sch{\"{o}}lkopf, Janzing, Peters, Sgouritsa, Zhang, and Mooij]{ScholkopfJPSZM12}
Bernhard Sch{\"{o}}lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris~M. Mooij.
\newblock On causal and anticausal learning.
\newblock In \emph{Proceedings of the 29th International Conference on Machine Learning, {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012}. icml.cc / Omnipress, 2012.
\newblock URL \url{http://icml.cc/2012/papers/625.pdf}.

\bibitem[Sch{\"{o}}lkopf et~al.(2021)Sch{\"{o}}lkopf, Locatello, Bauer, Ke, Kalchbrenner, Goyal, and Bengio]{9363924}
Bernhard Sch{\"{o}}lkopf, Francesco Locatello, Stefan Bauer, Nan~Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio.
\newblock Toward causal representation learning.
\newblock \emph{Proceedings of the {IEEE}}, 109\penalty0 (5):\penalty0 612--634, 2021.
\newblock \doi{10.1109/JPROC.2021.3058954}.
\newblock URL \url{https://doi.org/10.1109/JPROC.2021.3058954}.

\bibitem[Schuler(2005)]{schuler2005verbnet}
Karin~Kipper Schuler.
\newblock \emph{VerbNet: A broad-coverage, comprehensive verb lexicon}.
\newblock PhD thesis, University of Pennsylvania, 2005.
\newblock URL \url{https://repository.upenn.edu/dissertations/AAI3179808}.

\bibitem[Schwartz et~al.(2020)Schwartz, Dodge, Smith, and Etzioni]{Schwartz:2020greenai}
Roy Schwartz, Jesse Dodge, Noah~A. Smith, and Oren Etzioni.
\newblock Green {AI}.
\newblock \emph{Communications of the {ACM}}, 63\penalty0 (12):\penalty0 54--63, 2020.
\newblock URL \url{https://doi.org/10.1145/3381831}.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc~V. Le, Geoffrey~E. Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1ckMDqlg}.

\bibitem[Shen et~al.(2023{\natexlab{a}})Shen, Hou, Zhou, Du, Longpre, Wei, Chung, Zoph, Fedus, Chen, Vu, Wu, Chen, Webson, Li, Zhao, Yu, Keutzer, Darrell, and Zhou]{shen2023mixtureofexperts}
Sheng Shen, Le~Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung~Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu~Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou.
\newblock Mixture-of-experts meets instruction tuning:a winning combination for large language models, 2023{\natexlab{a}}.

\bibitem[Shen et~al.(2023{\natexlab{b}})Shen, Zhang, Cao, Tan, Chen, and Gan]{shen2023moduleformer}
Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan.
\newblock Moduleformer: Modularity emerges from mixture-of-experts, 2023{\natexlab{b}}.

\bibitem[Shi et~al.(2023)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung, Tay, Ruder, Zhou, Das, and Wei]{Shi2023}
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung~Won Chung, Yi~Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.
\newblock {Language Models are Multilingual Chain-of-Thought Reasoners}.
\newblock In \emph{Proceedings of ICLR 2023}, 2023.
\newblock URL \url{http://arxiv.org/abs/2210.03057}.

\bibitem[Siegelmann \& Sontag(1995)Siegelmann and Sontag]{sontag1995computational}
Hava~T. Siegelmann and Eduardo~D. Sontag.
\newblock On the computational power of neural nets.
\newblock \emph{Journal of Computer and System Sciences}, 50\penalty0 (1):\penalty0 132--150, 1995.
\newblock \doi{10.1006/jcss.1995.1013}.
\newblock URL \url{https://doi.org/10.1006/jcss.1995.1013}.

\bibitem[S{\o}gaard \& Goldberg(2016)S{\o}gaard and Goldberg]{sogaard2016deep}
Anders S{\o}gaard and Yoav Goldberg.
\newblock Deep multi-task learning with low level tasks supervised at lower layers.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  231--235, Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-2038}.
\newblock URL \url{https://aclanthology.org/P16-2038}.

\bibitem[Speer et~al.(2017)Speer, Chin, and Havasi]{Speer:2017conceptnet}
Robyn Speer, Joshua Chin, and Catherine Havasi.
\newblock Conceptnet 5.5: An open multilingual graph of general knowledge.
\newblock In \emph{Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, {USA}}, pp.\  4444--4451, 2017.
\newblock URL \url{http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972}.

\bibitem[Stanczak et~al.(2022)Stanczak, Ponti, Torroba~Hennigen, Cotterell, and Augenstein]{stanczak-etal-2022-neurons}
Karolina Stanczak, Edoardo Ponti, Lucas Torroba~Hennigen, Ryan Cotterell, and Isabelle Augenstein.
\newblock Same neurons, different languages: Probing morphosyntax in multilingual pre-trained models.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1589--1598, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.114}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.114}.

\bibitem[Stickland \& Murray(2019)Stickland and Murray]{Stickland2019BERTPALs}
Asa~Cooper Stickland and Iain Murray.
\newblock {BERT} and {PAL}s: Projected attention layers for efficient adaptation in multi-task learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  5986--5995. {PMLR}, 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/stickland19a.html}.

\bibitem[Stickland et~al.(2021)Stickland, Berard, and Nikoulina]{Stickland2021MultilingualDomainAdapt}
Asa~Cooper Stickland, Alexandre Berard, and Vassilina Nikoulina.
\newblock Multilingual domain adaptation for {NMT:} decoupling language and domain information with adapters.
\newblock In Lo{\"{\i}}c Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta~R. Costa{-}juss{\`{a}}, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno{-}Yepes, Philipp Koehn, Tom Kocmi, Andr{\'{e}} Martins, Makoto Morishita, and Christof Monz (eds.), \emph{Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021}, pp.\  578--598. Association for Computational Linguistics, 2021.
\newblock URL \url{https://aclanthology.org/2021.wmt-1.64}.

\bibitem[Strezoski et~al.(2019)Strezoski, van Noord, and Worring]{strezoski2019many}
Gjorgji Strezoski, Nanne van Noord, and Marcel Worring.
\newblock Many task learning with task routing.
\newblock In \emph{2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2019, Seoul, Korea (South), October 27 - November 2, 2019}, pp.\  1375--1384. {IEEE}, 2019.
\newblock \doi{10.1109/ICCV.2019.00146}.
\newblock URL \url{https://doi.org/10.1109/ICCV.2019.00146}.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and McCallum]{strubell-etal-2019-energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pp.\  3645--3650, Florence, Italy, July 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1355}.
\newblock URL \url{https://aclanthology.org/P19-1355}.

\bibitem[Sun et~al.(2020{\natexlab{a}})Sun, Shao, Li, Liu, Yan, Qiu, and Huang]{sun2020learning}
Tianxiang Sun, Yunfan Shao, Xiaonan Li, Pengfei Liu, Hang Yan, Xipeng Qiu, and Xuanjing Huang.
\newblock Learning sparse sharing architectures for multiple tasks.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pp.\  8936--8943. {AAAI} Press, 2020{\natexlab{a}}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/6424}.

\bibitem[Sun et~al.(2022)Sun, He, Qian, Huang, and Qiu]{Sun:2022bbtv2}
Tianxiang Sun, Zhengfu He, Hong Qian, Xuanjing Huang, and Xipeng Qiu.
\newblock {BBTv2:} pure black-box optimization can be comparable to gradient descent for few-shot learning.
\newblock \emph{CoRR}, abs/2205.11200, 2022.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.11200}.

\bibitem[Sun et~al.(2020{\natexlab{b}})Sun, Panda, Feris, and Saenko]{sun2020adashare}
Ximeng Sun, Rameswar Panda, Rog{\'{e}}rio Feris, and Kate Saenko.
\newblock {AdaShare: L}earning what to share for efficient deep multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020{\natexlab{b}}.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/634841a6831464b64c072c8510c7f35c-Abstract.html}.

\bibitem[Sung et~al.(2021)Sung, Nair, and Raffel]{sung2021training}
Yi{-}Lin Sung, Varun Nair, and Colin Raffel.
\newblock Training neural networks with fixed sparse masks.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  24193--24205, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/cb2653f548f8709598e8b5156738cc51-Abstract.html}.

\bibitem[Sung et~al.(2022)Sung, Cho, and Bansal]{Sung:2022modal}
Yi{-}Lin Sung, Jaemin Cho, and Mohit Bansal.
\newblock {VL-ADAPTER:} parameter-efficient transfer learning for vision-and-language tasks.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pp.\  5217--5227. {IEEE}, 2022.
\newblock \doi{10.1109/CVPR52688.2022.00516}.
\newblock URL \url{https://doi.org/10.1109/CVPR52688.2022.00516}.

\bibitem[Sutton(1986)]{sutton1986two}
Richard~S. Sutton.
\newblock Two problems with back propagation and other steepest descent learning procedures for networks.
\newblock In \emph{Proceedings of the Eighth Annual Conference of the Cognitive Science Society, 1986}, pp.\  823--832, 1986.
\newblock URL \url{https://cir.nii.ac.jp/crid/1572824499995923584}.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Richard~S. Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: {A} framework for temporal abstraction in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1-2):\penalty0 181--211, 1999.
\newblock \doi{10.1016/S0004-3702(99)00052-1}.
\newblock URL \url{https://doi.org/10.1016/S0004-3702(99)00052-1}.

\bibitem[Swietojanski et~al.(2016)Swietojanski, Li, and Renals]{Swietojanski:2016taslp}
Pawel Swietojanski, Jinyu Li, and Steve Renals.
\newblock Learning hidden unit contributions for unsupervised acoustic model adaptation.
\newblock \emph{{IEEE} {ACM} Transactions on Audio, Speech, and Language Processing}, 24\penalty0 (8):\penalty0 1450--1463, 2016.
\newblock \doi{10.1109/TASLP.2016.2560534}.
\newblock URL \url{https://doi.org/10.1109/TASLP.2016.2560534}.

\bibitem[Thomas et~al.(2022)Thomas, Kessler, and Karout]{Thomas:2022speech}
Bethan Thomas, Samuel Kessler, and Salah Karout.
\newblock Efficient adapter transfer of self-supervised speech models for automatic speech recognition.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022}, pp.\  7102--7106. {IEEE}, 2022.
\newblock URL \url{https://doi.org/10.1109/ICASSP43922.2022.9746223}.

\bibitem[Thompson et~al.(2018)Thompson, Khayrallah, Anastasopoulos, McCarthy, Duh, Marvin, McNamee, Gwinnup, Anderson, and Koehn]{Thompson:2018nmt}
Brian Thompson, Huda Khayrallah, Antonios Anastasopoulos, Arya~D. McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn.
\newblock Freezing subnetworks to analyze domain adaptation in neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation: Research Papers}, pp.\  124--132, Brussels, Belgium, October 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-6313}.
\newblock URL \url{https://aclanthology.org/W18-6313}.

\bibitem[Tomanek et~al.(2021)Tomanek, Zayats, Padfield, Vaillancourt, and Biadsy]{tomanek-etal-2021-residual}
Katrin Tomanek, Vicky Zayats, Dirk Padfield, Kara Vaillancourt, and Fadi Biadsy.
\newblock Residual adapters for parameter-efficient {ASR} adaptation to atypical and accented speech.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  6751--6760, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.541}.

\bibitem[Trask et~al.(2018)Trask, Hill, Reed, Rae, Dyer, and Blunsom]{trask2018neural}
Andrew Trask, Felix Hill, Scott~E Reed, Jack Rae, Chris Dyer, and Phil Blunsom.
\newblock Neural arithmetic logic units.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Ulrich(1995)]{ulrich1995role}
Karl Ulrich.
\newblock The role of product architecture in the manufacturing firm.
\newblock \emph{Research Policy}, 24\penalty0 (3):\penalty0 419--440, 1995.
\newblock \doi{https://doi.org/10.1016/0048-7333(94)00775-3}.
\newblock URL \url{https://doi.org/10.1016/0048-7333(94)00775-3}.

\bibitem[{\"U}st{\"u}n et~al.(2020){\"U}st{\"u}n, Bisazza, Bouma, and van Noord]{Ustun2020}
Ahmet {\"U}st{\"u}n, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord.
\newblock {UD}apter: Language adaptation for truly {U}niversal {D}ependency parsing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  2302--2315, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.180}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.180}.

\bibitem[{\"U}st{\"u}n et~al.(2021){\"U}st{\"u}n, Berard, Besacier, and Gall{\'e}]{ustun-etal-2021-multilingual}
Ahmet {\"U}st{\"u}n, Alexandre Berard, Laurent Besacier, and Matthias Gall{\'e}.
\newblock Multilingual unsupervised neural machine translation with denoising adapters.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  6650--6662, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.533}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.533}.

\bibitem[{\"U}st{\"u}n et~al.(2022){\"U}st{\"u}n, Bisazza, Bouma, van Noord, and Ruder]{Ustun:2022hyperx}
Ahmet {\"U}st{\"u}n, Arianna Bisazza, Gosse Bouma, Gertjan van Noord, and Sebastian Ruder.
\newblock Hyper-{X}: A unified hypernetwork for multi-task multilingual transfer.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  7934--7949, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.541}.

\bibitem[van~de Ven \& Tolias(2019)van~de Ven and Tolias]{van2019three}
Gido~M. van~de Ven and Andreas~S. Tolias.
\newblock Three scenarios for continual learning.
\newblock \emph{CoRR}, abs/1904.07734, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.07734}.

\bibitem[Vandenhende et~al.(2020)Vandenhende, Georgoulis, Gool, and Brabandere]{vandenhende2019branched}
Simon Vandenhende, Stamatios Georgoulis, Luc~Van Gool, and Bert~De Brabandere.
\newblock Branched multi-task networks: Deciding what layers to share.
\newblock In \emph{31st British Machine Vision Conference 2020, {BMVC} 2020, Virtual Event, UK, September 7-10, 2020}. {BMVA} Press, 2020.
\newblock URL \url{https://www.bmvc2020-conference.com/assets/papers/0213.pdf}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Vaswani2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\  5998--6008, 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Veli{\v{c}}kovi{\'c} \& Blundell(2021)Veli{\v{c}}kovi{\'c} and Blundell]{velivckovic2021neural}
Petar Veli{\v{c}}kovi{\'c} and Charles Blundell.
\newblock Neural algorithmic reasoning.
\newblock \emph{Patterns}, 2\penalty0 (7):\penalty0 100273, 2021.

\bibitem[Verga et~al.(2021)Verga, Sun, Baldini~Soares, and Cohen]{verga-etal-2021-adaptable}
Pat Verga, Haitian Sun, Livio Baldini~Soares, and William Cohen.
\newblock Adaptable and interpretable neural {M}emory{O}ver symbolic knowledge.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  3678--3691, Online, June 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.288}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.288}.

\bibitem[Vilar(2018)]{vilar-2018-learning}
David Vilar.
\newblock Learning hidden unit contribution for adapting neural machine translation models.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}, pp.\  500--505, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-2080}.
\newblock URL \url{https://aclanthology.org/N18-2080}.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and Titov]{voita-etal-2019-analyzing}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pp.\  5797--5808, Florence, Italy, July 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1580}.
\newblock URL \url{https://aclanthology.org/P19-1580}.

\bibitem[von Neumann(1945)]{von1945first}
John von Neumann.
\newblock First draft of a report on the {EDVAC}.
\newblock Technical report, University of Pennsylvania, 1945.
\newblock URL \url{https://doi.org/10.1109/85.238389}.

\bibitem[von Oswald et~al.(2020)von Oswald, Henning, Sacramento, and Grewe]{Oswald2020}
Johannes von Oswald, Christian Henning, Jo{\~{a}}o Sacramento, and Benjamin~F. Grewe.
\newblock Continual learning with hypernetworks.
\newblock In \emph{8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgwNerKvB}.

\bibitem[Vu et~al.(2022{\natexlab{a}})Vu, Barua, Lester, Cer, Iyyer, and Constant]{Vu2022prompt}
Tu~Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, and Noah Constant.
\newblock Overcoming catastrophic forgetting in zero-shot cross-lingual generation.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  9279--9300, Abu Dhabi, United Arab Emirates, December 2022{\natexlab{a}}. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.630}.

\bibitem[Vu et~al.(2022{\natexlab{b}})Vu, Barua, Lester, Cer, Iyyer, and Constant]{vu-etal-2022-overcoming}
Tu~Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, and Noah Constant.
\newblock Overcoming catastrophic forgetting in zero-shot cross-lingual generation.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  9279--9300, Abu Dhabi, United Arab Emirates, December 2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.630}.

\bibitem[Vu et~al.(2022{\natexlab{c}})Vu, Lester, Constant, Al-Rfou{'}, and Cer]{Vu2022spot}
Tu~Vu, Brian Lester, Noah Constant, Rami Al-Rfou{'}, and Daniel Cer.
\newblock {SP}o{T}: Better frozen model adaptation through soft prompt transfer.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  5039--5059, Dublin, Ireland, May 2022{\natexlab{c}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.346}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.346}.

\bibitem[Wagner et~al.(2005)Wagner, Mezey, and Calabretta]{wagner2005natural}
G{\"u}nter~P. Wagner, Jason Mezey, and Raffaele Calabretta.
\newblock Natural selection and the origin of modules.
\newblock In Werner Callebaut and Diego Rasskin-Gutman (eds.), \emph{Modularity: Understanding the Development and Evolution of Complex Natural Systems}, pp.\ ~33. MIT Press, 2005.
\newblock \doi{https://doi.org/10.7551/mitpress/4734.001.0001}.
\newblock URL \url{https://doi.org/10.7551/mitpress/4734.001.0001}.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy, and Bowman]{Wang:2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R. Bowman.
\newblock Superglue: {A} stickier benchmark for general-purpose language understanding systems.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  3261--3275, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html}.

\bibitem[Wang \& Van~hamme(2022)Wang and Van~hamme]{Wang:2022speech}
Pu~Wang and Hugo Van~hamme.
\newblock Bottleneck low-rank transformers for low-resource spoken language understanding.
\newblock In \emph{Proceedings of Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18-22 September 2022}, pp.\  1248--1252, 2022.
\newblock \doi{10.21437/Interspeech.2022-10801}.
\newblock URL \url{https://doi.org/10.21437/Interspeech.2022-10801}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Tang, Duan, Wei, Huang, Ji, Cao, Jiang, and Zhou]{wang-etal-2021-k}
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou.
\newblock {K-Adapter}: {I}nfusing {K}nowledge into {P}re-{T}rained {M}odels with {A}dapters.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pp.\  1405--1418, Online, August 2021{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-acl.121}.
\newblock URL \url{https://aclanthology.org/2021.findings-acl.121}.

\bibitem[Wang et~al.(2022)Wang, Agarwal, Mukherjee, Liu, Gao, Awadallah, and Gao]{Wang2022AdaMix}
Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed~Hassan Awadallah, and Jianfeng Gao.
\newblock {A}da{M}ix: Mixture-of-adaptations for parameter-efficient model tuning.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  5744--5760, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.388}.

\bibitem[Wang et~al.(2020)Wang, Lipton, and Tsvetkov]{wang-etal-2020-negative}
Zirui Wang, Zachary~C. Lipton, and Yulia Tsvetkov.
\newblock On negative interference in multilingual models: Findings and a meta-learning treatment.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  4438--4450, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.359}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.359}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Tsvetkov, Firat, and Cao]{wang2021gradient}
Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.
\newblock Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=F1vEjWK-lH\_}.

\bibitem[Watanabe(2019)]{watanabe2019interpreting}
Chihiro Watanabe.
\newblock Interpreting layered neural networks via hierarchical modular representation.
\newblock In \emph{Neural Information Processing - 26th International Conference, {ICONIP} 2019, Sydney, NSW, Australia, December 12-15, 2019, Proceedings, Part {V}}, volume 1143 of \emph{Communications in Computer and Information Science}, pp.\  376--388. Springer, 2019.
\newblock \doi{10.1007/978-3-030-36802-9\_40}.
\newblock URL \url{https://doi.org/10.1007/978-3-030-36802-9\_40}.

\bibitem[Webson \& Pavlick(2022)Webson and Pavlick]{webson-pavlick-2022-prompt}
Albert Webson and Ellie Pavlick.
\newblock Do prompt-based models really understand the meaning of their prompts?
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  2300--2344, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.167}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.167}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2022finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations}, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Wei et~al.(2022{\natexlab{c}})Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022{\natexlab{c}}.

\bibitem[Williams(1988)]{willianms1988toward}
Ronald~J. Williams.
\newblock Toward a theory of reinforcement-learning connectionist systems.
\newblock \emph{Technical Report NU-CCS-88-3, Northeastern University}, 1988.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock \emph{Machine Learning}, 8\penalty0 (3):\penalty0 229--256, 1992.
\newblock \doi{10.1007/BF00992696}.
\newblock URL \url{https://doi.org/10.1007/BF00992696}.

\bibitem[Winata et~al.(2020)Winata, Cahyawijaya, Lin, Liu, and Fung]{Winata:2020speech}
Genta~Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, and Pascale Fung.
\newblock Lightweight and efficient end-to-end speech recognition using low-rank transformer.
\newblock In \emph{2020 {IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020}, pp.\  6144--6148, 2020.
\newblock URL \url{https://doi.org/10.1109/ICASSP40776.2020.9053878}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Le~Scao, Gugger, Drame, Lhoest, and Rush]{wolf2019huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le~Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pp.\  38--45, Online, October 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-demos.6}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-demos.6}.

\bibitem[Wortsman et~al.(2020)Wortsman, Ramanujan, Liu, Kembhavi, Rastegari, Yosinski, and Farhadi]{wortsman2020supermasks}
Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, and Ali Farhadi.
\newblock Supermasks in superposition.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/ad1f8bb9b51f023cdc80cf94bb615aa9-Abstract.html}.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, and Schmidt]{Wortsman2022ModelSoups}
Mitchell Wortsman, Gabriel Ilharco, Samir~Ya Gadre, Rebecca Roelofs, Raphael~Gontijo Lopes, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt.
\newblock Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato (eds.), \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  23965--23998. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/wortsman22a.html}.

\bibitem[Xi et~al.(2022)Xi, Sun, Yu, Li, Peng, Zhang, Zhang, Wang, Chen, Wang, Liu, Feng, Han, Liu, Ding, and Wang]{Xi2022UFO}
Teng Xi, Yifan Sun, Deli Yu, Bi~Li, Nan Peng, Gang Zhang, Xinyu Zhang, Zhigang Wang, Jinwen Chen, Jian Wang, Lufei Liu, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, and Jingdong Wang.
\newblock {UFO:} unified feature optimization.
\newblock In Shai Avidan, Gabriel~J. Brostow, Moustapha Ciss{\'{e}}, Giovanni~Maria Farinella, and Tal Hassner (eds.), \emph{Computer Vision - {ECCV} 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part {XXVI}}, volume 13686 of \emph{Lecture Notes in Computer Science}, pp.\  472--488. Springer, 2022.
\newblock \doi{10.1007/978-3-031-19809-0\_27}.
\newblock URL \url{https://doi.org/10.1007/978-3-031-19809-0\_27}.

\bibitem[Yadav et~al.(2023)Yadav, Tam, Choshen, Raffel, and Bansal]{yadav2023resolving}
Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal.
\newblock Resolving interference when merging models, 2023.

\bibitem[Yang et~al.(2021)Yang, Lin, Men, Zhou, Jiang, Jia, Wang, Zhang, Wang, Li, Zhang, Lin, Qu, Zhou, and Yang]{yang2021m6}
An~Yang, Junyang Lin, Rui Men, Chang Zhou, Le~Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di~Zhang, Wei Lin, Lin Qu, Jingren Zhou, and Hongxia Yang.
\newblock Exploring sparse expert models and beyond.
\newblock \emph{CoRR}, abs/2105.15082, 2021.
\newblock URL \url{https://arxiv.org/abs/2105.15082}.

\bibitem[Yang et~al.(2019)Yang, Joglekar, Song, Newsome, and Wang]{yang2019task}
Guangyu~Robert Yang, Madhura~R. Joglekar, H.~Francis Song, William~T. Newsome, and Xiao-Jing Wang.
\newblock Task representations in neural networks trained to perform many cognitive tasks.
\newblock \emph{Nature Neuroscience}, 22\penalty0 (2):\penalty0 297--306, 2019.
\newblock \doi{https://doi.org/10.1038/s41593-018-0310-2}.
\newblock URL \url{https://doi.org/10.1038/s41593-018-0310-2}.

\bibitem[Yang \& Hospedales(2017)Yang and Hospedales]{Yang2016Deep}
Yongxin Yang and Timothy~M. Hospedales.
\newblock Deep multi-task representation learning: {A} tensor factorisation approach.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=SkhU2fcll}.

\bibitem[Yasunaga et~al.(2023)Yasunaga, Aghajanyan, Shi, James, Leskovec, Liang, Lewis, Zettlemoyer, and Yih]{Yasunaga:2023arxiv}
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen{-}Tau Yih.
\newblock Retrieval-augmented multimodal language modeling.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  39755--39769. {PMLR}, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/yasunaga23a.html}.

\bibitem[Ye et~al.(2021)Ye, Lin, and Ren]{ye-etal-2021-crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren.
\newblock {C}ross{F}it: A few-shot learning challenge for cross-task generalization in {NLP}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  7163--7189, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.572}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.572}.

\bibitem[You et~al.(2022)You, Feng, Su, and Yu]{you2022speechmoe2}
Zhao You, Shulin Feng, Dan Su, and Dong Yu.
\newblock Speechmoe2: Mixture-of-experts model with improved routing.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022}, pp.\  7217--7221. {IEEE}, 2022.
\newblock \doi{10.1109/ICASSP43922.2022.9747065}.
\newblock URL \url{https://doi.org/10.1109/ICASSP43922.2022.9747065}.

\bibitem[Yu et~al.(2020)Yu, Edunov, Tian, and Morcos]{Yu2020}
Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari~S. Morcos.
\newblock Playing the lottery with rewards and multiple languages: lottery tickets in {RL} and {NLP}.
\newblock In \emph{8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1xnXRVFwH}.

\bibitem[Yu et~al.(2023)Yu, Xiong, Yu, and Liu]{yu-etal-2023-augmentation}
Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.
\newblock Augmentation-adapted retriever improves generalization of language models as generic plug-in.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  2421--2436, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.136}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.136}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Tang, Dai, Zhou, Wu, and Shi]{Zhang2022SkillNet}
Fan Zhang, Duyu Tang, Yong Dai, Cong Zhou, Shuangzhi Wu, and Shuming Shi.
\newblock {SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural Language Understanding}.
\newblock \emph{CoRR}, abs/2203.03312, 2022{\natexlab{a}}.
\newblock \doi{10.48550/arxiv.2203.03312}.
\newblock URL \url{https://arxiv.org/abs/2203.03312}.

\bibitem[Zhang et~al.(2021)Zhang, Zheng, Mao, and Huang]{Zhang:2021ws}
Rongsheng Zhang, Yinhe Zheng, Xiaoxi Mao, and Minlie Huang.
\newblock Unsupervised domain adaptation with adapter.
\newblock \emph{CoRR}, abs/2111.00667, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.00667}.

\bibitem[Zhang et~al.(2014)Zhang, Luo, Loy, and Tang]{zhang2014facial}
Zhanpeng Zhang, Ping Luo, Chen~Change Loy, and Xiaoou Tang.
\newblock Facial landmark detection by deep multi-task learning.
\newblock In \emph{Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part {VI}}, volume 8694 of \emph{Lecture Notes in Computer Science}, pp.\  94--108. Springer, 2014.
\newblock \doi{10.1007/978-3-319-10599-4\_7}.
\newblock URL \url{https://doi.org/10.1007/978-3-319-10599-4\_7}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Zeng, Lin, Xiao, Han, Liu, Sun, and Zhou]{zhang2023emergent}
Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao, Xu~Han, Zhiyuan Liu, Maosong Sun, and Jie Zhou.
\newblock Emergent modularity in pre-trained transformers, 2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=XHuQacT6sa6}.

\bibitem[Zhao et~al.(2020)Zhao, Lin, Mi, Jaggi, and Sch{\"u}tze]{zhao-etal-2020-masking}
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch{\"u}tze.
\newblock Masking as an efficient alternative to finetuning for pretrained language models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  2226--2241, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.174}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.174}.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and Singh]{Zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language models.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  12697--12706. {PMLR}, 2021.
\newblock URL \url{http://proceedings.mlr.press/v139/zhao21c.html}.

\bibitem[Zhong et~al.(2022)Zhong, Chi, Gu, Wang, Yu, and Tang]{Zhang:2022metadmoe}
Tao Zhong, Zhixiang Chi, Li~Gu, Yang Wang, Yuanhao Yu, and Jin Tang.
\newblock Meta-dmoe: Adapting to domain shift by meta-distillation from mixture-of-experts.
\newblock \emph{CoRR}, abs/2210.03885, 2022.
\newblock URL \url{https://doi.org/10.48550/arXiv.2210.03885}.

\bibitem[Zhong et~al.(2021)Zhong, Friedman, and Chen]{zhong-etal-2021-factual}
Zexuan Zhong, Dan Friedman, and Danqi Chen.
\newblock Factual probing is [{MASK}]: Learning vs. learning to recall.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  5017--5033, Online, June 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.398}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.398}.

\bibitem[Zhou et~al.(2023)Zhou, Wan, Vuli\'{c}, and Korhonen]{han:2023autopeft}
Han Zhou, Xingchen Wan, Ivan Vuli\'{c}, and Anna Korhonen.
\newblock {AutoPEFT:} automatic configuration search for parameter-efficient fine-tuning.
\newblock \emph{CoRR}, abs/2301.12132, 2023.
\newblock \doi{10.48550/arXiv.2301.12132}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2301.12132}.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{zhou2019deconstructing}
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  3592--3602, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/1113d7a76ffceca1bb350bfe145467c6-Abstract.html}.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Yang, Loy, and Liu]{Zhou:2022modal}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pp.\  16795--16804, 2022{\natexlab{a}}.
\newblock URL \url{https://doi.org/10.1109/CVPR52688.2022.01631}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Lei, Liu, Du, Huang, Zhao, Dai, Chen, Le, and Laudon]{Zhou2022MoEExpert}
Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent~Y. Zhao, Andrew~M. Dai, Zhifeng Chen, Quoc Le, and James Laudon.
\newblock Mixture-of-experts with expert choice routing.
\newblock \emph{CoRR}, abs/2202.09368, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2202.09368}.

\bibitem[Zhu et~al.(2022)Zhu, Fan, and Alwan]{Zhu:2022speech}
Yunzheng Zhu, Ruchao Fan, and Abeer Alwan.
\newblock Towards better meta-initialization with task augmentation for kindergarten-aged speech recognition.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022}, pp.\  8582--8586, 2022.
\newblock URL \url{https://doi.org/10.1109/ICASSP43922.2022.9747599}.

\bibitem[Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and Fedus]{Zoph2022STMOE}
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus.
\newblock {ST-MoE:} designing stable and transferable sparse expert models.
\newblock \emph{CoRR}, abs/2202.08906, 2022.
\newblock \doi{10.48550/ARXIV.2202.08906}.
\newblock URL \url{https://arxiv.org/abs/2202.08906}.

\bibitem[Zuo et~al.(2022)Zuo, Liu, Jiao, Kim, Hassan, Zhang, Gao, and Zhao]{Zuo2022Taming}
Simiao Zuo, Xiaodong Liu, Jian Jiao, Young~Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo Zhao.
\newblock Taming sparsely activated transformer with stochastic experts.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=B72HXs80q4}.

\end{thebibliography}
