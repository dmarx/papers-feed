
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{bookmark}

\usepackage{hyperref}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[noabbrev,capitalise]{cleveref}
\crefformat{section}{#2\S~#1#3}
\Crefformat{section}{#2\S~#1#3}
\crefformat{subsection}{#2\S~#1#3}
\Crefformat{subsection}{#2\S~#1#3}

\usepackage{url}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{changepage}  
% \usepackage{titlesec}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{enumitem}
\setlist{leftmargin=2em}

\usepackage{multirow}
\usepackage{graphicx}

\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
\definecolor{cadmiumred}{rgb}{0.89, 0.0, 0.13}

% \setcounter{secnumdepth}{4}

% \titleformat{\paragraph}
% {\bfseries}{\theparagraph}{1em}{}
% \titlespacing*{\paragraph}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\title{Modular Deep Learning}
 
\author{\name Jonas Pfeiffer\thanks{Authors contributed equally.} \email jonaspfeiffer@google.com \\
       \addr Google DeepMind
       \AND
       \name Sebastian Ruder$^*$ \email ruder@google.com \\
       \addr Google DeepMind
       \AND
       \name Ivan VuliÄ‡ \email iv250@cam.ac.uk \\
       \addr University of Cambridge
        % \AND
       %  \name Kyunghyun Cho \email kyunghyun.cho@nyu.edu \\
       %  % \addr Department of Computer Science\\
       % New York University  
         \AND
        \name Edoardo M. Ponti$^*$ \email eponti@ed.ac.uk \\
        \addr University of Edinburgh \\
        University of Cambridge
       }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\DeclareMathOperator{\vect}{vec}

\def\month{11}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=z9EkXfvxta}} % Insert correct link to OpenReview for camera-ready version

\renewcommand*\contentsname{Table of Contents}
\begin{document}


\maketitle

\begin{abstract}
Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated.  These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference and discovery, programme simulation, and hierarchical reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer.  
\end{abstract}

% \input{TMLR_instructions}


{\footnotesize
\tableofcontents
}

\newpage

\section{Introduction and Motivation}
\label{sec:intro}
% Transfer learning
Transfer learning has recently become pervasive in machine learning technology, such as in natural language processing \citep{ruder2019transfer,brown2020language}, computer vision \citep{dosovitskiy2020image}, and reinforcement learning \citep{reed2022generalist}, among other areas. In its most successful incarnation, transfer learning consists of pre-training a model on vast amounts of raw data in a self-supervised fashion and subsequently fine-tuning it for new tasks based on a small number of labelled examples. Despite its success, this paradigm for transfer learning suffers from a series of limitations in various settings. Firstly, in multi-task fine-tuning, the learning signals from different tasks may \textit{negatively interfere} with each other \citep{mccloskey1989catastrophicinterference}. Similarly, in continuous learning, adapting to new examples can result in \textit{catastrophic forgetting} of knowledge acquired from previous examples \citep{sutton1986two, french1999catastrophic}.\footnote{These phenomena have also been referred to as spatial and temporal `\textit{crosstalk}' \citep{jacobs1991adaptive}.}
Secondly, in settings where the training and evaluation distributions are not identical, these models fail in \textit{generalising systematically} \citep{lake2018generalization,hupkes2020compositionality}. This makes models brittle and inaccurate and hampers their deployment in real-world applications, where distribution shifts are common.

% Modularity in biological and artificial systems
In contrast, many biological and artificial systems do not suffer from these weaknesses by virtue of their \textit{modularity} \citep{fodor1983modularity,ballard1986cortical}, defined as the correspondence between strongly interconnected components of a system (i.e., modules) and the functions they perform \citep{baldwin2000design,ulrich1995role}. In other words, each module is \textit{specialised} for a unique purpose, for which it is reused consistently. In animal brains, this favours \textit{evolvability}, the ability to adapt quickly to new environments, and \textit{resilience} to environment perturbations \citep{wagner2005natural} because it makes rewiring connections easier than in monolithic, entangled networks \citep{kashtan2005spontaneous}. Artificial systems, such as programming languages and computer hardware, are similarly designed in a modular fashion \citep{booch2008object,baldwin2000design} because this modular design favours consistency, ease of adaptation, and interpretability.


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{.15\linewidth}
    \centering
        % \vspace{3em}
        \includegraphics[width=.99\linewidth]{img/madx.pdf}
        % \vspace{1em}
        \caption{MAD-X }
        \label{fig:CaseStudy:MADX}
    \end{subfigure}
    \hspace{.5em}
    \begin{subfigure}[b]{.35\linewidth}
    \centering
    \vspace{3em}
        \includegraphics[width=.99\linewidth]{img/Polytropon.pdf}  
        % \vspace{2em}
        \caption{Polytropon}
    \label{fig:CaseStudy:Polytropon}
    \end{subfigure}
    \hspace{.5em}
    \begin{subfigure}[b]{.35\linewidth}
    \centering
        \includegraphics[width=.99\linewidth]{img/moe.pdf}  
        % \vspace{0.4em}
        \caption{Mixture-of-Expert Transformer}
        \label{fig:CaseStudy:MoE}
    \end{subfigure} 
    \caption{ Case studies of modular deep learning; best viewed in colour. Green components illustrate different routing functions (see \S~\ref{sec:routing}); shade-of-purple components illustrate modular computation functions (see \S \ref{sec:nature_modularity}). \ref{fig:CaseStudy:MADX}) MAD-X \protect\citep{pfeiffer-etal-2020-mad} uses Adapter layers with fixed routing for zero-shot cross-lingual transfer. \ref{fig:CaseStudy:Polytropon}) Polytropon \protect\citep{ponti2022combining} uses low-rank adapters \citep[LoRA;][]{hu2021lora} with hard learned routing for few-shot task adaptation. \ref{fig:CaseStudy:MoE}) MoE Transformers \citep[\textit{inter alia}]{fedus2021switch,Clark2022UnifiedScaling} use Multi-Layer Perceptrons with top-$k$ soft routing, in order to scale to larger model sizes. The three representative models illustrated here are only a fraction of possible configurations from the `configuration manifold' that can be created by varying the components surveyed in \S\ref{sec:nature_modularity}-\S\ref{sec:training_setting}.
    }
    %\vspace{-1mm}
\label{fig:CaseStudy}
% \vspace{-1.5mm}
\end{figure}

% Are vanilla NNs modular?
To what extent, then, do `vanilla' neural networks display the desirable property of being modular? In principle, given their fully connected nature, they could develop such a structure as a by-product of optimising a loss for a downstream task. Recent structural analyses based on hierarchical clustering of neurons revealed that vanilla neural networks can indeed learn such a modular pattern \citep{watanabe2019interpreting,casper2022graphical,Foroutan2022Discovering}. Favourable conditions for the emergence of modularity include multi-task learning \citep{dobs2022brain} and regularisation through dropout \citep{lange2022clustering}. In particular, from a structural perspective, populations of neurons may activate jointly in response to specific features of the input or the output classes\footnote{\citet{lange2022clustering} found that clusters identified through downstream (output) information do not match with the clusters identified through upstream (input) information. They attribute this phenomenon to their different roles, namely disentanglement of the input structure and composition of the output structure, respectively.}, resulting in similar changes in model performance when ablated \citep{meyes2020under}.
From a functional perspective, multi-task learning may lead to segregated, specialised sub-networks \citep{yang2019task,dobs2022brain}.
On the other hand, \citet{csordas2021are} revealed that a given sub-network does not tend to be re-used for similar sub-tasks nor to be combined with others to express more complex functions. In fact, in many cases, the performance of a model on simple tasks requiring a certain skill and composite tasks requiring a combination thereof is entirely uncorrelated \citep{li-etal-2022-quantifying}. 

% Modular neural networks
For this reason, previous work explored the idea of designing neural networks that are \textit{explicitly} modular \citep{jacobs1991task,rosenbaum2017routing,ponti2021inductive,mittal2022is}. 
This has the goal of achieving not only \textit{functional specialisation} \citep{zhang2023emergent}, but also \textit{re-usability} and \textit{composability}.
In particular, these methods involve identifying 1) \emph{modules} in a neural network that can be updated locally and asynchronously, without affecting the rest of the parameters; 2) a \textit{routing function} that chooses a subset of modules for each example or task; and 3) an \textit{aggregation function} that aggregates the outputs of the active modules. Each of these three ingredients can be manually specified or learned. We provide several case studies of different configurations of these components in \cref{fig:CaseStudy}.

%GOALS OF MODULARITY: NO INTERFERENCE
The main advantages of modular neural architectures are \textit{positive transfer}, \textit{compositionality}, and \textit{parameter efficiency}.
Firstly, modularity encourages positive transfer by encoding similar functions with the same module. At the same time, it prevents interference and forgetting by allocating distinct functions to different dedicated modules \citep{jacobs1991adaptive}. For instance, massively multilingual Transformer-based models in NLP are known to suffer from a `curse of multilinguality' \citep{conneau-etal-2020-unsupervised} due to the conflicting information that the gradient from each language-specific loss carries \citep{wang2021gradient}. A possible solution is augmenting these entangled, fully shared models with specialised modules responsible for individual languages \citep{pfeiffer-etal-2020-mad,Pfeiffer2022Lifting}. More generally, as the range of tasks modelled jointly by a single model becomes increasingly diverse, modularity may be instrumental in the advent of general-purpose, multi-modal agents that encompass vision, language, and action \citep{reed2022generalist}.

%GOALS OF MODULARITY: COMPOSITION
Secondly, modules representing different skills (at the task level) or features (at the example level) can be composed together and updated locally, without affecting the rest of the network. These two properties are crucial in two main settings, which correspond to different aspects of \textit{systematic generalisation}: one is the ability to \textit{re-compose}, i.e.\ zero-shot transfer to tasks consisting of new combinations of learned skills, or examples consisting of new combinations of observed features \citep{hupkes2020compositionality}. For instance, while modules for the Guaran\'i language and for dependency parsing can only be trained separately due to the lack of annotated data for dependency parsing in Guaran\'i, they can be composed to perform inference on this unobserved task--language combination \citep{pfeiffer-etal-2020-mad}. Similarly, in hierarchical reinforcement learning, an agent can follow different sequences of modular policies known as options in tasks requiring the completion of similar sub-goals in different orders \citep{sutton1999between,precup2000temporal}.
The other aspect of systematic generalisation is \textit{robustness}. In fact, if modules are taken to correspond to independent and reusable physical mechanisms \citep{ScholkopfJPSZM12}, \textit{local} shifts in their distributions require updating only the parameters accounting for the affected skills or features \citep{goyal2019recurrent,9363924}, while the rest of the model remains invariant to the change. In practice, the ability to perform local updates facilitates sample efficiency, as fewer examples are necessary to adapt models to new tasks \citep{Bengio2020meta,ponti2022combining}.

% GOALS: PARAM EFFICIENCY
Thirdly, an additional advantage of modular neural architectures is parameter and time \textit{efficiency}. In this framework, fine-tuning a model on a specific task only requires storing a modular adapter rather than a separate copy of the entire (typically large) model. What is more, modules can be added or removed on-the-fly in an incremental manner, adjusting the model capacity according to the task complexity. This ability is known as \textit{conditional computation} \citep{bengio2015conditional}. Finally, modularity enables language models to scale to larger numbers of parameters while retaining the same time complexity, by selecting only a small set of experts per example \citep{shazeer2017outrageously,fedus2021switch}.

% OUR SURVEY: MODULE IMPLEMENTATION
As the main contribution of this survey, we offer a unified view of modular deep learning, illustrating how many families of methods can be defined along four key dimensions: \textbf{1)} how they implement modules, which constitute the minimum unit of computation; \textbf{2)} how they select active modules through a routing function; \textbf{3)} how module outputs are aggregated; and \textbf{4)} how the modules are trained with the rest of the model.

For module implementation, we discuss sparse subnetworks \citep{hu2021lora,ansell2021composable}, adapter layers \citep{Rebuffi2018Adapters2,pfeiffer-etal-2020-mad}, and prefix tuning \citep{Li2020PrefixTuning}, among others. These methods have been proven as an effective way to adapt large pre-trained models, achieving better performance and sample efficiency than alternative strategies such as in-context learning \citep{Liu2022IA3}, which may be brittle~\citep{lu-etal-2022-fantastically}. In fact, modules can also take the form of human-engineered prompts, where the model is provided with input--output examples \citep{brown2020language} or task instructions \citep{wei2022finetuned}. While many module implementations share the same underlying functional form \citep{he-etal-2021-effectiveness}, they offer different trade-offs between efficiency and performance.

% ROUTING FUNCTIONS
We then discuss how routing functions control the flow of information to the modules: in fixed routing, module allocation is manually defined when expert knowledge is available\citep[\textit{inter alia}]{hampshire1992meta,rajendran2017adaapt}. In learned routing, a parameterised routing function is inferred during training. This, however, poses a series of challenges, such as training instability, module collapse, and overfitting \citep{rosenbaum2019routing}. Orthogonally, we also distinguish between hard and soft routing. In hard routing, only a subset of modules is activated \citep[\textit{inter alia}]{rosenbaum2017routing,ponti2022combining,fernando2017pathnet}. In soft routing, all modules are aggregated according to continuous scores \citep{jacobs1991adaptive,Jordan1994Hierarchical}. While soft routing is amenable to vanilla gradient descent, it is highly inefficient. On the other hand, hard routing requires approximate inference but facilitates conditional computation and module specialisation.
When multiple modules are selected, several \textit{aggregation} strategies are possible. 
For instance, these can be based on interpolating the parameters of active modules \citep{ansell2021composable} or an attention mechanism over the module outputs \citep{pfeiffer2020adapterfusion}. Alternative methods include input prompt concatenation \citep{vu-etal-2022-overcoming} and function composition \citep{andreas2016nmn}.

Finally, modules can be trained jointly with the rest of the base model in multi-task learning \citep{caruana1997multitask,ruder2017overview}, added sequentially in classic continual learning \citep{Rusu2016Progressive}, or integrated post-hoc into an already pre-trained and frozen model \citep{Rebuffi2017Adapters1,houlsby2019parameter}. The last scenario is most common with current state-of-the-art models, which are trained as dense, fully shared models and may be `modularised' after pre-training.

Crucially, this taxonomy reveals unexpected connections between several independent threads of research, including aggregation functions and mode connectivity \citep{Frankle2020LinearModeConnect}, routing and hypernetworks \citep{Ha2017HyperNetworks}, among others. 
 
We further illustrate a series of applications of modular networks in \textit{transfer learning} across different areas such as natural language processing, computer vision, and speech processing. In addition, we show how modularity plays an important role in causal inference and discovery, programme simulation, and hierarchical reinforcement learning.
 
We hope that our overview will spark future research on modular deep learning in areas that may benefit from it such as community-driven efforts to develop and maintain machine learning technology.  

\section{Modular Deep Learning}
 

This survey focuses on modular deep learning: namely, on models composed of modules. These are autonomous computation functions that, depending on their architecture and purpose, are variously referred to as adapters \citep{Rebuffi2017Adapters1,pfeiffer-etal-2020-adapterhub}, options \citep{sutton1999between,precup2000temporal}, or experts \citep{jacobs1991task,Jordan1994Hierarchical}. Crucially, these modules are distinguished from a routing function, which controls the information flow to the modules. Finally, an aggregation function aggregates their outputs. Modules can be optionally combined with fully shared (thus, non-modular) parameters as part of the same neural architecture. In order to provide a unified view of the landscape of modular deep learning, we create a taxonomy of four dimensions of variation: computation, routing, aggregation, and training. These dimensions are mutually independent; hence, many methods can be interpreted as different combinations of these dimensions, listed in \cref{ssec:taxonomy}. Concurrently, we provide a unified, consistent notation in \cref{ssec:notation}, which helps illuminate the relationship among such methods.

\subsection{Taxonomy}
\label{ssec:taxonomy}
\textbf{1) Computation function}: \textit{How is each module implemented?} (\S~\ref{sec:nature_modularity})
A module may consist of any component of a neural architecture, such as multiple copies of a model \citep{jacobs1991task} or one of its layers \citep{fedus2021switch}. Alternatively, as it is common in transfer learning, modules can be combined with a function parameterised by fully shared pre-trained weights. 
In this case, we distinguish between modification of parameters (parameter composition), concatenation with input features (input composition), and function composition by stacking neural modules. 

\textbf{2) Routing function:} \textit{How are active modules selected?} (\S~\ref{sec:routing}) Under \textit{fixed routing}, we categorise approaches where the routing function is fixed. This assumes that the specialisation of each module, as well as the combination of modules required for each task, is known \textit{a priori}. In \textit{learned routing}, the parameters of the routing mechanism are learned during training. In this case, routing is soft if all modules are ranked through a continuous score, or hard if each module is given a binary score (active or inactive).

\textbf{3) Aggregation function:} \textit{How are the outputs of the active modules aggregated?} (\S~\ref{sec:compositionality}) We differentiate between methods that compose the outputs of the active modules deterministically (e.g., based on a weighted average) from those where the aggregation function is implemented as a learnable neural network that depends on the output of all modules.

\textbf{4) Training setting:} \textit{How are the modules trained?} (\S~\ref{sec:training_setting}) Some methods, such as MoEs, train the modules (and possibly the routing function) jointly with the shared weights of a randomly initialised model. As an alternative, transfer learning approaches introduce modules \textit{post-hoc} after pre-training weights and adapt them during fine-tuning. In continuous learning settings, instead, new modules may be introduced iteratively for every new task in a sequence.

\begin{table}[t]
    \centering
    % \begin{tabularx}{0.63\textwidth}{|l|l|}
    \begin{tabular}{ll}
    \toprule
    Notation & Definition \\
    \midrule
        $\vx \in \mathcal{X}$ & Input data \\
        $\vy \in \mathcal{Y}$ & Output data \\
        $\vh \in \mathcal{H}$ & Hidden representation \\
        $t \in \mathcal{T}$ & Task index \\
        $f : \mathcal{X} \cup \mathcal{H} \rightarrow \mathcal{Y} \cup \mathcal{H}$ & A computation function \\
        $\vtheta $ & Shared parameters \\
        $M = \{\vphi_1, \dots, \vphi_{|M|}\}$ & Set of module parameters \\
        $\valpha \in \mathcal{A}$ & Vector of routing scores \\
        $r : \mathcal{X} \cup \mathcal{H} \cup \mathcal{T} \rightarrow \mathcal{A}$ & Routing function \\
        $\vrho$ & Routing parameters \\
        $g$ & Aggregation function \\
        $\vgamma$ & Aggregation parameters \\
    \bottomrule
    \end{tabular}
    \caption{Notation and definition of important variables, functions, and operators.}
    \label{tab:notation}
\end{table}

\subsection{Notation} 
\label{ssec:notation}
More formally, let a neural network $f_{\vtheta}: \mathcal{X} \rightarrow \mathcal{Y}$ be decomposed into a graph of sub-functions. In the simplest case, this graph is a linear chain $f_{\vtheta_1} \circ f_{\vtheta_2} \circ \dots \circ f_{\vtheta_l}$, where $\circ$ stands for function composition. These sub-functions refer to the model's $l$ layers, each with unique indexed parameters $\vtheta_i,i=1,\ldots,l$.\footnote{We abuse notation by treating indexing over functions, $f_i$, as identical to indexing over the parameters of a function, $f_{\theta_i}$. In this survey, both are used interchangeably.} In turn, these can be further decomposed recursively into a graph of their constituent sub-functions: for instance, a Transformer layer \citep{Vaswani2017} includes linear mappings for the query, key, value, and output, as well as a non-linear feed-forward network, and residual connections. We further denote the values of the parameters at initialisation as $\vtheta^{0}$, and the parameters after training are denoted as $\vtheta^\star$. 

Any $i$-th sub-function with input $\vx$ can be modified by a module with parameters $\vphi$ from the inventory $M_i = f_{\vphi_1}, \dots, f_{\vphi_{|M|}}$ in the following different ways:

\begin{enumerate}
    \item \textit{parameter composition}: $f_i^\prime(\vx) = f_{\vtheta_i \oplus \vphi}(\vx)$, where $\oplus$ stands for an operation that composes the original parameters with the module parameters, such as element-wise addition. An example is low-rank \citep{hu2021lora} or sparse \citep{ansell2021composable} adapters.
    \item \textit{input composition}: $f_i^\prime(\vx) = f_{\vtheta_i}([\vphi, \vx])$, where $[\cdot, \cdot]$ stands for concatenation. An example is prefix tuning \cite{Li2020PrefixTuning}.
    \item \textit{function composition}: $f_i^\prime(\vx) =  f_{\vphi} \circ f_{\vtheta_i} (\vx)$, where the outputs of the first function is fed into the second function. An example are adapter layers \citep{Rebuffi2017Adapters1}.
\end{enumerate}

For each $i$-th sub-function, multiple modules from the inventory $M_i$ can be selected through a routing function $r(\cdot)$, which returns a score $\alpha_j$ for each module $f_{\vphi_j}$ conditioned on the data itself, such as a language token or visual region $x$ or the full input $\vx$, or metadata such as the task identity $t \in \mathcal{T}$. Note that $\valpha$ can be fixed \textit{a priori} through expert knowledge or learned through an appropriate parameterisation $r_\vrho(\cdot)$, where $\vrho$ refers to (learnable) parameters of the routing function. Often, the routing function takes special forms: 
\begin{enumerate}
    \item In \textit{hard} routing, $\valpha \in \{0, 1\}^{|M|}$ is a discrete binary vector. If these parameters are learned, inference usually relies on score function estimators, stochastic re-parameterisation, or evolutionary algorithms.
    \item In \textit{soft} routing, $\valpha \in [0, 1]^{|M|}$ is a continuous probability distribution, such that $\sum_j \alpha_j = 1$.
    \item Finally, $\valpha \in \mathbb{R}^{|M|}$ can be an unnormalised score vector. This is the case in linear \textit{hypernetworks} \citep{Ha2017HyperNetworks}, where $\valpha$ is usually interpreted as a task embedding and the row-wise stacked module parameters $\Phi = [\vphi_1, \dots, \vphi_{|M|}]$ act as a parameter generator.
\end{enumerate}

Finally, the output of each module is combined through an aggregation function $g(\cdot)$.\footnote{To avoid clutter in terminology, throughout this work we use the term \textit{composition} to refer to the merger of computation functions (\S~\ref{sec:nature_modularity}), and the term \textit{aggregation} to refer to different approaches of combining the outputs of different modules (\S~\ref{sec:compositionality}).} The aggregation function usually takes two possible forms. One consists of a deterministic operation based on the routing scores (e.g., weighted averaging of module parameters or outputs). The other consists of a learnable neural network, such as an attention mechanism between the modules' inputs and outputs \citep{pfeiffer2020adapterfusion}. When we put the computation function, routing function, and aggregation function together, we obtain the general recipe for a modular function, illustrated in \cref{alg:cap}.

\begin{center}
\begin{minipage}{.5\linewidth}
\begin{algorithm}[H]
\caption{Forward pass of a modular function}\label{alg:cap}
Inputs: example $\vx$, task $t$

$\valpha \gets r_\vrho(\vx, t)$ \tcp{Routing}

$H \gets \{\}$

\For{$\vphi_j \in M_i$}{
    $\vh_j \gets f(\vx; \vtheta_i, \vphi_j)$ \tcp{Computation}
    
    $H \gets H \cup \vh_j$
}

$\vy \gets g_\gamma(\valpha, H)$ \tcp{Aggregation}

\end{algorithm}
\end{minipage}
\end{center}

Given shared parameters $\vtheta_i$ for the $i$-th sub-function and a corresponding inventory of modules $M_i$, we first sample a task $t$, and an input $\vx$. The routing scores $\valpha$ are obtained from the routing function $r(\cdot)$. We now compute the hidden representation $\vh_j$ of each module $\vphi_j$ and aggregate them with the function $g(\cdot)$ into the output $\vy$. We elaborate on the settings for training these different components in \cref{sec:training_setting}. We provide an overview of representative computation, routing, and aggregation functions in Table \ref{tab:overview_representative_methods}.

 

\begin{table}[p]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llll}
\toprule
 & Method & Reference & Function \\ \midrule
\multirow{18}{*}{\rotatebox[origin=c]{90}{\begin{tabular}[c]{@{}c@{}}Computation\\function\end{tabular}}} & Sparse subnetwork & \citet{Frankle2019} & $f^\prime = f_{\vtheta^\star\odot\vb}$ \\
 & Supermasks & \citet{wortsman2020supermasks} & $f^\prime = f_{\vtheta_0 \odot \vb}$ \\
 & Sparse fine-tuning & \citet{ansell2021composable} & $f^\prime = f_{\vtheta + \vb \odot \vphi}$ \\
 & Intrinsic dimension & \citet{Li2018intrinsic} & $f^\prime = f_{\vtheta+\vphi \rmM}$ \\
& Low-rank adaptation & \citet{hu2021lora} & $f^\prime_i = f_{\vtheta_i+ \text{vec}(\mB_i \mA_i)}$ \\
& Prompting & \citet{brown2020language} & $f_1^\prime = f_{\vtheta_1}([\vphi, \vx])$ where $\vphi = \text{Emb}(\vp)$ \\
& Retrieval augmentation & \citet{guu2020retrieval} & $f_1^\prime = f_{\vtheta_1}([\vphi, \vx])$ where $\vphi = \text{Emb}([\vp, \vc])$ \\
& Prompt tuning & \citet{Lester2021prompttuning} & $f_1^\prime = f_{\vtheta_1}([\vphi, \vx])$ \\
& Multi-layer prompt tuning & \citet{Li2020PrefixTuning} & $f_i^\prime = f_{\vtheta_i}([\vphi_i, \vx])$ \\
& Parameter sharing & \citet{ruder2017overview} & $f_{\vphi_i}^t = f_{\vphi_i}^s  \: \forall i \in \mathcal{G}$ \\
& Convolutional adapter & \citet{Rebuffi2017Adapters1} & $f_i^\prime = f_{\vphi_i}(f_{\vtheta_i}(\vx))$ where $f_{\vphi_i}(\vx) = \mF * \vx$ \\
& Transformer adapter & \citet{houlsby2019parameter} & $f_i^\prime = f_{\vphi_i}(f_{\vtheta_i}(\vx))$ where $f_{\vphi_i}(\vx) =  \mW^d(\sigma(\mW^u \vx))$ \\
& Compacter & \citet{Mahabadi2021Compacter} & $f_i^\prime = f_{\vphi_i}(f_{\vtheta_i}(\vx))$ where $\begin{array}{l} f_{\vphi_i}(\vx) =  \mW^d(\sigma(\mW^u \vx)), \\ \mW = \sum^n_{j=1} \mA_j \otimes \mB_j \end{array}$ \\
& Parallel adapter & \citet{Rebuffi2018Adapters2} & $f_i^\prime = f_{\vtheta_i}(\vx) + f_{\vphi_i}(\vx)$ \\
& Rescaling & \citet{Bilen2017Universal} &  $f_i^\prime = f_{\vtheta_i }(\vx) \odot \vphi$ \\
& Hypernetwork & \citet{platanios-etal-2018-contextual} & $f^\prime_i = (\valpha\mW)\vx$ \\ \midrule
\multirow{8}{*}{\rotatebox[origin=c]{90}{\begin{tabular}[c]{@{}c@{}}Routing\\function\end{tabular}}}
& Fixed routing & \citet{hampshire1992meta} & $f_i^\prime = \frac{1}{|K|} \sum_j f(\vphi_j) \, \mathbbm{1}_j(K)$\\
& Top-$1$ learned routing & \citet{rosenbaum2017routing} & $f_i^\prime = f(\vx; \vtheta_i, \vphi_j) \, \text{where} \, j = \text{argmax}[\valpha]$ \\
& Top-$k$ learned routing & \citet{goyal2019recurrent} & $f_i^\prime = \mathop{\text{cat}}_{j \in \, \text{top}_k[\valpha]} f(\vx; \vtheta_i, \vphi_j)$ \\
& Variable-size (threshold) & \citet{rahaman2021dynamic} & $f_i^\prime = \mathop{\text{cat}}_{j \in M \, \text{s.t.} \, \alpha_j > t} f(\vx; \vtheta_i, \vphi_j)$ \\
& Variable-size (soft partition) & \citet{ponti2022combining} & $f_i^\prime = \frac{1}{\sum \valpha}\mathop{\sum}_{j \in M \, \text{s.t.} \, \alpha_j = 1} f(\vx; \vtheta_i, \vphi_j)$\\
& Mixture of experts & \citet{jacobs1991adaptive} & $f_i^\prime = \sum_{j \in M} \alpha_j \, f(\vx; \vtheta_i, \vphi_j)$\\
& Weighted top-$k$ routing & \citet{shazeer2017outrageously} & $f_i^\prime = \sum_{j \in \, \text{top}_k[\valpha]} \frac{\alpha_j}{\sum \valpha} \, f(\vx; \vtheta_i, \vphi_j)$ \\
\midrule
\multirow{5}{*}{\rotatebox[origin=c]{90}{\begin{tabular}[c]{@{}c@{}}Aggregation\\function\end{tabular}}} & Sparse weight addition & \citet{ansell2021composable} & $f^\prime = f_{\vtheta_0 + \vphi_l + \vphi_t}$ \\
& Representation averaging & \citet{Ma2018Modeling} & $f_i^\prime =\sum_j^{|M_i|} \alpha_j \vh_j$ \\
& Input concatenation & \citet{vu-etal-2022-overcoming} & $f_i^\prime = f_\vtheta([\vphi_t, \vphi_l, \vx])$ \\ 
& Attention-based aggregation & \citet{pfeiffer2020adapterfusion} & $f_i^\prime = \text{Attn}(\vh_{i-1} \mQ_i, \mH_i \mK_i, \mH_i \mV_i)$ \\
& Sequential aggregation & \citet{pfeiffer-etal-2020-mad} & $f_i^\prime = f_{\vphi_t} ( f_{\vphi_l}(f_{\vtheta_0}(\vx)))$ \\
\bottomrule
\end{tabular}%
}
\caption{An overview of representative computation, routing, and aggregation functions. Each method is paired with a representative reference. In computation functions, skip connections are omitted for simplicity. Definitions: the model $f$, a model's sub-function $f_i$, model parameters $\vtheta$, module parameters $\vphi$, parameters at initialisation $\vtheta^{0}$, parameters after training $\vtheta^\star$, binary mask $\vb \in \{0, 1\}^{|\vtheta|}$, random matrix $\rmM$, group $G$, input $\vx$, a model's embedding layer $\text{Emb}(\cdot)$, text prompt $\vp$, retrieved context $\vc$, filter bank $\mF$, routing scores or task embedding $\valpha$, routing function $r$, subset of modules $K$, module inventory $M$.}
\label{tab:overview_representative_methods}
\end{table}

\section{Computation Function}
\label{sec:nature_modularity}

The computation function determines the design of a module. Various module architectures have been proposed such as MLP layers \citep{rosenbaum2017routing,kirsch2018modular,chang2018automatically},
independent RNNs \citep{goyal2019recurrent}, independent CNNs \citep{parascandolo2018learning}, or special-purpose architectures \citep{andreas2016nmn}. However, in transfer learning, modules are most often integrated into a base architecture whose parameters are fully shared. 
We identify three core methods to merge a \textit{single} module with the corresponding sub-function: parameter composition, input composition, and function composition. While all three methods instantiate modules differently, we demonstrate how they can be seen in a unified view in \Cref{sec:unifying_composition}. We provide example illustrations of the three computation functions (in addition to a hypernetwork) as part of a Transformer architecture in \cref{fig:nature_modularity} and provide a high-level overview of their trade-offs in Table \ref{tab:computation_function_comparison}, which we further discuss in the respective sections.\footnote{The comparison is mainly meant as a high-level guideline. Individual methods may have different trade-offs and mitigate certain weaknesses indicated in the table.}

\subsection{Parameter Composition}
\label{sec:nature_modularity:parameter_composition}

Parameter composition methods augment the function $f_{W}$ of a base model with weights $W \in \mathbb{R}^{o \times i}$ with module parameters $\Phi \in \mathbb{R}^{o \times i}$, where $i$ is the input dimensionality, and $o$ is the output dimensionality. In particular, the module inventory consists of a set of sparse or low-rank weights to ensure that the modules are parameter-efficient. Therefore, the resulting function is parameterised as $f_{{\vtheta \oplus \vphi}_i}$, where $\oplus$ stands for element-wise addition.

 


\begin{figure}[t]
    \centering
    \begin{subfigure}{.24\linewidth}
    \centering
        \vspace{3em}
        \includegraphics[width=.99\linewidth]{img/Nature_of_modularity_1.pdf}
        \vspace{0.4em}
        \caption{Parameter Composition }
        \label{fig:nature_modularity:parameter_composition}
    \end{subfigure}
    % \hspace{.5em}
    \begin{subfigure}{.24\linewidth}
    \centering
    \vspace{3em}
        \includegraphics[width=.535\linewidth]{img/Nature_of_modularity_2.pdf}  
        \vspace{1.4em}
        \caption{Input Composition}
    \label{fig:nature_modularity:input_composition}
    \end{subfigure}
    % \hspace{.5em}
    \begin{subfigure}{.24\linewidth}
    \centering
        \includegraphics[width=.535\linewidth]{img/Nature_of_modularity_3.pdf}  
        \vspace{0.5em}
        \caption{Function Composition}
        \label{fig:nature_modularity:function_composition}
    \end{subfigure}
        % \hspace{.5em}
    \begin{subfigure}{.24\linewidth}
    \centering
        \includegraphics[width=.99\linewidth]{img/Nature_of_modularity_4.pdf}  
        \caption{Hypernetwork}
        \label{fig:nature_modularity:hyper_network}
    \end{subfigure}
    \caption{Different modular designs for Transformer architectures; best viewed in colour. Task-specific modular components are illustrated in magenta and purple, respectively. 
    \textbf{(a)~Parameter~Composition}~(\S~\ref{sec:nature_modularity:parameter_composition}): A sparse sub-network in the linear layer as part of multi-head-attention.
     \textbf{(b)~Input~Composition}~(\S~\ref{sec:nature_modularity:input_composition}): Prefix-tuning~\citep{Li2020PrefixTuning} extends the input by prepending embeddings to the key and value matrices in the Transformer layer.  
     \textbf{(c)~Function~Composition}~(\S~\ref{sec:nature_modularity:layers}): Task-specific bottleneck layers transforming the hidden representations are inserted in each layer  \citep{houlsby2019parameter}.  
     \textbf{(d)~Hypernetwork}~(\S~\ref{sec:computation_function:hyper_network}): A small separate neural network generates modular parameters conditioned on metadata. We show its application to function composition but it is compatible with all computation functions.
    }
    %\vspace{-1mm}
\label{fig:nature_modularity}
% \vspace{-1.5mm}
\end{figure}

\paragraph*{Sparse Subnetworks} \label{sec:routing:sparsemasks} 

%SPARSITY
\textit{Sparsity} is a common inductive bias based on the assumptions (i) that only a small number of parameters of an over-parameterised model are relevant for a particular task, and that (ii) similar tasks share similar sub-networks. This is the case, for instance, for language subnetworks in multilingual language models \citep{stanczak-etal-2022-neurons,Foroutan2022Discovering}.

%PRUNING in randomly initialised nets
The most widespread method to induce sparsity is \textit{pruning}.
This can be interpreted as the application of a binary mask $\vb \in \{0, 1\}^{|\vtheta|}$ that selectively keeps or removes each connection in a model with trained parameters $\vtheta^\star$: $f^\prime = f_{\vtheta^\star\odot\vb}$ where $\odot$ is element-wise multiplication. The merger of $\vtheta$ and $\vb$ results in a sparse subnetwork, but the corresponding model parameters usually remain dense for hardware and software reasons.\footnote{In fact, sparse linear algebra operations on graphic processing units remain highly inefficient, if available at all. Examples include the sparse tensor classes in Pytorch: \url{https://pytorch.org/docs/stable/sparse.html}}  After training, the trained weights are sorted based on a criterion and a fraction (bottom-$k$) of the weights are set to zero. Examples of criteria include magnitude after convergence \citep{han2016dsd} and change of magnitude between initialisation and convergence \citep{Frankle2019}.

As pruning generally leads to a loss in performance due to the change in network connections, the non-pruned weights are typically re-wound to their initialisation value and re-trained. In practice, rather than pruning all weights in a single run, iterative pruning is carried out over multiple stages \citep{han2015learning,Frankle2019}. The models pruned in this fashion often retain---if not surpass ---the performance of the original dense model. The existence of a subnetwork with this property in any given randomly initialised model is known as the Lottery Ticket Hypothesis \citep[LTH;][]{Frankle2019,chen2020lottery}.
These `winning tickets' have also been shown to exist in RL and NLP \citep{Yu2020}, as well as in computer vision \citep{Frankle2020LinearModeConnect}. Subnetworks achieve above-random performance even when kept fixed at their random initialisation \citep{zhou2019deconstructing,wortsman2020supermasks, zhao-etal-2020-masking}, so $f^\prime = f_{\vtheta_0 \odot \vb}$. In this case, they are known as \emph{supermasks}.

% PRUNING in pre-trained models
Winning tickets also occur in pre-trained models, such as language models \citep{chen2020lottery,prasanna-etal-2020-bert}. These often outperform tickets from randomly initialised models \citep{prasanna-etal-2020-bert} and are less sensitive to specific hyper-parameter choices \citep{sun2020learning}.
Magnitude pruning, which relies on zeroth-order information (the absolute value of a weight), is sub-optimal in this setting as fine-tuned weights typically stay close to their pre-trained values. Thus, magnitude pruning selects a similar set of weights for pruning regardless of the downstream task. Pruning based on first-order (gradient-based) information better captures the task-specific relevance of each weight \citep{Molchanov2017}. For instance, movement pruning \citep{sanh2020movement} learns the mask $\vb$ jointly with the parameters $\vtheta$. As the mask is a discrete binary variable, they rely on straight-through estimators \citep{bengio2013estimating}. Alternatively,
$\vb$ can be first learned as a real-valued mask and then binarised via a thresholding function \citep{mallya2018piggyback}.

% SPARSE FINE-TUNING / LTH for ADAPTATION
In addition to pruning, sparsification techniques can be employed for \textit{adaptation}. In particular, a sparse module $\vphi$ can be merged with pre-trained parameters $\vtheta$. For instance, in Sparse Fine-Tuning \citep[SFT;][]{ansell2021composable} the LTH is re-purposed such that, instead of zeroing out weights with the lowest change in magnitude, they are simply frozen. Thus, only a subset of weights is fine-tuned.\footnote{This is typically implemented by masking the gradient based on the binary mask $\vb \odot \nabla_\vtheta \mathcal{L}(f_\vtheta, \mathcal{D})$ where $\mathcal{L}$ is a loss function and $\mathcal{D}$ is a dataset \citep{ansell2021composable}.} The difference between these and the original pre-trained model results in a sparse module $\vphi$ where $\phi_i = 0$ if $b_i = 0$, which can be plugged in and out of the model as $f^\prime_{\vtheta} = f_{\vtheta \oplus \vphi}$. \texttt{Diff pruning}~\citep{guo-etal-2021-parameter} instead obtains a sparse adapter by fine-tuning a dense difference vector $\vphi$ regularised to be sparse with a differentiable approximation to the
$L_0$-norm penalty. \citet{sung2021training} induce a fixed sparse mask by selecting the top-$k$ weights ranked according to (a diagonal approximation of) their Fisher information. This second-order information reveals the impact of the change of a parameter on the model predictions. Thus,

\begin{equation}
    \vb_j = \begin{cases}
        1 \quad \text{if} \, j \in \text{top-}k \, \frac{1}{n} \sum_{i=1}^n \mathop{\mathbb{E}}_{y \sim f_{\vtheta^\star}(y \mid \vx_i)} \left(\nabla_\vtheta \log f_{\vtheta^\star}(y \mid \vx_i)\right)^2\\
        0 \quad \text{otherwise}
    \end{cases}
\end{equation}

\begin{table}[]
\centering
% \resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
 &  & Parameter & Training & Inference & \multirow{2}{*}{Performance} & \multirow{2}{*}{Compositionality} \\
 &  & efficiency & efficiency & efficiency &  &  \\ \midrule
\begin{tabular}[c]{@{}l@{}}Parameter\\ composition \end{tabular} &  & \textcolor{ao}{\textbf{+}} &  \textcolor{cadmiumred}{\textbf{--}} & \textcolor{ao}{\textbf{++}} & \textcolor{ao}{\textbf{+}} & \textcolor{ao}{\textbf{+}} \\
\begin{tabular}[c]{@{}l@{}}Input\\ composition\end{tabular} &  & \textcolor{ao}{\textbf{++}} & \textcolor{cadmiumred}{\textbf{--}} & \textcolor{cadmiumred}{\textbf{--}} & \textcolor{cadmiumred}{\textbf{--}} & \textcolor{ao}{\textbf{+}} \\
\begin{tabular}[c]{@{}l@{}}Function\\ composition\end{tabular} &  & \textcolor{cadmiumred}{\textbf{--}} & \textcolor{ao}{\textbf{+}} & \textcolor{cadmiumred}{\textbf{--}} & \textcolor{ao}{\textbf{++}} & \textcolor{ao}{\textbf{+}} \\
\bottomrule
\end{tabular}%
% }
\caption{Comparison of computation functions along different dimensions. See the end of \S~\ref{sec:nature_modularity:parameter_composition} (parameter composition), \S~\ref{sec:nature_modularity:input_composition} (input composition), and \S~\ref{sec:nature_modularity:layers} (function composition) for further explanation. Compositionality is discussed in \S~\ref{sec:compositionality}.}
\label{tab:computation_function_comparison}
\end{table}

 

Beyond the sparsification of individual weights, sparse model adaptation can also be \textit{structured}. In this case, only a group of model sub-functions is fine-tuned, while the rest of the parameters remain frozen. The most common setting is for such a group to correspond to a subset of layers, e.g.\ the last one \citep{donahue2014decaf}. Groups can also relate to more fine-grained parts of the model. For instance, a group consisting of a model's bias parameters is a practical choice as this removes the need to store the model's intermediate activations \citep{Cai2020tinytl,ben-zaken-etal-2022-bitfit}. 
At the level of parameter tensors, some methods prune filters in CNNs \citep{anwar2017structured,newell2019feature} whereas others prune attention heads in pre-trained Transformers \citep{voita-etal-2019-analyzing,michel2019sixteen}. In structured diff pruning, members of a group are encouraged to share the same mask value \citep{guo-etal-2021-parameter}.

\paragraph*{Low-Rank Modules} 
%SUBSPACES
Similar to sparsity, another efficient solution is for the module parameters $\vphi_i$ to lie in a low-dimensional subspace. \citet{Li2018intrinsic} show that models can be optimised in a low-dimensional, randomly oriented subspace rather than the full parameter space. In this setting, the module parameters $\vphi \in \mathbb{R}^d$ are low-dimensional compared to the model parameters $\vtheta \in \mathbb{R}^D$ and $d \ll D$. A random matrix $\rmM \in \mathbb{R}^{d \times D}$ can be used to project from $d$ to $D$: $f^\prime_{\vtheta} = f_{\vtheta+\vphi \rmM}$. An efficient way to compute $\rmM$ is via the Fastfood transform \citep{le2013fastfood}, which factorises $\rmM$ as random linear matrices. Specifically, $\rmM = \rmH \rmG \Pi \rmH \rmB$ consists of a Hadamard matrix $\rmH$, a random diagonal matrix with independent standard normal entries $\rmG$, a random diagonal matrix with equal probability $\pm 1$ entries $\rmB$, and a random permutation matrix $\Pi$.
\citet{Li2018intrinsic} refer to the minimum $d$ that achieves within 90\% of the full-parameter model performance as the intrinsic dimensionality of a given task. \citet{aghajanyan-etal-2021-intrinsic} investigate the intrinsic dimensionality of various NLP tasks with different pre-trained models. They observe that it decreases during pre-training and that larger models have lower values.

However, storing the random matrices results in a substantial memory overhead and is slow to train \citep{Mahabadi2021Compacter}. If the weight matrix $\mW \in \mathbb{R}^{o \times i}$ is small enough, we can directly compose it into low-rank matrices $\mW = \lambda \mB \mA$ where $\mA \in \mathbb{R}^{k \times i}$ and $\mB \in \mathbb{R}^{o \times k}$, where $i$ is the input dimensionality, $o$ is the output dimensionality, $k$ is the rank of the matrix, and $\lambda$ is a scaling hyper-parameter. To save space, the factorisation may be only applied to certain groups of parameters $G$. In LoRA \citep{hu2021lora}, this group corresponds to the linear projections in the self-attention mechanisms of each Transformer layer: $f^\prime_j = f_{\vtheta_j+ \text{vec}(\mB_j \mA_j)} \forall f_j^\prime \in \mathcal{G}$.

Overall, parameter composition methods (both sparse and low-rank) are very parameter-efficient and often require updating less than 0.5\% of a model's parameters \citep{guo-etal-2021-parameter}. At inference time, they keep the model size constant or even reduce it, if the resulting model is sparse. This is compelling as it enables a plug-in replacement of the original with the modular model without any changes to the underlying architecture. Sparse modules, however, increase the time complexity of optimisation as they typically require multiple iterations of re-training. Finally, state-of-the-art parameter composition methods, e.g., LoRA \citep{hu2021lora} and SFT \citep{ansell2021composable} achieve strong performance in zero-shot and few-shot transfer. 

\subsection{Input Composition}
\label{sec:nature_modularity:input_composition}

Input composition methods augment a function's input $\vx$ by concatenating it with a parameter vector $\vphi_i$: $f_i^\prime(\vx) = f_{\vtheta_i}([\vphi_i, \vx])$. The most common strategy is to augment the input fed to the model's first layer $f_1$.

\paragraph*{Prompting} In a prompting setup with auto-regressive language models \citep{brown2020language} or encoders \citep{Schick2021Exploiting,Schick2021ItsNotJustSize}, the input prompt $\vp$ consists of (optional) instructions and (optional) in-context examples that have been converted to natural language. From a different perspective, the task-specific text prompt, when encoded using the model's embedding layer $\text{Emb}(\cdot)$, corresponds to modular parameters $\vphi$ that elicit the desired behaviour \citep{gao-etal-2021-making,Liu:2021survey}: $\text{Emb}(\vp) = \vphi$. More elaborate prompts $\vp$ such as rationales have led to improved few-shot reasoning performance \citep{wei2022chain,kojima2022large,Shi2023}. However, models are ostensibly sensitive to the formulation of the prompt as well as to the set and order of the (few-shot) examples \citep{Zhao2021calibrate,lu-etal-2022-fantastically,webson-pavlick-2022-prompt}. 

\paragraph*{Continuous Prompts} Instead, a continuous prompt vector $\vphi$ can be learned directly  \citep{Lester2021prompttuning,liu2021gpt,zhong-etal-2021-factual,hambardzumyan-etal-2021-warp}. However, if $\vphi$ is only concatenated with the first layer's input, the model has limited capacity to adapt to a specific task. As a result, such continuous (also called \textit{soft}) prompts perform poorly at smaller model sizes and on some harder tasks \citep{Mahabadi2021Compacter,liu-etal-2022-p}. To mitigate this, initialisation via multi-task learning has been proposed \citep{Vu2022spot}. As an alternative, module vectors $\vphi_i$ can be learned \textit{for each layer} of the model \citep[Figure~\ref{fig:nature_modularity:input_composition}; ][]{Li2020PrefixTuning,liu-etal-2022-p}. While this increases the number of parameters, it increases the modules' capacity to adapt to a given task. In practice, module parameters in the form of prefix vectors $\vphi_i = \mP_k^i, \mP_v^i \in \mathbb{R}^{l \times d}$ are prepended to the keys and values of every multi-head attention layer. Attention is defined as $f_i(\vx) = \text{Attn}(\vx \mW_q^i, \mC \mW_k^i, \mC \mW_v^i)$ where $\mW_q, \mW_k, \mW_v \in \mathbb{R}^{d \times d_h}$ are the projections that produce the queries, keys, and values, and $\mC \in \mathbb{R}^{m \times d}$ is a sequence of context vectors. Multi-layer prompt tuning thus takes the following form: 
\begin{equation}
    f_i^\prime(\vx) = \text{Attn}(\vx \mW_q^i, [\mP_k^i, \mC \mW_k^i], [\mP_v^i, \mC \mW_v^i]).
\end{equation}

\paragraph*{Retrieval Augmentation} Beyond individual prompts, the input can be augmented with additional context $\vc$ from a retrieval model. Retrieved documents are appended to the input and are used for conditioning the language model \citep{guu2020retrieval,lewis2020retrieval}: $\text{Emb}([\vp, \vc]) = \vphi$.

In summary, input composition is exceptionally parameter-efficient as it only adds a very small number of parameters. However, these parameters extend a model's context window, which makes them less efficient during training and inference. Prompt tuning methods also require large models to achieve decent performance.

\subsection{Function Composition}
\label{sec:nature_modularity:layers}

While parameter composition deals with individual weights and input composition methods act only on a function's input, function composition methods augment the model with new task-specific sub-functions (see Figure~\ref{fig:nature_modularity:function_composition}): $f_i^\prime(\vx) = f_{\vphi_i} \circ f_{\vtheta_i}(\vx) = f_{\vphi_i}(f_{\vtheta_i}(\vx))$, where $\circ$ stands for function composition.

\paragraph*{Parameter Sharing} Models in multi-task learning traditionally consist of shared layers $f_{\vtheta}$ stacked under task-specific modules $f_\vphi$ \citep{ruder2017overview}. Conversely, given models for tasks $t$ and $s$ expressed as a composition of functions $f_{\vphi_1}^t \circ \ldots \circ f_{\vphi_l}^t$ and $f_{\vphi_1}^s \circ \ldots \circ f_{\vphi_l}^s$, respectively, a multi-task architecture can also be obtained by tying sets of parameters between the models: $f_{\vphi_i}^t = f_{\vphi_i}^s  \: \forall i \in \mathcal{G}$ where the group $\mathcal{G}$ contains the set of shared layer indices.\footnote{In this view, there is no clear differentiation between model parameters $\vtheta$ and module parameters $\vphi$.} Many multi-task neural architectures can be characterised in terms of their definition of $G$, which determines which modules are task-specific and which ones are shared. This is the case, for instance, of `shared trunk' approaches in computer vision \citep{zhang2014facial,Ma2018Modeling} and approaches with supervision at different layers in NLP \citep{sogaard2016deep,sanh2019hierarchical,liu2019multi}.

 

Some approaches learn finer-grained interactions between pairs of modules.
\cite{Misra2016} propose the cross-stitch unit, which linearly combines the inputs at every layer\footnote{We omit the layer index $n$ to simplify the presentation.}: $(\widetilde{\vx}^t, \widetilde{\vx}^s) = \mW[\vx^t, \vx^s]$ where
\begin{equation*}
\begin{bmatrix}
\widetilde{\vx}_{ij}^t \\[2pt]
\widetilde{\vx}_{\strut ij}^s
\end{bmatrix}
= \begin{bmatrix}
\alpha^{tt}  & \alpha^{ts} \\
\alpha^{st}  & \alpha^{ss} \\
\end{bmatrix}
\begin{bmatrix}
\vx_{ij}^t \\[2pt]
\vx_{\strut ij}^s
\end{bmatrix}
\end{equation*}
and $\alpha \in \mathbb{R}$. Sluice networks \citep{ruder2019latent} extend cross-stitch units to multiple modules per layer and additionally employ a soft selection of the skip connections from all layers at the output layer $l$:
\begin{equation*}
{\widetilde{\vx}^{t\top}} = 
\begin{bmatrix}
           \beta_1^t \\
           \cdots \\
           \beta_l^t
         \end{bmatrix}^\top
\begin{bmatrix}
{\vx_{1}^t}^\top \: , & \ldots \: , & {\vx_{l}^t}^\top
\end{bmatrix}
\end{equation*}
and $\beta \in \mathbb{R}$. On the other hand, \citet{gao2019nddr} fuse features from multiple tasks through a 1x1 convolution. \citet{bragman2019stochastic} employ variational inference to assign filters in a CNN to task-specific or shared roles.

Rather than learning which modules should be shared among which tasks, which is a combinatorially large problem, \citet{lu2017fully} and \citet{vandenhende2019branched} start with a fully shared model and then dynamically widen it during training, by cloning function $f_{\vtheta_i}$ into new modules $f_{\vphi_{i,1}}, \ldots, f_{\vphi_{i,k}}$ shared among a smaller subset of tasks, in top-down order across layers. More information on parameter-sharing strategies in multi-task learning can be found in relevant surveys \citep{ruder2017overview,crawshaw2020multi}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{.2\linewidth}
    \centering
        \vspace{3em}
        \includegraphics[width=.5\linewidth]{img/Bottleneck_adapter.pdf}
        \vspace{0.4em}
        \caption{Sequential \\ Bottleneck Adapter}
        \label{fig:nature_modularity:bottleneckadapter}
    \end{subfigure}
    \hspace{3em}
    \begin{subfigure}[b]{.3\linewidth}
    \centering
    \vspace{3em}
        \includegraphics[width=.6\linewidth]{img/Parallel_adapter.pdf}  
        \vspace{1.4em}
        \caption{Parallel Bottleneck Adapter}
    \label{fig:nature_modularity:paralleladapter}
    \end{subfigure}
    % \hspace{.5em}
    \begin{subfigure}[b]{.3\linewidth}
    \centering
    
    \vspace{3em}
        \includegraphics[width=.4\linewidth]{img/IA3.pdf}  
        \vspace{0.5em}
        \caption{(IA)$^3$ }
        \label{fig:nature_modularity:ia3}
    \end{subfigure}
    \caption{ Different approaches of function composition. \textbf{(a) Sequential Bottleneck Adapter:} The first adapter architecture proposed for transformers which consists of two bottleneck layers placed after the multi-head attention (MHA) and feed-forward (FF) layers \citep{houlsby2019parameter}. \textbf{(b) Parallel Bottleneck Adapter:} Bottleneck layers processed in parallel to the MHA and FF layers of the pre-trained transformer components \citep{Rebuffi2018Adapters2,Stickland2019BERTPALs,He2021UnifiedAdapters}. \textbf{(c) (IA)$^3$}: Rescaling operations performed within the MHA and FF layers \citep{Liu2022IA3}.
    }
    %\vspace{-1mm}
\label{fig:nature_modularity:adapter_examples}
% \vspace{-1.5mm}
\end{figure}

 

\paragraph*{Adapter Layers}
As an alternative to parameter sharing, a new task-specific learnable function $f_{\vphi_i}$ can be composed with an (often frozen) shared function $f_{\vtheta_i}$. As the main purpose of such modules is adapting a pre-trained model to new tasks, they are also simply known as `adapter layers'. We provide examples of different adapter layers in \cref{fig:nature_modularity:adapter_examples}.

The adapter's design and composition with the pre-trained model are often modality-specific. In computer vision, the adapter typically consists of a $1 \times 1$ convolution, i.e., $f_{\vphi_i}(\vx) = F * \vx$ where $F$ is a bank of $\1 \times 1$ filters and $*$ is the convolution operation \citep{Rebuffi2017Adapters1}. The module is then inserted between the convolutional blocks of a pre-trained model, such as a ResNet \citep{He2016ResNet}. In NLP, a bottleneck architecture has become popular which consists of a down- and up-projection, coupled with an intermediate activation function $\sigma$: $f_{\vphi_i}(\vx) =  \mW^d(\sigma(\mW^u \vx))$ where $\mW^d \in \mathbb{R}^{d_\vx \times k}$ and $\mW^U \in \mathbb{R}^{k \times d_\vx}$, $d_\vx$ is the dimensionality of the input (typically the hidden dimension), and $k$ is the bottleneck dimension. $\sigma$ is commonly a non-linearity such as a ReLU unit \citep[Figure~\ref{fig:nature_modularity:bottleneckadapter}; ][]{houlsby2019parameter,pfeiffer-etal-2020-mad}. 
In a Transformer model, adapters are placed both after the multi-head attention and the feed-forward layer \citep{houlsby2019parameter},  just after the multi-head attention \citep{Bapna2019Adapters}, or just after  the feed-forward layer \citep{pfeiffer-etal-2020-mad}. 

Other variants for $\sigma$ such as the identity function, standard multi-head attention, and multi-head attention with shared projection matrices have also been explored \citep{Stickland2019BERTPALs}. \citet{Mahabadi2021Compacter} propose Compacter, a hyper-complex, low-rank adapter that reparameterises $\mW$ in the adapter as: $\mW = \sum^n_{i=1} \mA_i \otimes \mB_i$ where $\mA_i \in \mathbb{R}^{n \times n}$ is shared across layers ($n$ is a hyper-parameter), $\mB_i \in \mathbb{R}^{\frac{k}{n} \times \frac{d}{n}}$ is parameterised as a low-rank matrix $\mB_i = \vs_i \vt_i^\top$ and $\otimes$ is the Kronecker product.

Adapters can be routed sequentially or in parallel. Sequential adapters, are inserted between existing functions: $f_i^\prime(\vx) = f_{\vphi_i}(f_{\vtheta_i}(\vx))$ \citep{Rebuffi2017Adapters1,houlsby2019parameter}. Parallel adapters are applied in parallel to a pretrained function: $f_i^\prime(\vx) = \vx + f_{\vtheta_i}(\vx) + f_{\vphi_i}(\vx)$ \citep[Figure~\ref{fig:nature_modularity:paralleladapter}; ][]{Rebuffi2018Adapters2,Stickland2019BERTPALs,He2021UnifiedAdapters}. 
Moreover, adapters involve two residual connections: between the output of $f_{\vtheta_i}$ and the output of $f_{\vphi_i}$, which is further added to $\vx$ and normalised.
Adapters have been shown to lead to increased sample efficiency, flatter minima, and more robustness to hyper-parameter choices compared to standard model fine-tuning \citep{mahabadi2021parameter,he-etal-2021-effectiveness,han-etal-2021-robust}.

\paragraph*{Function Augmentation} Adapters and more complex module designs can also be used to augment a base model with information and behaviour that it otherwise would not be able to access. This can be through adapter layers pre-trained on specific domains \citep{wang-etal-2021-k} or other modalities \citep{alayrac2022flamingo}. Modules can also be designed to attend over explicit key-value memory representations of entities and facts \citep{verga-etal-2021-adaptable} and general domain knowledge \citep{cheng-etal-2023-decouple} to enable a model to perform certain types of operations such as arithmetic reasoning \citep{trask2018neural,andor-etal-2019-giving}. More broadly, function composition enables the use of arbitrarily complex auxiliary modules. We highlight how function composition has been used to inject knowledge into models in \S\ref{ss:knowledge_injection}. For an overview of how modules can be used to allow language models to use tools and to act, we direct the reader to \cite{auglms:2023}.

 
\paragraph*{Rescaling} The output representations can also be directly transformed via element-wise multiplication with a vector of learned parameters: $f_i^\prime(\vx) = f_{\vtheta_i }(\vx) \odot \vphi$. Crucially, this is equivalent to stacking the original function $f_{\vtheta_i }$ with a linear transformation $\mW = \mI\vphi$.
Such task-specific rescaling is typically applied to batch normalisation parameters in computer vision \citep{Bilen2017Universal} and to layer normalisation parameters in NLP \citep{houlsby2019parameter}. 

The adapter (IA)${}^3$ \citep[Figure~\ref{fig:nature_modularity:ia3}; ][]{Liu2022IA3} multiplies learned vectors with the keys and values in self-attention blocks and the intermediate activations in position-wise feedforward networks in the Transformer. Rescaling activations favours dimensions that are important for a given task. Multiplication with a binary mask is a special case of rescaling that incorporates sparsity: \citet{strezoski2019many} multiply a task-specific random binary mask $\vb$ with a function's input $\vx$ at every layer.

Overall, standard function composition methods such as adapter layers typically require more parameters as the new function depends on a model's input size and hidden size. While they do not require storing the gradients of the frozen parameters, they increase the number of operations at training and inference time. State-of-the-art function composition methods match or outperform standard fine-tuning.

 
 
\subsection{Hypernetworks}
\label{sec:computation_function:hyper_network}

% DEFINITION
In the above-mentioned adapters, different modules $\vphi_1, \ldots, \vphi_{|M|}$ correspond to disjoint sets of parameters. However, the modules may benefit from sharing information. Rather than learning $\vphi_i$ directly, a (small) neural network $\mW$, known as a hypernetwork, can generate the module parameters instead, conditioned on an embedding $\valpha$ \citep{Ha2017HyperNetworks,platanios-etal-2018-contextual}. Thus, $\phi = \mW\valpha$. As a result, the modules are `entangled', which violates the strong definition of modularity that postulates that modules are autonomous \citep{goyal2019recurrent}. In fact, in hypernetworks, computation and routing are inseparably intertwined. In fact, foreshadowing our discussion in \S~\ref{sec:routing:hyper_network}, the embedding $\valpha$ can also be interpreted as unnormalised, learned routing scores for each task. In turn, the parameter generator weight would correspond to a set of modules stacked column-wise: $\mW = [\vphi_1, \dots, \vphi_{|M|}]$.

Hypernetworks can also be conditioned on inputs $\vx$ (Figure~\ref{fig:nature_modularity:hyper_network}). For instance, in conditional batch normalisation \citep{de2017modulating}, rescaling parameters are generated based on a representation of the model input obtained via an LSTM. Feature-wise linear modulation \citep[FiLM;][]{Perez2018} generates an element-wise affine transformation that is applied to image features, conditioned on the linguistic input of the model, for text-and-vision tasks. In self-modulation for Generative Adversarial Networks \citep{Chen2019}, the affine transformation is applied to hidden representations of the generator conditioned on the noise sample.  \citet{Bertinetto2016Learning} conditions the parameter generator on individual examples, in order to perform one-shot learning.

Hypernetworks have been used to generate a diverse set of module parameters, including classifier heads \citep{ponti-etal-2021-parameter}, continuous prompts \citep{he2022hyperprompt}, and adapter layers \citep{Ustun2020,Ansell2021MADG,mahabadi2021parameter}, most commonly conditioned on task \citep{mahabadi2021parameter} or language embeddings \citep{Ustun2020,Baziotis2022MultilingualMTHyperAdapter}. Such task or language embeddings $\valpha$ can themselves be learned directly from random initialisations or fixed as the typological features of a language \citep{Ustun2020,Ansell2021MADG}. This is a strategy to integrate side (or metadata) information about the relationship among languages. Other examples of side information, such as the example label $y$, can be integrated into the hypernetwork input embedding via bi-linear interaction \citep{Chen2019}.


Nevertheless, even the smallest possible module generator network is a linear projection $\mW \in \mathbb{R}^{d_\vphi \times d_\valpha}$.
To make the hypernetwork more parameter-efficient, it can be shared across layers by conditioning it on the module position in the neural architecture, in addition to the task index \citep{mahabadi2021parameter}. In general, the hypernet can be conditioned on multiple (concatenated) embeddings: e.g., one corresponding to the task index and another to the language index. This allows the hypernetwork to generalise systematically to new task--language combinations at inference time. In particular, the hypernet can either generate a single module from all the embeddings \citep{ponti-etal-2021-parameter} or separate modules \citep{Ansell2021MADG,Ustun:2022hyperx}. In turn, the embedding combination chosen for any example is a form of hard routing (cf.\ \S~\ref{sssec:hardrouting}).

 

\subsection{Unifying Parameter, Input, and Function Composition} \label{sec:unifying_composition}

While the above methods may seem different, they all covertly share a similar functional form. \citet{He2021UnifiedAdapters} cast LoRA \citep{hu2021lora}, prefix tuning \citep{Li2020PrefixTuning}, and bottleneck adapters \citep{houlsby2019parameter}, representative methods of the three composition functions, into the same framework. We extend their framework to cover parameter composition, input composition, and function composition in general. Specifically, all modular computation functions can be reduced to function composition: the output of the function $f_{\vtheta_i}$ of a model is added to a new term that depends on a learned function $f_{\vphi}$: $f_i^\prime(\vx) = f_{\vtheta}(\vx) + f_{\vphi_i}(\vx)$. 

For function composition methods, this form is the most natural. In the case of parallel adapters, for instance, $f_i^\prime(\vx) =  f_{\vtheta_i}(\vx) + f_{\vphi_i}(\vx)$ where $f_{\vtheta_i}(\vx)$ may be a multi-head attention module $f_{\vtheta_i}(\vx) = \text{MHA}(\mC, \vx) = [\text{head}_1, \ldots, \text{head}_h] \mW_o$, with $\text{head}_j = \text{Attn} (\vx \mW_q^j, \mC \mW_k^j, \mC \mW_v^j)$, and $f_{\vphi_i}(\vx) = \mW^d(\sigma(\mW^u \vx))$. In this setting, $\vtheta_i$ and $\vphi_i$ are independent and must only agree regarding the dimensionality of their inputs and outputs.

For parameter composition methods, which modify the parameters directly,  the dimensionality of the module parameters $\vphi$ should match exactly the original parameters $\vtheta_i$. For instance, if we apply the module to a linear projection, then they should consist of weight matrices $\vtheta_i = \mW_i \in \mathbb{R}^{d_\vx \times k}$ and $\vphi_i = \mV_i \in \mathbb{R}^{d_\vx \times k}$, respectively. Because of linearity:
\begin{equation*}
f_i^\prime(\vx) = f_{\vtheta_i \oplus \vphi}(\vx) = f_{\mW + \mV} (\vx) = (\mW + \mV) \vx =  \mW \vx + \mV \vx = f_{\vtheta_i}(\vx) + f_{\vphi_i}(\vx)
\end{equation*}
For instance, in the case of LoRA \citep{hu2021lora}, $\mV = \lambda \mB_i \mA_i$. In the case of sparse adapters \citep{ansell2021composable}, $\mV$ is a sparse matrix. 

For input composition methods, with the form $f_i^\prime(\vx) = f_{\vtheta_i}([\vphi_i, \vx])$, the equivalence is derived as follows. Prefix tuning \citep{Li2020PrefixTuning} generalises other continuous prompt methods by concatenating prefix vectors $\vphi_i = \mP_k^i, \mP_v^i \in \mathbb{R}^{l \times d}$ to the keys and values of self-attention. \citet{He2021UnifiedAdapters} show that prefix tuning can be expressed in the following way:
\begin{align*}
f_i^\prime(\vx) & = \text{Attn}(\vx \mW_q^i, [\mP_k^i, \mC \mW_k^i], [\mP_v^i, \mC \mW_v^i]) \\
& = (1-\lambda(\vx)) f_{\vtheta_i}(\vx) + \lambda(\vx) \: \text{softmax}(\vx \mW_q \mP_k^\top) \mP_v
\end{align*}
where $\lambda(\vx)$ is a scalar that represents the sum of normalised attention weights on the prefixes and $f_{\vtheta_i}(\vx)$ is the attention module in a Transformer. If we set, $f_{\vphi_i}(\vx) = \text{softmax}(\vx \mW_q \mP_k^\top) \mP_v$, then we obtain a function composition $(1-\lambda(\vx)) f_{\vtheta_i}(\vx) + \lambda(\vx) f_{\vphi_i}(\vx)$ that incorporates a weighted addition. For function and parameter composition, in contrast, the sum is unweighted.

Overall, despite their conceptual differences, most modular approaches are similar in their functional form and can be expressed as function composition. In practice, the way different methods are realised, however, leads to different trade-offs, which we illustrate in Table \ref{tab:computation_function_comparison}. Recent empirical studies \citep{Mahabadi2021Compacter,He2021UnifiedAdapters,Liu2022IA3} provide further evidence for the strengths and weaknesses of different methods. For instance, prompt tuning \citep{Vu2022prompt} underperforms other methods due to limited capacity while intrinsic dimensionality \citep{aghajanyan-etal-2021-intrinsic} uses a very small number of parameters but leads to a large memory footprint and poor performance. Fine-tuning only biases \citep{ben-zaken-etal-2022-bitfit} has a small memory footprint but achieves lower performance. Finally, function composition methods such as adapter  layers \citep{pfeiffer2020adapterfusion} and compacter layers \citep{Mahabadi2021Compacter}, achieve the best performance, but add more parameters. (IA)$^3$ \citep{Liu2022IA3} mitigates this by composing a lightweight linear diagonal weight. 
Modular deep learning architectures, however, have many other differences beyond their choice of computation function. In the following sections, we discuss the routing, aggregation, and training settings for the modules presented so far.


\begin{tcolorbox}{%
    \begin{itemize}
      \setlength\itemsep{-.1em}
        \item Computation functions may consist of any neural module. Modules may modify the original \textbf{parameters}, be concatenated to the \textbf{input}, or composed with the original \textbf{function}.
        \item \textbf{Parameter composition} methods utilise sparsity or low-rank constraints. They are very parameter-efficient and efficient at inference time and show strong performance. 
        \item \textbf{Input composition} methods concatenate a function's input with a parameter vector via prompting, continuous prompts, and retrieval augmentation. They are extremely parameter-efficient but inefficient during training and inference and require large models.
        \item \textbf{Function composition} methods augment a model with arbitrary functions via parameter sharing, adapters, or rescaling. They require more parameters but often achieve the best performance.
        \item Rather than learning module parameters directly, \textbf{hypernetworks} can be used to generate module parameters, which enables sharing of information and conditioning on auxiliary information.
        \end{itemize}
    }%
\end{tcolorbox}

\section{Routing Function}
\label{sec:routing}
In the previous section, we described how to compose a sub-function $f_i$ with shared weights $\vtheta$ with a single module function with weights $\vphi$. However, in a modular neural architecture, \textit{multiple} modules are available from an inventory 
$M = f_{\vphi_1}, \dots, f_{\vphi_{|M|}}$.
% $M = \vphi_1, \dots, \vphi_{|M|}$. 
A decision-making process is required to determine which modules are active, conditioned on the model input or auxiliary metadata. This process is implemented through a routing function $r(\cdot)$ that assigns a score $\alpha_i$ to each module from the inventory $M$. These scores determine which subset of modules is active, i.e.\ contributes to the computation.
We provide an overview of different routing methods in \cref{fig:Routing}.

When metadata such as expert knowledge about sub-tasks (or skills) involved in a task is available, $r(\cdot)$ can be designed as a \textit{fixed} function, that is, each routing decision can be made \textit{a priori} (Figure~\ref{fig:routing:fixed_routing}). For instance, when using a language model to generate dialogue in Swahili, a task module for dialogue generation and a language module for Swahili can be selected. When no such prior information is available---for instance when modelling heterogeneous unlabelled data---routing of a given example needs to be \textit{learned} (Figures~\ref{fig:routing:hard_routing}-\ref{fig:routing:soft_routing}). In this case, the routing function can be conditioned on  the current example $\mathbf{x}$.\footnote{Alternative non-parametric routing strategies include random routing \citep{Zuo2022Taming, Wang2022AdaMix} or routing based on hash functions \citep{Roller2021Hash}.} 


Unfortunately, learned routing is crucially under-constrained, as multiple possible ways of decomposing tasks into sub-tasks are reasonable \citep{jacobs1991task}. In addition, it presents a series of unique challenges (see \S~\ref{ssec:challenges_routing}). In an empirical study on synthetic data, \citet{mittal2022is} found that learned routing is sub-optimal compared to fixed routing, as it tends to under-utilise modules and to specialise them to a lesser degree. This behaviour is exacerbated as the number of tasks in the data grows. In real-world applications, \citet{Muqeeth2022Models} report similar results; however, \citet{ponti2022combining} find that learned routing may surpass expert module selection even in settings where tasks are procedurally constructed to require certain skills, such as instruction following in simulated environments.

Learning-to-route can roughly be split into \textit{hard} routing  and \textit{soft} routing \citep{rosenbaum2019routing}. \textit{Hard} routing methods learn a binary selection of modules, similarly to the fixed routing scheme, where only a subset of modules is selected for each decision-making step (Figure~\ref{fig:routing:hard_routing}). Inference for hard routing systems typically builds on score function estimators \citep{willianms1988toward, williams1992simple} or stochastic re-parameterisation \citep{Jang2017Gumbel}. On the other hand, \textit{soft} routing methods learn a probability distribution over modules \citep[Figure~\ref{fig:routing:soft_routing}; ][]{jacobs1991task}.
While soft selection is more easily amenable to end-to-end learning via gradient descent,
hard selection may lead to a sparse architectural design, owing to the fact that inactive modules are not part of the forward and backward computation graph.
This reduces time complexity while augmenting the model capacity \citep{bengio2013estimating}.

While not the central focus of this paper, routing algorithms  have recently garnered significant attention, due to their efficiency implications. There exists an intricate interplay between routing techniques and sparsity within modular models: When a modular architecture exhibits sparsity, signifying that only a select few modules are active during inference,  a notable reduction in computational complexity during inference can be achieved \citep{fedus2022review}.\footnote{In \S \ref{sec:routing:softlearnedrouting} "Token-Level Routing" we provide a brief overview over recent works that focus on the efficiency aspect. However, it is important to note, that the focus of this paper is centred around modularity and not efficiency.} 
Finally, \citet{shen2023mixtureofexperts, jang2023exploring} showcase how sparse models, when combined with instruction tuning techniques, can yield substantial gains in performance and efficiency over dense models, potentially reshaping the landscape of large language model design. 

\begin{figure}[t]
    \centering
    \begin{subfigure}{.25\linewidth}
    \centering
        \vspace{1em}
        \includegraphics[width=.99\linewidth]{img/Deterministic_routing.pdf}
        % \vspace{0.4em}
        \caption{Fixed Routing }
        \label{fig:routing:fixed_routing}
    \end{subfigure}
    \hspace{.5em}
    \begin{subfigure}{.25\linewidth}
    \centering
    \vspace{0.6em}
        \includegraphics[width=.99\linewidth]{img/Hard_routing.pdf}  
        % \vspace{1.4em}
        \caption{Learned Routing (Hard)}
    \label{fig:routing:hard_routing}
    \end{subfigure}
    \hspace{.5em}
    \begin{subfigure}{.25\linewidth}
    \centering
        \vspace{0.6em}
        \includegraphics[width=.99\linewidth]{img/soft_routing.pdf}  
        \caption{Learned Routing (Soft)}
        \label{fig:routing:soft_routing}
    \end{subfigure}
    \caption{ Different routing methods. (a) \textbf{Fixed Routing:} Examples are passed to modules based on a pre-defined logic, known a priori. (b) \textbf{Hard Learned Routing:} Learned hard selection modules. (c) \textbf{Soft Learned Routing:} Soft selection and weighting of modules.
    }
    %\vspace{-1mm}
\label{fig:Routing}
% \vspace{-1.5mm}
\end{figure}

\subsection{Fixed Routing}
\label{sec:routing:deterministic}

Making the routing decision \textit{a priori}---i.e. when we utilise metadata (e.g. task identity $t$) to make the discrete routing decisions \textit{before} training---is referred to as fixed routing (Figure~\ref{fig:routing:fixed_routing}). Here the routing function $r(\cdot)$ simplifies to a  
selection of a subset of modules $K \subseteq M$ for the examples with certain metadata: 

\begin{equation}
r(\vphi_i) = 
   \begin{cases}
       1 \qquad \mathrm{if} \, i \in K\\
       0 \qquad \mathrm{otherwise}
   \end{cases}
\end{equation} 
This function defines a binary matrix $A \in \{0, 1\}^{|\mathcal{T}| \times |M|}$, where the number of rows corresponds to possible tasks and the number of columns corresponds to the size of the module inventory.

One simple example of fixed routing in multi-task learning is when all parameters, except the final classification layer, are shared among all tasks \citep{ruder2017overview}. Independently from the task identity, the examples are passed through the same network until after the penultimate layer. The penultimate layer's representations are then \textit{routed} to their respective final classification layer according to the task identity. This boils down to setting $|K| = 1$, with the additional constraint that tasks cannot share modules, which results in the allocation matrix being an identity matrix, $A = I$.  

While not immediately apparent, methods that adapt pre-trained models towards individual tasks \cite[][\textit{inter alia}]{Rebuffi2017Adapters1,Rebuffi2018Adapters2, houlsby2019parameter,Bapna2019Adapters, Li2020PrefixTuning, Liu2022IA3,hu2021lora, ansell2021composable,ben-zaken-etal-2022-bitfit}--as discussed in \Cref{sec:nature_modularity}--deterministically route representations through the newly introduced module $f_\vphi$.
Given that the pre-trained weights are frozen and modules trained on different tasks can be added or removed,
the components become modular even if they are developed asynchronously and independently of each other \citep{pfeiffer2020adapterfusion}. In a sense, community-based hubs of pre-trained adapters such as AdapterHub \citep{pfeiffer-etal-2020-adapterhub} can be considered as ever-evolving multi-task models, the development of whose components has been distributed throughout the community.\footnote{Alternatively, combining entire models stored in model repositories via distillation \citep{Khanuja2021} or averaging \citep{Matena2022} can also help avoid negative interference \citep{Shachar2022ColD}; however, this is usually less efficient and subject to  limitations such as those discussed later in \cref{sec:compositionality}.} 
Moreover, since newly introduced weights are encapsulated between frozen (shared) weights, adapted representations of intermediate layers are implicitly aligned as they are passed as input to the same frozen components.

\citet{hampshire1992meta} were possibly among the first to train independent experts for a series of sub-tasks known \textit{a priori}. In this case, the (fixed-size) subset of experts $K$ associated with each task $t$ is assumed as given, resulting in the rows of $A$ being $k$-way vectors.  
In cross-lingual transfer, any problem can be decomposed into a task and language variety. Fixed routing can select separate language and task components, and facilitate generalisation to new, unobserved combinations of tasks and languages at inference time \citep{pfeiffer-etal-2020-mad,ponti-etal-2021-parameter,Ustun:2022hyperx}. In this case, $|K| = 2$. Similarly, in reinforcement learning, \citet{heess2016learning} and \citet{devin2017learning} design a modular policy that is composed of a robot-specific module and a task-specific module, which are instantiated as separate neural networks. Composing these modules enables generalisation to unseen robot--task combinations.

Beyond task identity, routing can be performed based on other metadata such as language, domain, or modality information. 
\cite{Pfeiffer2022Lifting} add adapters for each language to a multilingual language model during pre-training on unlabelled text. \citet{Fan2021Beyond} route deterministically for multilingual machine translation according to the language family: as a consequence, all languages in a family share the same expert. In a similar vein, \cite{Gururangan2022Demix} add domain-specific adapters to language models, deterministically routing based on the text source domain. This concept was further extended by \citet{Li2022BranchTrainMerge}, who proposed the branch--train--merge method: copies of the same model are trained on different domains and then averaged. Finally, modality can also inform fixed routing, such as in vision-and-language models \citep{pfeiffer-etal-2022-xgqa}. This allows for adapting the encoders of different modality streams.
 

\subsection{Learned Routing}
\label{sec:routing:learned}

When the routing function $r(\cdot)$ is not known in advance, it can be implemented as a learnable neural network with parameters $\vrho$. In input, it receives the example representation $\vx$ or metadata such as the task $t$. In output, it returns routing scores $\valpha$. Usually, $r_\vrho$ is a linear projection or a Multi-Layer Perceptron. While the former represents a less expressive family of functions, the latter may collapse into ignoring the input features. Note that learning the routing function also implies that the specialisation of each module is unknown. Thus, modules are not trained on different sets of examples; rather, they are all trained jointly with the routing function.

\subsubsection{Challenges of Learned Routing}
\label{ssec:challenges_routing}
Learned routing introduces a number of challenges, including
\textit{training instability}, \textit{module collapse} \citep{kirsch2018modular}, and \textit{overfitting}. These were first systematically described by \citet{rosenbaum2019routing}, and we follow a similar taxonomy. In general, they identify two root causes for all these challenges: first, the need to balance between exploration and exploitation \citep{sutton1986two}. More specifically, routing must find the optimal trade-off between allocating information to the most suitable modules versus under-explored modules. Second, routing must share modules across examples or tasks in such a way as to reap the benefits of positive transfer while avoiding negative interference. We elaborate on the individual challenges below.

\textbf{Training Instability} emerges especially in the early phases of training; at this point, modules are randomly initialised and have no clear functional specialisation. Thus, the router cannot make any principled decision in selecting modules. On the other hand, modules do not start specialising until they are consistently routed to different subsets of tasks or examples.

Curriculum learning can mitigate this challenge to some extent \citep{chang2018automatically}, as simpler tasks require simpler sets of skills. However, this assumes that information about task complexity is available and that the data can be ordered accordingly. As an alternative, the router parameters can be trained with a different learning rate than the module parameters, either lower \citep{rosenbaum2017routing} or higher \citep{ponti2022combining}. These create two different dynamics: either the necessary skills for a task are determined after specialisation, or the relationship among tasks is figured out first and modules are updated accordingly. 

\textbf{Module Collapse} describes scenarios where only a small number of modules (in the extreme case, one) from the available inventory are selected. This leaves the remaining modules untrained and negatively impacts their overall diversity. Often, this results from excessively favouring exploitation over exploration, which leads to sub-optimal results. To amend this, \citet{ahn2019deep} use $\epsilon$-greedy routing for initial exploration of all modules and afterwards switch to learned routing. Other strategies to avoid module collapse include auxiliary losses for load balancing \citep{shazeer2017outrageously, fedus2021switch} and intrinsic rewards that encourage diversity in module selection \citep{Cases2019Recursive}. The choice of information that conditions the router also plays an important role: metadata, e.g.\ text genre \citep{Cases2019Recursive} or task identity \citep{kudugunta2021beyond}, make routing more robust than individual examples. The diversity of training tasks also facilitates diversity in routing selections \citep{chang2018automatically,caccia2022multihead}. \citet{dua-etal-2022-tricks} warms up the sampling temperature over training, in order to over-sample domains with fewer examples in unbalanced distributions. 

\textbf{Overfitting} to noise is a risk faced by deep modular networks due to their ability to model subsets of examples independently \citep{rosenbaum2019routing}. For instance, routing at the token level was shown to lead to performance drops in out-of-domain generalisation for MoEs \citep{artetxe2021efficient}. For a similar reason, gains in pre-training do not always translate into gains in fine-tuning for MoEs \citep{fedus2021switch}. Increased robustness can be achieved by routing conditioned on metadata if available \citep{chang2018automatically, Cases2019Recursive,kudugunta2021beyond}. 
In addition, strategies that favour the combinatorial behaviour of modules yield superior generalisation \citep{chang2018automatically,ponti2022combining}.

\subsubsection{Hard Learned Routing}
\label{sssec:hardrouting}
A model may learn how to select modules through \textit{hard} routing. This implies that the choice of whether a module is active or excluded from the computation graph is binary. Discrete decisions are not amenable to be learned through vanilla gradient descent: since small perturbations of parameters do not affect the selection of modules, the gradient of the loss with respect to the routing parameters is zero. Thus, various methods, including reinforcement learning, evolutionary algorithms, and stochastic re-parameterisation, have been proposed for inference. These are discussed separately below.

On the other hand, hard routing is more efficient than soft routing in terms of time and space complexity. In addition, binary selection implies that parameter updates are localised to a subset of modules. This reflects the intuition that the shifts in distribution of the variables in an environment are similarly local \citep{parascandolo2018learning,goyal2019recurrent}. Since the inactive module parameters are not affected, they remain invariant with respect to the distribution shift.
On top of this, this type of routing may result in variable-size sets of active modules.
This allocates model capacity according to task complexity, which
follows the principle of conditional computation \citep{bengio2015conditional}. In fact, it is fair to assume that the skills required for complex tasks are a superset of those of simpler tasks. For instance, dialogue modelling requires (among others) intent detection, slot filling, and conditional response generation. 

\paragraph*{Reinforcement Learning}
In Routing Networks \citep{rosenbaum2017routing}, Modular Networks \citep{kirsch2018modular}, and the Compositional Recursive Learner \citep[CRL;][]{chang2018automatically}, a router network is trained through reinforcement learning. Specifically, Routing Networks rely on multi-agent RL (MARL), Modular Networks rely on the score function estimator (REINFORCE), whereas the CRL relies on Proximal Policy Optimisation (PPO). Commonly, this family of methods alternate between a score function estimator for the routing parameters $\vrho$ and SGD for module parameters $\{\vphi_1, \dots, \vphi_{|M|}\}$. For a vanilla score function estimator, where routing is conditioned on the input example and $m \in M$, the update takes the form:

\begin{equation}
\begin{aligned}
    \nabla_\vrho \, \E_{\vx, \vy} \, p(\vy \mid \vx, \vtheta, \vphi_1, \dots, \vphi_{|M|}, \vrho) \approx \frac{1}{n} \sum_{i=0}^n \left[ \, p(\vy_i \mid \vx_i, \vtheta, \vphi_m) \, \nabla_\vrho \log p(m \mid \vx_i) \right]
\end{aligned}
\end{equation}

Under this lens, routing becomes a policy $\pi(m \mid \vx)$. If applied layer-wise, each hidden representation at a given layer $1 \geq t \leq l$ constitutes a state $\vh_t \in \mathcal{H}$. The routing policy determines the action, i.e.\ the selection of a module index $m$. In particular, this assumes that the inventory $M$ is shared across layers.\footnote{This encourages module re-usage at different layers.}
In turn, applying the transformation of the corresponding module on the input is equivalent to a transition function $\pi : \mathcal{H} \rightarrow \mathcal{H}$, which returns the next layer's hidden state $\vh_{t + 1}$. The loss function at the top layer corresponds to a (delayed) negative reward, i.e. $\mathcal{L}(\cdot) = - \mathcal{R}$.\footnote{Intrinsic rewards can be added, for instance favouring diversity in the module selection across time steps \citep{rosenbaum2017routing}.} Crucially, in this setting the transition functions are non-stationary, as the module parameters are amenable to change. Because modules are applied sequentially based on the policy, the number of steps of computation in the model can vary when a special halting action is available.

\paragraph*{Evolutionary Algorithms}
Alternatively, routing can be learned via a genetic algorithm. In PathNet \citep{fernando2017pathnet}, the loss function indicates the fitness of a configuration of active modules $K \subseteq M$.
For each task, two configurations are selected at random and trained until a stopping criterion is met. The one incurring the lower loss on a validation set overwrites the other. This copy, in turn, receives a random mutation, and then the procedure is repeated. In $\mu$Net \citep{Gesmundo2022Evolutionary,Gesmundo2022munet}, mutations involve cloning, insertion, and removal of layers. The fitness criteria include not only performance but also parameter efficiency. This approach has been extended to a multi-task setting where multiple agents update different modules asynchronously
\citep{Gesmundo2022Multiagent}. However, as is common for evolutionary algorithms, this search is brute-force and thus highly inefficient.

\paragraph*{Stochastic Re-parametrisation}
Hard routing can also be performed via a continuous relaxation of the discrete latent variable $\valpha$ determining the module allocation. Several stochastic re-parameterisations such as Gumbel-Softmax \citep{Jang2017Gumbel} or the Concrete distribution \citep{maddison2017the} have been proposed for this purpose. Compared to the score function estimator, stochastic re-parameterisations are biased but have lower variance. Moreover, they are differentiable, which makes a hard router trainable in an end-to-end fashion. For instance, AdaShare \citep{sun2020adashare} uses Gumbel-Sigmoid to learn a binary vector for each task that indicates whether a model layer should be included in the forward pass or skipped entirely. This may be interpreted as choosing between a parameterised module and an identity function at each layer.

Stochastic re-parameterisation also allows for selecting module subsets of \textit{varying sizes} for each layer. In Neural Interpreters \citep{rahaman2021dynamic}, this is based on a threshold. Each module is associated with a `signature vector'. The dot product between this vector and the output of an unnormalised routing function (`type inference') conditioned on a token determines a score. If this surpasses a certain threshold, then the module is allowed to access the given token. As an alternative, variable-size module routing can be achieved by learning a soft clustering (a.k.a.\ soft partition) of modules \citep{ponti2022combining,caccia2022multihead}. Thus, each entry $\alpha_{ij}$, which represents the routing of the $j$-th module to the $i$-th task, is constructed as follows:
\begin{align} \label{eq:allocation}
    {\alpha}_{i,j} =  \textrm{sigmoid} \left[ \log \frac{\textrm{sigmoid}({\hat\alpha}_{i,j}) \, u}{(1 - \textrm{sigmoid}({\hat\alpha}_{i,j})) \, (1 - u)}^{1/\tau}\right] \qquad u \sim \mathrm{Uniform}(0, 1).
\end{align}
where $\hat\alpha_{ij}$ represents the unnormalised routing score.
This latent variable also admits priors such as the Indian Buffet Process \citep{griffiths2011indian} to encourage both diversification and sharing of module subsets across tasks \citep{ponti2022combining}. \citet{caccia2022multihead} extend this framework to multi-head routing, where different modules can be allocated to contiguous subsets of dimensions of the layer's input and output. While this just requires as many copies of $\alpha$ as the number of subsets of dimension, it provides higher expressivity to the routing function.

\paragraph*{Top-$k$ Selection}
Finally, hard selection can rely on top-$k$ selection from (possibly unnormalised) scores $\valpha$ over modules. In the case of Independent Causal Mechanisms \citep{parascandolo2018learning}, $\valpha$ is given by a discriminator that scores the outputs of a generator, and $k=1$. In the case of Recurrent Independent Mechanisms \citep{goyal2019recurrent}, the scores are derived from attention between modules and the input, and $k>1$. These methods are grounded on the assumption that the competition among modules to be activated facilitates their specialisation (see \cref{ssec:causal} for more details).

\subsubsection{Soft Learned Routing} 
\label{sec:routing:softlearnedrouting}

\paragraph*{Mixture of Experts} To sidestep {discrete} selections of modules, several works propose \textit{soft} routing methods, where {all} modules are selected and aggregated according to a \textit{weighted combination}, i.e. a mixture of experts \citep[MoE;][]{jacobs1991adaptive,Jordan1994Hierarchical}.\footnote{In the following sections we use the term ``expert'' and ``module'' interchangeably to reflect common practice in the body of research on MoEs.} Here, the router learns a probability distribution over the available modules, i.e. $p(M) = r_\vrho(\cdot)$. Hence, routing and aggregation take place as:

\begin{equation}
\label{eq:MoE}
    f_i^\prime(\vx) = \sum_{\vphi_j \in M} r(\vphi_j) \, f(\vx; \vtheta_i, \vphi_j)
\end{equation}

In contrast to the discrete selection of {hard routing} methods, this setup is easily trained end-to-end via gradient descent. A number of works \citep[][\textit{inter alia}]{Eigen2013LearningFactored,Meyerson2018,wortsman2020supermasks} train a continuous weighting (i.e. a mixture) of all modules; however, this limits the degree of modularity as parameter updates are not local; instead, they always affect all modules. Additionally, activating \textit{all} modules for each example significantly increases the computational cost for each forward and backward pass through the network. To circumvent  this,  \citet{shazeer2017outrageously} and \citet{Lepikhin2021GShard} only route to the top-$k$  
of $|M|$ modules, where $1 < k < |M|$. The output representations of the $k$ {active} modules are averaged according to the respective routing weights, whose sum is re-normalised to 1. Thus, top-$k$ MoEs stand between hard routing, as only a subset of modules is active, and soft routing, as their average is weighted by the routing scores. In practice, a layer performs the following computation: 

\begin{equation}
    f_i^\prime(\vx) = \sum_{\vphi_j \in \, \text{top}_k[r(\bm{\vphi})]} \frac{r(\vphi_j)}{\sum_1^k r(\vphi)} \, f(\vx; \vtheta_i, \vphi_j)
\end{equation}
 
\citet{fedus2021switch} and \citet{Clark2022UnifiedScaling}  demonstrate that even top-$1$ routing can achieve competitive results for language modelling. 

\paragraph*{Token-Level Routing}
MoEs have recently undergone a revival as part of the efforts to scale Transformers. In particular, MoE Transformers route to a subset of Feed-Forward Network (FFN) modules per layer instead of a single FFN. The focus of these works is on computationally efficient training of very large models. This is achieved by splitting the input tokens across different (hardware) accelerators. The MoE routing algorithm is therefore required to (ideally) uniformly distribute the tokens of all the examples in an input batch across all accelerators, i.e. to \textit{load balance} computation across ``experts''. The dominating routing strategy is for each \textit{token} to choose the top-$k$ \textit{experts}  \citep{shazeer2017outrageously, Lepikhin2021GShard, fedus2021switch, Clark2022UnifiedScaling, yang2021m6, dua-etal-2022-tricks, hazimeh2021dselect, rajbhandari2022deepspeed, riquelme2021scaling, Du2022Glam, Zoph2022STMOE}. Alternative approaches let each \textit{expert} choose the top-$k$ \textit{tokens} \citep{you2022speechmoe2, Zhou2022MoEExpert} or \textit{globally} determine the best routing path  \citep{lewis2021base}.\footnote{For more details on load balancing methods we refer to \citet{fedus2022review}, Chapter 4.} 

However, since routing is conditioned on the token level,  
and the \textit{load balancing} restriction limits the system from routing an entire example to a single module,  
the system potentially has to relearn similar concepts in multiple modules. Hence, load balancing hinders the router from selecting the single best module for longer (e.g., repetitive) sequences. This is investigated further by \cite{lewis2021base}, who find  that sparse models  route syntactically and semantically similar {words} (in contrast to sentences or phrases)  to the same modules. This sheds light on the limited expressiveness of modules which are learned on the {token}-level. 
Since scaling is the main focus of these works, their goals are {orthogonal} to modular approaches centred on parameter efficiency, transfer--interference trade-offs, and combinatorial generalisation.

\paragraph*{Example-Level Routing}
Nevertheless, one could imagine obtaining the best of both worlds by hybridising sparse MoE Transformers models with deterministic or learned routing strategies from \cref{sec:routing:deterministic} and \cref{sssec:hardrouting}.
Instead of routing each individual token separately, all tokens of a single example can be routed to the same experts. 
\citet{kudugunta2021beyond} experiment with two versions of example-level routing for machine translation: In \textit{sentence-level} routing, they average pool over the token embeddings, and condition the router on the resulting representation. In \textit{task-level} routing, a task embedding is trained, based on which the router learns the distribution over modules. In a similar vein, \citet{Gupta2022Sparsely} and \citet{Xi2022UFO} implement  \textit{task-level} routing across modular experts to improve the amount of knowledge sharing during multi-task learning in NLP and computer vision, respectively. 

Since task identity (or other metadata) is not always given, especially in continual learning, it can be inferred through an auxiliary model. \Citet{van2019three} refer to this scenario as `\textit{class-incremental learning}'. For instance, the current task can be identified based on the lowest predictive uncertainty or an auxiliary task classifier \citep{Oswald2020}. In these cases, routing can depend on the predicted task identity.


\paragraph*{Mitigating Module Collapse}
To address the challenge of module collapse, which was previously discussed in \S \ref{ssec:challenges_routing}, several strategies have been introduced to enhance the effectiveness of models in utilizing the available experts' capacity. One such approach, presented by \citet{shen2023moduleformer}, introduces a novel loss function centered on Mutual Information. This loss aims to maximize the mutual information between the input and the target module, effectively mitigating module collapse issues. Another innovative solution, put forward by \citet{chi2022representation}, involves the modification of the routing algorithm. This modification incorporates techniques like dimension reduction, L2 normalization, and adjustment of gating temperature, all designed to address the challenges associated with module collapse. \citet{puigcerver2023sparse} employ a fully differentiable soft assignment mechanism by applying weighted combinations of representations to each module, allowing for enhanced model capacity without significantly increasing inference costs. \citet{muqeeth2023soft} tackle module collapse by employing a weighted average-based merging approach of the module's parameters.
 
\subsubsection{Hypernetworks} \label{sec:routing:hyper_network}

In addition to hard and soft routing, hypernetworks \citep{Ha2017HyperNetworks}, as introduced in \Cref{sec:computation_function:hyper_network}, can be considered a third kind of routing, with unnormalised routing scores.  
More formally, the parameters  $\vtheta_t \in \mathbb{R}^d$ for a task $t$ can be generated by a linear function $\Phi \alpha_t$. The task embedding $\alpha_t \in \mathbb{R}^{|M|}$ can be interpreted as the output of a task-level routing function with unnormalised scores over $|M|$ modules. In turn, the generator $\Phi \in \mathbb{R}^{d \times |M|}$ can be considered a matrix of module parameters stacked column-wise, where each module has $d$ parameters. Thus, the generated parameters $\vtheta_t$ is a linear combination of the columns of the linear generator. This is also reminiscent of tensor factorisation models where parameters are factorised into shared tensors and task-specific tensors \citep{Yang2016Deep}, which in hypernetworks correspond to the generator and the task embedding, respectively. However, hypernetworks learn both sets of parameters jointly rather than obtaining them from a factorisation of task-specific networks \textit{a posteriori}. 

\begin{figure}[t]
    \centering
    \begin{subfigure}{.30\linewidth}
    \centering
        \includegraphics[width=.99\linewidth]{img/layer_routing.pdf}  
        \caption{Layer-wise routing}
        \label{fig:routing:layer_routing}
        \end{subfigure}
            \begin{subfigure}{.30\linewidth}
    \centering
        \includegraphics[width=.99\linewidth]{img/naive_global_routing.pdf}  
        \caption{Global routing (naive)}
        \label{fig:routing:naive_global_routing}
    \end{subfigure}
    \begin{subfigure}{.30\linewidth}
    \centering
        \includegraphics[width=.99\linewidth]{img/true_global_routing.pdf}  
        \caption{Global routing}
        \label{fig:routing:true_global_routing}
    \end{subfigure}
    \caption{ Different routing levels. (a) \textbf{Layer-wise Routing:} The indices are chosen based on the input to the current layer. (b) \textbf{Naive Global Routing:} The same indices of modules are chosen for all the layers of the model. (c) \textbf{Global Routing:} The configuration (possibly different for each layer) is chosen globally. 
    }
    %\vspace{-1mm}
\label{fig:Routing_level}
% \vspace{-1.5mm}
\end{figure}

\subsection{Level of Routing}

Another aspect of designing a routing function is its level of granularity. Routing can select modules globally for the entire network, make different allocation decisions per layer, or even hierarchically select sub-routers. This last method is also referred to as `dispatched routing' by \citet{rosenbaum2017routing}. A naive version of global routing (Figure~\ref{fig:routing:naive_global_routing}) assumes that a single routing configuration is shared across layers. Allowing for different decisions per layer (\cref{fig:routing:layer_routing}) is more challenging as the space of potential architectures grows exponentially as $|M|^l$, where $l$ is the number of layers or sub-functions of the network. In fact, to compute the posterior over parameters, one would need to marginalise over every possible configuration of $A = [\alpha_1, \dots, \alpha_l]$. \citet{kirsch2018modular} resort to Expectation Maximisation to make it tractable. Instead, per-layer routing (\cref{fig:routing:true_global_routing}) assumes conditional independence among decisions, thus facilitating scaling.
Crucially, routing scores are sometimes employed not only to select a subset of modules but also to aggregate their outputs. This second purpose is addressed in more depth in \cref{sec:compositionality}.

Most methods assume that routing decisions occur in a sequence, whose length is bounded or unbounded. This is the case where the output of every layer is fed into the next. However, routing may also involve defining both the selection of modules and their order of composition (i.e., the model architecture). For instance, in Neural Module Networks \citep[NMNs;][]{andreas2016nmn,andreas2017modular}, the routing function consists of a parser that takes in a query and produces a dependency tree. This is post-processed and transformed into a tree graph where nodes are modules and directed edges control the flow of the information, i.e. route the output(s) of a subset of modules as input to another module. In Modular Meta Learning, \citet{alet2018modular} alternate between sampling compositional graphs using simulated annealing \citep{kirkpatrick1983optimization} and performing a step of gradient descent on the network parameters for a set of meta-training tasks.


\begin{tcolorbox}{
    \begin{itemize}
      \setlength\itemsep{-.1em}
        \item The Routing Function is a critical component in modular neural networks, responsible for determining how information flows through modules.
        \item Routing can be categorized into Fixed Routing, Learned Routing, and Hypernetworks.
        \begin{itemize}
            \item \textbf{Fixed Routing} uses predetermined rules to direct information flow.
            \item \textbf{Learned Routing} employs neural networks to dynamically allocate modules based on input data or task information.
            \begin{itemize}
                \item Challenges in Learned Routing include Training Instability, Module Collapse, and Overfitting, which require specialized strategies for mitigation.
             
                \item \textbf{Hard Learned Routing} involves discrete module selection, often requiring reinforcement learning or stochastic re-parameterization for training.
                \item \textbf{Soft Learned Routing} often use weighted combinations of modules, where predominantly top-$k$ routing strategies are employed for computational efficiency.
            \end{itemize}
            \item \textbf{Hypernetworks} offer a flexible approach by generating task-specific parameters with unnormalized routing scores.
        \end{itemize}
        \item Routing decisions can occur at different levels of granularity, including global, per-layer, and hierarchical routing.
        \end{itemize}
    }%
\end{tcolorbox}

\section{Aggregation Function}
\label{sec:compositionality}
 
While in the previous section on \textit{routing} we have covered the topic of how to \textit{select} different modules during training, we will now focus on how we can \textit{aggregate} these functions in order to combine the respective information. It is important to emphasise that, for the majority of current approaches, routing and aggregation are inseparable; that is, the \textit{selection} and \textit{aggregation} of modules are performed simultaneously.\footnote{Combining modules has the potential to significantly improve inference speed.} 
On the other hand, the strategies for \textit{aggregating} functions in this section are reminiscent of the taxonomy previously discussed for \textit{computation} functions (see \S\ref{sec:nature_modularity}); while in the latter we looked  into the composition of \textit{shared} components with \textit{modules}, in this section we provide insights into the composition of \textit{multiple modules}. This is often required when modules are recombined for zero-shot transfer or task-level generalisation (for more details on these applications, see \cref{sec:applications}).
 
In particular, for a subset of active modules $K \subseteq M_i$ the aggregation of modular components can (similarly) be realised on the \textit{parameter} level  
$f_i^\prime(\vx) = f_{\vphi_{1} \oplus \dots \oplus \vphi_{|K|}}(\vx)$, \textit{input} level $f_i^\prime(\vx) = f_{\vtheta_i}([\vphi_1, \dots, \vphi_{|K|}, \vx])$, as well as \textit{function} level $f_i^\prime(\vx) = f_{\vphi_{1}} \circ ... \circ f_{\vphi_{|K|}}(\vx)$. In addition, we cover \textit{output} or \textit{representation} level aggregation $f_i^\prime(\vx) = f_{\vtheta_i}(\vx) \oplus f_{\vphi_1}(\vx) \oplus \dots \oplus f_{\vphi_{|K|}}(\vx)$. Crucially, this differs from parameter aggregation if $f$ is non-linear.
We discuss these different strategies in the following sections. 

\subsection{Parameter Aggregation}
\label{sec:compositionality:det}

\paragraph*{Mode Connectivity}
A natural strategy to aggregate information from multiple modules is interpolating their weights. However, given that neural \textit{architectures} differ, and that hidden representations might not necessarily be equivalent (e.g. under invariance to invertible linear transformations) even if the model architectures are the same \citep{Kornblith2019SimilairtyofNN}, naively aggregating module weights may have catastrophic consequences. However, recent work on \textit{linear mode connectivity} \citep{Frankle2020LinearModeConnect} suggests that under certain conditions, it is in fact possible to interpolate between multiple models, which has positive ramifications for modular aggregation methods. To understand these conditions, we first provide a brief introduction to the constraints under which parameter aggregation is permissible.

 
 The phenomenon
where the minima found by two networks are connected
by a path of non-increasing error,  has been the subject of research for many years  \citep{Freeman2017Topology, pmlr-v80-draxler18a, Garipov2018LossSurface, Nagarajan2019UniformConvergence}. However,  most works demonstrate that mode paths are in fact not linear. While \cite{Nagarajan2019UniformConvergence} find linear paths between networks, their experimental setup requires initialising models with the same set of weights. \citet{Frankle2020LinearModeConnect} and \citet{Neyshabur2020WhatTransfered} demonstrate that this \textit{linear} mode connectivity phenomenon is closely linked to the \textit{Lottery Ticket Hypothesis} \citep{Frankle2019}, which suggests that only a small subset of randomly initialised weights are the main drivers for the final performance of a model---the so-called \textit{winning tickets} (see \cref{sec:nature_modularity:parameter_composition}). When interpolating between  models trained on different tasks but initialised with the same set of weights, the models tend to stay in the same loss basin, indicated by the lack  
of a sudden increase in loss when interpolating the weights. 
Consequently, it appears that the flatness of the basin of the loss landscape translates to better generalisation capabilities of a model. \citet{Gueta2023Knowledge}  find that fine-tuned models reside in distinct regions in weight space, and models within the same region exhibit high performance.  
On the other hand, \citet{Ainsworth2022GitReBasin} argue that the success of such interpolation is strongly connected to the inherent bias of the optimiser being used, and not the neural network architecture itself. % 

\paragraph*{Weight Interpolation} 

Building on the findings of interpolating the weights of models, \cite{ansell2021composable} propose Lottery Ticket Sparse
Fine-Tuning (LT-SFT), described in \cref{sec:routing:sparsemasks}. In particular, they identify language, and task-specific sub-networks $\vphi_l$ and $\vphi_t$.  
These can be aggregated by simply adding them to the base model parameters, i.e.\ $\vtheta^\prime = \vtheta_0 + \vphi_l + \vphi_t$. Instead of identifying task adaptations on subsets of model parameters,
\citet{Ilharco2022EditingModelsTaskArith} propose to edit entire models with further arithmetic operations. For example, for toxic language generation and language modelling tasks, by performing the arithmetic negation operation $\vtheta^{\prime} = \vtheta_0 + (\vphi_\text{general}  - \vphi_\text{toxic})$, their new model $f_{\vtheta^{\prime}}(\vx)$ generates less toxic text. This idea was influenced by the word analogy task (i.e., `word arithmetics') \citep{Mikolov2013DistributedRepresentations}.\footnote{$vec(\text{`King'}) - vec(\text{`Man'}) + vec(\text{`Woman'}) \approx vec(\text{`Queen'})$, with $vec(\cdot)$ denoting word embeddings of the respective words.}

Rather than interpolating sparse adapters,
\citet{asai-etal-2022-attempt} aggregate parameters of soft prompts learned via prefix tuning (\cref{sec:nature_modularity:input_composition}). In order to generalise to new tasks, (frozen) modules from past tasks and a learnable module created for the new task are interpolated according to the weights of an attention mechanism between the modules and the input.

\paragraph*{Model Merging}

Mode connectivity has enabled the fusion of entire models without extensive retraining, yielding performance improvements across a range of applications \citep{choshen2022fusing, Gupta2020Stochastic, yadav2023resolving, jin2023dataless}.  These developments have made frameworks, such as Git-Theta \citep{kandpal2023gittheta},  which facilitate collaborative model development through version control, reasonable. 
Soft Merging of Experts with Adaptive Routing (SMEAR) \citep{muqeeth2023soft} introduces gradient-based training for sparsely activated models, offering specialization benefits.

\subsection{Representation Aggregation}
Closely related to parameter aggregation, representation aggregation consists of interpolating the outputs of individual modules. Crucially, both operations are equivalent if the functions are linear: $(\alpha_i\Phi_i + \alpha_j\Phi_j)\vx = \alpha_i\Phi_i\vx + \alpha_j\Phi_j\vx$. However, this does not hold true for non-linear functions, e.g. if the module is an adapter layer \citep{houlsby2019parameter} or a feed-forward component of a Transformer layer \citep{fedus2021switch}.

\paragraph*{Weighted Representation Averaging}
At the $i$-th sub-function of the model, where multiple modules $\vphi \in M_i$ exist, the representations are passed through the (active) modules, outputting $|K_i|$ (latent) representations $\vh_1, \dots, \vh_{|K_i|}$.
One way of performing aggregation is to learn the weights $\valpha$ to interpolate over the hidden representations:
\begin{equation}
    f_i^\prime(\vx) =\sum_j^{|K_i|} \alpha_j \vh_j 
\end{equation} 
with $\alpha_j$ being a module-specific scalar weighting. 

This aggregation is equivalent to \cref{eq:MoE} when interpreting each weight $\alpha_j \in [0, 1]$ as the output of a soft router, i.e. $\alpha_j = r(\vphi_j)$. Consequently, all soft-learned routing approaches (e.g. MoE) that do not perform top-$1$ routing (see \S~\ref{sec:routing:softlearnedrouting}) also determine how to aggregate the representations of different modules. 

As an extension to the traditional MoE aggregation/routing function, \citet{Ma2018Modeling} propose to learn one aggregation function per task $t$ in a multi-task setup. \citet{Gururangan2022Demix} pre-train modular components for different textual domains $d \in \mathcal{D}$. When utilising the pre-trained modules on unseen data, they weight the output representations $\mathbf{h}_d$ of the respective domain modules $\vphi_d$ according to the posterior distribution over the input examples, i.e. $\valpha = p(\mathcal{D} \mid \mathbf{x})$:

\begin{equation}
    f_{i}^\prime(\vx)  = \sum_{d \in \mathcal{D}} p(d \mid \mathbf{x}) \, f_{\vphi_d}(\vx) 
\end{equation}

This posterior is inferred through the Bayes rule. This does not require any auxiliary model, and only relies on the original $d$-conditioned language model. In fixed routing, module representations are often averaged without weighting \citep{Zhang2022SkillNet, Chronopoulou2022EfficientHierarchical}. Similarly, in hard routing methods, the representations of all \textit{active} modules are averaged, such as in Polytropon \citep{ponti2022combining}, or summed, as in PathNet \citep{fernando2017pathnet}.\footnote{Note that the latter strategy leads to high variance in the norms of hidden representations if the router can select variable-size subsets of modules.} 

One disadvantage of simply learning gating parameters is that the weights do not depend on the hidden representations. Thus, they do not take into account their information content. This issue is tackled by \textit{attention-based aggregation} functions.
 

\paragraph*{Attention-Based Representation Aggregation}

Instead of inferring the weighting before a module has performed its transformation on the latent representation, the aggregation decision can take place \textit{afterwards}. This allows for identifying whether or not the information added by the respective module is ancillary to the target task. 
In AdapterFusion, \citet{pfeiffer2020adapterfusion} propose an attention mechanism \citep{Bahdanau2015NMT} between the stacked hidden representations $\mH_i$ produced by the modules and their input $\vx$:

\begin{equation}
    f_i(\vx) = \text{Attn}(\vx \mQ_i, \mH_i \mK_i, \mH_i \mV_i)
\end{equation} 

where $\mQ, \mK, \mV \in \mathbb{R}^{d \times h}$ are the projections that produce the queries, keys, and values, and $\vx$ is the input representation \textit{to} each of the modules (i.e., the output representation of the previous layer). $\mH_i \in \mathbb{R}^{|M| \times d}$ is a matrix consisting of row-wise stacking of the output representations $\vh_1, \dots, \vh_{|M_i|}$ of each module. In other words, the input of each module is interpreted as the query and the output of each module is interpreted as the value and key. The attention mechanism thus learns to  attend over the module representations and weigh them according to their relevance for the current task.

Instead of aggregating module outputs into a single representation, Recurrent Independent Mechanisms \citep{goyal2019recurrent} concatenate the outputs of the top-$k$ active modules. However, in between the application of recurrent computation functions, they exploit an attention mechanism over hidden representations to enable sparse communication among modules.

One major disadvantage of both \textit{weighted} and \textit{attention-based} representation averaging, is that---when used in combination with soft routing---they require a full forward pass through all modules, even if they contribute only marginally to the final aggregation. Thus, they incur significant increases in time and space complexity. While this can be mitigated by pruning (i.e., dropping) some modules during inference \citep{Rueckle2021AdapterDrop}, latency still remains an issue for scalability. Thus, top-$k$ hard routing offers a more efficient solution for both weighted averaging \citep{shazeer2017outrageously,Lepikhin2021GShard} and attention-based aggregation \citep{goyal2019recurrent}.
 

\subsection{Input Aggregation}

Input aggregation lends itself naturally to adapters such as prompts or prefix tuning \citep[see \cref{sec:nature_modularity:input_composition}]{brown2020language,Lester2021prompttuning,Li2020PrefixTuning}. In prompting, we have a set of instructions or few-shot examples $ \vphi_1, \dots , \vphi_{|K|}$. Given that the nature of prompting is to prepend the prompts to the input, aggregating the respective modules boils down to concatenating all prompts. That is, providing the model with \textit{multiple} instructions, or with multiple examples (i.e. few-shot in-context learning) is a version of module input aggregation $f_1^\prime(\vx) = f_{\vtheta_1}([\vphi_1, \dots, \vphi_{|K|}, \vx])$. This concept also extends to prefix-tuning, where we can simply concatenate all prefixes at every layer: $f_i^\prime(\vx) = f_{\vtheta_i}([\vphi_i^1, \dots, \vphi_i^{|K|}, \vx])$.

In the context of prompting, \cite{Schick2021SelfDiagnosis} leverage input aggregation by concatenating multiple textual descriptions of undesired behaviours of a language model to generate toxic text for model debiasing. In the context of prompt tuning, \cite{vu-etal-2022-overcoming} learn separate task and language soft prompts that are recombined for zero-shot cross-lingual transfer in summarization.
\cite{nayak2022learning} compose soft prompts of attributes and objects in visual tasks to generalise to new classes. Soft prompts can also be aggregated with methods different from concatenation such as attention-based parameter interpolation \citep{asai-etal-2022-attempt}. 

Furthermore, input aggregation methods have found significant utility in retrieval augmented generation \citep{lewis2020retrieval}, a technique where retrieval models are employed to retrieve external knowledge for addressing knowledge-intensive NLP tasks.\footnote{Notably, RAG is also discussed  in \S~\ref{sec:nature_modularity:input_composition} due to its dual capability of both input composition and aggregation, for instance when multiple documents are used in the retrieval process.} In RAG methods, retrieved documents are appended to the input, essentially aggregating external information with the model's input.  These methods facilitate knowledge injection and editing \citep{verga-etal-2021-adaptable, cheng-etal-2023-decouple}, allowing models to access and incorporate information from external sources, which can be crucial for tasks demanding domain-specific knowledge or real-time data updates. This aligns with the broader theme of knowledge enhancement within modular neural architectures, extending their capabilities to handle complex and dynamic information needs.\footnote{For further applications of RAG see \S~\ref{ss:knowledge_injection}.} 

\paragraph*{Hypernetworks}
Similarly to soft prompts, hypernetworks may aggregate information from different embeddings by combining them in the input to the parameter generator. For instance, in \citep{ponti-etal-2021-parameter} task and language embeddings are concatenated in the input when training a multilingual multi-task architecture where the encoder is fully shared and the hypernetwork generates the classifier head. By recombining embeddings appropriately, this method allows for inferring the parameters of unseen task--language combinations. Combinations of embeddings have been used to generate adapters in multilingual \citep{Ustun2020} and multi-task settings \citep{mahabadi2021parameter,pilault2021conditionally}. 
 

\subsection{Function Aggregation}
\label{sec:compositionality:function}

Finally, aggregation can be achieved on the function level; $f_i^\prime(\vx) = f_{\vphi_{1}} \circ f_{\vphi_{2}}(\vx)$. Different aggregation methods infer either a sequence or a (tree) structure that determines the order of the aggregation.

\paragraph*{Sequential Aggregation}

By performing a forward pass through multiple modules, where the input to the next module is the output of the previous one, the respective hidden representations are sequentially transformed: 
$f_i^\prime(\vx) = f_{\vphi_1} ( f_{\vphi_2} (\dots (f_{\vphi_{|M|}}(\vx))))$.

This form of information aggregation is often chosen in conjunction with \textit{fixed routing}, as discussed in \S~\ref{sec:routing:deterministic}, given that the routing order is determined by the role of each module (e.g. language and task adapters).
\citet{pfeiffer-etal-2020-mad, Pfeiffer2021UNKs} propose a two-stage setup where language-specific components are disentangled from task-specific components, in order to perform zero-shot cross-lingual transfer. First, language (adapter) modules $f_{\vphi_{l_s}}$ and $f_{\vphi_{l_t}}$ are trained on monolingual unlabelled data for the source language $s$ and the target language $t$, respectively. Then, in the second stage, the language component $f_{\vphi_{l_s}}$ is inserted but frozen, and a new (adapter) module is added for a task $f_{\vphi_{t}}$ and trained on annotated data for the source language: $f_{\vphi_{t}} (f_{\vphi_{l_s}}(\vx))$.
Since this effectively disentangles language from task information, this also enables zero-shot inference on the target language $t$ without annotated data. In particular, $f_{\vphi_{l_s}}$ is substituted with $f_{\vphi_{l_t}}$, thereby hierarchically aggregating the information from the respective modular components: $f_{\vphi_{t}} (f_{\vphi_{l_t}}(\vx))$.
Similarly, \citet{Stickland2021MultilingualDomainAdapt} perform function composition of a language module $f_{\vphi_{l}}$ and a domain module $f_{\vphi_{d}}$ for multilingual multi-domain machine translation. For more examples, see \cref{ssec:eff_ft}. 

\paragraph*{Hierarchical Aggregation} Alternatively, when global routers jointly determine the selection of modules and the model architecture, the order of function composition follows the structure of a tree.
For instance, Neural Module Networks \citep{andreas2016nmn} leverage a semantic parse to infer a graphical structure for module aggregation. While all leaf nodes find objects by identifying regions of an image through attention, intermediate nodes either transform or combine these representations (depending on the arity of the node). The root then predicts the label by describing or measuring the attended objects.




\begin{tcolorbox}{%
    \begin{itemize}
      \setlength\itemsep{-.1em}
        \item Aggregation functions play a crucial role in combining information from multiple modules in modular neural architectures.
        \item \textbf{Parameter aggregation} strategies interpolate weights of multiple modules and are influenced by concepts like linear mode connectivity.
        \item \textbf{Weighted representation averaging} and \textbf{attention-based aggregation} are methods for combining the outputs of modules, with attention mechanisms allowing for dynamic weighting based on relevance.
        \item \textbf{Input aggregation} methods, such as prompting and prefix tuning, involve concatenating instructions or examples to the input, enabling modular control over tasks and domains.
        \item \textbf{Hypernetworks} can also perform input aggregation by combining different embeddings.
%     \end{itemize}
% }%
% \end{tcolorbox}

% \begin{tcolorbox}{%
%     \begin{itemize}
%       \setlength\itemsep{-.1em}
        
        
        \item \textbf{Function aggregation} occurs on the function level and can be sequential or hierarchical, with different methods determining the order of aggregation.
        \item Sequential aggregation is often used with fixed routing, while hierarchical aggregation is employed when global routers jointly determine module selection and model architecture.
    \end{itemize}
}%
\end{tcolorbox}





\section{Training Setting} \label{sec:training_setting}

Finally, we explore the training settings for modular architectures. We can identify three main strategies in the literature: \textbf{1)} all modules are {jointly trained} for \textit{multi-task learning}; \textbf{2)} modules are introduced at different stages during \textit{continual learning}; and \textbf{3)} in \textit{transfer learning}, modules are added \textit{post-hoc} after pre-training, often as a way to fine-tune the model in a parameter-efficient fashion. Importantly, these strategies are not necessarily mutually exclusive and can be realised in combination. 

\subsection{Joint Multitask Learning}
\label{sec:training:pretraining}
 
In joint multi-task learning, there are two main settings. Firstly, task-specific parameterised components can be integrated into shared neural network architectures as a means to mitigate catastrophic forgetting or negative interference \citep{mccloskey1989catastrophicinterference, french1999catastrophic} and as a way to scale the model size efficiently \citep{kudugunta2021beyond}. In these scenarios, modules are often optimised on individual tasks via fixed routing and specialise accordingly \citep[][\textit{inter alia}; see \S~\ref{sec:routing:deterministic} for more details]{hampshire1992meta, rajendran2017adaapt}. 
As an alternative, the architecture can be fully modular, sharing only the parameters for learned routing \cite[][\textit{inter alia}; see \S~\ref{sec:routing:softlearnedrouting} for more details]{jacobs1991adaptive, jacobs1991task,rosenbaum2017routing,kirsch2018modular,chang2018automatically}.  

Joint training can also be performed before \textit{post-hoc} training: a shared base model can be pre-trained on multiple tasks as a warm-up before creating task-specific sparse subnetworks \citep{sun2020learning} or as a way to 
provide a useful initialisation for modular parameters \citep{Vu2022spot}. \citet{dua-etal-2022-tricks} convert a dense language model pre-trained on text data into an MoE by decomposing the learned feed-forward layers.
\cite{Pfeiffer2022Lifting} add language-specific layers during multilingual pre-training of a language model. This prepares the model to be extended to more languages \textit{post-hoc}; when new languages become available, a new (randomly initialised) learnable layer can be added to the inventory of modules, whereas the shared parameters remain untouched.
 



\subsection{Continual Learning}

In a similar vein to countering negative interference in multi-task learning, continual learning---that is, continuously integrating new data into the model---often aims at mitigating catastrophic forgetting (i.e., the knowledge learned at early stages of training should not get overwritten by updates to the model later on).  

Similar to the multi-task learning approaches discussed in \S~\ref{sec:training:pretraining}, new layers can be continuously introduced within the network which are only updated on the new data, keeping the others untouched. 
In methods like Progressive Networks \citep{Rusu2016Progressive}, PathNet \citep{fernando2017pathnet}, and PackNet \citep{Mallya2018packnet} when the model is trained on a new task, the parameters of the previous tasks are frozen; however, for new tasks, new modules may be learned, which connect to the existing set of modules. 
Often, the decision of inserting new modules at a given stage is made dynamically based on outlier detection \citep{ostapenko2021continual}.
Progressive Networks \citep{Rusu2016Progressive}, on the other hand, scale the model capacity linearly with the number of tasks. 
\citet{Aljundi2017Expert} train separate experts for every task and route new examples based on the distribution of the reconstruction errors of task-specific auto-encoders.

Instead of adding new parameters to the model, other works in the continual learning landscape identify subnetworks for different tasks. For instance, some works identify subnetworks of the model, which have not been used by previous tasks. Consequently, updating these parts of the model will have little effect on the previously learned knowledge \citep{Javaloy2022Rotograd}. Similarly, `supermasks' \citep[\S \ref{sec:nature_modularity:parameter_composition};][]{wortsman2020supermasks}, which learn a binary mask over a randomly initialised model, enable the extension to a potentially vast number of tasks during continual learning. Supermasks of previous tasks can be also linearly combined as a way to generalise to new tasks.

\subsection{Parameter-efficient Transfer Learning}
\label{sec:training:posthoc}

Recently, transfer learning has become the dominant strategy for state-of-the-art results on most tasks. Auxiliary self-supervised objectives are utilised to pre-train models on a large amount of data. Subsequently, the model's weights are fine-tuned on the target tasks \citep{Howard2018ulmfit,Devlin:2019bert}. Updating a small set of parameters of these large models has been demonstrated to perform equally well as full model fine-tuning, leading to the emergence of parameter-efficient fine-tuning strategies.

Most methods discussed in \S~\ref{sec:nature_modularity} that are applied to large pre-trained models can be considered as post-hoc adaptation. Modularity can be achieved through \textbf{parameter composition} (\S~\ref{sec:nature_modularity:parameter_composition}) using \textit{sparse subnetworks} \citep{Mehta2019, chen2020lottery,donahue2014decaf,Cai2020tinytl,ben-zaken-etal-2022-bitfit,guo-etal-2021-parameter}, or \textit{low-rank adapters} \citep{Li2018intrinsic, hu2021lora}, \textbf{input composition} (\S~\ref{sec:nature_modularity:input_composition}) by augmenting the function's input \citep{brown2020language, Li2020PrefixTuning}, and \textbf{function composition} (\S~\ref{sec:nature_modularity:layers}) through \textit{adapter layers} \citep{Rebuffi2017Adapters1,houlsby2019parameter} and \textit{rescaling} \citep{Liu2022IA3}. Additionally, hypernetworks can be used to generate the parameters of any of the above-mentioned types of modules (\S~\ref{sec:computation_function:hyper_network}). Essentially, all of these methods are tightly connected as they share the same functional form (\S~\ref{sec:unifying_composition}).

 \begin{tcolorbox}{%
    \begin{itemize}
      \setlength\itemsep{-.1em}
        \item There are three main training strategies: (1) Joint Multitask Learning, (2) Continual Learning, and (3) Parameter-efficient Transfer Learning.
        \item In \textbf{Joint Multitask Learning}, task-specific components are integrated into shared neural architectures, allowing modules to specialize via fixed or learned routing.
        \item \textbf{Continual Learning} methods aim to integrate new data while mitigating catastrophic forgetting, with options to introduce new modules dynamically or identify subnetworks for different tasks.
        \item \textbf{Parameter-efficient Transfer Learning} involves pre-training models on large datasets and fine-tuning on target tasks. Modular strategies can be applied post-hoc through various composition methods, including parameter composition, input composition, and function composition.
        \item These training strategies are not mutually exclusive and can be combined to achieve specific goals in modular neural architectures.
    \end{itemize}
}%
\end{tcolorbox}

\section{Applications in Transfer Learning}
\label{sec:applications}
Most applications of modular deep learning revolve around transfer learning. In particular, the two main purposes are: \textbf{1}) \textit{parameter-efficient} fine-tuning (\S~\ref{ssec:eff_ft}), which achieves superior efficiency, prevents negative interference, and enables zero-shot transfer; and \textbf{2}) zero/few-shot generalisation to new tasks (\S~\ref{ssec:task_gen}). In what follows, we provide a quick overview of transfer learning applications of modular deep learning. For the in-depth discussions and illustrations of the key concepts, we will first focus on applications in NLP, and then draw direct analogies with other deep learning areas such as speech processing, computer vision, and multi-modal (representation) learning.
In \S~\ref{sec:purposes}, we will explore additional purposes of modular deep learning, including hierarchical reinforcement learning, programme simulation, and causal inference.

 

\subsection{Parameter-Efficient Fine-tuning} 
\label{ssec:eff_ft}
Regardless of the application area, one of the principal uses of modules has been to boost parameter efficiency and decrease model storage requirements of fine-tuning, eschewing so-called \textit{full model fine-tuning} which requires storing a separate copy of the full model per task \citep{Howard2018ulmfit,Devlin:2019bert}, see \S\ref{sec:training:posthoc}. In the simplest formulation, all task-specific updates are pushed to the parameters of the lightweight modules, while the parameters of the large base model are kept \textit{frozen} throughout task fine-tuning. The modules then store \textit{task-specific knowledge} that can be composed with the `general-purpose' knowledge of the base model to adapt it to the task at hand. 
In NLP, this led to a number of research papers that introduced diverse modular architectures, as surveyed in \Cref{sec:nature_modularity} and \S~\ref{sec:training_setting}. A typical evaluation protocol is fine-tuning a type of module on the popular GLUE and SuperGLUE  benchmarks \citep{Wang:2019superglue}, comparing against full model fine-tuning or alternative modular architectures. The results usually corroborate either of two main goals: (i) improving performance with the same parameter budget versus (ii) maintaining performance with a smaller parameter budget~\citep{Mahabadi2021Compacter,han:2023autopeft}. In addition, modular adaptation has further benefits: first, it prevents negative interference between tasks \citep{Bapna2019Adapters}. Second, it allows for combining adapters to enable zero-shot transfer \citep{pfeiffer-etal-2020-mad}. In light of the enormous size of state-of-the-art large language models (LLMs), parameter-efficient fine-tuning has emerged as the main way to update the pretrained models \citep{hu2021lora}.

 

\subsubsection{Machine Translation}
\label{ss:nmt}

In the seminal work of \citet{Bapna2019Adapters}, \textit{bilingual} (i.e., language-pair) adapters (see \S\ref{sec:nature_modularity:layers}) were used to adapt a massively multilingual NMT model (spanning 103 languages) to a particular source--target translation direction. One benefit of such bilingual adapters is their ability to `skew' the multilingual model to the language pair at hand without losing the benefits of massively multilingual training for low-resource languages. Another positive effect of bilingual adapters concerns recovering the MT performance also for high-resource languages. High-resource languages might typically suffer from performance deterioration due to the particular interference phenomenon known as the `curse of multilinguality' \citep{conneau-etal-2020-unsupervised,wang-etal-2020-negative}: when (too) many languages compete for the fixed parameter budget of the model, the model's expressiveness and representation power deteriorates for all languages. The use of modules extends the parameter budget to recover the detrimental effects of multilingual inference through dedicated (i.e., modular) bilingual adaptation.  
Their work also demonstrates the superior performance of a multilingual model specialised towards a particular language pair over merely training a bilingual NMT model for the same pair from scratch.

However,  
fine-tuning bilingual adapters (or more generally, modules) for each translation direction assumes parallel data for all language pairs and requires $n(n-1)$ modules to cater for all possible language pairs (one dedicated module in the encoder and another module in the decoder). Therefore, follow-up work \citep{philip-etal-2020-monolingual,ustun-etal-2021-multilingual} aimed to learn \textit{monolingual} (i.e., language-specific) adapters. Again assuming standard encoder-decoder architectures for MT such as mBART \citep{liu-etal-2020-multilingual-denoising}, this design requires only $2n$ modules in total. Besides improving parameter efficiency, this also bypasses the critical dependency on parallel data for \textit{all} language pairs and allows for learning from monolingual data. Crucially, this design also enables translation to or from languages without parallel data, in a fully unsupervised way, and even to/from languages unseen by the base pre-trained encoder-decoder model. Put simply, when translating from language $l_{s}$ to $l_{t}$, only the encoder adapters for $l_{s}$ plus the decoder adapters for $l_{t}$ are activated: the model is able to translate from $l_{s}$ to $l_{t}$ without seeing a single parallel $l_{s}$ to $l_{t}$ sentence. This application in the field of NMT exemplifies the power of modular design: available components, which were previously learned locally and asynchronously, can be recombined in novel ways to generalise systematically to unseen applications (i.e., in this particular case, to unseen translation directions). This is one of the main goals of modular deep learning (\S~\ref{sec:intro}). 

The separation into dedicated language-specific modules mitigates interference and catastrophic forgetting; however, it also hinders any positive transfer between modules of similar languages. The positive transfer can be achieved through the use of hypernetworks (see \S\ref{sec:computation_function:hyper_network}): \cite{Baziotis2022MultilingualMTHyperAdapter} learn to generate monolingual language-specific adapters for NMT. In fact, sharing the parameter generator takes advantage of language similarities \citep{platanios-etal-2018-contextual}. As discussed in more detail later in \S\ref{sec:xling-transfer}, similar ideas of combining the modular design with hypernetworks have also been applied earlier and beyond NMT, e.g., for task fine-tuning with adapters in monolingual multi-task setups \citep{mahabadi2021parameter} and for cross-lingual transfer in single-task \citep{Ansell2021MADG} as well as in multi-task setups \citep{ponti-etal-2021-parameter,Ustun:2022hyperx}. 

The curse of multilinguality and catastrophic interference in multilingual MT models have also been tackled through sparse sub-networks (see \S~\ref{sec:routing:sparsemasks}). \citet{lin-etal-2021-learning} extract sparse sub-networks for specific language pairs from a trained multilingual MT model via pruning. Subnetworks are then trained separately in order to specialise towards the particular translation direction. In fact, there exist dedicated small sub-networks (which can be obtained via standard masking) that store language pair-specific knowledge within the large network, where such knowledge should not interfere with other language pair-specific sub-networks \citep{dua-etal-2022-tricks}. The same high-level idea has also been applied to \textit{domain adaptation of bilingual MT systems}: e.g., \citet{Liang:2020aaai} show that it is possible to learn domain-specific sub-networks when fine-tuning the MT system on new domains, where a single large network (i.e., the full neural MT system) comprises multiple disjoint domain-specific sub-networks specialised to particular domains.

Another approach that leverages modularity for an increased language-specific capacity in MT is mixture-of-experts. Each expert is typically dedicated to a particular language or translation direction \citep{kudugunta2021beyond,Costa:2022nllb}. To maintain feasible decoding time, the procedure works as follows: (i) during training, mix the inputs from different translation directions in the same batch, in order to learn the routing network and encourage positive transfer among related tasks; (ii) at inference time, different translation directions are decoded separately, and only the corresponding subset for elevant experts is loaded.
 

\subsubsection{Cross-Lingual Transfer}
\label{sec:xling-transfer}


 




NMT focuses on translation as a single task and modularity was exploited mainly to carve language-specific and/or domain-specific modules that can support multilingual and multi-domain systems, respectively. In more general cross-lingual transfer setups, the aim is to transfer large models \citep{Devlin:2019bert,conneau-etal-2020-unsupervised} fine-tuned \textit{for any task} (e.g., sequence labelling tasks such as NER, text classification tasks such as NLI, sentiment analysis or intent detection for dialogue systems) on one or more source languages (where such task annotations exist) to one or more target languages \citep{Hu:2020xtreme,Ruder:2021xtremer}. Ideally, the transfer should be achieved without fine-tuning the full  model \citep{Hu:2020xtreme}, which  results in catastrophic forgetting and negative interference, or requires the creation of separate model copies for each task.



The idea of training \textit{language modules} thus largely follows what already outlined for MT in \S\ref{ss:nmt}, with the addition of another set of dedicated modules that aim to capture task-related knowledge: \textit{task modules}. Such language modules and task modules can then be combined to \textbf{1)} favour zero-shot cross-lingual transfer for particular source-target directions \citep{pfeiffer-etal-2020-mad,Ansell2021MADG,ansell2021composable,Parovic2022BADX}; \textbf{2)} provide extra capacity to low-resource languages under-represented (or even not covered) in the large multilingual models such as mBERT or XLM-R \citep{Pfeiffer2021UNKs,Pfeiffer2022Lifting,ponti-etal-2020-xcopa,Faisal:2022arxiv}, independently from task knowledge; and \textbf{3)} enable handling unseen language--task or even language--domain--task configurations \citep{ponti-etal-2021-parameter,Stickland2021MultilingualDomainAdapt}. 

 


As an example of zero-shot cross-lingual transfer, the original MAD-X framework \citep[Figure~\ref{fig:CaseStudy:MADX}]{pfeiffer-etal-2020-mad} relies on bottleneck adapters to implement language and task modules: In particular: \textbf{1)} Language modules are inserted into each layer of the original neural model and are fine-tuned on (unsupervised) data of the particular language (e.g., via Masked Language Modelling) while the weights of the original model are kept fixed. \textbf{2)} After obtaining language modules, task modules are \textit{stacked} on top of the source language module(s) and are fine-tuned relying on the task objective and task-annotated data in the source language(s), while both the original model \textit{and} language modules are kept fixed. \textbf{3)} At inference, source language module(s) are replaced with the desired target language module while retaining the task module: this enables zero-shot task inference in the target language.


\begin{wrapfigure}{r}{0.37\textwidth}
  \begin{center}
    \includegraphics[width=0.3\textwidth]{img/hyperx.pdf}
  \end{center}
  \caption{Hyper-X \citep{Ustun:2022hyperx}: an example application of contextual module generation where a hypernetwork takes the concatenation of task, language and layer embeddings as input and generates a flat parameter vector. This is further reshaped into an adapter module within each Transformer layer. Learning independent layer embeddings and sharing a single hypernetwork across all layers \citep{Ansell2021MADG} (i) enables information sharing across layers, and (ii) reduces trainable parameters of the hyper-network by a factor corresponding to the number of layers.}
  \label{fig:app_hyperx}
\end{wrapfigure}

Recent work has introduced a spectrum of variations and enhancements to this core idea. For instance, inspired by the bilingual `translation direction' adapters for NMT systems (\S\ref{ss:nmt}), \cite{Parovic2022BADX} learn bilingual adapters instead of single language adapters to boost transfer for a particular language pair. \cite{Faisal:2022arxiv} and \cite{Chronopolou:2022arxiv} learn language family adapters to reduce data sparsity for low-resource languages and capitalise on language similarity and cross-language sharing. \cite{Stickland2021MultilingualDomainAdapt} decouple language and domain knowledge into dedicated modules (see also \S\ref{sec:domain_adap} later).  
Further, \cite{ansell2021composable} implement dedicated modules as sparse sub-networks, the so-called language and task masks, which can be composed with the base model via parameter composition. Following the analogy between language-specific and bilingual adapters, instead of learning separate language and task sub-networks, \cite{Foroutan2022Discovering} learn dedicated task--language sub-networks, demonstrating the variance in the extracted sub-networks across different task--language combinations. The use of such language sub-networks as language modules, even without dedicated task modules, improves cross-lingual transfer for dependency parsing when used within a meta-learning setup \citep{Choenni:2022arxiv}. \citet{litschko-etal-2022-parameter} compare sparse sub-networks and bottleneck adapters for transferring ranking functions for information retrieval tasks across languages and find them both superior to full model fine-tuning.

Finally, a body of work again focuses on `contextually generating' the modules via hypernetworks, aiming to increase efficiency and benefit from connections between different languages \textit{and} tasks. A representative example is the Hyper-X framework \citep{Ustun:2022hyperx} provided in Figure~\ref{fig:app_hyperx}, where the module parameter generation is conditioned on the (disentangled) task and language, and additionally on the index of the Transformer layer where the generated module is inserted. Each task and language are parameterised via separate embeddings, which enables adaptation to any task--language combination, where these embeddings are low-dimensional vectors which are learned together with the parameters of the hypernetwork (see Figure~\ref{fig:app_hyperx} again). The framework thus leverages supervision and positive transfer from both multiple tasks and languages. Hyper-X can be seen as a more general variant of a series of precursors backed by the idea of contextual generation: \citet{ponti-etal-2021-parameter} condition the hypernetwork on both task and language embeddings but generates only the model's classifier head. Other methods generate modules but condition the hypernetwork only on tasks in a monolingual setup \citep{mahabadi2021parameter} or only on languages in a cross-lingual transfer setup \citep{Ustun2020,Ansell2021MADG}.

 
\subsubsection{Domain Adaptation}
\label{sec:domain_adap}
As already hinted at in \S\ref{ss:nmt} and \S\ref{sec:xling-transfer}, dealing with different domains adds another tier to the modular design: domain-specific knowledge might be captured within dedicated \textit{domain modules}.\footnote{For instance, disentangling domain and language information yields benefits for NMT and cross-lingual transfer applications \citep{vilar-2018-learning,cooper-stickland-etal-2021-multilingual,pham-etal-2021-revisiting,Saunders:2022survey}.} This can again be accomplished through similar modular architectures as with language and task adapters. For instance, it is possible to inject domain-specific knowledge into (bottleneck) adapters \citep{Zhang:2021ws,Chronopoulou2022EfficientHierarchical} or to extract sparse domain-specific or task-specific sub-networks \citep{Thompson:2018nmt,ke-etal-2021-adapting} for multi-domain and multi-task learning. Mixture-of-experts also enable multi-domain joint learning as well as domain adaptation \citep{Guo:2018emnlp,Zhang:2022metadmoe}. Similar strategies have also been used in multi-domain and cross-domain speech processing and computer vision applications (see \S\ref{ss:speech} and \S\ref{ss:cv} later).

In domain adaptation, it is common to combine both shared parameters and domain modules that are learned jointly \citep{Bousmalis2016}. Beyond this standard setting, many approaches employ additional regularisation terms. The most common are \textbf{1)} a domain-adversarial loss on the shared parameters in order to encourage them to be domain-invariant \citep{ganin2016domain,chen-cardie-2018-multinomial}; \textbf{2)} an orthogonality constraint on the domain modules to ensure that they capture different information \citep{baktashmotlagh2013unsupervised,kim-etal-2017-adversarial}; and \textbf{3)} similarity constraints that bring representations of similar modules close together \citep{Bousmalis2016}.

 

%\subsection{Other Applications in NLP}
\subsubsection{Knowledge Injection}
\label{ss:knowledge_injection}
Naturally, dedicated modules can also be assigned to inject and store external knowledge (e.g., from manually curated external knowledge bases), which can then interact with language, domain, or task knowledge. This idea has been explored with diverse external knowledge sources. For instance, \citet{lauscher-etal-2020-common} aimed at complementing the distributional knowledge of large language models with conceptual and commonsense knowledge from ConceptNet \citep{Speer:2017conceptnet}. The external knowledge was captured within dedicated bottleneck adapters: they were fine-tuned via language modelling on synthetically created sentences from random walks over the ConceptNet graph structures. \cite{majewska-etal-2021-verb} stored verb-related knowledge from VerbNet \citep{schuler2005verbnet}, a human-created verb classification repository, into bottleneck adapters, and demonstrated its usefulness in a range of tasks that require understanding of verb semantics. Along similar lines, \citet{wang-etal-2021-k} offered a generalisation of these approaches where different knowledge sources (e.g., Wikipedia, WikiData) are mapped to different dedicated adapters, which can be aggregated according to the task at hand. The same idea has been explored by \citet{Lu:2021knowledge} in the biomedical domain, where the main knowledge sources were the UMLS Metathesaurus graph \citep{bodenreider2004unified} and biomedical Wikipedia articles. \citet{Lu:2021knowledge} also introduce another component, the so-called knowledge controller, which can be seen as a standard attention-based function aggregator from \S\ref{sec:compositionality:function}. As an example of another relevant application, \citet{lauscher-etal-2021-sustainable-modular} learned bottleneck adapters without manually curated external data, with the focus on model debiasing: the debiasing adapters were fine-tuned via standard language modelling on a counterfactually augmented corpus. 


Finally, the idea of modular knowledge injection is also directly linked to the retrieval-augmented language models in text-only settings~ \citep{Lewis:2020rag} as well as in multi-modal settings~\citep{Yasunaga:2023arxiv} where the standalone retrieval module, detached from the `main' language model, is responsible to fetch knowledge from some external memory or a knowledge base, and that knowledge is then used to condition the language model. In this design, the retrieval step and capability is made explicit and decoupled from the language model generation capability: as such, one can work directly on a retrieval module without the need to change the other components of the entire model~\citep{yu-etal-2023-augmentation}. The ability of standard language models to use external tools is also sparked by the modular design: different external tools specialised for performing particular functions (e.g., conducting Web search, performing mathematical operations) are stored as separate modules accessed from the main model via external API calls. For a comprehensive overview of such \textit{augmented language models}, we refer the reader to the recent survey \cite{auglms:2023}. 

 


\subsubsection{Speech Processing}
\label{ss:speech}

\begin{wrapfigure}{r}{0.27\textwidth}
  \begin{center}
    \includegraphics[width=0.17\textwidth]{img/wav2vec.pdf}
  \end{center}
  \caption{The structure of the wav2vec 2.0 model with task-specific bottleneck adapters for parameter-efficient ASR fine-tuning from \citet{Thomas:2022speech}; 
  $f(\cdot)$ denotes a convolutional encoder followed by 12 standard Transformer encoder blocks. For downstream ASR a linear classifier, $CTC(\cdot)$, is applied to the final encoder output.}
   \label{fig:app_speech}
   % \vspace{-5em}
\end{wrapfigure}

The use of modular deep learning for speech processing applications closely matches the ideas already exposed for NLP tasks. The landscape of the possible modular designs is exactly the same, where the only crucial differences are (i) the choice of the underlying large model, and (ii) the corresponding objective functions used to inject the specialised knowledge into the modules. For instance, the typical choice of the base model for automatic speech recognition (ASR) applications is one from the wav2vec family \citep{Baevski:2020wav2vec,Babu:2022xlsr}, while the ASR-oriented objective function is the standard Connectionist Temporal Classification (CTC) loss \citep{Graves:2006ctc}. The high-level modular structure remains the same, as illustrated in Figure~\ref{fig:app_speech} with an example from \citet{Thomas:2022speech}, which utilises standard bottleneck adapters.

 



While in theory a large variety of possible modular configurations from \Cref{sec:nature_modularity}-\S~\ref{sec:training_setting} can be applied to diverse speech processing tasks, the majority of current work in the area has indeed focused on the use of bottleneck (sequentially placed) adapters for ASR in monolingual and multilingual contexts. Before that, the concept of modularity can be traced to the work of \citet{Swietojanski:2016taslp}, where the model re-weights hidden units using small amounts of unsupervised data to better adapt to a particular speaker or an environment. More recently, bottleneck adapters have been used to perform ASR adaptation to atypical and accented speech \citep{tomanek-etal-2021-residual}, unseen speakers with limited adaptation data \citep{Wang:2022speech,Eeckt:2022speech,Chen:2022speech}, new domains and manners of speaking (e.g., children's speech) \citep{Fan:2022speech,Zhu:2022speech}, or to perform further model customisation to specific speakers \citep{Biadsy:2022speech,Sathyendra:2022speech} and for multilingual learning \citep{Kannan:2019interspeech,Hou:2022speech}. A notable exception, not resorting to adapter layers, is the method of \citep{Winata:2020speech} which aims to learn low-rank modules (\S~\ref{sec:nature_modularity:parameter_composition}), akin to the idea of LoRA \citep{hu2021lora}, for end-to-end ASR.




Multi-task (where the term `task' in this context can e.g. refer to different languages, domains, speakers, or accents) ASR setups have also witnessed the usage of mixture-of-experts, closely following the basic ideas already discussed for NMT (\S\ref{ss:nmt}) where different languages are assigned their dedicated modules through fixed routing. For instance, in speech processing, MoEs have been applied to multilingual ASR and cross-lingual ASR transfer \citep{Bai:2022speech,Gaur:2021speechmoe,Kumatani:2021arxiv}, while \cite{you2022speechmoe2} propose MoE for ASR with learned routing.

Beyond ASR, bottleneck adapters have also been used for speech translation \citep{le-etal-2021-lightweight}. Most recently, modular adapter-based approaches have been applied to text-to-speech methods (TTS) \citep{Hsieh:2022tts,Morioka:2022tts}, aiming to extend standard large multi-speaker TTS models such as FastPitch \citep{Lancucki:2021fastpitch} to new speakers without compromising the TTS quality for the seen speakers. From a high-level perspective, one can see a direct analogy of this goal to the objectives in the MT literature of extending multilingual MT systems to unseen languages without compromising seen languages (see \S\ref{ss:nmt} again).


\subsubsection{Computer Vision and Cross-Modal Learning}
\label{ss:cv}
In computer vision, similar to NLP and speech processing (\S\ref{ss:speech}), dedicated modules are again used to enable parameter-efficient fine-tuning across multiple tasks and domains \citep[\textit{among others}]{Rusu2016Progressive,Rebuffi2018Adapters2,Berriel:2019iccv,He:2022arxiv}. The core difference, again, is the choice of the actual neural architecture for the underlying model as well as for the modules: e.g., residual adapters \citep{Rebuffi2017Adapters1} consisted of simple $1 \times 1$ convolutions combined with the base ResNet neural model \citep{He2016ResNet} while other work learned task-specific convolutional filters \citep{newell2019feature,bragman2019stochastic}.
More recent work aims to exploit modular architectures from NLP (e.g., sequential or parallel adapters, LoRA, prefix tuning) with pretrained Vision Transformer (ViT) architectures \citep{dosovitskiy2020image}: e.g., \citet{He:2022arxiv} run a comparative empirical analysis of various modular architectures for vision tasks, while \citet{Chen:2021neurips} rely on sparse sub-networks. %as modules.

Modular design lends itself naturally to cross-modal and multi-modal applications, where different modalities may be captured by modality-specific parameters and routing can also be modality-conditioned. For instance, in multilingual vision-and-language (V\&L) settings, it is possible to conduct inference in languages that lack labelled task examples. In fact, language knowledge is again disentangled from the task and modality knowledge, and the knowledge for different input modality streams can be captured in dedicated modules. This idea has been heavily explored in recent work in multi-modal multi-task scenarios, both in monolingual \citep{Sung:2022modal} and multilingual contexts \citep{bugliarello2022iglue,pfeiffer-etal-2022-xgqa}, for tasks such as image captioning \citep{Zhou:2022modal,Gao:2021modal}, text-to-image generation \citep{Maharana:2022modal}, visual question answering \citep{Liu:2022delving,Sung:2022modal}, visual reasoning \citep{liu-etal-2021-visually}, etc. For instance, Flamingo \citep{alayrac2022flamingo} uses frozen pretrained vision and language models, and only trains adapter layers to handle sequences of arbitrarily interleaved visual and textual data. It is trained with a sequence modelling objective on Web-scale data \citep{li2021align} and displays impressive zero-shot and few-shot capabilities. \cite{pfeiffer-etal-2022-xgqa} use adapter modules to equip multilingual text-only models with the ability to also process the visual modality, as well as to equip monolingual multi-modal models to deal with input from multiple languages. \citet{Papalampidi:2022modal} rely on hierarchical adapters (akin to hierarchical representation aggregation discussed in \Cref{sec:compositionality}) for the task of summarising long videos into textual descriptions. \citet{Pan:2022modal} demonstrate that modular design also helps in image-to-video transfer tasks: they use adapter modules to equip a large image-based model without temporal knowledge with the ability to reason about dynamic video content. 


We note that in this survey, we aim to list some exemplary applications and draw parallels between different yet similar application areas such as NLP, speech processing, and computer vision. While we acknowledge that there exists a wealth of other work in these areas, we have no pretence of exhaustiveness.

 
 
\subsubsection{Comparison and Design Principles}
\label{sec:app_summary}
While a full-fledged comprehensive empirical study of the plethora of modular architectures across various application tasks and areas is still lacking, there exist initiatives such as the publicly available AdapterHub platform \citep{pfeiffer-etal-2020-adapterhub}: it provides (re)implementations of representative modular NLP architectures, within a unified framework tied to HuggingFace Transformers \citep{wolf2019huggingface}. Among others, AdapterHub includes representatives of each computation method in \Cref{sec:nature_modularity}: LoRA \citep{hu2021lora} (i.e., low-rank parameter composition), prefix tuning of \cite{Li2020PrefixTuning} (input composition) and a number of bottleneck adapter configurations (function composition). 
The existence of AdapterHub delineates another crucial advantage of modularity: \textit{reusability} of existing, already fine-tuned modules which can be (re)combined with the large neural models. In short, any practitioner can share or reuse a module specialised for a particular purpose (e.g., capturing specific task or language knowledge) with the community, facilitating community-wide sharing and thus avoiding time- and energy-costly repetitions of the same fine-tuning procedure.\footnote{The (concept of) reusability enabled by the modular design also positively impacts energy consumption \citep{strubell-etal-2019-energy}, making an important leap towards Green(er) AI \citep{Schwartz:2020greenai}.} As discussed in \Cref{sec:routing}, one can observe initiatives such as AdapterHub as continuously updating community-distributed multi-task models.

The discussion in this section also points to a more general principle: different end-goals even within the same end-application (e.g., NMT, cross-lingual transfer, domain adaptation) require rethinking the actual modular design, and the desired level and nature of modularity. For instance, if the goal in NMT (or cross-lingual transfer) is to boost performance for a particular translation or transfer direction, it might be useful to trade off some modularity for a better final performance by replacing language-specific monolingual modules with bilingual modules \citep{Bapna2019Adapters,Parovic2022BADX}. On the other hand, if the goal is to enable zero-shot or few-shot translation or transfer, the design with monolingual modules might be a better choice. In another example, if the focus is on MT or transfer for a particular low-resource language, the model designer should enable positive transfer to that language by `opening' the flow of information from a module storing knowledge on high-resource languages similar to the target language if such languages exist (e.g., from Spanish to Galician) \citep{ustun-etal-2021-multilingual}, or by learning modules for families or groups of similar languages \citep{Chronopolou:2022arxiv,Faisal:2022arxiv}. Analogously, related domains can also be grouped and hierarchically organised to enable positive transfer for domain adaptation \citep{Chronopoulou2022EfficientHierarchical}.

Other practical desiderata may also influence the selection of the actual modular design. If the final task performance is paramount, larger modules might be preferred, e.g., in order to offer enough extra capacity to store the wealth of language-specific information \citep{ansell2021composable}. However, if model compactness is paramount, the criterion for choosing a specific design is instead the trade-off between efficiency (in terms of parameters and/or train and test time) and task performance; the optimisation of this trade-off has been the focus of recent research \citep{Rueckle2021AdapterDrop,Mahabadi2021Compacter,mahabadi2021parameter,Sun:2022bbtv2}. In another example, if time efficiency during inference is a crucial requirement (e.g., real-time ASR in dialogue systems, low latency for information search systems) parameter composition methods such as sparse subnetworks or low-rank composition methods may be preferred over function composition methods as the latter increase the number of computations required during the forward pass, (see Table~\ref{tab:computation_function_comparison}). In yet another example, if storage requirements are a critical constraint, one cannot resort to huge mixture-of-expert models where billions of parameters must be stored \citep{Lepikhin2021GShard}.

\subsection{Task Generalisation}
\label{ssec:task_gen}
The diverse applications of modular deep learning covered so far almost exclusively focus on learning modules associated with (arguably) well-formed and interpretable `units of knowledge' such as languages, tasks, domains, dialects, accents, and speakers. However, modularity might also be achieved when such units are \textit{unknown}. This relies on jointly learning arbitrarily sized inventories of so-called latent \textit{skills} and a learned routing function (\S~\ref{sec:routing:learned}). Since such skills are learned end-to-end on a mixture of data from multiple tasks, they are often not straightforwardly interpretable. On the other hand, since arbitrary subsets of skills can be combined and each skill can be updated locally, these modular neural architectures are ideal for systematic generalisation to new tasks \citep{Zhang2022SkillNet,ponti2022combining}. 

In fact, another fundamental application in transfer learning is achieving zero-shot or few-shot generalisation to new tasks, where test examples are not i.i.d.\ with respect to training examples. The general experimental setup involves disjoint sets of training and test tasks. A model is pre-trained through multi-task learning on training tasks and then adapted to each new test task based on zero or few data points. Common examples of evaluation benchmarks for this setting include CrossFit \citep{ye-etal-2021-crossfit}, the T0 task suite \citep{sanh2022multitask}, or Natural Instructions \citep{mishra-etal-2022-cross}. While a common strategy to tackle this problem is instruction tuning \citep{sanh2022multitask,wei2022finetuned}, where models are fine-tuned prepending the instructions for each task, modular deep learning has emerged as a strong contender \citep{alet2018modular,kudugunta2021beyond,ponti2022combining}.

 

\section{Other Purposes of Modularity}
\label{sec:purposes}

\begin{figure}[t]
    \centering
        \vspace{3em}
        \includegraphics[width=.5\linewidth]{img/schema_hrl}
        \caption{An example of \textbf{Hierarchical~Reinforcement~Learning}~(\S~\ref{ssec:hrl}), Policy Sketches \citep{andreas2017modular}. Two high-level policies $\Pi$ corresponding to task instructions $\tau$ are illustrated. Each iteratively selects low-level policies $\pi$ (options) corresponding to sub-tasks $b$ from a shared inventory. These determine the choice of action given observations. In this case, options are implemented as predicate--argument pairs.}
        \label{fig:purpose_modularity:hrl}
    \end{figure}
    

In addition to scaling large models (for instance, through MoEs, as discussed in \S~\ref{sec:routing:softlearnedrouting}) and facilitating transfer learning, which we covered in \cref{sec:applications}, modularity serves multiple additional purposes. In particular, we devote this section to a cursory view of modularity for i) hierarchical reinforcement learning (\S~\ref{ssec:hrl}); ii) neural programme simulation (\S~\ref{ssec:programme}); iii) neural causal inference (\S~\ref{ssec:causal}). While most of these applications predate the advent of neural networks, (modular) deep learning expands the scope and potential of these lines of research for a series of reasons. First, it holds promise to induce the relevant latent structures (such as options, programmes, or causal graphs, respectively) in an end-to-end fashion. Second, it marries these traditional problems with the ability to jointly model low-level perception, such as vision and language.

\subsection{Hierarchical Reinforcement Learning}
\label{ssec:hrl}
The goal of reinforcement learning is to learn a policy, which predicts the next action of an agent based on past observation(s) from the environment, that maximises the return, i.e. the sum of future discounted rewards. However, many tasks span extremely dilated temporal horizons or provide only highly sparse or delayed rewards. In these cases, it becomes helpful to model intermediate abstractions between high-level goal specifications and low-level actions and observations \citep{sutton1999between,precup2000temporal}. This facilitates the planning abilities of the agent as well as their sample efficiency. In fact, the above-mentioned intermediate abstractions, known as \textit{options} or \textit{skills}, consist of sub-policies that are transferable across tasks.

More formally, each reinforcement learning task is a Markov Decision Process (MDP) consisting of states $\gS$ and actions $\gA$, a transition function $p : \gS \times \gA \times \gS \rightarrow [0, 1]$ and a reward function $r : \gS \times \gA \rightarrow \mathbb{R}$. We aim to learn a policy $\pi : \gS \times \gA \rightarrow [0, 1]$. We also define a value function as the expected (discounted) return from a given state $s$  as $V_\pi(s) = \E_\pi [\sum_{t=0}^\infty \gamma^t r_{t+1} \mid s_0 = s] $, as well as a Q function from a state $s$ and an action $a$ as $Q_\pi(s, a) = \E_\pi [\sum_{t=0}^\infty \gamma^t r_{t+1} \mid s_0 = s, a_0 = a] $. Following \citet{sutton1999between} and \citet{precup2000temporal}, each option $\omega \in \Omega$ is defined as a tuple $(\gI_\omega, \pi_\omega, \beta_\omega)$, where $\gI_\omega \subseteq \gS$ is the initiation set, $\pi_\omega : \gS \times \Omega \rightarrow [0, 1]$ the option-specific policy, and $\beta_\omega : \gS \rightarrow [0, 1]$ is the termination function. For simplicity, many works assume that $\forall s \in \gS, \forall \omega \in \Omega, s \in \gI_\omega$: in other words, all options are available at every state. Augmenting a task with options transforms it into a Semi-MDP, with corresponding functions $\gV_\Omega(\omega)$ and $\gQ_\Omega(s, \omega)$. 

% CHALLENGES: SPECIALIZATION, OPTION SPACE
Learning options involves a series of challenges \citep{jiang2019language}. Firstly, it is not trivial to specialise sub-policies towards distinct behaviours. This shortcoming is common to many modular architectures with learned routing \citep[see \S~\ref{sec:routing:learned}]{mittal2022is}. Not only this, the problem of hard learned routing has often been cast in a reinforcement learning framework (\S~\ref{sssec:hardrouting}).
Secondly, one must define the space where the actions of the high-level policy, which are latent variables, lie. In practice, one could treat them as a discrete, unordered set. In this case, a module from an inventory is chosen for a certain amount of time steps. However, alternative methods operate in structured spaces such as \textit{language}, which is more transferable and scalable due to its combinatorial nature. Thirdly, training multiple options dilates the training time and requires collecting an appropriate amount of experiences for each of them. Fourthly, if trained jointly, options change simultaneously with the master policy, which is a source of non-stationarity. As a consequence, previous experiences for the master policy become invalid if the options have been updated in the meantime. Again, this is reminiscent of the challenges of learned routing exposed in \S~\ref{sec:routing:learned}.

% SUPERVISED, SUBGOALS, HINDSIGHT
The simplest solution to circumvent end-to-end joint learning of the master policy and options is to provide \textit{separate supervision} to both \citep{sutton1999between,dayan1992feudal}. However, this may require extensive annotation, which is often not available. Thus, an alternative method is \textit{defining sub-goals}, i.e.\ states an agent should reach as a stepping stone towards the high-level goal \cite{dietterich2000hierarchical}. Nevertheless, this similarly fails to scale due to the exponentially growing number of combinations of sub-goals some tasks may entail. Moreover, this does not eschew the need to train individual sub-policies for each sub-goal. A partial remedy is offered by \textit{hindsight learning}, where an off-policy correction is introduced \citep{nachum2018data}. Specifically, the original target sub-goal of the current option is substituted with the one maximising the probability of the past sequence of low-level actions. Similarly, the master policy can be trained in hindsight through the currently predicted sequence of high-level sub-goals. Overall, relabelling past experiences significantly improves the model's sample efficiency.

%E2E + INTRINSIC REWARDS
A more radical solution to the challenge of scalability is jointly training both the master policy and options in an \textit{end-to-end} fashion. To this end, 
\citet{bacon2017option} put forth a new architecture, the option--critic, that discovers options from data, without supervision for the intermediate abstractions. This architecture is trained based on policy gradient theorems \citet{bacon2017option} derive for options. Moreover, they augment the set of actions $\gA$ available to each policy $\pi_\omega$ with a special end-of-policy action \textsc{eop} instead of explicitly modelling $\beta_\omega$.
Intuitively, 
formulating the execution as \textit{call-and-return}, a master policy $\pi_\Omega$ determines the active option $\omega$, whose policy $\pi_\omega$ is followed until the $\textsc{eop}$ action is chosen. At this point, control returns to the master policy to choose the next option, and so on until termination. A downside of this method is that it is unstable and often diverges to degenerate solutions \citep{jiang2019language}. Thus, several inductive biases have been proposed to correct it. A popular method is leveraging \textit{intrinsic rewards}: an auxiliary loss diversifies options by maximising the mutual information between each option and the next state conditioned on the current state \citep{florensa2017stochastic,kulkarni2016hierarchical}.

% LANGUAGE AS POLICY
An orthogonal question revolves around the ideal space for the option variables. In fact, compared to a discrete, unordered inventory of (possibly hard-coded) options, language affords more flexibility \citep{andreas2017modular,jiang2019language} as it solves many of the above-mentioned challenges. In fact, all sub-policies can be implemented through a single model conditioned on the linguistic label of the current option. This not only allows options to borrow statistical strength from each other but also makes options reusable in new tasks.
Moreover, the nature of language (through its infinite use of finite means) is suitable to capture the extremely complex combination of sub-goals of many reinforcement learning tasks. Note that linguistic options can be interpreted as a generalisation of sub-goals, as every instruction implicitly corresponds to a subset of states \citep{jiang2019language}.

In practice, to learn linguistic options,
\citet{andreas2017modular} assumes that `sketches' of options are provided for supervision (see \cref{fig:purpose_modularity:hrl}). To induce them, subsequent methods rely instead on synthetic experiences through relabelling
\citep{jiang2019language}, or restricted vocabularies and syntax such as predicate--argument pairs \citep{das2018neural}. Recently, the master policy has been frequently implemented as a large language model. Since these are pre-trained on text, they already contain world knowledge that can serve as a powerful inductive bias for grounded learning. For instance, \citet{huang2022language} use frozen language models to generate options through prompting in a zero-shot fashion.

 \begin{figure}[t]
    \centering
    \vspace{3em}
        \includegraphics[width=.7\linewidth]{img/schema_programme}
        \caption{An example of \textbf{Programme Simulation}~(\S~\ref{ssec:programme}): Differentiable Neural Computer \citep{graves2016hybrid}. A recurrent neural controller iteratively receives an input from the environment, writes to / reads from memory, and produces an output. Read and write operations are based on attention between generated keys and memory entries. A special mechanism keeps track of memory usage and temporal links between entries.}
    \label{fig:purpose_modularity:programme}
    \end{figure}

\subsection{Programme Simulation}
\label{ssec:programme}
% DYNAMIC
Another distinct purpose of modular architectures is to model programmes, as a means to induce them from data or to simulate symbolic algorithms. The simplest (and least expressive) family of programmes are Finite State Automata (FSA). These receive a neural implementation in the Compositional Recursive Learner \citep[CRL;][]{chang2018automatically}, similarly to Routing Networks \citep{rosenbaum2017routing} and
Modular Networks \citep{kirsch2018modular}. In these neural architectures, a loose equivalence can be drawn as follows: transformations induced by modules are transition functions (arcs in the graph), input and output representations are the states (nodes in the graph), and the input is the starting state. A memoryless routing function selects the transition based on the current state. Thus, the programme graph is constructed \textit{dynamically}. The final states are defined as those reached after the router selects a special end-of-computation action.

% GLOBAL
On the other hand, a programme graph can be constructed \textit{globally} based on the task description before processing the data. In particular, Neural Module Networks \citep[NMNs;][]{andreas2016nmn,andreas2016learning} rely on an off-the-shelf semantic parser (and custom rules) to map a query in natural language into a tree graph. The nodes of this graph are learnable modules characterised by: 1) their \textit{types} of input (raw image features and/or attention scores) and output (attention scores or labels); and 2) the particular \textit{instances} of a type, indicated as an argument in the form of a natural language string. For instance, the module \texttt{find[cat]} takes an image and returns attention scores over the regions that contain cats. Compositionality is achieved by sharing weights across modules with the same type or instance. NMNs have been further extended to be amenable to end-to-end training without the aid of an external parser \citep{Hu_2017_ICCV}. In this case, the mapping from queries to programme graphs is learned by imitating expert demonstrations while the module parameters are learned based on the downstream loss of visual question answering.

% MEMORY
In addition to the routing function and computation functions, a model can be extended with an external memory. In fact, these three mirror the fundamental components of a computer architecture: elementary operations, logical flow control, and a random-access memory that can be read and written to \citep{von1945first,graves2014neural}. While (appropriately wired) recurrent neural networks have been shown to be Turing-complete \citep{sontag1995computational}, separating the three functions into distinct components provides an inductive bias to simulate the workflow or a computer programme. Neural Turing Machines \citep[NTMs;][]{graves2014neural} introduced a fully differentiable read--write memory matrix that interfaces with the main recurrent network through an attentional mechanism. In particular, this memory can be addressed both based on content (i.e., the match between its entries and the current input) and based on location, in order to store and retrieve temporally ordered information in contiguous entries. NTMs were further extended into the Differentiable Neural Computer \citep[DNCs;][ \cref{fig:purpose_modularity:programme}]{graves2016hybrid}, which amended some of the limitations of NMTs, such as avoiding interference in the memory, freeing up previously written locations, and storing temporally ordered sequences in non-contiguous chunks. Another family of memory-augmented methods include the Neural Programmer Interpreter \citep[NPI;][]{Reed2016npi}. This model is trained with full supervision from execution traces or through reinforcement learning \citep{Pierrot2019alphanpi}. In particular, a core recurrent network receives information from a programme module, as well as representations from the environment module. In its output, it produces the index for the next sub-programme and its arguments (as well as a special termination symbol).

% SIMULATION
Finally, a recent thread of research focused on \textit{simulating} the behaviour of symbolic algorithms with vanilla (non-modular) neural networks. An example is neural algorithmic reasoning \citep{velivckovic2021neural}. First, a processor network is trained to emulate the output of a symbolic programme (e.g., Dijkstra's algorithm for shortest paths) that operates on abstract representations (e.g., weighted graphs). Second, encoder and decoder networks can be trained to operate on sensory real-world data while matching the input--output types expected by the processor network.

% APPLICATIONS
Among the main applications for programme simulations are settings where sub-problems are shared, such as multi-task or curriculum learning. By distilling the most common functionalities into modules, these can be reused to generalise compositionally to new sequences of sub-tasks. Another application is compositional reasoning, such as (visual) question answering \citep{andreas2016nmn,andreas2016learning}. In general, external memory is useful for reasoning over complex data structures, such as graphs \citep{graves2014neural,graves2016hybrid,Reed2016npi}. Finally, neural models can emulate symbolic algorithms to extend their capabilities to operate on sensory real-world data.

    \begin{figure}[t]
    \centering
        \includegraphics[width=.4\linewidth]{img/schema_causal}  
        \caption{An example of \textbf{Causal~Inference}~(\S~\ref{ssec:causal}): Causal Independent Mechanisms \citep{parascandolo2018learning}. A transformed example is routed to an expert which maps it to the original distribution. An adversarial discriminator attempts to distinguish between reconstructed and original examples.}
        \label{fig:purpose_modularity:causal}
    \end{figure}

\subsection{Causal Discovery and Inference}
\label{ssec:causal}

Modularity in the design of a model may be assumed to reflect the modularity in the (physical) mechanisms of the world. In fact, a crucial assumption in causal inference \citep{ScholkopfJPSZM12} is that such mechanisms underlying data generation are independent, as they do not influence each other, and reusable, as they may play a role in multiple distributions. Consequently, if one of the mechanisms, which defines a conditional distribution in the model graph, changes---possibly because of an intervention---the other modules remain invariant. If a machine learning model mirrors this modular structure, it is better suited to generalise in a sample-efficient way to new tasks: in fact, local distribution shifts require updating only the corresponding module parameters, which in turn results in faster adaptation \citep{Bengio2020meta,mittal2022is}.

The key challenge for this problem is how to specialise each module towards a specific mechanism based uniquely on observational data, especially when the number and nature of the mechanisms are unknown. Competition among the modules through top-\textit{k} routing (see \S~\ref{sssec:hardrouting}) is a common feature of many proposed solutions.\footnote{In addition to causal inference, this strategy is also inspired by the \textit{global workspace theory} \citep{baars2005global}. This theory postulates specialised modules compete to update a shared workspace, and the resulting communication bottleneck creates a crucial inductive bias in human cognition.}
\citet{parascandolo2018learning} show how to \textit{invert} causal independent mechanisms through a modular neural architecture, given data from the original distribution and an unlabelled mixture of their transformations (see \cref{fig:purpose_modularity:causal}). Their model consists of a mixture of experts and an adversarial discriminator, which enforces that the inverted transformation lies in the support of the original distribution. Another architecture relying on module competition and capable of modelling sequential data is Recurrent Independent Mechanisms \citep[RIMs;][]{goyal2019recurrent}. Here, the modules are recurrent networks with separate parameters, each representing a different transition dynamics. However, their states are not entirely independent, as active modules are allowed to communicate through attention. This reflects a second assumption, namely that the dependencies among variables are highly sparse \citep{mittal2022is}. Attention can also serve to direct the flow of bottom-up and top-down information \citep{pmlr-v119-mittal20a}.

% OBJECT CENTRIC INFERENCE
Another challenge of neural causal discovery is jointly inducing abstract latent variables (such as objects or entities) from low-level perception (e.g., pixels of an image) while simultaneously learning the causal graph underlying such variables, which determines how they interact \citep{ke2021systematic}. The lacklustre abilities of vanilla neural models to understand the compositional properties of symbolic building blocks, i.e.\ their `binding problem', arguably explains their current shortfalls in systematic generalisation \citep{greff2020binding}. \textit{Object-centric learning} holds promise to mitigate these limitations. For instance, it can be facilitated by slot attention, which is a fully differentiable and iterative attention mechanism that interfaces between perceptual representations and slots, a set of unordered placeholder variables \citep{NEURIPS2020_8511df98}.
\citep{goyal2021nps} propose Neural Production Systems, where rule templates can be bound to specific entities present in the working memory, in order to update their representations. In particular, rules are MLP modules and the matching with entities (triggering updates) is parameterised by attention.

% INTERVENTIONS
Crucially, observational data alone is often\footnote{Unless specific assumptions are made about the data generating process, such as linear but non-Gaussian data.} insufficient to learn structural causal models as they may not be identifiable \citep{pearl2009causality}. Hence the necessity to augment observation with \textit{interventions} and \textit{counterfactuals}. These allow for answering questions about cause--effect relationships rather than mere correlations. In real-world scenarios, however, the nature and number of interventions are unknown \citet{ke2021systematic}. In this setting, there is no formal guarantee that causal discovery succeeds. Yet, \citet{ke2019learning} finds that DAG discovery on interventional data based on continuous optimisation recovers causal graphs reliably. In particular, modular architectures surpass both vanilla models and graph neural networks \citep{ke2021systematic}. Recently, \citet{geffner2022deep} perform causal inference in a deep non-linear additive noise structural equation model, based on autoregressive flows. Variational inference is used to learn a posterior over causal graphs. The learned functions can be further used to estimate conditional average treatment effects based on simulations.

% APPLICATIONS
The main purpose of these deep modular methods is causal inference and discovery, which has applications in several branches of medicine and economics \citep{geffner2022deep}. In addition, these methods are particularly relevant in grounded settings, where the distribution of the observations from the environment changes as the agent learns better policies \citep{goyal2019recurrent}. Moreover, causal discovery can be combined with model-based RL methods to learn a self-supervised model of the environment, i.e.\ its variables and their causal dependencies, from trajectories of observations, actions, and rewards. This allows for simulating the potential outcomes of a policy before execution and thus estimating better value functions, which dramatically improves sample efficiency in agents \citep{ke2021systematic}. Another common application of this family of modular neural architectures is out-of-distribution generalisation: for instance, zero-shot transfer to images of different sizes or sequences of different lengths \citep{goyal2019recurrent}.

 

% \subsubsection*{Acknowledgments}
\iffalse
\section{Related Work}
\paragraph{Modularity in Biological Networks} The causes that led to the emergence of modularity in biological networks remain controversial. The leading hypothesis is the structure of the environments itself, where common sub-problems reoccur in different combinations \citep{kashtan2005spontaneous}. \citet{clune2013evolutionary} suggest instead a direct pressure to minimise neural connections, and thus computational costs: in simulated evolutionary dynamics, they found modularity to emerge when maximising performance and sparsity. Finally, \citet{schlosser2004modularity} notes that modularity makes organisms more robust by reducing the number of characters a mutation can affect (also known as pleiotropy). \citet{ballard1986cortical} hypothesises that modularity arises in the human brain cortex from two separate reasons, namely to represent high-dimensional variables and functions efficiently given the number of available neurons.
Finally, `neutral' explanations reject the role of evolution and reduce modularity to the intrinsic dynamics of network growth \citep{sole2008spontaneous}
Most likely, multiple causes contribute to the rise of modularity to a different degree \citep{wagner2007road}.

 

\paragraph{Linguistic Composition} An active area of research in NLP and cognitive science is the study of the compositional generalisation ability of current models. Such studies typically involve synthetic datasets that have been created to investigate the desired composition behaviour such as communication games involving a speaker and listener \citep{Andreas2019}, SCAN \citep{lake2018generalization}, which focuses on simple navigation commands paired with action sequences, COGS \citep{Kim2020COGS}, a semantic parsing dataset focusing on combinations of syntactic structures, and CFQ \citep{Keysers2020MeasuringComposition} consisting of questions paired with SPARQL queries, among others. \citet{Andreas2017} investigated the linguistic structure in representations and observed that certain structured relationships such as negation are encoded as linear transformations in the vector space.

Larger models do not significantly outperform smaller models on such compositional tasks while methods such as prompt tuning have been shown to be helpful \citep{Qiu2022}, highlighting the promise of modular approaches for such compositional generalisation. Beyond specialised architectures, data augmentation methods have also been successful in this setting \citep{Akyurek2021,yang-etal-2022-subs}.
\fi

\section{Conclusions}

\begin{tcolorbox}{%
    \begin{itemize}
      \setlength\itemsep{-.1em}
        \item \textbf{Modularity} is defined as the functional specialisation of the components of a system. 
        \item Specialised sub-networks may emerge in vanilla neural modules (from multitask training or regularisation), but they are seldom reused and recombined.
        \item Deep modular architectures rest on the separation between \textbf{computation} functions on the one hand and \textbf{routing} and \textbf{aggregation} functions on the other.
        \item Computation functions may consist of any neural module. Modules may modify the original \textbf{parameters}, be concatenated to the \textbf{input}, or composed with the original \textbf{function}.  
        \item All composition strategies are \textbf{equivalent} to summing the original output with a term depending on the new module. In practice, however, they offer different \textbf{trade-offs} between efficiency (in time and space, during training and inference) and performance.
        \item Routing controls the flow of information, i.e., module selection. In \textbf{fixed} routing, it is determined \textit{a priori} based on expert knowledge. When this is not available, routing parameters are \textbf{learned}.
        \item Learned routing is challenging because of \textbf{training instability}, \textbf{module collapse}, and \textbf{overfitting}. Thus, learned routing often underperforms fixed routing.
        \item Routing can be \textbf{conditioned} on (parts of) the input or metadata such as task identity. Routing can take place at different \textbf{levels}, such as {globally} for the whole model or {layer-wise}.
        \item \textbf{Soft} routing assigns every module a continuous score and performs a weighted combination of their outputs. It is amenable to being learned via gradient descent but is highly inefficient.
        \item \textbf{Hard} routing activates only a subset of modules via top-$1$, top-$k$, or variable-size selection. It is learned via reinforcement learning, evolutionary algorithms, or stochastic re-parameterisation. It corresponds to the principles of conditional computation and information bottleneck in cognition.
        \item \textbf{Hypernetworks} can be interpreted as combining unnormalised routing (task embedding) with modules (generator). They can in turn generate parameters of other modules.
        \item If routing selects multiple modules, these must be \textbf{aggregated} via a function. \item Module parameters or outputs can be \textbf{interpolated} for aggregation, according to scores from the routing function, an attention mechanism, or via simple averaging.
        \item Alternatively, aggregation may involve composing the module functions, either \textbf{sequentially} or based on a \textbf{tree graph} obtained from global routing.    
        \item The applications include \textbf{parameter-efficient fine-tuning} in NLP, computer vision, and speech processing. These rely on the same types of modules and fixed routing. In addition to increased efficiency, this prevents negative interference and enables zero-shot transfer.
        \item Modularity also serves the purpose of \textbf{generalising to new tasks} systematically, by recombining modules and locally updating them.
        \item Modular deep learning transcends the confines of private research: it enables \textbf{community-driven} sharing, expanding, reusing, and updating of the modules.
        \item In \textbf{hierarchical reinforcement learning}, modular \textbf{options} serve as abstractions between task goals and low-level actions and observations. They facilitate planning in long-horizon and sparse-reward tasks and increase sample efficiency due to transferability. 
        \item In \textbf{programme induction}, the components of deep models can mirror a computer architecture: modules are elementary operations and routing is logical flow control. These are often  augmented by an external read--write \textbf{memory}. Modules can also simulate symbolic algorithms.
        \item In \textbf{causal discovery and inference}, modules may be taken to correspond to physical mechanisms that are independent and reusable.
        \item Modular deep learning empowers these traditional applications by learning abstractions (options, programmes, causal graphs) \textbf{end-to-end from perceptual stimuli}.
    \end{itemize}
    }%
\end{tcolorbox}

\iftrue
\subsection{Future Work}

While recently modularity has attracted increasing attention in research, there remain many interesting open research questions along the axes of modularity introduced in this survey. We provide an overview of some of these directions for future work.

\paragraph*{Combination of Different Types of Computation Functions} Existing computation functions (see \Cref{sec:nature_modularity}) are mostly associated with a single category: parameter composition, input composition, or function composition. There are a few exceptions such as compacter \citep{Mahabadi2021Compacter}---low-rank adapters---which combine multiple types. In general, techniques from parameter composition that incorporate sparsity, a low-rank or structural constraint are agnostic of the form of the module. In practice, this should enable more efficient learning and aggregation.  

\paragraph*{Learned Module Structure} Most modules used in current works share the same architecture, which is reused across different settings. Depending on the skill or knowledge that should be learned, a module may need to be structured differently and might require access to another component or other type of data. In the extreme, a model may require a special-purpose architecture to be able to perform a specific capability \citep{andreas2016nmn}. As modules are more widely used, they may benefit from being learned in a more flexible manner, perhaps incorporating ideas from neural architecture search \citep{negrinho2019towards} in a module-specific space of architecture primitives. 

\paragraph*{Standardising Modularity Evaluation} Depending on the dimension studied, modular approaches may be evaluated based on a variety of factors including downstream performance, memory footprint, number of parameters, latency, and compositional generalisation. In order to make progress on modular models in general, evaluation should be standardised. Current evaluation is additionally mainly based on existing datasets that are re-purposed to enable modular evaluation such as by framing them in a zero-shot transfer setting. Future work on modularity evaluation should design forward-looking evaluation benchmarks that are designed to test the capabilities of the next generation of modular models such as assessing the composition of skills and acquisition of new types of reasoning abilities.

\paragraph*{Nature of Modular Representations} While modular representations have been aggregated and composed, it remains mostly unclear how the inductive bias of a computation function influences the modular representation that is learned. In addition, it remains unclear how computation functions differ on a representation level. Beyond the computation function, it is also unclear how the other dimensions of our taxonomy, i.e., the routing function, the aggregation function, and the training setting influence the nature of the modular representations.

\paragraph*{Hierarchical Modularity} Current approaches mostly do not differentiate between high-level and low-level skills and how they relate to each other. It might also be possible to designate particular parts of the model or dedicated modules to capture a set of specialised skills or options, and clearly distinguish between other (sets of) skills. At fine-tuning, even more specialised sub-modules could be learned focused only on the previously designated modules. One example might be learning fine-grained specialised subnetworks over larger subnetworks of the original model, offering gradual module specialisation.

\paragraph*{Learned Routing for Pre-training} Fixed routing (see \Cref{sec:routing}) is the most common strategy to disentangle knowledge into modular parts of the model. However, fixed routing limits the usability of the proposed methods as they cannot be used on data, which lacks the metadata needed for fixed routing; for instance, when training on heterogeneous data, metadata such as domain information often does not exist. While learned routing methods do not require this metadata to perform routing a priori, they suffer from training difficulties (as discussed in \Cref{sec:routing:learned}). This opens up research directions that enable modular pre-training with learned routing, which would make modular models applicable to a broader set of data. 

\paragraph{Modular Instruction Tuning} The main way in which current LLMs are specialised to particular downstream settings is via instruction tuning \citep{wei2021finetuned}, i.e., fine-tuning on a collection of tasks described via instructions. These tasks are increasingly defined based on a set of skills and capabilities that a model should learn, which opens the room to developing modular instruction tuning methods that enable the acquisition, updating, and composition of specialised knowledge.

\paragraph*{Benchmarking Routing Methods} Existing studies mainly evaluate routing methods based on performance but do not take into account how different routing strategies influence modular representations. In order to make progress on better routing methods, benchmarks and metrics are necessary that compare routing mechanisms from a modularity perspective across different settings. 

\paragraph*{Structured and Sparse Aggregation} Current aggregation methods (see \Cref{sec:compositionality}) combine the information from multiple modular components by applying arithmetic operations such as addition and subtraction across all parameters, which likely includes parameters that should not be modified. Structured or sparse aggregation methods could focus on aggregating information within salient subnetworks or parameter groups, which might make aggregation more efficient and improve out-of-distribution generalisation.

\paragraph*{Learned Aggregation Methods} Most aggregation methods are based on arithmetic operations. Depending on the nature of the modular information, it may be useful to (non-)linearly transform the representations. More complex domain-specific aggregation methods can be learned in conjunction with the modular representations to enable better generalisation to new settings. 

\paragraph*{Merging Modular Models} In recent work, merging models trained with different settings has led to improved performance \citep[\textit{inter alia}]{Wortsman2022ModelSoups}. Rather than requiring separate training runs of a model, a multi-task model can alternatively be trained with modular components that are designed to be merged at a later stage. This potentially allows for an architecture, which can be computationally efficiently trained while covering many modalities. 

\paragraph*{Extensible Multi-task Models}  Most approaches in multi-task learning have focused on training dense models, with a key limitation being that models cannot easily be extended to new settings. Focusing on training multi-task models with modular components ensures that the baseline models are much easier to adapt and extend to new settings. Given the trend of pre-training larger and larger models from scratch, modularising parts of such models and developing modular methods that can be shared across different architectures and model sizes may lead to more sustainable model development.

 
\section*{Acknowledgements}

Ivan Vuli\'{c} is supported by a personal Royal Society University Research Fellowship (no 221137; 2022--).

We are grateful to Colin Raffel for his comments and suggestions, which have greatly improved the manuscript. We thank Andrea Gesmundo for feedback on a draft of this paper. We are thankful to Kyunghyun Cho and Alessandro Sordoni for stimulating discussions. We thank the anonymous reviewers for helpful suggestions and feedback.
 

\bibliography{tmlr}
\bibliographystyle{tmlr}

% \appendix
% \section{Appendix} 

\end{document}
