@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@inproceedings{Wang:2022speech,
  author    = {Pu Wang and
               Van hamme, Hugo},
  title     = {Bottleneck Low-rank Transformers for Low-resource Spoken Language
               Understanding},
  booktitle = {Proceedings of Interspeech 2022, 23rd Annual Conference of the International Speech
               Communication Association, Incheon, Korea, 18-22 September 2022},
  pages     = {1248--1252},
  year      = {2022},
  url       = {https://doi.org/10.21437/Interspeech.2022-10801},
  doi       = {10.21437/Interspeech.2022-10801},
}

@article{Eeckt:2022speech,
  doi = {10.48550/arXiv.2203.16082},  
  url = {https://arxiv.org/abs/2203.16082},  
journal   = {CoRR},
volume = {abs/203.16082},
  author = {Eeckt, Steven Vander and Van hamme, Hugo},
  title = {Using Adapters to Overcome Catastrophic Forgetting in End-to-End Automatic Speech Recognition},
  publisher = {arXiv},
  year = {2022},
  }

@inproceedings{Zhu:2022speech,
  author    = {Yunzheng Zhu and
               Ruchao Fan and
               Abeer Alwan},
  title     = {Towards Better Meta-Initialization with Task Augmentation for Kindergarten-Aged
               Speech Recognition},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022},
  pages     = {8582--8586},
  year      = {2022},
  url       = {https://doi.org/10.1109/ICASSP43922.2022.9747599},
}

@inproceedings{Fan:2022speech,
  author    = {Ruchao Fan and
               Abeer Alwan},
  editor    = {Hanseok Ko and
               John H. L. Hansen},
  title     = {{DRAFT:} {A} Novel Framework to Reduce Domain Shifting in Self-supervised
               Learning and Its Application to Children's {ASR}},
  booktitle = {Interspeech 2022, 23rd Annual Conference of the International Speech
               Communication Association, Incheon, Korea, 18-22 September 2022},
  pages     = {4900--4904},
  publisher = {{ISCA}},
  year      = {2022},
  url       = {https://doi.org/10.21437/Interspeech.2022-11128},
  doi       = {10.21437/Interspeech.2022-11128},
  timestamp = {Wed, 12 Oct 2022 10:48:55 +0200},
  biburl    = {https://dblp.org/rec/conf/interspeech/FanA22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Kannan:2019interspeech,
  author    = {Anjuli Kannan and
               Arindrima Datta and
               Tara N. Sainath and
               Eugene Weinstein and
               Bhuvana Ramabhadran and
               Yonghui Wu and
               Ankur Bapna and
               Zhifeng Chen and
               Seungji Lee},
  title     = {Large-Scale Multilingual Speech Recognition with a Streaming End-to-End
               Model},
  booktitle = {Proceedings of Interspeech 2019, 20th Annual Conference of the International Speech
               Communication Association, Graz, Austria, 15-19 September 2019},
  pages     = {2130--2134},
  year      = {2019},
  url       = {https://doi.org/10.21437/Interspeech.2019-2858},
  doi       = {10.21437/Interspeech.2019-2858},
}

@inproceedings{Biadsy:2022speech,
  author    = {Fadi Biadsy and
               Youzheng Chen and
               Xia Zhang and
               Oleg Rybakov and
               Andrew Rosenberg and
               Pedro J. Moreno},
  title     = {A Scalable Model Specialization Framework for Training and Inference
               using Submodels and its Application to Speech Model Personalization},
  booktitle = {Interspeech 2022, 23rd Annual Conference of the International Speech
               Communication Association, Incheon, Korea, 18-22 September 2022},
  pages     = {5125--5129},
  publisher = {{ISCA}},
  year      = {2022},
  url       = {https://doi.org/10.21437/Interspeech.2022-10613},
  doi       = {10.21437/Interspeech.2022-10613},
  timestamp = {Wed, 26 Oct 2022 13:15:55 +0200},
  biburl    = {https://dblp.org/rec/conf/interspeech/BiadsyCZRRM22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Thomas:2022speech,
  author    = {Bethan Thomas and
               Samuel Kessler and
               Salah Karout},
  title     = {Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic
               Speech Recognition},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022},
  pages     = {7102--7106},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/ICASSP43922.2022.9746223},
}

@inproceedings{Bai:2022speech,
  author    = {Ye Bai and
               Jie Li and
               Wenjing Han and
               Hao Ni and
               Kaituo Xu and
               Zhuo Zhang and
               Cheng Yi and
               Xiaorui Wang},
  title     = {Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts
               for End-to-End Speech Recognition},
  booktitle = {Interspeech 2022, 23rd Annual Conference of the International Speech
               Communication Association, Incheon, Korea, 18-22 September 2022},
  pages     = {1676--1680},
  publisher = {{ISCA}},
  year      = {2022},
  url       = {https://doi.org/10.21437/Interspeech.2022-709},
  doi       = {10.21437/Interspeech.2022-709},
  timestamp = {Tue, 11 Oct 2022 19:11:50 +0200},
  biburl    = {https://dblp.org/rec/conf/interspeech/BaiLHNXZYW22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Winata:2020speech,
  author    = {Genta Indra Winata and
               Samuel Cahyawijaya and
               Zhaojiang Lin and
               Zihan Liu and
               Pascale Fung},
  title     = {Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank
               Transformer},
  booktitle = {2020 {IEEE} International Conference on Acoustics, Speech and Signal
               Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020},
  pages     = {6144--6148},
  year      = {2020},
  url       = {https://doi.org/10.1109/ICASSP40776.2020.9053878},
}

@inproceedings{Chen:2022speech,
  author    = {Zih{-}Ching Chen and
               Chin{-}Lun Fu and
               Chih{-}Ying Liu and
               Shang{-}Wen (Daniel) Li and
               Hung{-}yi Lee},
  title     = {Exploring Efficient-Tuning Methods in Self-Supervised Speech Models},
  booktitle = {{IEEE} Spoken Language Technology Workshop, {SLT} 2022, Doha, Qatar,
               January 9-12, 2023},
  pages     = {1120--1127},
  publisher = {{IEEE}},
  year      = {2023},
  url       = {https://doi.org/10.1109/SLT54892.2023.10023274},
  doi       = {10.1109/SLT54892.2023.10023274},
  timestamp = {Mon, 06 Feb 2023 22:19:30 +0100},
  biburl    = {https://dblp.org/rec/conf/slt/ChenFLLL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Kessler:2022speech,
  author    = {Samuel Kessler and
               Bethan Thomas and
               Salah Karout},
  title     = {An Adapter Based Pre-Training for Efficient and Scalable Self-Supervised
               Speech Representation Learning},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022},
  pages     = {3179--3183},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/ICASSP43922.2022.9747374},
  doi       = {10.1109/ICASSP43922.2022.9747374},
}

@article{Swietojanski:2016taslp,
  author    = {Pawel Swietojanski and
               Jinyu Li and
               Steve Renals},
  title     = {Learning Hidden Unit Contributions for Unsupervised Acoustic Model
               Adaptation},
  journal   = {{IEEE} {ACM} Transactions on Audio, Speech, and Language Processing},
  volume    = {24},
  number    = {8},
  pages     = {1450--1463},
  year      = {2016},
  url       = {https://doi.org/10.1109/TASLP.2016.2560534},
  doi       = {10.1109/TASLP.2016.2560534},
}

@inproceedings{tomanek-etal-2021-residual,
    title = "Residual Adapters for Parameter-Efficient {ASR} Adaptation to Atypical and Accented Speech",
    author = "Tomanek, Katrin  and
      Zayats, Vicky  and
      Padfield, Dirk  and
      Vaillancourt, Kara  and
      Biadsy, Fadi",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.541",
    pages = "6751--6760",
}

@inproceedings{wang-etal-2021-k,
    title = "{K-Adapter}: {I}nfusing {K}nowledge into {P}re-{T}rained {M}odels with {A}dapters",
    author = "Wang, Ruize  and
      Tang, Duyu  and
      Duan, Nan  and
      Wei, Zhongyu  and
      Huang, Xuanjing  and
      Ji, Jianshu  and
      Cao, Guihong  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.121",
    doi = "10.18653/v1/2021.findings-acl.121",
    pages = "1405--1418",
}

@inproceedings{lauscher-etal-2020-common,
    title = "Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers",
    author = "Lauscher, Anne  and
      Majewska, Olga  and
      Ribeiro, Leonardo F. R.  and
      Gurevych, Iryna  and
      Rozanov, Nikolai  and
      Glava{\v{s}}, Goran",
    booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.deelio-1.5",
    doi = "10.18653/v1/2020.deelio-1.5",
    pages = "43--49",
}

@inproceedings{majewska-etal-2021-verb,
    title = "Verb Knowledge Injection for Multilingual Event Processing",
    author = "Majewska, Olga  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran  and
      Ponti, Edoardo Maria  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.541",
    doi = "10.18653/v1/2021.acl-long.541",
    pages = "6952--6969",
}

@inproceedings{Sathyendra:2022speech,
  author    = {Kanthashree Mysore Sathyendra and
               Thejaswi Muniyappa and
               Feng{-}Ju Chang and
               Jing Liu and
               Jinru Su and
               Grant P. Strimel and
               Athanasios Mouchtaris and
               Siegfried Kunzmann},
  title     = {Contextual Adapters for Personalized Speech Recognition in Neural
               Transducers},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022},
  pages     = {8537--8541},
  year      = {2022},
  url       = {https://doi.org/10.1109/ICASSP43922.2022.9746126},
}

@article{Pan:2022modal,
  author    = {Junting Pan and
               Ziyi Lin and
               Xiatian Zhu and
               Jing Shao and
               Hongsheng Li},
  title     = {{ST-Adapter: P}arameter-Efficient Image-to-Video Transfer Learning for
               Action Recognition},
  journal   = {CoRR},
  volume    = {abs/2206.13559},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2206.13559},
}

@inproceedings{pfeiffer-etal-2022-xgqa,
    title = "x{GQA}: Cross-Lingual Visual Question Answering",
    author = "Pfeiffer, Jonas  and
      Geigle, Gregor  and
      Kamath, Aishwarya  and
      Steitz, Jan-Martin  and
      Roth, Stefan  and
      Vuli{\'c}, Ivan  and
      Gurevych, Iryna",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.196",
    doi = "10.18653/v1/2022.findings-acl.196",
    pages = "2497--2511",
}

@inproceedings{Lu:2021knowledge,
    title = "Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models",
    author = "Lu, Qiuhao  and
      Dou, Dejing  and
      Nguyen, Thien Huu",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.325",
    doi = "10.18653/v1/2021.findings-emnlp.325",
    pages = "3855--3865",
}

@inproceedings{Ma:2022retrieval,
  author    = {Xinyu Ma and
               Jiafeng Guo and
               Ruqing Zhang and
               Yixing Fan and
               Xueqi Cheng},
  title     = {Scattered or Connected? An Optimized Parameter-efficient Tuning Approach
               for Information Retrieval},
  booktitle = {Proceedings of the 31st {ACM} International Conference on Information
               {\&} Knowledge Management, Atlanta, GA, USA, October 17-21, 2022},
  pages     = {1471--1480},
  year      = {2022},
  url       = {https://doi.org/10.1145/3511808.3557445},
}

@inproceedings{litschko-etal-2022-parameter,
    title = "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval",
    author = "Litschko, Robert  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.90",
    pages = "1071--1082",
}

@inproceedings{gao-etal-2021-making,
    title = "Making Pre-trained Language Models Better Few-shot Learners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.295",
    doi = "10.18653/v1/2021.acl-long.295",
    pages = "3816--3830",
}

@inproceedings{Yang:2022iclr,
  author    = {Zonghan Yang and
               Yang Liu},
  title     = {On Robust Prefix-Tuning for Text Classification},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  year      = {2022},
  url       = {https://openreview.net/forum?id=eBCmOocUejf},
}

@inproceedings{le-etal-2021-lightweight,
    title = "Lightweight Adapter Tuning for Multilingual Speech Translation",
    author = "Le, Hang  and
      Pino, Juan  and
      Wang, Changhan  and
      Gu, Jiatao  and
      Schwab, Didier  and
      Besacier, Laurent",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.103",
    doi = "10.18653/v1/2021.acl-short.103",
    pages = "817--824",
    abstract = "Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pre-trained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.",
}

@article{Papalampidi:2022modal,
  author    = {Pinelopi Papalampidi and
               Mirella Lapata},
  title     = {Hierarchical3D Adapters for Long Video-to-text Summarization},
  journal   = {CoRR},
  volume    = {abs/2210.04829},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.04829},
  doi       = {10.48550/arXiv.2210.04829},
  eprinttype = {arXiv},
  eprint    = {2210.04829},
  timestamp = {Thu, 13 Oct 2022 14:33:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2210-04829.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Newman:2022iclr,
  author    = {Benjamin Newman and
               Prafulla Kumar Choubey and
               Nazneen Rajani},
  title     = {P-Adapters: Robustly Extracting Factual Information from Language
               Models with Diverse Prompts},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  year      = {2022},
  url       = {https://openreview.net/forum?id=DhzIU48OcZh},
}

@article{Gao:2021modal,
  author    = {Peng Gao and
               Shijie Geng and
               Renrui Zhang and
               Teli Ma and
               Rongyao Fang and
               Yongfeng Zhang and
               Hongsheng Li and
               Yu Qiao},
  title     = {{CLIP-Adapter: B}etter Vision-Language Models with Feature Adapters},
  journal   = {CoRR},
  volume    = {abs/2110.04544},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.04544},
}

@misc{caccia2022multihead,
  doi = {10.48550/ARXIV.2211.03831},
  
  url = {https://arxiv.org/abs/2211.03831},
  
  author = {Caccia, Lucas and Ponti, Edoardo and Liu, Lucas and Pereira, Matheus and Roux, Nicolas Le and Sordoni, Alessandro},
  
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi-Head Adapter Routing for Data-Efficient Fine-Tuning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{Zhou:2022modal,
  author    = {Kaiyang Zhou and
               Jingkang Yang and
               Chen Change Loy and
               Ziwei Liu},
  title     = {Conditional Prompt Learning for Vision-Language Models},
  booktitle = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
  pages     = {16795--16804},
  year      = {2022},
  url       = {https://doi.org/10.1109/CVPR52688.2022.01631},
}

@inproceedings{Maharana:2022modal,
  author    = {Adyasha Maharana and
               Darryl Hannan and
               Mohit Bansal},
  title     = {StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story
               Continuation},
  booktitle = {Computer Vision - {ECCV} 2022 - 17th European Conference, Tel Aviv,
               Israel, October 23-27, 2022, Proceedings, Part {XXXVII}},
  series    = {Lecture Notes in Computer Science},
  volume    = {13697},
  pages     = {70--87},
  publisher = {Springer},
  year      = {2022},
  url       = {https://doi.org/10.1007/978-3-031-19836-6\_5},
  doi       = {10.1007/978-3-031-19836-6\_5},
  timestamp = {Wed, 26 Oct 2022 09:56:25 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/MaharanaHB22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Sung:2022modal,
  author    = {Yi{-}Lin Sung and
               Jaemin Cho and
               Mohit Bansal},
  title     = {{VL-ADAPTER:} Parameter-Efficient Transfer Learning for Vision-and-Language
               Tasks},
  booktitle = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
  pages     = {5217--5227},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/CVPR52688.2022.00516},
  doi       = {10.1109/CVPR52688.2022.00516},
  timestamp = {Tue, 04 Oct 2022 17:56:08 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/Sung0B22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Stickland2019BERTPALs,
  author    = {Asa Cooper Stickland and
               Iain Murray},
  title     = {{BERT} and {PAL}s: Projected Attention Layers for Efficient Adaptation
               in Multi-Task Learning},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {5986--5995},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/stickland19a.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Stickland019.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{hampshire1992meta,
  author    = {John B. Hampshire and
               Alex Waibel},
  title     = {The Meta-Pi Network: Building Distributed Knowledge Representations
               for Robust Multisource Pattern Recognition},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {14},
  number    = {7},
  pages     = {751--769},
  year      = {1992},
  url       = {https://doi.org/10.1109/34.142911},
  doi       = {10.1109/34.142911},
  timestamp = {Wed, 17 May 2017 10:56:27 +0200},
  biburl    = {https://dblp.org/rec/journals/pami/HampshireW92.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ma2018Modeling,
  author    = {Jiaqi Ma and
               Zhe Zhao and
               Xinyang Yi and
               Jilin Chen and
               Lichan Hong and
               Ed H. Chi},
  title     = {Modeling Task Relationships in Multi-task Learning with Multi-gate
               Mixture-of-Experts},
  booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on
               Knowledge Discovery {\&} Data Mining, {KDD} 2018, London, UK,
               August 19-23, 2018},
  pages     = {1930--1939},
  publisher = {{ACM}},
  year      = {2018},
  url       = {https://doi.org/10.1145/3219819.3220007},
  doi       = {10.1145/3219819.3220007},
  timestamp = {Sun, 25 Oct 2020 22:35:27 +0100},
  biburl    = {https://dblp.org/rec/conf/kdd/MaZYCHC18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Han:2023autopeft,
  author    = {Han Zhou and
               Xingchen Wan and
               Ivan Vuli\'{c} and
               Anna Korhonen},
  title     = {{AutoPEFT:} Automatic Configuration Search for Parameter-Efficient Fine-Tuning},
  journal   = {CoRR},
  volume    = {abs/2301.12132},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2301.12132},
  doi       = {10.48550/arXiv.2301.12132},
  eprinttype = {arXiv},
  eprint    = {2301.12132},
  timestamp = {Tue, 31 Jan 2023 17:55:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2301-12132.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Gururangan2022Demix,
    title = "{DEM}ix Layers: Disentangling Domains for Modular Language Modeling",
    author = "Gururangan, Suchin  and
      Lewis, Mike  and
      Holtzman, Ari  and
      Smith, Noah A.  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.407",
    doi = "10.18653/v1/2022.naacl-main.407",
    pages = "5557--5576",
    abstract = "We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.",
}

@inproceedings{Pfeiffer2022Lifting,
    title = "Lifting the Curse of Multilinguality by Pre-training Modular Transformers",
    author = "Pfeiffer, Jonas  and
      Goyal, Naman  and
      Lin, Xi  and
      Li, Xian  and
      Cross, James  and
      Riedel, Sebastian  and
      Artetxe, Mikel",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.255",
    doi = "10.18653/v1/2022.naacl-main.255",
    pages = "3479--3495",
    abstract = "Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-Mod) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.",
}

@article{clune2013evolutionary,
  title={The evolutionary origins of modularity},
  author={Clune, Jeff and Mouret, Jean-Baptiste and Lipson, Hod},
  journal={Proceedings of the Royal Society b: Biological sciences},
  volume={280},
  number={1755},
  pages={20122863},
  year={2013},
  publisher={The Royal Society}
}

@article{sole2008spontaneous,
  title={Spontaneous emergence of modularity in cellular networks},
  author={Sol{\'e}, Ricard V and Valverde, Sergi},
  journal={Journal of The Royal Society Interface},
  volume={5},
  number={18},
  pages={129--133},
  year={2008},
  publisher={The Royal Society London}
}

@article{dietterich2000hierarchical,
  author    = {Thomas G. Dietterich},
  title     = {Hierarchical Reinforcement Learning with the {MAXQ} Value Function
               Decomposition},
  journal   = {Journal of Artificial Intelligence Research},
  volume    = {13},
  pages     = {227--303},
  year      = {2000},
  url       = {https://doi.org/10.1613/jair.639},
  doi       = {10.1613/jair.639},
  timestamp = {Mon, 21 Jan 2019 15:01:17 +0100},
  biburl    = {https://dblp.org/rec/journals/jair/Dietterich00.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kulkarni2016hierarchical,
  author    = {Tejas D. Kulkarni and
               Karthik Narasimhan and
               Ardavan Saeedi and
               Josh Tenenbaum},
  title     = {Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction
               and Intrinsic Motivation},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
               on Neural Information Processing Systems 2016, December 5-10, 2016,
               Barcelona, Spain},
  pages     = {3675--3683},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/hash/f442d33fa06832082290ad8544a8da27-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/KulkarniNST16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dayan1992feudal,
  author    = {Peter Dayan and
               Geoffrey E. Hinton},
  title     = {Feudal Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 5, {[NIPS} Conference,
               Denver, Colorado, USA, November 30 - December 3, 1992]},
  pages     = {271--278},
  publisher = {Morgan Kaufmann},
  year      = {1992},
  url       = {http://papers.nips.cc/paper/714-feudal-reinforcement-learning},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/DayanH92.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
florensa2017stochastic,
  author    = {Carlos Florensa and
               Yan Duan and
               Pieter Abbeel},
  title     = {Stochastic Neural Networks for Hierarchical Reinforcement Learning},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=B1oK8aoxe},
  timestamp = {Thu, 25 Jul 2019 14:25:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/FlorensaDA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{harb2018waiting,
  title={When waiting is not an option: Learning options with a deliberation cost},
  author={Harb, Jean and Bacon, Pierre-Luc and Klissarov, Martin and Precup, Doina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{
frans2018meta,
title={{META} {LEARNING} {SHARED} {HIERARCHIES}},
author={Kevin Frans and Jonathan Ho and Xi Chen and Pieter Abbeel and John Schulman},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SyX0IeWAW},
}

@article{levy2018hierarchical,
  title={Hierarchical reinforcement learning with hindsight},
  author={Levy, Andrew and Platt, Robert and Saenko, Kate},
  journal={arXiv preprint arXiv:1805.08180},
  year={2018}
}

@inproceedings{nachum2018data,
  author    = {Ofir Nachum and
               Shixiang Gu and
               Honglak Lee and
               Sergey Levine},
  title     = {Data-Efficient Hierarchical Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
               on Neural Information Processing Systems 2018, NeurIPS 2018, December
               3-8, 2018, Montr{\'{e}}al, Canada},
  pages     = {3307--3317},
  year      = {2018},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/NachumGLL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jiang2019language,
  author    = {Yiding Jiang and
               Shixiang Gu and
               Kevin Murphy and
               Chelsea Finn},
  title     = {Language as an Abstraction for Hierarchical Deep Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {9414--9426},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/0af787945872196b42c9f73ead2565c8-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/JiangGMF19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{wagner2005natural,
  title={Natural Selection and the Origin of Modules},
  author={Wagner, G{\"u}nter P. and Mezey, Jason and Calabretta, Raffaele},
  booktitle={Modularity: Understanding the Development and Evolution of Complex Natural Systems},
  pages={33},
  editor={Werner Callebaut and Diego Rasskin-Gutman},
  year={2005},
  publisher={MIT Press},
  url = {https://doi.org/10.7551/mitpress/4734.001.0001},
  doi = {https://doi.org/10.7551/mitpress/4734.001.0001},
}

@book{schlosser2004modularity,
  title={Modularity in development and evolution},
  author={Schlosser, Gerhard and Wagner, G{\"u}nter P and others},
  year={2004},
  publisher={University of Chicago Press}
}

@article{wagner2007road,
  title={The road to modularity},
  author={Wagner, G{\"u}nter P and Pavlicev, Mihaela and Cheverud, James M},
  journal={Nature Reviews Genetics},
  volume={8},
  number={12},
  pages={921--931},
  year={2007},
  publisher={Nature Publishing Group}
}

@article{kashtan2005spontaneous,
  title={Spontaneous evolution of modularity and network motifs},
  author={Kashtan, Nadav and Alon, Uri},
  journal={Proceedings of the National Academy of Sciences (PNAS)},
  volume={102},
  number={39},
  pages={13773--13778},
  year={2005},
  publisher={National Acad Sciences},
  doi = {https://doi.org/10.1073/pnas.0503610102},
  url = {https://doi.org/10.1073/pnas.0503610102},
}

@article{Fan2021Beyond,
  author    = {Angela Fan and
               Shruti Bhosale and
               Holger Schwenk and
               Zhiyi Ma and
               Ahmed El{-}Kishky and
               Siddharth Goyal and
               Mandeep Baines and
               Onur Celebi and
               Guillaume Wenzek and
               Vishrav Chaudhary and
               Naman Goyal and
               Tom Birch and
               Vitaliy Liptchinsky and
               Sergey Edunov and
               Michael Auli and
               Armand Joulin},
  title     = {Beyond English-Centric Multilingual Machine Translation},
  journal   = {Journal of Machine Learning Research},
  volume    = {22},
  pages     = {107:1--107:48},
  year      = {2021},
  url       = {http://jmlr.org/papers/v22/20-1307.html},
  timestamp = {Mon, 31 Jan 2022 17:23:36 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/FanBSMEGBCWCGBL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{baldwin2000design,
  title={Design rules: The power of modularity},
  author={Baldwin, Carliss Young and Clark, Kim B.},
  year={2000},
  publisher={MIT Press},
  url = {https://doi.org/10.7551/mitpress/2366.001.0001},
}

@inproceedings{csordas2021are,
  author    = {R{\'{o}}bert Csord{\'{a}}s and
               Sjoerd van Steenkiste and
               J{\"{u}}rgen Schmidhuber},
  title     = {Are Neural Nets Modular? Inspecting Functional Modularity Through
               Differentiable Weight Masks},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=7uVcpu-gMD},
  timestamp = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/CsordasSS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hupkes2020compositionality,
  author    = {Dieuwke Hupkes and
               Verna Dankers and
               Mathijs Mul and
               Elia Bruni},
  title     = {Compositionality Decomposed: How do Neural Networks Generalise?},
  journal   = {Journal of Artificial Intelligence Research},
  volume    = {67},
  pages     = {757--795},
  year      = {2020},
  url       = {https://doi.org/10.1613/jair.1.11674},
  doi       = {10.1613/jair.1.11674},
  timestamp = {Wed, 15 Apr 2020 17:53:14 +0200},
  biburl    = {https://dblp.org/rec/journals/jair/HupkesDMB20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{stanczak-etal-2022-neurons,
    title = "Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models",
    author = "Stanczak, Karolina  and
      Ponti, Edoardo  and
      Torroba Hennigen, Lucas  and
      Cotterell, Ryan  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.114",
    doi = "10.18653/v1/2022.naacl-main.114",
    pages = "1589--1598",
    abstract = "The success of multilingual pre-trained models is underpinned by their ability to learn representations shared by multiple languages even in absence of any explicit supervision. However, it remains unclear how these models learn to generalise across languages. In this work, we conjecture that multilingual pre-trained models can derive language-universal abstractions about grammar. In particular, we investigate whether morphosyntactic information is encoded in the same subset of neurons in different languages. We conduct the first large-scale empirical study over 43 languages and 14 morphosyntactic categories with a state-of-the-art neuron-level probe. Our findings show that the cross-lingual overlap between neurons is significant, but its extent may vary across categories and depends on language proximity and pre-training data size.",
}

@inproceedings{lake2018generalization,
  author    = {Brenden M. Lake and
               Marco Baroni},
  title     = {Generalization without Systematicity: On the Compositional Skills
               of Sequence-to-Sequence Recurrent Networks},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {2879--2888},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/lake18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/LakeB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ulrich1995role,
  title={The role of product architecture in the manufacturing firm},
  author={Ulrich, Karl},
  journal={Research Policy},
  volume={24},
  number={3},
  pages={419--440},
  year={1995},
  publisher={Elsevier},
  doi = {https://doi.org/10.1016/0048-7333(94)00775-3},
  url = {https://doi.org/10.1016/0048-7333(94)00775-3}
}

@article{reed2022generalist,
  author    = {Scott E. Reed and
               Konrad Zolna and
               Emilio Parisotto and
               Sergio Gomez Colmenarejo and
               Alexander Novikov and
               Gabriel Barth{-}Maron and
               Mai Gimenez and
               Yury Sulsky and
               Jackie Kay and
               Jost Tobias Springenberg and
               Tom Eccles and
               Jake Bruce and
               Ali Razavi and
               Ashley Edwards and
               Nicolas Heess and
               Yutian Chen and
               Raia Hadsell and
               Oriol Vinyals and
               Mahyar Bordbar and
               Nando de Freitas},
  title     = {A Generalist Agent},
  journal   = {CoRR},
  volume    = {abs/2205.06175},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.06175},
  doi       = {10.48550/arXiv.2205.06175},
  eprinttype = {arXiv},
  eprint    = {2205.06175},
  timestamp = {Tue, 17 May 2022 17:31:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-06175.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dosovitskiy2020image,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2021m6,
  author    = {An Yang and
               Junyang Lin and
               Rui Men and
               Chang Zhou and
               Le Jiang and
               Xianyan Jia and
               Ang Wang and
               Jie Zhang and
               Jiamang Wang and
               Yong Li and
               Di Zhang and
               Wei Lin and
               Lin Qu and
               Jingren Zhou and
               Hongxia Yang},
  title     = {Exploring Sparse Expert Models and Beyond},
  journal   = {CoRR},
  volume    = {abs/2105.15082},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.15082},
  eprinttype = {arXiv},
  eprint    = {2105.15082},
  timestamp = {Fri, 05 Aug 2022 15:40:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15082.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lewis2021base,
  author    = {Mike Lewis and
               Shruti Bhosale and
               Tim Dettmers and
               Naman Goyal and
               Luke Zettlemoyer},
  title     = {{BASE} Layers: Simplifying Training of Large, Sparse Models},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {6265--6274},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/lewis21a.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/LewisBDGZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{you2022speechmoe2,
  author    = {Zhao You and
               Shulin Feng and
               Dan Su and
               Dong Yu},
  title     = {Speechmoe2: Mixture-of-Experts Model with Improved Routing},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2022, Virtual and Singapore, 23-27 May 2022},
  pages     = {7217--7221},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/ICASSP43922.2022.9747065},
  doi       = {10.1109/ICASSP43922.2022.9747065},
  timestamp = {Tue, 07 Jun 2022 17:34:53 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/YouFSY22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{yadav2023resolving,
      title={Resolving Interference When Merging Models}, 
      author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
      year={2023},
      eprint={2306.01708},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jang2023exploring,
      title={Exploring the Benefits of Training Expert Language Models over Instruction Tuning}, 
      author={Joel Jang and Seungone Kim and Seonghyeon Ye and Doyoung Kim and Lajanugen Logeswaran and Moontae Lee and Kyungjae Lee and Minjoon Seo},
      year={2023},
      eprint={2302.03202},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shen2023mixtureofexperts,
      title={Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models}, 
      author={Sheng Shen and Le Hou and Yanqi Zhou and Nan Du and Shayne Longpre and Jason Wei and Hyung Won Chung and Barret Zoph and William Fedus and Xinyun Chen and Tu Vu and Yuexin Wu and Wuyang Chen and Albert Webson and Yunxuan Li and Vincent Zhao and Hongkun Yu and Kurt Keutzer and Trevor Darrell and Denny Zhou},
      year={2023},
      eprint={2305.14705},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{muqeeth2023soft,
      title={Soft Merging of Experts with Adaptive Routing}, 
      author={Mohammed Muqeeth and Haokun Liu and Colin Raffel},
      year={2023},
      eprint={2306.03745},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kandpal2023gittheta,
      title={Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models}, 
      author={Nikhil Kandpal and Brian Lester and Mohammed Muqeeth and Anisha Mascarenhas and Monty Evans and Vishal Baskaran and Tenghao Huang and Haokun Liu and Colin Raffel},
      year={2023},
      eprint={2306.04529},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jin2023dataless,
      title={Dataless Knowledge Fusion by Merging Weights of Language Models}, 
      author={Xisen Jin and Xiang Ren and Daniel Preotiuc-Pietro and Pengxiang Cheng},
      year={2023},
      eprint={2212.09849},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Gupta2020Stochastic,
  author       = {Vipul Gupta and
                  Santiago Akle Serrano and
                  Dennis DeCoste},
  title        = {Stochastic Weight Averaging in Parallel: Large-Batch Training that
                  Generalizes Well},
  journal      = {CoRR},
  volume       = {abs/2001.02312},
  year         = {2020},
  url          = {http://arxiv.org/abs/2001.02312},
  eprinttype    = {arXiv},
  eprint       = {2001.02312},
  timestamp    = {Mon, 13 Jan 2020 12:40:17 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2001-02312.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{choshen2022fusing,
      title={Fusing finetuned models for better pretraining}, 
      author={Leshem Choshen and Elad Venezian and Noam Slonim and Yoav Katz},
      year={2022},
      eprint={2204.03044},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Gueta2023Knowledge,
  author       = {Almog Gueta and
                  Elad Venezian and
                  Colin Raffel and
                  Noam Slonim and
                  Yoav Katz and
                  Leshem Choshen},
  title        = {Knowledge is a Region in Weight Space for Fine-tuned Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.04863},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.04863},
  doi          = {10.48550/arXiv.2302.04863},
  eprinttype    = {arXiv},
  eprint       = {2302.04863},
  timestamp    = {Mon, 13 Feb 2023 14:23:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-04863.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{riquelme2021scaling,
  author    = {Carlos Riquelme and
               Joan Puigcerver and
               Basil Mustafa and
               Maxim Neumann and
               Rodolphe Jenatton and
               Andr{\'{e}} Susano Pinto and
               Daniel Keysers and
               Neil Houlsby},
  title     = {Scaling Vision with Sparse Mixture of Experts},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {8583--8595},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:47 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/RiquelmePMNJPKH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{puigcerver2023sparse,
      title={From Sparse to Soft Mixtures of Experts}, 
      author={Joan Puigcerver and Carlos Riquelme and Basil Mustafa and Neil Houlsby},
      year={2023},
      eprint={2308.00951},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chi2022representation,
      title={On the Representation Collapse of Sparse Mixture of Experts}, 
      author={Zewen Chi and Li Dong and Shaohan Huang and Damai Dai and Shuming Ma and Barun Patra and Saksham Singhal and Payal Bajaj and Xia Song and Xian-Ling Mao and Heyan Huang and Furu Wei},
      year={2022},
      eprint={2204.09179},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

 @misc{shen2023moduleformer,
      title={ModuleFormer: Modularity Emerges from Mixture-of-Experts}, 
      author={Yikang Shen and Zheyu Zhang and Tianyou Cao and Shawn Tan and Zhenfang Chen and Chuang Gan},
      year={2023},
      eprint={2306.04640},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{liu-etal-2021-visually,
    title = "Visually Grounded Reasoning across Languages and Cultures",
    author = "Liu, Fangyu  and
      Bugliarello, Emanuele  and
      Ponti, Edoardo Maria  and
      Reddy, Siva  and
      Collier, Nigel  and
      Elliott, Desmond",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.818",
    doi = "10.18653/v1/2021.emnlp-main.818",
    pages = "10467--10485",
    abstract = "The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.",
}


@inproceedings{ponti-etal-2020-xcopa,
    title = "{XCOPA}: A Multilingual Dataset for Causal Commonsense Reasoning",
    author = "Ponti, Edoardo Maria  and
      Glava{\v{s}}, Goran  and
      Majewska, Olga  and
      Liu, Qianchu  and
      Vuli{\'c}, Ivan  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.185",
    doi = "10.18653/v1/2020.emnlp-main.185",
    pages = "2362--2376",
    abstract = "In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur{\'\i}mac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.",
}

@article{Costa:2022nllb,
  author    = {Marta R. Costa{-}juss{\`{a}} and
               James Cross and
               Onur {\c{C}}elebi and
               Maha Elbayad and
               Kenneth Heafield and
               Kevin Heffernan and
               Elahe Kalbassi and
               Janice Lam and
               Daniel Licht and
               Jean Maillard and
               Anna Sun and
               Skyler Wang and
               Guillaume Wenzek and
               Al Youngblood and
               Bapi Akula and
               Lo{\"{\i}}c Barrault and
               Gabriel Mejia Gonzalez and
               Prangthip Hansanti and
               John Hoffman and
               Semarley Jarrett and
               Kaushik Ram Sadagopan and
               Dirk Rowe and
               Shannon Spruit and
               Chau Tran and
               Pierre Andrews and
               Necip Fazil Ayan and
               Shruti Bhosale and
               Sergey Edunov and
               Angela Fan and
               Cynthia Gao and
               Vedanuj Goswami and
               Francisco Guzm{\'{a}}n and
               Philipp Koehn and
               Alexandre Mourachko and
               Christophe Ropers and
               Safiyyah Saleem and
               Holger Schwenk and
               Jeff Wang},
  title     = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2207.04672},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2207.04672},
  doi       = {10.48550/arXiv.2207.04672},
}

@inproceedings{Dua:2022naacl,
    title = "Tricks for Training Sparse Translation Models",
    author = "Dua, Dheeru  and
      Bhosale, Shruti  and
      Goswami, Vedanuj  and
      Cross, James  and
      Lewis, Mike  and
      Fan, Angela",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.244",
    doi = "10.18653/v1/2022.naacl-main.244",
    pages = "3340--3345",
}

@INPROCEEDINGS{You:2022speech,
  author={You, Zhao and Feng, Shulin and Su, Dan and Yu, Dong},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Speechmoe2: Mixture-of-Experts Model with Improved Routing}, 
  year={2022},
  pages={7217-7221},
  doi={10.1109/ICASSP43922.2022.9747065}
}


@inproceedings{Liang:2020aaai,
  author    = {Jianze Liang and
               Chengqi Zhao and
               Mingxuan Wang and
               Xipeng Qiu and
               Lei Li},
  title     = {Finding Sparse Structures for Domain Specific Neural Machine Translation},
  booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2021, Thirty-Third Conference on Innovative Applications of Artificial
               Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
               in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
               2021},
  pages     = {13333--13342},
  publisher = {{AAAI} Press},
  year      = {2021},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/17574},
  timestamp = {Mon, 07 Jun 2021 11:46:04 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/LiangZWQ021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Zuo2022Taming,
  author    = {Simiao Zuo and
               Xiaodong Liu and
               Jian Jiao and
               Young Jin Kim and
               Hany Hassan and
               Ruofei Zhang and
               Jianfeng Gao and
               Tuo Zhao},
  title     = {Taming Sparsely Activated Transformer with Stochastic Experts},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=B72HXs80q4},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/Zuo00KHZGZ22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Wang2022AdaMix,
    title = "{A}da{M}ix: Mixture-of-Adaptations for Parameter-efficient Model Tuning",
    author = "Wang, Yaqing  and
      Agarwal, Sahaj  and
      Mukherjee, Subhabrata  and
      Liu, Xiaodong  and
      Gao, Jing  and
      Awadallah, Ahmed Hassan  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.388",
    pages = "5744--5760",
    abstract = "Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address this, parameter-efficient fine-tuning (PEFT) techniques were introduced where small trainable components are injected in the PLM and updated during fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of adaptation modules {--} given the underlying PEFT method of choice {--} introduced in each Transformer layer while keeping most of the PLM weights frozen. For instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture of low rank decomposition matrices like LoRA to improve downstream task performance over the corresponding PEFT methods for fully supervised and few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the same computational cost and the number of tunable parameters as the underlying PEFT method. By only tuning 0.1-0.2{\%} of PLM parameters, we show that AdaMix outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for both NLU and NLG tasks.",
}

@inproceedings{rajbhandari2022deepspeed,
  author    = {Samyam Rajbhandari and
               Conglong Li and
               Zhewei Yao and
               Minjia Zhang and
               Reza Yazdani Aminabadi and
               Ammar Ahmad Awan and
               Jeff Rasley and
               Yuxiong He},
  title     = {DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training
               to Power Next-Generation {AI} Scale},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {18332--18346},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/rajbhandari22a.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/RajbhandariLYZA22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hazimeh2021dselect,
  author    = {Hussein Hazimeh and
               Zhe Zhao and
               Aakanksha Chowdhery and
               Maheswaran Sathiamoorthy and
               Yihua Chen and
               Rahul Mazumder and
               Lichan Hong and
               Ed H. Chi},
  title     = {DSelect-k: Differentiable Selection in the Mixture of Experts with
               Applications to Multi-Task Learning},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {29335--29347},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/f5ac21cd0ef1b88e9848571aeb53551a-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:49 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/HazimehZCSCMHC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dua-etal-2022-tricks,
    title = "Tricks for Training Sparse Translation Models",
    author = "Dua, Dheeru  and
      Bhosale, Shruti  and
      Goswami, Vedanuj  and
      Cross, James  and
      Lewis, Mike  and
      Fan, Angela",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.244",
    doi = "10.18653/v1/2022.naacl-main.244",
    pages = "3340--3345",
    abstract = "Multi-task learning with an unbalanced data distribution skews model learning towards high resource tasks, especially when model capacity is fixed and fully shared across all tasks. Sparse scaling architectures, such as BASELayers, provide flexible mechanisms for different tasks to have a variable number of parameters, which can be useful to counterbalance skewed data distributions. We find that that sparse architectures for multilingual machine translation can perform poorly out of the box and propose two straightforward techniques to mitigate this {---} a temperature heating mechanism and dense pre-training. Overall, these methods improve performance on two multilingual translation benchmarks compared to standard BASELayers and Dense scaling baselines, and in combination, more than 2x model convergence speed.",
}


@inproceedings{Baziotis2022MultilingualMTHyperAdapter,
    title = "Multilingual Machine Translation with Hyper-Adapters",
    author = "Baziotis, Christos  and
      Artetxe, Mikel  and
      Cross, James  and
      Bhosale, Shruti",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.77",
    pages = "1170--1185",
    abstract = "Multilingual machine translation suffers from negative interference across languages. A common solution is to relax parameter sharing with language-specific modules like adapters. However, adapters of related languages are unable to transfer information, and their total number of parameters becomes prohibitively expensive as the number of languages grows. In this work, we overcome these drawbacks using hyper-adapters {--} hyper-networks that generate adapters from language and layer embeddings. While past work had poor results when scaling hyper-networks, we propose a rescaling fix that significantly improves convergence and enables training larger hyper-networks. We find that hyper-adapters are more parameter efficient than regular adapters, reaching the same performance with up to 12 times less parameters. When using the same number of parameters and FLOPS, our approach consistently outperforms regular adapters. Also, hyper-adapters converge faster than alternative approaches and scale better than regular dense networks. Our analysis shows that hyper-adapters learn to encode language relatedness, enabling positive transfer across languages.",
}

@inproceedings{pfeiffer-etal-2020-adapterhub,
    title = "{A}dapter{H}ub: A Framework for Adapting Transformers",
    author = {Pfeiffer, Jonas  and
      R{\"u}ckl{\'e}, Andreas  and
      Poth, Clifton  and
      Kamath, Aishwarya  and
      Vuli{\'c}, Ivan  and
      Ruder, Sebastian  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.7",
    doi = "10.18653/v1/2020.emnlp-demos.7",
    pages = "46--54",
    abstract = "The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters{---}small learnt bottleneck layers inserted within each layer of a pre-trained model{---} ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic {``}stiching-in{''} of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml",
}

@inproceedings{bacon2017option,
  author    = {Pierre{-}Luc Bacon and
               Jean Harb and
               Doina Precup},
  editor    = {Satinder Singh and
               Shaul Markovitch},
  title     = {The Option-Critic Architecture},
  booktitle = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
               February 4-9, 2017, San Francisco, California, {USA}},
  pages     = {1726--1734},
  publisher = {{AAAI} Press},
  year      = {2017},
  url       = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14858},
  timestamp = {Tue, 19 Apr 2022 16:03:28 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/BaconHP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Clark2022UnifiedScaling,
  author    = {Aidan Clark and
               Diego de Las Casas and
               Aurelia Guy and
               Arthur Mensch and
               Michela Paganini and
               Jordan Hoffmann and
               Bogdan Damoc and
               Blake A. Hechtman and
               Trevor Cai and
               Sebastian Borgeaud and
               George van den Driessche and
               Eliza Rutherford and
               Tom Hennigan and
               Matthew J. Johnson and
               Albin Cassirer and
               Chris Jones and
               Elena Buchatskaya and
               David Budden and
               Laurent Sifre and
               Simon Osindero and
               Oriol Vinyals and
               Marc'Aurelio Ranzato and
               Jack W. Rae and
               Erich Elsen and
               Koray Kavukcuoglu and
               Karen Simonyan},
  title     = {Unified Scaling Laws for Routed Language Models},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {4057--4086},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/clark22a.html},
  timestamp = {Wed, 13 Jul 2022 16:58:13 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ClarkCGMPHDHCB022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Eigen2013LearningFactored,
  author    = {David Eigen and
               Marc'Aurelio Ranzato and
               Ilya Sutskever},
  title     = {Learning Factored Representations in a Deep Mixture of Experts},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.4314},
  timestamp = {Thu, 25 Jul 2019 14:36:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/EigenRS13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{brown2020language,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:56:50 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{booch2008object,
  author    = {Grady Booch and
               Robert A. Maksimchuk and
               Michael W. Engle and
               Bobbi J. Young and
               Jim Conallen and
               Kelli A. Houston},
  title     = {Object-oriented analysis and design with applications, third edition},
  journal   = {{ACM} {SIGSOFT} Software Engineering Notes},
  volume    = {33},
  number    = {5},
  year      = {2008},
  url       = {https://doi.org/10.1145/1402521.1413138},
  doi       = {10.1145/1402521.1413138},
  timestamp = {Thu, 17 Sep 2020 12:05:21 +0200},
  biburl    = {https://dblp.org/rec/journals/sigsoft/BoochMEYCH08.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sutton1986two,
  title={Two problems with back propagation and other steepest descent learning procedures for networks},
  author={Sutton, Richard S.},
  booktitle={Proceedings of the Eighth Annual Conference of the Cognitive Science Society, 1986},
  pages={823--832},
  year={1986},
  url = {https://cir.nii.ac.jp/crid/1572824499995923584},
}

@inproceedings{Javaloy2022Rotograd,
  author    = {Adri{\'{a}}n Javaloy and
               Isabel Valera},
  title     = {RotoGrad: Gradient Homogenization in Multitask Learning},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=T8wHz4rnuGL},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/JavaloyV22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Neyshabur2020WhatTransfered,
  author    = {Behnam Neyshabur and
               Hanie Sedghi and
               Chiyuan Zhang},
  title     = {What is being transferred in transfer learning?},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:00 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/NeyshaburSZ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fisher1922mathematical,
  title={On the mathematical foundations of theoretical statistics},
  author={Fisher, Ronald A},
  journal={Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character},
  volume={222},
  number={594-604},
  pages={309--368},
  year={1922},
  publisher={The Royal Society London}
}

@inproceedings{McMahan2017Communication,
  author    = {Brendan McMahan and
               Eider Moore and
               Daniel Ramage and
               Seth Hampson and
               Blaise Ag{\"{u}}era y Arcas},
  editor    = {Aarti Singh and
               Xiaojin (Jerry) Zhu},
  title     = {Communication-Efficient Learning of Deep Networks from Decentralized
               Data},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence
               and Statistics, {AISTATS} 2017, 20-22 April 2017, Fort Lauderdale,
               FL, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {54},
  pages     = {1273--1282},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v54/mcmahan17a.html},
  timestamp = {Wed, 29 May 2019 08:41:44 +0200},
  biburl    = {https://dblp.org/rec/conf/aistats/McMahanMRHA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhou2022MoEExpert,
  author    = {Yanqi Zhou and
               Tao Lei and
               Hanxiao Liu and
               Nan Du and
               Yanping Huang and
               Vincent Y. Zhao and
               Andrew M. Dai and
               Zhifeng Chen and
               Quoc Le and
               James Laudon},
  title     = {Mixture-of-Experts with Expert Choice Routing},
  journal   = {CoRR},
  volume    = {abs/2202.09368},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.09368},
  eprinttype = {arXiv},
  eprint    = {2202.09368},
  timestamp = {Thu, 03 Mar 2022 10:25:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2202-09368.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Lepikhin2021GShard,
  author    = {Dmitry Lepikhin and
               HyoukJoong Lee and
               Yuanzhong Xu and
               Dehao Chen and
               Orhan Firat and
               Yanping Huang and
               Maxim Krikun and
               Noam Shazeer and
               Zhifeng Chen},
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic
               Sharding},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=qrwe7XHTmYb},
  timestamp = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LepikhinLXCFHKS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Cases2019Recursive,
    title = "Recursive Routing Networks: Learning to Compose Modules for Language Understanding",
    author = "Cases, Ignacio  and
      Rosenbaum, Clemens  and
      Riemer, Matthew  and
      Geiger, Atticus  and
      Klinger, Tim  and
      Tamkin, Alex  and
      Li, Olivia  and
      Agarwal, Sandhini  and
      Greene, Joshua D.  and
      Jurafsky, Dan  and
      Potts, Christopher  and
      Karttunen, Lauri",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1365",
    doi = "10.18653/v1/N19-1365",
    pages = "3631--3648",
    abstract = "We introduce Recursive Routing Networks (RRNs), which are modular, adaptable models that learn effectively in diverse environments. RRNs consist of a set of functions, typically organized into a grid, and a meta-learner decision-making component called the router. The model jointly optimizes the parameters of the functions and the meta-learner{'}s policy for routing inputs through those functions. RRNs can be incorporated into existing architectures in a number of ways; we explore adding them to word representation layers, recurrent network hidden layers, and classifier layers. Our evaluation task is natural language inference (NLI). Using the MultiNLI corpus, we show that an RRN{'}s routing decisions reflect the high-level genre structure of that corpus. To show that RRNs can learn to specialize to more fine-grained semantic distinctions, we introduce a new corpus of NLI examples involving implicative predicates, and show that the model components become fine-tuned to the inferential signatures that are characteristic of these predicates.",
}


@article{Yuksel2012TwentyYears,
  author    = {Seniha Esen Y{\"{u}}ksel and
               Joseph N. Wilson and
               Paul D. Gader},
  title     = {Twenty Years of Mixture of Experts},
  journal   = {{IEEE} Trans. Neural Networks Learn. Syst.},
  volume    = {23},
  number    = {8},
  pages     = {1177--1193},
  year      = {2012},
  url       = {https://doi.org/10.1109/TNNLS.2012.2200299},
  doi       = {10.1109/TNNLS.2012.2200299},
  timestamp = {Fri, 29 Jan 2021 15:01:22 +0100},
  biburl    = {https://dblp.org/rec/journals/tnn/YukselWG12.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Chronopoulou2022EfficientHierarchical, 
    title = "Efficient Hierarchical Domain Adaptation for Pretrained Language Models",
    author = "Chronopoulou, Alexandra  and
      Peters, Matthew  and
      Dodge, Jesse",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.96",
    doi = "10.18653/v1/2022.naacl-main.96",
    pages = "1336--1351",
    abstract = "The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.",
}

@inproceedings{Jang2017Gumbel,
  author    = {Eric Jang and
               Shixiang Gu and
               Ben Poole},
  title     = {Categorical Reparameterization with Gumbel-Softmax},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=rkE3y85ee},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J.},
  journal={Machine Learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer},
url       = {https://doi.org/10.1007/BF00992696},
  doi       = {10.1007/BF00992696},
}

@article{willianms1988toward,
  title={Toward a theory of reinforcement-learning connectionist systems},
  author={Williams, Ronald J.},
  journal={Technical Report NU-CCS-88-3, Northeastern University},
  year={1988}
}


@article{ballard1986cortical,
  title={Cortical connections and parallel processing: Structure and function},
  author={Ballard, Dana H},
  journal={Behavioral and Brain Sciences},
  volume={9},
  number={1},
  pages={67--90},
  year={1986},
  publisher={Cambridge University Press},
  url = {https://psycnet.apa.org/doi/10.1017/S0140525X00021555},
}

@article{chai2016functional,
  title={Functional network dynamics of the language system},
  author={Chai, Lucy R and Mattar, Marcelo G and Blank, Idan Asher and Fedorenko, Evelina and Bassett, Danielle S},
  journal={Cerebral Cortex},
  volume={26},
  number={11},
  pages={4148--4159},
  year={2016},
  publisher={Oxford University Press}
}

@inproceedings{pfeiffer-etal-2020-mad,
    title = "{MAD-X}: {A}n {A}dapter-{B}ased {F}ramework for {M}ulti-{T}ask {C}ross-{L}ingual {T}ransfer",
    author = "Pfeiffer, Jonas  and
      Vuli{\'c}, Ivan  and
      Gurevych, Iryna  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.617",
    doi = "10.18653/v1/2020.emnlp-main.617",
    pages = "7654--7673",
    abstract = "The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.",
}

@book{fodor1983modularity,
  title={The modularity of Mind},
  author={Fodor, Jerry A.},
  year={1983},
  publisher={MIT Press},
  url = {https://mitpress.mit.edu/9780262560252/the-modularity-of-mind/},
}

@inproceedings{conneau-etal-2020-unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}



@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M.},
  journal={Trends in Cognitive Sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661399012942},
  doi = {https://doi.org/10.1016/S1364-6613(99)01294-2}
}

@inproceedings{das2018neural,
  author    = {Abhishek Das and
               Georgia Gkioxari and
               Stefan Lee and
               Devi Parikh and
               Dhruv Batra},
  title     = {Neural Modular Control for Embodied Question Answering},
  booktitle = {2nd Annual Conference on Robot Learning, CoRL 2018, Z{\"{u}}rich,
               Switzerland, 29-31 October 2018, Proceedings},
  series    = {Proceedings of Machine Learning Research},
  volume    = {87},
  pages     = {53--62},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v87/das18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:24 +0200},
  biburl    = {https://dblp.org/rec/conf/corl/DasGLPB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
shu2018hierarchical,
title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},
author={Tianmin Shu and Caiming Xiong and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SJJQVZW0b},
}

@inproceedings{vu-etal-2022-overcoming,
    title = "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation",
    author = "Vu, Tu  and
      Barua, Aditya  and
      Lester, Brian  and
      Cer, Daniel  and
      Iyyer, Mohit  and
      Constant, Noah",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.630",
    pages = "9279--9300",
    abstract = "In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.",
}

@inproceedings{
geffner2022deep,
title={Deep End-to-end Causal Inference},
author={Tomas Geffner and Javier Antoran and Adam Foster and Wenbo Gong and Chao Ma and Emre Kiciman and Amit Sharma and Angus Lamb and Martin Kukla and Nick Pawlowski and Miltiadis Allamanis and Cheng Zhang},
booktitle={NeurIPS 2022 Workshop on Causality for Real-world Impact},
year={2022},
url={https://openreview.net/forum?id=6DPVXzjnbDK}
}

@article{velivckovic2021neural,
  title={Neural algorithmic reasoning},
  author={Veli{\v{c}}kovi{\'c}, Petar and Blundell, Charles},
  journal={Patterns},
  volume={2},
  number={7},
  pages={100273},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{andreas2017modular,
  author    = {Jacob Andreas and
               Dan Klein and
               Sergey Levine},
  title     = {Modular Multitask Reinforcement Learning with Policy Sketches},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {166--175},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/andreas17a.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/AndreasKL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@phdthesis{precup2000temporal,
  title={Temporal Abstraction in Reinforcement Learning},
  author={Precup, Doina},
  year={2000},
  school={University of Massachusetts Amherst},
  url = {https://scholarworks.umass.edu/dissertations/AAI9978540},
}

@article{sutton1999between,
  author    = {Richard S. Sutton and
               Doina Precup and
               Satinder Singh},
  title     = {Between MDPs and Semi-MDPs: {A} Framework for Temporal Abstraction
               in Reinforcement Learning},
  journal   = {Artificial Intelligence},
  volume    = {112},
  number    = {1-2},
  pages     = {181--211},
  year      = {1999},
  url       = {https://doi.org/10.1016/S0004-3702(99)00052-1},
  doi       = {10.1016/S0004-3702(99)00052-1},
  timestamp = {Tue, 19 Apr 2022 16:03:28 +0200},
  biburl    = {https://dblp.org/rec/journals/ai/SuttonPS99.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li-etal-2022-quantifying,
    title = "Quantifying Adaptability in Pre-trained Language Models with 500 Tasks",
    author = "Li, Belinda  and
      Yu, Jane  and
      Khabsa, Madian  and
      Zettlemoyer, Luke  and
      Halevy, Alon  and
      Andreas, Jacob",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.346",
    doi = "10.18653/v1/2022.naacl-main.346",
    pages = "4696--4715",
    abstract = "When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.",
}

@inproceedings{Ha2017HyperNetworks,
  author    = {David Ha and
               Andrew M. Dai and
               Quoc V. Le},
  title     = {HyperNetworks},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=rkpACe1lx},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HaDL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lipson2007principles,
  title={Principles of modularity, regularity, and hierarchy for scalable systems},
  author={Lipson, Hod and others},
  journal={Journal of Biological Physics and Chemistry},
  volume={7},
  number={4},
  pages={125},
  year={2007},
  publisher={Citeseer}
}

@inproceedings{casper2022graphical,
  title={Graphical Clusterability and Local Specialization in Deep Neural Networks},
  author={Casper, Stephen and Hod, Shlomi and Filan, Daniel and Wild, Cody and Critch, Andrew and Russell, Stuart},
  booktitle={ICLR 2022 Workshop on PAIR$^2$Struct},
  year={2022},
  url={https://openreview.net/pdf?id=HreeeJvkue9},
}

@inproceedings{watanabe2019interpreting,
  author    = {Chihiro Watanabe},
  title     = {Interpreting Layered Neural Networks via Hierarchical Modular Representation},
  booktitle = {Neural Information Processing - 26th International Conference, {ICONIP}
               2019, Sydney, NSW, Australia, December 12-15, 2019, Proceedings, Part
               {V}},
  series    = {Communications in Computer and Information Science},
  volume    = {1143},
  pages     = {376--388},
  publisher = {Springer},
  year      = {2019},
  url       = {https://doi.org/10.1007/978-3-030-36802-9\_40},
  doi       = {10.1007/978-3-030-36802-9\_40},
  timestamp = {Thu, 05 Aug 2021 17:55:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iconip/Watanabe19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{artetxe2021efficient,
    title = "Efficient Large Scale Language Modeling with Mixtures of Experts",
    author = "Artetxe, Mikel  and
      Bhosale, Shruti  and
      Goyal, Naman  and
      Mihaylov, Todor  and
      Ott, Myle  and
      Shleifer, Sam  and
      Lin, Xi Victoria  and
      Du, Jingfei  and
      Iyer, Srinivasan  and
      Pasunuru, Ramakanth  and
      Anantharaman, Giridharan  and
      Li, Xian  and
      Chen, Shuohui  and
      Akin, Halil  and
      Baines, Mandeep  and
      Martin, Louis  and
      Zhou, Xing  and
      Koura, Punit Singh  and
      O{'}Horo, Brian  and
      Wang, Jeffrey  and
      Zettlemoyer, Luke  and
      Diab, Mona  and
      Kozareva, Zornitsa  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.804",
    pages = "11699--11732",
    abstract = "Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using {\textasciitilde}4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",
}

@inproceedings{Foroutan2022Discovering,
    title = "Discovering Language-neutral Sub-networks in Multilingual Language Models",
    author = "Foroutan, Negar  and
      Banaei, Mohammadreza  and
      Lebret, R{\'e}mi  and
      Bosselut, Antoine  and
      Aberer, Karl",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.513",
    pages = "7560--7575",
    abstract = "Multilingual pre-trained language models transfer remarkably well on cross-lingual downstream tasks. However, the extent to which they learn language-neutral representations (i.e., shared representations that encode similar phenomena across languages), and the effect of such representations on cross-lingual transfer performance, remain open questions.In this work, we conceptualize language neutrality of multilingual models as a function of the overlap between language-encoding sub-networks of these models. We employ the lottery ticket hypothesis to discover sub-networks that are individually optimized for various languages and tasks. Our evaluation across three distinct tasks and eleven typologically-diverse languages demonstrates that sub-networks for different languages are topologically similar (i.e., language-neutral), making them effective initializations for cross-lingual transfer with limited performance degradation.",
}

@inproceedings{Kim2020COGS,
  author    = {Najoung Kim and
               Tal Linzen},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {{COGS:} {A} Compositional Generalization Challenge Based on Semantic
               Interpretation},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {9087--9105},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.731},
  doi       = {10.18653/v1/2020.emnlp-main.731},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/KimL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gururangan-etal-2022-demix,
    title = "{DEM}ix Layers: Disentangling Domains for Modular Language Modeling",
    author = "Gururangan, Suchin  and
      Lewis, Mike  and
      Holtzman, Ari  and
      Smith, Noah A.  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.407",
    doi = "10.18653/v1/2022.naacl-main.407",
    pages = "5557--5576",
    abstract = "We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.",
}


@article{fedus2022review,
  author    = {William Fedus and
               Jeff Dean and
               Barret Zoph},
  title     = {A Review of Sparse Expert Models in Deep Learning},
  journal   = {CoRR},
  volume    = {abs/2209.01667},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2209.01667},
  doi       = {10.48550/arXiv.2209.01667},
  eprinttype = {arXiv},
  eprint    = {2209.01667},
  timestamp = {Mon, 26 Sep 2022 18:12:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2209-01667.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Liu2022IA3,
  author    = {Haokun Liu and
               Derek Tam and
               Mohammed Muqeeth and
               Jay Mohta and
               Tenghao Huang and
               Mohit Bansal and
               Colin Raffel},
  title     = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than
               In-Context Learning},
  journal   = {CoRR},
  volume    = {abs/2205.05638},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.05638},
  doi       = {10.48550/arXiv.2205.05638},
  eprinttype = {arXiv},
  eprint    = {2205.05638},
  timestamp = {Tue, 17 May 2022 17:31:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-05638.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Saxton2019AnalysingMath,
  author    = {David Saxton and
               Edward Grefenstette and
               Felix Hill and
               Pushmeet Kohli},
  title     = {Analysing Mathematical Reasoning Abilities of Neural Models},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=H1gR5iR5FX},
  timestamp = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SaxtonGHK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hupkes2018Visualisation,
  author    = {Dieuwke Hupkes and
               Sara Veldhoen and
               Willem H. Zuidema},
  title     = {Visualisation and 'Diagnostic Classifiers' Reveal How Recurrent and
               Recursive Neural Networks Process Hierarchical Structure},
  journal   = {J. Artif. Intell. Res.},
  volume    = {61},
  pages     = {907--926},
  year      = {2018},
  url       = {https://doi.org/10.1613/jair.1.11196},
  doi       = {10.1613/jair.1.11196},
  timestamp = {Mon, 26 Oct 2020 09:03:04 +0100},
  biburl    = {https://dblp.org/rec/journals/jair/HupkesVZ18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Veldhoen2016DiagnosticClassifiers,
  author    = {Sara Veldhoen and
               Dieuwke Hupkes and
               Willem H. Zuidema},
  editor    = {Tarek Richard Besold and
               Antoine Bordes and
               Artur S. d'Avila Garcez and
               Greg Wayne},
  title     = {Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical
               Structure},
  booktitle = {Proceedings of the Workshop on Cognitive Computation: Integrating
               neural and symbolic approaches 2016 co-located with the 30th Annual
               Conference on Neural Information Processing Systems {(NIPS} 2016),
               Barcelona, Spain, December 9, 2016},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {1773},
  publisher = {CEUR-WS.org},
  year      = {2016},
  url       = {http://ceur-ws.org/Vol-1773/CoCoNIPS\_2016\_paper6.pdf},
  timestamp = {Wed, 12 Feb 2020 16:44:20 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/VeldhoenHZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Johnson2017CLEVR,
  author    = {Justin Johnson and
               Bharath Hariharan and
               Laurens van der Maaten and
               Li Fei{-}Fei and
               C. Lawrence Zitnick and
               Ross B. Girshick},
  title     = {{CLEVR:} {A} Diagnostic Dataset for Compositional Language and Elementary
               Visual Reasoning},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {1988--1997},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/CVPR.2017.215},
  doi       = {10.1109/CVPR.2017.215},
  timestamp = {Sat, 30 May 2020 20:05:36 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/JohnsonHMFZG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Lancucki:2021fastpitch,
  author    = {Adrian Lancucki},
  title     = {Fastpitch: Parallel Text-to-Speech with Pitch Prediction},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages     = {6588--6592},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICASSP39728.2021.9413889},
  doi       = {10.1109/ICASSP39728.2021.9413889},
}



@inproceedings{Graves:2006ctc,
  author    = {Alex Graves and
               Santiago Fern{\'{a}}ndez and
               Faustino J. Gomez and
               J{\"{u}}rgen Schmidhuber},
  editor    = {William W. Cohen and
               Andrew W. Moore},
  title     = {Connectionist temporal classification: labelling unsegmented sequence
               data with recurrent neural networks},
  booktitle = {Machine Learning, Proceedings of the Twenty-Third International Conference
               {(ICML} 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006},
  series    = {{ACM} International Conference Proceeding Series},
  volume    = {148},
  pages     = {369--376},
  publisher = {{ACM}},
  year      = {2006},
  url       = {https://doi.org/10.1145/1143844.1143891},
  doi       = {10.1145/1143844.1143891},
  timestamp = {Tue, 19 Nov 2019 09:25:06 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/GravesFGS06.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{Gaur:2021speechmoe,
  author    = {Neeraj Gaur and
               Brian Farris and
               Parisa Haghani and
               Isabel Leal and
               Pedro J. Moreno and
               Manasa Prasad and
               Bhuvana Ramabhadran and
               Yun Zhu},
  title     = {Mixture of Informed Experts for Multilingual Speech Recognition},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
               {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages     = {6234--6238},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICASSP39728.2021.9414379},
}

@article{Kumatani:2021arxiv,
  author    = {Ken'ichi Kumatani and
               Robert Gmyr and
               Felipe Cruz Salinas and
               Linquan Liu and
               Wei Zuo and
               Devang Patel and
               Eric Sun and
               Yu Shi},
  title     = {Building a great multi-lingual teacher with sparsely-gated mixture
               of experts for speech recognition},
  journal   = {CoRR},
  volume    = {abs/2112.05820},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.05820},
}



@inproceedings{Babu:2022xlsr,
  author    = {Arun Babu and
               Changhan Wang and
               Andros Tjandra and
               Kushal Lakhotia and
               Qiantong Xu and
               Naman Goyal and
               Kritika Singh and
               Patrick von Platen and
               Yatharth Saraf and
               Juan Pino and
               Alexei Baevski and
               Alexis Conneau and
               Michael Auli},
  title     = {{XLS-R:} Self-supervised Cross-lingual Speech Representation Learning
               at Scale},
  booktitle = {Interspeech 2022, 23rd Annual Conference of the International Speech
               Communication Association, Incheon, Korea, 18-22 September 2022},
  pages     = {2278--2282},
  publisher = {{ISCA}},
  year      = {2022},
  url       = {https://doi.org/10.21437/Interspeech.2022-143},
  doi       = {10.21437/Interspeech.2022-143},
  timestamp = {Tue, 11 Oct 2022 19:11:50 +0200},
  biburl    = {https://dblp.org/rec/conf/interspeech/BabuWTLXGSPSPBC22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Yasunaga:2023arxiv,
  author       = {Michihiro Yasunaga and
                  Armen Aghajanyan and
                  Weijia Shi and
                  Richard James and
                  Jure Leskovec and
                  Percy Liang and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Wen{-}Tau Yih},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Retrieval-Augmented Multimodal Language Modeling},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {39755--39769},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/yasunaga23a.html},
}
@inproceedings{Lewis:2020rag,
  author       = {Patrick S. H. Lewis and
                  Ethan Perez and
                  Aleksandra Piktus and
                  Fabio Petroni and
                  Vladimir Karpukhin and
                  Naman Goyal and
                  Heinrich K{\"{u}}ttler and
                  Mike Lewis and
                  Wen{-}tau Yih and
                  Tim Rockt{\"{a}}schel and
                  Sebastian Riedel and
                  Douwe Kiela},
  title        = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
}

@inproceedings{yu-etal-2023-augmentation,
    title = "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
    author = "Yu, Zichun  and
      Xiong, Chenyan  and
      Yu, Shi  and
      Liu, Zhiyuan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.136",
    doi = "10.18653/v1/2023.acl-long.136",
    pages = "2421--2436",
}

@article{auglms:2023,
  author       = {Gr{\'{e}}goire Mialon and
                  Roberto Dess{\`{\i}} and
                  Maria Lomeli and
                  Christoforos Nalmpantis and
                  Ramakanth Pasunuru and
                  Roberta Raileanu and
                  Baptiste Rozi{\`{e}}re and
                  Timo Schick and
                  Jane Dwivedi{-}Yu and
                  Asli Celikyilmaz and
                  Edouard Grave and
                  Yann LeCun and
                  Thomas Scialom},
  title        = {Augmented Language Models: A Survey},
  journal      = {CoRR},
  volume       = {abs/2302.07842},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.07842},
  doi          = {10.48550/arXiv.2302.07842},
}

@inproceedings{DBLP:conf/nips/LewisPPPKGKLYR020,
  author       = {Patrick S. H. Lewis and
                  Ethan Perez and
                  Aleksandra Piktus and
                  Fabio Petroni and
                  Vladimir Karpukhin and
                  Naman Goyal and
                  Heinrich K{\"{u}}ttler and
                  Mike Lewis and
                  Wen{-}tau Yih and
                  Tim Rockt{\"{a}}schel and
                  Sebastian Riedel and
                  Douwe Kiela},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
  timestamp    = {Tue, 19 Jan 2021 15:57:07 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Baevski:2020wav2vec,
  author    = {Alexei Baevski and
               Yuhao Zhou and
               Abdelrahman Mohamed and
               Michael Auli},
  title     = {wav2vec 2.0: {A} Framework for Self-Supervised Learning of Speech
               Representations},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:22 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/BaevskiZMA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Morioka:2022tts,
  author    = {Nobuyuki Morioka and
               Heiga Zen and
               Nanxin Chen and
               Yu Zhang and
               Yifan Ding},
  title     = {Residual Adapters for Few-Shot Text-to-Speech Speaker Adaptation},
  journal   = {CoRR},
  volume    = {abs/2210.15868},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.15868},
}

@article{Hsieh:2022tts,
  author    = {Cheng{-}Ping Hsieh and
               Subhankar Ghosh and
               Boris Ginsburg},
  title     = {Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for
               New Speakers},
  journal   = {CoRR},
  volume    = {abs/2211.00585},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.00585},
  doi       = {10.48550/arXiv.2211.00585},
}

@article{Hou:2022speech,
  author    = {Wenxin Hou and
               Han Zhu and
               Yidong Wang and
               Jindong Wang and
               Tao Qin and
               Renjun Xu and
               Takahiro Shinozaki},
  title     = {Exploiting Adapters for Cross-Lingual Low-Resource Speech Recognition},
  journal   = {{IEEE} {ACM} Transactions on Audio Speech Language Processing},
  volume    = {30},
  pages     = {317--329},
  year      = {2022},
  url       = {https://doi.org/10.1109/TASLP.2021.3138674},
  doi       = {10.1109/TASLP.2021.3138674},
}

@article{Sun:2022bbtv2,
  author    = {Tianxiang Sun and
               Zhengfu He and
               Hong Qian and
               Xuanjing Huang and
               Xipeng Qiu},
  title     = {{BBTv2:} Pure Black-Box Optimization Can Be Comparable to Gradient Descent
               for Few-Shot Learning},
  journal   = {CoRR},
  volume    = {abs/2205.11200},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.11200},
}

@inproceedings{Mahabadi2021Compacter,
  author    = {Rabeeh Karimi Mahabadi and
               James Henderson and
               Sebastian Ruder},
  title     = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {1022--1035},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/081be9fdff07f3bc808f935906ef70c0-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:46 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/MahabadiHR21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ainsworth2022GitReBasin,
  author    = {Samuel K. Ainsworth and
               Jonathan Hayase and
               Siddhartha S. Srinivasa},
  title     = {Git Re-Basin: Merging Models modulo Permutation Symmetries},
  journal   = {CoRR},
  volume    = {abs/2209.04836},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2209.04836},
  doi       = {10.48550/arXiv.2209.04836},
  eprinttype = {arXiv},
  eprint    = {2209.04836},
  timestamp = {Tue, 27 Sep 2022 16:29:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2209-04836.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Wortsman2022ModelSoups,
  author    = {Mitchell Wortsman and
               Gabriel Ilharco and
               Samir Ya Gadre and
               Rebecca Roelofs and
               Raphael Gontijo Lopes and
               Ari S. Morcos and
               Hongseok Namkoong and
               Ali Farhadi and
               Yair Carmon and
               Simon Kornblith and
               Ludwig Schmidt},
  editor    = {Kamalika Chaudhuri and
               Stefanie Jegelka and
               Le Song and
               Csaba Szepesv{\'{a}}ri and
               Gang Niu and
               Sivan Sabato},
  title     = {Model soups: averaging weights of multiple fine-tuned models improves
               accuracy without increasing inference time},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {23965--23998},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/wortsman22a.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/WortsmanIGRLMNF22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Matena2021MergingModels,
  author    = {Michael Matena and
               Colin Raffel},
  title     = {Merging Models with Fisher-Weighted Averaging},
  journal   = {CoRR},
  volume    = {abs/2111.09832},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09832},
  eprinttype = {arXiv},
  eprint    = {2111.09832},
  timestamp = {Mon, 22 Nov 2021 16:44:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09832.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ilharco2022Patching,
  author    = {Gabriel Ilharco and
               Mitchell Wortsman and
               Samir Yitzhak Gadre and
               Shuran Song and
               Hannaneh Hajishirzi and
               Simon Kornblith and
               Ali Farhadi and
               Ludwig Schmidt},
  title     = {Patching open-vocabulary models by interpolating weights},
  journal   = {CoRR},
  volume    = {abs/2208.05592},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2208.05592},
  doi       = {10.48550/arXiv.2208.05592},
  eprinttype = {arXiv},
  eprint    = {2208.05592},
  timestamp = {Tue, 16 Aug 2022 16:44:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2208-05592.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ilharco2022EditingModelsTaskArith,
  author    = {Gabriel Ilharco and
               Marco T{\'{u}}lio Ribeiro and
               Mitchell Wortsman and
               Suchin Gururangan and
               Ludwig Schmidt and
               Hannaneh Hajishirzi and
               Ali Farhadi},
  title     = {Editing Models with Task Arithmetic},
  journal   = {CoRR},
  volume    = {abs/2212.04089},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.04089},
  doi       = {10.48550/arXiv.2212.04089},
  eprinttype = {arXiv},
  eprint    = {2212.04089},
  timestamp = {Mon, 02 Jan 2023 15:09:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-04089.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nayak2022learning,
  title={Learning to compose soft prompts for compositional zero-shot learning},
  author={Nayak, Nihal V and Yu, Peilin and Bach, Stephen H},
  journal={arXiv preprint arXiv:2204.03574},
  year={2022},
  url={https://arxiv.org/pdf/2204.03574.pdf},
}

@inproceedings{asai-etal-2022-attempt,
    title = "{ATTEMPT}: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts",
    author = "Asai, Akari  and
      Salehi, Mohammadreza  and
      Peters, Matthew  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.446",
    pages = "6655--6672",
    abstract = "This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts{---}small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt Tuning), obtains source prompts as encodings of large-scale source tasks into a small number of parameters and trains an attention module to interpolate the source prompts and a newly initialized target prompt for every instance in the target task. During training, only the target task prompt and the attention weights, which are shared between tasks in multi-task training, are updated, while the original LM and source prompts are intact. ATTEMPT is highly parameter-efficient (e.g., updates 2,300 times fewer parameters than full fine-tuning), while it overcomes instability of prompt tuning and achieves high task performance using learned knowledge from high-resource tasks. Moreover, it is modular using pre-trained soft prompts, and can flexibly add or remove source prompts for effective knowledge transfer. Our experimental results across 21 diverse NLP datasets show that ATTEMPT significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter-efficient tuning approaches that use 10 times more parameters. Finally, ATTEMPT outperforms previous work in few-shot learning settings.",
}

@inproceedings{Li2020PrefixTuning,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@inproceedings{Mikolov2013DistributedRepresentations,
  author    = {Tom{\'{a}}s Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Gregory S. Corrado and
               Jeffrey Dean},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual
               Conference on Neural Information Processing Systems 2013. Proceedings
               of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  pages     = {3111--3119},
  year      = {2013},
  url       = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/MikolovSCCD13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{He2021UnifiedAdapters,
  author    = {Junxian He and
               Chunting Zhou and
               Xuezhe Ma and
               Taylor Berg{-}Kirkpatrick and
               Graham Neubig},
  title     = {Towards a Unified View of Parameter-Efficient Transfer Learning},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=0RDcd5Axok},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HeZMBN22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Liu:2021survey,
  author    = {Pengfei Liu and
               Weizhe Yuan and
               Jinlan Fu and
               Zhengbao Jiang and
               Hiroaki Hayashi and
               Graham Neubig},
  title     = {Pre-train, Prompt, and Predict: {A} Systematic Survey of Prompting
               Methods in Natural Language Processing},
  journal   = {{ACM} Computing Surveys},
  volume    = {55},
  number    = {9},
  pages     = {195:1--195:35},
  year      = {2023},
  url       = {https://doi.org/10.1145/3560815},
  doi       = {10.1145/3560815},
  timestamp = {Wed, 15 Feb 2023 17:36:21 +0100},
  biburl    = {https://dblp.org/rec/journals/csur/LiuYFJHN23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{meyes2020under,
  author    = {Richard Meyes and
               Constantin Waubert de Puiseau and
               Andres Posada{-}Moreno and
               Tobias Meisen},
  title     = {Under the Hood of Neural Networks: Characterizing Learned Representations
               by Functional Neuron Populations and Network Ablations},
  journal   = {CoRR},
  volume    = {abs/2004.01254},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.01254},
  eprinttype = {arXiv},
  eprint    = {2004.01254},
  timestamp = {Wed, 08 Apr 2020 17:08:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-01254.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{
zhang2023emergent,
title={Emergent Modularity in Pre-trained Transformers},
author={Zhengyan Zhang and Zhiyuan Zeng and Yankai Lin and Chaojun Xiao and Xu Han and Zhiyuan Liu and Maosong Sun and Jie Zhou},
year={2022},
url={https://openreview.net/forum?id=XHuQacT6sa6}
}

@inproceedings{Rebuffi2018Adapters2,
  author    = {Sylvestre{-}Alvise Rebuffi and
               Hakan Bilen and
               Andrea Vedaldi},
  title     = {Efficient Parametrization of Multi-Domain Deep Neural Networks},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {8119--8127},
  publisher = {Computer Vision Foundation / {IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Rebuffi\_Efficient\_Parametrization\_of\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00847},
  timestamp = {Tue, 31 Aug 2021 14:00:32 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/RebuffiBV18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Yang2016Deep, 
  author    = {Yongxin Yang and
               Timothy M. Hospedales},
  title     = {Deep Multi-task Representation Learning: {A} Tensor Factorisation
               Approach},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=SkhU2fcll},
  timestamp = {Thu, 25 Jul 2019 14:25:52 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/YangH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Meyerson2017Beyond, 
  author    = {Elliot Meyerson and
               Risto Miikkulainen},
  title     = {Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer
               Ordering},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=BkXmYfbAZ},
  timestamp = {Thu, 25 Jul 2019 14:26:05 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MeyersonM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Bertinetto2016Learning, 
  author    = {Luca Bertinetto and
               Jo{\~{a}}o F. Henriques and
               Jack Valmadre and
               Philip H. S. Torr and
               Andrea Vedaldi},
  title     = {Learning feed-forward one-shot learners},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
               on Neural Information Processing Systems 2016, December 5-10, 2016,
               Barcelona, Spain},
  pages     = {523--531},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/hash/839ab46820b524afda05122893c2fe8e-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/BertinettoHVTV16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{He2016ResNet,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Identity Mappings in Deep Residual Networks},
  booktitle = {Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam,
               The Netherlands, October 11-14, 2016, Proceedings, Part {IV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {9908},
  pages     = {630--645},
  publisher = {Springer},
  year      = {2016},
  url       = {https://doi.org/10.1007/978-3-319-46493-0\_38},
  doi       = {10.1007/978-3-319-46493-0\_38},
  timestamp = {Wed, 25 Jan 2023 11:01:16 +0100},
  biburl    = {https://dblp.org/rec/conf/eccv/HeZRS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bilen2017Universal,
  author    = {Hakan Bilen and
               Andrea Vedaldi},
  title     = {Universal representations: The missing link between faces, text, planktons,
               and cat breeds},
  journal   = {CoRR},
  volume    = {abs/1701.07275},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.07275},
  eprinttype = {arXiv},
  eprint    = {1701.07275},
  timestamp = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BilenV17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Bapna2019Adapters,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1165",
    doi = "10.18653/v1/D19-1165",
    pages = "1538--1548",
    abstract = "Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",
}

@inproceedings{Rebuffi2017Adapters1,
  author    = {Sylvestre{-}Alvise Rebuffi and
               Hakan Bilen and
               Andrea Vedaldi},
  title     = {Learning multiple visual domains with residual adapters},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {506--516},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/RebuffiBV17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Roller2021Hash,
  author    = {Stephen Roller and
               Sainbayar Sukhbaatar and
               Arthur Szlam and
               Jason Weston},
  title     = {Hash Layers For Large Sparse Models},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {17555--17566},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:48 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/RollerSSW21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{Gesmundo2022Multiagent,
  author    = {Andrea Gesmundo},
  title     = {A Multi-Agent Framework for the Asynchronous and Collaborative Extension
               of Multitask {ML} Systems},
  journal   = {CoRR},
  volume    = {abs/2209.14745},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2209.14745},
  doi       = {10.48550/arXiv.2209.14745},
  eprinttype = {arXiv},
  eprint    = {2209.14745},
  timestamp = {Thu, 06 Oct 2022 14:41:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2209-14745.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Gesmundo2022ContinualDev,
  author    = {Andrea Gesmundo},
  title     = {A Continual Development Methodology for Large-scale Multitask Dynamic
               {ML} Systems},
  journal   = {CoRR},
  volume    = {abs/2209.07326},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2209.07326},
  doi       = {10.48550/arXiv.2209.07326},
  eprinttype = {arXiv},
  eprint    = {2209.07326},
  timestamp = {Tue, 27 Sep 2022 16:29:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2209-07326.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Gesmundo2022Evolutionary,
  author    = {Andrea Gesmundo and
               Jeff Dean},
  title     = {An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale
               Multitask Learning Systems},
  journal   = {CoRR},
  volume    = {abs/2205.12755},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.12755},
  doi       = {10.48550/arXiv.2205.12755},
  eprinttype = {arXiv},
  eprint    = {2205.12755},
  timestamp = {Mon, 30 May 2022 15:47:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-12755.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Gesmundo2022munet,
  author    = {Andrea Gesmundo and
               Jeff Dean},
  title     = {muNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-tuning
               Multitask Systems},
  journal   = {CoRR},
  volume    = {abs/2205.10937},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.10937},
  doi       = {10.48550/arXiv.2205.10937},
  eprinttype = {arXiv},
  eprint    = {2205.10937},
  timestamp = {Mon, 30 May 2022 15:47:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-10937.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Schick2021ItsNotJustSize,
    title = "It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.185",
    doi = "10.18653/v1/2021.naacl-main.185",
    pages = "2339--2352",
    abstract = "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much {``}greener{''} in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",
}

@inproceedings{Schick2021Exploiting,
    title = "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.20",
    doi = "10.18653/v1/2021.eacl-main.20",
    pages = "255--269",
    abstract = "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with {``}task descriptions{''} in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.",
}

@article{jacobs1991task,
  author    = {Robert A. Jacobs and
               Michael I. Jordan and
               Andrew G. Barto},
  title     = {Task Decomposition Through Competition in a Modular Connectionist
               Architecture: The What and Where Vision Tasks},
  journal   = {Cognitive Science},
  volume    = {15},
  number    = {2},
  pages     = {219--250},
  year      = {1991},
  url       = {https://doi.org/10.1207/s15516709cog1502\_2},
  doi       = {10.1207/s15516709cog1502\_2},
  timestamp = {Thu, 04 Jun 2020 19:37:04 +0200},
  biburl    = {https://dblp.org/rec/journals/cogsci/JacobsJB91.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Li2022BranchTrainMerge,
  author    = {Margaret Li and
               Suchin Gururangan and
               Tim Dettmers and
               Mike Lewis and
               Tim Althoff and
               Noah A. Smith and
               Luke Zettlemoyer},
  title     = {Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language
               Models},
  journal   = {CoRR},
  volume    = {abs/2208.03306},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2208.03306},
  doi       = {10.48550/arXiv.2208.03306},
  eprinttype = {arXiv},
  eprint    = {2208.03306},
  timestamp = {Wed, 10 Aug 2022 14:49:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2208-03306.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{jacobs1991adaptive,
  author    = {Robert A. Jacobs and
               Michael I. Jordan and
               Steven J. Nowlan and
               Geoffrey E. Hinton},
  title     = {Adaptive Mixtures of Local Experts},
  journal   = {Neural Computation},
  volume    = {3},
  number    = {1},
  pages     = {79--87},
  year      = {1991},
  url       = {https://doi.org/10.1162/neco.1991.3.1.79},
  doi       = {10.1162/neco.1991.3.1.79},
  timestamp = {Tue, 01 Sep 2020 13:12:47 +0200},
  biburl    = {https://dblp.org/rec/journals/neco/JacobsJNH91.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kirsch2018modular,
  author    = {Louis Kirsch and
               Julius Kunze and
               David Barber},
  title     = {Modular Networks: Learning to Decompose Neural Computation},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
               on Neural Information Processing Systems 2018, NeurIPS 2018, December
               3-8, 2018, Montr{\'{e}}al, Canada},
  pages     = {2414--2423},
  year      = {2018},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/310ce61c90f3a46e340ee8257bc70e93-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/KirschKB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{andreas2016nmn,
  author    = {Jacob Andreas and
               Marcus Rohrbach and
               Trevor Darrell and
               Dan Klein},
  title     = {Neural Module Networks},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {39--48},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.12},
  doi       = {10.1109/CVPR.2016.12},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/AndreasRDK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{alet2018modular,
  author    = {Ferran Alet and
               Tom{\'{a}}s Lozano{-}P{\'{e}}rez and
               Leslie Pack Kaelbling},
  title     = {Modular meta-learning},
  booktitle = {2nd Annual Conference on Robot Learning, CoRL 2018, Z{\"{u}}rich,
               Switzerland, 29-31 October 2018, Proceedings},
  series    = {Proceedings of Machine Learning Research},
  volume    = {87},
  pages     = {856--868},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v87/alet18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:24 +0200},
  biburl    = {https://dblp.org/rec/conf/corl/AletLK18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{shazeer2017outrageously,
  author    = {Noam Shazeer and
               Azalia Mirhoseini and
               Krzysztof Maziarz and
               Andy Davis and
               Quoc V. Le and
               Geoffrey E. Hinton and
               Jeff Dean},
  title     = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
               Layer},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=B1ckMDqlg},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ShazeerMMDLHD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{griffiths2011indian,
  title={The Indian Buffet Process: An Introduction and Review.},
  author={Griffiths, Thomas L and Ghahramani, Zoubin},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={4},
  year={2011},
  url={https://www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf},
}

@inproceedings{parascandolo2018learning,
  author    = {Giambattista Parascandolo and
               Niki Kilbertus and
               Mateo Rojas{-}Carulla and
               Bernhard Sch{\"{o}}lkopf},
  title     = {Learning Independent Causal Mechanisms},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {4033--4041},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/parascandolo18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ParascandoloKRS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kudugunta2021beyond,
    title = "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference",
    author = "Kudugunta, Sneha  and
      Huang, Yanping  and
      Bapna, Ankur  and
      Krikun, Maxim  and
      Lepikhin, Dmitry  and
      Luong, Minh-Thang  and
      Firat, Orhan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    year = "2021",
    url = "https://aclanthology.org/2021.findings-emnlp.304",
    pages = "3577--3599",
    abstract = "Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling multilingual translation models to billions of parameters without a proportional increase in training computation. However, MoE models are prohibitively large and practitioners often resort to methods such as distillation for serving. In this work, we investigate routing strategies at different granularity (token, sentence, task) in MoE models to bypass distillation. Experiments on WMT and a web-scale dataset suggest that task-level routing (task-MoE) enables us to extract smaller, ready-to-deploy sub-networks from large sparse models. On WMT, our task-MoE with 32 experts (533M parameters) outperforms the best performing token-level MoE model (token-MoE) by +1.0 BLEU on average across 30 language pairs. The peak inference throughput is also improved by a factor of 1.9x when we route by tasks instead of tokens. While distilling a token-MoE to a smaller dense model preserves only 32{\%} of the BLEU gains, our sub-network task-MoE, by design, preserves all the gains with the same inference cost as the distilled student model. Finally, when scaling up to 200 language pairs, our 128-expert task-MoE (13B parameters) performs competitively with a token-level counterpart, while improving the peak inference throughput by a factor of 2.6x.",
}

@inproceedings{liu2019multi,
    title = "Multi-Task Deep Neural Networks for Natural Language Understanding",
    author = "Liu, Xiaodong  and
      He, Pengcheng  and
      Chen, Weizhu  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1441",
    doi = "10.18653/v1/P19-1441",
    pages = "4487--4496",
    abstract = "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7{\%} (2.2{\%} absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",
}

@article{Liu:2019roberta,
author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
title = {{RoBERTa: A} Robustly Optimized {BERT} Pretraining Approach},
journal = {CoRR},
volume = {abs/1907.11692},
url = {https://arxiv.org/abs/1907.11692},
year = {2019},
}

@inproceedings{Devlin:2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{sanh2022multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author    = {Victor Sanh and
               Albert Webson and
               Colin Raffel and
               Stephen H. Bach and
               Lintang Sutawika and
               Zaid Alyafeai and
               Antoine Chaffin and
               Arnaud Stiegler and
               Teven Le Scao and
               Arun Raja and
               Manan Dey and
               M. Saiful Bari and
               Canwen Xu and
               Urmish Thakker and
               Shanya Sharma and
               Eliza Szczechla and
               Taewoon Kim and
               Gunjan Chhablani and
               Nihal V. Nayak and
               Debajyoti Datta and
               Jonathan Chang and
               Mike Tian{-}Jian Jiang and
               Han Wang and
               Matteo Manica and
               Sheng Shen and
               Zheng Xin Yong and
               Harshit Pandey and
               Rachel Bawden and
               Thomas Wang and
               Trishala Neeraj and
               Jos Rozen and
               Abheesht Sharma and
               Andrea Santilli and
               Thibault F{\'{e}}vry and
               Jason Alan Fries and
               Ryan Teehan and
               Stella Biderman and
               Leo Gao and
               Tali Bers and
               Thomas Wolf and
               Alexander M. Rush},
  booktitle={The Tenth International Conference on Learning Representations},
  year={2022},
  url={https://arxiv.org/pdf/2110.08207.pdf},
}

@inproceedings{Ustun:2022hyperx,
    title = "Hyper-{X}: A Unified Hypernetwork for Multi-Task Multilingual Transfer",
    author = {{\"U}st{\"u}n, Ahmet  and
      Bisazza, Arianna  and
      Bouma, Gosse  and
      van Noord, Gertjan  and
      Ruder, Sebastian},
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.541",
    pages = "7934--7949",
    abstract = "Massively multilingual models are promising for transfer learning across tasks and languages. However, existing methods are unable to fully leverage training data when it is available in different task-language combinations. To exploit such heterogeneous supervision, we propose Hyper-X, a single hypernetwork that unifies multi-task and multilingual learning with efficient adaptation. It generates weights for adapter modules conditioned on both tasks and language embeddings. By learning to combine task and language-specific knowledge, our model enables zero-shot transfer for unseen languages and task-language combinations. Our experiments on a diverse set of languages demonstrate that Hyper-X achieves the best or competitive gain when a mixture of multiple resources is available, while on par with strong baseline in the standard scenario. Hyper-X is also considerably more efficient in terms of parameters and resources compared to methods that train separate adapters. Finally, Hyper-X consistently produces strong results in few-shot scenarios for new languages, showing the versatility of our approach beyond zero-shot transfer.",
}


@inproceedings{Faisal:2022arxiv,
    title = "Phylogeny-Inspired Adaptation of Multilingual Models to New Languages",
    author = "Faisal, Fahim  and
      Anastasopoulos, Antonios",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.34",
    pages = "434--452",
}

@inproceedings{mahabadi2021parameter,
    title = "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks",
    author = "Mahabadi, Rabeeh Karimi  and
      Ruder, Sebastian  and
      Dehghani, Mostafa  and
      Henderson, James",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.47",
    doi = "10.18653/v1/2021.acl-long.47",
    pages = "565--576",
    abstract = "State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29{\%} parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/rabeehk/hyperformer.",
}

@inproceedings{pfeiffer2020adapterfusion,
    title = "{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning",
    author = {Pfeiffer, Jonas  and
      Kamath, Aishwarya  and
      R{\"u}ckl{\'e}, Andreas  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.39",
    doi = "10.18653/v1/2021.eacl-main.39",
    pages = "487--503",
    abstract = "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.",
}

@inproceedings{
pilault2021conditionally,
  author    = {Jonathan Pilault and
               Amine Elhattami and
               Christopher J. Pal},
  title     = {Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning
               in {NLP} Using Fewer Parameters {\&} Less Data},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=de11dbHzAMF},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/PilaultEP21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ponti2022combining,
  author    = {Edoardo M. Ponti and
               Alessandro Sordoni and
               Siva Reddy},
  title     = {Combining Modular Skills in Multitask Learning},
  journal   = {CoRR},
  volume    = {abs/2202.13914},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.13914},
  eprinttype = {arXiv},
  eprint    = {2202.13914},
  timestamp = {Thu, 03 Mar 2022 10:36:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2202-13914.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Frankle2020LinearModeConnect,
  author    = {Jonathan Frankle and
               Gintare Karolina Dziugaite and
               Daniel M. Roy and
               Michael Carbin},
  title     = {Linear Mode Connectivity and the Lottery Ticket Hypothesis},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {3259--3269},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/frankle20a.html},
  timestamp = {Wed, 10 Feb 2021 22:02:21 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/FrankleD0C20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Freeman2017Topology,
  author    = {C. Daniel Freeman and
               Joan Bruna},
  title     = {Topology and Geometry of Half-Rectified Network Optimization},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Bk0FWVcgx},
  timestamp = {Thu, 25 Jul 2019 14:26:03 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/FreemanB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Nagarajan2019UniformConvergence,
  author    = {Vaishnavh Nagarajan and
               J. Zico Kolter},
  title     = {Uniform convergence may be unable to explain generalization in deep
               learning},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {11611--11622},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/05e97c207235d63ceb1db43c60db7bbb-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/NagarajanK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Garipov2018LossSurface,
  author    = {Timur Garipov and
               Pavel Izmailov and
               Dmitrii Podoprikhin and
               Dmitry P. Vetrov and
               Andrew Gordon Wilson},
  title     = {Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
               on Neural Information Processing Systems 2018, NeurIPS 2018, December
               3-8, 2018, Montr{\'{e}}al, Canada},
  pages     = {8803--8812},
  year      = {2018},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/be3087e74e9100d4bc4c6268cdbe8456-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/GaripovIPVW18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v80-draxler18a,
  author    = {Felix Draxler and
               Kambis Veschgini and
               Manfred Salmhofer and
               Fred A. Hamprecht},
  title     = {Essentially No Barriers in Neural Network Energy Landscape},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {1308--1317},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/draxler18a.html},
  timestamp = {Wed, 29 Apr 2020 09:23:37 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/DraxlerVSH18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Schick2021SelfDiagnosis,
  author    = {Timo Schick and
               Sahana Udupa and
               Hinrich Sch{\"{u}}tze},
  title     = {Self-Diagnosis and Self-Debiasing: {A} Proposal for Reducing Corpus-Based
               Bias in {NLP}},
  journal   = {Trans. Assoc. Comput. Linguistics},
  volume    = {9},
  pages     = {1408--1424},
  year      = {2021},
  url       = {https://doi.org/10.1162/tacl\_a\_00434},
  doi       = {10.1162/tacl\_a\_00434},
  timestamp = {Fri, 02 Sep 2022 10:50:08 +0200},
  biburl    = {https://dblp.org/rec/journals/tacl/SchickUS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Kornblith2019SimilairtyofNN,
  author    = {Simon Kornblith and
               Mohammad Norouzi and
               Honglak Lee and
               Geoffrey E. Hinton},
  title     = {Similarity of Neural Network Representations Revisited},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {3519--3529},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/kornblith19a.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Kornblith0LH19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ansell2021composable,
    title = "Composable Sparse Fine-Tuning for Cross-Lingual Transfer",
    author = "Ansell, Alan  and
      Ponti, Edoardo  and
      Korhonen, Anna  and
      Vuli{\'c}, Ivan",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.125",
    doi = "10.18653/v1/2022.acl-long.125",
    pages = "1778--1796",
    abstract = "Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pretrained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.",
}

@inproceedings{houlsby2019parameter,
 author = {Neil Houlsby and
Andrei Giurgiu and
Stanislaw Jastrzebski and
Bruna Morrone and
Quentin de Laroussilhe and
Andrea Gesmundo and
Mona Attariyan and
Sylvain Gelly},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/HoulsbyGJMLGAG19.bib},
 booktitle = {Proceedings of the 36th International Conference on Machine Learning,
{ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
 pages = {2790--2799},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 11 Jun 2019 01:00:00 +0200},
 title = {Parameter-Efficient Transfer Learning for {NLP}},
 url = {http://proceedings.mlr.press/v97/houlsby19a.html},
 volume = {97},
 year = {2019}
}


@inproceedings{rusu2019meta,
  title={Meta-Learning with Latent Embedding Optimization},
  author={Rusu, Andrei A and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/references/pdf?id=Sy8k56P_N},
}

@inproceedings{bugliarello2022iglue,
  author    = {Emanuele Bugliarello and
               Fangyu Liu and
               Jonas Pfeiffer and
               Siva Reddy and
               Desmond Elliott and
               Edoardo Maria Ponti and
               Ivan Vuli\'{c}},
  title     = {{IGLUE:} {A} Benchmark for Transfer Learning across Modalities, Tasks,
               and Languages},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {2370--2392},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/bugliarello22a.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Bugliarello0PRE22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ponti2019modeling,
  title={Modeling language variation and universals: A survey on typological linguistics for natural language processing},
  author={Ponti, Edoardo Maria and O'Horan, Helen and Berzak, Yevgeni and Vuli{\'c}, Ivan and Reichart, Roi and Poibeau, Thierry and Shutova, Ekaterina and Korhonen, Anna},
  journal={Computational Linguistics},
  volume={45},
  number={3},
  pages={559--601},
  year={2019},
  publisher={MIT Press},
  url={https://watermark.silverchair.com/coli_a_00357.pdf},
}

@article{aribandi2021ext5,
  title={{ExT5}: Towards Extreme Multi-Task Scaling for Transfer Learning},
  author={Vamsi Aribandi and Yi Tay and Tal Schuster and Jinfeng Rao and Huaixiu Steven Zheng and Sanket Vaibhav Mehta and Honglei Zhuang and Vinh Q. Tran and Dara Bahri and Jianmo Ni and Jai Gupta and Kai Hui and Sebastian Ruder and Donald Metzler},
  journal={arXiv preprint arXiv:2111.10952},
  year={2021},
  url={https://arxiv.org/pdf/2111.10952.pdf},
}

@inproceedings{wei2021finetuned,
  author    = {Jason Wei and
               Maarten Bosma and
               Vincent Y. Zhao and
               Kelvin Guu and
               Adams Wei Yu and
               Brian Lester and
               Nan Du and
               Andrew M. Dai and
               Quoc V. Le},
  title     = {Finetuned Language Models are Zero-Shot Learners},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=gEZrGCozdqR},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/WeiBZGYLDDL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{min2021metaicl,
  title={Meta{ICL}: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2110.15943},
  year={2021},
  url={https://arxiv.org/pdf/2110.15943.pdf},
}

@inproceedings{misra2016cross,
author    = {Ishan Misra and
               Abhinav Shrivastava and
               Abhinav Gupta and
               Martial Hebert},
  title     = {Cross-Stitch Networks for Multi-task Learning},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {3994--4003},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.433},
  doi       = {10.1109/CVPR.2016.433},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/MisraSGH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{mccloskey1989catastrophicinterference,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J.},
  booktitle={Psychology of Learning and Motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier},
  url = {https://doi.org/10.1016/S0079-7421(08)60536-8},
  doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
}



@article{kingetsu2021neural,
      title={Neural Network Module Decomposition and Recomposition}, 
      author={Hiroaki Kingetsu and Kenichi Kobayashi and Taiji Suzuki},
      year={2021},
  journal={arXiv preprint arXiv:2112.13208},
      url={https://arxiv.org/pdf/2112.13208.pdf},
}

@phdthesis{ponti2021inductive,
  title={Inductive Bias and Modular Design for Sample-Efficient Neural Language Learning},
  author={Ponti, Edoardo M.},
  year={2021},
  school={University of Cambridge},
  doi = {https://doi.org/10.17863/CAM.66424},
  url = {https://doi.org/10.17863/CAM.66424},
}

@techreport{von1945first,
  title={First Draft of a Report on the {EDVAC}},
  author={von Neumann, John},
  year={1945},
  url       = {https://doi.org/10.1109/85.238389},
  doi       = {10.1109/85.238389},
  institution={University of Pennsylvania},
}

@article{sontag1995computational,
  author    = {Hava T. Siegelmann and
               Eduardo D. Sontag},
  title     = {On the Computational Power of Neural Nets},
  journal   = {Journal of Computer and System Sciences},
  volume    = {50},
  number    = {1},
  pages     = {132--150},
  year      = {1995},
  url       = {https://doi.org/10.1006/jcss.1995.1013},
  doi       = {10.1006/jcss.1995.1013},
  timestamp = {Tue, 16 Feb 2021 14:04:34 +0100},
  biburl    = {https://dblp.org/rec/journals/jcss/SiegelmannS95.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ponti-etal-2021-parameter,
    title = "Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages",
    author = "Ponti, Edoardo M.  and
      Vuli{\'c}, Ivan  and
      Cotterell, Ryan  and
      Parovic, Marinela  and
      Reichart, Roi  and
      Korhonen, Anna",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.25",
    doi = "10.1162/tacl_a_00374",
    pages = "410--428",
    abstract = "Abstract Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task{--}language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task{--}language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods. Our code is available at github.com/cambridgeltl/parameter-factorization.",
}

@article{eichenberg2021magma,
  title={MAGMA--Multimodal Augmentation of Generative Models through Adapter-based Finetuning},
  author={Eichenberg, Constantin and Black, Sidney and Weinbach, Samuel and Parcalabescu, Letitia and Frank, Anette},
  journal={arXiv preprint arXiv:2112.05253},
  year={2021}
}

@inproceedings{rajendran2017adaapt,
  author    = {Janarthanan Rajendran and
               Aravind S. Lakshminarayanan and
               Mitesh M. Khapra and
               P. Prasanna and
               Balaraman Ravindran},
  title     = {Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive
               Transfer from multiple sources in the same domain},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Sy6iJDqlx},
  timestamp = {Thu, 25 Jul 2019 14:26:02 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/RajendranLKPR17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fedus2021switch,
  author    = {William Fedus and
               Barret Zoph and
               Noam Shazeer},
  title     = {Switch Transformers: Scaling to Trillion Parameter Models with Simple
               and Efficient Sparsity},
  journal   = {CoRR},
  volume    = {abs/2101.03961},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.03961},
  eprinttype = {arXiv},
  eprint    = {2101.03961},
  timestamp = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-03961.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v80-parascandolo18a,
  title = 	 {Learning Independent Causal Mechanisms},
  author =       {Parascandolo, Giambattista and Kilbertus, Niki and Rojas-Carulla, Mateo and Sch{\"o}lkopf, Bernhard},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4036--4044},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  pdf = 	 {http://proceedings.mlr.press/v80/parascandolo18a/parascandolo18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/parascandolo18a.html},
  abstract = 	 {Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the first place. From the point of view of causal modeling, the structure of each distribution is induced by physical mechanisms that give rise to dependences between observables. Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a particular entailed data distribution, lending themselves to transfer between problems. We develop an algorithm to recover a set of independent (inverse) mechanisms from a set of transformed data points. The approach is unsupervised and based on a set of experts that compete for data generated by the mechanisms, driving specialization. We analyze the proposed method in a series of experiments on image data. Each expert learns to map a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss implications for transfer learning and links to recent trends in generative modeling.}
}


@article{Gupta2022Sparsely,
  author    = {Shashank Gupta and
               Subhabrata Mukherjee and
               Krishan Subudhi and
               Eduardo Gonzalez and
               Damien Jose and
               Ahmed Hassan Awadallah and
               Jianfeng Gao},
  title     = {Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners},
  journal   = {CoRR},
  volume    = {abs/2204.07689},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2204.07689},
  doi       = {10.48550/arXiv.2204.07689},
  eprinttype = {arXiv},
  eprint    = {2204.07689},
  timestamp = {Tue, 19 Apr 2022 17:11:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2204-07689.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Xi2022UFO,
  author    = {Teng Xi and
               Yifan Sun and
               Deli Yu and
               Bi Li and
               Nan Peng and
               Gang Zhang and
               Xinyu Zhang and
               Zhigang Wang and
               Jinwen Chen and
               Jian Wang and
               Lufei Liu and
               Haocheng Feng and
               Junyu Han and
               Jingtuo Liu and
               Errui Ding and
               Jingdong Wang},
  editor    = {Shai Avidan and
               Gabriel J. Brostow and
               Moustapha Ciss{\'{e}} and
               Giovanni Maria Farinella and
               Tal Hassner},
  title     = {{UFO:} Unified Feature Optimization},
  booktitle = {Computer Vision - {ECCV} 2022 - 17th European Conference, Tel Aviv,
               Israel, October 23-27, 2022, Proceedings, Part {XXVI}},
  series    = {Lecture Notes in Computer Science},
  volume    = {13686},
  pages     = {472--488},
  publisher = {Springer},
  year      = {2022},
  url       = {https://doi.org/10.1007/978-3-031-19809-0\_27},
  doi       = {10.1007/978-3-031-19809-0\_27},
  timestamp = {Tue, 10 Jan 2023 23:01:00 +0100},
  biburl    = {https://dblp.org/rec/conf/eccv/XiSY0PZZWCWLFHL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Stickland2021MultilingualDomainAdapt,
  author    = {Asa Cooper Stickland and
               Alexandre Berard and
               Vassilina Nikoulina},
  editor    = {Lo{\"{\i}}c Barrault and
               Ondrej Bojar and
               Fethi Bougares and
               Rajen Chatterjee and
               Marta R. Costa{-}juss{\`{a}} and
               Christian Federmann and
               Mark Fishel and
               Alexander Fraser and
               Markus Freitag and
               Yvette Graham and
               Roman Grundkiewicz and
               Paco Guzman and
               Barry Haddow and
               Matthias Huck and
               Antonio Jimeno{-}Yepes and
               Philipp Koehn and
               Tom Kocmi and
               Andr{\'{e}} Martins and
               Makoto Morishita and
               Christof Monz},
  title     = {Multilingual Domain Adaptation for {NMT:} Decoupling Language and
               Domain Information with Adapters},
  booktitle = {Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP
               2021, Online Event, November 10-11, 2021},
  pages     = {578--598},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://aclanthology.org/2021.wmt-1.64},
  timestamp = {Wed, 19 Jan 2022 17:10:33 +0100},
  biburl    = {https://dblp.org/rec/conf/wmt/SticklandBN21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{baars2005global,
  title={Global workspace theory of consciousness: toward a cognitive neuroscience of human experience},
  author={Baars, Bernard J.},
  journal={Progress in Brain Research},
  volume={150},
  pages={45--53},
  year={2005},
  publisher={Elsevier},
  doi = {10.1016/S0079-6123(05)50004-9},
  url = {https://10.1016/S0079-6123(05)50004-9}
}

@inproceedings{rahaman2021dynamic,
  author    = {Nasim Rahaman and
               Muhammad Waleed Gondal and
               Shruti Joshi and
               Peter V. Gehler and
               Yoshua Bengio and
               Francesco Locatello and
               Bernhard Sch{\"{o}}lkopf},
  title     = {Dynamic Inference with Neural Interpreters},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {10985--10998},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/5b4e9aa703d0bfa11041debaa2d1b633-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:47 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/RahamanGJGBLS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zoph2022STMOE,
  doi = {10.48550/ARXIV.2202.08906},
  
  url = {https://arxiv.org/abs/2202.08906},
  
  author = {Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{ST-MoE:} Designing Stable and Transferable Sparse Expert Models},
  
  journal = {CoRR},
  volume = {abs/2202.08906},
  
  year = {2022},
}



@InProceedings{Du2022Glam,
  author    = {Nan Du and
               Yanping Huang and
               Andrew M. Dai and
               Simon Tong and
               Dmitry Lepikhin and
               Yuanzhong Xu and
               Maxim Krikun and
               Yanqi Zhou and
               Adams Wei Yu and
               Orhan Firat and
               Barret Zoph and
               Liam Fedus and
               Maarten P. Bosma and
               Zongwei Zhou and
               Tao Wang and
               Yu Emma Wang and
               Kellie Webster and
               Marie Pellat and
               Kevin Robinson and
               Kathleen S. Meier{-}Hellstern and
               Toju Duke and
               Lucas Dixon and
               Kun Zhang and
               Quoc V. Le and
               Yonghui Wu and
               Zhifeng Chen and
               Claire Cui},
  title     = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {5547--5569},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/du22c.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/DuHDTLXKZYFZFBZ22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{gulcehre2016dynamic,
  title={Dynamic {N}eural {T}uring {M}achine with soft and hard addressing schemes},
  author={Gulcehre, Caglar and Chandar, Sarath and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1607.00036},
  year={2016},
  url={https://arxiv.org/pdf/1607.00036.pdf},
}

@inproceedings{
goyal2019recurrent,
  author    = {Anirudh Goyal and
               Alex Lamb and
               Jordan Hoffmann and
               Shagun Sodhani and
               Sergey Levine and
               Yoshua Bengio and
               Bernhard Sch{\"{o}}lkopf},
  title     = {Recurrent Independent Mechanisms},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=mLcmdlEUxy-},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/GoyalLHSLBS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Bahdanau2015NMT,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ostapenko2021continual,
  author    = {Oleksiy Ostapenko and
               Pau Rodr{\'{\i}}guez and
               Massimo Caccia and
               Laurent Charlin},
  title     = {Continual Learning via Local Module Composition},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {30298--30312},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:49 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/OstapenkoRCC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hu2021lora,
  author    = {Edward J. Hu and
               Yelong Shen and
               Phillip Wallis and
               Zeyuan Allen{-}Zhu and
               Yuanzhi Li and
               Shean Wang and
               Lu Wang and
               Weizhu Chen},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=nZeVKeeFYf9},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{ruder2019latent,
  title={Latent multi-task architecture learning},
  author={Ruder, Sebastian and Bingel, Joachim and Augenstein, Isabelle and S{\o}gaard, Anders},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={4822--4829},
  year={2019},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/4410/4288},
}

@article{rosenbaum2019routing,
  author    = {Clemens Rosenbaum and
               Ignacio Cases and
               Matthew Riemer and
               Tim Klinger},
  title     = {Routing Networks and the Challenges of Modular and Compositional Computation},
  journal   = {CoRR},
  volume    = {abs/1904.12774},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.12774},
  eprinttype = {arXiv},
  eprint    = {1904.12774},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-12774.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Vidoni:2020arxiv,
  author    = {Marko Vidoni and
               Ivan Vuli\'{c} and
               Goran Glava\v{s}},
  title     = {Orthogonal Language and Task Adapters in Zero-Shot Cross-Lingual Transfer},
  journal   = {CoRR},
  volume    = {abs/2012.06460},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.06460},
  eprinttype = {arXiv},
  eprint    = {2012.06460},
  timestamp = {Sat, 02 Jan 2021 15:43:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-06460.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Pfeiffer2021UNKs,
    title = "{UNK}s Everywhere: {A}dapting Multilingual Language Models to New Scripts",
    author = "Pfeiffer, Jonas  and
      Vuli{\'c}, Ivan  and
      Gurevych, Iryna  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.800",
    doi = "10.18653/v1/2021.emnlp-main.800",
    pages = "10186--10203",
    abstract = "Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model{'}s embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT{'}s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.",
}

@inproceedings{Ansell2021MADG,
    title = "{MAD}-{G}: {M}ultilingual Adapter Generation for Efficient Cross-Lingual Transfer",
    author = "Ansell, Alan  and
      Ponti, Edoardo Maria  and
      Pfeiffer, Jonas  and
      Ruder, Sebastian  and
      Glava{\v{s}}, Goran  and
      Vuli{\'c}, Ivan  and
      Korhonen, Anna",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.410",
    doi = "10.18653/v1/2021.findings-emnlp.410",
    pages = "4762--4781",
    abstract = "Adapter modules have emerged as a general parameter-efficient means to specialize a pretrained encoder to new domains. Massively multilingual transformers (MMTs) have particularly benefited from additional training of language-specific adapters. However, this approach is not viable for the vast majority of languages, due to limitations in their corpus size or compute budgets. In this work, we propose MAD-G (Multilingual ADapter Generation), which contextually generates language adapters from language representations based on typological features. In contrast to prior work, our time- and space-efficient MAD-G approach enables (1) sharing of linguistic knowledge across languages and (2) zero-shot inference by generating language adapters for unseen languages. We thoroughly evaluate MAD-G in zero-shot cross-lingual transfer on part-of-speech tagging, dependency parsing, and named entity recognition. While offering (1) improved fine-tuning efficiency (by a factor of around 50 in our experiments), (2) a smaller parameter budget, and (3) increased language coverage, MAD-G remains competitive with more expensive methods for language-specific adapter training across the board. Moreover, it offers substantial benefits for low-resource languages, particularly on the NER task in low-resource African languages. Finally, we demonstrate that MAD-G{'}s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available task-specific training data; and (ii) by further fine-tuning generated MAD-G adapters for languages with monolingual data.",
}

@inproceedings{lauscher-etal-2021-sustainable-modular,
    title = "Sustainable Modular Debiasing of Language Models",
    author = "Lauscher, Anne  and
      Lueken, Tobias  and
      Glava{\v{s}}, Goran",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.411",
    doi = "10.18653/v1/2021.findings-emnlp.411",
    pages = "4782--4797",
}

@phdthesis{schuler2005verbnet,
  title={VerbNet: A broad-coverage, comprehensive verb lexicon},
  author={Schuler, Karin Kipper},
  year={2005},
  school={University of Pennsylvania},
  url = {https://repository.upenn.edu/dissertations/AAI3179808},
}

@article{He:2022arxiv,
 author = {Xuehai He and
Chunyuan Li and
Pengchuan Zhang and
Jianwei Yang and
Xin Eric Wang},
 doi = {10.48550/arXiv.2203.16329},
 journal = {CoRR},
 title = {Parameter-efficient Fine-tuning for Vision Transformers},
 url = {https://doi.org/10.48550/arXiv.2203.16329},
 volume = {abs/2203.16329},
 year = {2022}
}


@inproceedings{Berriel:2019iccv,
  author    = {Rodrigo Ferreira Berriel and
               St{\'{e}}phane Lathuili{\`{e}}re and
               Moin Nabi and
               Tassilo Klein and
               Thiago Oliveira{-}Santos and
               Nicu Sebe and
               Elisa Ricci},
  title     = {Budget-Aware Adapters for Multi-Domain Learning},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {382--391},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00047},
  doi       = {10.1109/ICCV.2019.00047},
}


@inproceedings{Parovic2022BADX,
    title = "{BAD}-{X}: Bilingual Adapters Improve Zero-Shot Cross-Lingual Transfer",
    author = "Parovi{\'c}, Marinela  and
      Glava{\v{s}}, Goran  and
      Vuli{\'c}, Ivan  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.130",
    doi = "10.18653/v1/2022.naacl-main.130",
    pages = "1791--1799",
    abstract = "Adapter modules enable modular and efficient zero-shot cross-lingual transfer, where current state-of-the-art adapter-based approaches learn specialized language adapters (LAs) for individual languages. In this work, we show that it is more effective to learn bilingual language pair adapters (BAs) when the goal is to optimize performance for a particular source-target transfer direction. Our novel BAD-X adapter framework trades off some modularity of dedicated LAs for improved transfer performance: we demonstrate consistent gains in three standard downstream tasks, and for the majority of evaluated low-resource languages.",
}

@article{Zhang:2021ws,
  author    = {Rongsheng Zhang and
               Yinhe Zheng and
               Xiaoxi Mao and
               Minlie Huang},
  title     = {Unsupervised Domain Adaptation with Adapter},
  journal   = {CoRR},
  volume    = {abs/2111.00667},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.00667},
}

@inproceedings{Thompson:2018nmt,
    title = "Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation",
    author = "Thompson, Brian  and
      Khayrallah, Huda  and
      Anastasopoulos, Antonios  and
      McCarthy, Arya D.  and
      Duh, Kevin  and
      Marvin, Rebecca  and
      McNamee, Paul  and
      Gwinnup, Jeremy  and
      Anderson, Tim  and
      Koehn, Philipp",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6313",
    doi = "10.18653/v1/W18-6313",
    pages = "124--132",
}


@inproceedings{Romera:2012orthogonality,
  author    = {Bernardino Romera{-}Paredes and
               Andreas Argyriou and
               Nadia Berthouze and
               Massimiliano Pontil},
  title     = {Exploiting Unrelated Tasks in Multi-Task Learning},
  booktitle = {Proceedings of the Fifteenth International Conference on Artificial
               Intelligence and Statistics, {AISTATS} 2012, La Palma, Canary Islands,
               Spain, April 21-23, 2012},
  pages     = {951--959},
  year      = {2012},
  url       = {http://proceedings.mlr.press/v22/romera12.html},
  timestamp = {Wed, 29 May 2019 08:41:48 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/Romera-ParedesABP12.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Choenni:2022arxiv,
  author    = {Rochelle Choenni and
               Dan Garrette and
               Ekaterina Shutova},
  title     = {Data-Efficient Cross-Lingual Transfer with Language-Specific Subnetworks},
  journal   = {CoRR},
  volume    = {abs/2211.00106},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.00106},
  doi       = {10.48550/arXiv.2211.00106},
  eprinttype = {arXiv},
  eprint    = {2211.00106},
  timestamp = {Fri, 04 Nov 2022 13:48:49 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-00106.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Rusu2016Progressive, 
  author    = {Andrei A. Rusu and
               Neil C. Rabinowitz and
               Guillaume Desjardins and
               Hubert Soyer and
               James Kirkpatrick and
               Koray Kavukcuoglu and
               Razvan Pascanu and
               Raia Hadsell},
  title     = {Progressive Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1606.04671},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.04671},
  eprinttype = {arXiv},
  eprint    = {1606.04671},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RusuRDSKKPH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{galanti2020modularity,
  title={On the modularity of hypernetworks},
  author={Galanti, Tomer and Wolf, Lior},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020},
  url={https://research.fb.com/wp-content/uploads/2020/12/On-the-Modularity-of-Hypernetworks.pdf},
}

@article{fernando2017pathnet,
  author    = {Chrisantha Fernando and
               Dylan Banarse and
               Charles Blundell and
               Yori Zwols and
               David Ha and
               Andrei A. Rusu and
               Alexander Pritzel and
               Daan Wierstra},
  title     = {PathNet: Evolution Channels Gradient Descent in Super Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1701.08734},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.08734},
  eprinttype = {arXiv},
  eprint    = {1701.08734},
  timestamp = {Mon, 13 Aug 2018 16:49:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FernandoBBZHRPW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{crawshaw2020multi,
  author    = {Michael Crawshaw},
  title     = {Multi-Task Learning with Deep Neural Networks: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2009.09796},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.09796},
  eprinttype = {arXiv},
  eprint    = {2009.09796},
  timestamp = {Wed, 23 Sep 2020 15:51:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-09796.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ruder2017overview,
  author    = {Sebastian Ruder},
  title     = {An Overview of Multi-Task Learning in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1706.05098},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.05098},
  eprinttype = {arXiv},
  eprint    = {1706.05098},
  timestamp = {Mon, 13 Aug 2018 16:48:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Ruder17a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Meyerson2018,
annote = {- introduce soft ordering for multi-task learning; seems very similar to sluice networks / cross-stitch networks
- don't fully get it; has same task-specific parameters as cross-stitch networks; are F1, F2, and F3 in Figure 3 shared across layers?},
author = {Meyerson, Elliot and Miikkulainen, Risto},
booktitle = {Proceedings of ICLR 2018},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Meyerson, Miikkulainen - 2018 - Beyond Shared Hierarchies Deep Multitask Learning through Soft Layer Ordering.pdf:pdf},
mendeley-groups = {Adaptation/Multitask Learning},
title = {{Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering}},
year = {2018}
}

@inproceedings{Collobert2001Parallel,
  author    = {Ronan Collobert and
               Samy Bengio and
               Yoshua Bengio},
  editor    = {Thomas G. Dietterich and
               Suzanna Becker and
               Zoubin Ghahramani},
  title     = {A Parallel Mixture of SVMs for Very Large Scale Problems},
  booktitle = {Advances in Neural Information Processing Systems 14 [Neural Information
               Processing Systems: Natural and Synthetic, {NIPS} 2001, December 3-8,
               2001, Vancouver, British Columbia, Canada]},
  pages     = {633--640},
  publisher = {{MIT} Press},
  year      = {2001},
  url       = {https://proceedings.neurips.cc/paper/2001/hash/36ac8e558ac7690b6f44e2cb5ef93322-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/CollobertBB01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Jordan1994Hierarchical,
  author    = {Michael I. Jordan and
               Robert A. Jacobs},
  title     = {Hierarchical Mixtures of Experts and the {EM} Algorithm},
  journal   = {Neural Computation},
  volume    = {6},
  number    = {2},
  pages     = {181--214},
  year      = {1994},
  url       = {https://doi.org/10.1162/neco.1994.6.2.181},
  doi       = {10.1162/neco.1994.6.2.181},
  timestamp = {Fri, 25 Dec 2020 01:11:54 +0100},
  biburl    = {https://dblp.org/rec/journals/neco/JordanJ94.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Theis2015Generative,
  author    = {Lucas Theis and
               Matthias Bethge},
  editor    = {Corinna Cortes and
               Neil D. Lawrence and
               Daniel D. Lee and
               Masashi Sugiyama and
               Roman Garnett},
  title     = {Generative Image Modeling Using Spatial LSTMs},
  booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
               on Neural Information Processing Systems 2015, December 7-12, 2015,
               Montreal, Quebec, Canada},
  pages     = {1927--1935},
  year      = {2015},
  url       = {https://proceedings.neurips.cc/paper/2015/hash/2b6d65b9a9445c4271ab9076ead5605a-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/TheisB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Garmash2016Ensemble,
  author    = {Ekaterina Garmash and
               Christof Monz},
  editor    = {Nicoletta Calzolari and
               Yuji Matsumoto and
               Rashmi Prasad},
  title     = {Ensemble Learning for Multi-Source Neural Machine Translation},
  booktitle = {{COLING} 2016, 26th International Conference on Computational Linguistics,
               Proceedings of the Conference: Technical Papers, December 11-16, 2016,
               Osaka, Japan},
  pages     = {1409--1418},
  publisher = {{ACL}},
  year      = {2016},
  url       = {https://aclanthology.org/C16-1133/},
  timestamp = {Fri, 06 Aug 2021 00:39:56 +0200},
  biburl    = {https://dblp.org/rec/conf/coling/GarmashM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Guo:2018emnlp,
    title = "Multi-Source Domain Adaptation with Mixture of Experts",
    author = "Guo, Jiang  and
      Shah, Darsh  and
      Barzilay, Regina",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1498",
    doi = "10.18653/v1/D18-1498",
    pages = "4694--4703",
}

@article{Zhang:2022metadmoe,
  author    = {Tao Zhong and
               Zhixiang Chi and
               Li Gu and
               Yang Wang and
               Yuanhao Yu and
               Jin Tang},
  title     = {Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts},
  journal   = {CoRR},
  volume    = {abs/2210.03885},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.03885},
}

@article{bengio2015conditional,
  author    = {Emmanuel Bengio and
               Pierre{-}Luc Bacon and
               Joelle Pineau and
               Doina Precup},
  title     = {Conditional Computation in Neural Networks for faster models},
  journal   = {CoRR},
  volume    = {abs/1511.06297},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06297},
  eprinttype = {arXiv},
  eprint    = {1511.06297},
  timestamp = {Sat, 23 Jan 2021 01:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/BengioBPP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Aljundi2017Expert,
  author    = {Rahaf Aljundi and
               Punarjay Chakravarty and
               Tinne Tuytelaars},
  title     = {Expert Gate: Lifelong Learning with a Network of Experts},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {7120--7129},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/CVPR.2017.753},
  doi       = {10.1109/CVPR.2017.753},
  timestamp = {Fri, 27 Dec 2019 21:26:15 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/AljundiCT17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Rasmussen2001Infinite,
  author    = {Carl Edward Rasmussen and
               Zoubin Ghahramani},
  editor    = {Thomas G. Dietterich and
               Suzanna Becker and
               Zoubin Ghahramani},
  title     = {Infinite Mixtures of Gaussian Process Experts},
  booktitle = {Advances in Neural Information Processing Systems 14 [Neural Information
               Processing Systems: Natural and Synthetic, {NIPS} 2001, December 3-8,
               2001, Vancouver, British Columbia, Canada]},
  pages     = {881--888},
  publisher = {{MIT} Press},
  year      = {2001},
  url       = {https://proceedings.neurips.cc/paper/2001/hash/9afefc52942cb83c7c1f14b2139b09ba-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/RasmussenG01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Yao2009Hierarchical,
  author    = {Bangpeng Yao and
               Dirk B. Walther and
               Diane M. Beck and
               Li Fei{-}Fei},
  editor    = {Yoshua Bengio and
               Dale Schuurmans and
               John D. Lafferty and
               Christopher K. I. Williams and
               Aron Culotta},
  title     = {Hierarchical Mixture of Classification Experts Uncovers Interactions
               between Brain Regions},
  booktitle = {Advances in Neural Information Processing Systems 22: 23rd Annual
               Conference on Neural Information Processing Systems 2009. Proceedings
               of a meeting held 7-10 December 2009, Vancouver, British Columbia,
               Canada},
  pages     = {2178--2186},
  publisher = {Curran Associates, Inc.},
  year      = {2009},
  url       = {https://proceedings.neurips.cc/paper/2009/hash/a86c450b76fb8c371afead6410d55534-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/YaoWB009.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Shahbaba2009Nonlinear,
  author    = {Babak Shahbaba and
               Radford M. Neal},
  title     = {Nonlinear Models Using Dirichlet Process Mixtures},
  journal   = {Journal of Machine Learning Research},
  volume    = {10},
  pages     = {1829--1850},
  year      = {2009},
  url       = {https://dl.acm.org/doi/10.5555/1577069.1755846},
  doi       = {10.5555/1577069.1755846},
  timestamp = {Thu, 02 Jun 2022 13:58:57 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/ShahbabaN09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Deisenroth2015Distributed,
  author    = {Marc Peter Deisenroth and
               Jun Wei Ng},
  editor    = {Francis R. Bach and
               David M. Blei},
  title     = {Distributed Gaussian Processes},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning,
               {ICML} 2015, Lille, France, 6-11 July 2015},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {37},
  pages     = {1481--1490},
  publisher = {JMLR.org},
  year      = {2015},
  url       = {http://proceedings.mlr.press/v37/deisenroth15.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/DeisenrothN15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Tresp2000Mixture,
  author    = {Volker Tresp},
  editor    = {Todd K. Leen and
               Thomas G. Dietterich and
               Volker Tresp},
  title     = {Mixtures of Gaussian Processes},
  booktitle = {Advances in Neural Information Processing Systems 13, Papers from
               Neural Information Processing Systems {(NIPS)} 2000, Denver, CO, {USA}},
  pages     = {654--660},
  publisher = {{MIT} Press},
  year      = {2000},
  url       = {https://proceedings.neurips.cc/paper/2000/hash/9fdb62f932adf55af2c0e09e55861964-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/Tresp00.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ke-etal-2021-adapting,
    title = "Adapting {BERT} for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks",
    author = "Ke, Zixuan  and
      Xu, Hu  and
      Liu, Bing",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.378",
    doi = "10.18653/v1/2021.naacl-main.378",
    pages = "4746--4755",
}

@article{chen2018exploring,
  author    = {Junkun Chen and
               Kaiyu Chen and
               Xinchi Chen and
               Xipeng Qiu and
               Xuanjing Huang},
  title     = {Exploring Shared Structures and Hierarchies for Multiple {NLP} Tasks},
  journal   = {CoRR},
  volume    = {abs/1808.07658},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.07658},
  eprinttype = {arXiv},
  eprint    = {1808.07658},
  timestamp = {Sun, 02 Sep 2018 15:01:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-07658.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sun2020adashare,
  author    = {Ximeng Sun and
               Rameswar Panda and
               Rog{\'{e}}rio Feris and
               Kate Saenko},
  title     = {{AdaShare: L}earning What To Share For Efficient Deep Multi-Task Learning},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/634841a6831464b64c072c8510c7f35c-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:15 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/SunPFS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mallya2018piggyback,
  author    = {Arun Mallya and
               Dillon Davis and
               Svetlana Lazebnik},
  title     = {Piggyback: Adapting a Single Network to Multiple Tasks by Learning
               to Mask Weights},
  booktitle = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
               Germany, September 8-14, 2018, Proceedings, Part {IV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {11208},
  pages     = {72--88},
  publisher = {Springer},
  year      = {2018},
  url       = {https://doi.org/10.1007/978-3-030-01225-0\_5},
  doi       = {10.1007/978-3-030-01225-0\_5},
  timestamp = {Tue, 14 May 2019 10:00:45 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/MallyaDL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sun2020learning,
  author    = {Tianxiang Sun and
               Yunfan Shao and
               Xiaonan Li and
               Pengfei Liu and
               Hang Yan and
               Xipeng Qiu and
               Xuanjing Huang},
  title     = {Learning Sparse Sharing Architectures for Multiple Tasks},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {8936--8943},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/6424},
  timestamp = {Wed, 23 Nov 2022 07:49:00 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/SunSLLYQH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Frankle2019,
  author    = {Jonathan Frankle and
               Michael Carbin},
  title     = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=rJl-b3RcF7},
  timestamp = {Thu, 25 Jul 2019 13:03:15 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/FrankleC19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
maddison2017the,
  author    = {Chris J. Maddison and
               Andriy Mnih and
               Yee Whye Teh},
  title     = {The Concrete Distribution: {A} Continuous Relaxation of Discrete Random
               Variables},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=S1jE5L5gl},
  timestamp = {Thu, 25 Jul 2019 14:26:01 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MaddisonMT17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lin-etal-2021-learning,
    title = "Learning Language Specific Sub-network for Multilingual Machine Translation",
    author = "Lin, Zehui  and
      Wu, Liwei  and
      Wang, Mingxuan  and
      Li, Lei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.25",
    doi = "10.18653/v1/2021.acl-long.25",
    pages = "293--305",
    abstract = "Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradationon rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at https://github.com/NLP-Playground/LaSS.",
}

@inproceedings{rosenbaum2017routing,
  author    = {Clemens Rosenbaum and
               Tim Klinger and
               Matthew Riemer},
  title     = {Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task
               Learning},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=ry8dvM-R-},
  timestamp = {Thu, 25 Jul 2019 14:25:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/RosenbaumKR18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chang2018automatically,
  author    = {Michael Chang and
               Abhishek Gupta and
               Sergey Levine and
               Thomas L. Griffiths},
  title     = {Automatically Composing Representation Transformations as a Means
               for Generalization},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1ffQnRcKX},
  timestamp = {Thu, 28 Jul 2022 09:56:41 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChangGLG19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ahn2019deep,
  author    = {Chanho Ahn and
               Eunwoo Kim and
               Songhwai Oh},
  title     = {Deep Elastic Networks With Model Selection for Multi-Task Learning},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {6528--6537},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00663},
  doi       = {10.1109/ICCV.2019.00663},
  timestamp = {Thu, 05 Mar 2020 13:43:22 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/AhnKO19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
wang2021gradient,
  author    = {Zirui Wang and
               Yulia Tsvetkov and
               Orhan Firat and
               Yuan Cao},
  title     = {Gradient Vaccine: Investigating and Improving Multi-task Optimization
               in Massively Multilingual Models},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=F1vEjWK-lH\_},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/WangTF021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Schwarz2018progress,
annote = {- propose a new framework Progress & Compress for continual, sequential learning
- consists of two networks:
1. knowlege base that can solve previously encountered problems;
2. an active column that efficiently learns the current task; after learning, the active column is distilled into the knowledge base with a modified version of Elastic Weight Consolidation (EWC)
- cycle of active learning (progression) followed by consolidation (compression)
- nuanced comparison on different tasks; P&C is most useful in settings where some positive transfer is useful vs. methods that overcome catastrophic forgetting},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.06370v1},
author = {Schwarz, Jonathan and Luketina, Jelena and Czarnecki, Wojciech M. and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
booktitle = {Proceedings of ICML 2018},
eprint = {arXiv:1805.06370v1},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Schwarz et al. - 2018 - Progress & Compress A scalable framework for continual learning.pdf:pdf},
mendeley-groups = {Adaptation/Lifelong Learning},
title = {{Progress \& Compress : A scalable framework for continual learning}},
year = {2018}
}

@inproceedings{Mallya2018packnet,
  author    = {Arun Mallya and
               Svetlana Lazebnik},
  title     = {PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {7765--7773},
  publisher = {Computer Vision Foundation / {IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Mallya\_PackNet\_Adding\_Multiple\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00810},
  timestamp = {Tue, 31 Aug 2021 14:00:32 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/MallyaL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{guo-etal-2021-parameter,
    title = "Parameter-Efficient Transfer Learning with Diff Pruning",
    author = "Guo, Demi  and
      Rush, Alexander  and
      Kim, Yoon",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.378",
    doi = "10.18653/v1/2021.acl-long.378",
    pages = "4884--4896",
    abstract = "The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific {``}diff{''} vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5{\%} of the pretrained model{'}s parameters per task and scales favorably in comparison to popular pruning approaches.",
}

@inproceedings{ben-zaken-etal-2022-bitfit,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.1",
    doi = "10.18653/v1/2022.acl-short.1",
    pages = "1--9",
    abstract = "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
}

@inproceedings{wortsman2020supermasks,
  author    = {Mitchell Wortsman and
               Vivek Ramanujan and
               Rosanne Liu and
               Aniruddha Kembhavi and
               Mohammad Rastegari and
               Jason Yosinski and
               Ali Farhadi},
  title     = {Supermasks in Superposition},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/ad1f8bb9b51f023cdc80cf94bb615aa9-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:05 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/WortsmanRLKRYF20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{han2015learning,
 author    = {Song Han and
               Jeff Pool and
               John Tran and
               William J. Dally},
  title     = {Learning both Weights and Connections for Efficient Neural Network},
  booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
               on Neural Information Processing Systems 2015, December 7-12, 2015,
               Montreal, Quebec, Canada},
  pages     = {1135--1143},
  year      = {2015},
  url       = {https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/HanPTD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{han2016dsd,
  author    = {Song Han and
               Jeff Pool and
               Sharan Narang and
               Huizi Mao and
               Enhao Gong and
               Shijian Tang and
               Erich Elsen and
               Peter Vajda and
               Manohar Paluri and
               John Tran and
               Bryan Catanzaro and
               William J. Dally},
  title     = {{DSD:} Dense-Sparse-Dense Training for Deep Neural Networks},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=HyoST\_9xl},
  timestamp = {Fri, 20 Nov 2020 16:16:07 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/HanPNMGTEVPTCD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhou2019deconstructing,
  author    = {Hattie Zhou and
               Janice Lan and
               Rosanne Liu and
               Jason Yosinski},
  title     = {Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {3592--3602},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/1113d7a76ffceca1bb350bfe145467c6-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/ZhouLLY19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Cai2020tinytl,
  author    = {Han Cai and
               Chuang Gan and
               Ligeng Zhu and
               Song Han},
  title     = {TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:03 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/CaiGZ020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bodenreider2004unified,
  title={The unified medical language system ({UMLS}): integrating biomedical terminology},
  author={Bodenreider, Olivier},
  journal={Nucleic Acids Research},
  volume={32},
  number={suppl\_1},
  pages={D267--D270},
  year={2004},
  publisher={Oxford University Press},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC308795/}
}

@inproceedings{Speer:2017conceptnet,
  author    = {Robyn Speer and
               Joshua Chin and
               Catherine Havasi},
  title     = {ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
  booktitle = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
               February 4-9, 2017, San Francisco, California, {USA}},
  pages     = {4444--4451},
  year      = {2017},
  url       = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972},
}

@article{Liu:2022delving,
  author    = {Chen Liu and
               Jonas Pfeiffer and
               Anna Korhonen and
               Ivan Vuli\'{c} and
               Iryna Gurevych},
  title     = {Delving Deeper into Cross-lingual Visual Question Answering},
  journal   = {CoRR},
  volume    = {abs/2202.07630},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.07630},
}

@inproceedings{Chen:2021neurips,
  author    = {Tianlong Chen and
               Yu Cheng and
               Zhe Gan and
               Lu Yuan and
               Lei Zhang and
               Zhangyang Wang},
  title     = {Chasing Sparsity in Vision Transformers: An End-to-End Exploration},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {19974--19988},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/a61f27ab2165df0e18cc9433bd7f27c5-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:48 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/ChenCGYZW21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{donahue2014decaf,
  author    = {Jeff Donahue and
               Yangqing Jia and
               Oriol Vinyals and
               Judy Hoffman and
               Ning Zhang and
               Eric Tzeng and
               Trevor Darrell},
  title     = {DeCAF: {A} Deep Convolutional Activation Feature for Generic Visual
               Recognition},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning,
               {ICML} 2014, Beijing, China, 21-26 June 2014},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {32},
  pages     = {647--655},
  publisher = {JMLR.org},
  year      = {2014},
  url       = {http://proceedings.mlr.press/v32/donahue14.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/DonahueJVHZTD14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhang2022SkillNet, 
year = {2022}, 
title = {{SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural Language Understanding}}, 
author = {Zhang, Fan and Tang, Duyu and Dai, Yong and Zhou, Cong and Wu, Shuangzhi and Shi, Shuming}, 
journal = {CoRR}, 
doi = {10.48550/arxiv.2203.03312}, 
volume = {abs/2203.03312}, 
url = {https://arxiv.org/abs/2203.03312},
abstract = {{Prevailing deep models are single-purpose and overspecialize at individual tasks. However, when being extended to new tasks, they typically forget previously learned skills and learn from scratch. We address this issue by introducing SkillNet-NLU, a general-purpose model that stitches together existing skills to learn new tasks more effectively. The key feature of our approach is that it is sparsely activated guided by predefined skills. Different from traditional dense models that always activate all the model parameters, SkillNet-NLU only activates parts of the model parameters whose skills are relevant to the target task. When learning for a new task, our approach precisely activates required skills and also provides an option to add new skills. We evaluate on natural language understandings tasks and have the following findings. First, with only one model checkpoint, SkillNet-NLU performs better than task-specific fine-tuning and two multi-task learning baselines (i.e., dense model and Mixture-of-Experts model) on six tasks. Second, sparsely activated pre-training further improves the overall performance. Third, SkillNet-NLU significantly outperforms baseline systems when being extended to new tasks.}}, 
keywords = {}
}

@article{Muqeeth2022Models, 
year = {2022},  
title = {{Models with Conditional Computation Learn Suboptimal Solutions}}, 
author = {Muqeeth, Mohammed and Liu, Haokun and Raffel, Colin}, 
journal = {arXiv preprint}, 
url = {https://colinraffel.com/publications/icbinb2022models.pdf}, 
keywords = {}
}



@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
    abstract = "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are {``}fantastic{''} and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13{\%} relative improvement for GPT-family models across eleven different established text classification tasks.",
}

@inproceedings{he2022hyperprompt,
  author    = {Yun He and
               Huaixiu Steven Zheng and
               Yi Tay and
               Jai Prakash Gupta and
               Yu Du and
               Vamsi Aribandi and
               Zhe Zhao and
               YaGuang Li and
               Zhao Chen and
               Donald Metzler and
               Heng{-}Tze Cheng and
               Ed H. Chi},
  title     = {HyperPrompt: Prompt-based Task-Conditioning of Transformers},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {8678--8690},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/he22f.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/HeZTGDAZLCMCC22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ustun2020,
    title = "{UD}apter: Language Adaptation for Truly {U}niversal {D}ependency Parsing",
    author = {{\"U}st{\"u}n, Ahmet  and
      Bisazza, Arianna  and
      Bouma, Gosse  and
      van Noord, Gertjan},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.180",
    doi = "10.18653/v1/2020.emnlp-main.180",
    pages = "2302--2315",
    abstract = "Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.",
}

@inproceedings{Lester2021prompttuning,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.",
}

@article{liu2021gpt,
  author    = {Xiao Liu and
               Yanan Zheng and
               Zhengxiao Du and
               Ming Ding and
               Yujie Qian and
               Zhilin Yang and
               Jie Tang},
  title     = {{GPT} Understands, Too},
  journal   = {CoRR},
  volume    = {abs/2103.10385},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.10385},
  eprinttype = {arXiv},
  eprint    = {2103.10385},
  timestamp = {Wed, 22 Sep 2021 15:21:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-10385.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{liu-etal-2022-p,
    title = "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    author = "Liu, Xiao  and
      Ji, Kaixuan  and
      Fu, Yicheng  and
      Tam, Weng  and
      Du, Zhengxiao  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.8",
    doi = "10.18653/v1/2022.acl-short.8",
    pages = "61--68",
    abstract = "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1{\%}-3{\%} tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
}

@inproceedings{zhong-etal-2021-factual,
    title = "Factual Probing Is [{MASK}]: Learning vs. Learning to Recall",
    author = "Zhong, Zexuan  and
      Friedman, Dan  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.398",
    doi = "10.18653/v1/2021.naacl-main.398",
    pages = "5017--5033",
    abstract = "Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the model{'}s prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4{\%} of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle {``}learning{''} from {``}learning to recall{''}, providing a more detailed picture of what different prompts can reveal about pre-trained language models.",
}

@inproceedings{hambardzumyan-etal-2021-warp,
    title = "{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming",
    author = "Hambardzumyan, Karen  and
      Khachatrian, Hrant  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.381",
    doi = "10.18653/v1/2021.acl-long.381",
    pages = "4921--4933",
    abstract = "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.",
}

@inproceedings{vilar-2018-learning,
    title = "Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models",
    author = "Vilar, David",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2080",
    doi = "10.18653/v1/N18-2080",
    pages = "500--505",
}

@article{pham-etal-2021-revisiting,
    title = "Revisiting Multi-Domain Machine Translation",
    author = "Pham, MinhQuang  and
      Crego, Josep Maria  and
      Yvon, Fran{\c{c}}ois",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.2",
    doi = "10.1162/tacl_a_00351",
    pages = "17--35",
}

@article{Saunders:2022survey,
  author    = {Danielle Saunders},
  title     = {Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation:
               {A} Survey},
  journal   = {Journal of Artificial Intelligence Research},
  volume    = {75},
  pages     = {351--424},
  year      = {2022},
  url       = {https://doi.org/10.1613/jair.1.13566},
  doi       = {10.1613/jair.1.13566},
}


@inproceedings{wang-etal-2020-negative,
    title = "On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment",
    author = "Wang, Zirui  and
      Lipton, Zachary C.  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.359",
    doi = "10.18653/v1/2020.emnlp-main.359",
    pages = "4438--4450",
    abstract = "Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief, negative interference also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers{'} generalization on all languages. Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.",
}

@inproceedings{ustun-etal-2021-multilingual,
    title = "Multilingual Unsupervised Neural Machine Translation with Denoising Adapters",
    author = {{\"U}st{\"u}n, Ahmet  and
      Berard, Alexandre  and
      Besacier, Laurent  and
      Gall{\'e}, Matthias},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.533",
    doi = "10.18653/v1/2021.emnlp-main.533",
    pages = "6650--6662",
}

@inproceedings{philip-etal-2020-monolingual,
    title = "Monolingual Adapters for Zero-Shot Neural Machine Translation",
    author = "Philip, Jerin  and
      Berard, Alexandre  and
      Gall{\'e}, Matthias  and
      Besacier, Laurent",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.361",
    doi = "10.18653/v1/2020.emnlp-main.361",
    pages = "4465--4470",
}

@article{Chronopolou:2022arxiv,
  author    = {Alexandra Chronopoulou and
               Dario Stojanovski and
               Alexander Fraser},
  title     = {Language-Family Adapters for Multilingual Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2209.15236},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2209.15236},
  doi       = {10.48550/arXiv.2209.15236},
  eprinttype = {arXiv},
  eprint    = {2209.15236},
  timestamp = {Thu, 06 Oct 2022 14:41:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2209-15236.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liu-etal-2020-multilingual-denoising,
    title = "Multilingual Denoising Pre-training for Neural Machine Translation",
    author = "Liu, Yinhan  and
      Gu, Jiatao  and
      Goyal, Naman  and
      Li, Xian  and
      Edunov, Sergey  and
      Ghazvininejad, Marjan  and
      Lewis, Mike  and
      Zettlemoyer, Luke",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.47",
    doi = "10.1162/tacl_a_00343",
    pages = "726--742",
}

@inproceedings{Wang:2019superglue,
  author    = {Alex Wang and
               Yada Pruksachatkun and
               Nikita Nangia and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {SuperGLUE: {A} Stickier Benchmark for General-Purpose Language Understanding
               Systems},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {3261--3275},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/WangPNSMHLB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cooper-stickland-etal-2021-multilingual,
    title = "Multilingual Domain Adaptation for {NMT}: Decoupling Language and Domain Information with Adapters",
    author = "Cooper Stickland, Asa  and
      Berard, Alexandre  and
      Nikoulina, Vassilina",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.64",
    pages = "578--598",
}

@article{Hoefler2021,
abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
archivePrefix = {arXiv},
arxivId = {2102.00554},
author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
eprint = {2102.00554},
file = {:Users/ruder/Documents/Papers/2102.00554.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Generalization,Low memory,Performance,Sparsity},
title = {{Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks}},
volume = {22},
year = {2021}
}

@inproceedings{Molchanov2017,
  author    = {Pavlo Molchanov and
               Stephen Tyree and
               Tero Karras and
               Timo Aila and
               Jan Kautz},
  title     = {Pruning Convolutional Neural Networks for Resource Efficient Inference},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=SJGCiw5gl},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MolchanovTKAK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Mehta2019,
  author    = {Rahul Mehta},
  title     = {Sparse Transfer Learning via Winning Lottery Tickets},
  journal   = {CoRR},
  volume    = {abs/1905.07785},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.07785},
  eprinttype = {arXiv},
  eprint    = {1905.07785},
  timestamp = {Fri, 31 May 2019 12:53:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-07785.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chen2020lottery,
  author    = {Tianlong Chen and
               Jonathan Frankle and
               Shiyu Chang and
               Sijia Liu and
               Yang Zhang and
               Zhangyang Wang and
               Michael Carbin},
  title     = {The Lottery Ticket Hypothesis for Pre-trained {BERT} Networks},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/b6af2c9703f203a2794be03d443af2e3-Abstract.html},
  timestamp = {Tue, 09 Aug 2022 17:04:09 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/ChenFC0ZWC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{manessi2018automated,
  title={Automated pruning for deep neural network compression},
  author={Manessi, Franco and Rozza, Alessandro and Bianco, Simone and Napoletano, Paolo and Schettini, Raimondo},
  booktitle={2018 24th International conference on pattern recognition (ICPR)},
  pages={657--664},
  year={2018},
  organization={IEEE}
}

@inproceedings{Li2018intrinsic,
  author    = {Chunyuan Li and
               Heerad Farkhoor and
               Rosanne Liu and
               Jason Yosinski},
  title     = {Measuring the Intrinsic Dimension of Objective Landscapes},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=ryup8-WCW},
  timestamp = {Thu, 25 Jul 2019 14:25:52 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LiFLY18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{platanios-etal-2018-contextual,
    title = "Contextual Parameter Generation for Universal Neural Machine Translation",
    author = "Platanios, Emmanouil Antonios  and
      Sachan, Mrinmaya  and
      Neubig, Graham  and
      Mitchell, Tom",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1039",
    doi = "10.18653/v1/D18-1039",
    pages = "425--435",
}

@inproceedings{aghajanyan-etal-2021-intrinsic,
    title = "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    author = "Aghajanyan, Armen  and
      Gupta, Sonal  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.568",
    doi = "10.18653/v1/2021.acl-long.568",
    pages = "7319--7328",
    abstract = "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90{\%} of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
}

@inproceedings{Ruder:2021xtremer,
    title = "{XTREME}-{R}: Towards More Challenging and Nuanced Multilingual Evaluation",
    author = "Ruder, Sebastian  and
      Constant, Noah  and
      Botha, Jan  and
      Siddhant, Aditya  and
      Firat, Orhan  and
      Fu, Jinlan  and
      Liu, Pengfei  and
      Hu, Junjie  and
      Garrette, Dan  and
      Neubig, Graham  and
      Johnson, Melvin",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.802",
    doi = "10.18653/v1/2021.emnlp-main.802",
    pages = "10215--10245",
}

@article{le2013fastfood,
  author    = {Quoc Viet Le and
               Tam{\'{a}}s Sarl{\'{o}}s and
               Alexander Johannes Smola},
  title     = {Fastfood: Approximate Kernel Expansions in Loglinear Time},
  journal   = {CoRR},
  volume    = {abs/1408.3060},
  year      = {2014},
  url       = {http://arxiv.org/abs/1408.3060},
  eprinttype = {arXiv},
  eprint    = {1408.3060},
  timestamp = {Mon, 13 Aug 2018 16:46:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LeSS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hu:2020xtreme,
  author    = {Junjie Hu and
               Sebastian Ruder and
               Aditya Siddhant and
               Graham Neubig and
               Orhan Firat and
               Melvin Johnson},
  title     = {{XTREME:} {A} Massively Multilingual Multi-task Benchmark for Evaluating
               Cross-lingual Generalisation},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  pages     = {4411--4421},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/hu20b.html},
  timestamp = {Thu, 08 Apr 2021 11:46:39 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/HuRSNFJ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{Misra2016,
author    = {Ishan Misra and
               Abhinav Shrivastava and
               Abhinav Gupta and
               Martial Hebert},
  title     = {Cross-Stitch Networks for Multi-task Learning},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {3994--4003},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.433},
  doi       = {10.1109/CVPR.2016.433},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/MisraSGH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{negrinho2019towards,
  title={Towards modular and programmable architecture search},
  author={Negrinho, Renato and Gormley, Matthew and Gordon, Geoffrey J and Patil, Darshan and Le, Nghia and Ferreira, Daniel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kirkpatrick1983optimization,
  title={Optimization by simulated annealing},
  author={Kirkpatrick, Scott and Gelatt Jr, C. Daniel and Vecchi, Mario P.},
  journal={Science},
  volume={220},
  number={4598},
  pages={671--680},
  year={1983},
  publisher={American association for the advancement of science},
  doi = {https://doi.org/10.1126/science.220.4598.671},
  url = {https://doi.org/10.1126/science.220.4598.671},
}

@inproceedings{Rueckle2021AdapterDrop,
    title = "{AdapterDrop}: {O}n the Efficiency of Adapters in Transformers",
    author = {R{\"u}ckl{\'e}, Andreas  and
      Geigle, Gregor  and
      Glockner, Max  and
      Beck, Tilman  and
      Pfeiffer, Jonas  and
      Reimers, Nils  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.626",
    doi = "10.18653/v1/2021.emnlp-main.626",
    pages = "7930--7946",
    abstract = "Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.",
}

@article{Caccia2022MultiHeadAdapter,
  author    = {Lucas Caccia and
               Edoardo Maria Ponti and
               Lucas Liu and
               Matheus Pereira and
               Nicolas Le Roux and
               Alessandro Sordoni},
  title     = {Multi-Head Adapter Routing for Data-Efficient Fine-Tuning},
  journal   = {CoRR},
  volume    = {abs/2211.03831},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.03831},
  doi       = {10.48550/arXiv.2211.03831},
  eprinttype = {arXiv},
  eprint    = {2211.03831},
  timestamp = {Wed, 09 Nov 2022 17:33:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-03831.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Jaini2021scalable,
annote = {- propose an approach using expert modules consisting of 4 steps:
1. Pre-train a general baseline model.
2. Pre-train expert modules (or adapters) on (overlapping) subsets of the pre-training data based on the pre-training weights.
3. For each downstream task, compute the image representation per expert and select the expert that scores highest based on a kNN classifier (using the training labels).
4. Fine-tune the whole model on the target task.},
author = {Jaini, Priyank and Chen, Zhitang and Carbajal, Pablo and Law, Edith and Middleton, Laura and Regan, Kayla and Schaekermann, Mike and Trimponias, George and Tung, James and Poupart, Pascal},
booktitle = {Proceedings of ICLR 2021},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Jaini et al. - 2021 - Scalable Transfer Learning with Experit Models.pdf:pdf},
keywords = {ZSL},
title = {{Scalable Transfer Learning with Expert Models}},
year = {2021}
}

@article{GeigleTWEAC2021,
  author    = {Gregor Geigle and
               Nils Reimers and
               Andreas R{\"{u}}ckl{\'{e}} and
               Iryna Gurevych},
  title     = {{TWEAC:} Transformer with Extendable {QA} Agent Classifiers},
  journal   = {CoRR},
  volume    = {abs/2104.07081},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.07081},
  eprinttype = {arXiv},
  eprint    = {2104.07081},
  timestamp = {Thu, 14 Oct 2021 09:17:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-07081.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{alayrac2022flamingo,
  author    = {Jean{-}Baptiste Alayrac and
               Jeff Donahue and
               Pauline Luc and
               Antoine Miech and
               Iain Barr and
               Yana Hasson and
               Karel Lenc and
               Arthur Mensch and
               Katie Millican and
               Malcolm Reynolds and
               Roman Ring and
               Eliza Rutherford and
               Serkan Cabi and
               Tengda Han and
               Zhitao Gong and
               Sina Samangooei and
               Marianne Monteiro and
               Jacob Menick and
               Sebastian Borgeaud and
               Andrew Brock and
               Aida Nematzadeh and
               Sahand Sharifzadeh and
               Mikolaj Binkowski and
               Ricardo Barreira and
               Oriol Vinyals and
               Andrew Zisserman and
               Karen Simonyan},
  title     = {Flamingo: a Visual Language Model for Few-Shot Learning},
  journal   = {CoRR},
  volume    = {abs/2204.14198},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2204.14198},
  doi       = {10.48550/arXiv.2204.14198},
  eprinttype = {arXiv},
  eprint    = {2204.14198},
  timestamp = {Tue, 16 Aug 2022 23:07:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2204-14198.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li2021align,
  author    = {Junnan Li and
               Ramprasaath R. Selvaraju and
               Akhilesh Gotmare and
               Shafiq R. Joty and
               Caiming Xiong and
               Steven Chu{-}Hong Hoi},
  title     = {Align before Fuse: Vision and Language Representation Learning with
               Momentum Distillation},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {9694--9705},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/505259756244493872b7709a8a01b536-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:47 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/LiSGJXH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Qiu2022,
abstract = {Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve compositional generalization in semantic parsing? We evaluate encoder-decoder models up to 11B parameters and decoder-only models up to 540B parameters, and compare model scaling curves for three different methods for transfer learning: fine-tuning all parameters, prompt tuning, and in-context learning. We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations. In-context learning has positive scaling curves, but is generally outperformed by much smaller fine-tuned models. Prompt-tuning can outperform fine-tuning, suggesting further potential improvements from scaling as it exhibits a more positive scaling curve. Additionally, we identify several error trends that vary with model scale. For example, larger models are generally better at modeling the syntax of the output space, but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work.},
archivePrefix = {arXiv},
arxivId = {2205.12253},
author = {Qiu, Linlu and Shaw, Peter and Pasupat, Panupong and Shi, Tianze and Herzig, Jonathan and Pitler, Emily and Sha, Fei and Toutanova, Kristina},
booktitle = {Proceedings of EMNLP 2022},
eprint = {2205.12253},
file = {:Users/ruder/Documents/Papers/2205.12253.pdf:pdf},
title = {{Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing}},
url = {http://arxiv.org/abs/2205.12253},
year = {2022}
}

@inproceedings{ruder2019transfer,
    title = "Transfer Learning in Natural Language Processing",
    author = "Ruder, Sebastian  and
      Peters, Matthew E.  and
      Swayamdipta, Swabha  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorials",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-5004",
    doi = "10.18653/v1/N19-5004",
    pages = "15--18",
    abstract = "The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.",
}

@inproceedings{Vaswani2017,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All you Need},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {5998--6008},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mishra-etal-2022-cross,
    title = "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
    author = "Mishra, Swaroop  and
      Khashabi, Daniel  and
      Baral, Chitta  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.244",
    doi = "10.18653/v1/2022.acl-long.244",
    pages = "3470--3487",
    abstract = "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19{\%} better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.",
}

@inproceedings{
wei2022finetuned,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@inproceedings{ye-etal-2021-crossfit,
    title = "{C}ross{F}it: A Few-shot Learning Challenge for Cross-task Generalization in {NLP}",
    author = "Ye, Qinyuan  and
      Lin, Bill Yuchen  and
      Ren, Xiang",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.572",
    doi = "10.18653/v1/2021.emnlp-main.572",
    pages = "7163--7189",
    abstract = "Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.",
}

@inproceedings{Vu2022spot,
    title = "{SP}o{T}: Better Frozen Model Adaptation through Soft Prompt Transfer",
    author = "Vu, Tu  and
      Lester, Brian  and
      Constant, Noah  and
      Al-Rfou{'}, Rami  and
      Cer, Daniel",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.346",
    doi = "10.18653/v1/2022.acl-long.346",
    pages = "5039--5059",
    abstract = "There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks. More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000{\mbox{$\times$}} fewer task-specific parameters. To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer. Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.",
}

@inproceedings{zhang2014facial,
author    = {Zhanpeng Zhang and
               Ping Luo and
               Chen Change Loy and
               Xiaoou Tang},
  title     = {Facial Landmark Detection by Deep Multi-task Learning},
  booktitle = {Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich,
               Switzerland, September 6-12, 2014, Proceedings, Part {VI}},
  series    = {Lecture Notes in Computer Science},
  volume    = {8694},
  pages     = {94--108},
  publisher = {Springer},
  year      = {2014},
  url       = {https://doi.org/10.1007/978-3-319-10599-4\_7},
  doi       = {10.1007/978-3-319-10599-4\_7},
  timestamp = {Tue, 14 May 2019 10:00:45 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/ZhangLLT14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Keysers2020MeasuringComposition,
  author    = {Daniel Keysers and
               Nathanael Sch{\"{a}}rli and
               Nathan Scales and
               Hylke Buisman and
               Daniel Furrer and
               Sergii Kashubin and
               Nikola Momchev and
               Danila Sinopalnikov and
               Lukasz Stafiniak and
               Tibor Tihon and
               Dmitry Tsarkov and
               Xiao Wang and
               Marc van Zee and
               Olivier Bousquet},
  title     = {Measuring Compositional Generalization: {A} Comprehensive Method on
               Realistic Data},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SygcCnNKwr},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KeysersSSBFKMSS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gao2019nddr,
  author    = {Yuan Gao and
               Jiayi Ma and
               Mingbo Zhao and
               Wei Liu and
               Alan L. Yuille},
  title     = {{NDDR-CNN:} Layerwise Feature Fusing in Multi-Task CNNs by Neural
               Discriminative Dimensionality Reduction},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
               2019, Long Beach, CA, USA, June 16-20, 2019},
  pages     = {3205--3214},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2019},
  url       = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Gao\_NDDR-CNN\_Layerwise\_Feature\_Fusing\_in\_Multi-Task\_CNNs\_by\_Neural\_Discriminative\_CVPR\_2019\_paper.html},
  doi       = {10.1109/CVPR.2019.00332},
  timestamp = {Wed, 20 Jul 2022 09:01:42 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/Gao0ZLY19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{strezoski2019many,
  author    = {Gjorgji Strezoski and
               Nanne van Noord and
               Marcel Worring},
  title     = {Many Task Learning With Task Routing},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {1375--1384},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00146},
  doi       = {10.1109/ICCV.2019.00146},
  timestamp = {Thu, 05 Mar 2020 13:43:22 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/StrezoskiNW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sogaard2016deep,
    title = "Deep multi-task learning with low level tasks supervised at lower layers",
    author = "S{\o}gaard, Anders  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-2038",
    doi = "10.18653/v1/P16-2038",
    pages = "231--235",
}

@inproceedings{wolf2019huggingface,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@inproceedings{strubell-etal-2019-energy,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
}

@article{Schwartz:2020greenai,
  author    = {Roy Schwartz and
               Jesse Dodge and
               Noah A. Smith and
               Oren Etzioni},
  title     = {Green {AI}},
  journal   = {Communications of the {ACM}},
  volume    = {63},
  number    = {12},
  pages     = {54--63},
  year      = {2020},
  url       = {https://doi.org/10.1145/3381831},
}

@inproceedings{sanh2019hierarchical,
  author    = {Victor Sanh and
               Thomas Wolf and
               Sebastian Ruder},
  title     = {A Hierarchical Multi-Task Approach for Learning Embeddings from Semantic
               Tasks},
  booktitle = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI}
               2019, The Thirty-First Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii,
               USA, January 27 - February 1, 2019},
  pages     = {6949--6956},
  publisher = {{AAAI} Press},
  year      = {2019},
  url       = {https://doi.org/10.1609/aaai.v33i01.33016949},
  doi       = {10.1609/aaai.v33i01.33016949},
  timestamp = {Tue, 02 Feb 2021 07:59:51 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/SanhWR19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{heess2016learning,
  author    = {Nicolas Heess and
               Gregory Wayne and
               Yuval Tassa and
               Timothy P. Lillicrap and
               Martin A. Riedmiller and
               David Silver},
  title     = {Learning and Transfer of Modulated Locomotor Controllers},
  journal   = {CoRR},
  volume    = {abs/1610.05182},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.05182},
  eprinttype = {arXiv},
  eprint    = {1610.05182},
  timestamp = {Mon, 13 Aug 2018 16:47:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeessWTLRS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{devin2017learning,
  author    = {Coline Devin and
               Abhishek Gupta and
               Trevor Darrell and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Learning modular neural network policies for multi-task and multi-robot
               transfer},
  booktitle = {2017 {IEEE} International Conference on Robotics and Automation, {ICRA}
               2017, Singapore, Singapore, May 29 - June 3, 2017},
  pages     = {2169--2176},
  publisher = {{IEEE}},
  year      = {2017},
  url       = {https://doi.org/10.1109/ICRA.2017.7989250},
  doi       = {10.1109/ICRA.2017.7989250},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/icra/DevinGDAL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{de2017modulating,
  author    = {Harm de Vries and
               Florian Strub and
               J{\'{e}}r{\'{e}}mie Mary and
               Hugo Larochelle and
               Olivier Pietquin and
               Aaron C. Courville},
  title     = {Modulating early visual processing by language},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {6594--6604},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/6fab6e3aa34248ec1e34a4aeedecddc8-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/VriesSMLPC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Perez2018,
  author    = {Ethan Perez and
               Florian Strub and
               Harm de Vries and
               Vincent Dumoulin and
               Aaron C. Courville},
  title     = {FiLM: Visual Reasoning with a General Conditioning Layer},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th Innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {3942--3951},
  publisher = {{AAAI} Press},
  year      = {2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16528},
  timestamp = {Tue, 08 Mar 2022 21:46:35 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/PerezSVDC18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Chen2019,
  author    = {Ting Chen and
               Mario Lu\v{c}i\'{c} and
               Neil Houlsby and
               Sylvain Gelly},
  title     = {On Self Modulation for Generative Adversarial Networks},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=Hkl5aoR5tm},
  timestamp = {Thu, 25 Jul 2019 14:25:49 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChenLHG19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lu2017fully,
  author    = {Yongxi Lu and
               Abhishek Kumar and
               Shuangfei Zhai and
               Yu Cheng and
               Tara Javidi and
               Rog{\'{e}}rio Schmidt Feris},
  title     = {Fully-Adaptive Feature Sharing in Multi-Task Networks with Applications
               in Person Attribute Classification},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {1131--1140},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/CVPR.2017.126},
  doi       = {10.1109/CVPR.2017.126},
  timestamp = {Sun, 12 Feb 2023 18:48:22 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/LuKZCJF17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vandenhende2019branched,
 author    = {Simon Vandenhende and
               Stamatios Georgoulis and
               Luc Van Gool and
               Bert De Brabandere},
  title     = {Branched Multi-Task Networks: Deciding what layers to share},
  booktitle = {31st British Machine Vision Conference 2020, {BMVC} 2020, Virtual
               Event, UK, September 7-10, 2020},
  publisher = {{BMVA} Press},
  year      = {2020},
  url       = {https://www.bmvc2020-conference.com/assets/papers/0213.pdf},
  timestamp = {Wed, 03 Feb 2021 08:36:06 +0100},
  biburl    = {https://dblp.org/rec/conf/bmvc/VandenhendeGGB20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{newell2019feature,
  author    = {Alejandro Newell and
               Lu Jiang and
               Chong Wang and
               Li{-}Jia Li and
               Jia Deng},
  title     = {Feature Partitioning for Efficient Multi-Task Architectures},
  journal   = {CoRR},
  volume    = {abs/1908.04339},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.04339},
  eprinttype = {arXiv},
  eprint    = {1908.04339},
  timestamp = {Mon, 01 Feb 2021 18:33:24 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-04339.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bragman2019stochastic,
  author    = {Felix J. S. Bragman and
               Ryutaro Tanno and
               S{\'{e}}bastien Ourselin and
               Daniel C. Alexander and
               Manuel Jorge Cardoso},
  title     = {Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist
               and Generalist Convolution Kernels},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {1385--1394},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00147},
  doi       = {10.1109/ICCV.2019.00147},
  timestamp = {Thu, 23 Jun 2022 19:55:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/BragmanTOAC19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Shachar2022ColD,
  author    = {Shachar Don{-}Yehiya and
               Elad Venezian and
               Colin Raffel and
               Noam Slonim and
               Yoav Katz and
               Leshem Choshen},
  title     = {ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning},
  journal   = {arXiv prerint},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2212.01378},
  doi       = {10.48550/arXiv.2212.01378},
  timestamp = {Thu, 08 Dec 2022 15:26:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2212-01378.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ScholkopfJPSZM12,
  author    = {Bernhard Sch{\"{o}}lkopf and
               Dominik Janzing and
               Jonas Peters and
               Eleni Sgouritsa and
               Kun Zhang and
               Joris M. Mooij},
  title     = {On causal and anticausal learning},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning,
               {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
  publisher = {icml.cc / Omnipress},
  year      = {2012},
  url       = {http://icml.cc/2012/papers/625.pdf},
  timestamp = {Wed, 03 Apr 2019 17:43:35 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ScholkopfJPSZM12.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{greff2020binding,
  author    = {Klaus Greff and
               Sjoerd van Steenkiste and
               J{\"{u}}rgen Schmidhuber},
  title     = {On the Binding Problem in Artificial Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2012.05208},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.05208},
  eprinttype = {arXiv},
  eprint    = {2012.05208},
  timestamp = {Sat, 02 Jan 2021 15:43:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-05208.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@ARTICLE{9363924,
  author    = {Bernhard Sch{\"{o}}lkopf and
               Francesco Locatello and
               Stefan Bauer and
               Nan Rosemary Ke and
               Nal Kalchbrenner and
               Anirudh Goyal and
               Yoshua Bengio},
  title     = {Toward Causal Representation Learning},
  journal   = {Proceedings of the {IEEE}},
  volume    = {109},
  number    = {5},
  pages     = {612--634},
  year      = {2021},
  url       = {https://doi.org/10.1109/JPROC.2021.3058954},
  doi       = {10.1109/JPROC.2021.3058954},
  timestamp = {Tue, 01 Jun 2021 08:35:57 +0200},
  biburl    = {https://dblp.org/rec/journals/pieee/ScholkopfLBKKGB21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2016_20d135f0,
 author = {Lee, Stefan and Purushwalkam Shiva Prakash, Senthil and Cogswell, Michael and Ranjan, Viresh and Crandall, David and Batra, Dhruv},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles},
 url = {https://proceedings.neurips.cc/paper/2016/file/20d135f0f28185b84a4cf7aa51f29500-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{ustun-etal-2020-udapter,
    title = "{UD}apter: Language Adaptation for Truly {U}niversal {D}ependency Parsing",
    author = {{\"U}st{\"u}n, Ahmet  and
      Bisazza, Arianna  and
      Bouma, Gosse  and
      van Noord, Gertjan},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.180",
    doi = "10.18653/v1/2020.emnlp-main.180",
    pages = "2302--2315",
    abstract = "Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.",
}

@inproceedings{Oswald2020,
  author    = {Johannes von Oswald and
               Christian Henning and
               Jo{\~{a}}o Sacramento and
               Benjamin F. Grewe},
  title     = {Continual learning with hypernetworks},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SJgwNerKvB},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/OswaldHSG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{van2019three,
  author    = {Gido M. van de Ven and
               Andreas S. Tolias},
  title     = {Three scenarios for continual learning},
  journal   = {CoRR},
  volume    = {abs/1904.07734},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.07734},
  eprinttype = {arXiv},
  eprint    = {1904.07734},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-07734.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Aharoni2020,
author = {Aharoni, Roee and Goldberg, Yoav},
booktitle = {Proceedings of ACL 2020},
eprint = {2004.02105},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Aharoni, Goldberg - 2020 - Unsupervised Domain Clusters in Pretrained Language Models.pdf:pdf},
mendeley-groups = {Adaptation/Domain Adaptation},
title = {{Unsupervised Domain Clusters in Pretrained Language Models}},
url = {http://arxiv.org/abs/2004.02105},
year = {2020}
}

@inproceedings{Howard2018ulmfit,
 address = {Melbourne, Australia},
 author = {Howard, Jeremy  and
Ruder, Sebastian},
 booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/P18-1031},
 pages = {328--339},
 publisher = {Association for Computational Linguistics},
 title = {Universal Language Model Fine-tuning for Text Classification},
 url = {https://aclanthology.org/P18-1031},
 year = {2018}
}

@article{caruana1997multitask,
  author    = {Rich Caruana},
  title     = {Multitask Learning},
  journal   = {Machine Learning},
  volume    = {28},
  number    = {1},
  pages     = {41--75},
  year      = {1997},
  url       = {https://doi.org/10.1023/A:1007379606734},
  doi       = {10.1023/A:1007379606734},
  timestamp = {Tue, 05 Jul 2022 08:30:28 +0200},
  biburl    = {https://dblp.org/rec/journals/ml/Caruana97.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Matena2022,
  author    = {Michael Matena and
               Colin Raffel},
  title     = {Merging Models with Fisher-Weighted Averaging},
  journal   = {CoRR},
  volume    = {abs/2111.09832},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09832},
  eprinttype = {arXiv},
  eprint    = {2111.09832},
  timestamp = {Mon, 22 Nov 2021 16:44:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09832.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Khanuja2021,
    title = "{M}erge{D}istill: {M}erging Language Models using Pre-trained Distillation",
    author = "Khanuja, Simran  and
      Johnson, Melvin  and
      Talukdar, Partha",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.254",
    doi = "10.18653/v1/2021.findings-acl.254",
    pages = "2874--2887",
}

@inproceedings{Bousmalis2016,
  author    = {Konstantinos Bousmalis and
               George Trigeorgis and
               Nathan Silberman and
               Dilip Krishnan and
               Dumitru Erhan},
  editor    = {Daniel D. Lee and
               Masashi Sugiyama and
               Ulrike von Luxburg and
               Isabelle Guyon and
               Roman Garnett},
  title     = {Domain Separation Networks},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
               on Neural Information Processing Systems 2016, December 5-10, 2016,
               Barcelona, Spain},
  pages     = {343--351},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/hash/45fbc6d3e05ebd93369ce542e8f2322d-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/BousmalisTSKE16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chen-cardie-2018-multinomial,
    title = "Multinomial Adversarial Networks for Multi-Domain Text Classification",
    author = "Chen, Xilun  and
      Cardie, Claire",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1111",
    doi = "10.18653/v1/N18-1111",
    pages = "1226--1240",
    abstract = "Many text classification tasks are known to be highly domain-dependent. Unfortunately, the availability of training data can vary drastically across domains. Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multinomial adversarial network (MAN) to tackle this real-world problem of multi-domain text classification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theoretical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multiple probability distributions. MANs are thus a theoretically sound generalization of traditional adversarial networks that discriminate over two distributions. More specifically, for the MDTC task, MAN learns features that are invariant across multiple domains by resorting to its ability to reduce the divergence among the feature distributions of each domain. We present experimental results showing that MANs significantly outperform the prior art on the MDTC task. We also show that MANs achieve state-of-the-art performance for domains with no labeled data.",
}

@article{ganin2016domain,
  author    = {Yaroslav Ganin and
               Evgeniya Ustinova and
               Hana Ajakan and
               Pascal Germain and
               Hugo Larochelle and
               Fran{\c{c}}ois Laviolette and
               Mario Marchand and
               Victor S. Lempitsky},
  title     = {Domain-Adversarial Training of Neural Networks},
  journal   = {Journal of Machine Learning Research},
  volume    = {17},
  pages     = {59:1--59:35},
  year      = {2016},
  url       = {http://jmlr.org/papers/v17/15-239.html},
  timestamp = {Wed, 10 Jul 2019 15:28:07 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/GaninUAGLLML16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{baktashmotlagh2013unsupervised,
  author    = {Mahsa Baktashmotlagh and
               Mehrtash Tafazzoli Harandi and
               Brian C. Lovell and
               Mathieu Salzmann},
  title     = {Unsupervised Domain Adaptation by Domain Invariant Projection},
  booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2013, Sydney,
               Australia, December 1-8, 2013},
  pages     = {769--776},
  publisher = {{IEEE} Computer Society},
  year      = {2013},
  url       = {https://doi.org/10.1109/ICCV.2013.100},
  doi       = {10.1109/ICCV.2013.100},
  timestamp = {Thu, 14 Oct 2021 10:42:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/BaktashmotlaghHLS13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kim-etal-2017-adversarial,
    title = "Adversarial Adaptation of Synthetic or Stale Data",
    author = "Kim, Young-Bum  and
      Stratos, Karl  and
      Kim, Dongchan",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1119",
    doi = "10.18653/v1/P17-1119",
    pages = "1297--1307",
    abstract = "Two types of data shift common in practice are 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to current data (a temporal shift). Both cause a distribution mismatch between training and evaluation, leading to a model that overfits the flawed training data and performs poorly on the test data. We propose a solution to this mismatch problem by framing it as domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain. To this end, we use and build on several recent advances in neural domain adaptation such as adversarial training (Ganinet al., 2016) and domain separation network (Bousmalis et al., 2016), proposing a new effective adversarial training scheme. In both supervised and unsupervised adaptation scenarios, our approach yields clear improvement over strong baselines.",
}

@inproceedings{Andreas2017,
author = {Andreas, Jacob and Klein, Dan},
booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
eprint = {1707.08139},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Andreas, Klein - 2017 - Analogs of Linguistic Structure in Deep Representations.pdf:pdf},
mendeley-groups = {Fairness / interpretability / bias},
title = {{Analogs of Linguistic Strufcture in Deep Representations}},
url = {http://arxiv.org/abs/1707.08139},
year = {2017}
}

@inproceedings{Andreas2019,
author = {Andreas, Jacob},
booktitle = {Proceedings of ICLR 2019},
eprint = {1902.07181},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Andreas - 2019 - Measuring Compositionality in Representation Learning.pdf:pdf},
mendeley-groups = {Fairness / interpretability / bias},
title = {{Measuring Compositionality in Representation Learning}},
url = {http://arxiv.org/abs/1902.07181},
year = {2019}
}

@inproceedings{Akyurek2021,
author = {Aky{\"{u}}rek, Ekin and Aky{\"{u}}rek, Afra Feyza and Andreas, Jacob},
booktitle = {Proceedings of ICLR 2021},
eprint = {2010.03706},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Aky{\"{u}}rek, Aky{\"{u}}rek, Andreas - 2021 - Learning to recombine and resample data for compositional generalization.pdf:pdf},
issn = {23318422},
mendeley-groups = {Adaptation/Data noising / augmentation},
title = {{Learning to recombine and resample data for compositional generalization}},
year = {2021}
}

@inproceedings{yang-etal-2022-subs,
    title = "{SUBS}: Subtree Substitution for Compositional Semantic Parsing",
    author = "Yang, Jingfeng  and
      Zhang, Le  and
      Yang, Diyi",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.12",
    doi = "10.18653/v1/2022.naacl-main.12",
    pages = "169--174",
    abstract = "Although sequence-to-sequence models often achieve good performance in semantic parsing for i.i.d. data, their performance is still inferior in compositional generalization. Several data augmentation methods have been proposed to alleviate this problem. However, prior work only leveraged superficial grammar or rules for data augmentation, which resulted in limited improvement. We propose to use subtree substitution for compositional data augmentation, where we consider subtrees with similar semantic functions as exchangeable. Our experiments showed that such augmented data led to significantly better performance on Scan and GeoQuery, and reached new SOTA on compositional split of GeoQuery.",
}

@book{pearl2009causality,
  title={Causality},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge University Press},
  url = {https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B},
}

@article{ke2019learning,
  author    = {Nan Rosemary Ke and
               Olexa Bilaniuk and
               Anirudh Goyal and
               Stefan Bauer and
               Hugo Larochelle and
               Chris Pal and
               Yoshua Bengio},
  title     = {Learning Neural Causal Models from Unknown Interventions},
  journal   = {CoRR},
  volume    = {abs/1910.01075},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01075},
  eprinttype = {arXiv},
  eprint    = {1910.01075},
  timestamp = {Fri, 04 Oct 2019 12:28:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01075.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
ke2021systematic,
  author    = {Nan Rosemary Ke and
               Aniket Didolkar and
               Sarthak Mittal and
               Anirudh Goyal and
               Guillaume Lajoie and
               Stefan Bauer and
               Danilo Jimenez Rezende and
               Michael Mozer and
               Yoshua Bengio and
               Chris Pal},
  editor    = {Joaquin Vanschoren and
               Sai{-}Kit Yeung},
  title     = {Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement
               Learning},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on
               Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
               2021, virtual},
  year      = {2021},
  url       = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/8f121ce07d74717e0b1f21d122e04521-Abstract-round2.html},
  timestamp = {Thu, 05 May 2022 16:53:59 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/KeDMGLBRMBP21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v119-mittal20a,
  title = 	 {Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules},
  author =       {Mittal, Sarthak and Lamb, Alex and Goyal, Anirudh and Voleti, Vikram and Shanahan, Murray and Lajoie, Guillaume and Mozer, Michael and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6972--6986},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mittal20a/mittal20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/mittal20a.html},
  abstract = 	 {Robust perception relies on both bottom-up and top-down signals. Bottom-up signals consist of what’s directly observed through sensation. Top-down signals consist of beliefs and expectations based on past experience and the current reportable short-term memory, such as how the phrase ‘peanut butter and ...’ will be completed. The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow. We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention. Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data. We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \emph{bidirectional} information flow can improve results over strong baselines.}
}

@inproceedings{
Bengio2020meta,
  author    = {Yoshua Bengio and
               Tristan Deleu and
               Nasim Rahaman and
               Nan Rosemary Ke and
               S{\'{e}}bastien Lachapelle and
               Olexa Bilaniuk and
               Anirudh Goyal and
               Christopher J. Pal},
  title     = {A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=ryxWIgBFPS},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BengioDRKLBGP20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lange2022clustering,
author    = {Richard D. Lange and
               David S. Rolnick and
               Konrad P. Kording},
  title     = {Clustering units in neural networks: upstream vs downstream information},
  journal   = {CoRR},
  volume    = {abs/2203.11815},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2203.11815},
  doi       = {10.48550/arXiv.2203.11815},
  eprinttype = {arXiv},
  eprint    = {2203.11815},
  timestamp = {Tue, 29 Mar 2022 18:07:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2203-11815.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2019task,
  title={Task representations in neural networks trained to perform many cognitive tasks},
  author={Yang, Guangyu Robert and Joglekar, Madhura R. and Song, H. Francis and Newsome, William T. and Wang, Xiao-Jing},
  journal={Nature Neuroscience},
  volume={22},
  number={2},
  pages={297--306},
  year={2019},
  publisher={Nature Publishing Group US New York},
  doi = {https://doi.org/10.1038/s41593-018-0310-2},
  url = {https://doi.org/10.1038/s41593-018-0310-2},
}

@article{dobs2022brain,
  title={Brain-like functional specialization emerges spontaneously in deep neural networks},
  author={Dobs, Katharina and Martinez, Julio and Kell, Alexander JE and Kanwisher, Nancy},
  journal={Science Advances},
  volume={8},
  number={11},
  pages={eabl8913},
  year={2022},
  publisher={American Association for the Advancement of Science},
  url = {https://pubmed.ncbi.nlm.nih.gov/35294241/},
}

@article{graves2014neural,
  author    = {Alex Graves and
               Greg Wayne and
               Ivo Danihelka},
  title     = {Neural Turing Machines},
  journal   = {CoRR},
  volume    = {abs/1410.5401},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.5401},
  eprinttype = {arXiv},
  eprint    = {1410.5401},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GravesWD14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2020_8511df98,
  author    = {Francesco Locatello and
               Dirk Weissenborn and
               Thomas Unterthiner and
               Aravindh Mahendran and
               Georg Heigold and
               Jakob Uszkoreit and
               Alexey Dosovitskiy and
               Thomas Kipf},
  title     = {Object-Centric Learning with Slot Attention},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:46 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/LocatelloWUMHUD20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{graves2016hybrid,
  author    = {Alex Graves and
               Greg Wayne and
               Malcolm Reynolds and
               Tim Harley and
               Ivo Danihelka and
               Agnieszka Grabska{-}Barwinska and
               Sergio Gomez Colmenarejo and
               Edward Grefenstette and
               Tiago Ramalho and
               John P. Agapiou and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Karl Moritz Hermann and
               Yori Zwols and
               Georg Ostrovski and
               Adam Cain and
               Helen King and
               Christopher Summerfield and
               Phil Blunsom and
               Koray Kavukcuoglu and
               Demis Hassabis},
  title     = {Hybrid computing using a neural network with dynamic external memory},
  journal   = {Nature},
  volume    = {538},
  number    = {7626},
  pages     = {471--476},
  year      = {2016},
  url       = {https://doi.org/10.1038/nature20101},
  doi       = {10.1038/nature20101},
  timestamp = {Mon, 26 Oct 2020 08:43:10 +0100},
  biburl    = {https://dblp.org/rec/journals/nature/GravesWRHDGCGRA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{
mittal2022is,
  author    = {Sarthak Mittal and
               Yoshua Bengio and
               Guillaume Lajoie},
  title     = {Is a Modular Architecture Enough?},
  journal   = {CoRR},
  volume    = {abs/2206.02713},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2206.02713},
  doi       = {10.48550/arXiv.2206.02713},
  eprinttype = {arXiv},
  eprint    = {2206.02713},
  timestamp = {Tue, 14 Jun 2022 16:41:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2206-02713.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{huang2022language,
  title = 	 {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author =       {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9118--9147},
  year = 	 {2022},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/huang22a/huang22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/huang22a.html},
  abstract = 	 {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.}
}

@inproceedings{Hu_2017_ICCV,
 author = {Ronghang Hu and
Jacob Andreas and
Marcus Rohrbach and
Trevor Darrell and
Kate Saenko},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iccv/HuARDS17.bib},
 booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
Italy, October 22-29, 2017},
 doi = {10.1109/ICCV.2017.93},
 pages = {804--813},
 publisher = {{IEEE} Computer Society},
 timestamp = {Thu, 11 Jan 2018 00:00:00 +0100},
 title = {Learning to Reason: End-to-End Module Networks for Visual Question
Answering},
 url = {https://doi.org/10.1109/ICCV.2017.93},
 year = {2017}
}

@inproceedings{goyal2021nps,
  author    = {Aniket Didolkar and
               Anirudh Goyal and
               Nan Rosemary Ke and
               Charles Blundell and
               Philippe Beaudoin and
               Nicolas Heess and
               Michael Mozer and
               Yoshua Bengio},
  title     = {Neural Production Systems},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {25673--25687},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/d785bf9067f8af9e078b93cf26de2b54-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:49 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/DidolkarGKBBHMB21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Yu2020,
  author    = {Haonan Yu and
               Sergey Edunov and
               Yuandong Tian and
               Ari S. Morcos},
  title     = {Playing the lottery with rewards and multiple languages: lottery tickets
               in {RL} and {NLP}},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=S1xnXRVFwH},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/YuETM20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{prasanna-etal-2020-bert,
    title = "{W}hen {BERT} {P}lays the {L}ottery, {A}ll {T}ickets {A}re {W}inning",
    author = "Prasanna, Sai  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.259",
    doi = "10.18653/v1/2020.emnlp-main.259",
    pages = "3208--3229",
    abstract = "Large Transformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For fine-tuned BERT, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained BERT weights are potentially useful. We also study the {``}good{''} subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.",
}

@inproceedings{liang-etal-2021-super,
    title = "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization",
    author = "Liang, Chen  and
      Zuo, Simiao  and Chen, Minshuo  and Jiang, Haoming  and Liu, Xiaodong  and He, Pengcheng  and Zhao, Tuo  and Chen, Weizhu",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.510",
    doi = "10.18653/v1/2021.acl-long.510",
    pages = "6524--6538",
    abstract = "The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of {''}lottery tickets{''}, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as {''}winning tickets{''}, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as {''}super tickets{''}. We further show that the phase transition is task and model dependent {---} as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning.",
}

@inproceedings{zhao-etal-2020-masking,
    title = "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models",
    author = {Zhao, Mengjie  and
      Lin, Tao  and
      Mi, Fei  and
      Jaggi, Martin  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.174",
    doi = "10.18653/v1/2020.emnlp-main.174",
    pages = "2226--2241",
    abstract = "We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.",
}

@article{bengio2013estimating,
  author    = {Yoshua Bengio and
               Nicholas L{\'{e}}onard and
               Aaron C. Courville},
  title     = {Estimating or Propagating Gradients Through Stochastic Neurons for
               Conditional Computation},
  journal   = {CoRR},
  volume    = {abs/1308.3432},
  year      = {2013},
  url       = {http://arxiv.org/abs/1308.3432},
  eprinttype = {arXiv},
  eprint    = {1308.3432},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BengioLC13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sanh2020movement,
  author    = {Victor Sanh and
               Thomas Wolf and
               Alexander M. Rush},
  title     = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/eae15aabaa768ae4a5993a8a4f4fa6e4-Abstract.html},
  timestamp = {Sat, 09 Apr 2022 12:38:17 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/Sanh0R20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sung2021training,
  author    = {Yi{-}Lin Sung and
               Varun Nair and
               Colin Raffel},
  title     = {Training Neural Networks with Fixed Sparse Masks},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {24193--24205},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/cb2653f548f8709598e8b5156738cc51-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:49 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/SungNR21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{anwar2017structured,
  journal={ACM Journal on Emerging Technologies in Computing Systems (JETC)},
  author    = {Sajid Anwar and
               Kyuyeon Hwang and
               Wonyong Sung},
  title     = {Structured Pruning of Deep Convolutional Neural Networks},
  volume    = {13},
  number    = {3},
  pages     = {32:1--32:18},
  year      = {2017},
  url       = {https://doi.org/10.1145/3005348},
  doi       = {10.1145/3005348},
  timestamp = {Tue, 16 Aug 2022 23:09:52 +0200},
  biburl    = {https://dblp.org/rec/journals/jetc/AnwarHS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{voita-etal-2019-analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
    abstract = "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",
}

@inproceedings{michel2019sixteen,
  author    = {Paul Michel and
               Omer Levy and
               Graham Neubig},
  title     = {Are Sixteen Heads Really Better than One?},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {14014--14024},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/MichelLN19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zhao2021calibrate,
  author    = {Zihao Zhao and
               Eric Wallace and
               Shi Feng and
               Dan Klein and
               Sameer Singh},
  title     = {Calibrate Before Use: Improving Few-shot Performance of Language Models},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {12697--12706},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/zhao21c.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ZhaoWFK021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{webson-pavlick-2022-prompt,
    title = "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
    author = "Webson, Albert  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.167",
    doi = "10.18653/v1/2022.naacl-main.167",
    pages = "2300--2344",
    abstract = "Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively {``}good{''} prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models{'} impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans{'} use of task instructions.",
}

@inproceedings{he-etal-2021-effectiveness,
    title = "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation",
    author = "He, Ruidan  and
      Liu, Linlin  and
      Ye, Hai  and
      Tan, Qingyu  and
      Ding, Bosheng  and
      Cheng, Liying  and
      Low, Jiawei  and
      Bing, Lidong  and
      Si, Luo",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.172",
    doi = "10.18653/v1/2021.acl-long.172",
    pages = "2208--2222",
    abstract = "Adapter-based tuning has recently arisen as an alternative to fine-tuning. It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task. As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing. Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning. However, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness. In this paper, we study the latter. We first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM. We then empirically compare the two tuning methods on several downstream NLP tasks and settings. We demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates.",
}

@inproceedings{han-etal-2021-robust,
    title = "Robust Transfer Learning with Pretrained Language Models through Adapters",
    author = "Han, Wenjuan  and
      Pang, Bo  and
      Wu, Ying Nian",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.108",
    doi = "10.18653/v1/2021.acl-short.108",
    pages = "854--861",
    abstract = "Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.",
}

@inproceedings{Vu2022prompt,
    title = "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation",
    author = "Vu, Tu  and
      Barua, Aditya  and
      Lester, Brian  and
      Cer, Daniel  and
      Iyyer, Mohit  and
      Constant, Noah",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.630",
    pages = "9279--9300",
    abstract = "In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.",
}

@inproceedings{Reed2016npi,
  author    = {Scott E. Reed and
               Nando de Freitas},
  title     = {Neural Programmer-Interpreters},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06279},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ReedF15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Pierrot2019alphanpi,
  author    = {Thomas Pierrot and
               Guillaume Ligner and
               Scott E. Reed and
               Olivier Sigaud and
               Nicolas Perrin and
               Alexandre Laterre and
               David Kas and
               Karim Beguir and
               Nando de Freitas},
  title     = {Learning Compositional Neural Programs with Recursive Tree Search
               and Planning},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {14646--14656},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/95b431e51fc53692913da5263c214162-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/PierrotLRS0LKBF19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{andreas2016learning,
    title = "Learning to Compose Neural Networks for Question Answering",
    author = "Andreas, Jacob  and
      Rohrbach, Marcus  and
      Darrell, Trevor  and
      Klein, Dan",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1181",
    doi = "10.18653/v1/N16-1181",
    pages = "1545--1554",
}

@inproceedings{wang-etal-2021-efficient-test,
    title = "Efficient Test Time Adapter Ensembling for Low-resource Language Varieties",
    author = "Wang, Xinyi  and
      Tsvetkov, Yulia  and
      Ruder, Sebastian  and
      Neubig, Graham",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.63",
    doi = "10.18653/v1/2021.findings-emnlp.63",
    pages = "730--737",
    abstract = "Adapters are light-weight modules that allow parameter-efficient fine-tuning of pretrained models. Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b). However, this approach requires training a separate language adapter for every language one wishes to support, which can be impractical for languages with limited data. An intuitive solution is to use a related language adapter for the new language variety, but we observe that this solution can lead to sub-optimal performance. In this paper, we aim to improve the robustness of language adapters to uncovered languages without training new adapters. We find that ensembling multiple existing language adapters makes the fine-tuned model significantly more robust to other language varieties not included in these adapters. Building upon this observation, we propose Entropy Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions. Experiments on three diverse groups of language varieties show that our method leads to significant improvements on both named entity recognition and part-of-speech tagging across all languages.",
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@inproceedings{Shi2023,
author = {Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and Das, Dipanjan and Wei, Jason},
booktitle = {Proceedings of ICLR 2023},
eprint = {2210.03057},
file = {:Users/ruder/Documents/Papers/2210.03057.pdf:pdf},
title = {{Language Models are Multilingual Chain-of-Thought Reasoners}},
url = {http://arxiv.org/abs/2210.03057},
year = {2023}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{verga-etal-2021-adaptable,
    title = "Adaptable and Interpretable Neural {M}emory{O}ver Symbolic Knowledge",
    author = "Verga, Pat  and
      Sun, Haitian  and
      Baldini Soares, Livio  and
      Cohen, William",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.288",
    doi = "10.18653/v1/2021.naacl-main.288",
    pages = "3678--3691",
    abstract = "Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a {``}fact memory{''}. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5{\%} of the parameters. Most interestingly, we demonstrate that the model can be modified, without \textit{any} re-training, by updating the fact memory.",
}

@inproceedings{cheng-etal-2023-decouple,
    title = "Decouple knowledge from paramters for plug-and-play language modeling",
    author = "Cheng, Xin  and
      Lin, Yankai  and
      Chen, Xiuying  and
      Zhao, Dongyan  and
      Yan, Rui",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.901",
    doi = "10.18653/v1/2023.findings-acl.901",
    pages = "14288--14308",
    abstract = "Pre-trained language models (PLM) have made impressive results in a wide range of NLP tasks and it has been revealed that one of the key factors to their success is the parameters of these models implicitly learn various types of knowledge in the pre-training corpus.However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents us from understanding what kind of knowledge PLM needs to solve a certain task. In this paper, we introduce {pasted macro {`}MODEL{'}}, a pre-training model with differentiable plug-in memory (DPM). The key intuition behind is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the {pasted macro {`}MEMORY{'}}. We conduct extensive experiments under various settings to justify this design choice. In domain adaptation setting, {pasted macro {`}MODEL{'}} could be easily adapted to different domains with pluggable in-domain memory{---}obtaining 3.95 F1 improvements across four domains, without any in-domain training. {pasted macro {`}MODEL{'}} could also keep absorbing new knowledge after pre-training is done by knowledge updating operation in the {pasted macro {`}MEMORY{'}} without re-training. Finally, we show that by incorporating training samples into {pasted macro {`}MEMORY{'}} with knowledge prompting, {pasted macro {`}MODEL{'}} could further be improved by the instruction of in-task knowledge.",
}

@inproceedings{andor-etal-2019-giving,
    title = "Giving {BERT} a Calculator: Finding Operations and Arguments with Reading Comprehension",
    author = "Andor, Daniel  and
      He, Luheng  and
      Lee, Kenton  and
      Pitler, Emily",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1609",
    doi = "10.18653/v1/D19-1609",
    pages = "5947--5952",
    abstract = "Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable {`}programs{'} which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33{\%} absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.",
}

@article{trask2018neural,
  title={Neural arithmetic logic units},
  author={Trask, Andrew and Hill, Felix and Reed, Scott E and Rae, Jack and Dyer, Chris and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}