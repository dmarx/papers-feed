@misc{clark2018thinksolvedquestionanswering,
      title={{Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1803.05457}, 
}

@misc{lieber2024jambahybridtransformermambalanguage,
      title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
      author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
      year={2024},
      eprint={2403.19887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19887}, 
}

@misc{sakaguchi2019winograndeadversarialwinogradschema,
      title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1907.10641},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.10641}, 
}

@misc{paperno2016lambadadatasetwordprediction,
      title={The LAMBADA dataset: Word prediction requiring a broad discourse context}, 
      author={Denis Paperno and Germán Kruszewski and Angeliki Lazaridou and Quan Ngoc Pham and Raffaella Bernardi and Sandro Pezzelle and Marco Baroni and Gemma Boleda and Raquel Fernández},
      year={2016},
      eprint={1606.06031},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1606.06031}, 
}

@misc{deepseekai2024deepseekv2strongeconomicalefficient,
      title={{DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}}, 
      author={{DeepSeek AI} and others},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.04434}, 
}

@misc{sap2019socialiqacommonsensereasoningsocial,
      title={SocialIQA: Commonsense Reasoning about Social Interactions}, 
      author={Maarten Sap and Hannah Rashkin and Derek Chen and Ronan LeBras and Yejin Choi},
      year={2019},
      eprint={1904.09728},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09728}, 
}

@misc{gemmateam2024gemma2improvingopen,
      title={{Gemma 2: Improving Open Language Models at a Practical Size}}, 
      author={{Gemma Team} and others},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@misc{deepseekai2024deepseekllmscalingopensource,
      title={{DeepSeek LLM: Scaling Open-Source Language Models with Longtermism}}, 
      author={{DeepSeek AI} and others},
      year={2024},
      eprint={2401.02954},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.02954}, 
}

@misc{bai2023qwentechnicalreport,
      title={{Qwen Technical Report}}, 
      author={Jinze Bai and others},
      year={2023},
      eprint={2309.16609},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.16609}, 
}

@misc{peng2023rwkvreinventingrnnstransformer,
      title={{RWKV: Reinventing RNNs for the Transformer Era}}, 
      author={Bo Peng and others},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13048}, 
}

@misc{gemmateam2024gemmaopenmodelsbased,
      title={{Gemma: Open Models Based on Gemini Research and Technology}}, 
      author={{Gemma Team} and others},
      year={2024},
      eprint={2403.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08295}, 
}

@misc{ai2024yiopenfoundationmodels,
      title={{Yi: Open Foundation Models by 01.AI}}, 
      author={{01. AI} and others},
      year={2024},
      eprint={2403.04652},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.04652}, 
}

@misc{guo2024deepseekcoderlargelanguagemodel,
      title={{DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}}, 
      author={Daya Guo and Qihao Zhu and Dejian Yang and Zhenda Xie and Kai Dong and Wentao Zhang and Guanting Chen and Xiao Bi and Y. Wu and Y. K. Li and Fuli Luo and Yingfei Xiong and Wenfeng Liang},
      year={2024},
      eprint={2401.14196},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2401.14196}, 
}

@misc{jiang2023mistral7b,
      title={{Mistral 7B}}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{touvron2023llama2openfoundation,
      title={{Llama 2: Open Foundation and Fine-Tuned Chat Models}}, 
      author={Hugo Touvron and others},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{touvron2023llamaopenefficientfoundation,
      title={{LLaMA: Open and Efficient Foundation Language Models}}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{zellers2019hellaswagmachinereallyfinish,
      title={{HellaSwag: Can a Machine Really Finish Your Sentence?}}, 
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.07830}, 
}

@article{DBLP:journals/corr/abs-1911-11641,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA: Reasoning about Physical Commonsense in Natural Language}},
  journal      = {CoRR},
  volume       = {abs/1911.11641},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.11641},
  eprinttype    = {arXiv},
  eprint       = {1911.11641},
  timestamp    = {Thu, 11 Apr 2024 13:33:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-11641.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{lai2017racelargescalereadingcomprehension,
      title={{RACE: Large-scale ReAding Comprehension Dataset From Examinations}}, 
      author={Guokun Lai and Qizhe Xie and Hanxiao Liu and Yiming Yang and Eduard Hovy},
      year={2017},
      eprint={1704.04683},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1704.04683}, 
}

@misc{clark2019boolqexploringsurprisingdifficulty,
      title={{BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}}, 
      author={Christopher Clark and Kenton Lee and Ming-Wei Chang and Tom Kwiatkowski and Michael Collins and Kristina Toutanova},
      year={2019},
      eprint={1905.10044},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.10044}, 
}

@article{DBLP:journals/corr/abs-2005-14165,
  author       = {Tom B. Brown and
                  others},
  title        = {{Language Models are Few-Shot Learners}},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sap-etal-2022-neural,
    title = "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large {LM}s",
    author = "Sap, Maarten  and
      Le Bras, Ronan  and
      Fried, Daniel  and
      Choi, Yejin",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.248",
    doi = "10.18653/v1/2022.emnlp-main.248",
    pages = "3762--3780",
    abstract = "Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today{'}s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models{'} ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55{\%} and 60{\%} on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.",
}

@misc{mixtral22,
    author={{Mistral AI}},
    title={{Cheaper, Better, Faster, Stronger}},
    year={2024},
    url={https://mistral.ai/news/mixtral-8x22b/},
}

@inproceedings{Gordon2011ChoiceOP,
  title={Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning},
  author={Andrew S. Gordon and Zornitsa Kozareva and Melissa Roemmele},
  booktitle={AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:434646}
}

@misc{jiang2024mixtralexperts,
      title={{Mixtral of Experts}}, 
      author={Albert Q. Jiang and others},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.04088}, 
}

@misc{norm,
    author={Leo Gao},
    title={{Multiple Choice Normalization in LM Evaluation}},
    url={https://blog.eleuther.ai/multiple-choice-normalization/},
    year={2021},
}

@misc{eval-harness,
  author       = {Gao, Leo and others},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@misc{grattafiori2024llama3herdmodels,
      title={{The Llama 3 Herd of Models}}, 
      author={Aaron Grattafiori and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={{Measuring Massive Multitask Language Understanding}}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{mihaylov2018suitarmorconductelectricity,
      title={{Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}}, 
      author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
      year={2018},
      eprint={1809.02789},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1809.02789}, 
}