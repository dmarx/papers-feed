---
abstract: |
  ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.
author:
- |
  Łukasz Borchmann  
  Snowflake AI Research  
  `lukasz.borchmann@snowflake.com`
bibliography:
- anthology.bib
- custom.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: |
  In Case You Missed It:  
  ARC ‘Challenge’ Is Not That Challenging
---





# Introduction

A substantial set of benchmarks regularly employed in LLM testing consists of multiple-choice problems, commonly considered in a setup where each provided option is scored under the model, and the one with the highest likelihood is compared against the gold standard to determine accuracy. This refers, among others, to popular evaluators of MMLU , ARC Easy and Challenge , BoolQ , RACE , OpenBookQA , PIQA , SIQA , COPA , and HellaSwag .

<figure id="fig:difference">
<span class="image placeholder" data-original-image-src="images/difference_vertical.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>Difference between <span><strong>ARC Challenge</strong></span> and <span><strong>ARC Easy</strong></span> accuracies when considering each answer separately compared to seeing all options. The gap is vastly reduced, up to six times in this comparison.</figcaption>
</figure>

Details of this setup differ but generally follow one of the two conventions. Under one convention, the model considers each candidate answer in <span style="background-color: separation">separation</span> , without alternative options displayed (Figure ), while under the other, the model sees all candidate <span style="background-color: all">options</span> together in the prompt (Figure ). We argue that the first setup is commonly overused and rarely preferred since it does not simulate the natural reasoning context in which multiple choices are compared directly. Importantly, it introduces a false notion of how challenging a particular problem is, as switching from the first to the second might result in a 35% improvement in model accuracy, as shown in Section  experiments.

## Hardly answerable in separation

Consider the question, ‘Which of these items contains only a solution?’ Given the option ‘a jar of pickles,’ confronting a single item with a question and assessing whether pickles fulfill the definition of the solution suffices. They do not, so this option is incorrect. The question can be addressed under both evaluation setups because it does not require the availability of other options, such as ‘a can of mixed fruit.’

In contrast, some questions inherently demand comparative evaluation: let us think about ‘Which of these most likely has the *greatest* mass?’ and the option ‘puppy.’ This question’s answer cannot be determined without comparing the mass of the ’puppy’ to the masses of all other provided options. It is the greatest compared to ‘chicken’ or ‘lizard’ but not in the context of ‘horse’ or ‘elephant.’ Though it can work to some extent, relying on the likelihood assigned in <span style="background-color: separation">separation</span> to each of the animals is an unreasonable way of determining the heaviest one. It feels natural to provide the model with the <span style="background-color: all">options</span> to choose from instead because it allows the model to directly compare and contextualize choices, reflecting a more authentic reasoning process. This aspect, however, is commonly overlooked.

Specifically, such ‘hardly answerable in separation’ questions are prevalent in ARC datasets, constituting 21% of ARC Easy and 31% of ARC Challenge (see Appendix ). Despite this fact, it is widespread to evaluate them without seeing all of the options simultaneously .

# Impact on evaluation results

Figure  shows the difference in model accuracy when options are presented in isolation versus all at once. Not surprisingly, different setups hugely change the evaluation results, partly because of the vast presence of ‘hardly answerable in separation’ questions and partially because such a setup, equivalent to what human test takers see, doesn’t introduce unnecessary obstacles.

For example, switching from <span style="background-color: separation">separation</span> to <span style="background-color: all">options</span> improves the Llama 3.1 70B ARC Challenge accuracy from 64% to 93%, rendering this ARC subset significantly less challenging. Moreover, since the procedure change has a much higher impact on ARC Challenge than on ARC Easy, switching reduces the accuracy gap between these subsets as much as six-fold (Figure ). These findings suggest that the previously perceived difficulty was primarily an artifact of the evaluation method rather than the tasks’ complexity.

The difference seems somewhat known in the LLM community, but not broadly, and needs to be stated explicitly. E.g. concerning the Llama family, authors seem to silently switch from <span style="background-color: separation">separation</span> to <span style="background-color: all">options</span> between Llama 2 and Llama 3, similar to Mistral between Mixtral 8x7B and Mixtral 8x22B, or DeepSeek before their V2 (detailed assessment available in Appendix ).

# Are other benchmarks affected?

Yes. Analogous changes in evaluation procedures would vastly improve OpenBookQA scores. Concerning Llama 3.1 70B, one can achieve improvement from 48% to 89% (see Figure ). For some reason, most authors who switched from <span style="background-color: separation">separation</span> to <span style="background-color: all">options</span> in ARC evaluation did not follow on some other multi-choice problems.

<figure id="fig:openbookqa">
<span class="image placeholder" data-original-image-src="images/openbookqa.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption><span><strong>OpenBookQA</strong></span> evaluation results depending on whether the model sees other options or considers each answer separately. In a setup with options, current models outperform human test takers.</figcaption>
</figure>

<figure id="fig:siqa">
<span class="image placeholder" data-original-image-src="images/siqa.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption><span><strong>SIQA</strong></span> evaluation results depending on whether the model sees other options or considers each answer separately. Reformulation leads to up to 24% improvement.</figcaption>
</figure>

If they switched, they could notice that OpenBookQA is essentially solved, as current models achieve scores above human performance (92% compared to 96% of Qwen 2.5 72B).

In the case of SIQA, reformulation leads to a 24% increase in Llama 70B accuracy. However, the best models perform 5% below the human baseline, suggesting room for improvement (Figure ).

These dramatic increases, however, call into question previous interpretations of model capability on both SIQA and OpenBookQA.

# Why does it matter?

We argue that the benchmark’s challenge should result from the inherent complexity of the knowledge or reasoning required, not its formulation or evaluation procedure.

The <span style="background-color: separation">separation</span> setup is unnecessarily complicated and not consistent with how humans would approach the multi-choice problem, leading to existing assessments of human performance being incompatible. For example, the fact that strong LLMs perform 30% worse than humans on SIQA doesn’t mean they are deficient in commonsense reasoning about social situations if under <span style="background-color: all">options</span> the difference largely disappears. This mismatch can falsely suggest deficits in reasoning capabilities that are not truly present.

Notably, the gap between LLMs and humans on SIQA has been previously used to argue that LLMs might lack social intelligence and social commonsense .

# Suggestions for multi-choice eval

There are many arguments for using the <span style="background-color: all">options</span> for the evaluation of multi-choice QA problems. We have already described a few, including the presence of ‘hardly answerable in separation’ questions and the fact it is consistent with the usual approach to assessing human performance, as humans naturally consider all choices in a single context.

Other benefits include enabling compatible evaluation in a likelihood and generative manner, allowing one to obtain comparable scores with LLMs behind closed and limited APIs. Moreover, it eliminates the need to decide which normalization method to use when aggregating scores from several option tokens, which is, to some extent, arbitrary and impacts model ranking.

Nevertheless, it is not preferred in all cases commonly considered under the likelihood-scoring evaluation scheme.

## Why likelihood scoring in the first place?

Likelihood-based scoring is a natural choice for problems from pure language modeling, Winograd schemas, or fill-in-the-gap setups such as encountered in LAMBADA , HellaSwag , or WindoGrande datasets.

For other problems, it is effectively a variant of constrained decoding, that is, the model is restricted to selecting from given candidate options rather than generating open-ended text. It guarantees that models will not emit CoTs before answering the question and removes the need for output postprocessing, such as extracting the letter associated with the selected option and normalizing its casing. Moreover, it allows us to obtain meaningful results with base models, e.g. intermediate checkpoints from LLMs’ self-supervised pretraining since we are constraining the output to one of the most probable options under the model.

## To show, or not to show options

Suppose the options are of equal length, and it is not helpful to consider them simultaneously. This is the case when we deal with a straightforward yes/no response, and no comparative reasoning is necessary, as in the BoolQ dataset . In similar scenarios, there are no arguments for dropping <span style="background-color: separation">separation</span> in favor of <span style="background-color: all">options</span> .

We are in the position that the <span style="background-color: all">options</span> variant is preferred if there is a risk of a ‘hardly answerable in separation’ question presence (Section ) or it simply makes it easier to consider all of the options at once because it ensures the model can leverage direct comparisons. This seems to be the case for virtually all other multi-choice QA problems, such as MMLU , ARC , OpenBookQA , PIQA , or SIQA . In fact, most similar problems are already being evaluated in the <span style="background-color: all">options</span> scheme.

Arguably, <span style="background-color: all">options</span> has some possible or actual disadvantages: the order of the presented choices might impact the evaluation results (e.g. models might bias toward the first listed choice), it might be easier to exploit pattern recognition, and the setup requires slightly more compute. Nevertheless, we consider these to be outweighed by benefits and recommend broad use of the <span style="background-color: all">options</span> scheme.

# Limitations

Despite our recommendations and the benefits they entail, several limitations and uncertainties exist in identifying precisely how evaluation methods were employed in previously reported results.

Firstly, because authors of LLMs’ technical reports rarely or never report such details, our assessment of which of the <span style="background-color: all">options</span> and <span style="background-color: separation">separation</span> they employed is based on attempting to replicate reported accuracy scores under both setups and observing which condition aligns best (Appendix ). Fortunately, given the magnitude of the observed differences, the performance gap is so large that one can differentiate between the mentioned approaches with a high degree of certainty.

Secondly, in a search for the <span style="background-color: separation">separation</span> overuse candidates, we mainly relied on intuition. We considered the most popular benchmarks due to their widespread use and availability of performance data, later confirming the intuition experimentally. Though it was a tale of ARC, OpenBookQA, and SIQA, many other widely used benchmarks may benefit from revisiting their evaluation setup.

Finally, it could be a blog post, but it feels better to write a PDF.

# Summary

We draw the community’s attention to shifting from evaluating answers in isolation to evaluating them alongside all other options. Over the last year, such a change happened in the reported ARC Challenge and ARC Easy scores, vastly impacting their evaluation results. After discussing the implications, we considered whether other popular benchmarks might undergo similar reformulation, identifying OpenBookQA and SIQA as candidates. In the former, recent models outperform humans, even though there is a room of 40% between humans and LLMs in the widespread setup. The fact that the gap drastically narrows under the all-options evaluation method highlights how the testing format can distort perceived difficulty.

We concluded with a guideline for evaluating multi-choice problems, arguing that the setup where the model sees all options is preferred over considering each answer separately, except for casual or masked language modeling problems.

# Claiming setup used by other authors

As authors of LLM technical reports rarely or never provide such details, we redo their evaluations in <span style="background-color: all">options</span> and <span style="background-color: separation">separation</span> setups. If the reported score is in the same ballpark as one of these, and visibly distant from the other one, we claim they used the first. This assessment is backed by the notion that no other change in the prompt could cause a 20%+ improvement in ARC Challenge scores. The exception could be using a generative setup with CoT for some heavy reasoners, but we do not suspect authors to use CoT if they are not reporting this because it would be a serious flaw.

The results of this analysis for the ARC Challenge are presented in Table . Prompt ‘reverse engineering’ becomes more troublesome in the context of SIQA and OpenBookQA datasets (Table -) as some authors do not directly report scores but average them with other commonsense reasoning problems. We’re not claiming any setup for these.

Finally, some authors tackling OpenBookQA followed in normalizing the likelihood by the likelihood of the completion given ‘Answer:’ as context. To address this possibility, we introduce two additional variants referred to as <span style="background-color: brown">separation $_\mathrm{b}$</span> and <span style="background-color: brown2">options $_\mathrm{b}$</span> .

# Estimating number of questions hardly answerable in separation

To determine whether questions are answerable given a single option or require the context of other options, we process them in batches of 20 using `gpt-4o-2024-11-20` model and the following prompt with few-shot examples:

    Consider the question, "Which of these items contains only a solution?" Given the option "a jar of pickles," confronting a single item with a question and assessing whether pickles fulfill the definition of the solution suffices. They do not, so this option is incorrect. 

    Now let us think about "Which of these most likely has the greatest mass?" and the option "puppy." It can be considered only with other options because it is the greatest compared to "chicken" or "lizard" but not in the context of "horse" or "elephant".

    These questions represent two classes of questions: "answerable without other options" and "unanswerable without other options".

    Other examples of "answerable without other options" are:
    - Kerry made a simple flashlight. She recorded the following statements in her lab book. Which statement is an inference? (Answerable, because it suffices to compare options against the definition of inference)
    - A scientist on a field trip discovered a new organism. She examined its cells under a microscope and observed several different structures, including a nucleus, a cell wall, and some chloroplasts. This organism would correctly be classified in which of the following kingdoms? (Answerable, because it can be answered by deciding if the kingdom provided in the option can be associated with having a nucleus, a cell wall, and chloroplasts)
    - Many types of motion occur in our solar system. Which type of motion describes one Earth year? (Answerable, because it suffices to validate if the motion describes one year or not)
    - When trees develop leaves in the spring, 10 changes occur on the forest floor. Why does the development of leaves cause changes on the forest floor? (Answerable, because it is enough to verify if a particular option described the possible cause of change)
    - Using a softball bat to hit a softball is an example of using which simple machine? (Answerable, because all one needs to do is to check if the described simple machine is the explanation of how a softball bat works)
    - Which is a statement about climate? (Answerable, because it is possible to verify a single option against the climate definition)
    - How do word processors on computers benefit most students? (Answerable, because it can be answered in separation whether most students benefit from this feature of the word processor)
    - Photosynthesis occurs in which of these organisms? (Answerable because it suffices to check if the organism mentioned in the option performs photosynthesis)
    - Which two theories of Moon formation propose that much or all of the material comprising the Moon came from Earth? (Because it suffices to validate if both theories mentioned in a single option describe the Moon as formed from Earth material)
    - Plants and animals are composed of organic compounds. Which of the following are the common elements found in organic compounds? (Answerable, because it suffices to check if the option consists of compounds appearing in both plants and animals)

    Other examples of "unanswerable without other options" are:
    - A ball is dropped from different heights. When the ball is dropped from the highest height, it makes the greatest noise or vibration when it lands on the ground. What is the best explanation for the ball making the greatest noise? (Unanswerable, because in order to choose the best explanation, one needs to consider several explanations)
    - If an experiment results in data that do not support the hypothesis, what is the most likely step to take next? (Unanswerable, because in order to choose the most likely step, one needs to consider the less likely alternative)
    - When an igneous intrusion comes into contact with surrounding rock, the surrounding rock will (Unanswerable, because one can easily verify if an option describes the possible outcome of contact with surrounding rock)
    - A research scientist writes a paper on the initial regrowth of a forest after a fire has damaged the entire ecosystem. Which title would be best for the paper? (Unanswerable, because it is impossible to decide the best title without comparing it to other titles)
    - Jessica wants to see cells in an oak tree leaf. Which tool is best for Jessica to use to see the cells? (Unanswerable, because choosing the best tool depends on the set of tools considered and is ambiguous without a complete list of options considered)
    - Which factor is most likely to cause the number of rabbits living in an area to increase? (Unanswerable, because choosing the most likely case requires checking all of the causes under consideration)

    Now classify the following statements either as "unanswerable" or "answerable" in separation.

    Answer in a form of JSONL file containing "question", "category", and "explanation" keys.

    Questions to classify:
    [List of 20 questions and choices]

The model returned batches of JSONL, such as:

    {"question": "Which best describes the structure of an atom?", "category": "unanswerable", "explanation": "Determining the best description requires comparing all options to identify the most accurate one."}
    {"question": "Which is a statement about climate?", "category": "answerable", "explanation": "It is possible to verify each option against the definition of climate to determine the correct answer."}
    {"question": "During which activity should a student wear goggles?", "category": "answerable", "explanation": "It suffices to check if the activity described in the option requires goggles for safety."}
    {"question": "Which natural event occurs with the most frequency?", "category": "unanswerable", "explanation": "Determining the most frequent event requires comparing the frequency of all listed events."}

During this procedure, we estimated the percentage as 21% for ARC Easy and 31% for ARC Challenge.

# Evaluation details

All evaluations were conducted using `lm_eval` `1170ef9` . We used HF implementations and base variants of models (exact versions in Table ) with either default prompts and `acc_norm` metric or prompts outlined below.

Inferences were performed with bf16 precision, flash attention (whenever available), and dynamic batch size, using `transformers` 4.47.0 and `torch` 2.5.1 on eight NVIDIA H100 GPUs.

<div id="tab:models">

| Model                               |     |
|:------------------------------------|:----|
| `huggyllama/llama-65b`              |     |
| `meta-llama/Llama-2-70b-hf`         |     |
| `meta-llama/Meta-Llama-3-70B`       |     |
| `mistralai/Mistral-7B-v0.1`         |     |
| `mistralai/Mixtral-8x7B-v0.1`       |     |
| `mistralai/Mixtral-8x22B-v0.1`      |     |
| `deepseek-ai/deepseek-llm-67b-base` |     |
| `deepseek-ai/DeepSeek-V2`           |     |
| `Qwen/Qwen-14B`                     |     |
| `01-ai/Yi-6B`                       |     |
| `google/gemma-7b`                   |     |
| `google/gemma-2-27b`                |     |

Exact variants of models used for evaluation.

</div>

Concerning ARC Easy and Challenge datasets, for the <span style="background-color: separation">separation</span> setup, we follow the standard `lm_eval` configuration with:

    doc_to_text: "Question: {{question}}\nAnswer:"
    doc_to_target: "{{choices.label.index(answerKey)}}"
    doc_to_choice: "{{choices.text}}"

In contrast, for the <span style="background-color: all">options</span> setup, we use:

    doc_to_text: !function arc_utils.doc_to_text
    doc_to_target: "{{choices.label.index(answerKey)}}"
    doc_to_choice: "{{choices.label}}"

with `doc_to_text()` defined as:

    def doc_to_text(doc):
        prompt = "Question: " + doc["question"] + "\nOptions:\n"
        for l, t in zip(doc["choices"]["label"], doc["choices"]["text"]):
            prompt += l + '. ' + t + '\n'
        prompt += "Answer: "
        return prompt

Analogous changes were introduced to OpenBookQA and SIQA templates.
