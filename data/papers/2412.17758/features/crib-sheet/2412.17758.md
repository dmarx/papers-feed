- **ARC Challenge vs. ARC Easy**: ARC Challenge appears more difficult due to evaluation setup rather than inherent complexity; switching evaluation methods can significantly alter perceived difficulty.
  
- **Evaluation Setup**: Two conventions exist for evaluating multiple-choice problems:
  - **Separation**: Model evaluates each answer independently.
  - **Options**: Model evaluates all candidate answers simultaneously, reflecting a more natural reasoning process.

- **Impact of Evaluation Method**: 
  - Switching from separation to options can improve model accuracy by up to 35%.
  - Example: Llama 3.1 70B accuracy on ARC Challenge improves from 64% (separation) to 93% (options).

- **Hardly Answerable in Separation**: 
  - 21% of ARC Easy and 31% of ARC Challenge questions are inherently comparative and cannot be answered effectively in separation.
  
- **Model Performance Discrepancies**: 
  - Previous evaluations may falsely imply reasoning deficits due to the separation method.
  - Example: Llama 3.1 70B scores 48% on OpenBookQA in separation but 89% in options.

- **Recommendations for Multi-Choice Evaluation**:
  - Use options for evaluation to ensure models can leverage direct comparisons.
  - This method aligns with human test-taking behavior and reduces unnecessary complexity.

- **Limitations of Current Evaluations**: 
  - Lack of transparency in reporting evaluation methods in LLM studies complicates understanding of performance metrics.
  - Many benchmarks may benefit from reevaluation under the options method.

- **Conclusion**: 
  - The evaluation setup significantly influences perceived model capabilities; a shift to options is recommended for more accurate assessments of reasoning and knowledge.