- **ARC Challenge vs. ARC Easy**: ARC Challenge appears more difficult due to evaluation setup rather than inherent complexity; direct comparison of answer choices is crucial.
  
- **Evaluation Setup**: Two conventions exist:
  - **Separation**: Model evaluates each answer independently.
  - **Options**: Model evaluates all candidate answers together, simulating natural reasoning.

- **Impact of Evaluation Method**: 
  - Switching from separation to options can improve model accuracy significantly (e.g., Llama 3.1 70B accuracy from 64% to 93% on ARC Challenge).
  - The accuracy gap between ARC Challenge and Easy can be reduced by up to six-fold.

- **Hardly Answerable in Separation**: 
  - 21% of ARC Easy and 31% of ARC Challenge questions are inherently comparative, making them difficult to answer in separation.

- **Model Performance**: 
  - Performance on benchmarks like OpenBookQA and SIQA can improve dramatically with the options setup (e.g., Llama 3.1 accuracy on OpenBookQA from 48% to 89%).

- **Guidelines for Multi-Choice Evaluation**:
  - Prefer options setup for multi-choice QA problems to reflect true model capabilities.
  - Use likelihood scoring for language modeling tasks, but options are better for comparative reasoning.

- **Limitations**: 
  - Lack of transparency in reporting evaluation methods in LLM studies complicates understanding of performance metrics.
  
- **Conclusion**: 
  - Shift to evaluating answers alongside all options is recommended to avoid misinterpretation of model capabilities and perceived difficulty of tasks.