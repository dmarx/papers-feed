\section{Conclusion}
\label{sec:conclusion}

We introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length.
When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains,
where it matches or exceeds the performance of strong Transformer models.
We are excited about the broad applications of selective state space models to build foundation models for different domains, especially in emerging modalities requiring long context such as genomics, audio, and video.
Our results suggest that Mamba is a strong candidate to be a general sequence model backbone.
