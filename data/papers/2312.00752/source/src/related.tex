\section{Discussion: Selection Mechanism}
\label{sec:discussion:selection}

Our selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence.
It can also be viewed as related to ``fast weights''~\citep{schmidhuber1992learning,ba2016using}, which connects classical RNNs with the mechanism of linear attention~\citep{schlag2021linear}.
However, we believe that it is a distinct concept that is worth clarifying.

\paragraph{Gating.}
Gating originally referred to the gating mechanisms of RNNs such as the LSTM~\citep{lstm} and GRU~\citep{chung2014empirical},
or the gated equation \iftoggle{arxiv}{\eqref{eq:gates}}{} in \cref{thm:gating}.
This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN.
In particular, this affects the propagation of signal through time and causes inputs to interact along the sequence length dimension.

However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction (often with an activation function).
For example, \emph{elementwise} multiplicative components of neural network architectures (that do not interact along sequence length) are now commonly referred to as gated architectures~\citep{hua2022transformer,mehta2023long}, despite a very different meaning than the original RNN sense.
Thus we believe the original concept of \emph{RNN gating} versus the popular usage of \emph{multiplicative gating} actually have a very different semantic meaning.

\paragraph{Hypernetworks.}
Hypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks.
The original idea~\citep{ha2017hypernetworks} used it in a narrow sense to define a large RNN whose recurrent parameters are generated by a smaller RNN,
and other variants have been around for a long time~\citep{schmidhuber1992learning}.

\paragraph{Data-dependence.}
Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the model depend on the data~\citep{poli2023hyena}.

\paragraph{Example: GLU Activation.}
To illustrate the issues with these concepts, consider a simple diagonal linear layer $y = \bm{D}x$, where $\bm{D}$ is a diagonal weight parameter.
Now suppose that $\bm{D}$ is itself generated from a linear transformation of $x$, with an optional nonlinearity: $\bm{D} = \sigma(\bm{W} x)$.
Since it is diagonal, the multiplication becomes an elementwise product: $y = \sigma(\bm{W} x) \circ x$.

This is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative ``branch''),
hypernetworks (since the parameter $\bm{D}$ is generated by another layer), and data-dependent (since $\bm{D}$ depends on the data $x$).
However, this in fact simply defines a GLU function,
which is so simple that it is often considered just an activation function~\citep{dauphin2017language,shazeer2020glu}
instead of a meaningful layer.

\paragraph{Selection.}
Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating, hypernetworks, or data-dependence, so can an enormous range of other constructions---essentially anything with a multiplication, including standard attention mechanisms~\citep{bahdanau2015neural,vaswani2017attention} as well---and we find it uninformative to think of them as such.

Instead, we view it as most closely related to the gating mechanism of traditional RNNs,
which is a special case (\cref{thm:gating}) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of $\dt$ \citep{funahashi1993approximation,tallec2018can,gu2020hippo}.
We also eschew the term ``gating'' in favor of \emph{selection} to clarify the overloaded use of former.
More narrowly, we use selection to refer to the \emph{mechanistic} action of a model to select or ignore inputs and facilitate data interaction along the sequence length (\cref{sec:method:motivation}).
Beyond selective SSMs and gated RNNs, other examples may include input-dependent convolutions~\citep{yang2019condconv,lioutas2020time,kosma2023time,lutati2023focus} and even attention.




\section{Related Work}
\label{sec:related}

We overview several prior works related to our methods.
We mention that some of the most closely related models include recurrent layers such as S4, S5, and quasi-RNNs;
as well as end-to-end architectures such as H3, RetNet, and RWKV.

\subsection{S4 Variants and Derivatives}

We describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our method.

\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
  \item S4~\citep{gu2021combining,gu2022efficiently} introduced the first structured SSM, describing diagonal structure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a connection to continuous-time online memorization (HIPPO)~\citep{gu2020hippo}.
  \item DSS~\citep{gupta2022diagonal} first discovered the empirical effectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D~\citep{gu2022parameterization}.
  \item S5~\citep{smith2023s5} independently discovered the diagonal SSM approximation, and is the first S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to MIMO (multi-input multi-output) formulation.
    Our proposed S6 shares the scan, but differs by (i) keeping the SISO dimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the computation issue, (iii) adding the selection mechanism.

    \citet{lu2023structured} applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories.
    Their mechanism can be viewed as a particular hard-coded instance of a selection mechanism,
    where $\dA$ is manually set to $0$, instead of our learnable mechanism that depends on the input.
    It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries.
  \item Mega~\citep{ma2023mega} introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of being an exponential moving average (EMA).
    They additionally make an interesting connection of the discretization step of SSMs to an EMA \emph{damping} term.
    Contrary to findings in the original S4 papers, this was the first model to show that real-valued SSMs are empirically effective in certain settings or when combined with different architectural components.
  \item Liquid S4~\citep{hasani2023liquid} is also motivated by augmenting S4 with an input-dependent state transition. From this perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally and close to LTI.
  \item SGConv~\citep{li2023makes}, Hyena~\citep{poli2023hyena}, LongConv~\citep{fu2023simple}, MultiresConv~\citep{shi2023sequence}, and Toeplitz Neural Network~\citep{qin2023toeplitz} all focus on the convolutional representation of S4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do fast autoregressive inference directly.
\end{itemize}

Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually strictly LTI (linear time invariant).

\subsection{SSM Architectures}

We use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures
incorporating one of the previous SSMs as a black box layer.

\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
  \item GSS \citep{mehta2023long} was the first gated neural network architecture incorporating SSMs. It is motivated by the gated attention unit (GAU) of \citet{hua2022transformer} and looks quite similar to our block, except with additional projections. Most importantly, its projection \emph{contracts} the model dimension to reduce the state size of the SSM,
    while ours \emph{expands} the model dimension in order to increase the state size, based on the motivation in \cref{sec:method:motivation}.
  \item Mega \citep{ma2023mega} combined the EMA simplification of S4 described above into a hybrid architecture using an efficient attention approximation.
  \item H3~\citep{dao2023hungry} is motivated by combining S4 with linear attention \citep{katharopoulos2020transformers}. It is the first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures.
  \item Selective S4 \citep{wang2023selective} incorporates S4 as a black box to generate a binary mask which is multiplied on the input.
    While sharing the ``selection'' name, we consider this an architectural modification that is closer to architectural gating than a selection mechanism (\cref{sec:discussion:selection}).
    For example, we hypothesize that it would not solve the Selective Copying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0).
  \item RetNet~\citep{sun2023retentive} is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a special case where the state dimension is $N=1$.
    Although not framed as such, its recurrence can be viewed as a special case of a linear SSM.

    Its primary source of improvement is using a linear attention with large \emph{head dimension}, which can be viewed as another method to perform input-dependent state expansion.
    Using a larger head dimension in the context of linear attention variants was first done by H3, but not extensively used since this requires a proportional amount of extra computation.
    RetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention instead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.
  \item RWKV~\citep{peng2023rwkv} is another recent RNN designed for language modeling. It is based on AFT (attention-free Transformer~\citep{zhai2021attention}), another variant of linear attention. Its main ``WKV'' mechanism involves LTI recurrences and can be seen as the ratio of two SSMs.
\end{itemize}

We also highlight the gated attention unit (GAU) from \citet{hua2022transformer},
which was motivated by combining the Transformer's MHA and MLP blocks together and was an
inspiration for our architecture (\cref{sec:method:architecture}) combining the H3 and MLP blocks.

\subsection{Relationship to RNNs}

RNNs and SSMs are broadly related, as they both involve the concepts of \emph{recurrence} on a latent \emph{state}.


Several older RNNs such as the strongly typed RNN \citep{balduzzi2016strongly}, quasi-RNN (QRNN) \citep{bradbury2016quasi}, and simple recurrent unit (SRU) \citep{lei2017simple,lei2021attention} involve forms of gated RNNs without time-wise nonlinearities.
Because of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs above.
The main differences are:
\begin{itemize}
  \item They do not use state expansion ($N=1$) or selective $\B, \C$ parameters, both of which are important for performance (\cref{sec:exp:ablations}).
  \item They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism + discretization (\cref{thm:gating}).
    The connections to principled SSM theory provides better parameterizations and initializations\iftoggle{arxiv}{ (\cref{sec:method:details})}{}.
\end{itemize}

Additionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem~\citep{hochreiter1991untersuchungen,hochreiter2001gradient,pascanu2013difficulty},
both caused by their sequential nature.
The former could be solved for some of the above RNNs by leveraging the parallel scan~\citep{martin2018parallelizing},
but the latter was difficult without theory later developed for SSMs.
For example, modern structured SSMs differ in more careful parameterization of the recurrent dynamics inspired by classical SSM theory (e.g.\ through discretization~\citep{gu2021combining,gu2023train}), or direct analysis~\citep{orvieto2023resurrecting,kaul2020linear,gupta2022simplifying}).

We also note that there is a long line of work on orthogonal RNNs~\citep{arjovsky2016unitary,henaff2016recurrent,mhammedi2017efficient,vorontsov2017orthogonality,lezcano2019cheap}
which are motivated by constraining the $\dA$ transition matrix to be orthogonal or unitary,
in order to control its eigenvalues and prevent the vanishing gradient problem.
However, these had other limitations; we believe that these stem from the fact that orthogonal/unitary RNNs are also LTI.
For example, they are almost always evaluated on the Copying task which they can solve perfectly, but observed to struggle on the Selective Copying task~\citep{jing2019gated}.

\subsection{Linear Attention}

The Linear Attention (LA)~\citep{katharopoulos2020transformers} framework is an important result popularizing kernel attention and showing how it relates to recurrent autoregressive models.
Many variants have proposed alternative kernels and other modifications.
Random Feature Attention (RFA)~\citep{peng2021random} chooses the kernel feature map to approximate softmax attention (i.e. the $\exp$ feature map) using the random Fourier feature approximation of Gaussian kernels~\citep{rahimi2007random}.
Performer~\citep{choromanski2021rethinking} finds an approximation to the exponential kernel involving only positive features, which also allows the softmax normalization term.
TransNormer~\citep{qin2022devil} showed that the LA denominator term can be unstable and proposed replacing it with a LayerNorm.
cosFormer~\citep{qin2022cosformer} augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality.
Linear Randomized Attention~\citep{zheng2022linear} generalize RFA from the perspective of importance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the $\exp$-transformed numerator).

Aside from kernel attention, many other variants of efficient attention exist; the survey \citet{tay2022efficient} offers an extensive categorization of many of these.


\subsection{Long Context Models}

Long context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences.
However, these are often from a computational standpoint and have not been extensively validated.
These include:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
  \item Recurrent Memory Transformer~\citep{bulatov2023scaling}, a lightweight wrapper around a Transformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main result is similar to our Induction Heads extrapolation experiment (\cref{fig:induction}).
  \item LongNet~\citep{ding2023longnet}, which claimed to scale to 1B length but only evaluated on length $<100K$ for actual tasks.
  \item Hyena and HyenaDNA~\citep{poli2023hyena,nguyen2023hyenadna}, which claimed to leverage up to 1M context. However, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if quality improvements at 1M context are due to context length or due to more data and computation.
  \item Sparse Transformer~\citep{child2019generating} showed a proof-of-concept of using a strided sparse attention Transformer to model audio waveforms of length $2^{20}=1048576$, although did not discuss performance tradeoffs when controlling for computation and model size.
\end{itemize}
In contrast, we believe this work presents one of the first approaches to meaningfully demonstrate increasing performance with longer context.


%
