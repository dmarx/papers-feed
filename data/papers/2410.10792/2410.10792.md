---
abstract: |
  Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) *inversion* and (ii) *editing* of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.
author:
- |
  Litu Rout$^{1,2}$, Yujia Chen$^{1}$, Nataniel Ruiz$^{1}$,  
  **Constantine Caramanis**$^{2}$, **Sanjay Shakkottai**$^{2}$, **Wen-Sheng Chu**$^{1}$  
  $^1$ Google, $^2$ UT Austin  
  `{litu.rout,constantine,sanjay.shakkottai}@utexas.edu`  
  `{yujiachen,natanielruiz,wschu}@google.com`
bibliography:
- iclr2025_conference.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations
---





# Introduction

Vision generative models typically transform noise into images. Inverting such models, given a reference image, involves finding the structured noise that can regenerate the original image. Efficient inversion must satisfy two crucial properties. First, the structured noise should produce an image that is *faithful* to the reference image. Second, the resulting image should be easily *editable* using new prompts, allowing fine modifications over the image.

Diffusion Models (DMs) have become the mainstream approach for generative modeling of images , excelling at sampling from high-dimensional distributions . The sampling process follows a Stochastic Differential Equation known as reverse SDE . Notably, these models can invert a given image. Recent advances in DM inversion have shown a significant impact on conditional sampling, such as stroke-to-image synthesis , image editing  and stylization .

Despite its widespread usage, DM inversion faces critical challenges in *faithfulness* and *editability*. First, the stochastic nature of the process requires fine discretization of the reverse SDE , which increases expensive Neural Function Evaluations (NFEs). Coarse discretization, on the other hand, leads to less faithful outputs , even with deterministic methods like DDIM . Second, nonlinearities in the reverse trajectory introduce unwanted drift, reducing the accuracy of reconstruction . While existing methods enhance faithfulness by optimizing latent variables  or prompt embeddings , they tend to be less efficient, harder to edit, and rely on complex attention processors to align with a given prompt . These added complexities make such methods less suitable for real-world deployment.

For inversion and editing, we introduce a zero-shot conditional sampling algorithm using Rectified Flows (RFs) , a powerful alternative to DMs. Unlike DMs, where sampling is governed by a reverse SDE, RFs use an Ordinary Differential Equation known as reverse ODE, offering advantages in both efficient training and fast sampling. We construct a controlled forward ODE, initialized from a given image, to generate the initial conditions for the reverse ODE. The reverse ODE is then guided by an optimal controller, obtained through solving a Linear Quadratic Regulator (LQR) problem. We prove that the resulting new vector fields have a stochastic interpretation with an appropriate drift and diffusion. We evaluate RF inversion on stroke-to-image generation and image editing tasks, and show extensive qualitative results on other applications like cartoonization. Our method significantly improves photo realism in stroke-to-image generation, surpassing a state-of-the-art (SoTA) method  by 89%, while maintaining faithfulness to the input stroke. In addition, we show that RF inversion outperforms DM inversion  in faithfulness by 4.7% and in realism by 13.8% on LSUN-bedroom dataset . and show the qualitative results of our approach and a graphical illustration, respectively.

Our theoretical and practical contributions can be summarized as:

- We present an efficient inversion method for RF models, including Flux, that requires no additional training, latent optimization, prompt tuning, or complex attention processors.

- We develop a new vector field for RF inversion, interpolating between two competing objectives: consistency with a possibly corrupted input image, and consistency with the “true" distribution of clean images (§). We prove that this vector field is equivalent to a rectified SDE that interpolates between the stochastic equivalents of these competing objectives (§). We extend the theoretical results to design a stochastic sampler for Flux.

- We demonstrate the faithfulness and editability of RF inversion across three benchmarks: (i) LSUN-Bedroom, (ii) LSUN-Church, and (iii) SFHQ, on two tasks: stroke-to-image synthesis and image editing. In addition, we provide extensive qualitative results and conduct large-scale human evaluations to assess user preference metrics (§).

<figure id="fig:graph">
<p>[]</p>
<p><span><span class="image placeholder" data-original-image-src="pics/graph-v2.pdf" data-original-image-title="" width="40%">image</span></span> </p>
<figcaption> Graphical model illustrating (a) DDIM inversion and (b) RF inversion. Due to nonlinearities in DM trajectory, the DDIM inverted latent <span><span class="math inline">\(\rvx_1\)</span></span> significantly deviates from the original image <span> <span class="math inline">\(\rvy_0\)</span></span>. RF inversion without controller reduces this deviation, resulting in <span><span class="math inline">\(\rvx_1\)</span></span>. With controller, RF inversion further eliminates the reconstruction error, making <span><span class="math inline">\(\rvx_1\)</span></span> nearly identical to <span> <span class="math inline">\(\rvy_0\)</span></span>, which enhances the faithfulness. </figcaption>
</figure>

# Related Works

**DM Inversion.** Diffusion models have become the mainstream approach for generative modeling, making DM inversion an exciting area of research . Among training-free methods, SDEdit  adds noise to an image and uses the noisy latent as structured noise. For semantic image editing based on a given prompt, it simulates the standard reverse SDE starting from this structured noise. SDEdit requires no additional parameter training, latent variable optimization, or complex attention mechanisms. However, it is less faithful to the original image because adding noise in one step is equivalent to linear interpolation between the image and noise, while the standard reverse SDE follows a nonlinear path .

An alternate method, DDIM inversion , recursively adds predicted noise at each forward step and returns the final state as the structured noise (illustrated by $Y_t$ process in (a)). However, DDIM inversion often deviates significantly from the original image due to nonlinearities in the drift and diffusion coefficients, as well as inexact score estimates . To reduce this deviation, recent approaches optimize prompt embeddings  or latent variables , but they have high time complexity. Negative prompt inversion  speeds up the inversion process but sacrifices faithfulness. Methods like CycleDiffusion  and Direction Inversion  use inverted latents as references during editing, but they are either computationally expensive or not applicable to rectified flow models like Flux or SD3 .

**DM Editing.** Efficient inversion is crucial for real image editing. Once a structured noise is obtained by inverting the image, a new prompt is fed into the T2I generative model. Inefficient inversion often fails to preserve the original content and therefore requires complex editing algorithms. These editing algorithms can be broadly classified into (i) attention control, such as prompt-to-prompt , plug-and-play (PnP) , (ii) optimization-based methods like DiffusionCLIP , DiffuseIT , STSL , and (iii) latent masking to edit specific regions of an image using masks provided by the user  or automatically extracted from the generative model . We focus on efficient inversion, avoiding the need for complex editing algorithms.

**Challenges in RF Inversion.** Previous inversion or editing approaches have been tailored towards diffusion models and do not directly apply to SoTA rectified flow models like Flux. This limitation arises because the network architecture of Flux is MM-DiT , which is fundamentally different from the traditional UNet used in DMs . In MM-DiT, text and image information are entangled within the architecture itself, whereas in UNet, text conditioning is handled via cross-attention layers. Additionally, Flux primarily uses T5 text encoder, which lacks an aligned latent space for images, unlike CLIP encoders. Therefore, extending these prior methods to modern T2I generative models requires a thorough investigation. We take the first step by inverting and editing a given image using Flux.

**RF Inversion and Editing.** DMs  traditionally outperform RFs  in high-resolution image generation. However, recent advances have shown that RF models like Flux can surpass SoTA DMs in text-to-image (T2I) generation tasks . Despite this, their inversion and editing capabilities remain underexplored. In this paper, we introduce an efficient RF inversion method that avoids the need for training additional parameters , optimizing latent variables , prompt tuning , or using complex attention processors . While our focus is on inversion and editing, we also show that our framework can be easily extended to generative modeling.

**Filtering, Control and SDEs.** There is a rich literature on the connections between nonlinear filtering, optimal control and SDEs . These connections are grounded in the Fokker-Planck equation , which RF methods  heavily exploit in sampling. Our study focuses on rectified flows for conditional sampling, and shows that the resulting drift field also has an optimal control interpretation.

# Method

## Preliminaries

In generative modeling, the goal is to sample from a target distribution $p_0$ given a finite number of samples from that distribution. Rectified flows  represent a class of generative models that construct a source distribution $q_0$ and a time varying vector field $v_t(\rvx_t)$ to sample $p_0$ using an ODE: $$\begin{aligned}
    \label{eq:gen-ode}
    % \tag{RF-ODE}
    \mathrm{d}X_t = v_t(X_t) \mathrm{d}t,\quad X_0 \sim q_0, \quad t\in [0,1].
\end{aligned}$$ Starting from $X_0=\rvx_0$, the ODE is integrated from $t:0\rightarrow 1$ to yield a sample $\rvx_1$ distributed according to $p_0$ (e.g., the distribution over images). A common choice of $q_0$ is standard Gaussian $\gN\left(0,I\right)$ and $v_t\left(X_t\right) = -u(X_t,1-t;\varphi)$, where $u$ is a neural network parameterized by $\varphi$. The neural network is trained using the conditional flow matching objective as discussed below.

**Training Rectified Flows.** To train a neural network to serve as the vector field for the ODE , we couple samples from $p_0$ with samples from $q_0$ – which we call $p_1$ to simplify the notation – via a linear path: $Y_t = t Y_1 + (1-t)Y_0$. The resulting marginal distribution of $Y_t$ becomes: $$\begin{aligned}
    \label{eq:p-yt-marginal}
    p_t(\rvy_t) = \E_{Y_1 \sim p_1}\left[p_t(\rvy_t|Y_1) \right] = \int p_t(\rvy_t|\rvy_1) p_1(\rvy_1) \mathrm{d}\rvy_1.
\end{aligned}$$ Given an initial state $Y_0 = \rvy_0$ and a terminal state $Y_1=\rvy_1$, the linear path induces an ODE: $\mathrm{d}Y_t = u_t\left(Y_t|\rvy_1\right) \mathrm{d}t$ with the conditional vector field $u_t\left(Y_t|\rvy_1\right) = \rvy_1 - \rvy_0$. The marginal vector field is derived from the conditional vector field using the following relation : $$\begin{aligned}
    \label{eq:ut-marginal}
    u_t(\rvy_t) = \E_{Y_1 \sim p_1}\left[u_t\left(\rvy_t|Y_1\right) \frac{p_t(\rvy_t|Y_1)}{p_t(\rvy_t)} \right]
    = \int u_t\left(\rvy_t|\rvy_1\right) \frac{p_t(\rvy_t|\rvy_1)}{p_t(\rvy_t)} p_1(\rvy_1) \mathrm{d}\rvy_1.
\end{aligned}$$ We can then use a neural network $u(\rvy_t,t;\varphi)$, parameterized by $\varphi$, to approximate the marginal vector field $u_t(\rvy_t)$ through the flow matching objective defined as: $$\begin{aligned}
    \label{eq:fm-loss}
    \gL_{FM}(\varphi)\coloneqq \E_{t\sim \gU[0,1],Y_t\sim p_t}\left[\left\| u_t(Y_t) - u(Y_t,t;\varphi) \right\|_2^2\right].
\end{aligned}$$ For tractability, we can instead consider a different objective, called conditional flow matching: $$\begin{aligned}
    \label{eq:cfm-loss}
    \gL_{CFM}(\varphi)\coloneqq \E_{t\sim \gU[0,1],Y_t\sim p_t(\cdot|Y_1), Y_1 \sim p_1}\left[\left\| u_t(Y_t|Y_1) - u(Y_t,t;\varphi) \right\|_2^2\right].
\end{aligned}$$ $\gL_{CFM}$ and $\gL_{FM}$ have the identical gradients (), and are hence equivalent. However, $\gL_{CFM}(\varphi)$ is computationally tractable, unlike $\gL_{FM}(\varphi)$, and therefore preferred during training. Finally, the required vector field in is computed as $v_t\left(X_t\right) = -u(X_t,1-t;\varphi)$. In this way, rectified flows sample a data distribution by an ODE with a learned vector field.

## Connection between Rectified Flows and Linear Quadratic Regulator

The unconditional rectified flows (RFs) (e.g., Flux) from Section § above, enable image generation by simulating the vector field $v_t(\cdot)$ initialized with a sample of random noise. Subsequently, by simulating the reversed vector field $-v_{1-t}(\cdot)$ starting from the image, we get back the sample of noise that we started with. We formalize this statement below.

<div class="proposition">

**Proposition 1**.  * Given an image $\rvy_0$ and the vector field $v_t(\cdot)$ of the generative ODE , suppose the structured noise $\rvy_1$ is obtained by simulating an ODE: $$\begin{aligned}
\label{eq:inv-wo-control}
    \mathrm{d}Y_t = u_t(Y_t) \mathrm{d}t, \quad Y_0 = \rvy_0, \quad t\in[0,1]. 
\end{aligned}$$ If $u_t(\cdot) = -v_{1-t}(\cdot)$ and $X_0 = \rvy_1$, then the ODE  recovers the original image, i.e., $X_1 = \rvy_0$.*

</div>

**Implication.** Rectified flows enable exact inversion of a given image when the vector field of the generative ODE  is precisely known. Employing ODE  for the structured noise and ODE  to transform that noise back into an image, RF inversion accurately recovers the given image.

Suppose instead that we start with a *corrupted image* and simulate the reversed vector field $-v_{1-t}(\cdot)$. Then we obtain a noise sample. There are two salient aspects of this noise sample. First, it is consistent with the original image: when processed through $v_{t}(\cdot)$ it results in the same corrupted image. Second, if the image sample is “atypical” (e.g., corrupted, or, say, a stroke painting as in §), then the sample of noise is also likely to be atypical. In other words, the noise sample is only consistent to the (possibly corrupted) image sample.

Our goal is to modify the pipeline above so that even when we start with a corrupted image, we can get back a clean image (see stroke-to-image synthesis in  ), but for this, we need to processs by $v_{t}(\cdot)$ a noise sample that is closer to being “typical”. More generally, the goal is to create a pipeline that supports semantic editing of real images (§), e.g., changing age, or gender without relying on additional training, optimization, or complex attention processors.

Thus, as a first step, we derive an optimal controller that takes a minimum energy path to convert any image $Y_0$ (whether corrupted or not) to a given sample of random noise $Y_1 \sim p_1$ – i.e., noise that is typical for $p_1$. Specifically, we consider optimal control in a $d$-dimensional vector space $\R^d$: $$\begin{aligned}
    \label{eq:gen-lqr}
    V(c) \coloneqq  \int_0^1 \frac{1}{2}\left\|c\left(Z_t, t\right)\right\|_2^2 \mathrm{d}t + \frac{\lambda}{2} \left\|Z_1 - Y_1\right\|_2^2, ~\mathrm{d}Z_t = c\left(Z_t,t\right) \mathrm{d}t, ~Z_0 =\rvy_0,~Y_1 \sim p_1,
\end{aligned}$$ where $\lambda$ is the weight assigned to the terminal cost and $V(c)$ denotes the total cost of the control $c: \R^d\times [0,1] \rightarrow \R^d$. The minimization of $V(c)$ over the admissible set of controls, denoted by $\gC$, is known as the Linear Quadratic Regulator (LQR) problem. The solution of the LQR problem is given in **Proposition **, which minimizes the quadratic transport cost of the dynamical system.

<div class="proposition">

**Proposition 2**.  * For $Z_0 =\rvy_0$ and $Y_1 = \rvy_1$, the optimal controller of the LQR problem , denoted by $c^*\left(\cdot,t\right)$ is equal to the conditional vector field $u_t\left(\cdot|\rvy_1\right)$ of the rectified linear path $Y_t = tY_1 + (1-t)Y_0$ when $Y_0 = \rvy_0$, i.e., $c^*\left(\rvz_t,t\right) = u_t\left(\rvz_t|\rvy_1\right) = (\rvy_1 - \rvz_t)/(1-t)$.*

</div>

## Inverting Rectified Flows With Dynamic Control

So far, we have two vector fields. The first, from the RFs, transforms an image $Y_0$ typical for distribution $p_0$ to a typical sample of random Gaussian noise $Y_1 \sim p_1.$ As discussed above, if the image sample is atypical, then the sample of noise is also likely to be atypical.

We also have a second vector field resulting from the optimal control formulation that transforms *any* image (whether corrupted or not) to a noise sample that is typical-by-design from the distribution $p_1$. Therefore, this sample, when passed through the rectified flow ODE  results in a “typical” image from the “true” distribution $p_0$. This image is clean, i.e., typical for $p_0$, but it is not related to the image $Y_0$. Our controlled ODE, defined below, interpolates between these two differing objectives – *consistency with the given (possibly corrupted) image, and consistency with the distribution of images $p_0$* – with a tunable parameter $\gamma$: $$\begin{aligned}
    \label{eq:controlled-ODE}
    \mathrm{d}Y_t = \Big[u_t(Y_t) + \gamma \left(u_t(Y_t|\rvy_1) - u_t(Y_t)\right) \Big] \mathrm{d}t, \quad Y_0 = \rvy_0,
\end{aligned}$$ where $u_t(Y_t|\rvy_1) = c^*(Y_t,t)$ is computed based on the insights from **Proposition **, and $u_t(Y_t) = -v_{1-t}(Y_t)$ as established in **Proposition **. Here, we call $\gamma \in [0,1]$ the *controller guidance*. Thus, ODE  generalizes to editing applications, while keeping its inversion accuracy comparable.

When $\gamma=1$, the drift field of the ODE  becomes optimal controller of LQR problem , ensuring that the structured noise $Y_1 = \rvy_1$ adheres to the distribution $p_1$. Consequently, initializing the generative ODE  with $\rvy_1$ results in samples with high likelihood under the data distribution $p_0$.

Conversely, when $\gamma = 0$, the system follows the ODE  described in **Proposition **, resulting a structured noise $Y_1$ that is not guaranteed to follow the noise distribution $p_1$. However, initializing the generative ODE  with this noise precisely recovers the reference image $\rvy_0$.

Beyond this vector field interpolation intuition, we show in the next section § that the controlled ODE  has an SDE interpretation. As is well known , SDEs are robust to initial conditions, in proportion to the variance of the additive noise. Specifically, errors propagate over time in an ODE initialized with an incorrect or corrupted sample. However, SDEs (Markov processes) under appropriate conditions converge to samples from a carefully constructed invariant distribution with reduced sensitivity to the initial condition, resulting in a form of robustness to initialization. As we see, the parameter $\gamma$ (the controller guidance) appears in the noise term to the SDE, thus the SDE analysis in the next section again provides intuition on the trade-off between consistency to the (corrupted) image and consistency to the terminal invariant distribution.

<div class="remark">

**Remark 3**.  * We note that our analysis extends to the case where $\gamma$ is time-varying, though we omit these results for simplicity of notation. This is useful in practice, especially when $\rvy_0$ is a corrupted image, because for large $\gamma$ the stochastic evolution  moves toward a sample from the invariant measure $\gN\left(0,I \right)$. This noise encodes clean images. Starting from this noise, the corresponding reverse process operates in pure diffusion mode, resulting in a clean image. As the process approaches the terminal state, $\gamma$ is gradually reduced to ensure that $\rvy_0$ is encoded through $u_t(\cdot)$ into the final structured noise sample.*

</div>

## Controlled Rectified Flows as Stochastic Differential Equations

An SDE  is known to have an equivalent ODE formulation  under certain regularity conditions . In this section, we derive the opposite: an SDE formulation for our controlled ODE  from §. Let $W_t$ be a $d$-dimensional Brownian motion in a filtered probability space $(\Omega, \gF, \{\gF_t\}, \mathbb{P})$.

<div class="theorem">

**Theorem 4**.  * Fix any $T \in (0, 1)$. For any $t \in [0, T]$, the controlled ODE  is explicitly given by: $$\begin{aligned}
\label{eq:ode-controlled-vector-field}
    \mathrm{d}Y_t = \left[- \frac{1}{1-t}\left(Y_t - \gamma  \rvy_1\right) - \frac{(1-\gamma)t}{1-t} \nabla \log p_t(Y_t)\right]\mathrm{d}t,\quad Y_0 \sim p_0.
\end{aligned}$$ Its density evolution is identical to the density evolution of the following SDE: $$\begin{aligned}
    \label{eq:sde-controlled-vector-field}
    \mathrm{d}Y_t = -\frac{1}{1-t}\left( Y_t -\gamma  \rvy_1\right) \mathrm{d}t + \sqrt{\frac{2(1-\gamma)t}{1-t}} \mathrm{d}W_t, \quad Y_0\sim p_0.
\end{aligned}$$ Finally, denoting $p_t(\cdot)$ as the marginal pdf of $Y_t$, the density evolution is explicitly given by: $$\begin{aligned}
    \frac{\partial p_t(Y_t)}{\partial t} = \nabla \cdot \left[\left( \frac{1}{1-t}\left(Y_t - \gamma  \rvy_1\right) + \frac{(1-\gamma)t}{1-t} \nabla \log p_t(Y_t)\right)p_t(Y_t)\right].
\end{aligned}$$*

</div>

**Properties of SDE .** Elaborating on the intuition discussed at the end of §, when the controller guidance parameter $\gamma=0$, it becomes the stochastic equivalent of the standard RFs; see **Lemma ** for a precise statement. The resulting SDE is given by $$\begin{aligned}
    \label{eq:sde-optimal-vector-field}
    \mathrm{d}Y_t = -\frac{1}{1-t}Y_t \mathrm{d}t + \sqrt{\frac{2t}{1-t}} \mathrm{d}W_t, \quad Y_0\sim p_0,
\end{aligned}$$ which improves faithfulness to the image $Y_0$. When $\gamma=1$, the SDE  solves the LQR problem and drives towards the terminal state $Y_1 = \rvy_1$. This improves the generation quality, because the sample $Y_1$ is from the correct noise distribution $p_1$ as previously discussed in §. Therefore, a suitable choice of $\gamma$ retains faithfulness while simultaneously applying the desired edits.

Finally, we assume $T=1-\delta$ for sufficiently small $\delta$ (such that $0<\delta \ll 1$) to avoid irregularities at the boundary. This is typically considered in practice for numerical stability (even for diffusion models). Thus, in practice, the final sample $\rvy_{1-\delta}$ is returned as $\rvy_1$.

**Comparison with DMs.** Analogous to the SDE , the stochastic noising process of DMs is typically modeled by the Ornstein-Uhlenbeck (OU) process, governed by the following SDE: $$\begin{aligned}
    \label{eq:ddpm}
    \mathrm{d}Y_t = -Y_t \mathrm{d}t + \sqrt{2} \mathrm{d}W_t.
\end{aligned}$$ The corresponding ODE formulation is given by: $$\begin{aligned}
    \label{eq:ddim}
    \mathrm{d}Y_t = \left[-Y_t - \nabla \log p_t(Y_t)\right] \mathrm{d}t .
\end{aligned}$$ Instead, our approach is based on rectified flows (), which leads to a different ODE and consequently translates into a different SDE. As an additional result, we formalize the ODE derivation in **Lemma **. In **Lemma **, we show that the marginal distribution of this ODE is equal to that of an SDE with appropriate drift and diffusion terms. In **Proposition **, we show that the stationary distribution of this new SDE  converges to the standard Gaussian $\gN(0,I)$ in the limit as $t\rightarrow 1$.

The standard OU process  interpolates between the data distribution at time $t=0$ and a standard Gaussian as $t \to \infty$. The SDE , however, interpolates between the data distribution at time $t=0$ and a standard Gaussian at $t = 1$. In other words, it effectively “accelerates” time as it progresses to achieve the terminal Gaussian distribution. This is accomplished by modifying the coefficients of drift and diffusion as in to depend explicitly on time $t$. Thus, a sample path of appears like a noisy line, unlike that of the OU process (see Appendix  for numerical simulations).

## Controlled Reverse Flow using Rectified ODEs and SDEs

In this section, we develop an ODE and an SDE similar to our discussions above, but for the reverse direction (i.e., from noise to images).

**Reverse process using ODE.** Starting from the structured noise $\rvy_1$ obtained by integrating the controlled ODE , we construct another controlled ODE  for the reverse process (i.e., noise to image). In this process, the optimal controller uses the reference image $\rvy_0$ for guidance: $$\begin{aligned}
    \label{eq:gen-ode-w-controller}
    \mathrm{d}X_t = \Big[v_t(X_t) + \eta \left( v_t(X_t|\rvy_0) - v_t(X_t) \right)\Big] \mathrm{d}t, \quad X_0 = \rvy_1, \quad t\in [0,1],
\end{aligned}$$ where $\eta\!\in\![0,1]$ is the *controller guidance parameter* as before that controls faithfulness and editability of the given image $\rvy_0$. Similar to the analysis in **Proposition **, $v_t(X_t|\rvy_0)$ is obtained by solving the modified LQR problem : $$\begin{aligned}
    \label{eq:gen-lqr-rev}
    V(c) =  \int_0^1 \frac{1}{2}\left\|c\left(Z_t, t\right)\right\|_2^2 \mathrm{d}t + \frac{\lambda}{2} \left\|Z_1 - \rvy_0\right\|_2^2, ~\mathrm{d}Z_t = c\left(Z_t,t\right) \mathrm{d}t, \quad Z_0 =\rvy_1.
\end{aligned}$$ Solving , we get $c(Z_t,t) = \frac{\rvy_0 - Z_t}{1-t}$. Our controller steers the samples toward the given image $\rvy_0$. Thus, the controlled reverse ODE  effectively reduces the reconstruction error incurred in the standard reverse ODE  of RF models (e.g. Flux).

**Reverse process using SDE.** Finally, in **Theorem **, we provide the stochastic equivalent of our controlled reverse ODE  for generation. Recall that we initialize with the terminal structured noise by running the controlled forward ODE , along with a reference image $\rvy_0$. As discussed above, we terminate the inversion process at a time $T = 1 - \delta$ for numerical stability, resulting in a vector $\rvy_{1-\delta}$. Our reverse SDE thus starts at a corresponding time $\delta$ with this vector $\rvy_{1-\delta}$ at initialization, and terminates at time $T' < 1.$

<div class="theorem">

**Theorem 5**.  * Fix any $T' \in (\delta,1)$, and for any $t \in [\delta, T']$, the density evolution of the controlled ODE  initialized at $X_0 = \rvy_{1-\delta}$ is identical to the density evolution of the following SDE: $$\begin{aligned}
    \label{eq:stoch-rect-flow-cond-sampling}
    \mathrm{d}X_t = \left[\frac{(1-t-\eta)X_t+\eta t \rvy_0}{t(1-t)} + \frac{2(1-t)(1-\eta)}{t}  \nabla \log p_{1-t}(X_t) \right] \mathrm{d}t + \sqrt{\frac{2(1-t)(1-\eta)}{t}}  \mathrm{d}W_t.
\end{aligned}$$ Furthermore, denoting $q_t(\cdot)$ as the marginal pdf of $X_t$, its density evolution is given by: $$\begin{aligned}
    \frac{\partial q_t(X_t)}{\partial t} = \nabla \cdot \left[-\left(\frac{1-t-\eta}{t(1-t)}X_t +  \frac{\eta}{1-t}\rvy_0 + \frac{(1-t)}{t} (1-\eta) \nabla \log p_{1-t}(X_t)\right)q_t(Y_t)\right].
\end{aligned}$$*

</div>

**Properties of SDE .** When the controller parameter $\eta = 0$, we obtain a stochastic sampler for the pre-trained Flux, as given in **Lemma ** and compared qualitatively in . This case of our SDE  corresponds to the stochastic variant of standard RFs . Our key contribution lies in conditioning on $X_1=\rvy_0$ for inverting rectified flows. Importantly, our explicit construction does not require additional training or test-time optimization, enabling for the first time an efficient sampler for zero-shot inversion and editing using Flux. When $\eta = 1$, the score term and Brownian motion vanish from the SDE . The resulting drift becomes $\frac{\rvy_0 - X_t}{1-t}$, the optimal controller for the LQR problem , exactly recovering the given image $\rvy_0$.

<div class="remark">

**Remark 6**.  * Similar to **Remark **, our analysis extends to the case when $\eta$ is time-varying. This is useful in editing, as it allows the flow to initially move toward the given image $\rvy_0$ by choosing a large $\eta$. As the flow approaches $\rvy_0$ on the image manifold, $\eta$ is gradually reduced, ensuring that the text-guided edits are enforced through the unconditional vector field $v_t(\cdot)$ provided by Flux.*

</div>

# Algorithm: Inversion and Editing via Controlled ODEs

We describe the algorithm for RF inversion and editing using our controlled ODEs and .

**Problem Setup.** The user provides a text “prompt" to edit reference content, which could be a corrupt or a clean image. For the corrupt image guide, we use the dataset from SDEdit , which contains color strokes to convey high-level details. In this setting, the reference guide $\rvy_0$ is typically not a realistic image under the data distribution $p_0$. The objective is to transform this guide into a more realistic image under $p_0$ while maintaining faithfulness to the original guide.

For the clean image guide, the user provides a real image $\rvy_0$ along with an accompanying text “prompt" to specify the desired edits. The task is to apply text-guided edits to $\rvy_0$ while preserving its content. Examples include face editing, where the text might instruct change in age or gender.

**Procedure.** Our algorithm has two key steps: **inversion** and **editing**. We discuss each step below.

**Inversion.** The first step involves computing the structured noise $\rvy_1$ by employing our controlled ODE , initialized at the reference content $Y_0 = \rvy_0$. To compute the unconditional vector field, we use the pre-trained Flux model $u\left(\cdot,\cdot,\cdot;\varphi \right)$, which requires three inputs: the state $Y_t$, the time $t$, and the prompt embedding $\Phi(\text{prompt})$. During the inversion process, we use null prompt in the Flux model, i.e., $u_t(\rvy_t) = u(\rvy_t, t,\Phi(\text{``"});\varphi)$. For the conditional vector field, we apply the analytical solution derived in **Proposition **. The inversion process yields a latent variable that is then used to initialize our controlled ODE , i.e., $X_0 = \rvy_1$. In this phase, we again use the null prompt to compute the vector field $v_t(\rvx_t) = -u(\rvx_t, 1-t,\Phi(\text{``"});\varphi)$: see for the final output.

**Editing.** The second step involves text-guided editing of the reference content $\rvy_0$. This process is governed by our controlled ODE , where the vector field is computed using the desired text prompt within Flux: $v_t(X_t)= -u(\rvx_t, 1-t,\Phi(\text{prompt});\varphi)$. The controller guidance $\eta$ in balances faithfulness and editability: higher $\eta$ improves faithfulness but limits editability, while lower $\eta$ allows significant edits at the cost of reduced faithfulness. Consequently, the controller guidance $\eta$ provides a smooth interpolation between faithfulness and editability, a crucial feature in semantic image editing. Motivated by **Remark ** and ****, we consider a time-varying controller guidance $\eta_t$, such that for a fixed $\eta \in [0,1]$ and $\tau \in [0,1]$, $\eta_t = \eta ~\forall t \leq \tau$ and $0$ otherwise. illustrates the effect of controller guidance $\eta$ for $\tau = 0.3$; see Appendix  for a detailed ablation study.

<figure id="fig:control-strength">
<p><span class="image placeholder" data-original-image-src="pics/ablation-control-strength_low.pdf" data-original-image-title="" width="\linewidth">image</span> </p>
<figcaption> <strong>Effect of controller guidance <span class="math inline">\(\eta\)</span></strong> given the original image and the prompt: “A young man". Increasing <span class="math inline">\(\eta\)</span> improves the faithfulness to the original image, which is reconstructed at <span class="math inline">\(\eta\!=\!1\)</span>. </figcaption>
</figure>

<figure id="fig:stroke2image">
<p><span class="image placeholder" data-original-image-src="pics/stroke2image-small_low.pdf" data-original-image-title="" width="\linewidth">image</span> </p>
<figcaption> <strong>Stroke2Image generation.</strong> Our method generates photo-realistic images of bedroom or church given stroke paints, showing robustness to initial corruptions. </figcaption>
</figure>

<figure id="fig:glass-edit">
<p><span class="image placeholder" data-original-image-src="pics/glass-edit-small_low.pdf" data-original-image-title="" width="\linewidth">image</span> </p>
<figcaption> <strong>Image editing for adding face accessories.</strong> Prompt: “face of a man/woman wearing glasses". The proposed method better preserves the identity while applying the desired edits.  </figcaption>
</figure>

# Experimental Evaluation

We show that RF inversion outperforms DM inversion across three benchmarks: LSUN-church, LSUN-bedroom , and SFHQ  on two tasks: Stroke2Image generation and semantic image editing. Stroke2Image generation shows the robustness of our algorithm to initial corruption. In semantic image editing, we emphasize the ability to edit clean images without additional training, optimization, or complex attention processors.

**Baselines.** As this paper focuses on inverting flows, we compare with SoTA inversion approaches, such as NTI , DDIM Inversion , and SDEdit . We use the official NTI implementation for both NTI and DDIM inversion, and Diffusers library for SDEdit. Hyper-parameters for all these baselines are tuned for optimal performance. We compare with NTI for both direct prompt change and with prompt-to-prompt  editing. All methods are training-free; however, NTI  solves an optimization problem at each denoising step during inversion and uses P2P  attention processor during editing. We follow the evaluation protocol from SDEdit . More qualitative results and comparison are in Appendix §.

**Stroke2Image generation.** As discussed in §, our goal is to generate a photo-realistic image from a stroke paint (a corrupted image) and the text prompt “photo-realistic picture of a bedroom". In this case, the high level details in the stroke painting guide the reverse process toward a clean image.

In , we compare RF inversion (ours) with DM inversions. DM inversions propagate the corruption from the stroke painting into the structured noise, which leads to outputs resembling the input stroke painting. NTI optimizes null embeddings to align the reverse process with the DDIM forward trajectory. Although adding P2P to the NTI pipeline helps localized editing as in Figure , for corrupted images, it drives the reverse process even closer to the corruption. In contrast, our controlled ODE  yields a structured noise that is consistent with the corrupted image and also the invariant terminal distribution, as discussed in §, resulting in more realistic images.

In Table , we show that our method outperforms prior works in faithfulness and realism. On the test split of LSUN bedroom dataset, our approach is 4.7% more faithful and 13.79% more realistic than the best optimization free method SDEdit-SD1.5. Ours is 73% more realistic than the optimization-based method NTI, but comparable in L2. As discussed, NTI+P2P gets closer toward the corrupt image, which gives a very low L2 error, but the resulting image becomes unrealistic. Our approach is 89% more realistic than NTI+P2P. We observe similar gains on LSUN church dataset.

**User study.** We conduct a user study using Amazon Mechanical Turk to evaluate the overall performance of the our method. With 3 responses for each question, we collected in total 9,000 comparisons from 126 participants. As given in Table , our method outperforms all the other baselines by at least 59.67% in terms of overall satisfaction. More details are provided in Appendix §.

<figure id="fig:sem-edit-all">
<span class="image placeholder" data-original-image-src="pics/semantic-edit-all-v2_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> Editing (a) stylized expression, (b) age, (c) gender, and (d) object insert. Given an original image and a text prompt, our algorithm performs semantic image editing in the wild.  </figcaption>
</figure>

**Semantic Image Editing.** Given a *clean image* and a text “prompt", the objective is to modify the image according to the given text while preserving the contents of the image (identity for face images). In rectified linear paths, editing from a noisy latent becomes straightforward, further enhancing the efficiency of our approach. Compared with SoTA approaches (), our method requires no additional optimization or complex attention processors as in NTI +P2P. Thus, it is more efficient than a current SoTA approach, and importantly, more faithful to the original image while applying the desired edits.

In Table , we show that our method outperforms the optimization-free methods by at least 29% in face reconstruction, 6.6% in DINO patch-wise similarity, and 26.4% in CLIP-Image similarity while being comparable in prompt alignment metric CLIP-T. Importantly, our approach offers 54.11% gain in runtime, though it uses a larger ($\sim$<!-- -->12X) model, while staying comparable to NTI+P2P.

In , we showcase four complex editing tasks: (a) prompt-based stylization with the prompt: “face of a boy in disney 3d cartoon style", where facial expressions, such as “laugh" or “angry" are used for editing; (b) ability to control the age of a person; (c) interpolating between two concepts: “A man" $\leftrightarrow$ “A woman"; (d) sequentially inserting pepperoni and mushroom to an image of a pizza. We provide more examples of editing in the wild in Appendix §.

**Comparison using the same backbone: Flux.** In , we compare our method with SDEdit and DDIM inversion both adapted to Flux. NTI optimizes null embeddings to align with forward latents before applying text-guided edits via P2P, an approach well-suited for DMs that use both null and text embedding. However, this strategy cannot be applied to Flux, as it does not explicitly use null embedding. Consequently, we only reimplement SDEdit and DDIM inversion for Flux and compare them to our method. Since all methods leverage the same generative model, the improvements clearly stem from our controlled ODEs, grounded in a solid theoretical foundation (§).

<div id="tab:sfhq">

<table>
<caption> <strong>Quantitative results for face editing</strong> on SFHQ for “wearing glasses". </caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Method</strong></th>
<th style="text-align: center;"><strong>Face Rec.</strong> <span class="math inline">\(\downarrow\)</span></th>
<th style="text-align: center;"><strong>DINO</strong> <span class="math inline">\(\uparrow\)</span></th>
<th style="text-align: center;"><strong>CLIP-T</strong> <span class="math inline">\(\uparrow\)</span></th>
<th style="text-align: center;"><strong>CLIP-I</strong> <span class="math inline">\(\uparrow\)</span></th>
<th style="text-align: center;"><strong>Runtime(s)</strong> <span class="math inline">\(\downarrow\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">SDEdit-SD1.5</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">SDEdit-Flux</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">24</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DDIM Inv.</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">15</td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>NTI</p></td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">78</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>NTI+P2P</p></td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">85</td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>Ours</p></td>
<td style="text-align: center;"><span>0.442</span></td>
<td style="text-align: center;"><span>0.951</span></td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">39</td>
</tr>
</tbody>
</table>

**Quantitative results for face editing** on SFHQ for “wearing glasses".

</div>

# Conclusion

We present the first *efficient* approach for inversion and editing with the state-of-art rectified flow models such as Flux. Our method interpolates between two vector fields: *(i)* the unconditional RF field that transforms a “clean” image to “typical” noise, and *(ii)* a conditional vector field derived from optimal control that transforms *any* image (clean or not) to “typical” noise. Our new field thus navigates between these two competing objectives of *consistency with the given (possibly corrupted) image, and consistency with the distribution of clean images*. Theoretically, we show that this is equivalent to a new rectified SDE formulation, sharing this intuition of interpolation. Practically, we show that our method results in state-of-art zero-shot performance, without the need of additional training, optimization of latent variables, prompt tuning, or complex attention processors. We demonstrate the effectiveness of our method in stroke-to-image synthesis, face editing, object insertion, and stylization tasks, with large-scale human evaluation confirming user preference.

**Limitation.** The lack of comparison with *expensive* diffusion-based editing solutions may be viewed as a limitation. However, these implementations are either not available for Flux or not directly applicable due to Flux’s distinct multi-modal architecture. The key contribution of this paper lies in its theoretical foundations, validated using standard benchmarks and relevant baselines.

**Reproducibility.** The pseudocode and hyper-parameter details have been provided to reproduce the reported results in this paper.

# Broader Impact Statement

Semantic image inversion and editing have both positive and negative social impacts.

On the positive side, this technology enables (i) the generation of photo-realistic images from high level descriptions, such as stroke paintings, and (ii) the modification of clean images by changing various attributes like the age, gender, or adding glasses (§).

On the negative side, it can be misused by malicious users to manipulate photographs of individuals with inappropriate or offensive edits. Additionally, it carries the inherent risks associated with the underlying generative model.

To mitigate the negative social impacts, we enable safety features such as NSFW filters in the underlying generative model. Furthermore, we believe watermarking images generated by this technology can reduce misuse, especially in inversion and editing applications.

# Acknowledgments

This research has been supported by NSF Grant 2019844, a Google research collaboration award, and the UT Austin Machine Learning Lab.

# Additional Theoretical Results

In this section, we present the theoretical results omitted from the main draft due to space constraints. We formalize the ODE derivation of the standard rectified flows in **Lemma **.

<div class="lemma">

**Lemma 7**.  * Given a coupling $(Y_0, Y_1) \sim p_0 \times p_1$, consider the noising process $Y_t = tY_1 + (1-t)Y_0$. Then, the rectified flow ODE formulation with the optimal vector field is given by $$\begin{aligned}
\label{eq:ode-optimal-vector-field}
    \mathrm{d}Y_t = \left[ -\frac{1}{1-t}Y_t - \frac{t}{1-t} \nabla \log p_t(Y_t) \right]\mathrm{d}t,\quad Y_0 \sim p_0.
\end{aligned}$$ Furthermore, denoting $p_t(\cdot)$ as the marginal pdf of $Y_t$, its density evolution is given by: $$\begin{aligned}
    \label{eq:ode-optimal-vf-fpe}
    \frac{\partial p_t(Y_t)}{\partial t}= \nabla \cdot \left[\left( \frac{1}{1-t}Y_t + \frac{t}{1-t}~\nabla \log p_t(Y_t) \right)p_t(Y_t)\right].
\end{aligned}$$*

</div>

In **Lemma **, we show that the marginal distribution of the rectified flow  is equal to that of an SDE with appropriate drift and diffusion terms.

<div class="lemma">

**Lemma 8**.  * Fix any $T \in (0, 1)$, and for any $t \in [0, T]$, the density evolution of the rectified flow model is identical to the density evolution of the following SDE: $$\begin{aligned}
    \label{eq:app-sde-optimal-vector-field}
    \mathrm{d}Y_t = -\frac{1}{1-t}Y_t \mathrm{d}t + \sqrt{\frac{2t}{1-t}} \mathrm{d}W_t, \quad Y_0\sim p_0.
\end{aligned}$$*

</div>

In **Proposition **, we show that the stationary distribution of the SDE  converges to the standard Gaussian $\gN(0,I)$ in the limit as $t\rightarrow 1$.

<div class="proposition">

**Proposition 9**.  * Fix any $T \in (0, 1)$, and for any $t \in [0, T]$, the density evolution for the rectified flow ODE is same as that of the SDE . Furthermore, denoting $p_t(\cdot)$ as the marginal pdf of $Y_t$, its stationary distribution $p_t(Y_t) \propto \exp{(-\frac{\left\|Y_t\right\|^2}{2t})}$, which converges to $\gN\left(0,I\right)$ as $t\rightarrow1$.*

</div>

We note that Lemma  and Lemma  follow from the duality between the heat equation and the continuity equation , where it is classically known that one can interpret a diffusive term as a vector field that is affine in the score function, and vice-versa. This connection has been carefully used to study a large family of stochastic interpolants (that generalize rectified flows) in , and which can lead to a family of ODE-SDE pairs. In the lemmas above, we have provided explicit coefficients that have been directly derived, instead of using the stochastic interpolant formulation. Our key contribution lies in constructing a controlled ODEs  and , along with their equivalent SDEs  and in Theorem  and Theorem , respectively. This aids faithfulness and editability as discussed in §.

In **Lemma **, we derive a rectified SDE that transforms noise into images by reversing the stochastic equivalent of rectified flows .

<div class="lemma">

**Lemma 10**.  * Fix any small $\delta \in (0, 1)$, and for any $t \in [\delta, 1]$, the process $X_t$ governed by the SDE: $$\begin{aligned}
    \label{eq:stoch-rect-flow-sampling}
    \mathrm{d}X_t = \left[\frac{1}{t}X_t + \frac{2(1-t)}{t} \nabla \log p_{1-t}(X_t) \right] \mathrm{d}t + \sqrt{\frac{2(1-t)}{t}} \mathrm{d}W_t, \quad X_0 \sim p_1,
\end{aligned}$$ is the time-reversal of the SDE .*

</div>

**Implication.** The reverse SDE  provides a stochastic sampler for SoTA rectified flow models like Flux. Unlike diffusion-based generative models that explicitly model the score function $\nabla \log p_{t}(\cdot)$ in , rectified flows model a vector field, as discussed in §. However, given a neural network $u(\rvy_t,t;\varphi))$ approximating the vector field $u_t(\rvy_t)$, **Lemma ** offers an explicit formula for computing the score function: $$\begin{aligned}
    \label{eq:flux-to-score}
    \nabla \log p_t(Y_t) = -\frac{1}{t} Y_t - \frac{1-t}{t}u(Y_t,t;\varphi).
\end{aligned}$$ This score function is used to compute the drift and diffusion coefficients of the SDE , resulting in a practically implementable stochastic sampler for Flux. This extends the applicability of Flux to downstream tasks where SDE-based samplers have demonstrated practical benefits, as seen in diffusion models .

# Technical Proofs

This section contains technical proofs of the theoretical results presented in this paper.

## Proof of Proposition 

<div class="proof">

*Proof.* The standard approach to solving an LQR problem is the minimum principle theorem that can be found in control literature . We follow this approach and provide the full proof below for completeness.

The Hamiltonian of the LQR problem  is given by $$\begin{aligned}
    H(\rvz_t, \rvp_t, \rvc_t, t) = \frac{1}{2}\left\|\rvc_t\right\|^2 + \rvp_t^T\rvc_t.
\end{aligned}$$ For $\rvc_t^* = -\rvp_t$, the Hamiltonian attains its minumum value: $H(\rvz_t, \rvp_t, \rvc_t^*, t) = -\frac{1}{2}\left\|\rvp_t \right\|^2$. Using minimum principle theorem , we get $$\begin{aligned}
\label{eq:lqr-1}
\frac{\mathrm{d}\rvp_t}{\mathrm{d}t} 
    &= \nabla_{\rvz_t} H\left(\rvz_t, \rvp_t, \rvc_t^*, t\right) = 0;\\
    \label{eq:lqr-2}
    \frac{\mathrm{d}\rvz_t}{\mathrm{d}t} 
    &= \nabla_{\rvp_t} H\left(\rvz_t, \rvp_t, \rvc_t^*, t\right) = -\rvp_t;\\
    \label{eq:lqr-3}
    \rvz_0 &= \rvy_0;\\
    \label{eq:lqr-4}
    \rvp_1 &= \nabla_{\rvz_1}  \left( \frac{\lambda}{2} \left\|\rvz_1 - \rvy_1\right\|_2^2\right) = \lambda \left(\rvz_1 - \rvy_1 \right).
\end{aligned}$$ From , we know $\rvp_t$ is a constant $\rvp$. Using this constant in and integrating from $t\rightarrow1$, we have $\rvz_1 = \rvz_t - \rvp (1-t)$. Substituting $\rvz_1$ in , $$\begin{aligned}
    \rvp = \lambda (\rvz_t - \rvp(1-t)-\rvy_1) = \lambda (\rvz_t - \rvy_1) - \lambda (1-t)\rvp,
\end{aligned}$$ which simplifies to $$\begin{aligned}
    \rvp = \left(1+\lambda (1-t)\right)^{-1} \lambda (\rvz_t - \rvy_1)
    = \left(\frac{1}{\lambda}+(1-t)\right)^{-1} (\rvz_t - \rvy_1).
\end{aligned}$$ Taking the limit $\lambda \rightarrow \infty$, we get $\rvp = \frac{\rvz_t - \rvy_1}{1-t}$ and the optimal controller $\rvc_t^* = \frac{\rvy_1 - \rvz_t}{1-t}$. Since $u_t(\rvz_t | \rvy_1) = \rvy_1 - \rvy_0$, the proof follows by substituting $\rvy_0 = \frac{\rvz_t - t\rvy_1}{1-t}$. ◻

</div>

## Proof of Proposition 

<div class="proof">

*Proof.* Initializing the generative ODE  with the structured noise $\rvy_1$, we get $$\begin{aligned}
\label{eq:inv-wo-contrl-1}
    \frac{\mathrm{d}X_t}{\mathrm{d}t} = v_t(X_t), \quad X_0 =\rvy_1, \quad \forall t\in[0,1].
\end{aligned}$$ Substituting $u_t(\cdot) = -v_{1-t}(\cdot)$ in ODE , $$\begin{aligned}
    \frac{\mathrm{d}Y_t}{\mathrm{d}t} 
    = u_t(Y_t)
    = -v_{1-t}(Y_t),\quad Y_0 = \rvy_0, \quad \forall t\in[0,1].
\end{aligned}$$ Replacing $t\rightarrow (1-t)$, $$\begin{aligned}
\label{eq:inv-wo-contrl-2}
    \frac{\mathrm{d}Y_{1-t}}{\mathrm{d}t} 
    = v_{t}(Y_{1-t}), \quad \forall t\in[0,1].
\end{aligned}$$ Since and hold $\forall t\in[0,1]$ and $X_0 = \rvy_1$, then $X_t = Y_{1-t}$ that implies $X_1 = Y_0 = \rvy_0$. ◻

</div>

## Proof of Theorem 

<div class="proof">

*Proof.* From **Proposition **, we have $u_t(Y_t|Y_1) = \frac{Y_1 - Y_t}{1-t}$. In **Lemma **, we show that $$\begin{aligned}
    u_t(Y_t) = \left[ -\frac{1}{1-t}Y_t - \frac{t}{1-t} \nabla \log p_t(Y_t) \right].
\end{aligned}$$ Now, the controlled ODE  becomes: $$\begin{aligned}
    \mathrm{d}Y_t 
    & = \Big[u_t(Y_t) + \gamma \left(u_t(Y_t|Y_1) - u_t(Y_t)\right) \Big] \mathrm{d}t, \quad Y_0 \sim p_0, \quad Y_1 = \rvy_1\\
    & =  \left[(1-\gamma)\left( -\frac{1}{1-t}Y_t - \frac{t}{1-t} \nabla \log p_t(Y_t)\right) + \gamma \left(\frac{Y_1 - Y_t}{1-t}\right)\right]\mathrm{d}t\\
    & =  \left[-\frac{1}{1-t}Y_t - \frac{t}{1-t} (1-\gamma)\nabla \log p_t(Y_t) + \frac{\gamma}{1-t}Y_1\right]\mathrm{d}t\\
    & =  \left[-\frac{1}{1-t}\left(Y_t - \gamma Y_1\right) - \frac{t}{1-t}(1-\gamma) \nabla \log p_t(Y_t)\right]\mathrm{d}t.
\end{aligned}$$ Using continuity equation , the density evolution of the controlled ODE  then becomes: $$\begin{aligned}
\label{eq:sde-controlled-vector-field-1}
    \frac{\partial p_t(Y_t)}{\partial t} = \nabla \cdot \left[\left( \frac{1}{1-t}\left(Y_t - \gamma Y_1\right) + \frac{t}{1-t} (1-\gamma)\nabla \log p_t(Y_t)\right)p_t(Y_t)\right].
\end{aligned}$$ Applying Fokker-Planck equation  to the SDE , we have $$\begin{aligned}
    \frac{\partial p_t(Y_t)}{\partial t} 
    + \nabla \cdot \left[\left(-\frac{1}{1-t}\left( Y_t -\gamma Y_1\right) \right) p_t(Y_t) \right] 
    = \nabla \cdot \left[ \frac{t}{1-t}(1-\gamma) \nabla p_t(Y_t)\right],
\end{aligned}$$ which can be rearranged to equal completing the proof. ◻

</div>

## Proof of Lemma 

<div class="proof">

*Proof.* Given $(Y_0, Y_1) \sim p_0 \times p_1$, the conditional flow matching loss  can be reparameterized as: $$\begin{aligned}
    \gL_{CFM}(\varphi) \coloneqq \E_{t\sim \gU[0,1],(Y_0,Y_1)\sim p_1 \times p_0}\left[\left\| (Y_1 - Y_0) - u(Y_t,t;\varphi) \right\|_2^2\right],\quad Y_t = t Y_1+(1-t)Y_0,
\end{aligned}$$ where the optimal solution is given by the minimum mean squared estimator: $$\begin{aligned}
    \label{eq:ode-optimal-vector-field-1}
    u_t(\rvy_t) = \E_{(Y_0,Y_1)\sim p_1 \times p_0}\left[Y_1 - Y_0 | Y_t=\rvy_t\right].
\end{aligned}$$ Since $Y_t = tY_1 + (1-t)Y_0$, we use Tweedie’s formula  to compute $$\begin{aligned}
    \label{eq:ode-optimal-vector-field-2}
    \E\left[Y_0 | Y_t=\rvy_t\right] = \frac{1}{1-t}\rvy_t + \frac{t^2}{1-t} \nabla \log p_t(\rvy_t).
\end{aligned}$$ Using the above relation, we obtain the following: $$\begin{aligned}
    \nonumber
    \E\left[Y_1 | Y_t=\rvy_t\right] 
    & = \frac{1}{t} \E\left[Y_t - (1-t)Y_0 | Y_t = \rvy_t\right]\\
    \nonumber
    & = \frac{1}{t} \left(\rvy_t - (1-t) \E\left[Y_0 | Y_t=\rvy_t\right] \right)\\
    \nonumber
    & = \frac{1}{t}\left(\rvy_t - (1-t) \left(\frac{1}{1-t}\rvy_t + \frac{t^2}{1-t} \nabla \log p_t(\rvy_t) \right) \right)\\
    \label{eq:ode-optimal-vector-field-3}
    & = -t ~\nabla \log p_t(\rvy_t).
\end{aligned}$$ Combining and using linearity of expectation, we get $$\begin{aligned}
    u_t(\rvy_t) 
    & = \E\left[Y_1 | Y_t=\rvy_t\right] - \E\left[Y_0 | Y_t=\rvy_t\right] \\
    & =  -t ~\nabla \log p_t(\rvy_t) - \frac{1}{1-t}\rvy_t - \frac{t^2}{1-t} \nabla \log p_t(\rvy_t)\\
    & = -\frac{1}{1-t} \rvy_t - \frac{t}{1-t} \nabla \log p_t(\rvy_t),
\end{aligned}$$ The density evolution of $Y_t$ now immediately follows from the continuity equation applied to . ◻

</div>

## Proof of Lemma 

<div class="proof">

*Proof.* The Fokker-Planck equation of the SDE  is given by $$\begin{aligned}
    \label{eq:fokker-planck-controlled-sde}
    \frac{\partial p_t(Y_t)}{\partial t} + \nabla \cdot \left[ - \frac{1}{1-t}Y_t~p_t(Y_t) \right] = \nabla \cdot \left[ \frac{t}{1-t}~\nabla p_t(Y_t) \right].
\end{aligned}$$ Rearranging by multiplying and dividing $p_t(Y_t)$ in the right hand side, we get $$\begin{aligned}
    \label{eq:fokker-planck-controlled-sde-1}
    \frac{\partial p_t(Y_t)}{\partial t} = \nabla \cdot \left[\left( \frac{1}{1-t}Y_t + \frac{t}{1-t}~\nabla \log p_t(Y_t) \right)p_t(Y_t)\right].
\end{aligned}$$ To conclude, observe that that the density evolution above is identical to . ◻

</div>

## Proof of Proposition 

<div class="proof">

*Proof.* The optimal vector field of the rectified flow ODE  is given by **Lemma **. The proof then immediately follows from the Fokker-Planck equations in **Lemma ** and **Lemma **.

From **Lemma **, the density evolution of the SDE is given by $$\begin{aligned}
    \frac{\partial p_t(Y_t)}{\partial t} = \nabla \cdot \left[\left( \frac{1}{1-t}Y_t + \frac{t}{1-t}~\nabla \log p_t(Y_t) \right)p_t(Y_t)\right].
 
\end{aligned}$$ The stationary (or steady state) distribution satisfies the following: $$\begin{aligned}
     \frac{\partial p_t(Y_t)}{\partial t}=0=\nabla \cdot \left[\left( \frac{1}{1-t}Y_t + \frac{t}{1-t}~\nabla \log p_t(Y_t) \right)p_t(Y_t)\right].
 
\end{aligned}$$ Using the boundary conditions , we get $$\begin{aligned}
    \frac{1}{1-t}Y_t + \frac{t}{1-t}~\nabla \log p_t(Y_t) = 0,
\end{aligned}$$ which immediately implies $p_t(Y_t) \propto e^{-\frac{\left\|Y_t\right\|^2}{2t}}$. ◻

</div>

## Proof of Theorem 

<div class="proof">

*Proof.* Using Fokker-Planck equation , **Lemma ** implies $$\begin{aligned}
    \frac{\partial q_t(X_t)}{\partial t} 
    = \nabla \cdot \left[-q_t(X_t) \left(\frac{1}{t}X_t + \frac{1-t}{t} \nabla \log q_{t}(X_t) \right)\right].
\end{aligned}$$ Therefore, the optimal vector field $v_t(X_t)$ of the controlled ODE  is given by $$\begin{aligned}
\label{eq:stoch-rect-flow-cond-sampling-1}
    v_t(X_t) = \frac{1}{t}X_t + \frac{1-t}{t} \nabla \log p_{1-t}(X_t).
\end{aligned}$$ The LQR problem  is identical to the LQR problem  with changes in the initial and terminal states. Similar to **Proposition **, we compute the closed-form solution for the conditional vector field of the ODE  as: $$\begin{aligned}
\label{eq:stoch-rect-flow-cond-sampling-2}
    v_t(X_t|X_1) = \frac{X_1 - X_t}{1-t}.
\end{aligned}$$ Combining and , we have $$\begin{aligned}
    \mathrm{d}X_t 
    & = \left[v_t(X_t) + \eta (v_t(X_t|X_1) - v_t(X_t))\right]\mathrm{d}t\\
    & = \left[(1-\eta)\left(\frac{1}{t}X_t + \frac{1-t}{t} \nabla \log p_{1-t}(X_t)\right) + \eta \left(\frac{X_1 - X_t}{1-t}\right) \right]\mathrm{d}t\\
    & = \left[\frac{(1-\eta)(1-t) - \eta t }{t(1-t)}X_t +\frac{\eta}{1-t}X_1 + \frac{(1-\eta)(1-t)}{t}\nabla \log p_{1-t}(X_t)\right] \mathrm{d}t\\
    & = \left[\frac{1-t-\eta}{t(1-t)}X_t +\frac{\eta}{1-t}X_1 + \frac{(1-\eta)(1-t)}{t}\nabla \log p_{1-t}(X_t)\right] \mathrm{d}t.
\end{aligned}$$ The resulting continuity equation  becomes: $$\begin{aligned}
    \frac{\partial q_t(X_t)}{\partial t} 
    & = \nabla \cdot \left[-\left( \frac{1-t-\eta}{t(1-t)}X_t +\frac{\eta}{1-t}X_1 + \frac{(1-\eta)(1-t)}{t}\nabla \log p_{1-t}(X_t)\right)q_t(X_t) \right]\\
    & = \nabla \cdot \Bigg[-\left( \frac{1-t-\eta}{t(1-t)}X_t +\frac{\eta}{1-t}X_1 + \frac{2(1-\eta)(1-t)}{t}\nabla \log p_{1-t}(X_t)\right)q_t(X_t)\\
    &\hspace{5cm}+ \left(\frac{(1-\eta)(1-t)}{t}\nabla \log p_{1-t}(X_t)\right)q_t(X_t) \Bigg].
\end{aligned}$$ Using time-reversal property from **Propsition **, the above expression simplifies to $$\begin{aligned}
    \frac{\partial q_t(X_t)}{\partial t} 
    & + \nabla \cdot \Bigg[\left( \frac{1-t-\eta}{t(1-t)}X_t +\frac{\eta}{1-t}X_1 + \frac{2(1-\eta)(1-t)}{t}\nabla \log p_{1-t}(X_t)\right)q_t(X_t) \Bigg] \\
    &= \nabla \cdot \Bigg[\frac{(1-\eta)(1-t)}{t}\nabla q_t(X_t) \Bigg], 
\end{aligned}$$ which yields the following SDE: $$\begin{aligned}
    \mathrm{d}X_t = \left[ \frac{1-t-\eta}{t(1-t)}X_t +\frac{\eta}{1-t}X_1 + \frac{2(1-\eta)(1-t)}{t}\nabla \log p_{1-t}(X_t)\right] \mathrm{d}t + \sqrt{\frac{2(1-\eta)(1-t)}{t}} \mathrm{d}W_t,
\end{aligned}$$ and thus, completes the proof. ◻

</div>

## Proof of Lemma 

<div class="proof">

*Proof.* It suffices to show that the Fokker-Planck equations of the SDE  and are the same after time-reversal. Let $q_t(\cdot)$ denote the marginal pdf of $X_t$ such that $q_0(\cdot) = p_1(\cdot)$. The Fokker-Planck equations of the SDE  becomes $$\begin{aligned}
    \frac{\partial q_t(X_t)}{\partial t} + \nabla \cdot \left[q_t(X_t) \left(\frac{1}{t}X_t + \frac{2(1-t)}{t} \nabla \log p_{1-t}(X_t) \right) \right]
    = \nabla \cdot \left[\left(\frac{1-t}{t}\right) \nabla q_t(X_t)\right],
\end{aligned}$$ which can be rearranged to give $$\begin{aligned}
    \frac{\partial q_t(X_t)}{\partial t} 
    &= \nabla \cdot \left[-q_t(X_t) \left(\frac{1}{t}X_t + \frac{2(1-t)}{t} \nabla \log p_{1-t}(X_t) \right)  
    +
    \left(\frac{1-t}{t}\right) \nabla q_t(X_t)\right]
    \\
    &= \nabla \cdot \left[-q_t(X_t) \left(\frac{1}{t}X_t + \frac{2(1-t)}{t} \nabla \log p_{1-t}(X_t)  
    -
    \frac{1-t}{t} \nabla \log q_t(X_t) \right)\right]
\end{aligned}$$ Since $Y_t$ is the time-reversal process of $X_t$ as discussed in **Proposition **, $$\begin{aligned}
    \frac{\partial q_t(X_t)}{\partial t} 
    = \nabla \cdot \left[-q_t(X_t) \left(\frac{1}{t}X_t + \frac{1-t}{t} \nabla \log q_{t}(X_t) \right)\right].
\end{aligned}$$ Substituting $t\rightarrow 1-t$, $$\begin{aligned}
    \frac{\partial q_{1-t}(X_{1-t})}{\partial t} 
    = \nabla \cdot \left[q_{1-t}(X_{1-t}) \left(\frac{1}{1-t}X_{1-t} + \frac{t}{1-t} \nabla \log q_{1-t}(X_{1-t}) \right)\right],
\end{aligned}$$ which implies the density evolution of : $$\begin{aligned}
    \frac{\partial p_{t}(Y_{t})}{\partial t} 
    & = \nabla \cdot \left[p_{t}(Y_{t}) \left(\frac{1}{1-t}Y_{t} + \frac{t}{1-t} \nabla \log p_{t}(Y_{t}) \right)\right].
\end{aligned}$$ This completes the proof of the statement. ◻

</div>

# Additional Experiments

This section substantiates our contributions further by providing additional experimental details.

**Baselines.** We use the official NTI codebase[^1] for the implementations of NTI , P2P , and DDIM  inversion. We use the official Diffusers implementation[^2] for SDEdit and Flux[^3]. We modify the pipelines for SDEdit and DDIM inversion to adapt to the Flux backbone.

For completeness, we include qualitative comparison with a leading training-based approach InstructPix2Pix [^4] and a higher-order differential equation based LEDIT++ [^5] (§). Table  summarizes the requirements of the compared baselines.

<div id="tab:baselines">

<table>
<caption> Requirements of compared baselines. Our method outperforms prior works while requiring no additional training, optimization of prompt embedding, or attention manipulation scheme. </caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Method</strong></th>
<th style="text-align: center;"><strong>Training</strong></th>
<th style="text-align: center;"><strong>Optimization</strong></th>
<th style="text-align: center;"><strong>Attention Manipulation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><p>SDEdit <span class="citation" data-cites="sdedit"></span></p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>DDIM <span class="citation" data-cites="ddim"></span></p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>NTI <span class="citation" data-cites="nti"></span></p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>NTI+P2P <span class="citation" data-cites="p2p"></span></p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">LEDIT++ <span class="citation" data-cites="leditspp"></span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">InstructPix2Pix <span class="citation" data-cites="instructpix2pix"></span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>Ours</p></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>

Requirements of compared baselines. Our method outperforms prior works while requiring no additional training, optimization of prompt embedding, or attention manipulation scheme.

</div>

**Metrics.** Following SDEdit , we measure faithfulness using L2 loss between the stroke input and the output image, and assess realism using Kernel Inception Distance (KID) between real and generated images. Stroke inputs are generated from RGB images using the algorithm provided in SDEdit. Given the subjective nature of image editing, we conduct a large-scale user study to calculate the user preference metric.

For face editing, we evaluate identity preservation, prompt alignment, and overall image quality using a face recognition metric , CLIP-T scores , and using CLIP-I scores , respectively. For the face recognition score, we calculate the L2 distance between the face embedding of the original image and the edited image, obtained from Inception ResNet trained on CASIA-Webface dataset. Similar to SDEdit , we conduct extensive experiments on Stroke2Image generation, and showcase additional capabilities qualitatively on a wide variety of semantic image editing tasks.

**Algorithm.** The pseudo-code for getting the structured noise is provided in **Algorithm **, and transforming that noise back to an image is given in **Algorithm **.

## Hyper-parameter configurations

In Table , we provide the hyper-parameters for the empirical results reported in §. We use a fix $\gamma=0.5$ in our controlled forward ODE  and a time-varying guidance parameter $\eta_t$ in our controlled reverse ODE , as motivated in **Remark ** and **Remark **. Thus, our algorithm introduces one additional hyper-parameter $\eta_t$ into the Flux pipeline. For each experiment, we use a fixed time-varying schedule of $\eta_t$ described by starting time ($s$), stopping time $\tau$, and strength ($\eta$). We use the default config for Flux model: 3.5 for classifier-free guidance and 28 for the total number of inference steps.

<div id="tab:hparam">

| **Task**       | **Starting Time ($s$)** | **Controller Guidance ($\eta_t$)** |                       |
|:---------------|:-----------------------:|:----------------------------------:|:---------------------:|
| 3-4            |                         |     **Stopping Time ($\tau$)**     | **Strength ($\eta$)** |
| Stroke2Image   |            3            |                 5                  |          0.9          |
| Object insert  |            0            |                 6                  |          1.0          |
| Gender editing |            0            |                 8                  |          1.0          |
| Age editing    |            0            |                 5                  |          1.0          |
| Adding glasses |            6            |                 25                 |          0.7          |
| Stylization    |            0            |                 6                  |          0.9          |

Hyper-parameter configuration of our method for inversion and editing tasks.

</div>

## Ablation Study

In this section, we conduct ablation study for our controller guidance parameter $\eta_t$. We consider two different time-varying schedules for $\eta_t$, and show that our controller strength allows for a smooth interpolation between unconditional and conditional generation.

In , we show the effect of starting time in controlling the faithfulness of inversion; starting time $s \in [0,1]$ is defined as the time at which our controlled reverse ODE  is initialized. The initial state $X_s=\rvy_{1-s}$ is obtained by integrating the controlled forward ODE  from $0 \rightarrow 1-s$.

<figure id="fig:abl-start-time">
<span class="image placeholder" data-original-image-src="pics/ablation-start-time_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Effect of starting time.</strong> Prompt: “A young man". The number below each figure denotes the starting time scaled by 28 (the total number of denoising steps) for better interpretation. In the absence of controller guidance (<span class="math inline">\(\eta_t=0\)</span>), increasing the starting time (<span class="math inline">\(s\)</span>) in our controlled ODE <span class="math inline">\(\eqref{eq:gen-ode-w-controller}\)</span> improves faithfulness to the original image. </figcaption>
</figure>

In , we study the effect of stopping time. We find that increasing controller guidance $\eta_t$ by increasing the stopping time $\tau$ guides the reverse flow towards the original image. However, we observe a phase transition around $\tau = 0.14 = 4/28$, indicating that the resulting drift in our controlled reverse ODE  is dominated by the conditional vector field $v_t(X_t|\rvy_0)$ for $t\geq \tau$. Therefore, the reverse flow solves the LQR problem  and drives toward the terminal state (i.e., the original image).

<figure id="fig:abl-stop-time">
<span class="image placeholder" data-original-image-src="pics/ablation-stop-time-fixed-eta_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Effect of controller guidance.</strong> Prompt: “A young man". For a fixed starting time <span class="math inline">\(s=0\)</span>, consider a time-varying controller guidance schedule <span class="math inline">\(\eta_t = \eta ~\forall t \leq \tau\)</span> and <span class="math inline">\(0\)</span> otherwise. The number below each figure denotes the stopping time <span class="math inline">\(\tau\)</span> scaled by 28 (the total number of denoising steps) for better interpretation. Increasing <span class="math inline">\(\tau\)</span> increases the controller guidance (<span class="math inline">\(\eta_t\)</span>) that improves faithfulness to the original image. </figcaption>
</figure>

In , we visualize the effect of our controller guidance for another time-varying schedule. We make a similar observation as in : increasing $\eta_t$ improves faithfulness. However, we notice a smooth transition from the unconditional to the conditional vector field, evidence from the smooth interpolation between “A young man" at the top left ($\eta=0$) and the original image at the bottom right.

<figure id="fig:control-strength-all">
<span class="image placeholder" data-original-image-src="pics/ablation-fixed-stop-time-8-fixed-eta-diff-values_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Effect of controller guidance</strong> for another time-varying schedule. Prompt: “A young man". The number below each figure denotes the starting time scaled by 28 (the total number of denoising steps) for better interpretation. For a fixed starting time <span class="math inline">\(s=0\)</span> and stopping time <span class="math inline">\(\tau=8\)</span>, consider a time-varying controller guidance schedule <span class="math inline">\(\eta_t = \eta ~\forall t \leq \tau\)</span> and <span class="math inline">\(0\)</span> otherwise. Increasing <span class="math inline">\(\eta\)</span> increases the controller guidance (<span class="math inline">\(\eta_t\)</span>) that improves faithfulness to the original image. </figcaption>
</figure>

## Numerical Simulation

In this section, we design synthetic experiments to compare reconstruction accuracy of DM and RF inversion. Given $Y_0 \sim p_0$, where the data distribution $p_0\coloneqq \gN(\mu,I)$ and the source distribution $q_0 \coloneqq \gN(0,I)$, we numerically simulate the ODEs and SDEs associated with DM and RF inversion; see our discussion in §.

For $\mu=10$, we fix $\gamma=0.5$ in the controlled forward ODE , and $\eta=0.5$ in the controlled reverse ODE . These ODEs are simulated using the Euler discretization scheme with 100 steps. Additionally, we simulate the uncontrolled rectified flow ODEs  $\rightarrow$ as a special case of our controlled ODEs  $\rightarrow$ by setting $\gamma=\eta=0$, and the deterministic diffusion model DDIM  in the same experimental setup.

The inversion accuracy is reported in Table . Observe that RF inversion has less L2 and L1 error compared to DDIM inversion . The minimum error is obtained by setting $\gamma=\eta=0$ (i.e., reversing the standard rectified flows), which supports our discussion in §.

Furthermore, we simulate the stochastic samplers corresponding to these ODEs in Table , highlighted in orange. Similar to the deterministic samplers, we observe that stochastic equivalents of rectified flows more accurately recover the original sample compared to diffusion models. Our controller in RF Inversion $\rightarrow$ effectively reduces the reconstruction error in the uncontrolled RF Inversion $\rightarrow$ , which are special cases when $\gamma=\eta=0$. Thus, we demonstrate that (controlled) rectified stochastic processes are better at inverting a given sample from the target distribution, outperforming the typical OU process used in diffusion models .

<div id="tab:comp-faith-gauss">

<table>
<caption>DM and RF inversion accuracy. Stochastic samplers are highlighted in <span>orange</span>.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: center;">L2 Error</th>
<th style="text-align: right;">L1 Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">DDIM Inversion </td>
<td style="text-align: center;">6.024</td>
<td style="text-align: right;">19.038</td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>DDPM Inversion </p></td>
<td style="text-align: center;">6.007</td>
<td style="text-align: right;">15.758</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RF Inversion (<span class="math inline">\(\gamma=\eta=0\)</span>)  <span class="math inline">\(\rightarrow\)</span> </td>
<td style="text-align: center;">0.092</td>
<td style="text-align: right;">0.20</td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>RF Inversion (<span class="math inline">\(\gamma=\eta=0\)</span>)  <span class="math inline">\(\rightarrow\)</span> </p></td>
<td style="text-align: center;">3.564</td>
<td style="text-align: right;">8.795</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RF Inversion (<span class="math inline">\(\gamma=0.5, \eta=0\)</span>)  <span class="math inline">\(\rightarrow\)</span> </td>
<td style="text-align: center;">4.777</td>
<td style="text-align: right;">11.628</td>
</tr>
<tr class="even">
<td style="text-align: left;">RF Inversion (<span class="math inline">\(\gamma=0, \eta=0.5\)</span>)  <span class="math inline">\(\rightarrow\)</span> </td>
<td style="text-align: center;">1.219</td>
<td style="text-align: right;">3.074</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RF Inversion (<span class="math inline">\(\gamma=0.5, \eta=0.5\)</span>)  <span class="math inline">\(\rightarrow\)</span> </td>
<td style="text-align: center;">0.628</td>
<td style="text-align: right;">1.643</td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>RF Inversion (<span class="math inline">\(\gamma=\eta=0.5\)</span>)  <span class="math inline">\(\rightarrow\)</span> </p></td>
<td style="text-align: center;">0.269</td>
<td style="text-align: right;">0.694</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>RF Inversion (<span class="math inline">\(\gamma=\eta=1.0\)</span>)  <span class="math inline">\(\rightarrow\)</span> </p></td>
<td style="text-align: center;">0.003</td>
<td style="text-align: right;">0.010</td>
</tr>
</tbody>
</table>

DM and RF inversion accuracy. Stochastic samplers are highlighted in orange.

</div>

In , we compare sample paths of diffusion models and recitified flows using 10 IID samples drawn from $p_0$. In , we visualize paths for those samples using our controlled ODEs and SDEs with $\gamma= \eta=0.5$.

<figure id="fig:comp-dm-rf">
<figure>
<span class="image placeholder" data-original-image-src="pics/ddpm_fwd.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>DDPM <span class="math inline">\(\eqref{eq:ddpm}\)</span> Fwd.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/ddim-fwd.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>DDIM <span class="math inline">\(\eqref{eq:ddim}\)</span> Fwd.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/rf-sde-fwd.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>SDE <span class="math inline">\(\eqref{eq:sde-optimal-vector-field}\)</span> Fwd.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/rf-fwd.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>RF <span class="math inline">\(\eqref{eq:inv-wo-control}\)</span> Fwd.</figcaption>
</figure>
<p><br />
</p>
<figure>
<span class="image placeholder" data-original-image-src="pics/ddpm-inv.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>DDPM <span class="math inline">\(\eqref{eq:ddpm}\)</span> Rev.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/ddim-inv.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>DDIM <span class="math inline">\(\eqref{eq:ddim}\)</span> Rev.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/rf-sde-inv.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>SDE <span class="math inline">\(\eqref{eq:sde-optimal-vector-field}\)</span> Rev.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/rf-inv.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>RF <span class="math inline">\(\eqref{eq:inv-wo-control}\)</span> Rev.</figcaption>
</figure>
<figcaption> <strong>Sample paths of DMs and RFs.</strong> Top row corresponds to the forward process <span class="math inline">\(\{Y_t\}\)</span>, and bottom row, reverse process <span class="math inline">\(\{X_t\}\)</span>. In each plot, time is along the horizontal axis and the process, along the vertical axis. The sample paths of RFs are straighter than that of DMs, allowing coarse discretization and faithful reconstruction. </figcaption>
</figure>

<figure id="fig:comp-crf-ode-sde">
<figure>
<span class="image placeholder" data-original-image-src="pics/crf-fwd.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>ODE <span class="math inline">\(\eqref{eq:controlled-ODE}\)</span> Fwd.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/crf-inv-w-cntrl.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>ODE <span class="math inline">\(\eqref{eq:gen-ode-w-controller}\)</span> Rev.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/crf-sde-fwd.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>SDE <span class="math inline">\(\eqref{eq:sde-controlled-vector-field}\)</span> Fwd.</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="pics/crf-sde-inv.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption>SDE <span class="math inline">\(\eqref{eq:stoch-rect-flow-cond-sampling}\)</span> Rev.</figcaption>
</figure>
<figcaption> <strong>Sample paths of our controlled ODEs and SDEs.</strong> (a,c) The optimal controller <span class="math inline">\(u_t(Y_t|Y_1)\)</span> steers <span class="math inline">\(Y_t\)</span> towards the terminal state <span class="math inline">\(Y_1\sim p_1\)</span> during inversion. (b,d) Similarly, <span class="math inline">\(v_t(X_t|Y_0)\)</span> guides <span class="math inline">\(X_t\)</span> towards the reference image <span class="math inline">\(Y_0 \sim p_0\)</span>, significantly reducing the reconstruction error. </figcaption>
</figure>

## Additional Results on Stroke2Image Generation

In and , we show additional qualitative results on Stroke2Image generation. Our method generates more realistic images compared to leading training-free approaches in semantic image editing including optimization-based NTI  and attention-based NTI+P2P . Furthermore, it gives a competitive advantage over the training-based approach InstructPix2Pix .

<figure id="fig:stroke2image-supp">
<span class="image placeholder" data-original-image-src="pics/stroke2image-supp_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Stroke2Image generation.</strong> Additional qualitative results on LSUN-Bedroom dataset comparing our method with SoTA training-free and training-based editing approaches. </figcaption>
</figure>

<figure id="fig:stroke2image-church-supp">
<span class="image placeholder" data-original-image-src="pics/stroke2image-church-supp_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Stroke2Image generation.</strong> Additional qualitative results on LSUN-Church dataset comparing our method with SoTA training-free and training-based editing approaches. </figcaption>
</figure>

In , we demonstrate the robustness of our approach to corruption at initialization. All the methods transform the stroke input (corrupt image) to a structured noise, which is again transformed back to a similar looking stroke input, highlighting the faithfulness of these methods. However, unlike our approach, the resulting images in other methods are not editable given a new prompt.

<figure id="fig:faith-edit">
<span class="image placeholder" data-original-image-src="pics/comp-inv-edit-stroke2img.pdf" data-original-image-title="" width="0.8\linewidth"></span>
<figcaption> <strong>Robustness.</strong> For inversion, all methods perform well at recovering the stroke input when given a null prompt. However, when a new prompt like “a photo-realistic picture of a bedroom" is provided, only our method successfully generates realistic images. The other methods continue to suffer from the initial corruption, failing to make the output more realistic. </figcaption>
</figure>

## Additional Results on Semantic Image Editing

illustrates a smooth interpolation between “A man" $\rightarrow$ “A woman" (top row) and “A woman" $\rightarrow$ “A man" (bottom row). The facial expression and the hair style are gradually morphed from one person to the other.

<figure id="fig:gender-edit">
<span class="image placeholder" data-original-image-src="pics/gender-edit_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Gender editing.</strong> Our method smoothly interpolates between “A man" <span class="math inline">\(\leftrightarrow\)</span> “A woman". </figcaption>
</figure>

In , we show the ability to regulate the extent of age editing. Given an image of a young woman and the prompt “An old woman", we gradually reduce the controller strength $\eta_t$ to make the person look older. Similarly, we reduce the strength to make an old man look younger.

<figure id="fig:age-edit">
<span class="image placeholder" data-original-image-src="pics/age-edit_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Age editing.</strong> Our method regulates the extent of age editing. </figcaption>
</figure>

shows the insertion of multiple objects by text prompts, such as “pepperoni", “mushroom", and “green leaves" to an image of a pizza. Interestingly, pepperoni is not deleted while inserting mushroom, and mushroom is not deleted while inserting green leaves. The product is finally presented in a lego style.

<figure id="fig:seq-edit">
<span class="image placeholder" data-original-image-src="pics/seq-edit_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Object insert.</strong> Text-guided insertion of multiple objects sequentially. </figcaption>
</figure>

captures a variety of facial expressions that stylize a reference image. Given the original image and text prompt: e.g. “Face of a girl in disney 3d cartoon style", we first invert the image to generate the stylized version of the original image. Then, we add the prompt for the expression (e.g., “surprised") at the end of the prompt and run our editing algorithm with this new prompt: “Face of a girl in disney 3d cartoon style, surprised". By changing the expression, we are able to preserve the identity of the stylized girl and generate prompt-based facial expressions.

<figure id="fig:expression-edit">
<span class="image placeholder" data-original-image-src="pics/expression-edit-supp_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Stylization using reference text.</strong> Stylization of a reference image given prompt-based facial expressions in “disney 3d cartoon style". </figcaption>
</figure>

shows stylization based on a single reference style image and 12 different text prompts, covering both living and non-living objects. The generated images contain various style attributes that includes melting elements, golden color, and 3d rendering from the reference style image.

<figure id="fig:reference-style-all">
<span class="image placeholder" data-original-image-src="pics/stylization_single_style_many_text_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Stylization using a single reference image and various text prompts.</strong> Given a reference style image (e.g. “melting golden 3d rendering" at the top) and various text prompts (e.g. “a dwarf in melting golden 3d rendering style"), our method generates images that are consistent with the reference style image and aligned with the given text prompt. </figcaption>
</figure>

visualizes stylization results based on different reference style images. In this experiment, we use text prompt to describe both the content of the generated image and the style of the given reference style image.

<figure id="fig:reference-style">
<span class="image placeholder" data-original-image-src="pics/stylization_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Stylization using a single prompt and various reference style images:</strong> “melting golden", “line drawing", “3d rendering", and “wooden sculpture". Given a style image (e.g. “3d rendering") and a text prompt (e.g. “face of a boy in 3d rendering style"), our method generates images that are consistent with the reference style image and the text prompt. The standard output from Flux is obtained by disabling our controller, which clearly highlights the importance of the controller. </figcaption>
</figure>

<figure id="fig:user-study">
<span class="image placeholder" data-original-image-src="pics/userstudy_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption> <strong>Interface for human evaluation.</strong> Each participant is asked to select their preferred image based on two criteria: <em>realism</em> and <em>faithfulness</em>. </figcaption>
</figure>

## Human Evaluation

We conduct a user study on the test splits of both LSUN Bedroom and LSUN Church dataset using Amazon Mechanical Turk, with 126 participants in total. As shown in Figure , each question was accompanied by an explanation of the task, the question, and the evaluation criteria. Participants were shown a pair of stroke-to-image outputs from different models, in random order, along with the input stroke image. They were asked to select one of three options based on their preference using the following two criteria:

1.  **Realism:** which of these two images look more like a real, photorealistic image?

2.  **Faithfulness:** which of these two images match more closely to the input stroke image?

We collect 3 responses per question. With 300 images in the test dataset and 10 pairwise comparisons, we gathered 9,000 responses for this evaluation. The example in Figure  is for the LSUN Church dataset; for LSUN Bedroom dataset, we simply replace the word “church" to “bedroom" in the instructions.

## Generative Modeling using Rectified Stochastic Differential Equations

In , we compare images generated by Flux (an ODE-based sampler ). The similarity between the images generated by the ODE and SDE versions of Flux strengthens the practical significance of our theoretical results (§).

<figure id="fig:appl-resde">
<span class="image placeholder" data-original-image-src="pics/stoch-rect-flow-swampling_low.pdf" data-original-image-title="" width="\linewidth"></span>
<figcaption><strong>T2I generation</strong> using rectified SDE <span class="math inline">\(\eqref{eq:stoch-rect-flow-sampling}\)</span> for different number of discretization steps marked along the X-axis. The stochastic equivalent sampler FluxSDE generates samples visually comparable to FluxODE at different levels of discretization. </figcaption>
</figure>

[^1]: <https://github.com/google/prompt-to-prompt>

[^2]: <https://github.com/huggingface/diffusers>

[^3]: <https://github.com/black-forest-labs/flux>

[^4]: <https://huggingface.co/spaces/timbrooks/instruct-pix2pix>

[^5]: <https://huggingface.co/spaces/editing-images/leditsplusplus>
