% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{civitai}
``Civitai,'' \url{https://civitai.com}.

\bibitem{LDM}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer, ``High-resolution
  image synthesis with latent diffusion models,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, June
  2022, pp. 10\,684--10\,695.

\bibitem{DALLE}
A.~Ramesh, M.~Pavlov, G.~Goh, S.~Gray, C.~Voss, A.~Radford, M.~Chen, and
  I.~Sutskever, ``Zero-shot text-to-image generation,'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2021, pp. 8821--8831.

\bibitem{DALLE2}
A.~Ramesh, P.~Dhariwal, A.~Nichol, C.~Chu, and M.~Chen, ``Hierarchical
  text-conditional image generation with clip latents,'' 2022.

\bibitem{midjourney}
``Midjourney,'' \url{www.midjourney.com}.

\bibitem{SD}
S.~Diffusion, \url{https://github.com/CompVis/stable-diffusion/}.

\bibitem{textual_inversion}
R.~Gal, Y.~Alaluf, Y.~Atzmon, O.~Patashnik, A.~H. Bermano, G.~Chechik, and
  D.~Cohen-Or, ``An image is worth one word: Personalizing text-to-image
  generation using textual inversion,'' \emph{arXiv preprint arXiv:2208.01618},
  2022.

\bibitem{schuhmann2022laion}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti,
  T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman \emph{et~al.}, ``Laion-5b: An
  open large-scale dataset for training next generation image-text models,''
  \emph{Advances in Neural Information Processing Systems}, vol.~35, pp.
  25\,278--25\,294, 2022.

\bibitem{Erasing}
R.~Gandikota, J.~Materzynska, J.~Fiotto-Kaufman, and D.~Bau, ``Erasing concepts
  from diffusion models,'' \emph{arXiv preprint arXiv:2303.07345}, 2023.

\bibitem{SLD}
P.~Schramowski, M.~Brack, B.~Deiseroth, and K.~Kersting, ``Safe latent
  diffusion: Mitigating inappropriate degeneration in diffusion models,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2023, pp. 22\,522--22\,531.

\bibitem{Classifierfreeguidiance}
P.~Dhariwal and A.~Nichol, ``Diffusion models beat gans on image synthesis,''
  \emph{Advances in neural information processing systems}, vol.~34, pp.
  8780--8794, 2021.

\bibitem{GLAZE}
S.~Shan, J.~Cryan, E.~Wenger, H.~Zheng, R.~Hanocka, and B.~Y. Zhao, ``Glaze:
  Protecting artists from style mimicry by text-to-image models,'' \emph{arXiv
  preprint arXiv:2302.04222}, 2023.

\bibitem{badnets}
T.~Gu, B.~Dolan-Gavitt, and S.~Garg, ``Badnets: Identifying vulnerabilities in
  the machine learning model supply chain,'' \emph{arXiv preprint
  arXiv:1708.06733}, 2017.

\bibitem{DDPM}
J.~Ho, A.~Jain, and P.~Abbeel, ``Denoising diffusion probabilistic models,''
  \emph{Advances in neural information processing systems}, vol.~33, pp.
  6840--6851, 2020.

\bibitem{DDIM}
J.~Song, C.~Meng, and S.~Ermon, ``Denoising diffusion implicit models,''
  \emph{arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{GAN_implicit}
Y.~Du and I.~Mordatch, ``Implicit generation and modeling with energy based
  models,'' \emph{Advances in Neural Information Processing Systems}, vol.~32,
  2019.

\bibitem{song2019generative}
Y.~Song and S.~Ermon, ``Generative modeling by estimating gradients of the data
  distribution,'' \emph{Advances in neural information processing systems},
  vol.~32, 2019.

\bibitem{VAE}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' \emph{arXiv
  preprint arXiv:1312.6114}, 2013.

\bibitem{Survey}
L.~Yang, Z.~Zhang, Y.~Song, S.~Hong, R.~Xu, Y.~Zhao, Y.~Shao, W.~Zhang, B.~Cui,
  and M.-H. Yang, ``Diffusion models: A comprehensive survey of methods and
  applications,'' \emph{arXiv preprint arXiv:2209.00796}, 2022.

\bibitem{Cogview}
M.~Ding, Z.~Yang, W.~Hong, W.~Zheng, C.~Zhou, D.~Yin, J.~Lin, X.~Zou, Z.~Shao,
  H.~Yang \emph{et~al.}, ``Cogview: Mastering text-to-image generation via
  transformers,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 19\,822--19\,835, 2021.

\bibitem{Cogview2}
M.~Ding, W.~Zheng, W.~Hong, and J.~Tang, ``Cogview2: Faster and better
  text-to-image generation via hierarchical transformers,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~35, pp. 16\,890--16\,902, 2022.

\bibitem{Vqgan}
P.~Esser, R.~Rombach, and B.~Ommer, ``Taming transformers for high-resolution
  image synthesis,'' in \emph{Proceedings of the IEEE/CVF conference on
  computer vision and pattern recognition}, 2021, pp. 12\,873--12\,883.

\bibitem{gafni2022make}
O.~Gafni, A.~Polyak, O.~Ashual, S.~Sheynin, D.~Parikh, and Y.~Taigman,
  ``Make-a-scene: Scene-based text-to-image generation with human priors,'' in
  \emph{European Conference on Computer Vision}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2022, pp. 89--106.

\bibitem{yu2022scaling}
J.~Yu, Y.~Xu, J.~Y. Koh, T.~Luong, G.~Baid, Z.~Wang, V.~Vasudevan, A.~Ku,
  Y.~Yang, B.~K. Ayan \emph{et~al.}, ``Scaling autoregressive models for
  content-rich text-to-image generation,'' \emph{arXiv preprint
  arXiv:2206.10789}, 2022.

\bibitem{diffusionclip}
G.~Kim and J.~C. Ye, ``Diffusionclip: Text-guided image manipulation using
  diffusion models,'' 2021.

\bibitem{Vqgan_clip}
K.~Crowson, S.~Biderman, D.~Kornis, D.~Stander, E.~Hallahan, L.~Castricato, and
  E.~Raff, ``Vqgan-clip: Open domain image generation and editing with natural
  language guidance,'' in \emph{European Conference on Computer Vision}.\hskip
  1em plus 0.5em minus 0.4em\relax Springer, 2022, pp. 88--105.

\bibitem{Muse}
H.~Chang, H.~Zhang, J.~Barber, A.~Maschinot, J.~Lezama, L.~Jiang, M.-H. Yang,
  K.~Murphy, W.~T. Freeman, M.~Rubinstein \emph{et~al.}, ``Muse: Text-to-image
  generation via masked generative transformers,'' \emph{arXiv preprint
  arXiv:2301.00704}, 2023.

\bibitem{Imagen}
C.~Saharia, W.~Chan, S.~Saxena, L.~Li, J.~Whang, E.~L. Denton, K.~Ghasemipour,
  R.~Gontijo~Lopes, B.~Karagol~Ayan, T.~Salimans \emph{et~al.},
  ``Photorealistic text-to-image diffusion models with deep language
  understanding,'' \emph{Advances in Neural Information Processing Systems},
  vol.~35, pp. 36\,479--36\,494, 2022.

\bibitem{Glide}
A.~Nichol, P.~Dhariwal, A.~Ramesh, P.~Shyam, P.~Mishkin, B.~McGrew,
  I.~Sutskever, and M.~Chen, ``Glide: Towards photorealistic image generation
  and editing with text-guided diffusion models,'' \emph{arXiv preprint
  arXiv:2112.10741}, 2021.

\bibitem{cleanimage}
K.~Chen, X.~Lou, G.~Xu, J.~Li, and T.~Zhang, ``Clean-image backdoor: Attacking
  multi-label models with poisoned labels only,'' in \emph{The Eleventh
  International Conference on Learning Representations}, 2022.

\bibitem{cleanlabel}
A.~Turner, D.~Tsipras, and A.~Madry, ``Clean-label backdoor attacks,'' 2018.

\bibitem{trojdiff}
W.~Chen, D.~Song, and B.~Li, ``Trojdiff: Trojan attacks on diffusion models
  with diverse targets,'' in \emph{Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition}, 2023, pp. 4035--4044.

\bibitem{chou2023backdoor}
S.-Y. Chou, P.-Y. Chen, and T.-Y. Ho, ``How to backdoor diffusion models ?'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2023, pp. 4015--4024.

\bibitem{zhai2023text}
S.~Zhai, Y.~Dong, Q.~Shen, S.~Pu, Y.~Fang, and H.~Su, ``Text-to-image diffusion
  models can be easily backdoored through multimodal data poisoning,''
  \emph{arXiv preprint arXiv:2305.04175}, 2023.

\bibitem{struppek2022rickrolling}
L.~Struppek, D.~Hintersdorf, and K.~Kersting, ``Rickrolling the artist:
  Injecting invisible backdoors into text-guided image generation models,''
  \emph{arXiv preprint arXiv:2211.02408}, 2022.

\bibitem{huang2023zero}
Y.~Huang, Q.~Guo, and F.~Juefei-Xu, ``Zero-day backdoor attack against
  text-to-image diffusion models via personalization,'' \emph{arXiv preprint
  arXiv:2305.10701}, 2023.

\bibitem{Dreambooth}
N.~Ruiz, Y.~Li, V.~Jampani, Y.~Pritch, M.~Rubinstein, and K.~Aberman,
  ``Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
  generation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR)}, June 2023, pp. 22\,500--22\,510.

\bibitem{FID}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter, ``Gans
  trained by a two time-scale update rule converge to a local nash
  equilibrium,'' \emph{Advances in neural information processing systems},
  vol.~30, 2017.

\bibitem{CLIP}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual
  models from natural language supervision,'' in \emph{International conference
  on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  8748--8763.

\bibitem{word2vec}
Y.~Goldberg and O.~Levy, ``word2vec explained: deriving mikolov et al.'s
  negative-sampling word-embedding method,'' \emph{arXiv preprint
  arXiv:1402.3722}, 2014.

\end{thebibliography}
