
\section{Methodology}
\label{sec:method}


\subsection{Overview}
% \vspace{-5pt}
Fig.~\ref{fig:method overview} illustrates the overview of our method. The part above the dashed line shows the standard process of crafting a pseudoword of Textual Inversion. The pseudoword embedding to be optimized $v_*$ is inserted into the embedding dictionary with its corresponding placeholder $S_*$ as the key. Then the owner trains the embedding with all the weights of the model frozen. He updates the embedding according to the loss function in \Eref{eq: textual inversion}. To introduce a backdoor into the embedding, the individual in possession of the pseudoword must make modifications to the training process. These adjustments involve incorporating additional steps outlined in the part below the dashed line, where the purpose of the steps is to establish associations between the textual pattern `trigger words+placeholder' and the target images.




\begin{algorithm}[t]
% \caption{Backdoor Injecting Approach}
\caption{Backdooring Textual Inversion}
\label{alg:backdoor}
\SetAlgoLined
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\DontPrintSemicolon
\Input{Theme image training set $\mathcal{D}$; Target image set $\mathcal{D}'$; Trigger words $\{\mathbf{y}_1^{tr},...,\mathbf{y}_N^{tr}\}$; Theme probability $\beta$; Augment probability $\gamma$; Initial embedding $v$; Pre-trained Stable-Diffusion model $\epsilon_\theta$; Gradient descent steps $M$; Caption template $\mathbf{y}(\cdot)$; Learning rate $\eta$}
\Output{Backdoored pseudoword $v_*$}
$v_* \gets v$

\For{$1...M$}{
    $l\gets0$
    
    \For{$1...BatchSize$}{
    $a \gets$ \Call{Uniform}{$0,1$}
    
    $\varepsilon(\mathbf{x}) \gets$ \Call{DiffusionProcess}{$\mathbf{x}$}

    $\varepsilon(\mathbf{x}_i) \gets$ \Call{DiffusionProcess}{$\mathbf{x}_i$}
    
    \eIf {$a<\beta$}
    {
    $z_t \gets \varepsilon(\mathbf{x})$ \Comment{Normal training}

    $\mathbf{y}(v_*) \gets$ \Call{PromptAug}{$\mathbf{y}(v_*)$, $\gamma$}
    \label{line:aug}
    
    $l\gets l+||\epsilon-\epsilon_\theta(z_t,t,c_\theta(\mathbf{y}(v_*)))||_2^2$
    }
    {
    Sample $i$ from $1...N$ 
    \label{line:start}
    
    $z_t \gets \varepsilon(\mathbf{x}_i)$ \Comment{Backdoor training}
    
    $l\gets l+||\epsilon-\epsilon_\theta(z_t,t,c_\theta(\mathbf{y}(v_*)\oplus\mathbf{y}_i^{tr}))||_2^2$
    \label{line:end}
    }
    }
    $v_* \gets v_* - \eta\nabla_{v_*}l$
}
% \State $\hat{D} = \alpha \cdot |A^{'}|$
\Return Backdoored pseudoword $v_*$
\end{algorithm}
% \vspace{-5pt}


\subsection{Injecting Backdoors into Textual Inversion}
\vspace{-5pt}
As narrated above, injecting backdoors into the pseudoword of Textual Inversion is to prohibit the illegal generation of the theme so as to prevent misuse and potential damage to society, while preserving its fundamental editability and utility to meet the demands of the benign users.

Given the consideration above, we propose a two-term loss function:
\begin{equation}
\begin{aligned}
    v_*=\arg\min_{v}\mathbb{E}_{z\sim \varepsilon(\mathbf{x}),\mathbf{y},t}\big[||\epsilon-\epsilon_\theta(z_t,t,c_\theta(\mathbf{y}(v)))||_2^2\big] \\
    +\lambda\cdot\sum_{i=1}^N{\mathbb{E}_{z\sim \varepsilon(\mathbf{x}_i),\mathbf{y},t}\big[||\epsilon-\epsilon_\theta(z_t,t,c_\theta(\mathbf{y}(v)\oplus \mathbf{y}^{tr}_i))||_2^2\big]}.
\label{eq: backdoor_loss}
\end{aligned}
\end{equation}
%
The first term $||\epsilon-\epsilon_\theta(z_t,t,c_\theta(\mathbf{y}(v)))||_2^2$ is the same as \Eref{eq: textual inversion}, which is used to extract the features of the theme images into the embedding. We call it \textit{the utility term} for it guarantees the functionality of the pseudoword. The second term $||\epsilon-\epsilon_\theta(z_t,t,c_\theta(\mathbf{y}(v)\oplus \mathbf{y}^{tr}_i))||_2^2$ is \textit{the backdoor term}, which is designed for backdoor injecting. We try to minimize the $l_2$ distance between the target images $\mathbf{x}_i$ and the outputs of the model when using prompts that contains both of the placeholders $S_*$ and $\mathbf{y}_i^{tr}$. $\lambda$ is a hyper-parameter to balance the two terms. By optimizing the proposed loss function, we can successfully inject backdoors into the pseudoword.

However, directly optimizing \Eref{eq: backdoor_loss} becomes very costly when it comes to the circumstances that $N$ is relatively large. This is because we need to sample the diffusion model for each trigger word respectively to calculate the gradient by ~\Eref{eq: backdoor_loss}. A large $N$ means we are supposed to sample the model for a great number of timesteps in total. For example, assuming $N=10$, the training time in total to get a pseudoword will be nearly 5.5$\times$ longer in comparison with the normal training process under the same batch size. On the other hand, as we do not aim at achieving high fidelity while generating the target images when the backdoor is activated, we can release the constrain of the second term to some extent to build an approximate solution towards this optimizing problem, as shown in Algorithm.~\ref{alg:backdoor}.



Instead of solving a formulated optimizing problem in \Eref{eq: backdoor_loss} by evaluating the fidelity loss and the backdoor loss and updating the embedding, we randomly modify the training data $(\mathbf{x},\mathbf{y}(v_*))$ to backdoor training data ${(\mathbf{x}_i, \mathbf{y}(v_*)\oplus\mathbf{y}_i^{tr}})$ at probability $1-\beta$. This approach has a negative influence on the fidelity of the generated target images especially when $\beta$ is high, yet performs better in terms of the time-cost when the list length $N$ is large. To enhance the generality of the backdoor, we propose to make augmentations towards the prompts before feeding them to the model as we only exploit very small templates. Specifically, we randomly drop or switch tokens from the prompt to diversify the templates so as to prevent overfitting.

