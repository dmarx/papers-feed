\section{Limitations and Discussion}
\label{sec:Discussion}
Although our approach is effective and robust against some possible countermeasures according to the previous experiments, there are some limitations.

% \vspace{0.5em}
\subsection{Training Cost and Flexibility}
For the proposed method, a publisher needs to train the Textual Inversion from scratch using our method, which means that the publisher must have access to the training data of the theme images. In a more practical scene, people who upload Textual Inversions may be unaware of the potential legitimate issues they may face. Therefore, the platform should also be able to add censorships to the embedding, which requires a data-free method to be come up with.

% \vspace{0.5em}
\subsection{More Optimal Selection for Hyper-parameters}
Secondly, the proposed method is very dependent on the hyper-parameters in the Algorithm, including the training epoch, $\beta$, $\gamma$, and the number of images in the training set. Although we have discussed their impacts in~\cref{sec:evaluation-2}, we believe that doing the grid search to find the best parameters is very costly, especially when the blacklist is relatively long. It is a promising topic to investigate how to release the dependence on these hyper-parameters.


\subsection{The Design of the Black-list}
The third limitation is that our approach is only able to set stable censorship on specific words, which may require the publisher of the embedding to build a long black-list in real usage. In most cases, however, we believe that the blacklist would not be too long for the illegal words of a very specific theme image is rather limited. Future work may focus on censoring a group of synonyms simultaneously, \ie, semantic-wise censoring. A publisher of the embedding does not need to figure out every possibly sensitive word as he does in this paper. Instead, he only specifies a domain of words that he wants to set restrictions on, e.g. sexually explicit ones. We assume this is possible because the word embeddings of the synonyms tend to form a cluster in the feature space according to~\cite{word2vec}. This might also be a better approach to censoring the content.


\section{Conclusion}
\label{sec:Conclusion}
These years, the diffusion-model-based generative model is being rapidly improved by researchers as well as companies and becoming prevailing among the communities. However, the generated content may contain many sensitive content or even violate the taboo of our society. 
In this paper, we proposed a novel method to set restrictions on a popular personalization method, namely Textual Inversion to prevent it from being abused to craft illegal content. We inject some robust backdoors into the pseudoword of Textual Inversion, which will only be activated if there is some sensitive word in the input together with the pseudowords of Textual Inversion. We demonstrated that our approach is effective and robust. Further experiments verified its tolerance towards several naive countermeasures. 