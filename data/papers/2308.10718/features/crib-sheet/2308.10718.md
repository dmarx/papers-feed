- **Concept Censorship**: Regulating personalization models (Textual Inversion) to prevent malicious use while maintaining functionality for legitimate users.
  
- **Textual Inversion (TI)**: A lightweight personalization technique that crafts word embeddings (pseudo-words, noted as S*) to represent specific concepts, allowing users to generate personalized images without fine-tuning.

- **Backdoor Technique**: Injecting backdoors into Textual Inversion embeddings to censor sensitive words. Triggers are selected during training to prevent the model from generating images associated with these words.

- **Loss Function Modification**: The original loss function of Textual Inversion is modified to include a new term that balances the utility of the embedding while allowing for censorship of sensitive concepts.

- **Key Challenges**:
  - **Preserving Benign Fidelity**: Ensuring the embedding can still generate high-quality images.
  - **Preserving Benign Editability**: Allowing the censored pseudo-word to work with non-censored words for diverse outputs.
  - **Generality of Censorship**: Ensuring that all variations of a censored word are effectively blocked from generating sensitive content.

- **Denoising Diffusion Models**: The process involves iterative denoising of a noisy image to generate high-fidelity outputs. Key equations:
  - Inference process: 
    \[
    x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z
    \]
  - Training process:
    \[
    x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon
    \]
  - Optimization goal:
    \[
    L = \sum_{t=1}^{T} ||\epsilon - \epsilon_\theta(x_t, t)||^2
    \]

- **Experimental Validation**: Extensive experiments conducted on Stable Diffusion to demonstrate the effectiveness of the proposed method in censoring sensitive words while maintaining model utility.

- **Robustness Against Countermeasures**: The proposed method is designed to resist potential countermeasures that malicious users might employ to bypass censorship.

- **Ablation Studies**: Conducted to explore the design choices and validate the effectiveness of the backdooring approach in various scenarios. 

- **Public Resources**: Code, data, and results are available at [https://concept-censorship.github.io](https://concept-censorship.github.io).