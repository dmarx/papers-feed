\section{Introduction}%
\label{sec:introduction}


Knowledge source integration is the problem of meaningfully combining multiple knowledge sources.
%%
The problem is harder when (i) the sources are heterogeneous, i.e., adopt different vocabularies, formats, storage technologies, etc.; and (ii) the intended integration is virtual, i.e., is to be done dynamically, at query time.
%%
In the Semantic Web, where ``knowledge source'' usually means a set of OWL ontologies, the integration problem is often reduced to the ontology matching problem~\cite{Osman-I-2021}.
%%
In practice, however, determining correspondences between concepts and properties in ontologies is just one of the many issues involved.
%%
More often than not, the sources to be integrated don't support OWL or even RDF (think of graph databases, relational databases, REST APIs, CSV dumps, etc.) and either are just too large or change too frequently to be converted and ingested statically into a single knowledge base.


A scenario like this requires a more comprehensive solution.
%%
One that combines virtualization mechanisms to provide federated access to the sources, mapping mechanisms to reconcile differences in their vocabularies, and provenance mechanisms to enable telling which piece of knowledge came from which source.
%%
Although there are in the literature proposals to tackle each of these tasks separately, few attempt to provide an integrated solution.


In this paper, we present KIF\footnote{\KIFURL}, an open-source Python framework that uses Wikidata's data model and vocabulary~\cite{Vrandecic-D-2014} as a lingua franca to integrate heterogeneous knowledge sources.
%%
KIF is our attempt at a comprehensive solution to the hard version of the knowledge integration problem---the version in which the sources are heterogeneous and the integration is virtual.


The core abstraction of KIF is the store.
%%
A \emph{store} is an interface to a ``Wikidata view'' of a knowledge source, obtained by interpreting the source's content as a set of Wikidata-like statements.
%%
For example, KIF's built-in CSV store constructs Wikidata view of a CSV file by interpreting each of its cells (line-column pair) as a statement where the subject is given by the entity described by the line, the property is given by the header of the column, and the value by the content of the cell.
%%
Another built-in store type is the SPARQL store.
%%
It constructs a Wikidata view of a SPARQL endpoint by interpreting certain patterns in the endpoint's RDF graph as Wikidata-like statements.


% Another built-in store type is the SPARQL store which constructs a Wikidata view of a given SPARQL endpoint, which may or may not speak the Wikidata dialect of RDF.

% In the latter case, the user must provide a \emph{mapping} indicating how certain patterns in the endpoint's RDF are to be interpreted as Wikidata-like statements.


Having Wikidata as the target brings some advantages.
%%
First, KIF inherits a tried-and-tested data model with native support for structured data values plus the notions of references and qualifiers, used to represent provenance and context information.
%%
Second, if desired, one can reuse Wikidata's vast vocabulary, which at the time of writing has more than 110M items and 11K properties, covering various domains.
%%
Third, one can easily combine statements produced by any KIF store with statements coming from Wikidata itself, which can be accessed via an ordinary SPARQL store.
%%
Such a combination is done using a \emph{mixer} store, which virtually merges statements of one or more child stores.


The key to the flexibility of the store abstraction lies in its query interface.
%%
KIF stores are queried using a simple but expressive \emph{pattern language} defined in terms of Wikidata's data model.
%%
KIF includes a pattern compiler targeting SPARQL which can be parameterized with custom mappings to generate queries in RDF encodings other than Wikidata's.
%%
A mapping to the RDF encoding of PubChem~\cite{Kim-S-2023} (a large public base of chemical knowledge) is also included in the distribution.
%%
As we discuss later, this mapping was used together with other things to build an application that integrates statements about chemical compounds coming from PubChem, Wikidata, and IBM CIRCA~\cite{IBM-CIRCA} (a relational database of chemical data extracted from patents and other sources).


The rest of the paper is organized as follows.
%%
Section~\ref{sec:background} presents some background on Wikidata.
%%
Section~\ref{sec:design-and-implementation} presents the design and implementation of KIF\@.
%%
Section~\ref{sec:use-case-and-evaluation} discusses the evaluation of KIF over a use case in the domain of chemistry.
%%
Section~\ref{sec:related-work} discusses some related work.
%%
Section~\ref{sec:conclusion} presents our conclusions and future work.


% LocalWords:  CSV KIF lingua franca PubChem KIF's



%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "main"
%%% eval: (visual-line-mode 1)
%%% End:
