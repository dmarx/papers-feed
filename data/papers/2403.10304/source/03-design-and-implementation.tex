\section{Design and Implementation}%
\label{sec:design-and-implementation}


KIF is an integration framework based on Wikidata.
%%
The idea behind it is to use Wikidata to standardize the syntax and possibly the vocabulary of the underlying knowledge sources.
%%
Users can then query the sources through patterns described in terms of Wikidata's data model.
%%
The integration done by KIF is \emph{virtual} in the sense that syntax and vocabulary translations happen dynamically, at query time, guided by user-provided mappings.


As we mentioned before, the core abstraction of KIF is the store.
%%
A \emph{store} is an interface to a knowledge source.
%%
This can be a SPARQL endpoint, REST API, RDF file, CSV file, etc.
%%
The job of the store is to construct a ``Wikidata view'' of the knowledge source.
%%
The prototypical store is the SPARQL store, which we describe next.


\subsection{SPARQL Store}%
\label{sub:sparql-store}


The SPARQL store reads Wikidata-like statements from a given SPARQL endpoint.
%%
%If no mapping is specified at store creation time, it assumes that the endpoint speaks the Wikidata encoding of RDF\@.
%%
Here is how we create a SPARQL store pointing to WDQS, the public SPARQL query service of Wikidata:
%%
\pyconfile[firstline=1,lastline=2,linenos=true]{code/code.pycon}%
%%
At line~1, we import the namespace of KIF, whose Python module is called \code/kif_lib/.
%%
At line~2, we create a new store of type ``sparql'' pointing to WDQS and assign the result to variable \code/Wikidata/.
%%
As we did not pass an explicit mapping to the store constructor, it assumes the endpoint speaks Wikidata's RDF encoding, which is the case for WDQS\@.


We can read statements from the newly created \code/Wikidata/ store as follows:
%%
\pyconfile[firstline=3,lastline=7,linenos=true]{code/code.pycon}%
%%
At lines~3--5, we ask for statements with subject ``benzene'' (Q2270) and property ``median lethal dose (LED50)'' (P2240).
%%
The result is an iterator which is assigned to variable \code/it/.
%%
At line~6, we consume one statement from \code/it/ whose content is shown in line~7.
%%
Note that this is the same statement~\eqref{eq:stmt1} of Section~\ref{sec:background}.


\enlargethispage*{3pt}


\subsection{Patterns}%
\label{sub:patterns}


%%
%The apparent simplicity of the previous example is deceiving.
%%
%Let us take a closer look at what is going on behind the scenes.
%%
As we mentioned, in KIF, queries are specified as patterns defined in terms of Wikidata's data model.
%%
The \code{filter()} call in line~3 above is actually just a wrapper to a \code{match()} call, which evaluates a pattern over the knowledge source.


We can rewrite the previous \code/filter()/ in terms of \code/match()/ as follows:
%%
\pyconfile[firstline=8,lastline=11,linenos=true]{code/code.pycon}%
%%
At line~8, we create the pattern variable \code/x/ and use it to build the pattern \code/pat/ in lines~9--10.
%%
KIF patterns are templates for objects of the data model, i.e., objects in which certain parts are replaced by variables.
%%
Pattern \code/pat/ is a template for statements whose subject is ``benzene'' (Q2270) and whose snak is a value-snak with property ``LD50'' (P2240) and value \code/x/, i.e., any value.


% -- optional -- %
% Although we didn't restrict the type of \code/x/ when we create it in line~8, in KIF, variables that occur within a pattern are always assigned a specific object type.
% %%
% In the case of \code/pat/, because \code/x/ occurs as second argument of a \code/ValueSnak/, KIF knows it must be a value, and thus not a statement or snak (see Figure~\ref{fig:grammar}).
% %%
% What happens is that, at line~11, when the value-snak is constructed, variable \code/x/ is coerced into a homonymous variable of type ``value''.
% %%
% KIF ensures that homonymous variables always have compatible types within a same pattern.


Before detailing how the \code{match()} call in line~11 works (see Section~\ref{sub:the-match-call}), let us make a quick detour to show two other features of KIF.%, namely, its vocabulary module and its support for constraints.


\enlargethispage*{4pt}


\subsubsection{The vocabulary module}


When writing data model or pattern objects, we can use KIF's vocabulary module to make the code less verbose.
%%
For example, we can rewrite the previous pattern \code/pat/ (lines~9--10) more concisely as follows:
%%
\pyconfile[firstline=12,lastline=13,linenos=true]{code/code.pycon}%
%%
At line~12, we import KIF's Wikidata vocabulary module \code/wd/.
%%
At line~13, we use \code{wd.Q()} and \code{wd.P()} to construct the item ``benzene'' (Q2270) and property ``LD50'' (P2240) without having to write their URLs.
%%
But we can do better:
%%
\pyconfile[firstline=14,lastline=14,linenos=true]{code/code.pycon}%
%%
This constructs the same pattern by applying property \code{wd.median_lethal_dose}, which is predeclared in the \code/wd/ module, as if it were a Python function to arguments \code{wd.Q(2270)} and \code/x/.
%%
That the three versions construct exactly the same statement pattern object can be checked by a simple equality test:
%%
\pyconfile[firstline=15,lastline=16,linenos=true]{code/code.pycon}%


\subsubsection{Constraints}


Suppose we want to restrict the statements that match the previous pattern to those with a value in the range 4000--7000 mg/kg.
%%
We can do that by using method \code/where()/ to constraint the pattern as follows:
%%
\pyconfile[firstline=17,lastline=19,linenos=true]{code/code.pycon}%
%%
Method \code{where()} takes a boolean expression of variables.
%%
Here the resulting pattern, \code/pat3/ (lines~17--19), is a new pattern equal to \code/pat/ with the added constraint that the matched statement's value must be a quantity in mg/kg with amount~\code/x/ such that $4000\le\text{\code/x/}\le7000$, and with any lower- and upper-bound values.


%%
% TODO: Equal to -> extends.
%%


This ends our detour.  We can now get back to the \code{match()} method.


\subsection{The \texttt{match()} Method}%
\label{sub:the-match-call}


Method \code{match()} is the workhorse of the store API\@.
%%
It must be implemented by all store types and is the basis of the implementation of most other store API methods, including \code{filter()}.


The \code{match()} method takes a pattern $p$ as argument and returns a \emph{match} object which when iterated generates all instances of $p$ found in the store.
%%
% Although most of the time $p$ is a statement pattern, \code{match()} accepts any kind of pattern.
% %%
% For instance, if we give it the pattern $\text{\code{Quantity(8, Variable('u'))}}$ it matches all quantities with amount 8 and unit $u$ (any) in the store, independently of where such a quantity occurs (statement, qualifier, or reference).
%%
%%
The actual implementation of \code{match()} varies from store to store.
%%
But the general idea can be described as follows.
%%
First, the store compiles pattern $p$ into a query~$q$, written in query language of knowledge source.
%%
% (This compilation should take into account what pattern $p$ is trying to accomplish, e.g., if $p$ is a statement pattern, then $q$ should be a query that looks for things that are to be interpreted as Wikidata-like statements in the source.)
%%
Then it evaluates $q$ over the source, producing a result set $R$ such that each result $b$ in~$R$ is a binding of variables in~$q$.
%%
Finally, for each variable-value pair $(x,v)$ in~$b$, the store replaces the variable corresponding to $x$ in $p$ by the value $v$, generating a new match.


% To make matters more concrete, consider process of pattern compilation and evaluation used by the SPARQL store, illustrated in Figure~\ref{fig:pattern-evaluation}.


To make matters more concrete, let $p$ be pattern \code/pat3/ defined at the end of Section~\ref{sub:patterns} (lines~17--19).
%%
Figure~\ref{fig:pattern-evaluation} shows the steps taken by a SPARQL store evaluate the call \code/match(!$p$!)/.


In step~(1), the SPARQL store instantiates a new SPARQL compiler.
%%
Since no SPARQL mapping was given to the compiler, it assumes a default mapping targeting the RDF encoding of Wikidata.


In steps~(2) and~(3), the store uses the compiler to compile pattern $p$ into a SPARQL query~$q$ and a substitution~$\theta$.
%%
Note that compilation is compositional, i.e., the compilation of~$p$ is defined in terms of the compilation of its subpatterns.
%%
The substitution $\theta$ is a mapping from subpatterns of~$p$ into variables of~$q$.
%%
For instance, the $\theta$ of Figure~\ref{fig:pattern-evaluation} specifies that variable \code/?x/ of query~$q$ corresponds to variable \code/x/ of pattern~$p$.


In steps~(4) and~(5), the SPARQL store sends query~$q$ to the source's SPARQL endpoint and receives as a result the SPARQL result $R$.
%%
A SPARQL result is essentially a set of bindings of the variables selected by the query.
%%
Figure~\ref{fig:pattern-evaluation} shows that $R$ contains at least two bindings for variable \code/?x/, namely, 4700 and 6400.


\input{figs/pattern-evaluation}


Finally, in step~(6), for each binding~$b$ in $R$, the SPARQL store replaces the SPARQL variables in $\theta$ by their values in $b$ and applies the resulting substitution to the original pattern $p$ to obtain a new match.
%%
For example, the first match shown in step~(6) of Figure~\ref{fig:pattern-evaluation} is obtained by computing $\theta[\text{\code/?x/}\coloneq{4700}](p)$, i.e., replacing \code/?x/ by 4700 in~$\theta$ and applying the resulting substitution to~$p$.
%%
The result in this case is statement~\eqref{eq:stmt1} of Section~\ref{sec:background}.


\subsection{SPARQL Mapping for PubChem}%
\label{sub:sparql-mapping-for-pubchem}


The SPARQL store can be used to read statements from any SPARQL endpoint, provided it is supplied with an appropriate mapping.
%%
One such mapping already included in KIF is for the RDF distribution of PubChem~\cite{Fu-G-2015,Kim-S-2023}.
%%
Here is how we create a SPARQL store pointing to a local installation of PubChem's RDF\@:
%%
\pyconfile[firstline=20,lastline=22,linenos=true]{code/code.pycon}%


We won't go into detail here, but object \code/PubChemMapping()/ (line~22) tells the SPARQL store (actually, the SPARQL compiler) how to translate KIF patterns into graph patterns over PubChem's RDF graph.
%%
The resulting store, \code/PubChem/ (line~21), behaves as an ordinary SPARQL store.
%%
We can use it, for example, to obtain the mass (P2067) of benzene from PubChem:
%%
\pyconfile[firstline=23,lastline=28,linenos=true]{code/code.pycon}%


There are a couple of things to note here.
%%
First, the PubChem mapping provided by KIF adopts the Wikidata vocabulary whenever possible.
%%
For instance, it maps property ``mass'' (P2067) to the appropriate property in PubChem.
%%
The mapping also uses Wikidata units, e.g., ``dalton'' (Q483261) above.
%%
What it doesn't do is translate the ids of PubChem compounds.
%%
This explains the non-Wikidata URL in the subject of the returned statement (line~27).


This is also the reason we didn't use the Wikidata id of benzene, \code/wd.Q(2270)/, in the subject of the \code/filter()/ call above.
%%
Had we done that the result would be empty.
%%
Instead, we used the value-snak \code/wd.InChIKey('UHOV!\dots!')/.
%%
That is, we set the subject to any entity whose InChIKey (P235) is equal to ``UHOV\dots'', which happens to be the InChIKey of benzene.
%%
InChIKey is a universal identifier for chemical compounds which is defined in both Wikidata and PubChem.


Here is the pattern corresponding to the previous \code/filter()/ (lines~23--25):
%%
\pyconfile[firstline=30,lastline=32,linenos=true]{code/code.pycon}%
%%
It now should be clear what is happening: \code/filter()/ is searching for statements with subject~\code/x/, property ``mass'', and value~\code/y/, such that~\code/x/ is an entity with ``InChIKey'' equal to ``UHOV\dots''.


This example illustrates the use of a statement pattern (line~32) as a boolean constraint, i.e., as an argument to the \code/where()/ call.
%%
The constraint in this case plays the role of a \emph{fingerprint}, i.e., a test that identifies an entity indirectly, here using a universal identifier instead of a local one.
%%
The support for this kind of fingerprinting technique is crucial for enabling meaningful queries over combinations of knowledge sources, such as those obtained via mixer stores.


\subsection{The Mixer Store}%
\label{sub:the-mixer-store}


The mixer store combines one or more child stores into a new store which behaves as their virtual union.
%%
For example, we can combine \code/Wikidata/ (line~2) and \code/PubChem/ (lines~21--22) into a new store \code/mix/ of type ``mixer'' as follows:
%%
\pyconfile[firstline=33,lastline=35,linenos=true]{code/code.pycon}%
%%
From the user's point of view, \code/mix/ (line~33) is a store like any other.
%%
Its content can be thought of as the union of the statements in \code/Wikidata/ and \code/PubChem/.

At lines 34--35, we ask \code/mix/ for statements with subject ``benzene'', and assign the first two results to variables \code/stmt1/ and \code/stmt2/, respectively.
%%
One possibility here, for example, is that \code/mix/ returns first a statement from \code/Wikidata/ and then a statement from \code/PubChem/, say, those in lines~6--7 and~26--28.


% One possibility here, for example, is that \code/mix/ returns first from \code/Wikidata/ the statement in lines~6--7 (LD50 equals 4700 mg/kg) and then from \code/PubChem/ the statement in lines~26--28 (mass equals 78.11 dalton).


One way to determine which statement came from which child store, is to instruct them to attach extra references to their statements.
%%
For instance, here is how we can instruct \code/Wikidata/ to attach an extra reference to statements:
%%
\pyconfile[firstline=36,lastline=37,linenos=true]{code/code.pycon}%
%%
Now every statement produced by the \code/Wikidata/ store will be associated to one extra reference stating that the statement's ``reference URL'' (P854) is the address of the endpoint set in \code/Wikidata/.
%%
We won't go into details here, but the references of a statement can be obtained using the store API method \code{get_annotations()}.


To decouple statements from qualifiers, references, and rank---and avoid opaque ids---KIF introduces the notion of an annotation.
%%
An \emph{annotation} (or \emph{annotation record}) is a triple containing qualifiers (set of snaks), references (set of reference records), and rank.


In KIF, statements are identified by their content (subject and snak) and can be associated with one or more annotation records in a store.
%%
This deviates from the Wikidata RDF representation~\cite{Wikibase-RDF-Dump-Format}, in which statements are identified by opaque ids which carry its qualifiers, references, and rank.
%%
The rationale of KIF's approach is to relieve users from having to deal with opaque, meaningless ids---push this work to the framework.


\subsection{Other Store Types and Methods}%
\label{sub:other-store-types-and-methods}


Besides the SPARQL store and the mixer store, KIF comes with an RDF store and a CSV store.
%%
The RDF store reads statements from RDF files.
%%
It is essentially a SPARQL store that uses RDFLib~\cite{RDFLib} to load RDF files and evaluate SPARQL queries over their contents.
%%
The CSV store reads statements from CSV files.
%%
It expects a mapping specifying how line-columns pairs are to be interpreted as statements.
%%
Currently, the CSV store is implemented as a wrapper to the RDF store which first converts the CSV to RDF before loading it.
%%
We have plans for a more direct, non-RDF-based implementation though.


All stores implement a common store API, containing core methods \code{filter()} and \code{match()} discussed before plus methods to get statement annotations (qualifiers, references, and ranks) and entity descriptors (labels, aliases, and descriptions).
%%
The store API also has convenience methods for testing and counting pattern occurrences, and methods for obtaining store metadata.
%%
For more details, see the documentation of KIF~\cite{KIF}\@.


% LocalWords:  KIF CSV WDQS firstline lastline linenos kif sparql eq stmt
% LocalWords:  snak LD KIF's wd PubChem PubChem's PubChemMapping dalton
% LocalWords:  UHOV InChIKey snaks RDFLib



%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "main"
%%% eval: (visual-line-mode 1)
%%% End:
