\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock In \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Baykal et~al.(2024)Baykal, Cutler, Dikkala, Ghosh, Panigrahy, and Wang]{baykal2024alternating}
Cenk Baykal, Dylan Cutler, Nishanth Dikkala, Nikhil Ghosh, Rina Panigrahy, and Xin Wang.
\newblock Alternating updates for efficient transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2), 1994.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, 2020.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{allenai:arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv:1803.05457v1}, 2018.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine learning challenges workshop}. Springer, 2005.

\bibitem[De~Marneffe et~al.(2019)De~Marneffe, Simons, and Tonhauser]{de2019commitmentbank}
Marie-Catherine De~Marneffe, Mandy Simons, and Judith Tonhauser.
\newblock The commitmentbank: Investigating projection in naturally occurring discourse.
\newblock In \emph{proceedings of Sinn und Bedeutung}, volume~23, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}. Ieee, 2009.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{dolan2005automatically}
Bill Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Third international workshop on paraphrasing (IWP2005)}, 2005.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural networks.
\newblock In \emph{Proceedings of the thirteenth international conference on artificial intelligence and statistics}. JMLR Workshop and Conference Proceedings, 2010.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, et~al.]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{arXiv preprint arXiv:2402.00838}, 2024.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2016.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendryckstest2021}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International conference on machine learning}. PMLR, 2015.

\bibitem[Johannes~Welbl(2017)]{SciQ}
Matt~Gardner Johannes~Welbl, Nelson F.~Liu.
\newblock Crowdsourcing multiple choice science questions.
\newblock 2017.

\bibitem[Korthikanti et~al.(2022)Korthikanti, Casper, Lym, McAfee, Andersch, Shoeybi, and Catanzaro]{korthikanti2022reducing}
Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Reducing activation recomputation in large transformer models.
\newblock \emph{arXiv preprint arXiv:2205.05198}, 2022.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{liu2020understanding}
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han.
\newblock Understanding the difficulty of training transformers.
\newblock \emph{arXiv preprint arXiv:2004.08249}, 2020.

\bibitem[Ma et~al.(2023)Ma, Li, Yuan, and Zhao]{ma2023denseformer}
Haoyan Ma, Xiang Li, Xia Yuan, and Chunxia Zhao.
\newblock Denseformer: A dense transformer framework for person re-identification.
\newblock \emph{IET Computer Vision}, 17\penalty0 (5), 2023.

\bibitem[Ma et~al.(2024)Ma, Yang, Xiong, Chen, Yu, Zhang, May, Zettlemoyer, Levy, and Zhou]{ma2024megalodon}
Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou.
\newblock Megalodon: Efficient llm pretraining and inference with unlimited context length.
\newblock \emph{arXiv preprint arXiv:2404.08801}, 2024.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{OpenBookQA2018}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Muennighoff et~al.(2024)Muennighoff, Soldaini, Groeneveld, Lo, Morrison, Min, Shi, Walsh, Tafjord, Lambert, Gu, Arora, Bhagia, Schwenk, Wadden, Wettig, Hui, Dettmers, Kiela, Farhadi, Smith, Koh, Singh, and Hajishirzi]{muennighoff2024olmoeopenmixtureofexpertslanguage}
Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah~A. Smith, Pang~Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi.
\newblock Olmoe: Open mixture-of-experts language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2409.02060}.

\bibitem[Peebles \& Xie(2022)Peebles and Xie]{Peebles2022DiT}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock \emph{arXiv preprint arXiv:2212.09748}, 2022.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, and Gordon]{roemmele2011choice}
Melissa Roemmele, Cosmin~Adrian Bejan, and Andrew~S Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal reasoning.
\newblock In \emph{2011 AAAI spring symposium series}, 2011.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9), 2021.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock \emph{arXiv preprint arXiv:1904.09728}, 2019.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2017sparsely}
N~Shazeer, A~Mirhoseini, K~Maziarz, A~Davis, Q~Le, G~Hinton, and J~Dean.
\newblock The sparsely-gated mixture-of-experts layer.
\newblock \emph{Outrageously large neural networks}, 2017.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning, Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in natural language processing}, 2013.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock \emph{arXiv preprint arXiv:2402.00159}, 2024.

\bibitem[Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant]{talmor2018commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge.
\newblock \emph{arXiv preprint arXiv:1811.00937}, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Wang(2021)]{mesh-transformer-jax}
Ben Wang.
\newblock {Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Wortsman et~al.(2023)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, et~al.]{wortsman2023small}
Mitchell Wortsman, Peter~J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John~D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et~al.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock \emph{arXiv preprint arXiv:2309.14322}, 2023.

\bibitem[Xie et~al.(2023)Xie, Zhang, Guo, Tan, Bian, Awadalla, Menezes, Qin, and Yan]{xie2023residual}
Shufang Xie, Huishuai Zhang, Junliang Guo, Xu~Tan, Jiang Bian, Hany~Hassan Awadalla, Arul Menezes, Tao Qin, and Rui Yan.
\newblock Residual: Transformer with dual residual connections.
\newblock \emph{arXiv preprint arXiv:2304.14802}, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{zhang2019root}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\end{thebibliography}
