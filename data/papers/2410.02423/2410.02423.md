---
abstract: |
  In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm for solving imaging inverse problems. PnP methods leverage the strength of pre-trained denoisers, often deep neural networks, by integrating them in optimization schemes. While they achieve state-of-the-art performance on various inverse problems in imaging, PnP approaches face inherent limitations on more generative tasks like inpainting. On the other hand, generative models such as Flow Matching pushed the boundary in image sampling yet lack a clear method for efficient use in image restoration. We propose to combine the PnP framework with Flow Matching (FM) by defining a time-dependent denoiser using a pre-trained FM model. Our algorithm alternates between gradient descent steps on the data-fidelity term, reprojections onto the learned FM path, and denoising. Notably, our method is computationally efficient and memory-friendly, as it avoids backpropagation through ODEs and trace computations. We evaluate its performance on denoising, super-resolution, deblurring, and inpainting tasks, demonstrating superior results compared to existing PnP algorithms and Flow Matching based state-of-the-art methods.
author:
- Ségolène Martin$^{1, *}$ Anne Gagneux$^{2, *}$ Paul Hagemann$^{1, *}$ Gabriele Steidl$^{1}$
- |
    
    
bibliography:
- biblio.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "PnP-Flow: Plug-and-Play Image Restoration with Flow Matching"
---





# Introduction

Image restoration aims to recover an unknown image $x \in \R^d$ from a degraded observation $y \in \R^m$ $$y = {\mathrm{noisy}}(H x),$$ where $H: \R^d \to \R^m$ is a (linear) degradation operator and $\mathrm{noisy}$ describes the underlying noise model. Since the problem is usually ill-posed and high dimensional, its treatment is challenging. We assume that the image $x$ is sampled from a random variable $X \in \mathbb{R}^d$ with a density $p_X$, and the observation $y$ from a random variable $Y \in \mathbb{R}^m$ with a density $p_Y$. Then the maximum a posteriori estimator: searches for the value with the highest probability of the posterior $$\begin{aligned}
 \label{eq:generic_MAP}
\argmax_{x \in \R^d} \left\{ \log p_{X|Y=y}(x) \right\} =
\argmax_{x \in \R^d} \left\{ \log p_{Y|X=x}(y) + \log p_{X}(x) \right\},
\end{aligned}$$ where the first term of the right hand side is the fidelity to the data and the second term represents the prior distribution of the image. Since $p_X$ is generally unknown, and in the absence of training data, it is standard to instead consider a regularized optimization problem of the form: $$\label{eq:min_pb_regularized}
\argmin_{x \in \R^d} \left\{F(x) + R(x)
\right\},$$ where $F(x):= -\log p_{Y|X=x}(y)$ and $R:\R^d \to \R$ usually enforces some assumptions on the solution and ensures the existence of a (unique) minimizer. The regularized minimization problem in can be efficiently handled using proximal splitting methods .

Plug-and-Play (PnP) methods build upon the insight that the proximal step on the regularization term is effectively a denoising operation. They can be performed with non-learned denoisers such as BM3D , however recently these were outperformed by neural network-based approaches . While PnP methods have demonstrated considerable effectiveness, recent advances in generative models offer a more sophisticated framework for learning priors directly from data, surpassing the limitations of hand-crafted or neural denoisers.

In particular, generative models are used as regularizers in . Most of these references learn an invertible transport map $T$ between a Gaussian latent distribution and the data distribution, using the change of variables formula to obtain the estimated prior $p_X$ as an explicit function of the known latent density and the map $T$. In particular the normalizing flow network with the Real-NVP architecture is by default invertible and has an easy-to-evaluate log determinant of the Jacobian of $T$. However, this architecture yields rather poor results on large images. Therefore, recent work focuses more on learning *diffusion* models or *Flow Matching* models , which do not have these architectural constraints and scale better to large images. The main idea is to establish a path between the latent and the target distribution, which can be optimized in a *simulation-free* way.

In this paper, we focus on Flow Matching models, which learn a velocity field $v : [0,1] \times \R^d \longrightarrow \R^d$ going from the latent to the data distribution. Once the velocity field $v$ is learned, sampling from the target distribution can be done by solving an ODE $$\label{eq:forward_ODE}
 \partial_t f(t,x) = v_t(f(t,x)), \quad       f(0,x) = x,$$ where $x$ is sampled from the latent distribution.

However, using Flow Matching to regularize inverse problems such as image restoration tasks is non-trivial. Indeed, when directly solving the MAP problem using the change of variables formula, one faces numerical challenges due to the backpropagation through the ODE. We circumvent this problem by integrating the implicit Flow Matching prior into a custom denoiser that we plug into a Forward-Backward algorithm.

#### Contributions.

In this paper, we combine PnP methods with Flow Matching and propose a PnP Flow Matching algorithm. Our contributions are as follows:

- Inspired by optimal transport Flow Matching, we design a time-dependent denoiser based on a pre-trained velocity field $v$ learned through Flow Matching.

- This denoiser is integrated into an adapted Forward-Backward Splitting PnP framework, which cycles through a gradient step on the data-fidelity term, an interpolation step to reproject iterates onto flow trajectories, and a denoising step.

- Our algorithm is simple to implement, requires no backpropagation through the network, and is highly efficient in both memory usage and execution time compared to existing Flow Matching-based methods.

- We show that, on two datasets and across denoising, deblurring, inpainting, and super-resolution tasks, our method consistently outperforms state-of-the-art Flow Matching-based and PnP methods in terms of both PSNR and SSIM metrics. The code with all benchmark methods is available at: <https://github.com/annegnx/PnP-Flow>.

<figure id="fig:2DEX_gaussian_to_gaussian">
<figure id="fig:figure1">
<span class="image placeholder" data-original-image-src="figures/toy/final_0.00_toy.pdf" data-original-image-title="" width="\textwidth"></span>
<figcaption>t = 0</figcaption>
</figure>
<p></p>
<figure id="fig:figure2">
<span class="image placeholder" data-original-image-src="figures/toy/final_0.56_toy.pdf" data-original-image-title="" width="\textwidth"></span>
<figcaption>t = 0.56</figcaption>
</figure>
<figure id="fig:figure3">
<span class="image placeholder" data-original-image-src="figures/toy/final_0.89_toy.pdf" data-original-image-title="" width="\textwidth"></span>
<figcaption>t = 0.89</figcaption>
</figure>
<figcaption>Our method on a 2D denoising task (<span class="math inline">\(\sigma=1.5\)</span>) with Gaussian distributions. An OT Flow Matching model is trained to sample from <span class="math inline">\(P_1 = \mathcal N(m, s^2 \mathrm{Id})\)</span>, with <span class="math inline">\(m=7\)</span> and <span class="math inline">\(s=0.5\)</span>. At each time step, it performs a standard gradient step on the datafit, followed by a projection onto flow trajectories at time <span class="math inline">\(t\)</span>, and finally applies the time-dependent denoiser <span class="math inline">\(D_t\)</span>.</figcaption>
</figure>

# Background

We next provide background on both Plug-and-Play algorithms and Flow Matching models.

## Plug and Play

PnP algorithms were introduced as extensions of proximal splitting methods like Forward-Backward Splitting (FBS) or the alternating direction method of multipliers (ADMM). These are first-order optimization algorithms for solving problem , alternating between a proximal step on the regularization and a gradient and/or proximal step on the data-fidelity function. The proximal operator $\text{\normalfont prox}_{\gamma R}: \R^d \to \R^d$, $\gamma >0$ is defined by $\text{\normalfont prox}_{\gamma R} (y) \coloneqq \argmin_{x \in \R^d} \left\{\frac12 \| y-x\|^2 + \gamma R(x) \right\}$. In this paper, we assume that the data-fidelity $F$ is differentiable with Lipschitz continuous gradient and we focus on the FBS algorithm, corresponding to Algorithm . The convergence of FBS to a minimizer of $F+R$ is guaranteed as long as the stepsize is chosen as $\gamma \in (0,1/\text{Lip} (\nabla F))$, where $\text{Lip} (\nabla F)$ is the Lipschitz constant of $\nabla F$ .

Observing that computing $\text{\normalfont prox}_{\gamma R}$ corresponds to solving a Gaussian denoising problem with regularization $\gamma R$, proposed to replace it with an *off-the-shelf* denoiser $D: \R^d \longrightarrow \R^d$, which can be independently designed or learned (see Algorithm ).

Many PnP-FBS methods, such as use neural network denoisers. Most of these PnP-FBS algorithms do not converge and convergence guarantees usually assume non-expansiveness of the denoiser . Yet, constraining the Lipschitz constant of a neural network, e.g., through weight clipping, spectral normalization or averaged operator constructions harms in general its expressiveness .

## Flow Matching

Let $\mathcal P_2(\R^d)$ denote the subspace of probability measures on $\R^d$ having a finite second moment. Let $P_0$ denote the latent probability measure and $P_1$ the target probability measure (i.e., the data distribution). We denote $\cdot_\sharp$ the push-forward operation. We denote $\Gamma(P_0,P_1)$ the set of couplings $\pi \in \mathcal P(\R^d \times \R^d)$ having marginals $P_i \in \mathcal P_2(\R^d)$, $i=0,1$, and $\Gamma_{\text{o}}(P_0,P_1)$ the set of optimal couplings minimizing the Wasserstein-2 distance . We now start with any coupling $\pi \in \Gamma(P_0,P_1)$.

The core idea behind Flow Matching is to define a specific target probability path $t \mapsto P_t$, $t \in [0,1]$, between $P_0$ and $P_1$. Let $P_t \coloneqq (e_t)_\sharp \pi$, where the map $e_t(x_0, x_1) := (1-t)x_0 + tx_1$ interpolates between (a latent sample) $x_0$ and (a data sample) $x_1$. The path $P_t$ can be shown to be an absolutely continuous curve, so there exists a Borel vector field $v:[0,1]\times \R^d\to\R^d$ such that the curve satisfies the *continuity equation* $$\label{eq:continuity}\tag{\text{CE}}
\partial_t P_t +  \nabla\cdot(P_t v_t) = 0$$ in the sense of distributions . In addition, there exists a solution $f:[0,1] \times \R^d \to \R^d$ of the ODE $$\begin{aligned}
\label{eq:flow_ode}
\partial_t f(t,x)=v_t(f(t,x)), \quad f(0,x)=x
\end{aligned}$$ such that $P_t = f(t, \cdot)_\sharp P_0$. If $f$ is known, then a sample $x_1$ from the target distribution $P_1$ can be drawn by first sampling $x_0 \sim P_0$ and then defining $x_1 = f(1, x_0)$.

The goal of Flow Matching is to learn the velocity field of the flow ODE . This learning process consists in minimizing the loss function $$\begin{aligned}
\label{fm}
    \mathcal{L}_{\text{FM}}(\theta) :=\E_{t\sim \mathcal U [0,1],x\sim P_t}\left[\|v^\theta_t(x)-v_t(x)\|^2\right],
\end{aligned}$$ where $v^\theta$ is parametrized by a neural network with weights $\theta$. Unfortunately, in practice, the true velocity field $v_t(x)$ is not available. However, showed that minimizing $\mathcal{L}_{\text{FM}}(\theta)$ is equivalent to minimizing the Conditional Flow Matching (CFM) loss: $\mathcal{L}_{\text{CFM}}(\theta)  = \mathcal{L}_{\text{FM}}(\theta) + \text{const}$, where $$\begin{aligned}
 \label{cfm}
\mathcal{L}_{\text{CFM}}(\theta):=\E_{t\sim \mathcal U [0,1],(x_0,x_1)\sim\pi}\left[\|v^\theta_t \left(e_t(x_0, x_1)\right)-(x_1-x_0)\|^2\right] .
\end{aligned}$$ Minimizing this loss only requires sampling from the coupling $\pi$. For example, if $\pi = P_0 \times P_1$ is the independent coupling, we only need samples from the latent and target distributions. If instead, $\pi \in \Gamma_{\text{o}}(P_0, P_1)$ is an optimal coupling, then we can use a standard optimal transport solver to (approximately) sample from $\pi$, as proposed in .

#### Straight-line flows

Let $v\colon [0,1] \times \R^d \longrightarrow \R$ and let $f$ be a solution to the flow ODE associated with $v$. Given $(X_0,X_1) \sim \pi$, we call $(f,v)$ a *straight-line Flow Matching pair* connecting $X_0$ and $X_1$ if $X_t = f(t,X_0)$ almost surely, where $X_t$ is defined as $X_t:= e_t((X_0,X_1)) = t X_1 + (1-t)X_0$ . Note that this directly implies $v_t(f(t,X_0)) = \partial_t f(t,X_0)  = \partial_t X_t = X_1-X_0$ almost surely.

We will focus on such straight-line flows later in the paper. Straight or nearly straight paths are preferred since they represent the shortest distance between two points and can be simulated with fewer steps of an ODE solver. This has been explored in many recent works that want to speed up sampling, such as . A particular case of straight-line pair is given by OT couplings . Indeed, for $\pi \in \Gamma_{\text{o}}(P_0, P_1)$ with Monge map $T: \R^d \to \R^d$ such that $X_1=T(X_0)$, there exists a velocity field $v$ for which the pair $(P_t, v_t)$ is a solution of the continuity equation , and such that for all $x \in \R^d$ $$\label{eq:velocity_OT}
    v_t (f(t, x)) = T(x) - x,$$ where $f$ is the solution of the ODE , see for the precise conditions. By integrating , it is clear that $(f, v)$ is a straight-line Flow Matching for the coupling $(X_0, T(X_0))$, since $X_t = (1-t)X_0 + tX_1 = (1-t)X_0 + tT(X_0) = X_0 + \int_0^t v_s(f(s,X_0)) ds = f(t,X_0)$.

# PnP meets Flow Matching

## Denoising Operator from Flow Matching

Let $X_0 \sim P_0$ and $X_1 \sim P_1$ with joint distribution $(X_0, X_1) \sim \pi$. Assume we have access to a pre-trained velocity field $v^\theta$. Then we define, for $t\in [0,1]$, the following time-dependent denoiser $$\label{eq:denoiser}
    D_t  \coloneqq \mathrm{Id}+ (1-t) v^\theta_t,$$ and we propose to use it within a PnP framework.

To motivate the choice of the denoiser in , recall that for a fixed time $t \in [0, 1]$, the minimizer $v_t^*$ of the CFM loss over all possible vector fields reads $$v_t^* (x) = \mathbb{E}[X_1 - X_0|X_t = x],$$ where $X_t := e_t(X_0, X_1) = (1-t) X_0 + t X_1$ . Assume we are in the ideal case where $v_t^\theta= v_t^*$. Then it follows that, for any $x \in \R^d$ and $t \in [0,1]$, $$\begin{aligned}
      D_t (x) & = x + (1-t) v^*_t(x)  \label{helper}\\
      & = \mathbb E_{(X_0, X_1) \sim \pi} \left[ X_t + (1-t) (X_1 - X_0) | X_t = x\right] \nonumber \\
      & = \mathbb E_{(X_0, X_1) \sim \pi} \left[ X_1 | X_t = x \right] .
\end{aligned}$$ Hence, the operator $D_t$ can be understood at the best approximation of $X_1$ given the knowledge of $X_t$, which is also used in and .

Just as standard PnP denoisers minimize the MSE loss between noisy and clean samples, the operator $D_t$ is the minimizer of $L^2$-problem $\min_{g} \E_{(X_0, X_1) \sim \pi} [||X_1 -g(X_t)||^2]$, projecting any noisy point taken along the path onto the target distribution. In particular, the following proposition holds, demonstrating that the best denoising performance is achieved with straight-line flows.

<div class="proposition">

**Proposition 1**.  * Assume $v:=v^\theta$ is continuous and assume that, given $v$, the Flow ODE has a unique solution $f$. Then the denoising loss $\mathbb{E}_{(X_0,X_1) \sim \pi}[\Vert D_t(X_t) -X_1 \Vert^2]$ is equal to 0 for all $t \in [0,1]$, if and only if the couple $(f, v)$ is a straight-line Flow Matching pair between $X_0$ and $X_1$.*

</div>

The proof can be found in Appendix .

As stated in Section , a special case of straight-line flows is given by OT Flow Matching for which we recover $D_t(X_t) = X_1$. Indeed, in this case, $D_t$ reduces to $T(X_0) = X_1$, with $T$ the Monge map between $P_0$ and $P_1$. We next discuss whether diffusion models induce straight-line flows.

<div class="remark">

**Remark 2** (Flow Matching versus diffusion models). Contrary to OT Flow Matching, diffusion models, which are also flow ODE methods, do not generally induce straight paths. Indeed, during diffusion training, the target probability path takes the form $\tilde{X}_t = \alpha_t X_0 + \beta_t X_1$ with $\alpha_t \in (0,1)$ and $\beta_t =\sqrt{1-\alpha_t^2}$ , which clearly does not match the desired straight path $X_t = (1-t)X_0 + tX_1$. The non-straightness of the flow generated by diffusion models is illustrated in . On the other hand, a non-straight path obtained with Flow Matching (for instance, using an independent coupling $\pi$) can be *rectified* to a straighter one following the procedure described in .

</div>

## PnP Flow Matching Algorithm

In the previous section, we built a denoiser $D_t$ (defined in ). We now want to plug it in a Forward-Backward Splitting algorithm in order to solve inverse problems. Yet, our algorithm differs from the classical PnP-FBS (Algorithm ) in two key aspects. First, the iterations of the our algorithm depend on time because of the definition of $D_t$. Second, we introduce an intermediate reprojection step between the gradient step on the data-fidelity term and the denoising step. More precisely, at each time $t \in [0,1]$, given the current iterate $x$, our algorithm does the following updates:

1.  **Gradient step:** a gradient step on the data-fidelity term, mapping $x$ to $z= x- \gamma \nabla F(x)$ for a given learning rate $\gamma >0$.

2.  **Interpolation step:** In a standard Forward-Backward scheme, the denoiser operator is applied right after the gradient step. Yet, as discussed earlier, our operator $D_t$ was specifically designed to correctly denoise inputs drawn from the straight path $X_t = (1-t) X_0 + t X_1$. If the output $z$ from the gradient step at time $t$ does not lie in the support of $X_t$, there is a high chance that the denoising will not be effective, hence the need to “reproject” it along the flow paths before applying $D_t$. To achieve this, we perform a linear interpolation on $z$, as illustrated in Figure : at time $t$, we define $\tilde{z} = (1-t)\varepsilon + t z$, where $\varepsilon$ is a noise sample drawn from $P_0$. Note that while $\varepsilon$ is sampled from $P_0$, it is not necessarily coupled to $z \sim P_1$ via $\pi$. If it were, $D_t$ would map $\tilde{z}$ directly back to $z$, annihilating the denoising effect.

3.  **PnP Denoising step**: the operator $D_t$ is applied to the output $\tilde{z}$ of the interpolation step, regularizing the current image by pushing it towards the distribution of $X_1$.

 The resulting discrete-time algorithm is given in Algorithm . Figure illustrates the three steps of the algorithm on a denoising problem with a Gaussian prior.

<div class="center">

</div>

<div class="remark">

**Remark 3** (Averaging in the denoising step). Instead of drawing one noise realization, we can also average over multiple samples $\varepsilon \sim P_0$ in the last step of the algorithm: $x_{n+1} \coloneqq \E_{\varepsilon \sim P_0}[D_{t_n} (\tilde{z}_n(\varepsilon))]$ with $\tilde z_{n}(\varepsilon) \coloneqq (1-t_n)  \varepsilon +  t_n z_n$. The algorithm’s output is then deterministic. In practice, averaging over a few realizations slightly improves the numerical results.

</div>

#### Time dependent learning rate

Using a constant learning rate independent of time can give too much importance to the data fit. For example, if $\gamma_t = 1$ for all $t \in [0,1]$ on a simple denoising task, the algorithm will return the noisy sample $y$ since $D_1 = \rm{Id}$. To prevent this, $\gamma_t$ should decrease with $t$ to balance the contributions of the datafit and the denoiser. We set $\gamma_t = (1-t)^\alpha$ with $\alpha \in (0,1]$ for the remainder of the paper. This choice yields good numerical results in our experiments, but alternative values for $\gamma_t$ could also be explored.

#### Convergence

Assuming that the sequence produced by the algorithm is bounded in the infinite time regime, we have the following convergence result.

<div class="proposition">

**Proposition 4**.  * Assume that $F: \R^d \to \R$ is continuously differentiable and that the learned vector field $v: [0,1] \times \R^d  \rightarrow \R^d$ is continuous. Let the time sequence $(t_n)_{n \in \mathbb{N}}$ satisfy $\sum_{n=0}^\infty (1-t_n) < +\infty$ and let $\gamma_n \coloneqq 1-t_n$, $n \in \mathbb{N}$. If the sequence $(x_n)_{n\in \mathbb{N}}$ obtained by Algorithm is bounded, then it converges.*

</div>

The proof can be found in Appendix .

# Related Work

Our approach combining PnP with Flow Matching relates to several existing methods.

#### Pre-trained Flow Matching methods

Using pre-trained Flow Matching models for regularizing image inverse problems has been the focus of several recent works.

OT-ODE assumes a Gaussian latent distribution and uses Tweedie’s formula to derive a new velocity field $\tilde{v}_t(x, y) = \E[X_1 - X_0 \mid X_t = x, Y = y]$ from the original velocity field $v_t(x) = \E[X_1 - X_0 \mid X_t = x]$, all without requiring retraining. In practice, they sample from the posterior distribution $X_1 \mid Y$ by solving the ODE with the new velocity field using an Euler scheme.

In , the authors introduce Flow-Priors, a method to tackle the MAP problem by approximating it as a sequence of time-dependent MAP subproblems. Using Tweedie’s formula, they show that for $t < 1$, the gradient of the distribution $P_t$ of $X_t$ can be computed in closed form, allowing for the use of gradient descent to optimize these subproblems. However, the closed-form expression for the gradient relies on the assumption of an independent coupling $\pi$ and a Gaussian latent distribution. Besides, their method requires computing $\mathrm{Tr}\,\nabla v^\theta$, which is expensive.

In , an implicit regularization approach called D-Flow is considered: instead of minimizing the Gaussian data-fidelity function $x \mapsto \Vert Hx - y \Vert^2$, they minimize the latent loss $z \mapsto \Vert H(f(1,z)) - y \Vert^2$, where $f$ is a solution to the flow ODE given the pre-trained network. The two problems are theoretically equivalent since $f(1, \cdot)$ is invertible. However, since the latent loss is not convex, first- or second-order optimization methods may not find the global minimizer. Interestingly, this is beneficial because the true minimizer of the original problem is simply the pseudo-inverse, which may not be desirable. The authors optimize the latent loss by backpropagating through the ODE solution with a few Euler steps, though this remains computationally expensive.

#### PnP diffusion methods

While we present the first PnP method based on Flow Matching, related works combine diffusion models with the PnP framework. To our knowledge, the following are the only methods using PnP with pre-trained generative models instead of standard denoisers.

In , a Half Quadratic Splitting algorithm is used, alternating between a proximal step on the data-fidelity term and a proximal step on the regularization. Following the PnP strategy, the proximal step for the regularization term is replaced with a denoising step using a pre-trained diffusion model. The denoiser they use is reminiscent of the one we use, where the velocity is replaced by the gradient of the score function. Their method also includes an interpolation step with random noise, mapping the estimated image at each iteration back to the diffusion path.

Finally, conditional image restoration is explored in , which uses a more relaxed definition of “plug and play”. They integrate a pre-trained diffusion model under different conditions, leading to a variational objective similar to methods like .

# Numerical Experiments

## Baselines

We benchmark our method against three state-of-the-art Flow Matching-based restoration methods: OT-ODE , D-Flow and Flow Priors . As no official implementations were publicly available for these methods, we developed our own based on the descriptions provided in their respective publications. We have made every effort to ensure faithful implementations, included in the code attached to this paper. We also benchmark our method against PnP-Diff , a PnP algorithm based on diffusion models. Additionally, we compare our approach with the state-of-the-art PnP-FBS .

## Experimental setup

#### Datasets

We evaluate all methods on two datasets: CelebA , with images resized to $128 \times 128$, and AFHQ-Cat, a subset of the Animal FacesHQ dataset focused on the *cat* class, with images resized to $256 \times 256$. All images are normalized to the range $[-1, 1]$. For CelebA, we use the standard training, validation, and test splits. For AFHQ-Cat, as no validation split is provided, we randomly select 32 images from the test set to create a validation set.

#### Models

For each dataset, we trained a Flow Matching model from scratch using the Mini Batch OT Flow Matching approach , as this choice of coupling usually leads to straight paths. We used a standard Gaussian as the latent distribution and a U-Net taken from as the model. The training parameters were a learning rate of $10^{-4}$, 200 epochs with a batch size of 128 for CelebA, and 400 epochs with a batch size of 64 for AFHQ-Cat. We train the denoising network for the PnP method PnP-GS employing the same U-Net architecture. For PnP-Diff, because training a diffusion model with the same U-Net architecture as Flow-Matching yielded poor results due to insufficient number parameters, we used the pre-trained model from , implemented in the DeepInv library[^1]. Note that the pre-trained diffusion model was trained on the FFHQ dataset , making the comparison indirect, but we had no alternative.

#### Settings for the experiments

We evaluate the methods using 100 test images across five restoration problems: denoising with Gaussian noise ($\sigma=0.2$); deblurring using a $61\times 61$ Gaussian kernel ($\sigma_b=1.0$ for CelebA, $\sigma_b=3.0$ for AFHQ-Cat); super-resolution ($2\times$ downsampling for CelebA, $4\times$ for AFHQ-Cat); box-inpainting with a centered $s\times s$ mask ($s=40$ for CelebA, $s=80$ for AFHQ-Cat); and random pixel inpainting ($70 \%$ masked pixels). For deblurring, super-resolution, and box-inpainting, we add Gaussian noise with $\sigma=0.05$, and for random inpainting $\sigma=0.01$.

#### Hyper-parameters

We optimize the hyper-parameters for each method using a grid search on the validation set, selecting the configuration that yields the highest Peak Signal-to-Noise Ratio (PSNR). The optimal values identified for each dataset and problem scenario are detailed in Appendix . Our proposed method has two hyper-parameters: the exponent $\alpha$ in the learning rate schedule $\gamma_n = (1-t_n)^{\alpha}$, and the number of uniformly spaced time steps was set to $N = 100$ for most experiments. We averaged the results of the denoising step over 5 realizations of the interpolation step.

## Main results

We report benchmark results for all methods across several restoration tasks, measuring average PSNR and Structural Similarity (SSIM) on 100 test images. To ensure reproducibility, all experiments were seeded. Results are presented in Table  for CelebA and Table  for AFHQ-Cat. “N/A” indicates cases where the method is inapplicable; for instance, PnP-GS is not designed for generative tasks like box inpainting, and PnP-Diff relies on a diffusion model trained on another dataset, making its evaluation on box inpainting inappropriate.

The tables show that our method consistently ranks first or second in both reconstruction metrics across all tasks and datasets. More importantly, it demonstrates stability across tasks, unlike other methods. For example, D-Flow and Flow-Priors perform well in box inpainting but struggle with denoising, while PnP-GS, PnP-Diff, and OT-ODE excel in denoising and deblurring but perform worse on pseudo-generative tasks.

In terms of visual quality (Fig. , Fig. , and Appendix ), our method produces realistic, artifact-free images, though sometimes slightly over-smoothed. While D-Flow generates realistic images, it occasionally suffers from hallucinations (e.g., eye color shifts in CelebA denoising tasks). Flow-Priors introduces noise and artifacts, while OT-ODE captures textures well but struggles with image generation. Finally, Appendix  shows the progression of the reconstruction given by our method PnP-Flow with respect to time.

<figure id="fig:celeba">

<figcaption>Comparison of image restoration methods on the CelebA dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), random pixel inpainting (4th row), box-inpainting (5th row). N/A means “method not applicable”.</figcaption>
</figure>

<figure id="fig:cats">

<figcaption>Comparison of image restoration methods on the AFHQ-Cat dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), random pixel inpainting (4th row), box-inpainting (5th row). N/A means “method not applicable”.</figcaption>
</figure>

## Practical aspects 

#### Computation time and memory

All experiments in this section are conducted on a single NVIDIA RTX 6000 Ada Generation with 48GB RAM. We measure the averaged time per image to complete a deblurring task on the CelebA dataset. We also compute the peak GPU memory load per image. The results are averaged over 100 images (25 batches of 4 images each). We use the same settings as those used for reporting performance metrics on CelebA. Results are in Table .

#### Sensitivity to initialization

Notably, our algorithm does not rely on a good initialization. By starting the algorithm at time $t_0 = 0$, the linear interpolation step initially outputs a pure noise, which is then given to the denoiser. As a result, the algorithm’s performance is independent of the initialization. In Appendix , we stress that this is not the case for other methods.

#### Choice of the latent distribution

Our algorithm can be used with any latent distribution. This is in the spirit of Flow Matching models , which do not rely on a Gaussian latent as opposed to diffusion models. In particular, recently there has been a trend of modelling categorical data with Flow Matching . Our method does not rely on a Gaussian latent, contrary to the other Flow Matching restoration methods. As an example to illustrate the performance of our approach in a non Gaussian case, in Appendix we conduct an experiment with a Dirichlet latent distribution, inspired by .

#### Adaptability to any straight-line flows

Our denoiser is rooted in the straight-line flow framework, which motivates our use of OT Flow Matching. However, other choices of FM models are possible. Notably, Rectified Flows are of particular interest, as the method described allows for the straightening of any flow model. In Appendix , we show how our method performs similarly to what we observed in Section  using pre-trained Rectified Flows.

# Conclusion

We introduced PnP-Flow Matching, and compared it to common Flow Matching and PnP methods. A great strength of our method is its versatility: it requires few hyperparameters, uses minimal memory and computational resources, delivers very good performance across various inverse problems, and supports different latent distributions as well as flexible initialization. Regarding limitations, our reconstructions seem to be more on the smooth side, which relates to the denoising operation as an minimum mean squared estimator. This however is a common tradeoff in regularization of inverse problems . Next, it would be fruitful to use our method for other types of measurement noise such as Poisson noise and to make use of different latent distributions to model categorical distributions. In particular, Dirichlet distributions are common in the field of biological/molecular data, which is a data domain to be investigated.

#### Acknowledgements

Ségolène Martin’s work is carried out in the framework of the DFG funded Cluster of Excellence EXC 2046 MATH+ (project ID: AA5-8). The funding period of the project is from January 2024 until December 2025.

#### Ethics Statement

Our method makes use of pre-trained generative models, such that the general concerns of such models apply. In particular, these generative models carry inherent biases and can potentially be misused.

However, our work is foundational and we believe there are many relevant applications, such as medical imaging or scientific applications which outweigh the negative usages. Furthermore our work involves only small Flow Matching models, which carry very low risk of being used for malicious purposes.

#### Reproducibility Statement

We implemented all the baselines and release the code in the supplementary material. In Appendix we state the hyper-parameter search procedure and the found values. Further, all our theoretical results state the precise assumptions and contain full proofs.

# Appendix

## Proof of Proposition 

<div class="proof">

*Proof.* We have by the definition of the denoiser (it was also observed in ) $$\mathbb{E}_{(X_0,X_1) \sim \pi}[\Vert D_t(X_t) -X_1 \Vert^2] = (1-t)^2 \mathbb{E}_{(X_0,X_1) \sim \pi}[\Vert v_t(X_t)- (X_1-X_0)\Vert^2].$$ If we assume that the denoising loss is zero, this yields for all $t\in [0,1)$ that $v_t(X_t) = X_1 - X_0$ almost surely. By continuity this also follows for $t = 1$. By the same arguments as in we have that both $t \mapsto f(t, X_0)$ and $t \mapsto X_t$ are solution to the flow ODE initialized at $X_0$, since $$\begin{aligned}
    X_t = X_0 + t v_t(X_t) 
    = X_0+ \int_0^t v_s(X_s) ds\quad \text{a.s}.
\end{aligned}$$ where we used that $v$ is constant with time. This implies that $\partial_t X_t = v_t(X_t)$ almost surely. Together with the uniqueness of the ODE solution, it follows that $f(t,X_0) = X_t$ almost surely.

On the other hand, if $(f,v)$ is a straight-line Flow Matching pair connecting $X_0$ and $X_1$, then we obtain that $$v_t(X_t) = v_t(f(t, X_0)) = \partial_t f (t, X_0) =  \partial_t X_t = X_1 - X_0\quad \text{a.s}.$$ Therefore the denoising loss is zero. ◻

</div>

## Proof of Proposition 

We provide the proof for Proposition here below.

<div class="proof">

*Proof.* By the algorithm and definition of $D_{t_n}$, we have $$x_{n+1} = D_{t_n} (u_n) = u_n + (1-t_n) v_{t_n}^\theta (u_n),$$ where together with the definition of $\gamma_n$, $$u_n \coloneqq (1-t_n) \varepsilon 
+ t_n \left(x_n - \gamma_n \nabla F(x_n) \right) = t_n x_n   +
(1-t_n) \left(\varepsilon - t_n \nabla F(x_n) \right).$$ Hence we obtain $$\|x_{n+1} - x_n\| = (1-t_n) \| \varepsilon -x_n - t_n\nabla F(x_n) + (1-t_n) v_{t_n}^\theta (u_n) \|.$$ By assumption on $F$ and $v$ and since $(x_n)_n$ is bounded, the expression in the norm is bounded as well, say by $M >0$. Then, by assumption on $t_n$, we conclude $$\sum_{n=0}^{\infty} \|x_{n+1} - x_n\| = M \sum_{n=0}^{\infty} (1-t_n) < \infty,$$ so that $(x_n)_n$ is a Cauchy sequence and converges. ◻

</div>

## Numerical results using a latent Dirichlet distribution

One of the main practical advantages of Flow Matching over diffusion is that one can choose a latent distribution with is not Gaussian. Motivated by DNA design and other discrete data, there has been growing interest in using a Dirichlet latent distribution in Flow Matching . Here, we want to show that applying PnP-Flow with a Flow Matching model trained on a Dirichlet latent distribution still yields good reconstructions. For this, we use the MNIST dataset , rescaling each image to lie on the simplex, and train a Flow Matching model. We then apply this model to inpainting and super-resolution tasks and present the results. Importantly, the goal of this experiment is not to achieve state-of-the-art performance, but to illustrate that our algorithm generalizes to different latent distributions.

For this, we train a Dirichlet Flow Matching model for 200 epochs with a standard OT Flow Matching loss and a Dirichlet distribution with parameters $(1,...,1) \in \mathbb{R}^{784}$. This latent distribution is also used in and amounts to the uniform distribution on the 784-dimensional simplex. Note that the interpolation step now involves Dirichlet noise instead of Gaussian, as this noise is drawn according to $P_0$. We reconstruct our images with $\gamma =1$ and $300$ steps. Remarkably, almost no modifications for the algorithm are needed. In particular, the generated images $x$ almost lie perfectly on the simplex without any normalization. We show the images in Figure . We compare to , where we adapt the regularization in the algorithm so that the optimized variable $z$ lies on the simplex, i.e., we use the loss $$L(z) = \Vert y - H(f(1,z)) \Vert^2 + \lambda \Vert (\sum_{i,j} z_{i,j})-1) \Vert^2,$$ for a single latent image $z$, where $f(1,z)$ is realized doing 5 Euler steps using the mid point rule and $\lambda$ is a regularization constant.

As one can see, the PnP Flow outperforms D-Flow on all tasks. Note that the other Flow Matching baselines are not usable since their algorithm heavily depends on Gaussian paths.

<figure id="fig:diri">

<figcaption>Dirichlet Flow Matching experiment on Simplex-MNIST, for denoising (1st row), super-resolution (2nd row), box-inpainting (3rd row). We measure the reconstruction error as the mean L2 distance (called MSE) between ground truth and reconstruction averaged over the 16 images.</figcaption>
</figure>

## Sensitivity to initialization

Interestingly, our method is inherently independent of the algorithm’s initialization. For the image restoration problems considered in this work, initialization is not so much of a concern because a reasonable starting point for the solution $x$ is given by $H^\top y$, where $y$ is the observation and $H$ is the degradation operator. However, for more complex problems where $H^\top y$ is far from resembling a natural image, such as in CT reconstruction or phase retrieval , this property becomes much more relevant.

In contrast, competing methods are more sensitive to initialization and cannot be started from any value. As recommended in the paper, our implementation of OT-ODE is initialized with $t_0 y + (1-t_0) \epsilon$ where $\epsilon \sim \mathcal{N}(0,I_d)$ and $t_0$ is the initialization time. The latent variable in D-Flow is initialized as $\alpha T^{-1}(H^\top y) + (1-\alpha) \epsilon$, where $T^{-1}$ is the reverse flow, $\epsilon \sim \mathcal{N}(0,I_d)$ is a random Gaussian noise, and $\alpha \in (0,1)$ is a blending coefficient. Flow-Priors, on the other hand, is initialized with random noise and, like our method, does not depend on a “good” initialization.

In Figure , we illustrate the impact of changing the standard initialization for all methods to a black image on a Gaussian deblurring task, comparing the robustness of each approach to poor initialization.

<figure>

<figcaption>Comparison of restoration methods on the CelebA dataset, for a Gaussian deblurring task and for two different initializations: default initialization recommended for each method (1st row), initialization set to the zero image (2rd row). </figcaption>
</figure>

## Experiments with Rectified Flows

The image restoration experiments we carry out in the core of the paper use a pre-trained OT Flow Matching model. Yet, the theory behind our method holds for any straight-line Flow Matching model. In this section, we show how our method PnP-Flow compares to Flow Priors and OT-ODE using, for all three methods, a pre-trained Rectified Flow model on CelebA-HQ dataset with image dimension $256\times 256$. We use the checkpoint provided by the Rectified Flow repository[^2]. We were not able to run D-Flow using the Rectified Flow model. Quantitative results are in Table and qualitative results on paintbrush inpainting are displayed in Figure .

<figure>

<figcaption>Comparison of restoration methods on the CelebA-HQ dataset for paintbrush inpainting using Rectified Flow model. </figcaption>
</figure>

## Progression of the PnP-Flow reconstruction with time

Figure  presents the progression of the reconstruction outputed by the PnP-Flow with respect to time.

<figure id="fig:progression">
<p></p>
<figcaption>Results for random inpainting using PnP-Flow across different iterations (time steps) with corresponding PSNR values. As expected, in the early iterations, the output resembles a natural face but diverges from the noisy observation. PSNR improves progressively with each iteration.</figcaption>
</figure>

## Additional visual results

Here, we provide additional visual results, where we take the same ground truth image for the different inverse problems, see Fig.  and Fig.  for CelebA dataset and Fig.  and Fig.  for AFHQ-cat dataset.

<figure id="fig:celeba_same_1">

<figcaption>Comparison of image restoration methods on the CelebA dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), free-form inpainting (4th row).</figcaption>
</figure>

<figure id="fig:celeba_same_2">

<figcaption>Comparison of image restoration methods on the CelebA dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), free-form inpainting (4th row).</figcaption>
</figure>

<figure id="fig:cat_same_1">

<figcaption>Comparison of restoration methods on the AFHQ-Cat dataset, for: denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), free-form inpainting (4th row).</figcaption>
</figure>

<figure id="fig:cat_same_2">

<figcaption>Comparison of image restoration methods on the AFHQ-Cat dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), free-form inpainting (4th row).</figcaption>
</figure>

## Optimal hyper-parameters values

We implemented the Gradient Step denoiser from following the repository[^3]. For random inpainting, the PnP algorithm is the Half Quadratic Splitting with the parameters given in the paper . For denoising, we only apply the trained denoiser once, feeding it with the true noise level $\sigma$. For all other methods, we use Proximal Gradient Descent (PGD), as prescribed in . Note that we do not constrain the Lipschitz constant of the denoiser. For the PGD case, we tune three hyper-parameters: the learning rate $\gamma \in \{0.99, 2.0 \}$ in the gradient step (which is related to the regularization parameter $\lambda$ in the paper), the inertia parameter $\alpha \in \{0.3, 0.5, 0.8, 1.0\}$ which coresponds to the relaxed denoiser $D^\alpha_\sigma = \alpha D_\sigma + (1-\alpha) \mathrm{Id}$, and the factor $\sigma_f \in \{1., 1.2, 1.5, 1.8, 2., 3., 4., 5.\}$ so that the denoiser gets as noise map $\sigma_f \times \sigma$ (where $\sigma$ is the true noise level). We also considered the number of iterations as a hyperparameter that we tuned on the validation set (with maximum number of iterations fixed to $100$).

For PnP-Diff, we tuned two parameters: the regularization parameter $\lambda \in \{1.0, 5.0, 10.0, 100.0, 1000.0\}$ and the blending parameter $\zeta \in \{0.1, 0.3, 0.5, 1.0\}$. The number of iterations was fixed to 100.

For D-Flow, we adjusted the blending parameter for initialization $\alpha \in \{0.1, 0.3, 0.5\}$ and the regularization parameter $\lambda \in \{0.1, 0.01, 0.001\}$. We observed that the PSNR of the reconstruction did not consistently increase across iterations (the method does not always converge); thus, we fine-tuned the value of the last iteration while keeping it below 20 for computational efficiency. The number of iterations for the inner LBFGS optimization was set to 20, and the number of Euler step when solving the ODE to 5, as recommended in the paper .

For OT-ODE, we tuned the initial time $t_0 \in \{0.1, 0.2, 0.3, 0.4\}$ and the type of learning step $\gamma$ (either $\sqrt{t}$ or constant). The number of iterations was set to 100.

For Flow-Priors, we considered -as described in the paper- the two hyper-parameters $\eta \in \{10^{-3}, 10^{-2}, 10^{-1} \}$ and $\lambda \in \{10^2, 10^3, 10^4, 10^4\}$ which respectively correspond to the step size for the gradient descent and the guidance weight (weight put on the data likelihood). The number of iterations was set to 100 and the number of iner iterations to $K=1$.

Finally, for our method PnP-Flow, we adjusted the exponent in the learning rate $\alpha \in \{0.01, 0.1, 0.3, 0.5, 0.8, 1.0\}$ and the number of time steps $N \in \{100, 200, 500\}$. When increasing $N$ beyond 100 resulted in less than a 0.2 dB improvement in PSNR, we set $N = 100$ for computational efficiency. We average the output of the denoising step in Algorithm  over 5 realizations of the interpolation step. To speed up the algorithm, this number can be reduced to 1 with only a minor impact on performance.

[^1]: [https://github.com/deepinv/deepinv](#DeepInv repository)

[^2]: [https://github.com/gnobitab/RectifiedFlow](#gnobitab/RectifiedFlow)

[^3]: [https://github.com/samuro95/GSPnP](#GSPNP repository)
