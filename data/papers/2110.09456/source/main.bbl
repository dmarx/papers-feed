\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization, 2016.

\bibitem[Bachlechner et~al.(2020)Bachlechner, Majumder, Mao, Cottrell, and
  McAuley]{bachlechner2020rezero}
Thomas Bachlechner, Bodhisattwa~Prasad Majumder, Huanru~Henry Mao, Garrison~W
  Cottrell, and Julian McAuley.
\newblock Rezero is all you need: Fast convergence at large depth.
\newblock \emph{arXiv preprint arXiv:2003.04887}, 2020.

\bibitem[Baevski \& Auli(2019)Baevski and Auli]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ByxZX20qFQ}.

\bibitem[Balduzzi et~al.(2017)Balduzzi, Frean, Leary, Lewis, Ma, and
  McWilliams]{balduzzi2017shattered}
David Balduzzi, Marcus Frean, Lennox Leary, JP~Lewis, Kurt Wan-Duo Ma, and
  Brian McWilliams.
\newblock The shattered gradients problem: If resnets are the answer, then what
  is the question?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  342--350. PMLR, 2017.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Le~bras, Gao, and Choi]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Ronan Le~bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  34\penalty0 (05):\penalty0 7432--7439, Apr. 2020.
\newblock \doi{10.1609/aaai.v34i05.6239}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/6239}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Chen et~al.(2021)Chen, Cheng, Wang, Gan, Wang, and
  Liu]{chen2021earlybert}
Xiaohan Chen, Yu~Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing
  Liu.
\newblock Earlybert: Efficient bert training via early-bird lottery tickets,
  2021.

\bibitem[Conneau et~al.(2020)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzmán, Grave, Ott, Zettlemoyer, and Stoyanov]{conneau2020unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock 2020.

\bibitem[Ding et~al.(2021)Ding, Yang, Hong, Zheng, Zhou, Yin, Lin, Zou, Shao,
  Yang, and Tang]{ding2021cogview}
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da~Yin, Junyang
  Lin, Xu~Zou, Zhou Shao, Hongxia Yang, and Jie Tang.
\newblock Cogview: Mastering text-to-image generation via transformers, 2021.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gelu}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Huang et~al.(2020)Huang, Perez, Ba, and Volkovs]{huang2020improving}
Xiao~Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4475--4483. PMLR, 2020.

\bibitem[Lieber et~al.(2021)Lieber, Sharir, Lenz, and Shoham]{J1WhitePaper}
Opher Lieber, Or~Sharir, Barak Lenz, and Yoav Shoham.
\newblock Jurassic-1: Technical details and evaluation.
\newblock Technical report, AI21 Labs, August 2021.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{liu2020understanding}
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han.
\newblock Understanding the difficulty of training transformers.
\newblock \emph{arXiv preprint arXiv:2004.08249}, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen,
  Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, and
  Wu]{micikevicius2018mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, and Hao Wu.
\newblock Mixed precision training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{mihaylov-etal-2018-suit}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2381--2391, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1260}.
\newblock URL \url{https://www.aclweb.org/anthology/D18-1260}.

\bibitem[Mostafazadeh et~al.(2016)Mostafazadeh, Chambers, He, Parikh, Batra,
  Vanderwende, Kohli, and Allen]{mostafazadeh-etal-2016-corpus}
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra,
  Lucy Vanderwende, Pushmeet Kohli, and James Allen.
\newblock A corpus and cloze evaluation for deeper understanding of commonsense
  stories.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  839--849, San Diego, California, June 2016. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/N16-1098}.
\newblock URL \url{https://www.aclweb.org/anthology/N16-1098}.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock \textsc{fairseq}: A fast, extensible toolkit for sequence modeling.
\newblock 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock pp.\  8024--8035, 2019.

\bibitem[Press et~al.(2020{\natexlab{a}})Press, Smith, and
  Levy]{press2020improving}
Ofir Press, Noah~A. Smith, and Omer Levy.
\newblock Improving transformer models by reordering their sublayers,
  2020{\natexlab{a}}.

\bibitem[Press et~al.(2020{\natexlab{b}})Press, Smith, and
  Lewis]{press2020shortformer}
Ofir Press, Noah~A Smith, and Mike Lewis.
\newblock Shortformer: Better language modeling using shorter inputs.
\newblock \emph{arXiv preprint arXiv:2012.15832}, 2020{\natexlab{b}}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock Technical report, OpenAI, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock 21:\penalty0 1--67, 2020.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock \emph{arXiv preprint arXiv:2102.12092}, 2021.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Le~Bras, Bhagavatula, and
  Choi]{sakaguchi2020winogrande}
Keisuke Sakaguchi, Ronan Le~Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  34\penalty0 (05):\penalty0 8732--8740, Apr. 2020.
\newblock \doi{10.1609/aaai.v34i05.6399}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/6399}.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer, 2020.

\bibitem[So et~al.(2021)So, Mańke, Liu, Dai, Shazeer, and Le]{so2021primer}
David~R. So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and
  Quoc~V. Le.
\newblock Primer: Searching for efficient transformers for language modeling,
  2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock 2019.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu.
\newblock On layer normalization in the transformer architecture, 2020.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers-etal-2019-hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock {H}ella{S}wag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4791--4800, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1472}.
\newblock URL \url{https://www.aclweb.org/anthology/P19-1472}.

\bibitem[Zhu et~al.(2021)Zhu, Ni, Xu, Kong, Huang, and
  Goldstein]{zhu2021gradinit}
Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W~Ronny Huang, and Tom Goldstein.
\newblock Gradinit: Learning to initialize neural networks for stable and
  efficient training.
\newblock \emph{arXiv preprint arXiv:2102.08098}, 2021.

\bibitem[Zhu et~al.(2019)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{zhu2015bookcorpus}
Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock \emph{arXiv preprint arXiv:1506.06724}, 2019.

\end{thebibliography}
