import torch

from flow_matching.path import MixtureDiscreteProbPath, DiscretePathSample
from flow_matching.path.scheduler import PolynomialConvexScheduler
from flow_matching.loss import MixturePathGeneralizedKL
from flow_matching.solver import MixtureDiscreteEulerSolver
from flow_matching.utils import ModelWrapper


model = ...  # Define a trainable velocity model
optimizer = torch.optim.Adam(model.parameters())

scheduler = PolynomialConvexScheduler(n=1.0)
path = MixtureDiscreteProbPath(scheduler=scheduler)
loss_fn = MixturePathGeneralizedKL(path=path)  # Generalized KL Bregman divergence

for x_0, x_1 in dataloader:  # Samples from $\pi_{0,1}$ of shape [batch_size, *data_dim]
    t = torch.rand(batch_size) * (1.0 - 1e-3)  # Randomize time $t \sim U[0,1-10^{-3}]$
    sample: DiscretePathSample = path.sample(t=t, x_0=x_0, x_1=x_1)  # Sample the conditional path
    model_output = model(sample.x_t, sample.t)

    loss = loss_fn(logits=model_output, x_1=sample.x_1, x_t=sample.x_t, t=sample.t) # CDFM loss

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

class ProbabilityDenoiser(ModelWrapper):
    def forward(self, x: torch.Tensor, t: torch.Tensor, **extras) -> torch.Tensor:
        logits = self.model(x, t, **extras)
        return torch.nn.functional.softmax(logits.float(), dim=-1)

# Sample $X_1$
probability_denoiser = ProbabilityDenoiser(model=model)
x_0 = torch.randint(size=[batch_size, *data_dim])  # Specify the initial condition
solver = MixtureDiscreteEulerSolver(
    model=probability_denoiser,
    path=path,
    vocabulary_size=vocabulary_size
)

step_size = 1 / 100
x_1 = solver.sample(x_init=x_0, step_size=step_size, time_grid=torch.tensor([0.0, 1.0-1e-3]))
