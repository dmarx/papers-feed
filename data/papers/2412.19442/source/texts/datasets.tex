\section{Text and Multi-modal Datasets}\label{sec:dataset}


In this section, we introduce the text and multi-modal datasets used to evaluate LLM efficiency.

\subsection{Text Dataset}\label{ssec:text_dataset}

% Text dataset, as the most commonly seen datasets involved in LLM's training, validation and testing, has diverged into many different tasks.
% We summarized some long-context datasets that was adapted and used in state-of-the-art benchmark frameworks, namely \textbf{L-Eval}~\cite{an_l-eval:_2023}, 
% \textbf{M4LE}~\cite{kwan_m4le:_2023}, \textbf{BAMBOO}~\cite{dong2023bamboo}, \textbf{LongBench}~\cite{bai_longbench:_2023}, \textbf{LRA}~\cite{tay_long_lra_2020}, 
% \textbf{SCORLLS}~\cite{shaham_scrolls:_2022}, \textbf{ZEROSCROLLS}~\cite{shaham_zeroscrolls:_2023}  \textbf{LooGLE}~\cite{li_loogle:_2023}, \textbf{LongEval}~\cite{longchat2023}  and \textbf{StreamingEval}~\cite{DBLP:conf/iclr/XiaoTCHL24}.  
% We do our survey at dataset level, and looked into the original dataset for more detailed information.  \ref{tab:text-dataset-qa}, \ref{tab:text-dataset-summarization}, \ref{tab:text-dataset-nli-classification}, \ref{tab:text-dataset-generation}, \ref{tab:text-dataset-retrieval}, \ref{tab:text-dataset-aggregation} describe datasets that are ready-to-use, or have a large text resource to build new datasets with. In all, the tasks text datasets apply to can be categorized into the following types:

We collect a lot of long-context datasets from state-of-the-art benchmark frameworks and various papers, including L-Eval\cite{an_l-eval:_2023}, M4LE\cite{kwan_m4le:_2023}, BAMBOO\cite{dong2023bamboo}, LongBench\cite{bai_longbench:_2023}, LRA\cite{tay_long_lra_2020}, SCROLLS\cite{shaham_scrolls:_2022}, ZEROSCROLLS\cite{shaham_zeroscrolls:_2023}, LooGLE\cite{li_loogle:_2023}, LongEval\cite{longchat2023}, and StreamingEval\cite{DBLP:conf/iclr/XiaoTCHL24}.
Specifically,
we categorize these datasets into different tasks, including question answering, text summarization, text reasoning, text retrieval, and text generation.

%Text-QA
\begin{table*}[t]
    \centering
    \caption{Text question answering (QA) dataset. In the \textbf{Avg. Len: }  average length, \textbf{Tok:}  tokens;  \textbf{W: } words. In the Instances column, \textbf{Doc:} documents, \textbf{Q:} questions, \textbf{Inst:} instructions. Particularly, AltQA, PaperQA and MeetingQA have two datasets  with different length levels, and is separated with $/$. \\
    %In the Metric column, \textbf{EM:} Exact Match. \textbf{Acc:} Accuracy. \textbf{METEOR}~\cite{denkowski-lavie-2011-meteor} is a machine translation evaluation metric that incorporates factors like stemming, synonym matching, and word order to provide a more nuanced assessment of translation quality. 
    %\textbf{BLEU}\cite{papineni-etal-2002-bleu} is also a metric designed for machine translation. 
    %\textbf{Rouge}~\cite{lin-2004-rouge} and its variants measure model's performance by calculating overlap between model output and reference answer with unigram(Rouge-1), bigram(Rouge-2), LCS(Rouge-L), etc.  
    %\textbf{F1}~\cite{rajpurkar-etal-2016-squad-F1} calculates unigram overlap between model output and answers after processing elments like white-spaces and stop-words. 
    Particularly, for datasets from L-Eval, the GPT-4 metric means the win-rate against Turbo-16K, judged by GPT-4. $\Delta L$ is the length difference between answer length and ground truth. For NarrativeQA, \textbf{MRR: }Mean Reciprocal Rank .} 
    \renewcommand{\arraystretch}{1.5} % 调整行间距
    \setlength{\tabcolsep}{1pt} % 减小列间距
    \label{tab:text-dataset-qa}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\textbf{Task} &
  \textbf{Name} &
  \textbf{Source} &
  \textbf{Instances} &
  \textbf{Avg Len} &
  \textbf{Metric} &
  \textbf{Lang.} \\ \hline
QA &
  \href{https://github.com/abacusai/long-context}{AltQA}~\cite{pal2023giraffeadventuresexpandingcontext} &
  Wikipedia &
  200/200 &
  3243/13,084 Tok&
  Acc &
  EN \\ \hline

QA &
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{PaperQA(BAMBOO)}~\cite{dong2023bamboo} &
  Paper &
  100/100 &
  3101/6838 Tok&
  Acc &
  EN \\ \hline

QA &
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{MeetingQA(BAMBOO}~\cite{dong2023bamboo}  &
  Meeting &
  100/100 &
  2738/9838 Tok&
  Acc &
  EN \\ \hline



QA &
  \href{https://huggingface.co/datasets/mandarjoshi/trivia_qa}{TriviaQA}~\cite{joshi_triviaqa:_2017}  &
  Web Question, Wiki &
  95,956 Q, 662,659 Doc&
  17,370 W&
  EM, F1 &
  EN \\ \hline



QA&
  \href{https://github.com/OpenLMLab/LEval}{TOEFL(L-Eval)}~\cite{an_l-eval:_2023} &
  TOFEL-QA\cite{tseng_towards_tofelqa_2016} &
  15 Doc, 269 Inst&
  3907 Tok&
   Rouge-L, GPT-4, $\Delta L$&
  EN \\ \hline


QA&
   \href{https://github.com/OpenLMLab/LEval}{Coursera(L-Eval)}~\cite{an_l-eval:_2023} &
  Video Subtitles&
  15 Doc, 172 Inst&
  9075 Tok&
   Rouge-L, GPT-4, $\Delta L$&
  EN \\ \hline


QA &
   \href{https://github.com/OpenLMLab/LEval}{SFiction(L-Eval)}~\cite{an_l-eval:_2023} &
  SFGram \cite{sfgram}, fiction&
  7 Doc, 64 Inst&
  16,381 Tok&
   Rouge-L, GPT-4, $\Delta L$&
  EN \\ \hline


QA &
   \href{https://github.com/OpenLMLab/LEval}{LongFQA(L-Eval)}~\cite{an_l-eval:_2023} &
  Financial Transcripts&
  6 Doc, 52 Inst&
  6032 Tok&
   Rouge-L, GPT-4, $\Delta L$&
  EN \\ \hline

QA &
   \href{https://github.com/OpenLMLab/LEval}{CUAD(L-Eval)}~\cite{an_l-eval:_2023} &
  CUAD\cite{hendrycks_cuad:_2021} &
  20 Doc, 130 Inst&
  30,966 Tok&
   Rouge-L, GPT-4, $\Delta L$&
  EN \\ \hline


QA &
  \href{https://github.com/KwanWaiChung/M4LE}DuoRC~\cite{kwan_m4le:_2023} &
  Movie &
   &
  3572 W&
  Acc &
  EN \\ \hline


QA &
  \href{https://ai.google.com/research/NaturalQuestions/download}{NQ~\cite{kwiatkowski_nq_2019}}&
  Wiki &
  307,373 &
  9005 W &
  Rouge &
  EN \\ \hline


  
QA-SG&
  \href{https://github.com/google-deepmind/narrativeqa}{NarrativeQA}~\cite{narrativeqa}  &
  Story &
  1572 Doc&
  62,528 Tok&
  \begin{tabular}[c]{@{}c@{}}BLEU, METEOR, \\ Rouge-L, MRR\end{tabular}&
  EN \\ \hline

QA-SG &
  \href{https://huggingface.co/datasets/THUDM/LongBench}{NarrativeQA(LongBench)}~\cite{bai_longbench:_2023} &
  Story &
  200 &
  18,409 W &
  F1 &
  EN \\ \hline

QA-SG&
  \href{https://huggingface.co/datasets/allenai/qasper}{Qasper}~\cite{dasigi_dataset_2021_qasper} &
  Paper &
  1585 &
  5001 W&
  F1 &
  EN \\ \hline

QA-SG &
  \href{https://huggingface.co/datasets/THUDM/LongBench}{Qasper(LongBench)}~\cite{bai_longbench:_2023}  &
  Paper &
  200 &
  3619 W&
  F1 &
  EN \\ \hline

QA-SG&
  \href{https://huggingface.co/datasets/THUDM/LongBench}{MultifieldQA-en}~\cite{bai_longbench:_2023}  &
  \begin{tabular}[c]{@{}c@{}}Paper, Legal, \\ Gov, Google\end{tabular}&
  200 &
  4459 W&
  F1 &
  EN \\ \hline

QA-SG&
  \href{https://huggingface.co/datasets/THUDM/LongBench}{MultifieldQA-zh}~\cite{yang_hotpotqa:_2018} &
\begin{tabular}[c]{@{}c@{}}Paper, Legal, \\ Gov, Google\end{tabular}&
  200 &
  6701 W&
  F1 &
  ZH \\ \hline


  QA-SG &
  \href{https://github.com/nyu-mll/quality}{QuALITY}~\cite{pang_quality_dataset:_2021} &
  Story, magazine&
  381 Doc, 6737 Q&
  4203 W&
  EM &
  EN \\ \hline


  
QA-MT&
  \href{https://huggingface.co/datasets/hotpotqa/hotpot_qa}{HotpotQA}~\cite{yang_hotpotqa:_2018} &
  Wiki &
  112,779 &
  1138 W&
  EM, F1 &
  EN \\ \hline


QA-MT&
  \href{https://huggingface.co/datasets/THUDM/LongBench}{HotpotQA(LongBench)}~\cite{bai_longbench:_2023} &
  Wiki &
  200 &
  9151 W&
  F1 &
  EN \\ \hline


QA-MT&
  \href{https://github.com/Alab-NII/2wikimultihop}{2WikiMultihopQA}~\cite{ho_multihopqa_2020} &
  Wiki &
  192,606 Q&
  639 W&
  EM, F1 &
  EN \\ \hline


QA-MT&
  \href{https://github.com/stonybrooknlp/musique}{MuSiQue}~\cite{trivedi_musique:_2021} &
  Wiki &
  24,814 &
  1827 W&
  F1 &
  EN \\ \hline

QA-MT&
  \href{https://github.com/baidu/DuReader}{DuReader}~\cite{he_dureader:_2017} &
  Baidu &
  200,000 Q, 1,000,000 Doc&
  396 W&
  BLEU, Rouge-L &
  ZH,EN \\ \hline
  
QA+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{NewsQA(M4LE)}~\cite{kwan_m4le:_2023} &
  News &
  - &
  3679 W&
  Acc &
  EN \\ \hline

QA+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{C3(M4LE)}~\cite{kwan_m4le:_2023} &
  Textbook&
  - &
  3797 W&
  Acc &
  ZH \\




 \hline
\end{tabular}
\end{table*}



\subsubsection{Question Answering (QA) Task}
Dataset for this task usually consist of question-answer pairs, and documents that contains the answer to the question. For a model to run such task, documents and questions are usually used as the model input, while the output can differ greatly. Some datasets' answers are closed-ended, meaning that the model should only output its answer in designated form, typically multiple choice answers, while the open-ended answers take a more free form. According to the number of documents involved in a question-answer pair, we can categorize QA task datasets into single-doc QA(QA-SG) and multiple-doc QA(QA-MT). The detailed statistics of the datasets for question answering are provided in Table~\ref{tab:text-dataset-qa}.


%Text-SUM
\begin{table*}[]
    \centering
    \caption{Text Dataset-Summarization. In the \textbf{Avg. Len: }  average length, \textbf{Tok:}  tokens;  \textbf{W: } words. In the Instances column, \textbf{Doc:} documents, \textbf{Q:} questions, \textbf{Inst:} instructions. Particularly, SPACE has the concept of 'Entity', and R/Ent stands for reviews per entity. Sum stands for summary.  
    In the Metric column, \textbf{EM:} Exact Match. \textbf{PM:} Partial Match. \textbf{Acc:} Accuracy. 
    %\textbf{METEOR}~\cite{denkowski-lavie-2011-meteor} and \textbf{BLEU}\cite{papineni-etal-2002-bleu} are a machine translation evaluation metric.
    %that incorporates factors like stemming, synonym matching, and word order to provide a more nuanced assessment of translation quality. 
    %\textbf{BLEU}\cite{papineni-etal-2002-bleu} is also a metric designed for machine translation. 
    %\textbf{Rouge}~\cite{lin-2004-rouge} measures model's performance with the ngram overlap between model output and reference answer. \textbf{F1}~\cite{rajpurkar-etal-2016-squad-F1} calculates unigram overlap between model output and answers after processing elments like white-spaces and stop-words. 
    %\textbf{BERT}~\cite{zhang2020bertscoreevaluatingtextgeneration} is a text generation task metric using contextual embeddings. 
    For MultiNews, \textbf{Rouge-SU} skip bigrams when having a distance larger than 4 words. Particularly, LooGLE utilizes GPT-4 for its QA and summarization task, using it for answer's semantic judgement.}
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{1pt} % 减小列间距
    \label{tab:text-dataset-summarization}

\begin{tabular}{c|c|c|c|c|c|c}
\hline
\textbf{Task} &
  \textbf{Name} &
  \textbf{Source} &
  \textbf{Instances} &
  \textbf{Avg Len} &
  \textbf{Metric} &
  \textbf{Lang.} \\ \hline
 SUM& 
\href{https://huggingface.co/datasets/ccdv/cnn_dailymail}{CNN/Dailymail}~\cite{nallapati_abstractive_dailymail_2016}& News& 
300,000& 
766 W& 
Rouge-1/2/L&
EN\\ \hline
 SUM
 & \href{https://huggingface.co/datasets/EdinburghNLP/xsum}{XSum}~\cite{narayan_dont_xsum_2018}
 & News
 & 400,000
 & 431 W
 & Rouge-1/2/L
 &EN\\ \hline
 SUM
 & \href{https://github.com/Yale-LILY/QMSum}{QMSum}~\cite{zhong_qmsum:_2021}
 & Meeting
 & 232 Meets, 1808 Q
 & 9070 W
 & Rouge-1/2/L
 &EN\\  \hline
 SUM
 & \href{https://github.com/Alex-Fabbri/Multi-News}{MultiNews}~\cite{fabbri_multi-news:_2019}
 & News
 & 51,216
 & 5866 W %2103.49*2.788
 & Rouge-1/2/SU
 &EN\\ \hline
 
\begin{tabular}[c]{@{}c@{}}SUM-QB+\\Reasoning+\\QA\end{tabular}& %Note: also support Cloze task 
  \href{https://github.com/bigai-nlco/LooGLE}{LooGLE}~\cite{li_loogle:_2023} &
  \begin{tabular}[c]{@{}c@{}}Papers, Wiki,\\ Movie, TV \end{tabular}&
  776 Doc, 6448 Q&
  \begin{tabular}[c]{@{}c@{}}19,367 W\\24,005 Tok\end{tabular}&
  \begin{tabular}[c]{@{}c@{}}BLEU, Rouge, METEOR, \\BERT, GPT4, \\EM, PM \end{tabular}&
  EN,ZH \\ \hline

SUM &
  \href{https://huggingface.co/datasets/ccdv/govreport-summarization}{GovReport}~\cite{huang_govreport_2021} &
  Gov &
  19,466 &
  \begin{tabular}[c]{@{}c@{}}9409.4 W\end{tabular}&
  Rouge-1/2/L&
  EN \\ \hline
SUM &
  \href{https://github.com/hahahawu/VCSum}{VCSUM}~\cite{wu_vcsum:_2023} &
  Meeting &
  239 &
  14,107 Tok&
  F1, Gold Rouge-1&
  ZH \\ \hline
%SUM &
%  \href{https://arxiv.org/src/1911.12237v2/anc}{SAMSum}~\cite{gliwa_samsum_2019} &
%  Dialogue &
%  16,369 &
%  109 W &
%  Rouge&
%  EN \\ \hline
SUM &
  \href{https://github.com/mingdachen/SummScreen}{SummScreenFD}~\cite{chen_summscreen:_2021} &
  TV &
  269,000 &
  6613 Tok&
  Rouge&
  EN \\ \hline

SUM &
  \href{https://evasharma.github.io/bigpatent/}{BigPatent}~\cite{sharma_bigpatent:_2019} &
  Patent &
  1,341,362 &
  3573 W&
  Rouge-1/2/L&
  EN \\ \hline
SUM &
  \href{https://github.com/stangelid/qt}{SPACE}~\cite{angelidis_extractive_space_2021} &
  Review&
  \begin{tabular}[c]{@{}c@{}}50 Entities, \\1,140,000 Reviews, \\100R/Ent, \\1050 Sum\end{tabular}&
  15,532 W &
  Rouge-1/2/L&
  EN \\ \hline
SUM &
  \href{https://github.com/nyu-mll/SQuALITY}{SQuALITY} ~\cite{wang2022squality}&
  Story&
  625&
  \begin{tabular}[c]{@{}c@{}}5200 W\end{tabular}&
  \begin{tabular}[c]{@{}c@{}}Rouge-1/2/L, \\METEOR, \\BERT\end{tabular}&
  EN \\ \hline
SUM+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{CNNNews(M4LE)}~\cite{kwan_m4le:_2023} &
  News &
  - &
  3754 W &
  Rouge-L &
  EN \\ \hline
SUM+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{CEPSUM(M4LE)}~\cite{kwan_m4le:_2023} &
  E-Commerce &
  - &
  4003 W &
  Rouge-L & 
  ZH \\  \hline
SUM+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{LCSTS(M4LE)}~\cite{kwan_m4le:_2023} &
  News &
  - &
  4102 W &
  Rouge-L &
  ZH \\ \hline
SUM+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{NCLS(M4LE)}~\cite{kwan_m4le:_2023} &
  NCLS~\cite{zhu_ncls:_2019} &
  - &
  3470 W&
  Rouge-L &
  EN,ZH \\ \hline
SUM+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{WikiHow}~\cite{kwan_m4le:_2023} &
  Wiki &
  - &
  3514 W &
  Rouge-L &
  EN \\ \hline
SUM+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{News2016}~\cite{kwan_m4le:_2023} &
  News &
  - &
  3785 W &
  Rouge-L &
  ZH \\ \hline

SUM &
  \href{https://github.com/KwanWaiChung/M4LE}{Pubmed(M4LE)}~\cite{kwan_m4le:_2023} &
  Medical &
  1267 &
  3678 W &
  Rouge-L &
  EN \\ \hline
SUM &
  \href{https://github.com/KwanWaiChung/M4LE}{BookSum(M4LE)}~\cite{kwan_m4le:_2023} &
  Book &
  - &
  2643 W &
  Rouge-L &
  EN \\ \hline
SUM &
  \href{https://github.com/KwanWaiChung/M4LE}{CNewsum(M4LE)}~\cite{kwan_m4le:_2023} &
  News &
  690 &
  1883 W &
  Rouge-L &
  ZH \\ \hline
SUM &
  \href{https://github.com/KwanWaiChung/M4LE}{CLTS+(M4LE)}~\cite{kwan_m4le:_2023} &
  News &
  - &
  3158 W &
  Rouge-L &
  ZH \\ \hline
SUM &
  \href{https://github.com/KwanWaiChung/M4LE}{Arxiv(M4LE)}~\cite{kwan_m4le:_2023} &
  Paper&
  1550 &
  3748 W &
  Rouge-L &
  EN \\ \hline


 
\end{tabular}
\end{table*}

\begin{itemize}[leftmargin=10pt]
    \item \textbf{Qasper~\cite{dasigi_dataset_2021_qasper}} consists of 5049 questions based on 1585 papers on NLP.  Question is from NLP practitioners that only have read the abstract and title of a paper, then another set of practitioners answer these questions by reading through the whole paper. The supporting evidences is provided correspondingly. Each instance of the dataset consists of a question, an answer, corresponding paper and supporting evidence. Instances built by LongBench~\cite{bai_longbench:_2023} doesn't require evidence. 
    \item  \textbf{HotpotQA~\cite{yang_hotpotqa:_2018} } is a typical for a multi-doc QA dataset. It's built based on Wikipedia, and each instance consists of multiple documents, a question, an answer and supporting facts. Supporting facts is a set of paragraph indexes, annotated manually. 
    \item \textbf{AltQA~\cite{pal2023giraffeadventuresexpandingcontext}} is based on google's NQ~\cite{kwiatkowski_nq_2019} dataset. The answer are all numerical. The original document is "altered" so that each occurrences of the numerical answer is different from the original document, so as to avoid data contamination from pretraining. This dataset is also used in BAMBOO~\cite{dong2023bamboo} benchmark.
    \item \textbf{PaperQA} and \textbf{MeetingQA} from BAMBOO~\cite{dong2023bamboo} benchmark are question answering tasks in the form of multiple-choice.  Each instance of the two datasets consists of question , evidence, answer and corresponding content.
    \item \textbf{NarrativeQA}~\cite{narrativeqa} uses complex narratives that are self-contained as input documents. Both books and movie scripts are used. For question construction, annotators are only given a story summary, and are asked to write questions based on it. For each story(1572 stories in total), about 30 question-answer pairs are constructed from each summary-story pair. Notably, because of the consistency in story context, the task can be simplified to selecting a correct answer from all answers that relates to the story. 
    \item \textbf{MultifieldQA}~\cite{bai_longbench:_2023} is an original dataset from Longbench. Its contents covers scientific papers, legal documents, government reports and google results. The dataset has both Chinese and English version, and each instance consists of context built on documents, and a question-answer pair. 
    \item \textbf{2WikiMultihopQA}~\cite{ho_multihopqa_2020} is a multi-document QA dataset built on Wikipedia and Wikidata. WikiData is a Knowledge Graph database, from which the author was able to extract the (subject entity, property, object entity) triple that corresponds to a Wikipidia document. These triples are used as evidences in each QA pair, as a way for model to show its inference process. The dataset consists of 192,606 questions in total.
    \item \textbf{Musique~\cite{trivedi_musique:_2021}} is also a multi-document dataset(or multi-hop dataset, as the paper refers to). Its data is extracted from existing single-hop QA datasets. These single-hop QAs are then composed into multi-hop QA pairs. In addition, Musique add some unanswerable QA pairs in order to further test model's ability. There are 24,814 answerable questions in Musique, and each answerable question corresponds to an unanswerable question.
    \item \textbf{DuReader}~\cite{he_dureader:_2017} is a multi-document QA dataset, whose data is based on Baidu search results. It consists of 200,000 questions, 1,000,000 documents and 420,000 answers. Each instance contains a question, multiple possible answers(also possible to be empty), and multiple documents. 
    \item \textbf{TriviaQA}~\cite{joshi_triviaqa:_2017 } is a multi-document reading comprehension QA dataset. All QA pairs are from 14 trivia websites, written by trivia enthusiasts.  For each QA pair, 6 supporting documents(evidence) are provided, collected from Bing search API as well as Wikipedia. The total number of QA pairs is 95,956, with a total of 662,659 supporting documents, the average length of each document is 2895 words.
    \item \textbf{TOEFL(L-Eval)}~\cite{an_l-eval:_2023} collect lectures from the TOEFL Practice Online as context . Each instance consists of a long input of lectures, multiple instructions(questions) and corresponding answers. 
    \item \textbf{Coursera(L-Eval)}~\cite{an_l-eval:_2023} is a dataset built on Coursera website. Similar to TOFEL, Each instance consists of a long input of lectures, multiple instructions and corresponding answers.     
    \item \textbf{SFiction(L-Eval)}~\cite{an_l-eval:_2023} is based on scientific fictions, in which context real-world principles don't apply. The questions contained in the documents ask the model to answer it based on either contextual information or real-world knowledge, as a way to test model hallucination. 
    \item \textbf{LongFQA(L-Eval)}~\cite{an_l-eval:_2023} is an open-ended QA dataset on finance based on earnings call transcripts. 
    \item \textbf{CUAD(L-Eval)}\cite{an_l-eval:_2023} is drawn from the CUAD~\cite{hendrycks_cuad:_2021} dataset, which use legal contract as its context. 
    \item \textbf{QuALITY}~\cite{pang_quality_dataset:_2021} is a multiple-choice single-document QA dataset. It uses science fictions, magazine articles and nonfiction articles as input documents. The question is written by those that have read the full document. Each instance contains a document, a multiple-choice questions and corresponding answers. Notably, part of the questions are unanswerable. 
    \item \textbf{NewsQA}~\cite{kwan_m4le:_2023} and \textbf{DuoRC}~\cite{kwan_m4le:_2023} are English QA datasets, constructed from news and movie plots, respectively. 
    \item \textbf{C3}~\cite{kwan_m4le:_2023} is a multiple-choice QA dataset, based on second-language Chinese exams.  
    \item \textbf{NQ}~\cite{kwiatkowski_nq_2019} is a QA dataset based on Wikipedia pages. Each instance(or example, as referred to in original paper) consists of a question, corresponding wikipedia page, a long answer and a short answer. 
\end{itemize}



%Text-NLI

\begin{table*}[]
    \centering
    \caption{Text Reasoning/Classification Datasets. \textbf{CLS:} Classification. In the \textbf{Avg. Len: }  average length, \textbf{Tok:}  tokens;  \textbf{W: } words. In the Instances column, \textbf{Doc:} documents, \textbf{Inst:} instructions. In the Metric column, \textbf{EM:} Exact Match.  \textbf{Acc:} Accuracy. %\textbf{Rouge}~\cite{lin-2004-rouge} and its variants measure model's performance by calculating overlap between model output and reference answer with unigram(Rouge-1), bigram(Rouge-2), LCS(Rouge-L), etc. \textbf{F1}~\cite{rajpurkar-etal-2016-squad-F1} calculates unigram overlap between model output and answers after processing elments like white-spaces and stop-words. Particularly, for datasets from L-Eval, the GPT-4 metric means the win-rate against Turbo-16K, judged by GPT-4. $\Delta L$ is the length difference between answer length and ground truth.
    }
    \label{tab:text-dataset-reasoning}
    
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{1pt} % 减小列间距
    \label{tab:text-dataset-nli-classification}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\textbf{Task} &
  \textbf{Name} &
  \textbf{Source} &
  \textbf{Instances} &
  \textbf{Avg Len} &
  \textbf{Metric} &
  \textbf{Lang.} \\ \hline
CLS/Reasoning &
  \href{https://github.com/google-research/long-range-arena}{Long ListOps} ~\cite{tay_long_lra_2020}&
  Generated&
  100,003 &
  3106 W &
  Acc &
  EN \\ \hline
%CLS &
%  \href{https://github.com/google-research/long-range-arena}{Byte-Level Text Classification} ~\cite{tay_long_lra_2020}&
%  Reviews&
%   &
%  4000 Tok&
%  Acc(CLS) &
%  EN \\ \hline
Reasoning &
  \href{https://stanfordnlp.github.io/contract-nli/}{ContractNLI} ~\cite{koreeda-manning-2021-contractnli-dataset}&
  Legal &
  10,319 & %607*17
  2254 Tok&
  EM &
  EN \\ \hline
CLS &
  \href{https://huggingface.co/datasets/THUDM/LongBench}{LSHT(LongBench)}~\cite{bai_longbench:_2023} &
  News &
  200 &
  22,337 W &
  Acc &
  ZH \\ \hline
Reasoning &
  \href{https://github.com/OpenLMLab/LEval}{GSM(16 shot)}~\cite{an_l-eval:_2023} &
  GSM8K \cite{cobbe_training——gsm8k_2021} &
  100 Doc, 100 Inst&
  5557 Tok&
  Rouge-L, GPT-4, $\Delta L$&
  EN \\ \hline
Reasoning &
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{SenHallu(BAMBOO)} ~\cite{dong2023bamboo}&
  Paper &
  200/200 &
  3170/6357 Tok&
  Precision, Recall, F1&
  EN    \\ \hline
Reasoning &
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{AbsHallu(BAMBOO)} ~\cite{dong2023bamboo}&
  Paper &
  200/200 &
  3314/6445 Tok&
  Precision, Recall, F1&
  EN \\ \hline
CLS &
  \href{https://github.com/alinapetukhova/mn-ds-news-classification}{MNDS News} ~\cite{petukhova_mn-ds:_2023}&
  News &
  10,917 &
  637 W &
  Acc &
  EN \\  \hline
\end{tabular}
\end{table*}


\subsubsection{Text Summarization Task}
A summarization dataset is a curated collection of texts and their corresponding summaries. They typically include diverse content, such as news articles, scientific papers, or conversational data, paired with concise and accurate summaries. 
The detailed statistics of the datasets for text summarization are listed in Table~\ref{tab:text-dataset-summarization}.

\begin{itemize}[leftmargin=10pt]
    \item \textbf{CNN/Dailymail}~\cite{nallapati_abstractive_dailymail_2016}, \textbf{GovReport}~\cite{huang_govreport_2021}, and \textbf{XSum}~\cite{narayan_dont_xsum_2018} include a document and its corresponding summary in each instance. CNN/Dailymail is based on over 300,000 news articles, GovReport is based on 14,466 long government reports, and XSum is based on BBC news. 
 \item \textbf{MultiNews}~\cite{fabbri_multi-news:_2019} is a multi-doc summary dataset, each instance consists of multiple news and a summary. 
 \item \textbf{Loogle}~\cite{li_loogle:_2023} is based on papers, WikiPedia, movie and TV scripts. Each long input text corresponds to mutiple question-answer-summary triad. In total there are 776 documents and 6,448 questions. Average document length is 19.367 words. 
 \item \textbf{VCSUM}~\cite{wu_vcsum:_2023} is based on real-world Chinese meeting transcripts. Each meeting tarnscript corresponds to a headline, segmentation summaries and an overall summary. There're 239 meetings in total. 
 %\item \textbf{SAMSum}~\cite{gliwa_samsum_2019} is based on message-like conversations, written by fluent English users. Each instance contains a dialogue and corresponding summary. 
 \item \textbf{SummScreenFD}~\cite{chen_summscreen:_2021} is based on TV transcripts. Each instance consists of a TV transcript containing conversations, scenes and actor actions, and a summary(recapitulation, as referred to in original paper). 
 \item \textbf{BigPatent}~\cite{sharma_bigpatent:_2019} is based on 1,341,362 patent documents. The highlight of this dataset is that important information is distributed evenly in patent documents, compared to other types of documents. Each instance contains a document and its corresponding summary(human written abstract). 
 \item \textbf{SPACE}~\cite{angelidis_extractive_space_2021} is based on reviews of 50 hotels. The highlight of the dataset is that the summaries are written in 6 different aspects, based on the hotel's review. Each hotel constructs an instance, containing the hotel's name, multiple reviews, summaries of different aspects and an overall summary. 
 \item \textbf{SQuality}~\cite{wang2022squality} is based on the same stories domain as QuALITY~\cite{pang_quality_dataset:_2021} dataset. It's a query-based summarization dataset. Each instance contains a story, multiple summarization questions, and multiple summarizations that corresponds to each questions. There are 625 QA pairs in total. 
 \item \textbf{CNNNews(M4LE)}~\cite{kwan_m4le:_2023} is based on CNN English news. Each instance of the dataset is paired with a multi-sentence summary. %NOTE: not M4LE original
 \item \textbf{CEPSUM(M4LE)}~\cite{kwan_m4le:_2023} is based on product information from Chinese e-commerce platform. Each instance contains a product description and corresponding summary. 
 \item \textbf{LCSTS(M4LE)}~\cite{kwan_m4le:_2023} is a summarization dataset in Chinese. It consists of over 2 million posts from a Chinese micro-blogging website, each post is paired with a summary. M4LE selects instances whose article has over 30 words.
 \item \textbf{NCLS(M4LE)}~\cite{kwan_m4le:_2023} is a summarization dataset with articles and corresponding summaries in different language, which highlights model's cross-lingual ability. Original NCLS is constructed from CNNNews and LCSTS.
 \item \textbf{WikiHow(M4LE)}~\cite{kwan_m4le:_2023} is based on procedural descriptions on Wikipedia. Each article is entitled with a beginning of "How to...". Each paragraph of the article describes one step in the procedure, and corresponds to short summary. These summaries are then put together as the suymmary of the article. 
 \item \textbf{News2016(M4LE)}~\cite{kwan_m4le:_2023} is based on ove 2 million news articles in Chinese. For each article, its title is used as golden summary. M4LE remove instances whose length is less than 200 words or over 800 words. 
 \item \textbf{PubMed(M4LE)}~\cite{kwan_m4le:_2023} is based on medical papers. In M4LE, each paper's abstract is used as the summary of the paper. 
 \item \textbf{BookSum(M4LE)}~\cite{kwan_m4le:_2023} is a dataset containing 405 English books, whose contents covers plays, novels and short stories. Each chapter of the content corresponds to a human-written summary. 
 \item \textbf{CNewsum(M4LE)}~\cite{kwan_m4le:_2023} is based on 304,307 news articles in Chinese. Each article corresponds to a human-written summary.
 \item \textbf{CLTS+(M4LE)}~\cite{kwan_m4le:_2023} is based on CLTS~\cite{zhu_natural_clts_2020}. CLTS contains over 180,000 Chinese articles, and CLTS+ uses back translation to make summaries more abstractive. M4LE selects part of these instances for benchmark.
 \item \textbf{Arxiv(M4LE)}~\cite{kwan_m4le:_2023} is based on papers collected from arXiv.org. For each paper, its abstract is used as golden summary.

\end{itemize}

\subsubsection{Text Reasoning Task}
A reasoning task involves the ability of a model to draw logical conclusions, make inferences, or solve problems based on given information. It requires understanding relationships, patterns, or rules within the data to arrive at accurate and coherent outcomes.Natural Language Inference(NLI) can be considered a subset of reasoning. It highlights model's ability to perform logical inference instructed by natural language.In an NLI task, the typical goal is to determine the relationship between two pieces of text: a premise and a hypothesis.
The detailed statistics of the datasets for text reasoning are listed in Tab.~\ref{tab:text-dataset-reasoning}.

\begin{itemize}[leftmargin=10pt]
    \item \textbf{Long Listops}~\cite{tay_long_lra_2020} is a mathematical reasoning dataset. It inputs an listop expression, instructing the model to perform calculation and output the exact numeric answer. A listop expression has a hierarchical structure that involves a set of simple mathematical operators. The final answer is a number in 0-9, described in original paper as "a ten-way classification task". 
    \item  \textbf{GSM}~\cite{cobbe_training——gsm8k_2021} is a mathematcal reasoning dataset, which describes mathematical problems in natural language and ask the model to solve it.
    \item \textbf{ContractNLI}~\cite{koreeda-manning-2021-contractnli-dataset} uses contracts as context, and provides hypothesis, answer, and added evidence to each instance as well. The task requires model to judge the relationship between the hypothesis and context. Each instance contains 607 contracts, each contract has 17 annotated hypothesis and corresponding answers. 
    \item \textbf{LSHT(LongBench)}~\cite{bai_longbench:_2023} is a Chinese classification dataset. It's based on Xinhua News. The model is asked to classify the input news articles into different categories.
    \item \textbf{SenHallu}~\cite{dong2023bamboo} and \textbf{AbsHallu} ~\cite{dong2023bamboo}use content and a related hypothesis as model's input, and instruct the model to determine whether the hypothesis is true based on the content. The false hypothesis(hallucination, as referred to by original paper) is generated by GPT.
    \item \textbf{MNDS News}~\cite{petukhova_mn-ds:_2023} is a classification dataset consisting of 10.917 news articles. The news articles have 17 first level categories and 109 second-level categories. 
\end{itemize}



%Text-RET

\begin{table*}[]
    \centering
    \caption{Text Dataset-Retrieval. In the \textbf{Avg. Len: }  average length, \textbf{W: } words. Particularly, LongEval, StreamingEval and TopicRet is more of a data generation method, which makes their length and instance number flexible, denoted by '-'. In the Metric column, \textbf{Acc:} Accuracy. \textbf{F1}~\cite{rajpurkar-etal-2016-squad-F1} calculates unigram overlap between model output and answers after processing elments like white-spaces and stop-words. }
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{1pt} % 减小列间距
    \label{tab:text-dataset-retrieval}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\textbf{Task} &
  \textbf{Name} &
  \textbf{Source} &
  \textbf{Instances} &
  \textbf{Avg Len} &
  \textbf{Metric} &
  \textbf{Lang.} \\ \hline
CLS/RET &
  \href{https://huggingface.co/datasets/THUDM/LongBench}{TREC(LongBench)}~\cite{bai_longbench:_2023} &
  Web Question &
  200 &
  5177 W &
  Acc &
  EN \\ \hline
RET &
  \href{https://github.com/DachengLi1/LongChat}{LongEval} ~\cite{longchat2023} &
  Conversations&
  - &
  - &
  Acc&
  EN \\ \hline
RET &
  \href{http://arxiv.org/abs/2309.17453}{StreamingEval} ~\cite{DBLP:conf/iclr/XiaoTCHL24} &
  LongChat\cite{longchat2023} &
  -&
  - &
  Acc&
  EN \\ \hline
RET &
  \href{https://github.com/OpenLMLab/LEval}{TopicRet(L-Eval)} ~\cite{an_l-eval:_2023} &
  LongChat\cite{longchat2023} &
  -&
  -&
  Acc&
  EN \\ \hline
%RET &
%  \href{https://github.com/KwanWaiChung/M4LE}{WoW(M4LE)}~\cite{kwan_m4le:_2023} &
%  Wiki &
%   &
%  3434W &
%  Acc &
%  EN \\ \hline
RET &
  \href{https://github.com/KwanWaiChung/M4LE}{DRCD(M4LE)}~\cite{kwan_m4le:_2023} &
  Wiki &
  - &
  3617 W&
  Acc &
  ZH \\ \hline
%RET &
%  \href{https://github.com/google-research/long-range-arena}{Byte-Level Document Retrieval} ~\cite{tay_long_lra_2020}&
%  Papers&
%   &
%  9386.2 W &
%  Acc&
%  EN \\ \hline
CLS+RET &
  \href{https://github.com/KwanWaiChung/M4LE}MARC~\cite{kwan_m4le:_2023} &
  E-Commerce &
  2200 &
  3543 W&
  F1 &
  EN,ZH \\ \hline
CLS+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{Online Shopping(M4LE)}~\cite{kwan_m4le:_2023} &
  E-Commerce &
  2200 &
  3714 W&
  F1 &
  ZH \\ \hline

CLS+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{MNDS News(M4LE)}~\cite{kwan_m4le:_2023} &
  MNDS News~\cite{petukhova_mn-ds:_2023} &
  - &
  3805 W &
  Acc &
  EN \\ \hline
CLS+RET &
  \href{https://github.com/KwanWaiChung/M4LE}{THUCNews(M4LE)}~\cite{kwan_m4le:_2023} &
  News &
  - &
  3721 W&
  Acc &
  ZH \\ \hline
\end{tabular}
\end{table*}

%Text-Aggregation



\subsubsection{Text Retrieval Task}
A retrieval task in LLM benchmarks evaluates a model's ability to retrieve relevant information from a large collection of data based on a given query. It tests the model's understanding of the query, semantic matching, and efficiency in identifying the most relevant documents or pieces of information. 
The detailed statistics of the datasets for text retrieval are listed in Table~\ref{tab:text-dataset-retrieval}.

\begin{itemize}[leftmargin=10pt]
    \item \textbf{LongChat}~\cite{longchat2023} has two subtask dataset for retrieval. Coarse-grained Topic Retrieval dataset use a long document that talk about a number of different topics, and instrutct the model to retrieve the first topic of the document. Fine-grained Line retrieval, on the other hand, is more challenging, which present the model with multiple lines that contain a diffrernt number and label, with similar line patterns. The model is asked to retrieve the number of a specific labeled line.Notably, such dataset can be easily constructed or generated, so it's easy to create an ultra long dataset of this type. Because the dataset is easily constructed by definition, the length of the dataset and the number of instances is indefinite. 
    \item \textbf{StreamingEval}\cite{DBLP:conf/iclr/XiaoTCHL24} construct a line retrieval task based on LongChat, which makes a query in every 10 lines, with its answer about 20 lines above, so as to evaluate the streaming conversation scenario.
    \item  \textbf{TopicRet}~\cite{an_l-eval:_2023} on the other hand, is based on the coarse-grained topic retrieval task, but ask about the second or third topic instead of the first one, so as to make the task more challenging.
%    \item \textbf{WOW(M4LE)}~\cite{kwan_m4le:_2023}  %NOTE(xzc): Not described in M4LE
    \item \textbf{DRCD(M4LE)}~\cite{kwan_m4le:_2023} is a reading comprehension dataset. In M4LE, DRCD is constructed into two subset, one(DRCD explicit) require model to return the articles' IDs related to a given topic, and another subset(DRCD semantic) requires the model to answer specific questions given multiple paragraphs. 
%    \item \textbf{Byte-Level Document Retrieval}~\cite{tay_long_lra_2020}
    \item \textbf{MARC}~\cite{kwan_m4le:_2023} consists of  bilingual(namely English and Chinese) 
reviews. The model is asked to identify all positive reviews and retrieve them. 
\item \textbf{Online Shopping(M4LE)}~\cite{kwan_m4le:_2023} is based on 60K product reviews on Chinese e-commerce platforms. Reviews are categorized into positive and negative. 
\end{itemize}

%Text-GEN
\begin{table*}[]
    \centering
    \caption{Text Dataset-Generation.In the \textbf{Avg. Len: }  average length, \textbf{Tok:}  tokens;  \textbf{W: } words. In the Instances column, \textbf{Doc:} documents, \textbf{Inst:} instructions. In the Metric column, \textbf{EM:} Exact Match. \textbf{Acc:} Accuracy. 
    %\textbf{BLEU}\cite{papineni-etal-2002-bleu}, denoting Bilingual Evaluation Understudy, is also a metric designed for machine translation. \textbf{Rouge}~\cite{lin-2004-rouge} and its variants measure model's performance by calculating overlap between model output and reference answer with unigram(Rouge-1), bigram(Rouge-2), LCS(Rouge-L), etc. \textbf{F1}~\cite{rajpurkar-etal-2016-squad-F1} calculates unigram overlap between model output and answers after processing elments like white-spaces and stop-words. \textbf{BERT}~\cite{zhang2020bertscoreevaluatingtextgeneration} is a text generation task metric using contextual embeddings. \textbf{Edit Sim} is a metric based on edit diatance. Particularly, in MultiDoc2Dia, \textbf{SacreBLEU}~\cite{post-2018-call-sacrebleu} is a unified reference version of BLEU. For datasets from L-Eval, the GPT-4 metric means the win-rate against Turbo-16K, judged by GPT-4. $\Delta L$ is the length difference between answer length and ground truth.
    }
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{1pt} % 减小列间距
    \label{tab:text-dataset-generation}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\textbf{Task} &
  \textbf{Name} &
  \textbf{Source} &
  \textbf{Instances} &
  \textbf{Avg Len} &
  \textbf{Metric} &
  \textbf{Lang.} \\ \hline
GEN &
  \href{https://github.com/microsoft/CodeBERT/tree/master/LongCoder}{LCC} ~\cite{guo_longcoder_lcc:_2023}&
  Code &
  360000 &
  1337 W &
  EM, Edit Sim &
  Python/CSharp/Java \\ \hline
GEN &
  \href{https://github.com/Leolty/repobench}{RepoBench-P(LongBench)}~\cite{bai_longbench:_2023} &
  Code &
  500 &
  4206 W &
  Edit Sim &
  Python/Java \\ \hline
GEN/RET &
  \href{https://github.com/IBM/multidoc2dial}{MultiDoc2Dial} \cite{feng_multidoc2dial:_2021}&
  Doc2Dial \cite{feng_doc2dial:_2020} &
  \begin{tabular}[c]{@{}c@{}}488 Doc, \\4796 Dialogues\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}4283 T\end{tabular}&
  \begin{tabular}[c]{@{}c@{}}F1, EM, SacreBLEU, Recall\end{tabular}&
  EN \\ \hline
GEN &
   \href{https://github.com/OpenLMLab/LEval}{OpenReview(L-Eval)} ~\cite{an_l-eval:_2023}&
  ASAP-Review\cite{yuan_can_asap_review_2021} &
  20 Doc 60 Inst &
  11,170 Tok&
   Rouge-L, GPT-4, $\Delta L$&
   EN\\ \hline
GEN &
  \href{https://github.com/neulab/ReviewAdvisor}{ASAP-Review} ~\cite{yuan_can_asap_review_2021}&
  Paper&
  \begin{tabular}[c]{@{}c@{}}8877 Papers, \\25,986 Reviews\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}6782 W/Paper\end{tabular}&
  Rouge-1/2/L, BERT&
  EN \\ \hline
GEN &
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{ShowsPred} ~\cite{dong2023bamboo}&
  TV Shows &
  100/100 &
  2389/4860 Tok&
  Acc&
  EN \\ \hline
GEN &
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{MeetingPred} ~\cite{dong2023bamboo}&
  Meeting &
  100/100 &
  3689/11578 Tok&
  Acc&
  EN \\ \hline
GEN-Code &
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{PrivateEval} ~\cite{dong2023bamboo}&
  Code &
  152/152 &
  3149/6230 Tok&
  Pass@1&
  EN, Python \\ \hline
GEN-Code&
   \href{https://github.com/OpenLMLab/LEval}{CodeU(L-Eval)} ~\cite{an_l-eval:_2023}&
  Code&
  90 Doc 10 Inst&
  31,575 Tok&
   Rouge-L, GPT-4, $\Delta L$&
  Python \\ 

 \hline
\end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{Text Dataset-Aggregation. In the \textbf{Avg. Len: }  average length, \textbf{Tok:}  tokens;  \textbf{W: } words. In the Instances column, \textbf{Doc:} documents, \textbf{Inst:} instructions. In the Metric column, \textbf{Acc:} Accuracy. \textbf{ES:} Exponential Similarity, \textbf{CI:} Concordance Index}
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{1pt} % 减小列间距
    \label{tab:text-dataset-aggregation}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\textbf{Task} &
  \textbf{Name} &
  \textbf{Source} &
  \textbf{Instances} &
  \textbf{Avg Len} &
  \textbf{Metric} &
  \textbf{Lang.} \\ \hline

AGG &
  \href{https://github.com/tau-nlp/zero_scrolls}{SpaceDigest} ~\cite{shaham_zeroscrolls:_2023}&
  Reviews &
  500 &
  5481 W &
  ES & %zeroscrolls ES -> exponential similarity and Cidx -> concordance index.
  EN \\ \hline
AGG &
  \href{https://github.com/tau-nlp/zero_scrolls}{BookSumSort} ~\cite{shaham_zeroscrolls:_2023}&
  Literature &
  500 &
  6840 W&
  CI & % concordance index
  EN \\ \hline
AGG &
  \href{https://huggingface.co/datasets/THUDM/LongBench}{PassageRetrieval}-en~\cite{bai_longbench:_2023} &
  Wiki &
  200 &
  9289 W &
  Acc &
  EN \\ \hline
AGG &
  \href{https://huggingface.co/datasets/THUDM/LongBench}{PassageRetrieval-zh}~\cite{bai_longbench:_2023} &
  C4 Dataset &
  200 &
  6745 W &
  Acc &
  ZH \\ \hline
AGG &
  \href{https://huggingface.co/datasets/THUDM/LongBench}{PassageCount} ~\cite{bai_longbench:_2023} &
  Wiki &
  200 &
  11,141 W &
  Acc &
  EN \\ \hline
AGG &
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{ShowsReport(BAMBOO)} ~\cite{dong2023bamboo}&
  TV Shows &
  200/200 &
  2992/6411 Tok&
  CI & %Bamboo refered concordance index as CI
  EN \\ \hline
AGG&
  \href{https://github.com/RUCAIBox/BAMBOO/tree/main/datasets}{ReportSumSort(BAMBOO)} ~\cite{dong2023bamboo}&
  Reports &
  150/150 &
  3753/8309 Tok&
  CI &
  EN \\ \hline
\end{tabular}
\end{table*}


\subsubsection{Text Generation Task}
Generation tasks require model to generate contents based on the given instructions and context. 
The detailed statistics of the datasets for text generation are listed in Table~\ref{tab:text-dataset-generation}.

\begin{itemize}[leftmargin=10pt]
    \item \textbf{MultiDoc2Dial}~\cite{feng_multidoc2dial:_2021} gives model a dialogue history and all involved documents, and instruct model to generate the next turn of the dialogue.
    \item \textbf{OpenReview(L-Eval)}~\cite{an_l-eval:_2023}, which is based on \textbf{ASAP-Review}~\cite{yuan_can_asap_review_2021}, provides LLM with a paper and instruct it to generate a review.
    \item \textbf{ShowsPred} and \textbf{MeetingPred}~\cite{dong2023bamboo} use dialogue history as input, and ask model to infer which role said the last turn of the conversation. Apart from natural language context, code generation is also an important implementation for LLMs.
    \item \textbf{LCC}~\cite{guo_longcoder_lcc:_2023} gives model long code snippets as context, and instruct model to generate the following line of code.
    \item \textbf{RepoBench-P}~\cite{liu_repobench:_2023} requires model to retrieve toe most relevant code snippets from a long input, and then generate code according to the instruction.
    \item \textbf{PrivateEval}~\cite{dong2023bamboo} use API documents and a code snippet as input, and instruct the model to generate 
code acccordingly. Notably, to avoid data contamination caused by pre-training, the keywords in API documents are modified, making the document "private".
\item \textbf{CodeU}~\cite{dong2023bamboo} use the same practice of modifying keyword, only that it uses modified source code of public library, rather than API document, as an input. 
\end{itemize}



\subsubsection{Aggregation Task}
Aggregation task involves understanding and aggregating information from the whole input to answer complex instructions, such as calculating the percentage of positive comments given a set of comments of different attitudes. 
The detailed statistics of the datasets for text aggregation are listed in Table~\ref{tab:text-dataset-aggregation}.

\begin{itemize}[leftmargin=10pt]
    \item \textbf{SpaceDigest}~\cite{shaham_zeroscrolls:_2023} give the model a set of hotel reviews, and ask the model to output the percentage of positive reviews in the context.
    \item \textbf{BookSumSort}~\cite{shaham_zeroscrolls:_2023}, \textbf{ReportSumSort}~\cite{dong2023bamboo}, and \textbf{ShowsSort}~\cite{dong2023bamboo} use shuffled paragraphs from book summaries, TV transcripts or government reports as context, and ask the model to sort them in the correct order.
    \item \textbf{PassageCount}~\cite{bai_longbench:_2023} selects multiple passage, duplicates some of the paragraphs, and put all those paragraphs into an instance after shuffling. The model is then asked to determine how many documents are used to construct this instance.
    \item \textbf{PassageRetrieval}~\cite{bai_longbench:_2023}, on the other hand, selects 30 wikipedia passages, and use GPT-3.5-Turbo to write a summary for one of them. Then these passages and the generated summary are used as the model input. The model is then instructed to tell which passage was the summary generated from.
\end{itemize}

\subsubsection{Evaluation Metric for Text Datasets}\label{sssec:text_metric}
General evaluation metrics used by text datasets mentioned above include \textbf{Exact Match}~\cite{DBLP:journals/corr/RajpurkarZLL16}, \textbf{Partial Match}, \textbf{Accuracy}, \textbf{Recall}, \textbf{Precision}, \textbf{F1}, \textbf{BLEU}~\cite{papineni-etal-2002-bleu}, \textbf{SacreBLEU}~\cite{post-2018-call-sacrebleu}, \textbf{Rouge}~\cite{lin-2004-rouge}, \textbf{METEOR}~\cite{denkowski-lavie-2011-meteor}, \textbf{BERT}~\cite{zhang2020bertscoreevaluatingtextgeneration}, \textbf{Edit Similarity}, \textbf{Pass@k}~\cite{chen2021evaluatinglargelanguagemodels} , \textbf{Exponential Similarity}, \textbf{Concordance Index},  \textbf{Mean Reciprocal Rank}. In addition to general evaluation metrics, some more specific metrics are used in particular benchmarks. For datasets from L-Eval~\cite{an_l-eval:_2023}, the \textbf{GPT-4} metric means the win-rate against Turbo-16K, judged by GPT-4. \textbf{$\Delta L$} is the length difference between answer length and ground truth. For LooGLE~\cite{li_loogle:_2023}, it utilizes \textbf{GPT-4} for its QA and summarization task, using it for answer's semantic judgment.
%And particularly...
%\textbf{GPT-4, $\Delta L$} %L-Eval
%\textbf{MRR} %narrativeqa
%\textbf{Partial Match, GPT-4} %Loogle
%\textbf{Gold Rouge-1} %VCSUM
%\textbf{Precision, Recall} %BAMBOO
%REF: https://zhuanlan.zhihu.com/p/405658103 https://zhuanlan.zhihu.com/p/130570024
\begin{itemize}[leftmargin=10pt]
    \item \textbf{Exact Match (EM)}~\cite{DBLP:journals/corr/RajpurkarZLL16} is a metric used to evaluate the accuracy of models in tasks like question answering or text generation. It measures the percentage of predictions that exactly match the ground truth answer, considering both the content and format.% EM is a strict metric, as even minor differences (e.g., punctuation or phrasing) between the prediction and the reference can result in a non-match. %NOTE: Verified
    \item \textbf{Partial Match (PM)} metric evaluates the similarity between a model's output and the reference by allowing partial credit for partially correct answers. Unlike strict metrics like Exact Match (EM), PM accounts for overlaps or shared elements, such as keywords or phrases, making it more flexible in assessing performance.% This metric is particularly useful in tasks where approximate correctness is acceptable or expected. %NOTE: No paper! May be different!
    \item \textbf{Accuracy} is a metric used to evaluate the overall performance of a model by measuring the proportion of correctly predicted instances (both positive and negative) out of the total instances. %It is simple to calculate and widely used, but it may not be reliable for imbalanced datasets where one class dominates. In such cases, alternative metrics like F1 score, Precision, or Recall are often more informative. %NOTE: Verified
    \item \textbf{Recall} is a metric used to evaluate a model's ability to retrieve all relevant instances in a dataset. It is calculated as the ratio of correctly retrieved relevant items to the total number of relevant items, emphasizing completeness. %High recall indicates that the model retrieves most of the relevant items, but it does not account for the precision or quality of the retrieved results. %NOTE: Verified
    \item \textbf{Precision} is a metric used to evaluate the accuracy of a model by measuring the proportion of correctly predicted positive instances out of all predicted positive instances. %NOTE: Verified
    \item  \textbf{F1} is a performance measure that combines Precision and Recall into a single score using their harmonic mean. It provides a balanced evaluation, especially useful in datasets with imbalanced classes, by considering both false positives and false negatives. %Verified
    \item \textbf{BLEU}~\cite{papineni-etal-2002-bleu}, is a widely used metric for evaluating the quality of machine-generated text, especially in machine translation. It works by comparing n-grams in the generated output with reference texts to measure overlap, while applying penalties for overly short outputs to ensure fluency. %Verified
    \item \textbf{SacreBLEU}~\cite{post-2018-call-sacrebleu} is a standardized version of the BLEU metric used to evaluate machine translation quality. It simplifies BLEU's implementation by fixing preprocessing steps like reference handling to ensure consistent and reproducible results across different systems. %NOTE: verified
    \item  \textbf{Rouge}~\cite{lin-2004-rouge} and its variants measure model's performance by calculating overlap between model output and reference answer with unigram(\textbf{Rouge-1}), bigram(\textbf{Rouge-2}), LCS(\textbf{Rouge-L}), etc. \textbf{Gold Rouge-1} in VCSUM dataset refers to using high-quality reference summaries (gold standards) for evaluation, ensuring reliable and meaningful comparisons. %Verified
    \item \textbf{METEOR}~\cite{denkowski-lavie-2011-meteor} (Metric for Evaluation of Translation with Explicit ORdering) is a text evaluation metric designed to assess the quality of machine translation. %It improves upon BLEU by considering exact matches, synonyms, stemming, and word order, providing a more nuanced evaluation of semantic similarity between generated and reference texts. %Verified
    \item \textbf{BERT}~\cite{zhang2020bertscoreevaluatingtextgeneration} metric, often referred to as BERTScore, is a text evaluation metric that uses contextual embeddings from the BERT model to compare similarity between generated and reference texts. %Unlike traditional metrics like BLEU or ROUGE, it captures semantic meaning by aligning tokens based on their contextual representations. This makes BERTScore more effective at evaluating nuanced and meaning-based similarities in natural language tasks.  %Verified
    \item    \textbf{Edit Similarity} is a metric that measures the similarity between two text sequences based on the minimum number of edit operations required to transform one sequence into another. It is derived from the concept of edit distance such as Levenshtein distance. % and is often normalized to produce a similarity score between 0 and 1. %NOTE: Verified
    \item \textbf{Pass@k}~\cite{chen2021evaluatinglargelanguagemodels} evaluates the performance of a model by measuring the percentage that at least one of the top k generated outputs contains a correct solution. In datasets we surveyed, only \textbf{Pass@1} is used. %A higher pass@1 score indicates that the model is more likely to produce the correct result on its first attempt.  %NOTE: Verified, do we need change it to pass@k?
    \item \textbf{Exponential Similarity} is a metric that measures the similarity between two items by exponentially weighting their differences, giving more importance to smaller discrepancies.% This approach ensures that minor differences have a larger impact on the similarity score, making it sensitive to fine-grained variations. It is particularly useful in tasks where small deviations are critical, such as text similarity or pattern recognition. %NOTE: Uncertain
    \item \textbf{Concordance Index} is a metric used to evaluate the predictive accuracy of models, particularly in survival analysis or ranking tasks. % A C-index of 1 indicates perfect prediction, while 0.5 suggests random performance, making it a useful measure for assessing ranking consistency.  %NOTE: Uncertain
    \item \textbf{Mean Reciprocal Rank (MRR)} is an evaluation metric commonly used in information retrieval and recommendation systems to measure the quality of ranked results. It calculates the reciprocal of the rank of the first relevant item in a result list and averages it across all queries.% MRR emphasizes the importance of placing relevant results higher in the ranking, making it particularly useful for tasks where early retrieval of relevant items is critical. 

\end{itemize}

%\subsubsection{Text Translation Task}  %NOTE: Maybe not really necessary?
%Translation tasks involve training models to accurately convert text from one language to another while preserving meaning, context, and tone. These tasks help improve multilingual capabilities of language models, enabling better cross-cultural communication and understanding. Due to the task's straightforward nature, it's easy to create/obtain long datasets from sources like subtitles, transcripts. 

%\begin{itemize}
%    \item \textbf{TedTalks(M4LE)} The TED Talks dataset is a multilingual parallel corpus derived from TED Talk transcripts. It includes translations between English and six other languages, organized into three pairs of linguistically related languages: Galician-Portuguese, Azerbaijani-Turkish, and Belarusian-Russian. These pairs span Romance, Turkic, and Slavic language families, offering a diverse resource for studying translation across languages with varying similarities. 
%    \item \textbf{OpenSubtitles(M4LE)} 
%    \item \textbf{News Commentary(M4LE)}
%\end{itemize}



\begin{table*}[]
    \centering
    \caption{Multimodal Dataset.  Specfically, for data type, \textbf{Img}: Image; \textbf{T}: text; \textbf{V:} Video. For task abbreviation, \textbf{Conv}: conversation task; \textbf{Desc}: description task;  \textbf{Reas: } reasoning task; \textbf{Perc:} perception task;  \textbf{Pred: } prediction; \textbf{NTH: }  needle in the haystack; \textbf{SUMM:}  summary.
    For instance and average column, \textbf{Q}: questions; \textbf{W}: words; \textbf{s}: seconds.
    For example, \textbf{54 Img, 150 Q} denote that there are 54 images with 150 questions.    }
    \label{Multimodal Dataset}
    \renewcommand{\arraystretch}{1.4} % 调整行间距
    \setlength{\tabcolsep}{1.5pt} % 减小列间距
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Tasks &
  Name &
  Data &
  Source &
  Instance &
  Average &
  Metric &
  Language \\ \midrule
  
Conv, Desc, Reas &
  \href{https://github.com/LLaVA-Annonymous/LLaVA}{LLaVA-Bench}~\cite{liu2023llava} &
  Img, T &
  COCO, In-The-Wild &
  54 Img, 150 Q &
  1 Img, 59.9 W &
  Relative Score &
  EN \\ \midrule
Perc, Reas &
  \href{https://github.com/open-compass/MMBench}{MMBench}~\cite{MMBench} &
  Img, T &
  Internet &
  2948 Q &
  1 Img, 114.5 W &
  Acc &
  EN/CN \\\midrule

  \begin{tabular}[c]{@{}c@{}}Pred, Count, \\ NIH, Retrieval\end{tabular}  &
  \href{https://milebench.github.io/}{MileBench}~\cite{song2024milebench} &
  Img, T &
  \begin{tabular}[c]{@{}c@{}}Public, \\ self-building\end{tabular} &
  6440 Q &
  15.2 Img, 422.3 W &
  Acc, ROUGE-L &
  EN \\\midrule
  
\begin{tabular}[c]{@{}c@{}}Reas, NIH, SUMM,\\ Desc, Order, Count\end{tabular} &
  \href{https://github.com/JUNJIE99/MLVU}{MLVU}~\cite{MLVU} &
  V, T &
  \begin{tabular}[c]{@{}c@{}}Public, \\ self-collection\end{tabular} &
  1334 V, 2593 Q &
  704.6s V, 39.7 W &
  M-Avg, G-Avg &
  EN \\\midrule
  
Reas, Retrieval &
  \href{https://longvideobench.github.io/}{LongVideoBench}~\cite{wu2024longvideobench} &
  V, T &
  web-collected &
  3763 V, 6678 Q &
  730.5s V, 49.5 W &
  Acc &
  EN \\\midrule
  
Perc, Recognition, Reas &
  \href{https://video-mme.github.io/home_page.html}{Video-MME}~\cite{fu2024video} &
  V, T &
  YouTube &
  900 V, 2700 Q &
  1017.9s V &
  Acc &
  EN \\\midrule
  
Desc, Reas &
  \href{https://github.com/doc-doc/NExT-QA}{NExT-QA}~\cite{xiao2021next} &
  V, T &
\begin{tabular}[c]{@{}c@{}}YouTube, \\ TV Show, Public\end{tabular} &
  1000 V, 47962 Q &
  44s V, 25.5 W &
  Acc, WUPS &
  EN \\\midrule
  
Perc, Count, Reas &
  \href{https://github.com/OpenGVLab/Ask-Anything/blob/main/video_chat2/MVBENCH.md}{MVBench}~\cite{2023videochat} &
  V, T &
  Public &
  4000 Q &
  16.7s V, 31.3 W &
  Acc &
  EN \\\midrule
  
Decs &
  \href{https://github.com/xudejing/video-question-answering}{MSVD-QA}~\cite{xu2017video} &
  V, T &
  MSVD &
  1970 V, 50505 Q &
  10s V &
  Acc &
  EN \\\midrule
  
Desc &
  \href{https://github.com/xudejing/video-question-answering}{MSRVYY-QA}~\cite{xu2017video} &
  V, T &
  MSRVTT &
  10000 V, 243690 Q &
  15s V &
  Acc &
  EN \\ \bottomrule
\end{tabular}
\end{table*}

\subsection{Multimodal Datasets and Evaluation Metric}\label{ssec:multimodal_dataset}

\subsubsection{Multimodal Datasets}

Multimodal datasets have emerged to address the need for a comprehensive understanding of the complex real world by integrating diverse data types such as text, images, audio, and video. These datasets drive advancements in AI, particularly in machine learning and deep learning, by offering rich and diverse data to train more robust and versatile models.
We analyze the multimodal benchmarks listed in Table \ref{Multimodal Dataset}, highlighting their distinct focuses. Each benchmark is built upon one or more multimodal datasets, involving their collection, processing, and the use of specific validation metrics. Below, we provide a detailed introduction and description of each multimodal benchmark.

\begin{itemize}[leftmargin=10pt]
    \item \textbf{LLaVA-Bench}~\cite{liu2023llava}
The benchmark is structured around image-ground-truth textual description-question-answer triplets, segmented across COCO and In-The-Wild datasets. It assesses a model's proficiency in multimodal instruction adherence and visual reasoning. By employing a suite of tasks and metrics, it quantifies the model's ability to comprehend and act on visual-language directives, articulate comprehensive descriptions, and engage in intricate reasoning processes.

\item \textbf{MMBench}~\cite{MMBench}
This benchmark serves as a bilingual multimodal benchmark, facilitating a comparative analysis of VLM performance across English and Chinese linguistic contexts. It distinctively assesses multimodal models using a hierarchical taxonomy of abilities, stringent quality assurance measures, and a dual-language evaluation framework. Unlike other benchmarks, MMBench~\cite{MMBench} incorporates the CircularEval strategy for comprehensive evaluation and utilizes LLMs for precise extraction of choices, setting it apart from its counterparts.

\item \textbf{MileBench}~\cite{song2024milebench}
evaluates the multi-modal long-context capabilities of LLMs, including both diagnostic and realistic evaluation sets. It emphasizes long-context and multi-image tasks. This unique focus allows it to capture the complexity and diversity of real-world multimodal challenges, setting it apart from existing benchmarks. The dataset in MileBench~\cite{song2024milebench} is characterized by its inclusion of long texts integrated with multiple images, reflecting real-world scenarios where context is key. It contains a diverse range of tasks that require both comprehension and generation. %making it a comprehensive tool for evaluating LLMs.

\item \textbf{MLVU}~\cite{MLVU}
is a holistic benchmark, designed to gauge the capabilities of multi-modal LLMs in comprehending  video content, transcends the constraints of its predecessors by significantly increasing video durations, encompassing diverse video genres, and crafting a spectrum of assessment tasks. This benchmark offers an extensive array of tasks and video genres to evaluate the comprehensive competencies of MLLMs. It highlights the substantial potential for enhancement in current methodologies and emphasizes the critical factors of context length, image comprehension quality, and the selection of LLM architecture for future progress.

\item \textbf{LongVideoBench}~\cite{wu2024longvideobench}
This benchmark offers an extensive benchmarking framework aimed at assessing the capacity of large multimodal models (LMMs) to comprehend lengthy videos with subtitles, extending up to an hour. It places a strong focus on the retrieval and reasoning capabilities over extended, interwoven video and language data streams, tackling the challenge of single-frame bias and underscoring its proficiency in evaluating multimodal comprehension in long contexts.

\item \textbf{Video-MME}~\cite{fu2024video}
A benchmark for comprehensive evaluation, it assesses the proficiency of Multi-modal Large Language Models (MLLMs) in analyzing videos. This dataset comprises a wide array of 900 videos spanning diverse domains and subfields, ensuring extensive scenario coverage. It encompasses videos with lengths ranging from 11 seconds to 1 hour to gauge model flexibility across various time frames. Furthermore, it incorporates various data modalities, including subtitles and audio tracks, to evaluate the comprehensive competencies of MLLMs. The benchmark aims to test the models' capacity for sequential visual data comprehension, with an emphasis on temporal reasoning and the processing of multi-modal inputs.

\item \textbf{NExT-QA}~\cite{xiao2021next}
Advancing video comprehension from mere description to explanation of causal, temporal, and descriptive actions, a video question answering (VideoQA) benchmark has been established. This benchmark boasts a dataset with 5,440 videos and approximately 52K manually annotated question-answer pairs, sorted into causal, temporal, and descriptive categories. It poses a challenge to QA models to engage in reasoning about causal and temporal actions and to decipher complex object interactions within daily activities. Distinguished from other video benchmarks, this benchmark specifically focuses on causal and temporal action reasoning within realistic videos that are rich in object interactions. It stands as one of the largest manually annotated VideoQA datasets, offering support for both multiple-choice and open-ended questions, and includes a variety of videos that mirror real-life scenarios.

\item \textbf{MVBench}~\cite{2023videochat}
Featuring a substantial dataset, the benchmark comprises 200 multiple-choice question-answer (QA) pairs for each of the 20 temporal understanding tasks, amassing a total of 4,000 QA pairs. It draws from a variety of videos across 11 public datasets, spanning diverse domains and scenes, thereby testing models' abilities to comprehend temporal sequences. The benchmark automates the generation of multiple-choice QA pairs from existing video annotations, minimizing human involvement and ensuring a fair evaluation process.

\item \textbf{MSVD-QA}~\cite{xu2017video}
The MSVD dataset is a collection of 1,970 video clips with descriptive captions, initially for video captioning. It features diverse real-world scenarios and assesses multimodal learning models' capabilities in understanding video content and generating natural language descriptions.

\item \textbf{MSRVTT-QA}~\cite{xu2017video}
The MSR-VTT dataset comprises 10,000 video clips with 20 human-transcribed sentences each, focusing on connecting video content with language descriptions. It evaluates multimodal learning models' ability to comprehend video information and translate it into coherent captions, testing their video understanding and language generation skills in a more complex and diverse environment.
\end{itemize}


\subsubsection{Evaluation Metric for Multimodal Datasets}
The evaluation metrics for multimodal datasets include \textbf{Relative Score}, \textbf{Accuracy}, \textbf{ROUGE-L}, \textbf{M-Avg}, \textbf{G-Avg}, \textbf{WUPS}. 
Several common metrics, including \textbf{Accuracy}, \textbf{ROUHE-L}, have been introduced in Sec.~\ref{sssec:text_metric}.
Here, we only introduce the special metrics of multimodal datasets, which include \textbf{Relativa Score}, \textbf{M-Avg}, \textbf{G-Avg}, \textbf{WUPS} as follows:
\begin{itemize}[leftmargin=10pt]
    \item \textbf{Relative Score} This metric is used in LLaVA-Bench to evaluate the performance of multimodal models by comparing their outputs to a reference model, typically text-based GPT-4. It is calculated as the percentage ratio of the candidate model's score to the reference model's score, based on dimensions such as helpfulness, relevance, accuracy, and level of detail.
    
    \item \textbf{M-Avg} Multiple-Choice Average  is calculated as the mean accuracy across all multiple-choice tasks in the MLVU benchmark. The accuracy for each task is determined by the proportion of correctly predicted answers compared to the total number of questions within that task.

    \item \textbf{G-Axg} Generation Average s calculated as the mean score across all generation tasks in the MLVU benchmark. Each task is evaluated on multiple dimensions (e.g., Accuracy, Relevance, Completeness, and Reliability) using GPT-4, with scores ranging from 1 to 5. The overall score for each task is the average of these dimensions, and G-Avg is the mean of these task-level scores.

    \item \textbf{WUPS}~\cite{k2012newsimilaritymeasuretaxonomy} Wu-Palmer Similarity measures the semantic similarity between two words based on their positions in a taxonomy (e.g., WordNet). It calculates how closely related two words are by considering their least common ancestor (LCS).

\end{itemize}