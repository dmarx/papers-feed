 \begin{figure*}[h!]
\centering
\small
\tikzset{
    basic/.style  = {draw, text width=2cm, align=center, font=\sffamily, rectangle},
    root/.style   = {basic, rounded corners=2pt, thin, align=center, fill=white,text width=8cm, rotate=90, font=\footnotesize},
    dnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=nblue, text width=7cm, font=\footnotesize},
    dnode_2/.style = {basic, thin, rounded corners=2pt, align=center, fill=nblue, text width=3.5cm, font=\footnotesize},
    dnode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=nblue, text width=2.5cm, font=\footnotesize}, 
    mnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=7cm, font=\footnotesize},
    mnode_2/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=3.5cm, font=\footnotesize},
    mnode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=2.5cm, font=\footnotesize},
    snode/.style = {basic, thin, rounded corners=2pt, align=center, fill=npurple,text width=7cm, font=\footnotesize},
    snode_2/.style = {basic, thin, rounded corners=2pt, align=center, fill=npurple,text width=3.5cm, font=\footnotesize},
    snode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=npurple,text width=2.5cm, font=\footnotesize},
    dataset_node/.style = {basic, thin, rounded corners=2pt, align=center, fill=nyellow, text width=3.5cm, font=\footnotesize},
    dataset_node_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=nyellow, text width=2.5cm, font=\footnotesize},
    dataset_node_2/.style = {basic, thin, rounded corners=2pt, align=center, fill=nyellow, text width=3.5cm, font=\footnotesize},
    tnode/.style = {basic, thin, align=left, fill=pink!60, text width=15em, align=center},
    xnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=blue!20,text width=5cm,},
    wnode/.style = {basic, thin, align=left, fill=pink!10!blue!80!red!10, text width=6.5em},
    %edge from parent/.style = {draw=black, edge from parent fork right}
    %edge from parent/.style = {draw=black, edge from parent fork down}
}
%
\begin{forest} 
for tree={
    if level=0{
        grow=east,
        growth parent anchor=east,
        parent anchor=south,
        child anchor=west,
        edge path={\noexpand\path[\forestoption{edge},->, >={latex}] 
             (!u.parent anchor) -- +(5pt,0pt) |- (.child anchor)
             \forestoption{edge label};},
    }
    {
        grow=east,
        growth parent anchor=east,
        parent anchor=east,
        child anchor=west,
        edge path={\noexpand\path[\forestoption{edge},->, >={latex}] 
             (!u.parent anchor) -- +(5pt,0pt) |- (.child anchor)
             \forestoption{edge label};},
    }
}
% l sep is used for arrow distance
[KV Cache Management for Large Language Models, root   
    [Datasets and Benchmarks (Sec.~\ref{sec:dataset}), dataset_node_1
        [{Multi-modal Datasets (Sec.~\ref{ssec:multimodal_dataset})}, dataset_node
        ]
        [Text Datasets (Sec.~\ref{ssec:text_dataset}), dataset_node
        ]
    ]
    [System-level Optimization (Sec.~\ref{sec:system-level-opt}), snode_1
        [Hardware-aware Design \\ (Sec.~\ref{sec:sys_hd}), snode_2
            [SSD-based Design (Sec.~\ref{sec:sys_hd_ssd}), snode]
            [Heterogeneous Design (Sec.~\ref{sec:sys_hd_heter}), snode]
            [I/O-based Design (Sec.~\ref{sec:sys_hd_io}), snode]
            [Single/Multi-GPU Design (Sec.~\ref{sec:sys_hd_gpu}), snode]
        ]
        [Scheduling \\ (Sec.~\ref{sec:sys_sch}), snode_2
            [Layer-specific and Hierarchical Scheduling (Sec.~\ref{sec:sys_sch_lhs}), snode]
            [Preemptive and Fairness-oriented Scheduling (Sec.~\ref{sec:sys_sch_pfs}), snode]
            [Prefix-aware Scheduling (Sec.~\ref{sec:sys_sch_ps}), snode]
        ]
        [Memory Management \\ (Sec.~\ref{sec:sys_mm}), snode_2
            [Prefix-aware Design (Sec.~\ref{sec:sys_mm_pd}), snode]
            [Architectural Design (Sec.~\ref{sec:sys_mm_ad}), snode]
        ]
    ]
    [Model-level Optimization (Sec.~\ref{sec:model-level-opt}), mnode_1
        [Non-transformer Architecture (Sec.~\ref{sec:model_nontrans}), mnode_2
            [Hybrid Architecture (Sec.~\ref{sec:model_nontrans_ha}), mnode]
            [Adaptive Sequence Processing Architecture (Sec.~\ref{sec:model_nontrans_na}), mnode]
        ]
        [Architecture Alteration (Sec.~\ref{sec:model_newarch}), mnode_2
            [Augmented Architecture (Sec.~\ref{sec:model_newarch_aug}), mnode]
            [Enhanced Attention (Sec.~\ref{sec:model_newarch_attn}), mnode]
        ]
        [Attention Grouping and Sharing (Sec.~\ref{sec:model_sharing}), mnode_2
            [Cross-Layer Sharing (Sec.~\ref{sec:model_sharing_cross}), mnode]
            [Intra-Layer Grouping (Sec.~\ref{sec:model_sharing_intra}), mnode]
        ]
    ]
    [Token-level Optimization (Sec.~\ref{sec:token_level}), dnode_1
        [KV Cache Low-rank Decomposition  (Sec.~\ref{ssec:kv_low_rank}), dnode_2
            [Learned Low-rank Approximation  (Sec~\ref{sssec:kv_low_rank_learned}), dnode]
            [Tensor Decomposition  (Sec~\ref{sssec:kv_low_rank_tensor}), dnode]
            [Singular Value Decomposition  (Sec~\ref{sssec:kv_low_rank_svd}), dnode]
        ]
         [KV Cache \\Quantization  (Sec.~\ref{ssec:kv_quant}), dnode_2
            [Outlier Redistribution  (Sec~\ref{sssec:outlier_redistribution}), dnode]
            [Mixed-precision Quantization  (Sec~\ref{sssec:kv_quant_mixed_precision}), dnode]
            [Fixed-precision Quantization  (Sec~\ref{sssec:kv_quant_fixed_precision}), dnode]
        ]
        [KV Cache \\Merging  (Sec.~\ref{ssec:kv_merge}), dnode_2
            [Cross-layer Merging  (Sec~\ref{sssec:kv_merge_cross_layer}), dnode]
            [Intra-layer Merging  (Sec~\ref{sssec:kv_merge_intra_layer}), dnode]
        ]
        [KV Cache Budget  Allocation  (Sec.~\ref{ssec:kv_budget}), dnode_2
            [Head-wise Budget Allocation  (Sec~\ref{sssec:kv_budget_head_wise}), dnode]
            [Layer-wise Budget Allocation  (Sec~\ref{sssec:kv_budget_layer_wise}), dnode]
        ]
        [KV Cache \\Selection  (Sec.~\ref{ssec:cache_sel}),dnode_2
            [Dynamic Selection without Permanent Eviction  (Sec~\ref{sssec:dynamic_kv_no_permanent}), dnode]
            [Dynamic Selection with Permanent Eviction  (Sec~\ref{sssec:dynamic_kv_permanent}), dnode]
            [Static KV Cache Selection  (Sec~\ref{sssec:static_kv}), dnode]
        ]
    ]
]
\end{forest}
%\vspace{-1em}
\caption{Taxonomy of KV Cache Management for Large Language Models.}
\label{fig:framework}
\centering

\end{figure*}


\section{Taxonomy}
\label{sec:taxonomy}

In the above sections, we analyzed how the number of cached Key-Value (KV) pairs significantly impacts both the computation time and the additional memory required during inference. Efficient KV cache management is critical to balancing performance improvements and resource utilization, especially as sequence lengths and model sizes continue to grow.
After carefully reviewing existing approaches, we categorize KV cache optimization strategies into three levels: token-level optimization, model-level optimization, and system-level optimizations. 
Each level addresses specific aspects of the challenges associated with KV cache management and offers distinct techniques to enhance efficiency.
The detailed taxonomy is illustrated in  Fig.~\ref{fig:framework}. 
\begin{itemize}[leftmargin=8pt]
    \item \textbf{Token-Level Optimization} refers to improving KV cache management efficiency by focusing on fine-grained  the careful selection, organization, and compression  at the token level, requiring no architectural changes to the original model.    
    While KV cache selection (Sec.~\ref{ssec:cache_sel}) focuses on prioritizing and storing only the most relevant tokens. 
    KV cache budget allocation (Sec.~\ref{ssec:kv_budget}) dynamically distributes memory resources across tokens to ensure efficient cache utilization under limited memory. 
    Furthermore, KV cache merging (Sec.~\ref{ssec:kv_merge}) reduces redundancy by combining similar or overlapping KV pairs, while KV Cache Quantization (Sec.~\ref{ssec:kv_quant}) minimizes the memory footprint by reducing the precision of cached KV pairs. 
    Finally, KV cache low-rank decomposition (Sec.~\ref{ssec:kv_low_rank}) uses  low-rank decomposition technique to reduce cache size.
    
    \item \textbf{Model-level Optimization} refers to designing an efficient model structure to optimize KV cache management. This can further refer to several strategies: Attention grouping and sharing (Sec.~\ref{sec:model_sharing}) methods examine the redundant functionality of key and values and group and share KV cache within or across transformer layers. Architecture alterations (Sec. \ref{sec:model_newarch} emerge to design new attention mechanisms or construct extrinsic modules for KV optimization. Furthermore, there are also works designing or combining non-transformer architectures~\ref{sec:model_nontrans} that adopt other memory efficient designs like recurrent neural networks to optimize the KV cache in traditional transformers. 
    
    \item \textbf{System-level Optimization} refers to optimizing the KV Cache management through two classic low-level aspects: memory management (Sec.~\ref{sec:sys_mm}) and scheduling (Sec.~\ref{sec:sys_sch}). 
    While memory management techniques focusing on architectural innovations like virtual memory adaptation, intelligent prefix sharing, and layer-aware resource allocation, scheduling strategies have evolved to address diverse optimization goals through prefix-aware methods for maximizing cache reuse, preemptive techniques for fair context switching, and layer-specific mechanisms for fine-grained cache control.
    In addition, we provide a detailed introduction for hardware accelerator design in Sec.~\ref{sec:sys_hd}, including single/multi-GPU, I/O-based solutions, heterogeneous computing and SSD-based solutions.
\end{itemize}

