%\todo{整体check表格}

\section{Model-level Optimization}\label{sec:model-level-opt}
% {\color{red}please check this paper whther in this level: Lossless KV Cache Compression to 2\% (already included: CLLA)}
\begin{figure*}[t]
\centering
\tikzset{
    basic/.style  = {draw, text width=2cm, align=center, font=\sffamily, rectangle},
    root/.style   = {basic, rounded corners=2pt, thin, align=center, fill=white,text width=8cm, rotate=90, font=\footnotesize},
    dnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen, text width=3.5cm, font=\footnotesize},
    dnode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=2cm, font=\footnotesize},
    mnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=3.5cm, font=\footnotesize},
    mnode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=2.5cm, font=\footnotesize}, 
    mnode_2/.style = {basic, thin, rounded corners=2pt, align=center, fill=ngreen,text width=2.5cm, font=\footnotesize}, 
    snode/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!30,text width=3.5cm, font=\footnotesize},
    snode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!30,text width=2.5cm, font=\footnotesize},
    tnode/.style = {basic, thin, align=left, fill=pink!60, text width=15em, align=center},
    xnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=blue!20,text width=5cm,},
    wnode/.style = {basic, thin, rounded corners=2pt, align=left, fill=white,text width=5cm, font=\footnotesize},
    %edge from parent/.style = {draw=black, edge from parent fork right}
    %edge from parent/.style = {draw=black, edge from parent fork down}
}
%
\begin{forest} 
for tree={
    grow=east,
    growth parent anchor=east,
    parent anchor=east,
    child anchor=west,
    edge path={\noexpand\path[\forestoption{edge},->, >={latex}] 
         (!u.parent anchor) -- +(5pt,0pt) |- (.child anchor)
         \forestoption{edge label};}
}
% l sep is used for arrow distance
[Model-level \\Optimization (Sec.~\ref{sec:model-level-opt}), mnode
    [Non-transformer Architecture (Sec.~\ref{sec:model_nontrans}), mnode
        [Hybrid Architecture (Sec.~\ref{sec:model_nontrans_ha}), mnode
            [{MixCon \cite{xuMixConHybridArchitecture2024},
            GoldFinch \cite{goldsteinGoldFinchHighPerformance2024},
            RecurFormer \cite{yanRecurFormerNotAll2024}}, wnode]
        ]
        [Adaptive Sequence Processing Architecture (Sec.~\ref{sec:model_nontrans_na}), mnode
            [{RWKV \cite{pengRWKVReinventingRNNs2023},
            Mamba \cite{guMambaLinearTimeSequence2024},
            RetNet \cite{sunRetentiveNetworkSuccessor2023},
            MCSD \cite{yangMCSDEfficientLanguage2024}}, wnode]
        ]
    ]
    [Architecture Alteration (Sec.~\ref{sec:model_newarch}), mnode
        [Augmented \\Architecture (Sec.~\ref{sec:model_newarch_aug}), mnode
            [{YOCO \cite{sunYouOnlyCache2024}, 
            CEPE \cite{yenLongContextLanguageModeling2024a}, 
            XC-Cache \cite{monteiroXCCacheCrossAttendingCached2024}, 
            Block Transformer \cite{hoBlockTransformerGlobaltoLocal2024}}, wnode]
        ]
        [Enhanced Attention (Sec.~\ref{sec:model_newarch_attn}), mnode
            [{
            MLA \cite{deepseek-aiDeepSeekV2StrongEconomical2024},
            FLASH \cite{huaTransformerQualityLinear2022}, 
            Infini-Attention \cite{munkhdalaiLeaveNoContext2024}}, wnode]
        ]
    ]
    [Attention Grouping and Sharing (Sec.~\ref{sec:model_sharing}), mnode
        [Cross-Layer Sharing (Sec.~\ref{sec:model_sharing_cross}), mnode
            [{CLA \cite{brandonReducingTransformerKeyValue2024}, 
            LCKV \cite{wuLayerCondensedKVCache2024}, 
            SA  \cite{liaoKVCachingShared2024},
            MLKV \cite{zuhriMLKVMultiLayerKeyValue2024},
            LISA \cite{muCrosslayerAttentionSharing2024},
            Wu et al. \cite{wuSystematicStudyCrossLayer2024},
            CLLA \cite{yangLosslessKVCache2024},
            DHA \cite{chenDHALearningDecoupledHead2024},
            SVFormer \cite{zhouValueResidualLearning2024}}, wnode]
        ]
        [Intra-Layer Grouping (Sec.~\ref{sec:model_sharing_intra}), mnode
            [{MQA\cite{shazeerFastTransformerDecoding2019}, 
            GQA \cite{ainslieGQATrainingGeneralized2023}, 
            AsymGQA \cite{chenOptimisedGroupedQueryAttention2024a}, 
            Weighted GQA \cite{chinnakonduruWeightedGroupedQuery2024}, 
            QCQA \cite{joshiQCQAQualityCapacityaware2024}, 
            KDGQA \cite{khanUniformQueryDistribution2024}, GQKVA\cite{javadiGQKVAEfficientPretraining2023}}, wnode]
        ]
    ]
]
\end{forest}

\caption{Taxonomy of the model based KV optimization for Large Language Models.}
\label{fig:taxonomy_model_based}
\end{figure*}

In model-level optimization, new architectures or mechanisms are designed for transformers to allow more efficient reuse of KV cache. Typically, these methods require retraining or fine-tuning of the model to come into operation. Nevertheless, efficient transformation pipelines have also been proposed to allow for a fast deployment to new architectures. According to where and how the refinement was made to the models, we separate related works to the grouping and sharing mechanisms within or cross layers (Sec.~\ref{sec:model_sharing}), implementing architecture modification or augmentation (Sec.~\ref{sec:model_newarch}), and incorporating non-transformer architectures for optimization (Sec.~\ref{sec:model_nontrans}).
The taxonomy of the model-level optimization is shown in Fig.~\ref{fig:taxonomy_model_based}.



\subsection{Attention Grouping and Sharing}\label{sec:model_sharing}

This section explores attention grouping and sharing methods as effective strategies for optimizing key-value (KV) management. We categorize the approaches into two distinct subtypes: intra-layer grouping (Sec. \ref{sec:model_sharing_intra}) that focuses on grouping query, key, and value heads within individual layers to reduce redundancy and improve efficiency, and cross-layer sharing \ref{sec:model_sharing_cross} that shares key, value, or attention components across layers to improve information reuse and reduce KV cache requirements.
The summary of attention grouping and sharing is listed in Tab.~\ref{tab:model_sharing}.

\subsubsection{Intra-layer Grouping}\label{sec:model_sharing_intra}

Shazeer first introduced Multi-Query Attention (MQA) \cite{shazeerFastTransformerDecoding2019} that modified the traditional multi-head attention mechanism. In MQA, all attention heads in a transformer block share a single key and value. This simple strategy can greatly accelerate the decoding procedure. The experiments of the author show that MQA would gain much efficiency with only minor quality degradation incurring.

MQA is a radical strategy that would cause not just quality degradation, but also training instability.  GQA (Grouped Query Attention) \cite{ainslieGQATrainingGeneralized2023} introduced a trade-off solution by dividing the query heads into multiple groups, while each group shares its own keys and values. In addition, an uptraining process is proposed to efficiently convert existing MHA models to GQA configurations by mean-pooling the key and value heads associated with each group. Empirical evaluations demonstrated that GQA models achieve performance close to the original MHA models while offering inference time comparable to MQA.


\begin{table*}[t]
    \centering
    \caption{The summary of Model-based Attention Grouping and Sharing approaches. 
    %\\ \textcolor{blue}{Not checked means not mentioned, which is not equivalent to not compatible.} 
    }
    \label{tab:model_sharing}
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{2.3pt} % 减小列间距
    \begin{tabular}{c|cc|c|c|c}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & 
        \multicolumn{2}{c|}{\makecell{\textbf{Applied Location}}} & 
        \multirow{2}{*}{\makecell{\textbf{Intra-layer Grouped} \\ \textbf{Component}}} & 
        \multirow{2}{*}{\makecell{\textbf{Cross-layer Shared} \\ \textbf{Component}}} & 
        \multirow{2}{*}{\makecell{\textbf{Retraining Required}}}
        \\ 
        \cline{2-3}
        & \multicolumn{1}{c|}{\textbf{\makecell{Intra-layer}}} & \textbf{Cross-layer} & & &  \\
        \midrule
        MQA~\cite{shazeerFastTransformerDecoding2019} & \multicolumn{1}{c|}{\checkmark} & & K, V & - & \checkmark \\
        GQA~\cite{ainslieGQATrainingGeneralized2023} & \multicolumn{1}{c|}{\checkmark} & & K, V & - & Uptrain \\
        AsymGQA~\cite{chenOptimisedGroupedQueryAttention2024a} & \multicolumn{1}{c|}{\checkmark} & & K,V & - & Finetune \\
        Weighted GQA~\cite{chinnakonduruWeightedGroupedQuery2024} & \multicolumn{1}{c|}{\checkmark} & & K,V & - & Uptrain \& Finetune \\
        QCQA~\cite{joshiQCQAQualityCapacityaware2024} & \multicolumn{1}{c|}{\checkmark} & & K, V & - & \checkmark \\
        KDGQA \cite{khanUniformQueryDistribution2024} & \multicolumn{1}{c|}{\checkmark} & & K, V & - & \checkmark \\
        GQKVA~\cite{javadiGQKVAEfficientPretraining2023} & \multicolumn{1}{c|}{\checkmark}& & Q, K, V & - & \checkmark \\
        CLA~\cite{brandonReducingTransformerKeyValue2024} & \multicolumn{1}{c|}{\checkmark} & \checkmark & K, V & K, V & \checkmark \\
        LCKV~\cite{wuLayerCondensedKVCache2024} & \multicolumn{1}{c|}{} & \checkmark & - & K, V & \checkmark\\
        SA~\cite{liaoKVCachingShared2024} & \multicolumn{1}{c|}{} & \checkmark & - & Attention Weight & \checkmark\\
        MLKV~\cite{zuhriMLKVMultiLayerKeyValue2024} & \multicolumn{1}{c|}{\checkmark} & \checkmark & K, V & K, V & Uptrain\\
        LISA~\cite{muCrosslayerAttentionSharing2024} & \multicolumn{1}{c|}{} & \checkmark & & Q, K, V & Lightweight adaption \\
        Wu et al.~\cite{wuSystematicStudyCrossLayer2024} & \multicolumn{1}{c|}{} & \checkmark & - & Q, K, V & \checkmark \\
        CLLA~\cite{yangLosslessKVCache2024} & \multicolumn{1}{c|}{} & \checkmark & - & Q, K, V & \checkmark \\
        DHA~\cite{chenDHALearningDecoupledHead2024} & \multicolumn{1}{c|}{\checkmark} & \checkmark & K, V & Q, K, V & Lightweight adaption\\
        SVFormer~\cite{zhouValueResidualLearning2024} & \multicolumn{1}{c|}{} & \checkmark & - & V & \checkmark\\      
        \bottomrule
    \end{tabular}
\end{table*}

There were several extensions based on GQA. AsymGQA \cite{chenOptimisedGroupedQueryAttention2024a} extends GQA by proposing an activation-informed merging strategy. Instead of grouping the heads by uniform clustering, AsymGQA dynamically determines the grouping of quries based on their activations similarities during training and constructs an asymmetric group results, which leads to better optimization and generalization. Weighted GQA \cite{chinnakonduruWeightedGroupedQuery2024} introduces additional trainable weights to each key and value head, which can be seamlessly integrated into existing GQA models. By tuning weights during training, it improves the performance of the model without additional inference overhead. QCQA \cite{joshiQCQAQualityCapacityaware2024} utilizes an evolutionary algorithm to identify the optimal query head groupings for GQA, which is guided by a computationally efficient fitness function that leverages the weight-sharing error and the KV cache to evaluate text generation quality and memory capacity. KDGQA \cite{khanUniformQueryDistribution2024} argues that many variances of GQA adopt a fixed grouping strategy, thus lacking dynamic adaptability to the evolving of key-value interactions during training. Their Dynamic Key-Driven GQA address these issues by allocating groups using key head norms adaptively during training, resulting in a flexible strategy to query head grouping and enhance the performance. 

GQKVA \cite{javadiGQKVAEfficientPretraining2023} advances the grouping strategy and comes up with a generalized query, key and value grouping mechanism. It first introduces MKVA and GKVA, in which the key and value are grouped to share the same query. Based on this, GQKVA is proposed to separately group the query and key-value pairs. Typically, queries are partitioned into \(g_q\) groups, and keys and values are partitioned into \(g_{kv}\) groups, and each combination of query and key-value pairs would interact using dot product attention. This results in \(g_q \times g_{kv}\) distinct outputs. It generalized different group strategy on query, key and value and preserves good computational efficiency and comparable performance as MHA.

\subsubsection{Cross-layer Sharing}\label{sec:model_sharing_cross}

Brandon et al. introduce Cross Layer Attention (CLA) \cite{brandonReducingTransformerKeyValue2024} that extends the ideas of GQA and MQA by sharing the key and value heads between adjacent layers, further reduce the redundancy in the KV cache. This achieves an additional 2\(\times\) KV cache size reduction compared to MQA, significantly improving memory efficiency without altering computational complexity.

LCKV \cite{wuLayerCondensedKVCache2024} proposes only to compute and cache the key and value for a small subset of layers, even only the top layer, then let queries in bottom layers pair the saved keys and values for inference. This method not only drastically improves the inference speed and reduces memory consumption but is also orthogonal to existing memory-saving techniques, enabling straightforward integration for further optimization. While such a mechanism makes next token computation depend on top layer keys and values of previous tokens, which contradict to the parallel training of transformers, LCKV introduces an approximate training methods to support parallel training.

SA (Shared Attention) \cite{liaoKVCachingShared2024} proposes reuse of computed attention weights across multiple layers, rather than recalculating them for each layer. Unlike other methods focusing on sharing key-value caches, SA leverages the isotropic tendencies of attention distributions observed in pre-trained LLMs to directly share attention weights, greatly reducing both computational overhead and memory usage. 

MLKV (Multi-Layer Key-Value) \cite{zuhriMLKVMultiLayerKeyValue2024} introduces a simple KV head sharing mechanism across multiple transformer layers. MLKV uses the same single KV head as MQA within a layer, but it also shares this KV head with multiple layers. This extreme strategy reduces the cache size to almost 1\% of normal GQA strategies, and experiments show that MLKV still has comparable performance. 

LISA (Lightweight Substitute for Attention) \cite{muCrosslayerAttentionSharing2024} makes a comprehensive analysis for the similarity of attention patterns across layers. Directly sharing attention weights across layers is ineffective because of the misalignment of the attention head and the sensitivity of shallow layers. LISA~\cite{muCrosslayerAttentionSharing2024} addresses challenges by incorporating tiny feed-forward networks to align attention heads between layers and using low-rank matrices to approximate variations in layer-wise attention weights. This achieves a 6× compression of query and key parameters while maintaining high accuracy and perplexity.

Wu et al. \cite{wuSystematicStudyCrossLayer2024} introduce a unified framework that systematically analyzes and optimizes the cross-layer Key-Value cache sharing mechanism. They consolidate several existing methods, explore novel variants within a cohesive structure, and make thorough evaluations of these methods. The study finds that 2 times reduction to KV cache size can outperform standard transformers in throughput without substantial accuracy loss, while further reduction requires alternative design with additional training costs. With the analysis results, they offer insight into the choice of appropriate KV sharing methods based on the specific requirement or constraints.
\begin{table*}[t]
    \centering
    \caption{The summary of Model-based Intra-layer approaches.}
    \label{tab:model_intra}
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{2.3pt} % 减小列间距
    \begin{tabular}{c|cc|cc}
        \toprule
        \multirow{2}{*}\textbf{Method} & 
        \multicolumn{2}{c|}{\makecell{\textbf{Alteration Type}}} & 
        \multirow{2}{*}{\makecell{\textbf{KV Cache} \\ \textbf{Management}}} & 
        %\makecell{\textbf{?} \\ \textbf{?}} 
        \multirow{2}{*}{\makecell{\textbf{Retraining} \\ \textbf{Requirement}}} \\ 
        \cline{2-3}
        & \multicolumn{1}{c|}{\makecell{\textbf{Enhanced} \\ \textbf{Attention}}} & \multicolumn{1}{c|}{\makecell{\textbf{Augmented} \\ \textbf{Architecture}}} & & \\
        \midrule
        MLA \cite{deepseek-aiDeepSeekV2StrongEconomical2024} & \multicolumn{1}{c|}{\checkmark} & & Latent compression & \checkmark \\
        FLASH \cite{huaTransformerQualityLinear2022} & \multicolumn{1}{c|}{\checkmark} & &  Linear approximation & \checkmark \\
        Infini-Attention \cite{munkhdalaiLeaveNoContext2024} & \multicolumn{1}{c|}{\checkmark} & & Compressive cache & \checkmark\\
        YOCO~\cite{sunYouOnlyCache2024} & \multicolumn{1}{c|}{} & \checkmark & Single global KV cache & \checkmark \\
        CEPE~\cite{yenLongContextLanguageModeling2024a} & \multicolumn{1}{c|}{} & \checkmark & Parallel encoding with cross-attn & Lightweight \\
        XC-Cache~\cite{monteiroXCCacheCrossAttendingCached2024} & \multicolumn{1}{c|}{} & \checkmark & Encoder cross-attention & \checkmark \\
        Block Transformer~\cite{hoBlockTransformerGlobaltoLocal2024}           & \multicolumn{1}{c|}{} & \checkmark & Hierarchical local KV & Lightweight \\
        \bottomrule
    \end{tabular}
\end{table*}

CLLA (Cross-Layer Latent Attention) \cite{yangLosslessKVCache2024} introduces an integrated framework combining multiple strategies: attention head size and dimension reduction, cross-layer cache sharing, and KV cache quantization. By unifying these strategies, CLLA achieves extreme KV cache compression to less than 2\% of the original model size while maintaining performance levels comparable with uncompressed models.

DHA (Decoupled Head Attention) \cite{chenDHALearningDecoupledHead2024} addresses redundancy in MHA and adaptively configures shared groups for key and value heads across layers, reducing KV cache requirements. Observing that clustering and fusing similar heads can reduce KV cache size without significant performance reduction, DHA designs a search, fusion, and continued pre-training framework that can progressively transform MHA checkpoints into DHA models through linear fusion of head parameters, preserving the pre-trained knowledge with small pre-training budget.

Observing that later layers in traditional transformers overly rely on narrow regions of attention, Zhou et al. \cite{zhouValueResidualLearning2024} introduce ResFormer that utilizes residual connections from the value embeddings of the first layer to all subsequent layers, effectively approximating cross-layer attention without incurring significant computational costs. They then propose a simplified variant SVFormer that shares a single value embedding across all layers, dramatically reducing the KV cache size by nearly half while maintaining competitive performance. The proposed architectures are flexible to incorporate with other KV-efficient strategies for additional memory savings. 

\subsubsection{Summary and Future Directions} 
This section highlights innovative strategies for optimizing memory and computational efficiency through intra-layer grouping and cross-layer sharing mechanisms. However, several avenues for improvement remain. First, maintaining performance while optimizing efficiency, especially for precision-sensitive tasks, requires further investigation. Methods that implement radical grouping and sharing mechanisms may compromise the model fidelity for tasks requiring high precision. Second, scalability across diverse model architectures and sizes is essential. Works such as DHA~\cite{chenDHALearningDecoupledHead2024} and LISA~\cite{muCrosslayerAttentionSharing2024}, which rely on specific architectural assumptions, may struggle to generalize to emerging LLMs or non-standard configurations. Third, the dynamics of attention across both time and layers are largely under-explored. Most existing methods rely on static or pre-determined grouping and sharing strategies, neglecting the temporal and contextual variations in attention patterns.

To address these challenges and unlock the full potential of attention optimization, future research should focus on the following aspects. First, developing universal frameworks for attention grouping and sharing that require minimal retraining to enhance adaptability and usability. Second, synergistic integration with other optimization techniques, such as quantization and pruning, has significant potential to achieve even greater efficiency gains. While some works like CLLA~\cite{yangLosslessKVCache2024} have begun to address these opportunities, more exploration could be carried out to unlock new levels of efficiency. Third, more dynamic and temporal modeling could be leveraged to adaptively adjust grouping and sharing during runtime to better capture the contextual requirements of different tasks and sequences. Finally, a deeper understanding of the downstream impacts of these techniques on fine-tuning and transfer learning is crucial for their effective application in real-world scenarios.


 


\subsection{Architecture Alteration}\label{sec:model_newarch}

This section explores architectural modifications to optimize KV cache usage. We categorize these methods into two subsections: methods that refine the attention mechanism for KV cache efficiency (Sec. \ref{sec:model_newarch_attn}), and methods that introduce structural changes for better KV management (\ref{sec:model_newarch_aug}). Many of these works build upon the broader landscape of efficient attention mechanisms (e.g., Linear Transformer~\cite{katharopoulos2020transformers}, Performer~\cite{choromanski2020rethinking}, LinFormer~\cite{wang2020linformer}, etc.). Since our focus lies on methods directly impacting KV cache handling, for a comprehensive overview of efficient attention mechanisms, we refer readers to dedicated surveys~\cite{zhou2024survey}.
The summary of architecture alteration for KV reuse is listed in Tab.~\ref{tab:model_intra}.


\subsubsection{Enhanced Attention}\label{sec:model_newarch_attn}
\begin{table*}[t]
    \centering
    \caption{The summary of Non-Transformer Architectures.}
    \label{tab:model_nontrans}
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{2.3pt} % 减小列间距
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Method} & 
        \makecell{\textbf{Key Mechanism}} & 
        \makecell{\textbf{No Traditional KV Cache}} & 
        \makecell{\textbf{KV Cache Compression}} \\ 
        \midrule
        RWKV~\cite{pengRWKVReinventingRNNs2023} & RNN-like with Transformer parallelism &\checkmark & \\
        Mamba~\cite{guMambaLinearTimeSequence2024} & Selective state-space model & \checkmark & \\
        RetNet~\cite{sunRetentiveNetworkSuccessor2023} & Retention mechanism  & & \checkmark \\
        MCSD~\cite{yangMCSDEfficientLanguage2024} & Slope-decay fusion &\checkmark & \\
        MixCon~\cite{xuMixConHybridArchitecture2024} & Transformer + Conba + MoE & \checkmark & \\
        GoldFinch~\cite{goldsteinGoldFinchHighPerformance2024} & RWKV + Modified Transformer & & \checkmark \\
        RecurFormer~\cite{yanRecurFormerNotAll2024} & Mamba replacing some attention heads & & \checkmark  \\
        \bottomrule
    \end{tabular}
\end{table*}

 

DeepSeek-V2 \cite{deepseek-aiDeepSeekV2StrongEconomical2024} introduced Multi-Head Latent Attention (MLA) that adopts a low-rank KV joint compression mechanism, replacing the full KV cache with compressed latent vectors. The model adopts trainable projection and expansion matrices to do the compression. This compression mechanism significantly reduces the memory requirement of the KV cache and allows the model to handle sequences up to 128K tokens. 

FLASH \cite{huaTransformerQualityLinear2022} incorporates the Gated Attention Unit (GAU) to replace the MHA mechanism in traditional transformers. GAU utilizes a single-head attention mechanism with gating functions that selectively modulates importance in information flow. FLASH employs a linear approximation method for attention computation through GAU module, which makes the model efficiently handle long contexts without the quadratic scaling of traditional self-attention, thus mitigating heavy KV cache issues.

Infini-Attention \cite{munkhdalaiLeaveNoContext2024} adopts representation compression to store long-term content. Furthermore, they introduce a hybrid attention mechanism of masked local attention and long-term linear attention. The masked local attention replaces the standard MHA to let the model only concentrate on local contexts, while the long-term linear attention utilizes compressed memory for far-reaching dependencies and uses linear attention for efficient aggregation. Thus, infini-attention combines both local fine-grained and long-range compressed states, allowing a seamless balance between long-term and short-term context modeling.

\subsubsection{Augmented Architecture}\label{sec:model_newarch_aug}

YOCO \cite{sunYouOnlyCache2024} builds a decoder-decoder architecture composed of two modules: a self-decoder and a cross-decoder. The self-decoder efficiently encodes global key-value caches, while the cross-decoder reuses these caches via cross-attention. This design ensures that key-value pairs are only cached once, substantially reducing GPU memory usage while maintaining global attention capabilities. YOCO’s computation flow also enables the prefilling to early exit, allowing faster prefill stages without altering the final output.

CEPE \cite{yenLongContextLanguageModeling2024a} interleaves additional cross-attention layers between the self-attention and feed-forward layers in the decoder model. It employs a small encoder to process long inputs chunk-by-chunk to encoded representations as cross-attention layers' inputs. In this way, CEPE can prevent the needs for KV cache for every token and reduce computational cost by processing contexts in parallel. This also facilitates an existing LLMs to expand its contexts while preserving the scalability and generalizability.







XC-Cache \cite{monteiroXCCacheCrossAttendingCached2024} also utilizes an encoder to interleave cross-attention layers within existing self-attention layers in pre-trained decoder-only models to prevent explicit prompt caching. The encoder processes the context and converts it into a compact set of key-value pairs that summarize the essential information. It also finds that pre-trained causal decoders can be used to replace an encoder for representations extraction, further reducing the training costs on additional encoder. 

Block Transformer \cite{hoBlockTransformerGlobaltoLocal2024} introduces a hierarchical global-to-local architecture by combining coarse-grained global attention and fine-grained local attention. In lower layers, tokens are grouped into fixed-size blocks, allowing global context modeling with reduced KV cache overhead. In upper layers, attention operates within individual blocks, enabling lightweight, detailed token decoding with a smaller local KV cache.

\subsubsection{Summary and Future Directions} 
This section explores research that introduces novel attention mechanisms or architectural modifications to improve KV cache management. Although these approaches demonstrate significant progress in enabling longer context windows and faster inference, several challenges remain. First, many methods, such as CEPE~\cite{yenLongContextLanguageModeling2024a} and XC-Cache~\cite{monteiroXCCacheCrossAttendingCached2024} demonstrate strong performance on retrieval-augmented tasks but may not generalize well across diverse workloads. This necessitates further research into task-adaptive KV cache optimization strategies that dynamically adjust caching behavior to optimize for different task demands. Secondly, integrating these novel mechanisms into existing pretrained models often requires extensive retraining, hindering their adoption in resource-constrained environments. Developing lightweight, modular approaches for retrofitting efficient KV caching into existing architectures is crucial for a wider practical impact. Finally, the robustness and stability of these new mechanisms under real-world conditions, such as noisy or dynamically changing inputs, require further investigation. Addressing these limitations could improve reliability and efficiency in practical deployments.



\subsection{Non-Transformer Architecture}\label{sec:model_nontrans}

While transformers are struggling with KV cache issues, researchers have revisited principles from traditional sequential architectures, such as recurrent neural networks (RNNs)~\cite{salehinejad2017recent}, which inherently process sequences without the need for explicit KV caches. 
Inspired by the lightweight and memory-efficient design of RNNs and efficient attention mechanisms,  non-transformer architectures~\cite{xu2024integrating, hasani2022liquid, smith2022simplified, wang2022pretraining,guMambaLinearTimeSequence2024,pengRWKVReinventingRNNs2023} have emerged, 
such as Mamba \cite{guMambaLinearTimeSequence2024} and RWKV \cite{pengRWKVReinventingRNNs2023}, offering promising alternatives. While there are a large type of new architectures, we only list methods associated with KV optimization. 
For further understanding to efficient non-transformer works, please refer to these surveys~\cite{zhou2024survey,xu2024survey,qu2024survey,patro2024mamba}.
The summary of non-transformer is listed in Tab.~\ref{tab:model_nontrans}.


\subsubsection{Adaptive Sequence Processing Architectures}\label{sec:model_nontrans_na}

RWKV \cite{pengRWKVReinventingRNNs2023}, which means Receptance Weighted Key Value, is an architecture that combines the strengths of RNNs and transformers to achieve efficient sequence processing. RWKV integrates a linear attention mechanism, enabling parallelizable training like transformers while retaining the efficient inference characteristics of RNNs. By formulating the architecture to operate as either a transformer or an RNN, RWKV achieves constant computational and memory complexity during inference, overcoming the quadratic scaling issues of transformers.

Mamba \cite{guMambaLinearTimeSequence2024} is built based on state space sequence models (SSMs) \cite{guParameterizationInitializationDiagonal2022, guCombiningRecurrentConvolutional2021}. Inspired by the state space systems, SSMs build scalable and memory-efficient long-range sequence modeling frameworks. Mamba improves SSMs by making parameters input-dependent, allowing information to be selectively propagated or forgotten along the sequence based on the current token. This addresses the inability of traditional SSMs to effectively handle the complexity of nonlinear dependencies in natural languages. Mamba omits attention and even MLP blocks, relying entirely on these selective state spaces for sequence modeling. It also develops a hardware-aware parallel algorithm for efficient recurrent computations in training and inference. Mamba achieves linear scaling in sequence length, demonstrating exceptional performance on sequences of up to a million tokens.

% \textcolor{blue}{Hawk and Griffin \cite{deGriffinMixingGated2024} Do we need to includes more works? Then there would be plenty of works related.}

RetNet \cite{sunRetentiveNetworkSuccessor2023} introduces Retentive Network that combines elements of recurrence and attention, presenting a novel retention mechanism for sequence modeling that offers training parallelism, low-cost inference, and scalable performance together. The proposed Multi-scale Retention Module (MSR) enables support to multiple computation paradigms: the parallel representation is similar to self-attention that adds support to casual masks and parallel training. The recurrent representation is similar to RNN that allows low-cost inference by maintaining state across sequence decoding. The chunkwise recurrent representation constructs a hybrid form to the former representations to further enables handling long sequences. These combined characteristics position RetNet as a strong alternative to transformers without a heavy KV cache mechanism.

MCSD \cite{yangMCSDEfficientLanguage2024} features the new block called Multi-Channel Slope and Decay, which is made up of two sections: The slope section can capture local features across short temporal spans, and the decay section can capture global features across long temporal spans. The sections are fused through element-wise operations. During inference, the process would be reformat into a recurrent representation, allowing both spatial and temporal efficiency, minimizing the need for maintaining a large KV cache.

\subsubsection{Hybrid Architecture}\label{sec:model_nontrans_ha}

With these non-transformer architecture, some methods construct mixed models to alleviate KV cache necessities while keeping some peculiarities and merits of the self-attention mechanism.

MixCon \cite{xuMixConHybridArchitecture2024} introduces a new architecture called Conba. Inspired by control theory, the Conba layer incorperates a feedback and adaptive control mechanism that can adapt to different sequence-modeling tasks and requirements dynamically with good computational efficiency. Furthermore, MixCon integrates the Mixture of Experts (MoE) module, which dynamically selects the most relevant experts to process parts of the sequence. Combining the transformer layer, the Conba layer, and the MoE module, MixCon constructs a hybrid model with good balance between attention effectiveness and computational efficiency and significantly reduces the total size of the KV cache.

GoldFinch \cite{goldsteinGoldFinchHighPerformance2024} first introduces several new architectures, including the GOLD layer, which combines the Llama and RWKV channel mixer with several improvements, and the enhanced Finch model (RWKV-6) that has significantly reduced parameters without sacrificing efficiency and performance. GoldFinch also proposes a novel mechanism called TokenCat to produce a highly compressed global key cache using the output of Finch layers. GoldFinch builds a hybrid architecture that constructs the key cache in the early layers and consumes the key cache to produce output without the traditional value cache in the top layers, providing a compact and reusable cache pipeline with linear scaling.

% LongLLaVA \cite{wangLongLLaVAScalingMultimodal2024} proposes a hybrid architecture combining Mamba and Transformer blocks to enhance the long-context capabilities of multi-modal large language models. \textcolor{red}{need reword}

RecurFormer \cite{yanRecurFormerNotAll2024} argues that not all transformer heads need to participate in the self-attention mechanism. The work recognizes that certain attention heads show recency-aware behavior which focus on local and short-range dependencies, dissipate the computation resource but gives little contribution. After identifying these heads, RecurFormer replaces them with the Mamba components, achieving straightforward KV cache reduction.

%\textcolor{blue}{There are many more hybrid models that we have not included in this survey due to our limited discussion scope.}

\subsubsection{Summary and Future Directions} 
By exploring non-transformer modules such as recurrent and hybrid designs, these methods have introduced novel paradigms that balance performance with computational efficiency, and also alleviate the KV cache issues in traditional transformer architectures. Future research should focus on several key areas. First, improving the scalability of recurrent architectures, such as RWKV~\cite{pengRWKVReinventingRNNs2023} and Mamba~\cite{guMambaLinearTimeSequence2024}, remains critical. Although these methods reduce memory and computational costs, their performance in capturing ultra-long-range dependencies lags behind transformers. Second, hybrid designs such as MixCon~\cite{xuMixConHybridArchitecture2024} and GoldFinch~\cite{goldsteinGoldFinchHighPerformance2024} highlight the potential of integrating diverse modules, yet their complexity introduces challenges in training stability and interpretability. Third, the overall generalization capabilities and robustness of non-transformer architectures, while efficient, need require further exploration for diverse input modalities.


