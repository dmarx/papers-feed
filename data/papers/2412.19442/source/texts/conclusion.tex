\section{Conclusion}
\label{sec:conclusion}
Advancements in LLMs have driven significant progress on various fields, but their high computational and memory demands during inference pose challenges, especially for long-context and real-time applications. KV cache management offers an effective solution by optimizing memory, reducing redundant computation, and improving performance.
This survey reviews KV cache management strategies across token-level, model-level, and system-level optimizations.
Token-level optimizations focus on fine-grained control of KV cache through selection, budget allocation, merging, quantization, and low-rank decomposition, enabling efficient resource allocation without altering model architectures. Model-level optimizations leverage architectural innovations, such as attention grouping and non-transformer designs, to enhance the efficiency of KV reuse. System-level optimizations further complement these efforts by employing advanced memory management, scheduling techniques, and hardware-aware designs to optimize resource utilization across diverse computing environments.



Despite the progress made, substantial opportunities remain for future exploration. Key areas include the development of real-time, task-specific budget allocation strategies, dynamic workload handling, advanced distributed coordination for KV cache in multi-node systems, and hardware-aware innovations to leverage emerging architectures like computational storage and processing-in-memory. Additionally, integrating reinforcement learning and adaptive algorithms could enable more intelligent and responsive KV cache management, further enhancing LLM efficiency across diverse deployment scenarios.

 