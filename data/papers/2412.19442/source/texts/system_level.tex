\section{System-level Optimization}
\label{sec:system-level-opt}

\begin{figure*}[h]
\centering
\tikzset{
    basic/.style  = {draw, text width=2cm, align=center, font=\sffamily, rectangle},
    root/.style   = {basic, rounded corners=2pt, thin, align=center, fill=white,text width=8cm, rotate=90, font=\footnotesize},
    dnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=yellow!30,text width=3.5cm, font=\footnotesize},
    dnode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=yellow!30,text width=2cm, font=\footnotesize},
    mnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=blue!10,text width=3.5cm, font=\footnotesize},
    mnode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=blue!10,text width=2cm, font=\footnotesize}, 
    snode/.style = {basic, thin, rounded corners=2pt, align=center, fill=npurple,text width=3.5cm, font=\footnotesize},
    snode_1/.style = {basic, thin, rounded corners=2pt, align=center, fill=npurple,text width=2cm, font=\footnotesize},
    tnode/.style = {basic, thin, align=left, fill=pink!60, text width=15em, align=center},
    xnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=blue!20,text width=5cm,},
    wnode/.style = {basic, thin, rounded corners=2pt, align=left, fill=white,text width=5.8cm, font=\footnotesize},
    wnode_1/.style = {basic, thin, rounded corners=2pt, align=left, fill=white,text width=4.5cm, font=\footnotesize},
    wnode_2/.style = {basic, thin, rounded corners=2pt, align=left, fill=white,text width=6cm, font=\footnotesize},
    %edge from parent/.style = {draw=black, edge from parent fork right}
    %edge from parent/.style = {draw=black, edge from parent fork down}
}
%
\begin{forest} 
for tree={
    grow=east,
    growth parent anchor=east,
    parent anchor=east,
    child anchor=west,
    edge path={\noexpand\path[\forestoption{edge},->, >={latex}] 
         (!u.parent anchor) -- +(5pt,0pt) |- (.child anchor)
         \forestoption{edge label};}
}
% l sep is used for arrow distance
[System-level Optimization (Sec.~\ref{sec:system-level-opt}), snode
    [Hardware-aware \\Design (Sec.~\ref{sec:sys_hd}), snode
        [SSD-based \\Design (Sec.~\ref{sec:sys_hd_ssd}), snode
            [{FlexGen~\cite{DBLP:conf/icml/0007ZYLRCLRSZ23}, InstInfer~\cite{pan2024instinferinstorageattentionoffloading}}, wnode_1]
        ]  
        [Heterogeneous \\Design (Sec.~\ref{sec:sys_hd_heter}), snode
            [{NEO~\cite{jiang2024neosavinggpumemory}, FastDecode~\cite{he2024fastdecodehighthroughputgpuefficientllm}, FlexInfer~\cite{xu2024vtensorflexiblevirtualtensor}, InfiniGen~\cite{lee2024infinigenefficientgenerativeinference}, Pensieve~\cite{DBLP:journals/corr/abs-2312-05516}, FastServe~\cite{wu2024fastdistributedinferenceserving}, PartKVRec~\cite{jiang2024efficientllminferenceioaware}}, wnode_1]
        ]  
        [I/O-based \\Design (Sec.~\ref{sec:sys_hd_io}), snode
            [{FlashAttention~\cite{DBLP:conf/nips/DaoFERR22}, Bifurcated Attention~\cite{athiwaratkun2024bifurcatedattentionacceleratingmassively}, PartKVRec~\cite{jiang2024efficientllminferenceioaware}, HCache~\cite{DBLP:journals/corr/abs-2410-05004}, Cake~\cite{jin2024computeloadkvcache}, FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching}}, wnode_1]
        ] 
        [Single/Multi-GPU Design (Sec.~\ref{sec:sys_hd_gpu}), snode
            [{HydraGen~\cite{juravsky2024hydragenhighthroughputllminference}, DeFT~\cite{yao2024deftdecodingflashtreeattention}, vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23}, ORCA~\cite{DBLP:conf/osdi/YuJKKC22}, DistServe~\cite{DBLP:conf/osdi/ZhongLCHZL0024}, Multi-Bin Batching~\cite{guldogan2024multibin}, Tree Attention~\cite{shyam2024treeattentiontopologyawaredecoding}}, wnode_1]
        ] 
    ]
    [Scheduling (Sec.~\ref{sec:sys_sch}), snode
        [Layer-specific and Hierarchical Scheduling (Sec.~\ref{sec:sys_sch_lhs}), snode
            [{LayerKV~\cite{xiong2024layerkvoptimizinglargelanguage}, CachedAttention~\cite{gao2024costefficientlargelanguagemodel}, ALISA~\cite{zhao2024alisaacceleratinglargelanguage}, LAMPS~\cite{shahout2024fastinferenceaugmentedlarge}}, wnode_1]
        ]  
        [Preemptive and Fairness-oriented Scheduling (Sec.~\ref{sec:sys_sch_pfs}), snode
            [{FastServe~\cite{wu2024fastdistributedinferenceserving}, FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching}}, wnode_1]
        ]  
        [Prefix-aware Scheduling (Sec.~\ref{sec:sys_sch_ps}), snode
            [{BatchLLM~\cite{zheng2024batchllmoptimizinglargebatched}, RadixAttention~\cite{zheng2024sglangefficientexecutionstructured}}, wnode_1]
        ] 
    ]
    [Memory \\Management (Sec.~\ref{sec:sys_mm}), snode
        [Prefix-aware Design (Sec.~\ref{sec:sys_mm_pd}), snode
            [{ChunkAttention~\cite{ye2024chunkattentionefficientselfattentionprefixaware}, MemServe~\cite{hu2024memservecontextcachingdisaggregated}}, wnode_1]
        ]  
        [Architectural Design (Sec.~\ref{sec:sys_mm_ad}), snode
            [{vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23}, vTensor~\cite{xu2024vtensorflexiblevirtualtensor}, LeanKV~\cite{zhang2024unifyingkvcachecompression}}, wnode_1]
        ] 
    ]
]
\end{forest}

\caption{Taxonomy of the System-level Optimization for KV Cache Management.}
\label{fig:sys_framework}
\end{figure*}


% [Added] 1.COMPUTE OR LOAD KV CACHE? WHY NOT BOTH\\
% [Exists] Fast State Restoration in LLM Serving with HCa\\

Recent system-level optimizations for KV cache in LLM inference can be broadly categorized into three main directions: memory management (Sec.~\ref{sec:sys_mm}), scheduling strategies (Sec.~\ref{sec:sys_sch}), and hardware-aware designs (Sec.~\ref{sec:sys_hd}). These complementary approaches collectively demonstrate the rich design space for system-level optimizations in LLM inference, each addressing different aspects of the performance, efficiency, and resource utilization challenges. The Taxonomy of the system-level optimization is   in Fig.~\ref{fig:sys_framework}.

\subsection{Memory Management}\label{sec:sys_mm}

Recent advances in KV cache memory management for large language model (LLM) inference reveal three distinct approaches aimed at enhancing memory efficiency. Architectural designs, exemplified by vLLM with PagedAttention~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23} and vTensor~\cite{xu2024vtensorflexiblevirtualtensor}, adapt classical operating system principles to create flexible, dynamic memory allocation systems that optimize the use of physical memory through sophisticated mapping and virtual memory abstractions. Prefix-aware designs like ChunkAttention~\cite{ye2024chunkattentionefficientselfattentionprefixaware} and MemServe~\cite{hu2024memservecontextcachingdisaggregated} further refine this approach by organizing data structures to enable efficient cache deduplication and sharing of common prefixes, thereby improving both memory utilization and computational efficiency. Together, these innovations illustrate the potential for significant enhancements in LLM serving via memory management.

\subsubsection{Architectural Design}\label{sec:sys_mm_ad} 

The first category focuses on architectural innovations in memory management, led by vLLM with PagedAttention~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23}, which adapts OS-inspired paging concepts by partitioning KV caches into fixed-size blocks with non-contiguous storage. PagedAttention partitions KV caches into fixed-size blocks that can be stored non-contiguously in physical memory, while vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23} implements a virtual memory-like system that manages these blocks through a sophisticated mapping mechanism. This architecture separates logical and physical KV blocks, enabling dynamic memory allocation and flexible block management through block tables that track mapping relationships and fill states. This memory management approach enables efficient memory utilization both within and across requests, demonstrating how classical OS memory management principles can be effectively adapted for LLM inference optimization.

This approach is further enhanced by vTensor~\cite{xu2024vtensorflexiblevirtualtensor}, which introduces a virtual memory abstraction that decouples computation from defragmentation through three key components: the vTensor Scheduler which generates memory management policies based on meta information, the vTensor Operation which translates these policies into CUDA VMM operations, and the vTensor Pool which maintains virtual tensor mappings. VTS processes instructions and creates policies based on memory state tracking, while VTO executes these policies through asynchronous GPU operations. VTP completes the cycle by managing virtual tensor storage and updating meta information for subsequent memory operations.

LeanKV~\cite{zhang2024unifyingkvcachecompression} combines unified paging with heterogeneous quantization and dynamic sparsity mechanisms. It implements Hetero-KV quantization to store keys and values at different precisions, complemented by a per-head dynamic sparsity mechanism that adapts memory allocation based on token importance across different attention heads and requests. To efficiently execute these strategies, LeanKV~\cite{zhang2024unifyingkvcachecompression} introduces an advanced on-GPU memory management system featuring three key components: unified paging for flexible memory organization, a circular free page list for efficient coordination, and a bidirectional page table for minimal metadata overhead. 

\subsubsection{Prefix-aware Design}\label{sec:sys_mm_pd} 

Some latest works emphasize optimizing data organization structures through prefix-aware designs. ChunkAttention~\cite{ye2024chunkattentionefficientselfattentionprefixaware} restructures KV cache management by organizing chunks within a prefix tree structure, enabling runtime detection and sharing of common prefixes. It breaks down traditional monolithic KV cache tensors into smaller, manageable chunks organized within a prefix tree structure, enabling efficient runtime detection and sharing of common prefixes across multiple requests. This architectural design brings two significant memory management benefits: efficient KV cache deduplication through prefix tree-based organization, and improved data locality through a two-phase partition algorithm for self-attention computation. By enabling dynamic identification and sharing of common prompt prefixes across multiple requests, ChunkAttention~\cite{ye2024chunkattentionefficientselfattentionprefixaware} optimizes both memory utilization and computational efficiency, demonstrating how intelligent chunking and prefix-aware cache management can significantly enhance LLM serving efficiency.

MemServe~\cite{hu2024memservecontextcachingdisaggregated} extends this concept to distributed settings with its MemPool system, which orchestrates both CPU DRAM and GPU HBM resources across serving instances, managing active and historical KV caches through a comprehensive set of distributed memory pool APIs. It presents a prompt token-based indexing layer for historical KV cache retrieval, cross-instance data exchange mechanisms that abstract away hardware heterogeneity, and a global scheduler implementing a prompt tree-based locality-aware policy for enhanced cache reuse, collectively resulting in significant improvements in job completion time and time-to-first-token performance.

% {\color{red}\subsubsection{Layer-aware Sharing Strategy}\label{sec:sys_mm_ls}
% }

% {\color{red}
% The third category focuses on layer-specific optimizations and innovative sharing strategies. PrefixKV~\cite{wang2024prefixkvadaptiveprefixkv} reframes the challenge of determining KV cache sizes into a global optimization problem. 
% It systematically searches for the optimal global prefix configuration across all layers. 
% Through an adaptive layer-wise KV retention strategy based on binary search, PrefixKV determines the optimal KV cache size for each layer independently. 
% This approach ensures maximum preservation of contextual information at each layer, thereby enhancing the quality of model generation.

% Rather than following the intuitive path of sharing similar cache contents, KVSharer~\cite{yang2024kvsharerefficientinferencelayerwise} strategically shares dissimilar KV caches across different layers, resulting in both reduced memory footprint and improved prefill performance. This counterintuitive approach proves particularly effective as it maintains model performance while achieving significant memory savings. The system's plug-and-play design also enables seamless integration with existing intra-layer KV cache compression techniques~\cite{DBLP:conf/nips/Zhang00CZC0TRBW23,yang2024pyramidinferpyramidkvcache}, creating a complementary memory optimization strategy that enhances overall memory efficiency without requiring model retraining.
% }


These approaches often complement each other, suggesting potential benefits in combining multiple strategies. For instance, LeanKV~\cite{zhang2024unifyingkvcachecompression}'s integration of compression with page-based management and MemServe~\cite{hu2024memservecontextcachingdisaggregated}'s combination of distributed memory management with prefix-aware caching demonstrate the effectiveness of hybrid approaches. The diversity of these solutions reflects both the complexity of KV cache management and the rich opportunity space for continued innovation in optimizing LLM inference systems. Tab.\ref{tab:memory_management} provides a comparison of various memory management techniques for KV Cache, highlighting key features such as paged memory, virtual memory, dynamic sparsity, prefix sharing, and distributed memory.


\subsubsection{Summary and Future Directions} 
The exploration of memory management strategies for KV caches in large language model inference reveals a promising landscape of innovations that enhance memory efficiency and overall system performance. Architectural advancements, such as those seen in vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23} and LeanKV~\cite{zhang2024unifyingkvcachecompression}, adapt traditional memory management principles for modern AI applications by incorporating paging and virtual memory concepts for dynamic allocation. Prefix-aware designs like ChunkAttention~\cite{ye2024chunkattentionefficientselfattentionprefixaware} and MemServe~\cite{hu2024memservecontextcachingdisaggregated} optimize data organization, enabling the detection and sharing of common prefixes, which reduces redundancy and speeds up inference. 
% Meanwhile, layer-aware strategies exemplified by PrefixKV and KVSharer challenge conventional approaches by optimizing cache sizes and strategically sharing dissimilar caches across layers. 

Future work should advance memory management innovations through multiple synergistic directions: investigating adaptive memory hierarchies that dynamically adjust to workload patterns and resource constraints, exploring novel compression techniques that preserve quick access while reducing memory footprint, developing intelligent prefetching mechanisms that anticipate and preload frequently accessed cache entries, researching hardware-aware optimization strategies that leverage emerging memory technologies like computational storage and processing-in-memory units, and designing distributed cache coherence protocols that efficiently maintain consistency across multiple inference nodes. Additionally, the exploration of machine learning-based approaches could enable predictive memory allocation that learns from historical access patterns, while the investigation of specialized data structures could yield more efficient prefix detection and sharing mechanisms. These advancements, combined with research into heterogeneous memory systems that intelligently coordinate different memory types based on access patterns and performance requirements, would significantly enhance the scalability and efficiency of LLM inference systems across diverse deployment scenarios.

\begin{table}[t]
    \centering
    \caption{Comparison of Memory Management Techniques for KV Cache Optimization.}
    \label{tab:memory_management}
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{2.3pt} % 减小列间距
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Method} & 
        \makecell{\textbf{Paged} \\ \textbf{Memory}} & 
        \makecell{\textbf{Virtual} \\ \textbf{Memory}} & 
        \makecell{\textbf{Dynamic} \\ \textbf{Sparsity}} & 
        \makecell{\textbf{Prefix} \\ \textbf{Sharing}} & 
        \makecell{\textbf{Distributed} \\ \textbf{Memory}} \\ 
        \midrule
        vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23}      & \checkmark & \checkmark &            &                     &                                   \\
        vTensor~\cite{xu2024vtensorflexiblevirtualtensor} &            & \checkmark &            &                     &                                 \\
        LeanKV~\cite{zhang2024unifyingkvcachecompression} & \checkmark &            & \checkmark &                     &                                  \\
        \makecell{ChunkAtt- \\ ention~\cite{ye2024chunkattentionefficientselfattentionprefixaware}} &            &            &            & \checkmark            &                                  \\
        MemServe~\cite{hu2024memservecontextcachingdisaggregated} &            &            &            & \checkmark            & \checkmark                       \\
        % PrefixKV~\cite{wang2024prefixkvadaptiveprefixkv} &            &            &            &                     &                       & \checkmark \\
        % KVSharer~\cite{yang2024kvsharerefficientinferencelayerwise} &            &            &            &                     &                       & \checkmark \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Scheduling}\label{sec:sys_sch}

Based on these scheduling-oriented works, we can categorize KV cache scheduling optimizations into three main approaches: 1) prefix-aware scheduling strategies, represented by BatchLLM~\cite{zheng2024batchllmoptimizinglargebatched} and RadixAttention~\cite{zheng2024sglangefficientexecutionstructured}; 2) preemptive and fairness-oriented scheduling, exemplified by FastServe~\cite{wu2024fastdistributedinferenceserving} and FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching}; 3) layer-specific and hierarchical scheduling approaches, demonstrated by LayerKV~\cite{xiong2024layerkvoptimizinglargelanguage}, CachedAttention~\cite{gao2024costefficientlargelanguagemodel}, and ALISA~\cite{zhao2024alisaacceleratinglargelanguage}. These approaches collectively address different aspects of scheduling optimization, from memory efficiency to fairness and latency reduction, while specialized solutions like LAMPS~\cite{shahout2024fastinferenceaugmentedlarge} extend these concepts to specific use cases such as API-augmented LLM requests, demonstrating the rich design space in KV cache scheduling optimization.

\begin{table*}[ht]
    \centering
    \caption{Comparison of Scheduling Approaches for KV Cache Optimization.}
    \label{tab:scheduling_comparison}
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{3pt} % 减小列间距
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Method} & 
        \makecell{\textbf{Prefix-aware}} & 
        \makecell{\textbf{Preemptive}} & 
        \makecell{\textbf{Fairness-oriented}} & 
        \makecell{\textbf{Layer-specific}} & 
        \makecell{\textbf{Hierarchical}} & 
        \makecell{\textbf{Dynamic}} \\ 
        \midrule
        BatchLLM~\cite{zheng2024batchllmoptimizinglargebatched}       & \checkmark &            &            &            &            &            \\
        RadixAttention~\cite{zheng2024sglangefficientexecutionstructured} & \checkmark &            &            &            &            & \checkmark \\
        FastServe~\cite{wu2024fastdistributedinferenceserving}       &            & \checkmark & \checkmark &            &            &            \\
        FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching} &            & \checkmark & \checkmark &            &            &            \\
        LayerKV~\cite{xiong2024layerkvoptimizinglargelanguage}       &            &            &            & \checkmark &            &            \\
        CachedAttention~\cite{gao2024costefficientlargelanguagemodel} &            &            &            & \checkmark & \checkmark &            \\
        ALISA~\cite{zhao2024alisaacceleratinglargelanguage}          &            &            &            & \checkmark &            & \checkmark \\
        LAMPS~\cite{shahout2024fastinferenceaugmentedlarge}          &            &            &            &            & \checkmark & \checkmark \\
        \bottomrule
    \end{tabular}
\end{table*}

\subsubsection{Prefix-aware Scheduling}\label{sec:sys_sch_ps}

Unlike traditional LRU-based cache management systems where shared KV contexts might be prematurely evicted or unnecessarily extended in memory, BatchLLM~\cite{zheng2024batchllmoptimizinglargebatched} implements explicit global prefix identification and coordinated scheduling of requests sharing common KV cache content. It schedules requests at the granularity of prefix-sharing groups, ensuring optimal KV cache reuse while minimizing cache lifetime - requests with identical prefixes are deliberately scheduled together to maximize KV cache sharing efficiency. This scheduling approach is complemented by a dynamic programming algorithm that optimizes first-level prefix patterns, enabling more efficient KV cache management and reducing scheduling overhead. 

RadixAttention~\cite{zheng2024sglangefficientexecutionstructured} builds around a radix tree structure, replacing traditional FCFS scheduling with an intelligent cache-aware approach that prioritizes requests based on matched prefix lengths. It implements dynamic memory management where cached tokens and running requests share the same memory pool, controlled by an LRU eviction policy that strategically removes leaf nodes while preserving valuable ancestor prefixes. This is complemented by a reference counting mechanism that prevents eviction of actively used cache entries during continuous batching while enabling efficient memory reclamation when nodes become unused. 


\subsubsection{Preemptive and Fairness-oriented scheduling}\label{sec:sys_sch_pfs} FastServe~\cite{wu2024fastdistributedinferenceserving} implements a proactive KV cache management strategy that coordinates cache movement between GPU and host memory, overlapping data transmission with computation to minimize latency impact. This is integrated with a skip-join Multi-Level Feedback Queue scheduler that makes KV cache scheduling decisions based on input length information, allowing jobs to enter appropriate priority queues directly while avoiding unnecessary demotions through higher-priority queues. By combining token-level preemption with sophisticated KV cache management and intelligent queue placement, FastServe~\cite{wu2024fastdistributedinferenceserving} achieves significant performance improvements over traditional run-to-completion systems like vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23}.

FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching} introduces a fairness-oriented KV cache scheduling system that addresses the overhead challenges of preemptive scheduling in LLM serving. There are three key mechanisms: enhancing I/O utilization through intelligent cache movement scheduling, minimizing GPU idle time during context switches, and eliminating redundant I/O operations in multi-turn conversations. Unlike traditional block-based KV cache memory policies that prioritize memory efficiency at the cost of fragmentation and granularity limitations, FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching} implements a balanced approach that maintains efficient memory usage while facilitating smoother context switching. This integrated scheduling approach enables dynamic priority adjustments for fairness while minimizing the performance impact of context switches.

\subsubsection{Layer-specific and Hierarchical Scheduling}\label{sec:sys_sch_lhs}

LayerKV~\cite{xiong2024layerkvoptimizinglargelanguage} introduces a novel layer-wise KV cache scheduling approach to address the growing TTFT (Time to First Token) latency challenges in large-context LLM serving. The contribution lies in its fine-grained, layer-specific KV cache block allocation and management strategy, which departs from traditional monolithic cache management approaches. By implementing layer-wise KV block scheduling and offloading mechanisms, LayerKV~\cite{xiong2024layerkvoptimizinglargelanguage} enables more efficient memory utilization and reduces queuing delays that typically occur when large context windows compete for limited GPU KV cache blocks. It is complemented by an SLO-aware scheduler that optimizes cache allocation decisions based on service level objectives, allowing for dynamic management of memory resources across model layers. 


CachedAttention~\cite{gao2024costefficientlargelanguagemodel} introduces a hierarchical scheduling approach consisting of three-tier strategies: layer-wise pre-loading coordinates KV cache movement across storage hierarchies using scheduler-aware fetching and eviction policies, asynchronous saving overlaps I/O operations with GPU computation, and intelligent cache placement decisions are made based on scheduler hints to ensure frequently accessed KV caches reside in faster memory tiers. It also presents a novel positional encoding decoupling mechanism that prevents KV cache invalidation during context window overflow through effective truncation strategies. 



ALISA~\cite{zhao2024alisaacceleratinglargelanguage} introduces a dual-level KV cache scheduling framework that combines algorithmic sparsity with system-level optimization. At the algorithm level, the Sparse Window Attention mechanism identifies and prioritizes the most important tokens for attention computation, creating a mixture of global dynamic and local static sparse patterns that significantly reduces KV cache memory requirements. At the system-level, its three-phase token-level dynamic scheduler that manages KV tensor allocation and optimizes the trade-off between caching and recomputation. The scheduler makes dynamic decisions about which tokens to cache in GPU memory versus recompute, based on their importance and system resource constraints. 

LAMPS~\cite{shahout2024fastinferenceaugmentedlarge} implements a predictive scheduling mechanism that estimates both pre-API outputs and optimal memory handling strategies during API calls, choosing between preserving, discarding, or swapping KV cache content based on predicted memory waste.


% NEO~\cite{jiang2024neosavinggpumemory}




\subsubsection{Summary and Future Directions} 
Tab.\ref{tab:scheduling_comparison} compares scheduling approaches for KV cache optimization based on their support for prefix-awareness, preemptive scheduling, fairness, layer-specific optimizations, hierarchical structures, and dynamic adaptability.
The advancements in scheduling strategies for KV cache management in large language model inference highlight a multifaceted approach to optimizing performance, memory efficiency, and fairness. By categorizing these strategies into prefix-aware, preemptive and fairness-oriented, and layer-specific scheduling, we see diverse methodologies addressing different challenges. For instance, prefix-aware strategies like BatchLLM~\cite{zheng2024batchllmoptimizinglargebatched} and RadixAttention~\cite{zheng2024sglangefficientexecutionstructured} enhance cache reuse by intelligently grouping requests based on shared prefixes, minimizing cache lifetime and reducing overhead. Meanwhile, preemptive approaches such as FastServe~\cite{wu2024fastdistributedinferenceserving} and FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching} implement proactive management techniques that optimize cache movement and scheduling, significantly improving latency and ensuring fairness during context switching. Layer-specific scheduling methods like LayerKV~\cite{xiong2024layerkvoptimizinglargelanguage}, CachedAttention~\cite{gao2024costefficientlargelanguagemodel}, and ALISA~\cite{zhao2024alisaacceleratinglargelanguage} further refine cache allocation by implementing fine-grained management strategies tailored to the unique demands of different model layers. 

Future work should advance these KV cache scheduling innovations through several interlinked dimensions: developing adaptive hybrid systems that dynamically select optimal scheduling strategies based on real-time workload characteristics, exploring predictive models that anticipate user request patterns to proactively optimize cache allocation, investigating automated parameter tuning mechanisms that adjust scheduling policies across different deployment scenarios, designing context-aware architectures that intelligently balance prefix sharing with fairness requirements, and researching novel cache coherence protocols that efficiently handle distributed inference scenarios. Additionally, the integration of reinforcement learning approaches could enable self-optimizing schedulers that learn from historical usage patterns, while the exploration of hardware-software co-design could yield specialized accelerators that directly support efficient KV cache management operations. These advancements would collectively enhance the robustness, efficiency, and adaptability of LLM inference systems across diverse operational conditions and deployment scales.
Finally, considering LLM serving~\cite{yao2024cacheblend}, different scheduling and sharing for multiple users and queries may lead to potential privacy leaks. Therefore, privacy protection techniques for LLM serving in multi-user scenarios, such as differential privacy~\cite{zhao2022survey,dong2021residual,dong2023continual
}, are worth further investigation.

\begin{table*}[ht]
    \centering
    \caption{Comparison of Hardware-aware Design Approaches for KV Cache Optimization.}
    \label{tab:hardware_design_comparison}
    \renewcommand{\arraystretch}{1.3} % 调整行间距
    \setlength{\tabcolsep}{3pt} % 减小列间距
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Method} & 
        \makecell{\textbf{Single/Multi-GPU}} & 
        \makecell{\textbf{I/O-aware}} & 
        \makecell{\textbf{Heterogeneous}} & 
        \makecell{\textbf{SSD-based}} \\ 
        \midrule
        Bifurcated Attention~\cite{athiwaratkun2024bifurcatedattentionacceleratingmassively} &            & \checkmark &            &            \\
        Cake~\cite{jin2024computeloadkvcache}                              &            &            &            & \checkmark \\
        DeFT~\cite{yao2024deftdecodingflashtreeattention}                   & \checkmark &            &            &            \\
        DistServe~\cite{DBLP:conf/osdi/ZhongLCHZL0024}                     &            &            & \checkmark &            \\
        FastDecode~\cite{he2024fastdecodehighthroughputgpuefficientllm}    &            & \checkmark &            &            \\
        FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching}     & \checkmark &            &            &            \\
        FlexGen~\cite{DBLP:conf/icml/0007ZYLRCLRSZ23}                      &            & \checkmark &            &            \\
        FlexInfer~\cite{xu2024vtensorflexiblevirtualtensor}                &            &            &            & \checkmark \\
        FlashAttention~\cite{DBLP:conf/nips/DaoFERR22}                     & \checkmark &            & \checkmark &            \\
        HCache~\cite{DBLP:journals/corr/abs-2410-05004}                    &            &            & \checkmark &            \\
        HydraGen~\cite{juravsky2024hydragenhighthroughputllminference}       & \checkmark &            &            &            \\
        InfiniGen~\cite{lee2024infinigenefficientgenerativeinference}      &            &            & \checkmark &            \\
        InstInfer~\cite{pan2024instinferinstorageattentionoffloading}      &            &            &            &            \\
        Multi-Bin Batching~\cite{guldogan2024multibin}                     &            &            &            & \checkmark \\
        NEO~\cite{jiang2024neosavinggpumemory}                             &            &            & \checkmark &            \\
        ORCA~\cite{DBLP:conf/osdi/YuJKKC22}                                & \checkmark &            &            &            \\
        PartKVRec~\cite{jiang2024efficientllminferenceioaware}             &            & \checkmark &            &            \\
        Pensieve~\cite{DBLP:journals/corr/abs-2312-05516}                  &            & \checkmark &            &            \\
        Tree Attention~\cite{shyam2024treeattentiontopologyawaredecoding}  &            & \checkmark &            &            \\
        vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23}                          & \checkmark &            &            &            \\
        \bottomrule
    \end{tabular}
\end{table*}





\subsection{Hardware-aware Design}\label{sec:sys_hd}

Recent hardware-aware optimizations for KV cache management span several key directions based on different hardware architectures and constraints. Single/Multi-GPU designs focus on optimizing memory access patterns, GPU kernel designs for efficient attention computation, and parallel processing with load balancing. IO-based designs optimize data movement across memory hierarchies through asynchronous I/O and intelligent prefetching mechanisms. Heterogeneous designs orchestrate computation and memory allocation across CPU-GPU tiers. SSD-based solutions have evolved from basic offloading approaches to more sophisticated designs, with InstInfer leveraging computational storage drives (CSDs) to perform in-storage attention computation, effectively bypassing PCIe bandwidth limitations. These approaches demonstrate how hardware-aware designs can significantly improve LLM inference efficiency by carefully considering and exploiting the characteristics of different hardware components and their interconnections.

\subsubsection{Single/Multi-GPU Design}\label{sec:sys_hd_gpu}
Based on these works focusing on GPU-oriented designs, we can categorize the approaches into several key strategies for KV cache optimization. First, shared prefix optimization approaches like HydraGen~\cite{juravsky2024hydragenhighthroughputllminference} and DeFT~\cite{yao2024deftdecodingflashtreeattention} focus on efficient GPU memory utilization through batched prefix computations and tree-structured attention patterns. 
Rather than maintaining separate KV caches for each sequence with identical prefixes, HydraGen~\cite{juravsky2024hydragenhighthroughputllminference} decomposes attention computation to leverage a single shared KV cache for common prefixes across multiple requests. It enables efficient GPU memory utilization through two mechanisms: batched prefix KV cache access across sequences and separate handling of unique suffix KV caches. 
For DeFT~\cite{yao2024deftdecodingflashtreeattention}, its core contributions are twofold: KV-Guided Grouping, which optimizes GPU memory access patterns by intelligently managing shared prefix KV caches to minimize redundant global-to-shared memory transfers, and Flattened Tree KV Splitting, which ensures balanced workload distribution across GPU compute units while minimizing computational redundancy. 


Second, distributed processing frameworks exemplified by vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23} and ORCA~\cite{DBLP:conf/osdi/YuJKKC22} optimize multi-GPU scenarios through sophisticated memory management and synchronization mechanisms. 
vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23} also implements a KV cache manager that coordinates memory allocation across distributed GPU workers in model-parallel deployments, where each GPU handles a subset of attention heads while sharing the same logical-to-physical block mapping. This GPU-aware design enables efficient memory utilization through near-zero fragmentation and flexible KV cache sharing, while supporting Megatron-LM style tensor parallelism where GPUs execute in SPMD fashion with synchronized block-wise matrix operations. The scheduler broadcasts control messages containing input tokens and block tables to GPU workers, allowing them to independently process their assigned attention heads while maintaining memory coherence through all-reduce operations, effectively eliminating redundant memory management synchronization overhead and maximizing GPU utilization across distributed resources.

ORCA~\cite{DBLP:conf/osdi/YuJKKC22} distributes model layers across GPUs using both intra-layer and inter-layer parallelism, where each worker process manages multiple GPU-controlling threads and coordinates KV cache access through an Attention KV manager. ORCA's GPU-aware design minimizes CPU-GPU synchronization overhead by separating control message communication from tensor data transfer (via NCCL), allowing each GPU thread to efficiently access KV cache memory using request IDs and token indices. 

Third, phase-aware designs like DistServe~\cite{DBLP:conf/osdi/ZhongLCHZL0024} separate prefill and decoding phases across GPU resources to optimize their distinct memory access patterns. Novel batching strategies are represented by Multi-Bin Batching~\cite{guldogan2024multibin}, which focuses on length-aware request grouping for improved GPU utilization, while advanced parallel computation frameworks like Tree Attention~\cite{shyam2024treeattentiontopologyawaredecoding} introduce sophisticated reduction algorithms for efficient attention computation across multiple GPUs. 
DistServe~\cite{DBLP:conf/osdi/ZhongLCHZL0024} recognizes that prefill and decoding phases have distinct KV cache utilization characteristics and memory access patterns: prefill requires intensive computation with growing KV cache sizes for processing input tokens, while decoding maintains a fixed KV cache size for generating output tokens. By physically separating these phases onto different GPUs, DistServe enables optimized GPU memory management and KV cache access patterns specific to each phase, eliminating interference between prefill's bursty memory access patterns and decoding's steady-state KV cache utilization. 
Multi-Bin Batching~\cite{guldogan2024multibin} introduces a length-aware batching strategy helps minimize GPU idle time and memory fragmentation that typically occurs when processing requests of varying lengths in the same batch, as it ensures that the KV cache memory allocated for each batch is utilized more uniformly across all requests.
Tree Attention~\cite{shyam2024treeattentiontopologyawaredecoding} implements a tree-based reduction algorithm that fundamentally changes how attention values are computed and aggregated across GPUs, enabling more efficient handling of KV cache data through partial reductions that significantly reduce memory bandwidth requirements and peak memory usage. 

These approaches can collectively demonstrate how hardware-aware designs can significantly improve the LLM  efficiency by carefully considering GPU architecture characteristics and memory hierarchy constraints.

\subsubsection{I/O-based Design}\label{sec:sys_hd_io}

Recent I/O-focused optimizations for KV cache management span several key dimensions, targeting different levels of the memory hierarchy. At the GPU level, approaches like FlashAttention~\cite{DBLP:conf/nips/DaoFERR22} and Bifurcated Attention~\cite{athiwaratkun2024bifurcatedattentionacceleratingmassively} optimize data movement between HBM and SRAM through sophisticated tiling strategies and split attention computations, while CPU-GPU data movement optimizations are addressed by systems like PartKVRec~\cite{jiang2024efficientllminferenceioaware}, which tackles PCIe bandwidth bottlenecks through hybrid recomputation and transfer strategies, and HCache~\cite{DBLP:journals/corr/abs-2410-05004}, which optimizes intermediate activation storage and restoration. 

FlashAttention~\cite{DBLP:conf/nips/DaoFERR22} employs a tiling strategy that carefully manages KV cache access patterns, reducing redundant memory operations by keeping frequently accessed portions of the KV cache in fast SRAM while systematically fetching and evicting data blocks to minimize HBM accesses. 
Bifurcated Attention~\cite{athiwaratkun2024bifurcatedattentionacceleratingmassively} presents an I/O-aware approach to optimize KV cache access patterns during shared-context batch decoding by strategically splitting attention computations into two distinct GEMM operations. It specifically targets the memory bandwidth bottleneck in high-batch scenarios with long contexts by minimizing repeated KV cache accesses, maintaining the same computational FLOPs while drastically reducing memory I/O operations. 
For PartKVRec~\cite{jiang2024efficientllminferenceioaware}, its key innovation lies in its hybrid strategy of partial KV cache recomputation on the GPU while simultaneously transferring the remaining cache data from CPU memory, effectively hiding PCIe transfer latency. The implementation employs a sophisticated I/O-aware scheduling system that analyzes input characteristics and hardware capabilities to determine the optimal balance between recomputation and data transfer, dynamically managing KV cache movement to maximize PCIe bandwidth utilization while minimizing GPU idle time. 
HCache~\cite{DBLP:journals/corr/abs-2410-05004} strategically stores and restores intermediate activations instead of complete KV cache states, implementing a bubble-free restoration scheduler that carefully balances computation and I/O operations to maximize bandwidth utilization. A key innovation is its chunk-based storage manager that addresses the I/O pattern mismatch between saving (layer-before-token) and restoration (token-before-layer) operations, optimizing data layout and access patterns to reduce I/O overhead.
Cake~\cite{jin2024computeloadkvcache} addresses the fundamental I/O bottleneck in loading cached KV states from disk to GPU memory. It introduces a bidirectional parallelized strategy that simultaneously leverages both computational and I/O resources. This hybrid approach dynamically balances between loading cached KV states from storage and computing them on GPUs, adapting automatically to varying system conditions without manual parameter tuning. 

Context management optimizations are exemplified by FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching}, which implements efficient context switching mechanisms for multi-user scenarios through granular memory management policies. 
FastSwitch~\cite{shen2024fastswitchoptimizingcontextswitching} addresses I/O inefficiencies in traditional block-based KV cache approaches by implementing a more granular and continuous memory management policy that minimizes I/O overhead during preemption and context switching. 

These approaches demonstrate how careful consideration of I/O patterns and memory hierarchy characteristics can significantly improve LLM inference efficiency by minimizing data movement and maximizing bandwidth utilization across different storage tiers.

% Deja Vu~\cite{DBLP:conf/icml/LiuWDZY0S0TRC23}

\subsubsection{Heterogeneous Design}\label{sec:sys_hd_heter}

Recent heterogeneous computing approaches for KV Cache demonstrate diverse strategies for optimizing CPU-GPU collaboration. Systems like NEO~\cite{jiang2024neosavinggpumemory} and FastDecode~\cite{he2024fastdecodehighthroughputgpuefficientllm} implement strategic workload distribution through CPU offloading of attention computations, while FlexInfer~\cite{xu2024vtensorflexiblevirtualtensor} introduces virtual memory abstractions for optimal resource coordination. 

NEO~\cite{jiang2024neosavinggpumemory} advances heterogeneous computing for LLM inference by implementing strategic CPU offloading of attention computations and KV cache states. Through asymmetric GPU-CPU pipelining and load-aware scheduling, it optimally balances workloads across both computing platforms, enabling larger GPU batch sizes without latency penalties. 
For FastDecode~\cite{he2024fastdecodehighthroughputgpuefficientllm}, its key contribution lies in its strategic offloading of memory-bound KV cache operations to distributed CPU resources, leveraging the aggregate memory capacity and computing power of multiple CPU nodes rather than treating CPUs as mere storage devices. By utilizing CPUs for KV cache computations and storage while keeping compute-intensive operations on GPUs, it creates an efficient pipeline that maximizes resource utilization across the heterogeneous infrastructure, enabling larger batch sizes and higher throughput.
FlexInfer~\cite{xu2024vtensorflexiblevirtualtensor} orchestrates CPU-GPU resource utilization for LLM inference by introducing the virtual memory-based abstraction vTensor. 


Advanced caching and prefetching mechanisms are exemplified by InfiniGen~\cite{lee2024infinigenefficientgenerativeinference}, which employs speculative prefetching for KV cache entries, and Pensieve~\cite{DBLP:journals/corr/abs-2312-05516}, which implements multi-tier caching for conversation states. 
For InfiniGen~\cite{lee2024infinigenefficientgenerativeinference}, its key innovation lies in its prediction mechanism that operates across the heterogeneous architecture, using partial computation of attention inputs and modified query-key weights to identify and prefetch only the most relevant KV cache entries from CPU memory to GPU. 
Pensieve~\cite{DBLP:journals/corr/abs-2312-05516} introduces a heterogeneous computing architecture specifically designed for multi-turn conversation LLM serving by implementing a sophisticated multi-tier caching strategy across GPU and CPU resources. This stateful approach manages KV cache data across the heterogeneous memory hierarchy, maintaining conversation history states across multiple hardware tiers rather than recomputing them for each interaction. 

Sophisticated scheduling and preemption strategies are demonstrated by FastServe~\cite{wu2024fastdistributedinferenceserving}, which focuses on token-level preemption and proactive memory management, and PartKVRec~\cite{jiang2024efficientllminferenceioaware}, which balances data transfer and recomputation through dynamic scheduling. 
For FastServe~\cite{wu2024fastdistributedinferenceserving}, its token-level preemption capability is supported by a sophisticated heterogeneous memory management system that proactively coordinates KV cache data movement between GPU and host memory. It implements a skip-join Multi-Level Feedback Queue scheduler that manages computational resources across the CPU-GPU boundary, optimizing both computation scheduling and data movement. 
PartKVRec~\cite{jiang2024efficientllminferenceioaware} employs a scheduler that dynamically optimizes the distribution of tasks across the heterogeneous hardware platform, using a profiler to analyze both hardware capabilities and workload characteristics. 

These approaches collectively showcase how heterogeneous architectures can be effectively leveraged to overcome single-device limitations while maintaining efficient resource utilization and minimizing communication overhead between CPU and GPU resources.

\subsubsection{Solid-state Disk (SSD)-based Design}\label{sec:sys_hd_ssd}

Recent SSD-based approaches for KV cache management demonstrate an evolution in storage utilization strategies, from traditional extension of the memory hierarchy to computational storage innovations. 
FlexGen~\cite{DBLP:conf/icml/0007ZYLRCLRSZ23} introduces an SSD-based approach to KV cache management that extends the memory hierarchy across GPU, CPU memory, and disk storage, optimizing high-throughput LLM inference on resource-constrained hardware through intelligent tensor storage and access pattern optimization determined by linear programming. The system's key innovations include coordinated data placement across all three storage tiers, optimized access patterns to minimize SSD latency impact, aggressive 4-bit compression for both model weights and attention cache, and efficient utilization of SSD storage as a memory hierarchy extension for KV cache management. 
InstInfer~\cite{pan2024instinferinstorageattentionoffloading} introduces a more revolutionary approach by leveraging computational storage drives (CSDs) to perform attention computations directly within the storage layer, transforming SSDs from passive storage devices into active computational units and utilizing the high internal bandwidth of flash memory channels to bypass traditional PCIe bandwidth limitations. 

These approaches demonstrate how storage devices can be effectively integrated into LLM inference systems, either as memory hierarchy extensions or as computational resources, to enable efficient processing of large models and long sequences in resource-constrained environments.
Tab.\ref{tab:hardware_design_comparison} compares hardware-aware design approaches for KV cache optimization across four key features: Single/Multi-GPU support, I/O-awareness, heterogeneous computing, and SSD-based design.

\subsubsection{Summary and Future Directions} 
Recent advancements in hardware-aware designs for KV cache management emphasize optimizing performance based on specific hardware architectures and constraints, demonstrating significant enhancements in large language model inference efficiency. Approaches like HydraGen~\cite{juravsky2024hydragenhighthroughputllminference} and vLLM~\cite{DBLP:conf/sosp/KwonLZ0ZY0ZS23} in single and multi-GPU designs focus on efficient memory access patterns and load balancing, while I/O-based strategies such as FlashAttention~\cite{DBLP:conf/nips/DaoFERR22} and PartKVRec~\cite{jiang2024efficientllminferenceioaware} tackle data movement bottlenecks through intelligent prefetching and scheduling mechanisms. Additionally, heterogeneous designs exemplified by NEO~\cite{jiang2024neosavinggpumemory} and FastDecode~\cite{he2024fastdecodehighthroughputgpuefficientllm} effectively leverage CPU-GPU collaboration to maximize resource utilization. 

Future work should advance this research through multiple interconnected directions: exploring novel architectural designs that combine specialized hardware accelerators with optimized memory hierarchies, investigating hybrid systems that leverage computational storage drives and processing-in-memory capabilities, developing self-adaptive algorithms that dynamically optimize resource allocation based on workload patterns, researching advanced compression techniques that maintain model fidelity while reducing memory requirements, and designing intelligent scheduling mechanisms that efficiently coordinate heterogeneous computing resources including CPUs, GPUs, and custom accelerators. These improvements, working in concert, would enhance both the performance and scalability of LLM inference systems across diverse deployment scenarios, from edge devices to data centers, while maintaining adaptability to emerging hardware innovations and varying computational demands.

