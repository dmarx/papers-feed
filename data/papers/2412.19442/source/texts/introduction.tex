\section{Introduction}
Large Language Models (LLMs)~\cite{hadi2023survey,zhu2023survey}, trained on massive corpora, have revolutionized various domains such as natural language processing~\cite{DBLP:journals/corr/abs-2307-06435, DBLP:journals/csur/MinRSVNSAHR24,xu2024large
}, computer vision~\cite{DBLP:conf/nips/LiuLWL23a,DBLP:journals/pami/ZhangHJL24,DBLP:journals/corr/abs-2306-16410}, and multi-modal~\cite{DBLP:conf/acl/ZhangY0L0C024,DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24, DBLP:conf/bigdataconf/WuGCWY23} tasks. 
Their ability to understand context and perform logical reasoning has enabled remarkable success in various fields, such as 
time series analysis~\cite{jin2023large,ma2024survey}, 
recommendation~\cite{tan2023user,wu2024survey},
autonomous driving~\cite{yang2023llm4drive,chen2024driving, fu2024drive},
and
healthcare~\cite{qiu2023large,zhou2023survey1}.
These breakthroughs are powered by state-of-the-art architectures and training paradigms, enabling models to achieve unparalleled performance across diverse tasks.
Prominent LLMs, such as GPT~\cite{brown2020language,radford2018improving,radford2019language},
 LLaMA~\cite{DBLP:journals/corr/abs-2302-13971, dubey2024llama},  DeepSeek~\cite{DBLP:conf/acl/DaiDZXGCLZYWXLH24,deepseek-aiDeepSeekV2StrongEconomical2024,DBLP:journals/corr/abs-2403-05525}, 
Mistral~\cite{DBLP:journals/corr/abs-2401-04088,DBLP:journals/corr/abs-2310-06825}, and GLM~\cite{DBLP:conf/iclr/ZengLDWL0YXZXTM23,DBLP:conf/acl/DuQLDQY022}, 
are built on the foundational transformer architecture~\cite{vaswani2017attention}, which excels at capturing long-range dependencies in sequential data. However, despite their powerful capabilities, the computational and memory demands of LLMs, particularly during inference, present significant challenges when scaling them to real-world, long-context, and real-time applications.

A critical bottleneck in LLM inference lies in the efficient management of Key-Value (KV) pairs. 
Recently, caching techniques~\cite{gracioli2015survey,podlipnig2003survey} 
have been extensively employed to store previously computed intermediate results, allowing their reuse in subsequent inference steps to accelerate the model, such as graph neural networks~\cite{DBLP:conf/cikm/LiC21, DBLP:journals/pacmmod/LiSCY23,lin2020pagraph}.
Fortunately, the auto-regressive generation mechanism inherent to LLMs presents an opportunity to leverage KV caching for efficient text generation. 
Specifically,
auto-regressive generation enables LLMs to produce text token by token, with each token conditioned on all previously generated ones. While this approach is highly effective for generating coherent and contextually relevant outputs, it suffers from poor scalability with long input sequences, as the computational and memory requirements grow quadratically with sequence length. The KV cache addresses this issue by storing key and value matrices from previous decoding steps, enabling their reuse and significantly reducing redundant computations. 



Several recent surveys~\cite{zhu2023survey,zhuang2023survey,park2024comprehensive,wang2024model,ding2023efficiency,miao2023towards,wan2023efficient,zhou2024survey,tang2024survey,kachris2024survey,xu2023parameter,albalak2024survey,Awesome-LLM-Inference@2024} have explored the domain of efficient LLMs. These surveys primarily examine various aspects of LLM efficiency, presenting valuable insights while leaving room for further refinement and innovation.
In particular, many of these works
primarily focus on holistic approaches to improving LLM efficiency, examining a wide range of techniques across multiple dimensions, such as  span data-level optimizations (e.g., prompt engineering), model architecture-level optimizations (e.g., efficient transformer designs), and system-level optimizations (e.g., task scheduling).
For instance, Ding et al.\cite{ding2023efficiency} explore efficiency techniques that integrate data-level and model architecture perspectives, while Miao et al.\cite{miao2023towards} examine efficient LLM inference from a comprehensive system-level perspective. Similarly, Tang et al.\cite{tang2024survey}, Wan et al.\cite{wan2023efficient}, and Xu et al.~\cite{xu2023parameter} provide analyses that encompass data, model, and system-level optimizations, reflecting holistic approaches to LLM acceleration.

On the other hand, some surveys focus on more specialized aspects for LLM acceleration.
For example,
Zhu et al.\cite{zhu2023survey}, Park et al.\cite{park2024comprehensive}, Wang et al.\cite{wang2024model}, and Tang et al.\cite{tang2024survey} focus on model compression as a key aspect of model-level optimization. Similarly, Kachris et al.\cite{kachris2024survey} examine hardware acceleration strategies tailored for LLMs, while Xu et al.\cite{xu2023parameter} investigate parameter-efficient tuning approaches. Albalak et al.\cite{albalak2024survey} discuss data selection strategies to enhance the efficiency of LLM training, and Xia et al.\cite{xia2024unlocking} highlight collaborative techniques, such as speculative decoding~\cite{leviathan2023fast,kim2024speculative}, to accelerate model inference.
Li et al.~\cite{li2024prompt} focus on prompt compression.
Similar to our work, Shi et al.\cite{shi2024keep}, Li et al.\cite{li2024scbench}, and Yuan et al.~\cite{yuan2024kv} also explore the use of KV caches to accelerate LLMs. 
However, our survey is both complementary and more comprehensive, offering a detailed taxonomy of KV cache management for text-based and multi-modal LLMs.
We categorize techniques across token-level, model-level, and system-level perspectives and include benchmarks for both text and multi-modal scenarios.
In particular, complementing existing KV cache surveys, we provide a detailed comparison of the differences and advantages of existing models at the token-level, model-level, and system-level.







%\citet{yuan2024llm}

Specifically,
this survey provides a comprehensive overview of the current state of KV cache management and its role in accelerating LLM inference. We begin by introducing the transformer architecture and the role of the KV cache in enabling efficient auto-regressive text generation. We then analyze the challenges associated with KV cache management, including its impact on computational complexity, memory usage, and real-time performance. Following this, we present a taxonomy of existing optimization techniques, categorizing them into token-level, model-level, and system-level optimization approaches. 
Additionally, we discuss datasets and evaluation metrics used to benchmark these techniques and provide insights into their effectiveness across various tasks and applications.
 
