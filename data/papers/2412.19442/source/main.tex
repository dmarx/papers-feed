\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{enumitem}
\usepackage{bm}

\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{enumerate}

\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{makecell}
\usepackage{tikz}
\usepackage{forest}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{array}

\usetikzlibrary{trees,positioning,shapes,shadows,arrows.meta}

\ifCLASSOPTIONcompsoc
 \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
 \usepackage[caption=false,font=footnotesize]{subfig}
\fi

\usepackage{url}

\usepackage{booktabs} % for table
\usepackage{multirow} % for table
\usepackage{xspace}
\usepackage{xcolor}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\revise}[1]{\textcolor{blue}{#1}}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{hyperref}
%\hypersetup{hypertex=true,
%            colorlinks=true,
%            linkcolor=black,
%            anchorcolor=black,
%            citecolor=black}

\newenvironment{packed_itemize}{
	\begin{itemize}[leftmargin=*]
		\setlength{\itemsep}{3pt}
		\setlength{\parskip}{-0.5pt}
		\setlength{\parsep}{-8pt}
	}{\end{itemize}}

% \definecolor{nblue}{RGB}{161,186,216}
% \definecolor{nyellow}{RGB}{239,239,154}
% \definecolor{ngreen}{RGB}{233,175,180}

\definecolor{ngreen}{HTML}{D5E8D4}
\definecolor{nblue}{HTML}{DAE8FC}
\definecolor{npurple}{HTML}{E1D5E7}
\definecolor{nyellow}{RGB}{255, 255, 200} % A light and shallow yellow


\begin{document}

\title{A Survey on Large Language Model Acceleration based on KV Cache Management}

 


\author{Haoyang Li,
 Yiming Li,
 Anxin Tian,
 Tianhao Tang,
 Zhanchao Xu,
 Xuejia Chen,
 Nicole Hu,\\
 Wei Dong,
 Qing Li~\IEEEmembership{Fellow, IEEE},
 Lei Chen ~\IEEEmembership{Fellow, IEEE},
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem Haoyang Li, Qing Li,  Department of Computing, The Hong Kong Polytechnic University, China. \\
E-mail: haoyang-comp.li@polyu.edu.hk, qing-prof.li@polyu.edu.hk.
\IEEEcompsocthanksitem Yiming Li, Anxin Tian, Tianhao Tang, Lei Chen, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, China.\\
E-mail: yliix@connect.ust.hk, atian@connect.ust.hk, ttangae@cse.ust.hk, leichen@cse.ust.hk.
\IEEEcompsocthanksitem Zhanchao Xu, Xuejia Chen, Department of Computer Science and Technology, Huazhong University of Science and Technology. \\
E-mail: xuzhanchaomail@163.com, gresham15437@gmail.com.
\IEEEcompsocthanksitem Nicole Hu, The Chinese University of Hong Kong. 
\IEEEcompsocthanksitem Wei Dong, Department of Computing and Data Science,
Nanyang Technological University.
E-mail: wei\_dong@ntu.edu.sg.
\IEEEcompsocthanksitem Corresponding authors: Yiming Li, Anxin Tian. \\
E-mail: yliix@connect.ust.hk, atian@connect.ust.hk.
}}
  

 

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version





\maketitle

\begin{abstract}
Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. 
Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. 
Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications.
The curated paper list for KV cache management is  in:   \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.
\end{abstract}

% \newpage
% \tableofcontents
% \newpage

\input{texts/introduction}
\input{texts/preliminary}
\input{texts/taxonomy}
\input{texts/token_level}
\input{texts/model_level}
\input{texts/system_level}
\input{texts/datasets}
%\input{texts/discussion}
\input{texts/conclusion}

%\section*{Acknowledgements}


%\bibliography{refs/intro,refs/Cache4LLM,KV_Cache/refs/system-level}
\bibliography{refs/Cache4LLM,refs/DatasetLevel,refs/MultimodalBenchmark}

\bibliographystyle{IEEEtran}

% \appendix
% \section{Appendix}


\end{document}
