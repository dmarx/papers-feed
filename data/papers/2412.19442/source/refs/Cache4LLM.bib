@article{yuan2024llm,
  title={Llm inference unveiled: Survey and roofline model insights},
  author={Yuan, Zhihang and Shang, Yuzhang and Zhou, Yang and Dong, Zhen and Zhou, Zhe and Xue, Chenhao and Wu, Bingzhe and Li, Zhikai and Gu, Qingyi and Lee, Yong Jae and others},
  journal={arXiv preprint arXiv:2402.16363},
  year={2024}
}


@article{zhu2023survey,
  title={A survey on model compression for large language models},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={arXiv preprint arXiv:2308.07633},
  year={2023}
}

@article{park2024comprehensive,
  title={A Comprehensive Survey of Compression Algorithms for Language Models},
  author={Park, Seungcheol and Choi, Jaehyeon and Lee, Sojin and Kang, U},
  journal={arXiv preprint arXiv:2401.15347},
  year={2024}
}

@article{wang2024model,
  title={Model Compression and Efficient Inference for Large Language Models: A Survey},
  author={Wang, Wenxiao and Chen, Wei and Luo, Yicong and Long, Yongliu and Lin, Zhengkai and Zhang, Liye and Lin, Binbin and Cai, Deng and He, Xiaofei},
  journal={arXiv preprint arXiv:2402.09748},
  year={2024}
}

@article{ding2023efficiency,
  title={The efficiency spectrum of large language models: An algorithmic survey},
  author={Ding, Tianyu and Chen, Tianyi and Zhu, Haidong and Jiang, Jiachen and Zhong, Yiqi and Zhou, Jinxin and Wang, Guangzhi and Zhu, Zhihui and Zharkov, Ilya and Liang, Luming},
  journal={arXiv preprint arXiv:2312.00678},
  year={2023}
}

@article{miao2023towards,
  title={Towards efficient generative large language model serving: A survey from algorithms to systems},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Jin, Hongyi and Chen, Tianqi and Jia, Zhihao},
  journal={arXiv preprint arXiv:2312.15234},
  year={2023}
}

@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and Chowdhury, Mosharaf and others},
  journal={arXiv preprint arXiv:2312.03863},
  volume={1},
  year={2023}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

 

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{yang2023baichuan,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}

@inproceedings{li2023long,
  title={How Long Can Context Length of Open-Source LLMs truly Promise?},
  author={Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
  booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year={2023}
}

@article{workshop2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}

@article{du2021glm,
  title={Glm: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10360},
  year={2021}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{yang2023harnessing,
  title={Harnessing the power of llms in practice: A survey on chatgpt and beyond},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  journal={ACM Transactions on Knowledge Discovery from Data},
  year={2023},
  publisher={ACM New York, NY}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

 

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}



@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@phdthesis{wang2022enabling,
  title={Enabling High Performance in Large-Scale Block Storage Systems through New Caching and Garbage Collection Algorithms},
  author={Wang, Qiuping},
  year={2022},
  school={The Chinese University of Hong Kong (Hong Kong)}
}

@article{podlipnig2003survey,
  title={A survey of web cache replacement strategies},
  author={Podlipnig, Stefan and B{\"o}sz{\"o}rmenyi, Laszlo},
  journal={ACM Computing Surveys (CSUR)},
  volume={35},
  number={4},
  pages={374--398},
  year={2003},
  publisher={ACM New York, NY, USA}
}
@article{gracioli2015survey,
  title={A survey on cache management mechanisms for real-time embedded systems},
  author={Gracioli, Giovani and Alhammad, Ahmed and Mancuso, Renato and Fr{\"o}hlich, Ant{\^o}nio Augusto and Pellizzoni, Rodolfo},
  journal={ACM Computing Surveys (CSUR)},
  volume={48},
  number={2},
  pages={1--36},
  year={2015},
  publisher={ACM New York, NY, USA}
}
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
@article{patro2024mamba,
  title={Mamba-360: Survey of state space models as transformer alternative for long sequence modelling: Methods, applications, and challenges},
  author={Patro, Badri Narayana and Agneeswaran, Vijay Srinivas},
  journal={arXiv preprint arXiv:2404.16112},
  year={2024}
}
@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}

@inproceedings{kwasniewska2019deep,
  title={Deep learning optimization for edge devices: Analysis of training quantization parameters},
  author={Kwasniewska, Alicja and Szankin, Maciej and Ozga, Mateusz and Wolfe, Jason and Das, Arun and Zajac, Adam and Ruminski, Jacek and Rad, Paul},
  booktitle={IECON 2019-45th Annual Conference of the IEEE Industrial Electronics Society},
  volume={1},
  pages={96--101},
  year={2019},
  organization={IEEE}
}

@inproceedings{zhou2018adaptive,
  title={Adaptive quantization for deep neural network},
  author={Zhou, Yiren and Moosavi-Dezfooli, Seyed-Mohsen and Cheung, Ngai-Man and Frossard, Pascal},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bondarenko2023quantizable,
  title={Quantizable transformers: Removing outliers by helping attention heads do nothing},
  author={Bondarenko, Yelysei and Nagel, Markus and Blankevoort, Tijmen},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={75067--75096},
  year={2023}
}



@article{jiang2018linear,
  title={A linear speedup analysis of distributed deep learning with sparse and quantized communication},
  author={Jiang, Peng and Agrawal, Gagan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{lin2016fixed,
  title={Fixed point quantization of deep convolutional networks},
  author={Lin, Darryl and Talathi, Sachin and Annapureddy, Sreekanth},
  booktitle={International conference on machine learning},
  pages={2849--2858},
  year={2016},
  organization={PMLR}
}



@article{tang2024survey,
  title={A Survey on Transformer Compression},
  author={Tang, Yehui and Wang, Yunhe and Guo, Jianyuan and Tu, Zhijun and Han, Kai and Hu, Hailin and Tao, Dacheng},
  journal={arXiv preprint arXiv:2402.05964},
  year={2024}
}

@article{golden2024flash,
  title={Is Flash Attention Stable?},
  author={Golden, Alicia and Hsia, Samuel and Sun, Fei and Acun, Bilge and Hosmer, Basil and Lee, Yejin and DeVito, Zachary and Johnson, Jeff and Wei, Gu-Yeon and Brooks, David and others},
  journal={arXiv preprint arXiv:2405.02803},
  year={2024}
}
@inproceedings{chen2024driving,
  title={Driving with llms: Fusing object-level vector modality for explainable autonomous driving},
  author={Chen, Long and Sinavski, Oleg and H{\"u}nermann, Jan and Karnsund, Alice and Willmott, Andrew James and Birch, Danny and Maund, Daniel and Shotton, Jamie},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={14093--14100},
  year={2024},
  organization={IEEE}
}
@inproceedings{fu2024drive,
  title={Drive like a human: Rethinking autonomous driving with large language models},
  author={Fu, Daocheng and Li, Xin and Wen, Licheng and Dou, Min and Cai, Pinlong and Shi, Botian and Qiao, Yu},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={910--919},
  year={2024}
}

@inproceedings{yang2023llm4drive,
  title={Llm4drive: A survey of large language models for autonomous driving},
  author={Yang, Zhenjie and Jia, Xiaosong and Li, Hongyang and Yan, Junchi},
  booktitle={NeurIPS 2024 Workshop on Open-World Agents},
  year={2023}
}




 

@misc{golden2024flashattentionstable,
      title={Is Flash Attention Stable?}, 
      author={Alicia Golden and Samuel Hsia and Fei Sun and Bilge Acun and Basil Hosmer and Yejin Lee and Zachary DeVito and Jeff Johnson and Gu-Yeon Wei and David Brooks and Carole-Jean Wu},
      year={2024},
      eprint={2405.02803},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.02803}, 
}

#KV quant
@article{hooper2024kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@article{liu2024kivi,
  title={Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}
@article{liu2024intactkv,
  title={IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact},
  author={Liu, Ruikang and Bai, Haoli and Lin, Haokun and Li, Yuening and Gao, Han and Xu, Zhengzhuo and Hou, Lu and Yao, Jun and Yuan, Chun},
  journal={arXiv preprint arXiv:2403.01241},
  year={2024}
}

%%%%%%%%% START OF Yiming's Part
@article{DBLP:journals/corr/abs-2404-15574,
  author       = {Wenhao Wu and
                  Yizhong Wang and
                  Guangxuan Xiao and
                  Hao Peng and
                  Yao Fu},
  title        = {Retrieval Head Mechanistically Explains Long-Context Factuality},
  journal      = {CoRR},
  volume       = {abs/2404.15574},
  year         = {2024},
}

@article{DBLP:journals/corr/abs-2407-15891,
  author       = {Hanlin Tang and
                  Yang Lin and
                  Jing Lin and
                  Qingsen Han and
                  Shikuan Hong and
                  Yiwu Yao and
                  Gongyi Wang},
  title        = {RazorAttention: Efficient {KV} Cache Compression Through Retrieval
                  Heads},
  journal      = {CoRR},
  volume       = {abs/2407.15891},
  year         = {2024},
}

@article{DBLP:journals/corr/abs-2410-10819,
  author       = {Guangxuan Xiao and
                  Jiaming Tang and
                  Jingwei Zuo and
                  Junxian Guo and
                  Shang Yang and
                  Haotian Tang and
                  Yao Fu and
                  Song Han},
  title        = {DuoAttention: Efficient Long-Context {LLM} Inference with Retrieval
                  and Streaming Heads},
  journal      = {CoRR},
  volume       = {abs/2410.10819},
  year         = {2024},
}

@article{DBLP:journals/corr/abs-2410-19258,
  author       = {Yu Fu and
                  Zefan Cai and
                  Abedelkadir Asi and
                  Wayne Xiong and
                  Yue Dong and
                  Wen Xiao},
  title        = {Not All Heads Matter: {A} Head-Level {KV} Cache Compression Method
                  with Integrated Retrieval and Reasoning},
  journal      = {CoRR},
  volume       = {abs/2410.19258},
  year         = {2024},
}

@article{DBLP:journals/corr/abs-2406-02069,
  author       = {Zefan Cai and
                  Yichi Zhang and
                  Bofei Gao and
                  Yuliang Liu and
                  Tianyu Liu and
                  Keming Lu and
                  Wayne Xiong and
                  Yue Dong and
                  Baobao Chang and
                  Junjie Hu and
                  Wen Xiao},
  title        = {PyramidKV: Dynamic {KV} Cache Compression based on Pyramidal Information
                  Funneling},
  journal      = {CoRR},
  volume       = {abs/2406.02069},
  year         = {2024},
}

@inproceedings{DBLP:conf/acl/YangHGHZ024,
  author       = {Dongjie Yang and
                  Xiaodong Han and
                  Yan Gao and
                  Yao Hu and
                  Shilin Zhang and
                  Hai Zhao},
  title        = {PyramidInfer: Pyramid {KV} Cache Compression for High-throughput {LLM}
                  Inference},
  booktitle    = {ACL},
  pages        = {3258--3270},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
}

@inproceedings{anonymous2024cake,
  title={{CAKE}: Cascading and Adaptive {KV} Cache Eviction with Layer Preferences},
  author={Anonymous},
  booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=EQgEMAD4kv},
  note={under review}
}

@inproceedings{
  anonymous2024dynamickv,
  title={Dynamic{KV}: Task-Aware Adaptive {KV} Cache Compression for Long Context {LLM}s},
  author={Anonymous},
  booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=uHkfU4TaPh},
  note={under review}
}

@article{DBLP:journals/corr/abs-2410-12876,
  author       = {Zihao Zeng and
                  Bokai Lin and
                  Tianqi Hou and
                  Hao Zhang and
                  Zhijie Deng},
  title        = {In-context KV-Cache Eviction for LLMs via Attention-Gate},
  journal      = {CoRR},
  volume       = {abs/2410.12876},
  year         = {2024},
}

@article{DBLP:journals/corr/abs-2407-11550,
  author       = {Yuan Feng and
                  Junlin Lv and
                  Yukun Cao and
                  Xike Xie and
                  S. Kevin Zhou},
  title        = {Ada-KV: Optimizing {KV} Cache Eviction by Adaptive Budget Allocation
                  for Efficient {LLM} Inference},
  journal      = {CoRR},
  volume       = {abs/2407.11550},
  year         = {2024},
}

@inproceedings{
  anonymous2024identify,
  title={Identify Critical {KV} Cache in {LLM} Inference from an Output Perturbation Perspective},
  author={Anonymous},
  booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=lRTDMGYCpy},
  note={under review}
}

@misc{li2024xkvpersonalizedkvcache,
      title={XKV: Personalized KV Cache Memory Reduction for Long-Context LLM Inference}, 
      author={Weizhuo Li and Zhigang Wang and Yu Gu and Ge Yu},
      year={2024},
      eprint={2412.05896},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.05896}, 
}

@article{DBLP:journals/corr/abs-2410-13846,
  author       = {Xuan Zhang and
                  Cunxiao Du and
                  Chao Du and
                  Tianyu Pang and
                  Wei Gao and
                  Min Lin},
  title        = {SimLayerKV: {A} Simple Framework for Layer-Level {KV} Cache Reduction},
  journal      = {CoRR},
  volume       = {abs/2410.13846},
  year         = {2024},
}


@misc{hooper2024squeezedattentionacceleratinglong,
      title={Squeezed Attention: Accelerating Long Context Length LLM Inference}, 
      author={Coleman Hooper and Sehoon Kim and Hiva Mohammadzadeh and Monishwaran Maheswaran and June Paik and Michael W. Mahoney and Kurt Keutzer and Amir Gholami},
      year={2024},
      eprint={2411.09688},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.09688}, 
}

@inproceedings{DBLP:conf/icml/TangZZXKH24,
  author       = {Jiaming Tang and
                  Yilong Zhao and
                  Kan Zhu and
                  Guangxuan Xiao and
                  Baris Kasikci and
                  Song Han},
  title        = {{QUEST:} Query-Aware Sparsity for Efficient Long-Context {LLM} Inference},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
}

@article{DBLP:journals/corr/abs-2407-09450,
  author       = {Zafeirios Fountas and
                  Martin Benfeghoul and
                  Adnan Oomerjee and
                  Fenia Christopoulou and
                  Gerasimos Lampouras and
                  Haitham Bou{-}Ammar and
                  Jun Wang},
  title        = {Human-like Episodic Memory for Infinite Context LLMs},
  journal      = {CoRR},
  volume       = {abs/2407.09450},
  year         = {2024},
}


@article{DBLP:journals/corr/abs-2408-03675,
  author       = {Yilong Chen and
                  Guoxia Wang and
                  Junyuan Shang and
                  Shiyao Cui and
                  Zhenyu Zhang and
                  Tingwen Liu and
                  Shuohuan Wang and
                  Yu Sun and
                  Dianhai Yu and
                  Hua Wu},
  title        = {{NACL:} {A} General and Effective {KV} Cache Eviction Framework for
                  LLMs at Inference Time},
  journal      = {CoRR},
  volume       = {abs/2408.03675},
  year         = {2024},
}

@misc{xiao2024infllmtrainingfreelongcontextextrapolation,
      title={InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory}, 
      author={Chaojun Xiao and Pengle Zhang and Xu Han and Guangxuan Xiao and Yankai Lin and Zhengyan Zhang and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.04617},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04617}, 
}

@inproceedings{DBLP:conf/naacl/HanWPX0JW24,
  author       = {Chi Han and
                  Qifan Wang and
                  Hao Peng and
                  Wenhan Xiong and
                  Yu Chen and
                  Heng Ji and
                  Sinong Wang},
  title        = {LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language
                  Models},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21,
                  2024},
  pages        = {3991--4008},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
}

@inproceedings{DBLP:conf/iclr/Ge0LZ0024,
  author       = {Suyu Ge and
                  Yunan Zhang and
                  Liyuan Liu and
                  Minjia Zhang and
                  Jiawei Han and
                  Jianfeng Gao},
  title        = {Model Tells You What to Discard: Adaptive {KV} Cache Compression for
                  LLMs},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
}

@article{DBLP:journals/corr/abs-2410-16179,
  author       = {Zhuoming Chen and
                  Ranajoy Sadhukhan and
                  Zihao Ye and
                  Yang Zhou and
                  Jianyu Zhang and
                  Niklas Nolte and
                  Yuandong Tian and
                  Matthijs Douze and
                  L{\'{e}}on Bottou and
                  Zhihao Jia and
                  Beidi Chen},
  title        = {MagicPIG: {LSH} Sampling for Efficient {LLM} Generation},
  journal      = {CoRR},
  volume       = {abs/2410.16179},
  year         = {2024},
}

@article{DBLP:journals/tacl/LiuLHPBPL24,
  author       = {Nelson F. Liu and
                  Kevin Lin and
                  John Hewitt and
                  Ashwin Paranjape and
                  Michele Bevilacqua and
                  Fabio Petroni and
                  Percy Liang},
  title        = {Lost in the Middle: How Language Models Use Long Contexts},
  journal      = {Trans. Assoc. Comput. Linguistics},
  volume       = {12},
  pages        = {157--173},
  year         = {2024},
}

@inproceedings{DBLP:conf/icml/RibarCHBLO24,
  author       = {Luka Ribar and
                  Ivan Chelombiev and
                  Luke Hudlass{-}Galley and
                  Charlie Blake and
                  Carlo Luschi and
                  Douglas Orr},
  title        = {SparQ Attention: Bandwidth-Efficient {LLM} Inference},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@inproceedings{DBLP:conf/mlsys/0002TTYCWXDG024,
  author       = {Ji Lin and
                  Jiaming Tang and
                  Haotian Tang and
                  Shang Yang and
                  Wei{-}Ming Chen and
                  Wei{-}Chen Wang and
                  Guangxuan Xiao and
                  Xingyu Dang and
                  Chuang Gan and
                  Song Han},
  editor       = {Phillip B. Gibbons and
                  Gennady Pekhimenko and
                  Christopher De Sa},
  title        = {{AWQ:} Activation-aware Weight Quantization for On-Device {LLM} Compression
                  and Acceleration},
  booktitle    = {Proceedings of the Seventh Annual Conference on Machine Learning and
                  Systems, MLSys 2024, Santa Clara, CA, USA, May 13-16, 2024},
  publisher    = {mlsys.org},
  year         = {2024},
  url          = {https://proceedings.mlsys.org/paper\_files/paper/2024/hash/42a452cbafa9dd64e9ba4aa95cc1ef21-Abstract-Conference.html},
  timestamp    = {Fri, 02 Aug 2024 15:28:19 +0200},
  biburl       = {https://dblp.org/rec/conf/mlsys/0002TTYCWXDG024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{DBLP:journals/corr/abs-2405-04532,
  author       = {Yujun Lin and
                  Haotian Tang and
                  Shang Yang and
                  Zhekai Zhang and
                  Guangxuan Xiao and
                  Chuang Gan and
                  Song Han},
  title        = {QServe: {W4A8KV4} Quantization and System Co-design for Efficient
                  {LLM} Serving},
  journal      = {CoRR},
  volume       = {abs/2405.04532},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.04532},
  doi          = {10.48550/ARXIV.2405.04532},
  eprinttype    = {arXiv},
  eprint       = {2405.04532},
  timestamp    = {Thu, 13 Jun 2024 21:49:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-04532.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/XiLCZ23,
  author       = {Haocheng Xi and
                  Changhao Li and
                  Jianfei Chen and
                  Jun Zhu},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Training Transformers with 4-bit Integers},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/99fc8bc48b917c301a80cb74d91c0c06-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/XiLCZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{wei2023outlier,
  title={Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling},
  author={Wei, Xiuying and Zhang, Yunchen and Li, Yuhang and Zhang, Xiangguo and Gong, Ruihao and Guo, Jinyang and Liu, Xianglong},
  journal={arXiv preprint arXiv:2304.09145},
  year={2023}
}

@article{shao2023omniquant,
  title={Omniquant: Omnidirectionally calibrated quantization for large language models},
  author={Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2308.13137},
  year={2023}
}

@article{DBLP:journals/corr/abs-2409-10516,
  author       = {Di Liu and
                  Meng Chen and
                  Baotong Lu and
                  Huiqiang Jiang and
                  Zhenhua Han and
                  Qianxi Zhang and
                  Qi Chen and
                  Chengruidong Zhang and
                  Bailu Ding and
                  Kai Zhang and
                  Chen Chen and
                  Fan Yang and
                  Yuqing Yang and
                  Lili Qiu},
  title        = {RetrievalAttention: Accelerating Long-Context {LLM} Inference via
                  Vector Retrieval},
  journal      = {CoRR},
  volume       = {abs/2409.10516},
  year         = {2024},
}

@inproceedings{DBLP:conf/nips/LiuDLWXXKS23,
  author       = {Zichang Liu and
                  Aditya Desai and
                  Fangshuo Liao and
                  Weitao Wang and
                  Victor Xie and
                  Zhaozhuo Xu and
                  Anastasios Kyrillidis and
                  Anshumali Shrivastava},
  title        = {Scissorhands: Exploiting the Persistence of Importance Hypothesis
                  for {LLM} {KV} Cache Compression at Test Time},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
}


@article{DBLP:journals/corr/abs-2407-21018,
  author       = {Yuhui Xu and
                  Zhanming Jie and
                  Hanze Dong and
                  Lei Wang and
                  Xudong Lu and
                  Aojun Zhou and
                  Amrita Saha and
                  Caiming Xiong and
                  Doyen Sahoo},
  title        = {ThinK: Thinner Key Cache by Query-Driven Pruning},
  journal      = {CoRR},
  volume       = {abs/2407.21018},
  year         = {2024},
}

@misc{wu2024tokenselectefficientlongcontextinference,
      title={TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection}, 
      author={Wei Wu and Zhuoshi Pan and Chao Wang and Liyi Chen and Yunchu Bai and Kun Fu and Zheng Wang and Hui Xiong},
      year={2024},
      eprint={2411.02886},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02886}, 
}

@misc{xu2024recycledattentionefficientinference,
      title={Recycled Attention: Efficient inference for long-context language models}, 
      author={Fangyuan Xu and Tanya Goyal and Eunsol Choi},
      year={2024},
      eprint={2411.05787},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.05787}, 
}

@inproceedings{DBLP:conf/mlsys/AdnanAJNSK24,
  author       = {Muhammad Adnan and
                  Akhil Arunkumar and
                  Gaurav Jain and
                  Prashant J. Nair and
                  Ilya Soloveychik and
                  Purushotham Kamath},
  title        = {Keyformer: {KV} Cache reduction through key tokens selection for Efficient
                  Generative Inference},
  booktitle    = {Proceedings of the Seventh Annual Conference on Machine Learning and
                  Systems, MLSys 2024, Santa Clara, CA, USA, May 13-16, 2024},
  publisher    = {mlsys.org},
  year         = {2024},
}


%%%%%%%%% START OF SYSTEM LEVEL

@inproceedings{DBLP:conf/nips/DaoFERR22,
  author       = {Tri Dao and
                  Daniel Y. Fu and
                  Stefano Ermon and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:29 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/DaoFERR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/icml/0007ZYLRCLRSZ23,
  author       = {Ying Sheng and
                  Lianmin Zheng and
                  Binhang Yuan and
                  Zhuohan Li and
                  Max Ryabinin and
                  Beidi Chen and
                  Percy Liang and
                  Christopher R{\'{e}} and
                  Ion Stoica and
                  Ce Zhang},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {FlexGen: High-Throughput Generative Inference of Large Language Models
                  with a Single {GPU}},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {31094--31116},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/sheng23a.html},
  timestamp    = {Tue, 20 Feb 2024 13:40:11 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/0007ZYLRCLRSZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{zhao2024atom,
  title={Atom: Low-bit quantization for efficient and accurate llm serving},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={196--209},
  year={2024}
}
@article{yue2024wkvquant,
  title={Wkvquant: Quantizing weight and key/value cache for large language models gains more},
  author={Yue, Yuxuan and Yuan, Zhihang and Duanmu, Haojie and Zhou, Sifan and Wu, Jianlong and Nie, Liqiang},
  journal={arXiv preprint arXiv:2402.12065},
  year={2024}
}
@inproceedings{DBLP:conf/icml/LiuWDZY0S0TRC23,
  author       = {Zichang Liu and
                  Jue Wang and
                  Tri Dao and
                  Tianyi Zhou and
                  Binhang Yuan and
                  Zhao Song and
                  Anshumali Shrivastava and
                  Ce Zhang and
                  Yuandong Tian and
                  Christopher R{\'{e}} and
                  Beidi Chen},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {22137--22176},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/liu23am.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/LiuWDZY0S0TRC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/osdi/YuJKKC22,
  author       = {Gyeong{-}In Yu and
                  Joo Seong Jeong and
                  Geon{-}Woo Kim and
                  Soojeong Kim and
                  Byung{-}Gon Chun},
  editor       = {Marcos K. Aguilera and
                  Hakim Weatherspoon},
  title        = {Orca: {A} Distributed Serving System for Transformer-Based Generative
                  Models},
  booktitle    = {16th {USENIX} Symposium on Operating Systems Design and Implementation,
                  {OSDI} 2022, Carlsbad, CA, USA, July 11-13, 2022},
  pages        = {521--538},
  publisher    = {{USENIX} Association},
  year         = {2022},
  url          = {https://www.usenix.org/conference/osdi22/presentation/yu},
  timestamp    = {Tue, 11 Oct 2022 16:51:12 +0200},
  biburl       = {https://dblp.org/rec/conf/osdi/YuJKKC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{duanmu2024skvq,
  title={SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models},
  author={Duanmu, Haojie and Yuan, Zhihang and Li, Xiuhong and Duan, Jiangfei and Zhang, Xingcheng and Lin, Dahua},
  journal={arXiv preprint arXiv:2405.06219},
  year={2024}
}

@article{he2024zipvl,
  title={Zipvl: Efficient large vision-language models with dynamic token sparsification and kv cache compression},
  author={He, Yefei and Chen, Feng and Liu, Jing and Shao, Wenqi and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2410.08584},
  year={2024}
}
@article{tao2024asymkv,
  title={AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asymmetric Quantization Configurations},
  author={Tao, Qian and Yu, Wenyuan and Zhou, Jingren},
  journal={arXiv preprint arXiv:2410.13212},
  year={2024}
}
@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}
@article{saxena2024eigen,
  title={Eigen Attention: Attention in Low-Rank Space for KV Cache Compression},
  author={Saxena, Utkarsh and Saha, Gobinda and Choudhary, Sakshi and Roy, Kaushik},
  journal={arXiv preprint arXiv:2408.05646},
  year={2024}
}
@article{zhang2024zero,
  title={Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference},
  author={Zhang, Zeyu and Shen, Haiying},
  journal={arXiv preprint arXiv:2408.04107},
  year={2024}
}
@article{matsui2018survey,
  title={A survey of product quantization},
  author={Matsui, Yusuke and Uchida, Yusuke and J{\'e}gou, Herv{\'e} and Satoh, Shin'ichi},
  journal={ITE Transactions on Media Technology and Applications},
  volume={6},
  number={1},
  pages={2--10},
  year={2018},
  publisher={The Institute of Image Information and Television Engineers}
}

@article{jegou2010product,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}
@article{lin2024qserve,
  title={Qserve: W4a8kv4 quantization and system co-design for efficient llm serving},
  author={Lin, Yujun and Tang, Haotian and Yang, Shang and Zhang, Zhekai and Xiao, Guangxuan and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2405.04532},
  year={2024}
}
@article{zhang2024pqcache,
  title={Pqcache: Product quantization-based kvcache for long context llm inference},
  author={Zhang, Hailin and Ji, Xiaodong and Chen, Yilin and Fu, Fangcheng and Miao, Xupeng and Nie, Xiaonan and Chen, Weipeng and Cui, Bin},
  journal={arXiv preprint arXiv:2407.12820},
  year={2024}
}
@article{yu2024effectively,
  title={Effectively Compress KV Heads for LLM},
  author={Yu, Hao and Yang, Zelan and Li, Shen and Li, Yong and Wu, Jianxin},
  journal={arXiv preprint arXiv:2406.07056},
  year={2024}
}

@article{zhang2024simlayerkv,
  title={SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction},
  author={Zhang, Xuan and Du, Cunxiao and Du, Chao and Pang, Tianyu and Gao, Wei and Lin, Min},
  journal={arXiv preprint arXiv:2410.13846},
  year={2024}
}

@article{liu2024unlocking,
  title={Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression},
  author={Liu, Peiyu and Gao, Ze-Feng and Zhao, Wayne Xin and Ma, Yipeng and Wang, Tao and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2405.12591},
  year={2024}
}

@article{zandieh2024qjl,
  title={QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead},
  author={Zandieh, Amir and Daliri, Majid and Han, Insu},
  journal={arXiv preprint arXiv:2406.03482},
  year={2024}
}
@article{he2024zipcache,
  title={ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification},
  author={He, Yefei and Zhang, Luoming and Wu, Weijia and Liu, Jing and Zhou, Hong and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14256},
  year={2024}
}
@article{zhang2023h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}
@article{yang2024no,
  title={No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization},
  author={Yang, June Yong and Kim, Byeongwook and Bae, Jeongin and Kwon, Beomseok and Park, Gunho and Yang, Eunho and Kwon, Se Jung and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2402.18096},
  year={2024}
}
@article{kang2024gear,
  title={Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}
@article{dong2024qaq,
  title={QAQ: Quality Adaptive Quantization for LLM KV Cache},
  author={Dong, Shichen and Cheng, Wen and Qin, Jiayu and Wang, Wei},
  journal={arXiv preprint arXiv:2403.04643},
  year={2024}
}
@inproceedings{DBLP:conf/sosp/KwonLZ0ZY0ZS23,
  author       = {Woosuk Kwon and
                  Zhuohan Li and
                  Siyuan Zhuang and
                  Ying Sheng and
                  Lianmin Zheng and
                  Cody Hao Yu and
                  Joseph Gonzalez and
                  Hao Zhang and
                  Ion Stoica},
  title        = {Efficient Memory Management for Large Language Model Serving with
                  PagedAttention},
  booktitle    = {Proceedings of the 29th Symposium on Operating Systems Principles,
                  {SOSP} 2023, Koblenz, Germany, October 23-26, 2023},
  pages        = {611--626},
  publisher    = {{ACM}},
  year         = {2023}
}

@misc{shen2024fastswitchoptimizingcontextswitching,
      title={FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware Large Language Model Serving}, 
      author={Ao Shen and Zhiyao Li and Mingyu Gao},
      year={2024},
      eprint={2411.18424},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.18424}, 
}

@misc{jiang2024neosavinggpumemory,
      title={NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference}, 
      author={Xuanlin Jiang and Yang Zhou and Shiyi Cao and Ion Stoica and Minlan Yu},
      year={2024},
      eprint={2411.01142},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2411.01142}, 
}

@misc{jiang2024efficientllminferenceioaware,
      title={Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation}, 
      author={Chaoyi Jiang and Lei Gao and Hossein Entezari Zarch and Murali Annavaram},
      year={2024},
      eprint={2411.17089},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.17089}, 
}

@misc{xu2024vtensorflexiblevirtualtensor,
      title={vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving}, 
      author={Jiale Xu and Rui Zhang and Cong Guo and Weiming Hu and Zihan Liu and Feiyang Wu and Yu Feng and Shixuan Sun and Changxu Shao and Yuhong Guo and Junping Zhao and Ke Zhang and Minyi Guo and Jingwen Leng},
      year={2024},
      eprint={2407.15309},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2407.15309}, 
}

@misc{xiong2024layerkvoptimizinglargelanguage,
      title={LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management}, 
      author={Yi Xiong and Hao Wu and Changxu Shao and Ziqing Wang and Rui Zhang and Yuhong Guo and Junping Zhao and Ke Zhang and Zhenxuan Pan},
      year={2024},
      eprint={2410.00428},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2410.00428}, 
}

@misc{hu2024memservecontextcachingdisaggregated,
      title={MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool}, 
      author={Cunchen Hu and Heyang Huang and Junhao Hu and Jiang Xu and Xusheng Chen and Tao Xie and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
      year={2024},
      eprint={2406.17565},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2406.17565}, 
}

@misc{gao2024costefficientlargelanguagemodel,
      title={Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention}, 
      author={Bin Gao and Zhuomin He and Puru Sharma and Qingxuan Kang and Djordje Jevdjic and Junbo Deng and Xingkun Yang and Zhou Yu and Pengfei Zuo},
      year={2024},
      eprint={2403.19708},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19708}, 
}

@misc{he2024fastdecodehighthroughputgpuefficientllm,
      title={FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines}, 
      author={Jiaao He and Jidong Zhai},
      year={2024},
      eprint={2403.11421},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2403.11421}, 
}


@misc{zheng2024batchllmoptimizinglargebatched,
      title={BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching}, 
      author={Zhen Zheng and Xin Ji and Taosong Fang and Fanghao Zhou and Chuanjie Liu and Gang Peng},
      year={2024},
      eprint={2412.03594},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.03594}, 
}
@article{malik2018low,
  title={Low-rank tucker decomposition of large tensors using tensorsketch},
  author={Malik, Osman Asif and Becker, Stephen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{chang2024palu,
  title={Palu: Compressing KV-Cache with Low-Rank Projection},
  author={Chang, Chi-Chih and Lin, Wei-Cheng and Lin, Chien-Yu and Chen, Chong-Yan and Hu, Yu-Fang and Wang, Pei-Shuo and Huang, Ning-Chi and Ceze, Luis and Abdelfattah, Mohamed S and Wu, Kai-Chiang},
  journal={arXiv preprint arXiv:2407.21118},
  year={2024}
}
@article{sun2024shadowkv,
  title={ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference},
  author={Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},
  journal={arXiv preprint arXiv:2410.21465},
  year={2024}
}
@article{dong2024get,
  title={Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference},
  author={Dong, Harry and Yang, Xinyu and Zhang, Zhenyu and Wang, Zhangyang and Chi, Yuejie and Chen, Beidi},
  journal={arXiv preprint arXiv:2402.09398},
  year={2024}
}
@article{zhang2024lorc,
  title={LORC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy},
  author={Zhang, Rongzhi and Wang, Kuang and Liu, Liyuan and Wang, Shuohang and Cheng, Hao and Zhang, Chao and Shen, Yelong},
  journal={arXiv preprint arXiv:2410.03111},
  year={2024}
}
@article{liu2021enabling,
  title={Enabling lightweight fine-tuning for pre-trained language model compression based on matrix product operators},
  author={Liu, Peiyu and Gao, Ze-Feng and Zhao, Wayne Xin and Xie, Zhi-Yuan and Lu, Zhong-Yi and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2106.02205},
  year={2021}
}
@article{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27168--27183},
  year={2022}
}
@misc{wang2024prefixkvadaptiveprefixkv,
      title={PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation}, 
      author={Ao Wang and Hui Chen and Jianchao Tan and Kefeng Zhang and Xunliang Cai and Zijia Lin and Jungong Han and Guiguang Ding},
      year={2024},
      eprint={2412.03409},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.03409}, 
}

@misc{liu2024clusterkvmanipulatingllmkv,
      title={ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression}, 
      author={Guangda Liu and Chengwei Li and Jieru Zhao and Chenqi Zhang and Minyi Guo},
      year={2024},
      eprint={2412.03213},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.03213}, 
}
@article{ma2024affinequant,
  title={Affinequant: Affine transformation quantization for large language models},
  author={Ma, Yuexiao and Li, Huixia and Zheng, Xiawu and Ling, Feng and Xiao, Xuefeng and Wang, Rui and Wen, Shilei and Chao, Fei and Ji, Rongrong},
  journal={arXiv preprint arXiv:2403.12544},
  year={2024}
}
@inproceedings{lin2024duquant,
  title={Duquant: Distributing outliers via dual transformation makes stronger quantized llms},
  author={Lin, Haokun and Xu, Haobo and Wu, Yichen and Cui, Jingzhi and Zhang, Yingtao and Mou, Linzhan and Song, Linqi and Sun, Zhenan and Wei, Ying},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{dong2021residual,
  title={Residual sensitivity for differentially private multi-way joins},
  author={Dong, Wei and Yi, Ke},
  booktitle={Proceedings of the 2021 International Conference on Management of Data},
  pages={432--444},
  year={2021}
}

@inproceedings{dong2023continual,
  title={Continual observation under user-level differential privacy},
  author={Dong, Wei and Luo, Qiyao and Yi, Ke},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={2190--2207},
  year={2023},
  organization={IEEE}
}

@article{liu2024clusterkv,
  title={ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression},
  author={Liu, Guangda and Li, Chengwei and Zhao, Jieru and Zhang, Chenqi and Guo, Minyi},
  journal={arXiv preprint arXiv:2412.03213},
  year={2024}
}

@inproceedings{lin2020pagraph,
  title={Pagraph: Scaling gnn training on large graphs via computation-aware caching},
  author={Lin, Zhiqi and Li, Cheng and Miao, Youshan and Liu, Yunxin and Xu, Yinlong},
  booktitle={Proceedings of the 11th ACM Symposium on Cloud Computing},
  pages={401--415},
  year={2020}
}

@article{zhao2022survey,
  title={A survey on differential privacy for unstructured data content},
  author={Zhao, Ying and Chen, Jinjun},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--28},
  year={2022},
  publisher={ACM New York, NY}
}


@inproceedings{wang2020austere,
  title={Austere flash caching with deduplication and compression},
  author={Wang, Qiuping and Li, Jinhong and Xia, Wen and Kruus, Erik and Debnath, Biplob and Lee, Patrick PC},
  booktitle={2020 USENIX Annual Technical Conference (USENIX ATC 20)},
  pages={713--726},
  year={2020}
}
@article{agarap2018deep,
  title={Deep learning using rectified linear units},
  author={Agarap, Abien Fred},
  journal={arXiv preprint arXiv:1803.08375},
  year={2018}
}

@article{zhao2023length,
  title={Length extrapolation of transformers: A survey from the perspective of position encoding},
  author={Zhao, Liang and Feng, Xiaocheng and Feng, Xiachong and Qin, Bin and Liu, Ting},
  journal={arXiv preprint arXiv:2312.17044},
  year={2023}
}
@article{zheng2021rethinking,
  title={Rethinking positional encoding},
  author={Zheng, Jianqiao and Ramasinghe, Sameera and Lucey, Simon},
  journal={arXiv preprint arXiv:2107.02561},
  year={2021}
}
 @article{zhong2024aim,
  title={AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning},
  author={Zhong, Yiwu and Liu, Zhuoming and Li, Yin and Wang, Liwei},
  journal={arXiv preprint arXiv:2412.03248},
  year={2024}
}
@article{cheng2017quantized,
  title={Quantized CNN: A unified approach to accelerate and compress convolutional networks},
  author={Cheng, Jian and Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={10},
  pages={4730--4743},
  year={2017},
  publisher={IEEE}
}
@inproceedings{kuleshov2015tensor,
  title={Tensor factorization via matrix factorization},
  author={Kuleshov, Volodymyr and Chaganty, Arun and Liang, Percy},
  booktitle={Artificial Intelligence and Statistics},
  pages={507--516},
  year={2015},
  organization={PMLR}
}
@article{zhou2017tensor,
  title={Tensor factorization for low-rank tensor completion},
  author={Zhou, Pan and Lu, Canyi and Lin, Zhouchen and Zhang, Chao},
  journal={IEEE Transactions on Image Processing},
  volume={27},
  number={3},
  pages={1152--1163},
  year={2017},
  publisher={IEEE}
}
@article{haeffele2015global,
  title={Global optimality in tensor factorization, deep learning, and beyond},
  author={Haeffele, Benjamin D and Vidal, Ren{\'e}},
  journal={arXiv preprint arXiv:1506.07540},
  year={2015}
}

@inproceedings{zhou2023dataset,
  title={Dataset quantization},
  author={Zhou, Daquan and Wang, Kai and Gu, Jianyang and Peng, Xiangyu and Lian, Dongze and Zhang, Yifan and You, Yang and Feng, Jiashi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17205--17216},
  year={2023}
}

@article{sun2024flatquant,
  title={FlatQuant: Flatness Matters for LLM Quantization},
  author={Sun, Yuxuan and Liu, Ruikang and Bai, Haoli and Bao, Han and Zhao, Kang and Li, Yuening and Hu, Jiaxin and Yu, Xianzhi and Hou, Lu and Yuan, Chun and others},
  journal={arXiv preprint arXiv:2410.09426},
  year={2024}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

 

@inproceedings{DBLP:conf/acl/DaiDZXGCLZYWXLH24,
  author       = {Damai Dai and
                  Chengqi Deng and
                  Chenggang Zhao and
                  R. X. Xu and
                  Huazuo Gao and
                  Deli Chen and
                  Jiashi Li and
                  Wangding Zeng and
                  Xingkai Yu and
                  Y. Wu and
                  Zhenda Xie and
                  Y. K. Li and
                  Panpan Huang and
                  Fuli Luo and
                  Chong Ruan and
                  Zhifang Sui and
                  Wenfeng Liang},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts
                  Language Models},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {1280--1297},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.70},
  doi          = {10.18653/V1/2024.ACL-LONG.70},
  timestamp    = {Tue, 24 Sep 2024 10:55:52 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/DaiDZXGCLZYWXLH24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{xu2024large,
  title={Large language models for generative information extraction: A survey},
  author={Xu, Derong and Chen, Wei and Peng, Wenjun and Zhang, Chao and Xu, Tong and Zhao, Xiangyu and Wu, Xian and Zheng, Yefeng and Wang, Yang and Chen, Enhong},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186357},
  year={2024},
  publisher={Springer}
}
@article{hadi2023survey,
  title={A survey on large language models: Applications, challenges, limitations, and practical usage},
  author={Hadi, Muhammad Usman and Qureshi, Rizwan and Shah, Abbas and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali and others},
  journal={Authorea Preprints},
  year={2023},
  publisher={Authorea}
}

@article{ma2024survey,
  title={A survey on time-series pre-trained models},
  author={Ma, Qianli and Liu, Zhen and Zheng, Zhenjing and Huang, Ziyang and Zhu, Siying and Yu, Zhongzhong and Kwok, James T},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}
@article{jin2023large,
  title={Large models for time series and spatio-temporal data: A survey and outlook},
  author={Jin, Ming and Wen, Qingsong and Liang, Yuxuan and Zhang, Chaoli and Xue, Siqiao and Wang, Xue and Zhang, James and Wang, Yi and Chen, Haifeng and Li, Xiaoli and others},
  journal={arXiv preprint arXiv:2310.10196},
  year={2023}
}
@article{zhou2023survey1,
  title={A survey of large language models in medicine: Progress, application, and challenge},
  author={Zhou, Hongjian and Liu, Fenglin and Gu, Boyang and Zou, Xinyu and Huang, Jinfa and Wu, Jinge and Li, Yiru and Chen, Sam S and Zhou, Peilin and Liu, Junling and others},
  journal={arXiv preprint arXiv:2311.05112},
  year={2023}
}
@article{qiu2023large,
  title={Large ai models in health informatics: Applications, challenges, and the future},
  author={Qiu, Jianing and Li, Lin and Sun, Jiankai and Peng, Jiachuan and Shi, Peilun and Zhang, Ruiyang and Dong, Yinzhao and Lam, Kyle and Lo, Frank P-W and Xiao, Bo and others},
  journal={IEEE Journal of Biomedical and Health Informatics},
  year={2023},
  publisher={IEEE}
}

@article{tan2023user,
  title={User modeling in the era of large language models: Current research and future directions},
  author={Tan, Zhaoxuan and Jiang, Meng},
  journal={arXiv preprint arXiv:2312.11518},
  year={2023}
}
@article{wu2024survey,
  title={A survey on large language models for recommendation},
  author={Wu, Likang and Zheng, Zhi and Qiu, Zhaopeng and Wang, Hao and Gu, Hongchao and Shen, Tingjia and Qin, Chuan and Zhu, Chen and Zhu, Hengshu and Liu, Qi and others},
  journal={World Wide Web},
  volume={27},
  number={5},
  pages={60},
  year={2024},
  publisher={Springer}
}

@article{kim2024speculative,
  title={Speculative decoding with big little decoder},
  author={Kim, Sehoon and Mangalam, Karttikeya and Moon, Suhong and Malik, Jitendra and Mahoney, Michael W and Gholami, Amir and Keutzer, Kurt},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@article{li2024prompt,
  title={Prompt Compression for Large Language Models: A Survey},
  author={Li, Zongqian and Liu, Yinhong and Su, Yixuan and Collier, Nigel},
  journal={arXiv preprint arXiv:2410.12388},
  year={2024}
}

@article{li2024scbench,
  title={SCBench: A KV Cache-Centric Analysis of Long-Context Methods},
  author={Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others},
  journal={arXiv preprint arXiv:2412.10319},
  year={2024}
}

@article{yuan2024kv,
  title={Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable approaches},
  author={Yuan, Jiayi and Liu, Hongyi and Zhong, Shaochen and Chuang, Yu-Neng and Li, Songchen and Wang, Guanchu and Le, Duy and Jin, Hongye and Chaudhary, Vipin and Xu, Zhaozhuo and others},
  journal={arXiv preprint arXiv:2407.01527},
  year={2024}
}

@article{shi2024keep,
  title={Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption},
  author={Shi, Luohe and Zhang, Hongyi and Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={arXiv preprint arXiv:2407.18003},
  year={2024}
} 
@article{zhuang2023survey,
  title={A survey on efficient training of transformers},
  author={Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
  journal={arXiv preprint arXiv:2302.01107},
  year={2023}
}
@article{xia2024unlocking,
  title={Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding},
  author={Xia, Heming and Yang, Zhe and Dong, Qingxiu and Wang, Peiyi and Li, Yongqi and Ge, Tao and Liu, Tianyu and Li, Wenjie and Sui, Zhifang},
  journal={arXiv preprint arXiv:2401.07851},
  year={2024}
}


@article{albalak2024survey,
  title={A survey on data selection for language models},
  author={Albalak, Alon and Elazar, Yanai and Xie, Sang Michael and Longpre, Shayne and Lambert, Nathan and Wang, Xinyi and Muennighoff, Niklas and Hou, Bairu and Pan, Liangming and Jeong, Haewon and others},
  journal={arXiv preprint arXiv:2402.16827},
  year={2024}
}

@article{xu2023parameter,
  title={Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment},
  author={Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
  journal={arXiv preprint arXiv:2312.12148},
  year={2023}
}
@article{kachris2024survey,
  title={A survey on hardware accelerators for large language models},
  author={Kachris, Christoforos},
  journal={arXiv preprint arXiv:2401.09890},
  year={2024}
}

 @article{zhou2024survey,
  title={A survey on efficient inference for large language models},
  author={Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and others},
  journal={arXiv preprint arXiv:2404.14294},
  year={2024}
}

@inproceedings{DBLP:conf/iclr/KimYYS24,
  author       = {Jang{-}Hyun Kim and
                  Junyoung Yeom and
                  Sangdoo Yun and
                  Hyun Oh Song},
  title        = {Compressed Context Memory for Online Language Model Interaction},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=64kSvC4iPg},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/KimYYS24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/nips/Mu0G23,
  author       = {Jesse Mu and
                  Xiang Li and
                  Noah D. Goodman},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Learning to Compress Prompts with Gist Tokens},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/3d77c6dcc7f143aa2154e7f4d5e22d68-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Mu0G23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




 @inproceedings{DBLP:conf/iclr/RaePJHL20,
  author       = {Jack W. Rae and
                  Anna Potapenko and
                  Siddhant M. Jayakumar and
                  Chloe Hillier and
                  Timothy P. Lillicrap},
  title        = {Compressive Transformers for Long-Range Sequence Modelling},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=SylKikSYDH},
  timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/RaePJHL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/XiaoLSWDH23,
  author       = {Guangxuan Xiao and
                  Ji Lin and
                  Micka{\"{e}}l Seznec and
                  Hao Wu and
                  Julien Demouth and
                  Song Han},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {SmoothQuant: Accurate and Efficient Post-Training Quantization for
                  Large Language Models},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {38087--38099},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/xiao23c.html},
  timestamp    = {Fri, 10 Nov 2023 21:09:29 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/XiaoLSWDH23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ashkboos2024quarot,
  title={Quarot: Outlier-free 4-bit inference in rotated llms},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Cameron, Pashmina and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2404.00456},
  year={2024}
}

@article{chen2024prefixquant,
  title={PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs},
  author={Chen, Mengzhao and Liu, Yi and Wang, Jiahao and Bin, Yi and Shao, Wenqi and Luo, Ping},
  journal={arXiv preprint arXiv:2410.05265},
  year={2024}
}

 

@article{DBLP:journals/corr/abs-2402-17762,
  author       = {Mingjie Sun and
                  Xinlei Chen and
                  J. Zico Kolter and
                  Zhuang Liu},
  title        = {Massive Activations in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2402.17762},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.17762},
  doi          = {10.48550/ARXIV.2402.17762},
  eprinttype    = {arXiv},
  eprint       = {2402.17762},
  timestamp    = {Mon, 25 Mar 2024 15:38:17 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-17762.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}
@misc{zhang2024unifyingkvcachecompression,
      title={Unifying KV Cache Compression for Large Language Models with LeanKV}, 
      author={Yanqi Zhang and Yuwei Hu and Runyuan Zhao and John C. S. Lui and Haibo Chen},
      year={2024},
      eprint={2412.03131},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.03131}, 
}

@misc{sharma2024minikvpushinglimitsllm,
      title={MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache}, 
      author={Akshat Sharma and Hangliang Ding and Jianping Li and Neel Dani and Minjia Zhang},
      year={2024},
      eprint={2411.18077},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.18077}, 
}

@misc{tu2024vlcachesparsitymodalityawarekv,
      title={VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration}, 
      author={Dezhan Tu and Danylo Vashchilenko and Yuzhe Lu and Panpan Xu},
      year={2024},
      eprint={2410.23317},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.23317}, 
}


@misc{zhao2024buzzbeehivestructuredsparsekv,
      title={BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference}, 
      author={Junqi Zhao and Zhijin Fang and Shu Li and Shaohui Yang and Shichao He},
      year={2024},
      eprint={2410.23079},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.23079}, 
}

@misc{fu2024headsmatterheadlevelkv,
      title={Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning}, 
      author={Yu Fu and Zefan Cai and Abedelkadir Asi and Wayne Xiong and Yue Dong and Wen Xiao},
      year={2024},
      eprint={2410.19258},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.19258}, 
}


 @article{DBLP:journals/corr/abs-2405-14366,
  author       = {Akide Liu and
                  Jing Liu and
                  Zizheng Pan and
                  Yefei He and
                  Gholamreza Haffari and
                  Bohan Zhuang},
  title        = {MiniCache: {KV} Cache Compression in Depth Dimension for Large Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2405.14366},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.14366},
  doi          = {10.48550/ARXIV.2405.14366},
  eprinttype    = {arXiv},
  eprint       = {2405.14366},
  timestamp    = {Wed, 19 Jun 2024 08:52:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-14366.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/XiaoTCHL24,
  author       = {Guangxuan Xiao and
                  Yuandong Tian and
                  Beidi Chen and
                  Song Han and
                  Mike Lewis},
  title        = {Efficient Streaming Language Models with Attention Sinks},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=NG7sS51zVF},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/XiaoTCHL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/Zhang00CZC0TRBW23,
  author       = {Zhenyu Zhang and
                  Ying Sheng and
                  Tianyi Zhou and
                  Tianlong Chen and
                  Lianmin Zheng and
                  Ruisi Cai and
                  Zhao Song and
                  Yuandong Tian and
                  Christopher R{\'{e}} and
                  Clark W. Barrett and
                  Zhangyang Wang and
                  Beidi Chen},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {{H2O:} Heavy-Hitter Oracle for Efficient Generative Inference of Large
                  Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html},
  timestamp    = {Fri, 15 Mar 2024 12:30:47 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Zhang00CZC0TRBW23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/osdi/ZhongLCHZL0024,
  author       = {Yinmin Zhong and
                  Shengyu Liu and
                  Junda Chen and
                  Jianbo Hu and
                  Yibo Zhu and
                  Xuanzhe Liu and
                  Xin Jin and
                  Hao Zhang},
  editor       = {Ada Gavrilovska and
                  Douglas B. Terry},
  title        = {DistServe: Disaggregating Prefill and Decoding for Goodput-optimized
                  Large Language Model Serving},
  booktitle    = {18th {USENIX} Symposium on Operating Systems Design and Implementation,
                  {OSDI} 2024, Santa Clara, CA, USA, July 10-12, 2024},
  pages        = {193--210},
  publisher    = {{USENIX} Association},
  year         = {2024},
  url          = {https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin},
  timestamp    = {Tue, 16 Jul 2024 22:11:07 +0200},
  biburl       = {https://dblp.org/rec/conf/osdi/ZhongLCHZL0024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2410-05004,
  author       = {Shiwei Gao and
                  Youmin Chen and
                  Jiwu Shu},
  title        = {Fast State Restoration in {LLM} Serving with HCache},
  journal      = {CoRR},
  volume       = {abs/2410.05004},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2410.05004},
  doi          = {10.48550/ARXIV.2410.05004},
  eprinttype    = {arXiv},
  eprint       = {2410.05004},
  timestamp    = {Tue, 12 Nov 2024 18:39:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-05004.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{
guldogan2024multibin,
title={Multi-Bin Batching for Increasing {LLM} Inference Throughput},
author={Ozgur Guldogan and Jackson Kunde and Kangwook Lee and Ramtin Pedarsani},
year={2024},
url={https://openreview.net/forum?id=WVmarX0RNd}
}


@misc{shyam2024treeattentiontopologyawaredecoding,
      title={Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters}, 
      author={Vasudev Shyam and Jonathan Pilault and Emily Shepperd and Quentin Anthony and Beren Millidge},
      year={2024},
      eprint={2408.04093},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.04093}, 
}


@misc{yang2024pyramidinferpyramidkvcache,
      title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference}, 
      author={Dongjie Yang and XiaoDong Han and Yan Gao and Yao Hu and Shilin Zhang and Hai Zhao},
      year={2024},
      eprint={2405.12532},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.12532}, 
}

@misc{juravsky2024hydragenhighthroughputllminference,
      title={Hydragen: High-Throughput LLM Inference with Shared Prefixes}, 
      author={Jordan Juravsky and Bradley Brown and Ryan Ehrlich and Daniel Y. Fu and Christopher Ré and Azalia Mirhoseini},
      year={2024},
      eprint={2402.05099},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05099}, 
}

@misc{ye2024chunkattentionefficientselfattentionprefixaware,
      title={ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition}, 
      author={Lu Ye and Ze Tao and Yong Huang and Yang Li},
      year={2024},
      eprint={2402.15220},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.15220}, 
}

@misc{yang2024tokenleftbehindreliable,
      title={No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization}, 
      author={June Yong Yang and Byeongwook Kim and Jeongin Bae and Beomseok Kwon and Gunho Park and Eunho Yang and Se Jung Kwon and Dongsoo Lee},
      year={2024},
      eprint={2402.18096},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18096}, 
}

@misc{athiwaratkun2024bifurcatedattentionacceleratingmassively,
      title={Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs}, 
      author={Ben Athiwaratkun and Sujan Kumar Gonugondla and Sanjay Krishna Gouda and Haifeng Qian and Hantian Ding and Qing Sun and Jun Wang and Jiacheng Guo and Liangfu Chen and Parminder Bhatia and Ramesh Nallapati and Sudipta Sengupta and Bing Xiang},
      year={2024},
      eprint={2403.08845},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.08845}, 
}


@misc{zhao2024alisaacceleratinglargelanguage,
      title={ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching}, 
      author={Youpeng Zhao and Di Wu and Jun Wang},
      year={2024},
      eprint={2403.17312},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.17312}, 
}


@misc{yao2024deftdecodingflashtreeattention,
      title={DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference}, 
      author={Jinwei Yao and Kaiqi Chen and Kexun Zhang and Jiaxuan You and Binhang Yuan and Zeke Wang and Tao Lin},
      year={2024},
      eprint={2404.00242},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.00242}, 
}

@misc{pan2024instinferinstorageattentionoffloading,
      title={InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference}, 
      author={Xiurui Pan and Endian Li and Qiao Li and Shengwen Liang and Yizhou Shan and Ke Zhou and Yingwei Luo and Xiaolin Wang and Jie Zhang},
      year={2024},
      eprint={2409.04992},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2409.04992}, 
}

@misc{lee2024infinigenefficientgenerativeinference,
      title={InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management}, 
      author={Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim},
      year={2024},
      eprint={2406.19707},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.19707}, 
}


@misc{willette2024trainingfreeexponentialcontextextension,
      title={Training-Free Exponential Context Extension via Cascading KV Cache}, 
      author={Jeffrey Willette and Heejun Lee and Youngwan Lee and Myeongjae Jeon and Sung Ju Hwang},
      year={2024},
      eprint={2406.17808},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17808}, 
}

@misc{shahout2024fastinferenceaugmentedlarge,
      title={Fast Inference for Augmented Large Language Models}, 
      author={Rana Shahout and Cong Liang and Shiji Xin and Qianru Lao and Yong Cui and Minlan Yu and Michael Mitzenmacher},
      year={2024},
      eprint={2410.18248},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.18248}, 
}

@misc{zheng2024sglangefficientexecutionstructured,
      title={SGLang: Efficient Execution of Structured Language Model Programs}, 
      author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      year={2024},
      eprint={2312.07104},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.07104}, 
}

@article{DBLP:journals/corr/abs-2312-05516,
  author       = {Lingfan Yu and
                  Jinyang Li},
  title        = {Stateful Large Language Model Serving with Pensieve},
  journal      = {CoRR},
  volume       = {abs/2312.05516},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2312.05516},
  doi          = {10.48550/ARXIV.2312.05516},
  eprinttype    = {arXiv},
  eprint       = {2312.05516},
  timestamp    = {Wed, 03 Jan 2024 14:03:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2312-05516.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{wu2024fastdistributedinferenceserving,
      title={Fast Distributed Inference Serving for Large Language Models}, 
      author={Bingyang Wu and Yinmin Zhong and Zili Zhang and Shengyu Liu and Fangyue Liu and Yuanhang Sun and Gang Huang and Xuanzhe Liu and Xin Jin},
      year={2024},
      eprint={2305.05920},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.05920}, 
}

@misc{jin2024computeloadkvcache,
      title={Compute Or Load KV Cache? Why Not Both?}, 
      author={Shuowei Jin and Xueshen Liu and Qingzhao Zhang and Z. Morley Mao},
      year={2024},
      eprint={2410.03065},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.03065}, 
}

%%%%%%%%% END OF SYSTEM LEVEL


%%%%%%%%% START OF MODEL LEVEL
% Model based KV cache
%% intra layer
@misc{agarwalCHAIClusteredHead2024,
	title = {{{CHAI}}: {{Clustered Head Attention}} for {{Efficient LLM Inference}}},
	shorttitle = {{{CHAI}}},
	author = {Agarwal, Saurabh and Acun, Bilge and Hosmer, Basil and Elhoushi, Mostafa and Lee, Yejin and Venkataraman, Shivaram and Papailiopoulos, Dimitris and Wu, Carole-Jean},
	year = {2024},
	month = apr,
	number = {arXiv:2403.08058},
	eprint = {2403.08058},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2403.08058},
	url = {http://arxiv.org/abs/2403.08058},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@inproceedings{DBLP:conf/iclr/ZengLDWL0YXZXTM23,
  author       = {Aohan Zeng and
                  Xiao Liu and
                  Zhengxiao Du and
                  Zihan Wang and
                  Hanyu Lai and
                  Ming Ding and
                  Zhuoyi Yang and
                  Yifan Xu and
                  Wendi Zheng and
                  Xiao Xia and
                  Weng Lam Tam and
                  Zixuan Ma and
                  Yufei Xue and
                  Jidong Zhai and
                  Wenguang Chen and
                  Zhiyuan Liu and
                  Peng Zhang and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {{GLM-130B:} An Open Bilingual Pre-trained Model},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=-Aw0rrrPUF},
  timestamp    = {Tue, 06 Aug 2024 21:09:04 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZengLDWL0YXZXTM23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/DuQLDQY022,
  author       = {Zhengxiao Du and
                  Yujie Qian and
                  Xiao Liu and
                  Ming Ding and
                  Jiezhong Qiu and
                  Zhilin Yang and
                  Jie Tang},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {{GLM:} General Language Model Pretraining with Autoregressive Blank
                  Infilling},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {320--335},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-long.26},
  doi          = {10.18653/V1/2022.ACL-LONG.26},
  timestamp    = {Mon, 04 Sep 2023 20:40:36 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/DuQLDQY022.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2307-06435,
  author       = {Humza Naveed and
                  Asad Ullah Khan and
                  Shi Qiu and
                  Muhammad Saqib and
                  Saeed Anwar and
                  Muhammad Usman and
                  Nick Barnes and
                  Ajmal Mian},
  title        = {A Comprehensive Overview of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2307.06435},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.06435},
  doi          = {10.48550/ARXIV.2307.06435},
  eprinttype    = {arXiv},
  eprint       = {2307.06435},
  timestamp    = {Mon, 11 Nov 2024 17:13:58 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-06435.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/csur/MinRSVNSAHR24,
  author       = {Bonan Min and
                  Hayley Ross and
                  Elior Sulem and
                  Amir Pouran Ben Veyseh and
                  Thien Huu Nguyen and
                  Oscar Sainz and
                  Eneko Agirre and
                  Ilana Heintz and
                  Dan Roth},
  title        = {Recent Advances in Natural Language Processing via Large Pre-trained
                  Language Models: {A} Survey},
  journal      = {{ACM} Comput. Surv.},
  volume       = {56},
  number       = {2},
  pages        = {30:1--30:40},
  year         = {2024},
  url          = {https://doi.org/10.1145/3605943},
  doi          = {10.1145/3605943},
  timestamp    = {Fri, 27 Oct 2023 20:40:08 +0200},
  biburl       = {https://dblp.org/rec/journals/csur/MinRSVNSAHR24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{DBLP:journals/corr/abs-2306-16410,
  author       = {William Berrios and
                  Gautam Mittal and
                  Tristan Thrush and
                  Douwe Kiela and
                  Amanpreet Singh},
  title        = {Towards Language Models That Can See: Computer Vision Through the
                  {LENS} of Natural Language},
  journal      = {CoRR},
  volume       = {abs/2306.16410},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.16410},
  doi          = {10.48550/ARXIV.2306.16410},
  eprinttype    = {arXiv},
  eprint       = {2306.16410},
  timestamp    = {Mon, 03 Jul 2023 12:06:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-16410.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/bigdataconf/WuGCWY23,
  author       = {Jiayang Wu and
                  Wensheng Gan and
                  Zefeng Chen and
                  Shicheng Wan and
                  Philip S. Yu},
  editor       = {Jingrui He and
                  Themis Palpanas and
                  Xiaohua Hu and
                  Alfredo Cuzzocrea and
                  Dejing Dou and
                  Dominik Slezak and
                  Wei Wang and
                  Aleksandra Gruca and
                  Jerry Chun{-}Wei Lin and
                  Rakesh Agrawal},
  title        = {Multimodal Large Language Models: {A} Survey},
  booktitle    = {{IEEE} International Conference on Big Data, BigData 2023, Sorrento,
                  Italy, December 15-18, 2023},
  pages        = {2247--2256},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/BigData59044.2023.10386743},
  doi          = {10.1109/BIGDATA59044.2023.10386743},
  timestamp    = {Tue, 20 Aug 2024 07:54:43 +0200},
  biburl       = {https://dblp.org/rec/conf/bigdataconf/WuGCWY23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/ZhangY0L0C024,
  author       = {Duzhen Zhang and
                  Yahan Yu and
                  Jiahua Dong and
                  Chenxing Li and
                  Dan Su and
                  Chenhui Chu and
                  Dong Yu},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {MM-LLMs: Recent Advances in MultiModal Large Language Models},
  booktitle    = {Findings of the Association for Computational Linguistics, {ACL} 2024,
                  Bangkok, Thailand and virtual meeting, August 11-16, 2024},
  pages        = {12401--12430},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.findings-acl.738},
  doi          = {10.18653/V1/2024.FINDINGS-ACL.738},
  timestamp    = {Tue, 24 Sep 2024 10:55:39 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZhangY0L0C024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24,
  author       = {Can Cui and
                  Yunsheng Ma and
                  Xu Cao and
                  Wenqian Ye and
                  Yang Zhou and
                  Kaizhao Liang and
                  Jintai Chen and
                  Juanwu Lu and
                  Zichong Yang and
                  Kuei{-}Da Liao and
                  Tianren Gao and
                  Erlong Li and
                  Kun Tang and
                  Zhipeng Cao and
                  Tong Zhou and
                  Ao Liu and
                  Xinrui Yan and
                  Shuqi Mei and
                  Jianguo Cao and
                  Ziran Wang and
                  Chao Zheng},
  title        = {A Survey on Multimodal Large Language Models for Autonomous Driving},
  booktitle    = {{IEEE/CVF} Winter Conference on Applications of Computer Vision Workshops,
                  {WACVW} 2024 - Workshops, Waikoloa, HI, USA, January 1-6, 2024},
  pages        = {958--979},
  publisher    = {{IEEE}},
  year         = {2024},
  url          = {https://doi.org/10.1109/WACVW60836.2024.00106},
  doi          = {10.1109/WACVW60836.2024.00106},
  timestamp    = {Wed, 16 Oct 2024 16:36:23 +0200},
  biburl       = {https://dblp.org/rec/conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@article{DBLP:journals/pami/ZhangHJL24,
  author       = {Jingyi Zhang and
                  Jiaxing Huang and
                  Sheng Jin and
                  Shijian Lu},
  title        = {Vision-Language Models for Vision Tasks: {A} Survey},
  journal      = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  volume       = {46},
  number       = {8},
  pages        = {5625--5644},
  year         = {2024},
  url          = {https://doi.org/10.1109/TPAMI.2024.3369699},
  doi          = {10.1109/TPAMI.2024.3369699},
  timestamp    = {Fri, 02 Aug 2024 21:40:05 +0200},
  biburl       = {https://dblp.org/rec/journals/pami/ZhangHJL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/nips/LiuLWL23a,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Qingyang Wu and
                  Yong Jae Lee},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Visual Instruction Tuning},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/LiuLWL23a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2310-06825,
  author       = {Albert Q. Jiang and
                  Alexandre Sablayrolles and
                  Arthur Mensch and
                  Chris Bamford and
                  Devendra Singh Chaplot and
                  Diego de Las Casas and
                  Florian Bressand and
                  Gianna Lengyel and
                  Guillaume Lample and
                  Lucile Saulnier and
                  L{\'{e}}lio Renard Lavaud and
                  Marie{-}Anne Lachaux and
                  Pierre Stock and
                  Teven Le Scao and
                  Thibaut Lavril and
                  Thomas Wang and
                  Timoth{\'{e}}e Lacroix and
                  William El Sayed},
  title        = {Mistral 7B},
  journal      = {CoRR},
  volume       = {abs/2310.06825},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.06825},
  doi          = {10.48550/ARXIV.2310.06825},
  eprinttype    = {arXiv},
  eprint       = {2310.06825},
  timestamp    = {Thu, 26 Oct 2023 16:46:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-06825.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2401-04088,
  author       = {Albert Q. Jiang and
                  Alexandre Sablayrolles and
                  Antoine Roux and
                  Arthur Mensch and
                  Blanche Savary and
                  Chris Bamford and
                  Devendra Singh Chaplot and
                  Diego de Las Casas and
                  Emma Bou Hanna and
                  Florian Bressand and
                  Gianna Lengyel and
                  Guillaume Bour and
                  Guillaume Lample and
                  L{\'{e}}lio Renard Lavaud and
                  Lucile Saulnier and
                  Marie{-}Anne Lachaux and
                  Pierre Stock and
                  Sandeep Subramanian and
                  Sophia Yang and
                  Szymon Antoniak and
                  Teven Le Scao and
                  Th{\'{e}}ophile Gervet and
                  Thibaut Lavril and
                  Thomas Wang and
                  Timoth{\'{e}}e Lacroix and
                  William El Sayed},
  title        = {Mixtral of Experts},
  journal      = {CoRR},
  volume       = {abs/2401.04088},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.04088},
  doi          = {10.48550/ARXIV.2401.04088},
  eprinttype    = {arXiv},
  eprint       = {2401.04088},
  timestamp    = {Wed, 24 Jan 2024 16:19:32 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-04088.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2302-13971,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.13971},
  doi          = {10.48550/ARXIV.2302.13971},
  eprinttype    = {arXiv},
  eprint       = {2302.13971},
  timestamp    = {Mon, 28 Aug 2023 21:26:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-13971.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{DBLP:journals/corr/abs-2309-10305,
  author       = {Aiyuan Yang and
                  Bin Xiao and
                  Bingning Wang and
                  Borong Zhang and
                  Ce Bian and
                  others
                 },
  title        = {Baichuan 2: Open Large-scale Language Models},
  journal      = {CoRR},
  volume       = {abs/2309.10305},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.10305},
  doi          = {10.48550/ARXIV.2309.10305},
  eprinttype    = {arXiv},
  eprint       = {2309.10305},
  timestamp    = {Fri, 12 Apr 2024 08:11:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-10305.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2403-05525,
  author       = {Haoyu Lu and
                  Wen Liu and
                  Bo Zhang and
                  Bingxuan Wang and
                  Kai Dong and
                  Bo Liu and
                  Jingxiang Sun and
                  Tongzheng Ren and
                  Zhuoshu Li and
                  Hao Yang and
                  Yaofeng Sun and
                  Chengqi Deng and
                  Hanwei Xu and
                  Zhenda Xie and
                  Chong Ruan},
  title        = {DeepSeek-VL: Towards Real-World Vision-Language Understanding},
  journal      = {CoRR},
  volume       = {abs/2403.05525},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.05525},
  doi          = {10.48550/ARXIV.2403.05525},
  eprinttype    = {arXiv},
  eprint       = {2403.05525},
  timestamp    = {Fri, 24 May 2024 22:53:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-05525.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ainslieGQATrainingGeneralized2023,
	title = {{{GQA}}: {{Training Generalized Multi-Query Transformer Models}} from {{Multi-Head Checkpoints}}},
	shorttitle = {{{GQA}}},
	author = {Ainslie, Joshua and {Lee-Thorp}, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
	year = {2023},
	month = dec,
	number = {arXiv:2305.13245},
	eprint = {2305.13245},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2305.13245},
	url = {http://arxiv.org/abs/2305.13245},
	urldate = {2024-12-07},
	archiveprefix = {arXiv}
}

@inproceedings{chenOptimisedGroupedQueryAttention2024a,
	title = {Optimised {{Grouped-Query Attention Mechanism}} for {{Transformers}}},
	booktitle = {Workshop on {{Efficient Systems}} for {{Foundation Models II}} @ {{ICML2024}}},
	author = {Chen, Yuang and Zhang, Cheng and Gao, Xitong and Mullins, Robert D. and Constantinides, George Anthony and Zhao, Yiren},
	year = {2024},
	month = jul,
	url = {https://openreview.net/forum?id=13MMghY6Kh},
	urldate = {2024-12-09},
	langid = {english}
}

@misc{chinnakonduruWeightedGroupedQuery2024,
	title = {Weighted {{Grouped Query Attention}} in {{Transformers}}},
	author = {Chinnakonduru, Sai Sena and Mohapatra, Astarag},
	year = {2024},
	month = jul,
	number = {arXiv:2407.10855},
	eprint = {2407.10855},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2407.10855},
	url = {http://arxiv.org/abs/2407.10855},
	urldate = {2024-12-09},
	archiveprefix = {arXiv}
}

@misc{deepseek-aiDeepSeekV2StrongEconomical2024,
	title = {{{DeepSeek-V2}}: {{A Strong}}, {{Economical}}, and {{Efficient Mixture-of-Experts Language Model}}},
	shorttitle = {{{DeepSeek-V2}}},
	author = {{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Xu, Hanwei and Yang, Hao and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Chen, Jin and Yuan, Jingyang and Qiu, Junjie and Song, Junxiao and Dong, Kai and Gao, Kaige and Guan, Kang and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Pan, Ruizhe and Xu, Runxin and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Zheng, Size and Wang, T. and Pei, Tian and Yuan, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Liu, Xin and Xie, Xin and Yu, Xingkai and Song, Xinnan and Zhou, Xinyi and Yang, Xinyu and Lu, Xuan and Su, Xuecheng and Wu, Y. and Li, Y. K. and Wei, Y. X. and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Zheng, Yi and Zhang, Yichao and Xiong, Yiliang and Zhao, Yilong and He, Ying and Tang, Ying and Piao, Yishi and Dong, Yixin and Tan, Yixuan and Liu, Yiyuan and Wang, Yongji and Guo, Yongqiang and Zhu, Yuchen and Wang, Yuduan and Zou, Yuheng and Zha, Yukun and Ma, Yunxian and Yan, Yuting and You, Yuxiang and Liu, Yuxuan and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Hao, Zhewen and Shao, Zhihong and Wen, Zhiniu and Xu, Zhipeng and Zhang, Zhongyu and Li, Zhuoshu and Wang, Zihan and Gu, Zihui and Li, Zilin and Xie, Ziwei},
	year = {2024},
	month = jun,
	number = {arXiv:2405.04434},
	eprint = {2405.04434},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2405.04434},
	url = {http://arxiv.org/abs/2405.04434},
	urldate = {2024-12-07},
	archiveprefix = {arXiv}
}

@inproceedings{huaTransformerQualityLinear2022,
	title = {Transformer {{Quality}} in {{Linear Time}}},
	booktitle = {International {{Conference}} on {{Machine Learning}}, {{ICML}} 2022, 17-23 {{July}} 2022, {{Baltimore}}, {{Maryland}}, {{USA}}},
	author = {Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc V.},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesv{\'a}ri, Csaba and Niu, Gang and Sabato, Sivan},
	year = {2022},
	series = {Proceedings of {{Machine Learning Research}}},
	volume = {162},
	pages = {9099--9117},
	publisher = {PMLR},
	url = {https://proceedings.mlr.press/v162/hua22a.html},
	urldate = {2024-12-09}
}

@misc{huEfficientLongrangeLanguage2024,
	title = {Efficient {{Long-range Language Modeling}} with {{Self-supervised Causal Retrieval}}},
	author = {Hu, Xiang and Teng, Zhihao and Wu, Wei and Tu, Kewei},
	year = {2024},
	month = oct,
	number = {arXiv:2410.01651},
	eprint = {2410.01651},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2410.01651},
	url = {http://arxiv.org/abs/2410.01651},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@misc{javadiGQKVAEfficientPretraining2023,
	title = {{{GQKVA}}: {{Efficient Pre-training}} of {{Transformers}} by {{Grouping Queries}}, {{Keys}}, and {{Values}}},
	shorttitle = {{{GQKVA}}},
	author = {Javadi, Farnoosh and Ahmed, Walid and Hajimolahoseini, Habib and Ataiefard, Foozhan and Hassanpour, Mohammad and Asani, Saina and Wen, Austin and Awad, Omar Mohamed and Liu, Kangling and Liu, Yang},
	year = {2023},
	month = dec,
	number = {arXiv:2311.03426},
	eprint = {2311.03426},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2311.03426},
	url = {http://arxiv.org/abs/2311.03426},
	urldate = {2024-12-09},
	archiveprefix = {arXiv}
}

@misc{joshiQCQAQualityCapacityaware2024,
	title = {{{QCQA}}: {{Quality}} and {{Capacity-aware}} Grouped {{Query Attention}}},
	shorttitle = {{{QCQA}}},
	author = {Joshi, Vinay and Laddha, Prashant and Sinha, Shambhavi and Omer, Om Ji and Subramoney, Sreenivas},
	year = {2024},
	month = jun,
	number = {arXiv:2406.10247},
	eprint = {2406.10247},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2406.10247},
	url = {http://arxiv.org/abs/2406.10247},
	urldate = {2024-12-09},
	archiveprefix = {arXiv}
}
@article{yao2024cacheblend,
  title={CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion},
  author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  journal={arXiv preprint arXiv:2405.16444},
  year={2024}
}
@article{salehinejad2017recent,
  title={Recent advances in recurrent neural networks},
  author={Salehinejad, Hojjat and Sankar, Sharan and Barfett, Joseph and Colak, Errol and Valaee, Shahrokh},
  journal={arXiv preprint arXiv:1801.01078},
  year={2017}
}

@article{xu2024integrating,
  title={Integrating Mamba and Transformer for Long-Short Range Time Series Forecasting},
  author={Xu, Xiongxiao and Liang, Yueqing and Huang, Baixiang and Lan, Zhiling and Shu, Kai},
  journal={arXiv preprint arXiv:2404.14757},
  year={2024}
}
@article{hasani2022liquid,
  title={Liquid structural state-space models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}
@article{smith2022simplified,
  title={Simplified state space layers for sequence modeling},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  journal={arXiv preprint arXiv:2208.04933},
  year={2022}
}

@article{wang2022pretraining,
  title={Pretraining without attention},
  author={Wang, Junxiong and Yan, Jing Nathan and Gu, Albert and Rush, Alexander M},
  journal={arXiv preprint arXiv:2212.10544},
  year={2022}
}


@article{qu2024survey,
  title={A survey of mamba},
  author={Qu, Haohao and Ning, Liangbo and An, Rui and Fan, Wenqi and Derr, Tyler and Liu, Hui and Xu, Xin and Li, Qing},
  journal={arXiv preprint arXiv:2408.01129},
  year={2024}
}
@article{xu2024survey,
  title={A survey on vision mamba: Models, applications and challenges},
  author={Xu, Rui and Yang, Shu and Wang, Yihui and Du, Bo and Chen, Hao},
  journal={arXiv preprint arXiv:2404.18861},
  year={2024}
}




@misc{Awesome-LLM-Inference@2024,
  title={Awesome-LLM-KV-Cache: A curated list of Awesome LLM Inference Papers with codes},
  url={https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache},
  note={Open-source software available at https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache},
  author={Zefan-Cai, etc},
  year={2024}
}
@article{chen2024sepllm,
  title={SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator},
  author={Chen, Guoxuan and Shi, Han and Li, Jiawei and Gao, Yihang and Ren, Xiaozhe and Chen, Yimeng and Jiang, Xin and Li, Zhenguo and Liu, Weiyang and Huang, Chao},
  journal={arXiv preprint arXiv:2412.12094},
  year={2024}
}

@misc{khanUniformQueryDistribution2024,
	title = {Beyond {{Uniform Query Distribution}}: {{Key-Driven Grouped Query Attention}}},
	shorttitle = {Beyond {{Uniform Query Distribution}}},
	author = {Khan, Zohaib and Khaquan, Muhammad and Tafveez, Omer and Samiwala, Burhanuddin and Raza, Agha Ali},
	year = {2024},
	month = aug,
	number = {arXiv:2408.08454},
	eprint = {2408.08454},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2408.08454},
	url = {http://arxiv.org/abs/2408.08454},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@misc{linMatryoshkaKVAdaptiveKV2024,
	title = {{{MatryoshkaKV}}: {{Adaptive KV Compression}} via {{Trainable Orthogonal Projection}}},
	shorttitle = {{{MatryoshkaKV}}},
	author = {Lin, Bokai and Zeng, Zihao and Xiao, Zipeng and Kou, Siqi and Hou, Tianqi and Gao, Xiaofeng and Zhang, Hao and Deng, Zhijie},
	year = {2024},
	month = oct,
	number = {arXiv:2410.14731},
	eprint = {2410.14731},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2410.14731},
	url = {http://arxiv.org/abs/2410.14731},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@inproceedings{DBLP:conf/cikm/LiC21,
  author       = {Haoyang Li and
                  Lei Chen},
  title        = {Cache-based {GNN} System for Dynamic Graphs},
  booktitle    = {{CIKM} '21: The 30th {ACM} International Conference on Information
                  and Knowledge Management, Virtual Event, Queensland, Australia, November
                  1 - 5, 2021},
  pages        = {937--946},
  publisher    = {{ACM}},
  year         = {2021},
  url          = {https://doi.org/10.1145/3459637.3482237},
  doi          = {10.1145/3459637.3482237},
  timestamp    = {Mon, 05 Aug 2024 15:14:25 +0200},
  biburl       = {https://dblp.org/rec/conf/cikm/LiC21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/pacmmod/LiSCY23,
  author       = {Yiming Li and
                  Yanyan Shen and
                  Lei Chen and
                  Mingxuan Yuan},
  title        = {Orca: Scalable Temporal Graph Neural Network Training with Theoretical
                  Guarantees},
  journal      = {Proc. {ACM} Manag. Data},
  volume       = {1},
  number       = {1},
  pages        = {52:1--52:27},
  year         = {2023},
  url          = {https://doi.org/10.1145/3588737},
  doi          = {10.1145/3588737},
  timestamp    = {Mon, 19 Jun 2023 16:36:09 +0200},
  biburl       = {https://dblp.org/rec/journals/pacmmod/LiSCY23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@misc{munkhdalaiLeaveNoContext2024,
	title = {Leave {{No Context Behind}}: {{Efficient Infinite Context Transformers}} with {{Infini-attention}}},
	shorttitle = {Leave {{No Context Behind}}},
	author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
	year = {2024},
	month = aug,
	number = {arXiv:2404.07143},
	eprint = {2404.07143},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2404.07143},
	url = {http://arxiv.org/abs/2404.07143},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@misc{shazeerFastTransformerDecoding2019,
	title = {Fast {{Transformer Decoding}}: {{One Write-Head}} Is {{All You Need}}},
	shorttitle = {Fast {{Transformer Decoding}}},
	author = {Shazeer, Noam},
	year = {2019},
	month = nov,
	number = {arXiv:1911.02150},
	eprint = {1911.02150},
	publisher = {arXiv},
	doi = {10.48550/arXiv.1911.02150},
	url = {http://arxiv.org/abs/1911.02150},
	urldate = {2024-12-07},
	archiveprefix = {arXiv}
}

@inproceedings{DBLP:conf/emnlp/WanWLHZJW024,
  author       = {Zhongwei Wan and
                  Ziang Wu and
                  Che Liu and
                  Jinfa Huang and
                  Zhihong Zhu and
                  Peng Jin and
                  Longyue Wang and
                  Li Yuan},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {{LOOK-M:} Look-Once Optimization in {KV} Cache for Efficient Multimodal
                  Long-Context Inference},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2024, Miami, Florida, USA, November 12-16, 2024},
  pages        = {4065--4078},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.findings-emnlp.235},
  timestamp    = {Mon, 18 Nov 2024 09:05:59 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/WanWLHZJW024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%% inter layer
@misc{brandonReducingTransformerKeyValue2024,
	title = {Reducing {{Transformer Key-Value Cache Size}} with {{Cross-Layer Attention}}},
	author = {Brandon, William and Mishra, Mayank and Nrusimha, Aniruddha and Panda, Rameswar and Kelly, Jonathan Ragan},
	year = {2024},
	month = may,
	number = {arXiv:2405.12981},
	eprint = {2405.12981},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2405.12981},
	url = {http://arxiv.org/abs/2405.12981},
	urldate = {2024-12-07},
	archiveprefix = {arXiv}
}

@misc{chenDHALearningDecoupledHead2024,
	title = {{{DHA}}: {{Learning Decoupled-Head Attention}} from {{Transformer Checkpoints}} via {{Adaptive Heads Fusion}}},
	shorttitle = {{{DHA}}},
	author = {Chen, Yilong and Zhang, Linhao and Shang, Junyuan and Zhang, Zhenyu and Liu, Tingwen and Wang, Shuohuan and Sun, Yu},
	year = {2024},
	month = jun,
	number = {arXiv:2406.06567},
	eprint = {2406.06567},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2406.06567},
	url = {http://arxiv.org/abs/2406.06567},
	urldate = {2024-12-09},
	archiveprefix = {arXiv}
}

@misc{hoBlockTransformerGlobaltoLocal2024,
	title = {Block {{Transformer}}: {{Global-to-Local Language Modeling}} for {{Fast Inference}}},
	shorttitle = {Block {{Transformer}}},
	author = {Ho, Namgyu and Bae, Sangmin and Kim, Taehyeon and Jo, Hyunjik and Kim, Yireun and Schuster, Tal and Fisch, Adam and Thorne, James and Yun, Se-Young},
	year = {2024},
	month = nov,
	number = {arXiv:2406.02657},
	eprint = {2406.02657},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2406.02657},
	url = {http://arxiv.org/abs/2406.02657},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@misc{liaoKVCachingShared2024,
	title = {Beyond {{KV Caching}}: {{Shared Attention}} for {{Efficient LLMs}}},
	shorttitle = {Beyond {{KV Caching}}},
	author = {Liao, Bingli and Vargas, Danilo Vasconcellos},
	year = {2024},
	month = jul,
	number = {arXiv:2407.12866},
	eprint = {2407.12866},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2407.12866},
	url = {http://arxiv.org/abs/2407.12866},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@inproceedings{monteiroXCCacheCrossAttendingCached2024,
	title = {{{XC-Cache}}: {{Cross-Attending}} to {{Cached Context}} for {{Efficient LLM Inference}}},
	shorttitle = {{{XC-Cache}}},
	booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024, {{Miami}}, {{Florida}}, {{USA}}, {{November}} 12-16, 2024},
	author = {Monteiro, Jo{\~a}o and Marcotte, {\'E}tienne and No{\"e}l, Pierre-Andr{\'e} and Zantedeschi, Valentina and V{\'a}zquez, David and Chapados, Nicolas and Pal, Christopher and Taslakian, Perouz},
	editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	year = {2024},
	pages = {15284--15302},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/2024.findings-emnlp.896},
	urldate = {2024-12-08}
}

@misc{muCrosslayerAttentionSharing2024,
  title = {Cross-Layer {{Attention Sharing}} for {{Large Language Models}}},
  author = {Mu, Yongyu and Wu, Yuzhang and Fan, Yuchun and Wang, Chenglong and Li, Hengyu and He, Qiaozhi and Yang, Murun and Xiao, Tong and Zhu, Jingbo},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2408.01890},
  url = {https://arxiv.org/abs/2408.01890},
  urldate = {2024-12-09},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{sunYouOnlyCache2024,
	title = {You {{Only Cache Once}}: {{Decoder-Decoder Architectures}} for {{Language Models}}},
	shorttitle = {You {{Only Cache Once}}},
	author = {Sun, Yutao and Dong, Li and Zhu, Yi and Huang, Shaohan and Wang, Wenhui and Ma, Shuming and Zhang, Quanlu and Wang, Jianyong and Wei, Furu},
	year = {2024},
	month = may,
	number = {arXiv:2405.05254},
	eprint = {2405.05254},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2405.05254},
	url = {http://arxiv.org/abs/2405.05254},
	urldate = {2024-12-07},
	archiveprefix = {arXiv}
}

@inproceedings{wuLayerCondensedKVCache2024,
	title = {Layer-{{Condensed KV Cache}} for {{Efficient Inference}} of {{Large Language Models}}},
	booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}}), {{ACL}} 2024, {{Bangkok}}, {{Thailand}}, {{August}} 11-16, 2024},
	author = {Wu, Haoyi and Tu, Kewei},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	year = {2024},
	pages = {11175--11188},
	publisher = {Association for Computational Linguistics},
	doi = {10.18653/V1/2024.ACL-LONG.602},
	url = {https://doi.org/10.18653/v1/2024.acl-long.602},
	urldate = {2024-12-08}
}

@misc{wuSystematicStudyCrossLayer2024,
	title = {A {{Systematic Study}} of {{Cross-Layer KV Sharing}} for {{Efficient LLM Inference}}},
	author = {Wu, You and Wu, Haoyi and Tu, Kewei},
	year = {2024},
	month = oct,
	number = {arXiv:2410.14442},
	eprint = {2410.14442},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2410.14442},
	url = {http://arxiv.org/abs/2410.14442},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@misc{yangLosslessKVCache2024,
	title = {Lossless {{KV Cache Compression}} to 2\%},
	author = {Yang, Zhen and Han, J. N. and Wu, Kan and Xie, Ruobing and Wang, An and Sun, Xingwu and Kang, Zhanhui},
	year = {2024},
	month = oct,
	number = {arXiv:2410.15252},
	eprint = {2410.15252},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2410.15252},
	url = {http://arxiv.org/abs/2410.15252},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@inproceedings{yenLongContextLanguageModeling2024a,
	title = {Long-{{Context Language Modeling}} with {{Parallel Context Encoding}}},
	booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}}), {{ACL}} 2024, {{Bangkok}}, {{Thailand}}, {{August}} 11-16, 2024},
	author = {Yen, Howard and Gao, Tianyu and Chen, Danqi},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	year = {2024},
	pages = {2588--2610},
	publisher = {Association for Computational Linguistics},
	doi = {10.18653/V1/2024.ACL-LONG.142},
	url = {https://doi.org/10.18653/v1/2024.acl-long.142},
	urldate = {2024-12-08}
}

@misc{zhouValueResidualLearning2024,
	title = {Value {{Residual Learning For Alleviating Attention Concentration In Transformers}}},
	author = {Zhou, Zhanchao and Wu, Tianyi and Jiang, Zhiyun and Lan, Zhenzhong},
	year = {2024},
	month = dec,
	number = {arXiv:2410.17897},
	eprint = {2410.17897},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2410.17897},
	url = {http://arxiv.org/abs/2410.17897},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@misc{zuhriMLKVMultiLayerKeyValue2024,
	title = {{{MLKV}}: {{Multi-Layer Key-Value Heads}} for {{Memory Efficient Transformer Decoding}}},
	shorttitle = {{{MLKV}}},
	author = {Zuhri, Zayd Muhammad Kawakibi and Adilazuarda, Muhammad Farid and Purwarianti, Ayu and Aji, Alham Fikri},
	year = {2024},
	month = oct,
	number = {arXiv:2406.09297},
	eprint = {2406.09297},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2406.09297},
	url = {http://arxiv.org/abs/2406.09297},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

%% Non-Transformer structure?
@inproceedings{daoTransformersAreSSMs2024,
	title = {Transformers Are {{SSMs}}: {{Generalized Models}} and {{Efficient Algorithms Through Structured State Space Duality}}},
	shorttitle = {Transformers Are {{SSMs}}},
	booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}, {{ICML}} 2024, {{Vienna}}, {{Austria}}, {{July}} 21-27, 2024},
	author = {Dao, Tri and Gu, Albert},
	year = {2024},
	publisher = {OpenReview.net},
	url = {https://openreview.net/forum?id=ztn8FCR1td},
	urldate = {2024-12-08}
}

@misc{deGriffinMixingGated2024,
  title = {Griffin: {{Mixing Gated Linear Recurrences}} with {{Local Attention}} for {{Efficient Language Models}}},
  shorttitle = {Griffin},
  author = {De, Soham and Smith, Samuel L. and Fernando, Anushan and Botev, Aleksandar and {Cristian-Muraru}, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet, Arnaud and Budden, David and Teh, Yee Whye and Pascanu, Razvan and Freitas, Nando De and Gulcehre, Caglar},
  year = {2024},
  month = feb,
  number = {arXiv:2402.19427},
  eprint = {2402.19427},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.19427},
  url = {http://arxiv.org/abs/2402.19427},
  urldate = {2024-12-19},
  archiveprefix = {arXiv}
}


@misc{goldsteinGoldFinchHighPerformance2024,
	title = {{{GoldFinch}}: {{High Performance RWKV}}/{{Transformer Hybrid}} with {{Linear Pre-Fill}} and {{Extreme KV-Cache Compression}}},
	shorttitle = {{{GoldFinch}}},
	author = {Goldstein, Daniel and Obeid, Fares and Alcaide, Eric and Song, Guangyu and Cheah, Eugene},
	year = {2024},
	month = jul,
	number = {arXiv:2407.12077},
	eprint = {2407.12077},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2407.12077},
	url = {http://arxiv.org/abs/2407.12077},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}
@article{bondarenko2021understanding,
  title={Understanding and overcoming the challenges of efficient transformer quantization},
  author={Bondarenko, Yelysei and Nagel, Markus and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2109.12948},
  year={2021}
}

@article{wei2022outlier,
  title={Outlier suppression: Pushing the limit of low-bit transformer language models},
  author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17402--17414},
  year={2022}
}
@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{liu2024spinquant,
  title={SpinQuant--LLM quantization with learned rotations},
  author={Liu, Zechun and Zhao, Changsheng and Fedorov, Igor and Soran, Bilge and Choudhary, Dhruv and Krishnamoorthi, Raghuraman and Chandra, Vikas and Tian, Yuandong and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2405.16406},
  year={2024}
}

@misc{guMambaLinearTimeSequence2024,
	title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
	shorttitle = {Mamba},
	author = {Gu, Albert and Dao, Tri},
	year = {2024},
	month = may,
	number = {arXiv:2312.00752},
	eprint = {2312.00752},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2312.00752},
	url = {http://arxiv.org/abs/2312.00752},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@inproceedings{guCombiningRecurrentConvolutional2021,
  title = {Combining {{Recurrent}}, {{Convolutional}}, and {{Continuous-time Models}} with {{Linear State Space Layers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled Kamal and Dao, Tri and Rudra, Atri and Re, Christopher},
  year = {2021},
  month = nov,
  url = {https://openreview.net/forum?id=yWd42CWN3c},
  urldate = {2024-12-14},
  langid = {english}
}

@inproceedings{guParameterizationInitializationDiagonal2022,
  title = {On the {{Parameterization}} and {{Initialization}} of {{Diagonal State Space Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 35: {{Annual Conference}} on {{Neural Information Processing Systems}} 2022, {{NeurIPS}} 2022, {{New Orleans}}, {{LA}}, {{USA}}, {{November}} 28 - {{December}} 9, 2022},
  author = {Gu, Albert and Goel, Karan and Gupta, Ankit and R{\'e}, Christopher},
  editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
  year = {2022},
  publisher = {arXiv},
  urldate = {2024-12-14}
}

@misc{pengRWKVReinventingRNNs2023,
  title = {{{RWKV}}: {{Reinventing RNNs}} for the {{Transformer Era}}},
  shorttitle = {{{RWKV}}},
  author = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and He, Xuzheng and Hou, Haowen and Lin, Jiaju and Kazienko, Przemyslaw and Kocon, Jan and Kong, Jiaming and Koptyra, Bartlomiej and Lau, Hayden and Mantri, Krishna Sri Ipsit and Mom, Ferdinand and Saito, Atsushi and Song, Guangyu and Tang, Xiangru and Wang, Bolun and Wind, Johan S. and Wozniak, Stanislaw and Zhang, Ruichong and Zhang, Zhenyuan and Zhao, Qihang and Zhou, Peng and Zhou, Qinghua and Zhu, Jian and Zhu, Rui-Jie},
  year = {2023},
  month = dec,
  number = {arXiv:2305.13048},
  eprint = {2305.13048},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.13048},
  url = {http://arxiv.org/abs/2305.13048},
  urldate = {2024-12-14},
  archiveprefix = {arXiv}
}

@misc{sunRetentiveNetworkSuccessor2023,
	title = {Retentive {{Network}}: {{A Successor}} to {{Transformer}} for {{Large Language Models}}},
	shorttitle = {Retentive {{Network}}},
	author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
	year = {2023},
	month = aug,
	number = {arXiv:2307.08621},
	eprint = {2307.08621},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2307.08621},
	url = {http://arxiv.org/abs/2307.08621},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

@misc{wangLongLLaVAScalingMultimodal2024,
	title = {{{LongLLaVA}}: {{Scaling Multi-modal LLMs}} to 1000 {{Images Efficiently}} via a {{Hybrid Architecture}}},
	shorttitle = {{{LongLLaVA}}},
	author = {Wang, Xidong and Song, Dingjie and Chen, Shunian and Zhang, Chen and Wang, Benyou},
	year = {2024},
	month = oct,
	number = {arXiv:2409.02889},
	eprint = {2409.02889},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2409.02889},
	url = {http://arxiv.org/abs/2409.02889},
	urldate = {2024-12-09},
	archiveprefix = {arXiv}
}

@incollection{xuMixConHybridArchitecture2024,
	title = {{{MixCon}}: {{A Hybrid Architecture}} for {{Efficient}} and {{Adaptive Sequence Modeling}}},
	shorttitle = {{{MixCon}}},
	booktitle = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
	author = {Xu, Xin and Lin, Zhouchen},
	editor = {Endriss, Ulle and Melo, Francisco S. and Bach, Kerstin and {Bugar{\'i}n-Diz}, Alberto and {Alonso-Moral}, Jos{\'e} M. and Barro, Sen{\'e}n and Heintz, Fredrik},
	year = {2024},
	month = oct,
	publisher = {IOS Press},
	doi = {10.3233/FAIA240593},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA240593},
	urldate = {2024-12-09},
	copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
	isbn = {978-1-64368-548-9},
	langid = {english}
}

@article{yang2024kvsharerefficientinferencelayerwise,
  author       = {Yifei Yang and
                  Zouying Cao and
                  Qiguang Chen and
                  Libo Qin and
                  Dongjie Yang and
                  Hai Zhao and
                  Zhi Chen},
  title        = {KVSharer: Efficient Inference via Layer-Wise Dissimilar {KV} Cache
                  Sharing},
  journal      = {CoRR},
  volume       = {abs/2410.18517},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2410.18517},
  doi          = {10.48550/ARXIV.2410.18517},
  eprinttype    = {arXiv},
  eprint       = {2410.18517},
  timestamp    = {Fri, 29 Nov 2024 11:08:03 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-18517.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{yangMCSDEfficientLanguage2024,
	title = {{{MCSD}}: {{An Efficient Language Model}} with {{Diverse Fusion}}},
	shorttitle = {{{MCSD}}},
	author = {Yang, Hua and Li, Duohai and Li, Shiman},
	year = {2024},
	month = jul,
	number = {arXiv:2406.12230},
	eprint = {2406.12230},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2406.12230},
	url = {http://arxiv.org/abs/2406.12230},
	urldate = {2024-12-09},
	archiveprefix = {arXiv}
}

@article{DBLP:journals/corr/abs-2407-08454,
  author       = {Zheng Wang and
                  Boxiao Jin and
                  Zhongzhi Yu and
                  Minjia Zhang},
  title        = {Model Tells You Where to Merge: Adaptive {KV} Cache Merging for LLMs
                  on Long-Context Tasks},
  journal      = {CoRR},
  volume       = {abs/2407.08454},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.08454},
  doi          = {10.48550/ARXIV.2407.08454},
  eprinttype    = {arXiv},
  eprint       = {2407.08454},
  timestamp    = {Fri, 16 Aug 2024 14:50:29 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-08454.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{yanRecurFormerNotAll2024,
	title = {{{RecurFormer}}: {{Not All Transformer Heads Need Self-Attention}}},
	shorttitle = {{{RecurFormer}}},
	author = {Yan, Ruiqing and Zheng, Linghan and Du, Xingbo and Zou, Han and Guo, Yufeng and Yang, Jianfei},
	year = {2024},
	month = oct,
	number = {arXiv:2410.12850},
	eprint = {2410.12850},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2410.12850},
	url = {http://arxiv.org/abs/2410.12850},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

% not sure
@inproceedings{DBLP:conf/icml/0002DLZ00J24,
  author       = {Yuxin Zhang and
                  Yuxuan Du and
                  Gen Luo and
                  Yunshan Zhong and
                  Zhenyu Zhang and
                  Shiwei Liu and
                  Rongrong Ji},
  title        = {CaM: Cache Merging for Memory-efficient LLMs Inference},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=LCTmppB165},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/0002DLZ00J24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2406-13035,
  author       = {Zhongwei Wan and
                  Xinjian Wu and
                  Yu Zhang and
                  Yi Xin and
                  Chaofan Tao and
                  Zhihong Zhu and
                  Xin Wang and
                  Siqi Luo and
                  Jing Xiong and
                  Mi Zhang},
  title        = {{D2O:} Dynamic Discriminative Operations for Efficient Generative
                  Inference of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2406.13035},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.13035},
  doi          = {10.48550/ARXIV.2406.13035},
  eprinttype    = {arXiv},
  eprint       = {2406.13035},
  timestamp    = {Mon, 26 Aug 2024 14:12:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-13035.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{nawrotDynamicMemoryCompression2024,
	title = {Dynamic {{Memory Compression}}: {{Retrofitting LLMs}} for {{Accelerated Inference}}},
	shorttitle = {Dynamic {{Memory Compression}}},
	booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}, {{ICML}} 2024, {{Vienna}}, {{Austria}}, {{July}} 21-27, 2024},
	author = {Nawrot, Piotr and Lancucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M.},
	year = {2024},
	publisher = {OpenReview.net},
	url = {https://openreview.net/forum?id=tDRYrAkOB7},
	urldate = {2024-12-08}
}

@misc{wangLoMALosslessCompressed2024,
	title = {{{LoMA}}: {{Lossless Compressed Memory Attention}}},
	shorttitle = {{{LoMA}}},
	author = {Wang, Yumeng and Xiao, Zhenyang},
	year = {2024},
	month = feb,
	number = {arXiv:2401.09486},
	eprint = {2401.09486},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2401.09486},
	url = {http://arxiv.org/abs/2401.09486},
	urldate = {2024-12-08},
	archiveprefix = {arXiv}
}

### addition
@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{choromanski2020rethinking,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Quincy and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{ma2021luna,
  title={Luna: Linear unified nested attention},
  author={Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2441--2453},
  year={2021}
}

%%%%%%%%% END OF MODEL LEVEL
