
%%%%%%%%% BEGIN OF DATASET LEVEL
@misc{pal2023giraffeadventuresexpandingcontext,
      title={Giraffe: Adventures in Expanding Context Lengths in LLMs}, 
      author={Arka Pal and Deep Karkhanis and Manley Roberts and Samuel Dooley and Arvind Sundararajan and Siddartha Naidu},
      year={2023},
      eprint={2308.10882},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.10882}, 
}

@article{dong2023bamboo,
  title={BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models},
  author={Dong, Zican and Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2309.13345},
  year={2023}
}

@article{narrativeqa,
author = {Tom\'a\v s Ko\v cisk\'y and Jonathan Schwarz and Phil Blunsom and
          Chris Dyer and Karl Moritz Hermann and G\'abor Melis and
          Edward Grefenstette},
title = {The {NarrativeQA} Reading Comprehension Challenge},
journal = {Transactions of the Association for Computational Linguistics},
url = {https://TBD},
volume = {TBD},
year = {2018},
pages = {TBD},
}


@misc{bai_longbench:_2023,
	title = {{LongBench}: {A} {Bilingual}, {Multitask} {Benchmark} for {Long} {Context} {Understanding}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{LongBench}},
	url = {https://arxiv.org/abs/2308.14508},
	doi = {10.48550/ARXIV.2308.14508},
	abstract = {Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at https://github.com/THUDM/LongBench.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and Dong, Yuxiao and Tang, Jie and Li, Juanzi},
	year = {2023},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{dasigi_dataset_2021_qasper,
	title = {A {Dataset} of {Information}-{Seeking} {Questions} and {Answers} {Anchored} in {Research} {Papers}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2105.03011},
	doi = {10.48550/ARXIV.2105.03011},
	abstract = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
	year = {2021},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{yang_hotpotqa:_2018,
	title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
	shorttitle = {{HotpotQA}},
	url = {http://arxiv.org/abs/1809.09600},
	doi = {10.48550/arXiv.1809.09600},
	abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	month = sep,
	year = {2018},
	note = {arXiv:1809.09600},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ho_multihopqa_2020,
	address = {Barcelona, Spain (Online)},
	title = {Constructing {A} {Multi}-hop {QA} {Dataset} for {Comprehensive} {Evaluation} of {Reasoning} {Steps}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.580},
	doi = {10.18653/v1/2020.coling-main.580},
	language = {en},
	urldate = {2024-12-14},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Ho, Xanh and Duong Nguyen, Anh-Khoa and Sugawara, Saku and Aizawa, Akiko},
	year = {2020},
	pages = {6609--6625},
}

@misc{trivedi_musique:_2021,
	title = {{MuSiQue}: {Multihop} {Questions} via {Single}-hop {Question} {Composition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{MuSiQue}},
	url = {https://arxiv.org/abs/2108.00573},
	doi = {10.48550/ARXIV.2108.00573},
	abstract = {Multihop reasoning remains an elusive goal as existing multihop benchmarks are known to be largely solvable via shortcuts. Can we create a question answering (QA) dataset that, by construction, {\textbackslash}emph\{requires\} proper multihop reasoning? To this end, we introduce a bottom-up approach that systematically selects composable pairs of single-hop questions that are connected, i.e., where one reasoning step critically relies on information from another. This bottom-up methodology lets us explore a vast space of questions and add stringent filters as well as other mechanisms targeting connected reasoning. It provides fine-grained control over the construction process and the properties of the resulting \$k\$-hop questions. We use this methodology to create MuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop questions. Relative to existing datasets, MuSiQue-Ans is more difficult overall (3x increase in human-machine gap), and harder to cheat via disconnected reasoning (e.g., a single-hop model has a 30 point drop in F1). We further add unanswerable contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope our datasets will help the NLP community develop models that perform genuine multihop reasoning.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
	year = {2021},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{he_dureader:_2017,
	title = {{DuReader}: a {Chinese} {Machine} {Reading} {Comprehension} {Dataset} from {Real}-world {Applications}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{DuReader}},
	url = {https://arxiv.org/abs/1711.05073},
	doi = {10.48550/ARXIV.1711.05073},
	abstract = {This paper introduces DuReader, a new large-scale, open-domain Chinese ma- chine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated. (2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community. (3) scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far. Experiments show that human performance is well above current state-of-the-art baseline systems, leaving plenty of room for the community to make improvements. To help the community make these improvements, both DuReader and baseline systems have been posted online. We also organize a shared competition to encourage the exploration of more models. Since the release of the task, there are significant improvements over the baselines.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {He, Wei and Liu, Kai and Liu, Jing and Lyu, Yajuan and Zhao, Shiqi and Xiao, Xinyan and Liu, Yuan and Wang, Yizhong and Wu, Hua and She, Qiaoqiao and Liu, Xuan and Wu, Tian and Wang, Haifeng},
	year = {2017},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{joshi_triviaqa:_2017,
	title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{TriviaQA}},
	url = {https://arxiv.org/abs/1705.03551},
	doi = {10.48550/ARXIV.1705.03551},
	abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
	year = {2017},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{an_l-eval:_2023,
	title = {L-{Eval}: {Instituting} {Standardized} {Evaluation} for {Long} {Context} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {L-{Eval}},
	url = {https://arxiv.org/abs/2307.11088},
	doi = {10.48550/ARXIV.2307.11088},
	abstract = {Recently, there has been growing interest in extending the context length of large language models (LLMs), aiming to effectively process long inputs of one turn or conversations with more extensive histories. While proprietary models such as GPT-4 and Claude can largely preserve the reasoning ability in an extended context, open-source models are still progressing through the early stages of development. To bridge this gap, we propose L-Eval to institute a more standardized evaluation for long context language models (LCLMs) addressing two key aspects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k\${\textbackslash}sim\$200k tokens). On the other hand, we investigate the effectiveness in evalution metrics for LCLMs. Results show that popular n-gram matching metrics generally can not correlate well with human judgment, and thus we strongly advocate for length-instruction-enhanced (LIE) evaluation and employing LLM judges. We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of more principled evaluation of these models.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {An, Chenxin and Gong, Shansan and Zhong, Ming and Zhao, Xingjian and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
	year = {2023},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}


@article{kwiatkowski_nq_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	issn = {2307-387X},
	shorttitle = {Natural {Questions}},
	url = {https://direct.mit.edu/tacl/article/43518},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	language = {en},
	urldate = {2024-12-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	month = nov,
	year = {2019},
	pages = {453--466},
}


@misc{huang_govreport_2021,
	title = {Efficient {Attentions} for {Long} {Document} {Summarization}},
	url = {http://arxiv.org/abs/2104.02112},
	doi = {10.48550/arXiv.2104.02112},
	abstract = {The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
	month = apr,
	year = {2021},
	note = {arXiv:2104.02112},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhong_qmsum:_2021,
	title = {{QMSum}: {A} {New} {Benchmark} for {Query}-based {Multi}-domain {Meeting} {Summarization}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{QMSum}},
	url = {https://arxiv.org/abs/2104.05938},
	doi = {10.48550/ARXIV.2104.05938},
	abstract = {Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at {\textbackslash}url\{https://github.com/Yale-LILY/QMSum\}.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Awadallah, Ahmed Hassan and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and Radev, Dragomir},
	year = {2021},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{fabbri_multi-news:_2019,
	title = {Multi-{News}: a {Large}-{Scale} {Multi}-{Document} {Summarization} {Dataset} and {Abstractive} {Hierarchical} {Model}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Multi-{News}},
	url = {https://arxiv.org/abs/1906.01749},
	doi = {10.48550/ARXIV.1906.01749},
	abstract = {Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and release our data and code in hope that this work will promote advances in summarization in the multi-document setting.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Fabbri, Alexander R. and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir R.},
	year = {2019},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{wu_vcsum:_2023,
	title = {{VCSUM}: {A} {Versatile} {Chinese} {Meeting} {Summarization} {Dataset}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{VCSUM}},
	url = {https://arxiv.org/abs/2305.05280},
	doi = {10.48550/ARXIV.2305.05280},
	abstract = {Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at https://github.com/hahahawu/VCSum.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Wu, Han and Zhan, Mingjie and Tan, Haochen and Hou, Zhaohui and Liang, Ding and Song, Linqi},
	year = {2023},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@inproceedings{gliwa_samsum_2019,
	address = {Hong Kong, China},
	title = {{SAMSum} {Corpus}: {A} {Human}-annotated {Dialogue} {Dataset} for {Abstractive} {Summarization}},
	shorttitle = {{SAMSum} {Corpus}},
	url = {https://www.aclweb.org/anthology/D19-5409},
	doi = {10.18653/v1/D19-5409},
	language = {en},
	urldate = {2024-12-14},
	booktitle = {Proceedings of the 2nd {Workshop} on {New} {Frontiers} in {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Gliwa, Bogdan and Mochol, Iwona and Biesek, Maciej and Wawer, Aleksander},
	year = {2019},
	pages = {70--79},
}


@misc{chen_summscreen:_2021,
	title = {{SummScreen}: {A} {Dataset} for {Abstractive} {Screenplay} {Summarization}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{SummScreen}},
	url = {https://arxiv.org/abs/2104.07091},
	doi = {10.48550/ARXIV.2104.07091},
	abstract = {We introduce SummScreen, a summarization dataset comprised of pairs of TV series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be found and integrated to form the succinct plot descriptions in the recaps. Also, TV scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief. This information is rarely contained in recaps. Since characters are fundamental to TV series, we also propose two entity-centric evaluation metrics. Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors. An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts. Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors. Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Chen, Mingda and Chu, Zewei and Wiseman, Sam and Gimpel, Kevin},
	year = {2021},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{pang_quality_dataset:_2021,
	title = {{QuALITY}: {Question} {Answering} with {Long} {Input} {Texts}, {Yes}!},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{QuALITY}},
	url = {https://arxiv.org/abs/2112.08608},
	doi = {10.48550/ARXIV.2112.08608},
	abstract = {To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Our baseline models perform poorly on this task (55.4\%) and significantly lag behind human performance (93.5\%).},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and Bowman, Samuel R.},
	year = {2021},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@inproceedings{sharma_bigpatent:_2019,
	address = {Florence, Italy},
	title = {{BIGPATENT}: {A} {Large}-{Scale} {Dataset} for {Abstractive} and {Coherent} {Summarization}},
	shorttitle = {{BIGPATENT}},
	url = {https://www.aclweb.org/anthology/P19-1212},
	doi = {10.18653/v1/P19-1212},
	language = {en},
	urldate = {2024-12-14},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Eva and Li, Chen and Wang, Lu},
	year = {2019},
	pages = {2204--2213},
}

@article{angelidis_extractive_space_2021,
	title = {Extractive {Opinion} {Summarization} in {Quantized} {Transformer} {Spaces}},
	volume = {9},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00366/98621/Extractive-Opinion-Summarization-in-Quantized},
	doi = {10.1162/tacl_a_00366},
	abstract = {Abstract
            We present the Quantized Transformer (QT), an unsupervised system for extractive opinion summarization. QT is inspired by Vector- Quantized Variational Autoencoders, which we repurpose for popularity-driven summarization. It uses a clustering interpretation of the quantized space and a novel extraction algorithm to discover popular opinions among hundreds of reviews, a significant step towards opinion summarization of practical scope. In addition, QT enables controllable summarization without further training, by utilizing properties of the quantized space to extract aspect-specific summaries. We also make publicly available Space, a large-scale evaluation benchmark for opinion summarizers, comprising general and aspect-specific summaries for 50 hotels. Experiments demonstrate the promise of our approach, which is validated by human studies where judges showed clear preference for our method over competitive baselines.},
	language = {en},
	urldate = {2024-12-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Angelidis, Stefanos and Amplayo, Reinald Kim and Suhara, Yoshihiko and Wang, Xiaolan and Lapata, Mirella},
	month = mar,
	year = {2021},
	pages = {277--293},
}

@article{wang2022squality,
  title={S{Q}u{ALITY}: Building a Long-Document Summarization Dataset the Hard Way},
  author={Wang, Alex and Pang, Richard Yuanzhe and Chen, Angelica and Phang, Jason and Bowman, Samuel R.},
  journal={arXiv preprint 2205.11465},
  year={2022}
}

@misc{tay_long_lra_2020,
	title = {Long {Range} {Arena}: {A} {Benchmark} for {Efficient} {Transformers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Long {Range} {Arena}},
	url = {https://arxiv.org/abs/2011.04006},
	doi = {10.48550/ARXIV.2011.04006},
	abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from \$1K\$ to \$16K\$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
	year = {2020},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@inproceedings{koreeda-manning-2021-contractnli-dataset,
    title = "{C}ontract{NLI}: A Dataset for Document-level Natural Language Inference for Contracts",
    author = "Koreeda, Yuta  and
      Manning, Christopher",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.164",
    pages = "1907--1919",
    abstract = "Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose {``}document-level natural language inference (NLI) for contracts{''}, a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as {``}Some obligations of Agreement may survive termination.{''}) and a contract, and it is asked to classify whether each hypothesis is {``}entailed by{''}, {``}contradicting to{''} or {``}not mentioned by{''} (neutral to) the contract as well as identifying {``}evidence{''} for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (a) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (b) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement.",
}

@article{petukhova_mn-ds:_2023,
	title = {{MN}-{DS}: {A} {Multilabeled} {News} {Dataset} for {News} {Articles} {Hierarchical} {Classification}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2306-5729},
	shorttitle = {{MN}-{DS}},
	url = {https://www.mdpi.com/2306-5729/8/5/74},
	doi = {10.3390/data8050074},
	abstract = {This article presents a dataset of 10,917 news articles with hierarchical news categories collected between 1 January 2019 and 31 December 2019. We manually labeled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.},
	language = {en},
	number = {5},
	urldate = {2024-12-14},
	journal = {Data},
	author = {Petukhova, Alina and Fachada, Nuno},
	month = apr,
	year = {2023},
	pages = {74},
}

@misc{shaham_zeroscrolls:_2023,
	title = {{ZeroSCROLLS}: {A} {Zero}-{Shot} {Benchmark} for {Long} {Text} {Understanding}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{ZeroSCROLLS}},
	url = {https://arxiv.org/abs/2305.14196},
	doi = {10.48550/ARXIV.2305.14196},
	abstract = {We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Shaham, Uri and Ivgi, Maor and Efrat, Avia and Berant, Jonathan and Levy, Omer},
	year = {2023},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
}


@misc{guo_longcoder_lcc:_2023,
	title = {{LongCoder}: {A} {Long}-{Range} {Pre}-trained {Language} {Model} for {Code} {Completion}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{LongCoder}},
	url = {https://arxiv.org/abs/2306.14893},
	doi = {10.48550/ARXIV.2306.14893},
	abstract = {In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Guo, Daya and Xu, Canwen and Duan, Nan and Yin, Jian and McAuley, Julian},
	year = {2023},
	keywords = {Software Engineering (cs.SE), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@inproceedings{feng_multidoc2dial:_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{MultiDoc2Dial}: {Modeling} {Dialogues} {Grounded} in {Multiple} {Documents}},
	shorttitle = {{MultiDoc2Dial}},
	url = {https://aclanthology.org/2021.emnlp-main.498},
	doi = {10.18653/v1/2021.emnlp-main.498},
	language = {en},
	urldate = {2024-12-14},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Song and Patel, Siva Sankalp and Wan, Hui and Joshi, Sachindra},
	year = {2021},
	pages = {6162--6176},
}

@misc{yuan_can_asap_review_2021,
	title = {Can {We} {Automate} {Scientific} {Reviewing}?},
	copyright = {Creative Commons Zero v1.0 Universal},
	url = {https://arxiv.org/abs/2102.00176},
	doi = {10.48550/ARXIV.2102.00176},
	abstract = {The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question "can we automate scientific reviewing?", discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate first-pass peer reviews for scientific papers. Arguably the most difficult part of this is defining what a "good" review is in the first place, so we first discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We finally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github.com/neulab/ReviewAdvisor, as well as a ReviewAdvisor system: http://review.nlpedia.ai/.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Yuan, Weizhe and Liu, Pengfei and Neubig, Graham},
	year = {2021},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{longchat2023,
    title = {How Long Can Open-Source LLMs Truly Promise on Context Length?},
    url = {https://lmsys.org/blog/2023-06-29-longchat},
    author = {Dacheng Li* and Rulin Shao* and Anze Xie and Ying Sheng and Lianmin Zheng and Joseph E. Gonzalez and Ion Stoicaand Xuezhe Ma and and Hao Zhang},
    month = {June},
    year = {2023}
}

@misc{li_loogle:_2023,
	title = {{LooGLE}: {Can} {Long}-{Context} {Language} {Models} {Understand} {Long} {Contexts}?},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{LooGLE}},
	url = {https://arxiv.org/abs/2311.04939},
	doi = {10.48550/ARXIV.2311.04939},
	abstract = {Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards "true long-context understanding".},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
	year = {2023},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@inproceedings{nallapati_abstractive_dailymail_2016,
	address = {Berlin, Germany},
	title = {Abstractive {Text} {Summarization} using {Sequence}-to-sequence {RNNs} and {Beyond}},
	url = {http://aclweb.org/anthology/K16-1028},
	doi = {10.18653/v1/K16-1028},
	language = {en},
	urldate = {2024-12-15},
	booktitle = {Proceedings of {The} 20th {SIGNLL} {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Nallapati, Ramesh and Zhou, Bowen and Dos Santos, Cicero and Gulcehre, Caglar and Xiang, Bing},
	year = {2016},
	pages = {280--290},
}

@misc{narayan_dont_xsum_2018,
	title = {Don't {Give} {Me} the {Details}, {Just} the {Summary}! {Topic}-{Aware} {Convolutional} {Neural} {Networks} for {Extreme} {Summarization}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1808.08745},
	doi = {10.48550/ARXIV.1808.08745},
	abstract = {We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question "What is the article about?". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	year = {2018},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{cobbe_training——gsm8k_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2110.14168},
	doi = {10.48550/ARXIV.2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	year = {2021},
	keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{liu_repobench:_2023,
	title = {{RepoBench}: {Benchmarking} {Repository}-{Level} {Code} {Auto}-{Completion} {Systems}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{RepoBench}},
	url = {https://arxiv.org/abs/2306.03091},
	doi = {10.48550/ARXIV.2306.03091},
	abstract = {Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Liu, Tianyang and Xu, Canwen and McAuley, Julian},
	year = {2023},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
}


@misc{dinarelli_seq2biseq_penn_treebank:_2019,
	title = {{Seq2Biseq}: {Bidirectional} {Output}-wise {Recurrent} {Neural} {Networks} for {Sequence} {Modelling}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{Seq2Biseq}},
	url = {https://arxiv.org/abs/1904.04733},
	doi = {10.48550/ARXIV.1904.04733},
	abstract = {During the last couple of years, Recurrent Neural Networks (RNN) have reached state-of-the-art performances on most of the sequence modelling problems. In particular, the "sequence to sequence" model and the neural CRF have proved to be very effective in this domain. In this article, we propose a new RNN architecture for sequence labelling, leveraging gated recurrent layers to take arbitrarily long contexts into account, and using two decoders operating forward and backward. We compare several variants of the proposed solution and their performances to the state-of-the-art. Most of our results are better than the state-of-the-art or very close to it and thanks to the use of recent technologies, our architecture can scale on corpora larger than those used in this work.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Dinarelli, Marco and Grobol, Loïc},
	year = {2019},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
}


@inproceedings{zhu_ncls:_2019,
	address = {Hong Kong, China},
	title = {{NCLS}: {Neural} {Cross}-{Lingual} {Summarization}},
	shorttitle = {{NCLS}},
	url = {https://www.aclweb.org/anthology/D19-1302},
	doi = {10.18653/v1/D19-1302},
	language = {en},
	urldate = {2024-12-16},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Junnan and Wang, Qian and Wang, Yining and Zhou, Yu and Zhang, Jiajun and Wang, Shaonan and Zong, Chengqing},
	year = {2019},
	pages = {3052--3062},
}

@article{kwan_m4le:_2023,
  title={M4le: A multi-ability multi-range multi-task multi-domain long-context evaluation benchmark for large language models},
  author={Kwan and Wai-Chung and Zeng and Xingshan and Wang and Yufei and Sun and Yusen and Liand Liangyou and Shang and Lifeng and Liu and Qun and Wong and Kam-Fai},
  journal={arXiv preprint arXiv:2310.19240},
  year={2023}
}


@misc{hendrycks_cuad:_2021,
	title = {{CUAD}: {An} {Expert}-{Annotated} {NLP} {Dataset} for {Legal} {Contract} {Review}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{CUAD}},
	url = {https://arxiv.org/abs/2103.06268},
	doi = {10.48550/ARXIV.2103.06268},
	abstract = {Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.},
	urldate = {2024-12-21},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
	year = {2021},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
}


@misc{feng_doc2dial:_2020,
	title = {doc2dial: {A} {Goal}-{Oriented} {Document}-{Grounded} {Dialogue} {Dataset}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {doc2dial},
	url = {https://arxiv.org/abs/2011.06623},
	doi = {10.48550/ARXIV.2011.06623},
	abstract = {We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes about 4800 annotated conversations with an average of 14 turns that are grounded in over 480 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.},
	urldate = {2024-12-22},
	publisher = {arXiv},
	author = {Feng, Song and Wan, Hui and Gunasekara, Chulaka and Patel, Siva Sankalp and Joshi, Sachindra and Lastras, Luis A.},
	year = {2020},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}


@misc{shaham_scrolls:_2022,
	title = {{SCROLLS}: {Standardized} {CompaRison} {Over} {Long} {Language} {Sequences}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{SCROLLS}},
	url = {https://arxiv.org/abs/2201.03533},
	doi = {10.48550/ARXIV.2201.03533},
	abstract = {NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including Longformer Encoder-Decoder, indicate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.},
	urldate = {2024-12-22},
	publisher = {arXiv},
	author = {Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and Levy, Omer},
	year = {2022},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{tseng_towards_tofelqa_2016,
	title = {Towards {Machine} {Comprehension} of {Spoken} {Content}: {Initial} {TOEFL} {Listening} {Comprehension} {Test} by {Machine}},
	shorttitle = {Towards {Machine} {Comprehension} of {Spoken} {Content}},
	url = {http://arxiv.org/abs/1608.06378},
	doi = {10.48550/arXiv.1608.06378},
	abstract = {Multimedia or spoken content presents more attractive information than plain text content, but it's more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors.},
	urldate = {2024-12-22},
	publisher = {arXiv},
	author = {Tseng, Bo-Hsiang and Shen, Sheng-Syun and Lee, Hung-Yi and Lee, Lin-Shan},
	month = aug,
	year = {2016},
	note = {arXiv:1608.06378},
	keywords = {Computer Science - Computation and Language},
}

@misc{sfgram,
  author = {Schaetti, Nils},
  title = {SFGram: a dataset containing thousands of scienc-fiction books and novels},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/nschaetti/EchoTorch}},
}


@book{zhu_natural_clts_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science} {Ser}},
	title = {Natural {Language} {Processing} and {Chinese} {Computing}: 9th {CCF} {International} {Conference}, {NLPCC} 2020, {Zhengzhou}, {China}, {October} 14-18, 2020, {Proceedings}, {Part} {I}},
	isbn = {9783030604509},
	shorttitle = {Natural {Language} {Processing} and {Chinese} {Computing}},
	abstract = {Intro -- Preface -- Organization -- Contents - Part I -- Contents - Part II -- Oral - Conversational Bot/QA -- FAQ-Based Question Answering via Knowledge Anchors -- 1 Introduction -- 2 Related Work -- 3 Methodology -- 3.1 Overall Architecture -- 3.2 Knowledge Graph Construction -- 3.3 Query Anchoring -- 3.4 Multi-channel Query-Document Matching -- 3.5 Implementation Details -- 4 Experiments -- 4.1 Dataset and Knowledge Graph -- 4.2 Experimental Settings -- 4.3 Online and Offline Query-Document Matching -- 4.4 Analysis on Query Anchoring -- 4.5 Ablation Tests on Knowledge Anchors -- 4.6 Case Study -- 5 Conclusion and Future Work -- References -- Deep Hierarchical Attention Flow for Visual Commonsense Reasoning -- 1 Introduction -- 2 Related Work -- 2.1 Visual Question Answering -- 2.2 Visual Commonsense Reasoning -- 3 Approach -- 3.1 Overview -- 3.2 Feature Representation -- 3.3 Language and Vision Fusion -- 3.4 Response Modeling -- 3.5 Correlation Comparison -- 3.6 Implementation and Training Details -- 4 Experiments -- 4.1 Datasets and Evaluation Metrics -- 4.2 Results, Ablations, and Analysis -- 5 Conclusion -- References -- Dynamic Reasoning Network for Multi-hop Question Answering -- 1 Introduction -- 2 Related Work -- 3 Model -- 3.1 Task Definition and Overview -- 3.2 Passage Selector -- 3.3 Encoding Query and Context -- 3.4 Entity Graph Construction -- 3.5 Dynamic Reasoning Network -- 3.6 Answer Prediction -- 4 Experiments -- 4.1 Datasets and Evaluation Metrics -- 4.2 Implementation Details -- 4.3 Overall Results -- 4.4 Ablation Studies -- 4.5 Results Analysis -- 5 Conclusion -- References -- Memory Attention Neural Network for Multi-domain Dialogue State Tracking -- 1 Introduction -- 2 Related Work -- 3 Proposed Framework -- 3.1 Input Encoding Module -- 3.2 Context-Aware Slot Embedding -- 3.3 Multi-hop Gated Memory Network},
	language = {eng},
	number = {v.12430},
	publisher = {Springer International Publishing AG},
	author = {Zhu, Xiaodan},
	collaborator = {Zhang, Min and Hong, Yu and He, Ruifang},
	year = {2020},
}

@inproceedings{denkowski-lavie-2011-meteor,
    title = "Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems",
    author = "Denkowski, Michael  and
      Lavie, Alon",
    editor = "Callison-Burch, Chris  and
      Koehn, Philipp  and
      Monz, Christof  and
      Zaidan, Omar F.",
    booktitle = "Proceedings of the Sixth Workshop on Statistical Machine Translation",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-2107",
    pages = "85--91",
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{rajpurkar-etal-2016-squad-F1,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{post-2018-call-sacrebleu,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}

@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@article{DBLP:journals/corr/RajpurkarZLL16,
  author       = {Pranav Rajpurkar and
                  Jian Zhang and
                  Konstantin Lopyrev and
                  Percy Liang},
  title        = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  journal      = {CoRR},
  volume       = {abs/1606.05250},
  year         = {2016},
  url          = {http://arxiv.org/abs/1606.05250},
  eprinttype    = {arXiv},
  eprint       = {1606.05250},
  timestamp    = {Mon, 24 Aug 2020 14:01:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RajpurkarZLL16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Voorhees1999TheTQ,
  title={The TREC-8 Question Answering Track Report},
  author={Ellen M. Voorhees},
  booktitle={Text Retrieval Conference},
  year={1999},
  url={https://api.semanticscholar.org/CorpusID:16944215}
}

@misc{chen2021evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}