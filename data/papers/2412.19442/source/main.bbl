% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{hadi2023survey}
M.~U. Hadi, R.~Qureshi, A.~Shah, M.~Irfan, A.~Zafar, M.~B. Shaikh, N.~Akhtar,
  J.~Wu, S.~Mirjalili \emph{et~al.}, ``A survey on large language models:
  Applications, challenges, limitations, and practical usage,'' \emph{Authorea
  Preprints}, 2023.

\bibitem{zhu2023survey}
X.~Zhu, J.~Li, Y.~Liu, C.~Ma, and W.~Wang, ``A survey on model compression for
  large language models,'' \emph{arXiv preprint arXiv:2308.07633}, 2023.

\bibitem{DBLP:journals/corr/abs-2307-06435}
\BIBentryALTinterwordspacing
H.~Naveed, A.~U. Khan, S.~Qiu, M.~Saqib, S.~Anwar, M.~Usman, N.~Barnes, and
  A.~Mian, ``A comprehensive overview of large language models,'' \emph{CoRR},
  vol. abs/2307.06435, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2307.06435}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/csur/MinRSVNSAHR24}
\BIBentryALTinterwordspacing
B.~Min, H.~Ross, E.~Sulem, A.~P.~B. Veyseh, T.~H. Nguyen, O.~Sainz, E.~Agirre,
  I.~Heintz, and D.~Roth, ``Recent advances in natural language processing via
  large pre-trained language models: {A} survey,'' \emph{{ACM} Comput. Surv.},
  vol.~56, no.~2, pp. 30:1--30:40, 2024. [Online]. Available:
  \url{https://doi.org/10.1145/3605943}
\BIBentrySTDinterwordspacing

\bibitem{xu2024large}
D.~Xu, W.~Chen, W.~Peng, C.~Zhang, T.~Xu, X.~Zhao, X.~Wu, Y.~Zheng, Y.~Wang,
  and E.~Chen, ``Large language models for generative information extraction: A
  survey,'' \emph{Frontiers of Computer Science}, vol.~18, no.~6, p. 186357,
  2024.

\bibitem{DBLP:conf/nips/LiuLWL23a}
\BIBentryALTinterwordspacing
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee, ``Visual instruction tuning,'' in
  \emph{Advances in Neural Information Processing Systems 36: Annual Conference
  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
  USA, December 10 - 16, 2023}, A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko,
  M.~Hardt, and S.~Levine, Eds., 2023. [Online]. Available:
  \url{http://papers.nips.cc/paper\_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/pami/ZhangHJL24}
\BIBentryALTinterwordspacing
J.~Zhang, J.~Huang, S.~Jin, and S.~Lu, ``Vision-language models for vision
  tasks: {A} survey,'' \emph{{IEEE} Trans. Pattern Anal. Mach. Intell.},
  vol.~46, no.~8, pp. 5625--5644, 2024. [Online]. Available:
  \url{https://doi.org/10.1109/TPAMI.2024.3369699}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2306-16410}
\BIBentryALTinterwordspacing
W.~Berrios, G.~Mittal, T.~Thrush, D.~Kiela, and A.~Singh, ``Towards language
  models that can see: Computer vision through the {LENS} of natural
  language,'' \emph{CoRR}, vol. abs/2306.16410, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2306.16410}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/acl/ZhangY0L0C024}
\BIBentryALTinterwordspacing
D.~Zhang, Y.~Yu, J.~Dong, C.~Li, D.~Su, C.~Chu, and D.~Yu, ``Mm-llms: Recent
  advances in multimodal large language models,'' in \emph{Findings of the
  Association for Computational Linguistics, {ACL} 2024, Bangkok, Thailand and
  virtual meeting, August 11-16, 2024}, L.~Ku, A.~Martins, and V.~Srikumar,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2024, pp. 12\,401--12\,430. [Online]. Available:
  \url{https://doi.org/10.18653/v1/2024.findings-acl.738}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24}
\BIBentryALTinterwordspacing
C.~Cui, Y.~Ma, X.~Cao, W.~Ye, Y.~Zhou, K.~Liang, J.~Chen, J.~Lu, Z.~Yang,
  K.~Liao, T.~Gao, E.~Li, K.~Tang, Z.~Cao, T.~Zhou, A.~Liu, X.~Yan, S.~Mei,
  J.~Cao, Z.~Wang, and C.~Zheng, ``A survey on multimodal large language models
  for autonomous driving,'' in \emph{{IEEE/CVF} Winter Conference on
  Applications of Computer Vision Workshops, {WACVW} 2024 - Workshops,
  Waikoloa, HI, USA, January 1-6, 2024}.\hskip 1em plus 0.5em minus 0.4em\relax
  {IEEE}, 2024, pp. 958--979. [Online]. Available:
  \url{https://doi.org/10.1109/WACVW60836.2024.00106}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/bigdataconf/WuGCWY23}
\BIBentryALTinterwordspacing
J.~Wu, W.~Gan, Z.~Chen, S.~Wan, and P.~S. Yu, ``Multimodal large language
  models: {A} survey,'' in \emph{{IEEE} International Conference on Big Data,
  BigData 2023, Sorrento, Italy, December 15-18, 2023}, J.~He, T.~Palpanas,
  X.~Hu, A.~Cuzzocrea, D.~Dou, D.~Slezak, W.~Wang, A.~Gruca, J.~C. Lin, and
  R.~Agrawal, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, 2023, pp.
  2247--2256. [Online]. Available:
  \url{https://doi.org/10.1109/BigData59044.2023.10386743}
\BIBentrySTDinterwordspacing

\bibitem{jin2023large}
M.~Jin, Q.~Wen, Y.~Liang, C.~Zhang, S.~Xue, X.~Wang, J.~Zhang, Y.~Wang,
  H.~Chen, X.~Li \emph{et~al.}, ``Large models for time series and
  spatio-temporal data: A survey and outlook,'' \emph{arXiv preprint
  arXiv:2310.10196}, 2023.

\bibitem{ma2024survey}
Q.~Ma, Z.~Liu, Z.~Zheng, Z.~Huang, S.~Zhu, Z.~Yu, and J.~T. Kwok, ``A survey on
  time-series pre-trained models,'' \emph{IEEE Transactions on Knowledge and
  Data Engineering}, 2024.

\bibitem{tan2023user}
Z.~Tan and M.~Jiang, ``User modeling in the era of large language models:
  Current research and future directions,'' \emph{arXiv preprint
  arXiv:2312.11518}, 2023.

\bibitem{wu2024survey}
L.~Wu, Z.~Zheng, Z.~Qiu, H.~Wang, H.~Gu, T.~Shen, C.~Qin, C.~Zhu, H.~Zhu,
  Q.~Liu \emph{et~al.}, ``A survey on large language models for
  recommendation,'' \emph{World Wide Web}, vol.~27, no.~5, p.~60, 2024.

\bibitem{yang2023llm4drive}
Z.~Yang, X.~Jia, H.~Li, and J.~Yan, ``Llm4drive: A survey of large language
  models for autonomous driving,'' in \emph{NeurIPS 2024 Workshop on Open-World
  Agents}, 2023.

\bibitem{chen2024driving}
L.~Chen, O.~Sinavski, J.~H{\"u}nermann, A.~Karnsund, A.~J. Willmott, D.~Birch,
  D.~Maund, and J.~Shotton, ``Driving with llms: Fusing object-level vector
  modality for explainable autonomous driving,'' in \emph{2024 IEEE
  International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2024, pp. 14\,093--14\,100.

\bibitem{fu2024drive}
D.~Fu, X.~Li, L.~Wen, M.~Dou, P.~Cai, B.~Shi, and Y.~Qiao, ``Drive like a
  human: Rethinking autonomous driving with large language models,'' in
  \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision}, 2024, pp. 910--919.

\bibitem{qiu2023large}
J.~Qiu, L.~Li, J.~Sun, J.~Peng, P.~Shi, R.~Zhang, Y.~Dong, K.~Lam, F.~P.-W. Lo,
  B.~Xiao \emph{et~al.}, ``Large ai models in health informatics: Applications,
  challenges, and the future,'' \emph{IEEE Journal of Biomedical and Health
  Informatics}, 2023.

\bibitem{zhou2023survey1}
H.~Zhou, F.~Liu, B.~Gu, X.~Zou, J.~Huang, J.~Wu, Y.~Li, S.~S. Chen, P.~Zhou,
  J.~Liu \emph{et~al.}, ``A survey of large language models in medicine:
  Progress, application, and challenge,'' \emph{arXiv preprint
  arXiv:2311.05112}, 2023.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' 2018.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  vol.~1, no.~8, p.~9, 2019.

\bibitem{DBLP:journals/corr/abs-2302-13971}
\BIBentryALTinterwordspacing
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.~Lachaux, T.~Lacroix,
  B.~Rozi{\`{e}}re, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin,
  E.~Grave, and G.~Lample, ``Llama: Open and efficient foundation language
  models,'' \emph{CoRR}, vol. abs/2302.13971, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2302.13971}
\BIBentrySTDinterwordspacing

\bibitem{dubey2024llama}
A.~Dubey, A.~Jauhri, A.~Pandey, A.~Kadian, A.~Al-Dahle, A.~Letman, A.~Mathur,
  A.~Schelten, A.~Yang, A.~Fan \emph{et~al.}, ``The llama 3 herd of models,''
  \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem{DBLP:conf/acl/DaiDZXGCLZYWXLH24}
\BIBentryALTinterwordspacing
D.~Dai, C.~Deng, C.~Zhao, R.~X. Xu, H.~Gao, D.~Chen, J.~Li, W.~Zeng, X.~Yu,
  Y.~Wu, Z.~Xie, Y.~K. Li, P.~Huang, F.~Luo, C.~Ruan, Z.~Sui, and W.~Liang,
  ``Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts
  language models,'' in \emph{Proceedings of the 62nd Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers), {ACL}
  2024, Bangkok, Thailand, August 11-16, 2024}, L.~Ku, A.~Martins, and
  V.~Srikumar, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2024, pp. 1280--1297. [Online]. Available:
  \url{https://doi.org/10.18653/v1/2024.acl-long.70}
\BIBentrySTDinterwordspacing

\bibitem{deepseek-aiDeepSeekV2StrongEconomical2024}
\BIBentryALTinterwordspacing
{DeepSeek-AI}, A.~Liu, B.~Feng, B.~Wang, B.~Wang, B.~Liu, C.~Zhao, C.~Dengr,
  C.~Ruan, D.~Dai, D.~Guo, D.~Yang, D.~Chen, D.~Ji, E.~Li, F.~Lin, F.~Luo,
  G.~Hao, G.~Chen, G.~Li, H.~Zhang, H.~Xu, H.~Yang, H.~Zhang, H.~Ding, H.~Xin,
  H.~Gao, H.~Li, H.~Qu, J.~L. Cai, J.~Liang, J.~Guo, J.~Ni, J.~Li, J.~Chen,
  J.~Yuan, J.~Qiu, J.~Song, K.~Dong, K.~Gao, K.~Guan, L.~Wang, L.~Zhang, L.~Xu,
  L.~Xia, L.~Zhao, L.~Zhang, M.~Li, M.~Wang, M.~Zhang, M.~Zhang, M.~Tang,
  M.~Li, N.~Tian, P.~Huang, P.~Wang, P.~Zhang, Q.~Zhu, Q.~Chen, Q.~Du, R.~J.
  Chen, R.~L. Jin, R.~Ge, R.~Pan, R.~Xu, R.~Chen, S.~S. Li, S.~Lu, S.~Zhou,
  S.~Chen, S.~Wu, S.~Ye, S.~Ma, S.~Wang, S.~Zhou, S.~Yu, S.~Zhou, S.~Zheng,
  T.~Wang, T.~Pei, T.~Yuan, T.~Sun, W.~L. Xiao, W.~Zeng, W.~An, W.~Liu,
  W.~Liang, W.~Gao, W.~Zhang, X.~Q. Li, X.~Jin, X.~Wang, X.~Bi, X.~Liu,
  X.~Wang, X.~Shen, X.~Chen, X.~Chen, X.~Nie, X.~Sun, X.~Wang, X.~Liu, X.~Xie,
  X.~Yu, X.~Song, X.~Zhou, X.~Yang, X.~Lu, X.~Su, Y.~Wu, Y.~K. Li, Y.~X. Wei,
  Y.~X. Zhu, Y.~Xu, Y.~Huang, Y.~Li, Y.~Zhao, Y.~Sun, Y.~Li, Y.~Wang, Y.~Zheng,
  Y.~Zhang, Y.~Xiong, Y.~Zhao, Y.~He, Y.~Tang, Y.~Piao, Y.~Dong, Y.~Tan,
  Y.~Liu, Y.~Wang, Y.~Guo, Y.~Zhu, Y.~Wang, Y.~Zou, Y.~Zha, Y.~Ma, Y.~Yan,
  Y.~You, Y.~Liu, Z.~Z. Ren, Z.~Ren, Z.~Sha, Z.~Fu, Z.~Huang, Z.~Zhang, Z.~Xie,
  Z.~Hao, Z.~Shao, Z.~Wen, Z.~Xu, Z.~Zhang, Z.~Li, Z.~Wang, Z.~Gu, Z.~Li, and
  Z.~Xie, ``{{DeepSeek-V2}}: {{A Strong}}, {{Economical}}, and {{Efficient
  Mixture-of-Experts Language Model}},'' Jun. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2405.04434}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2403-05525}
\BIBentryALTinterwordspacing
H.~Lu, W.~Liu, B.~Zhang, B.~Wang, K.~Dong, B.~Liu, J.~Sun, T.~Ren, Z.~Li,
  H.~Yang, Y.~Sun, C.~Deng, H.~Xu, Z.~Xie, and C.~Ruan, ``Deepseek-vl: Towards
  real-world vision-language understanding,'' \emph{CoRR}, vol. abs/2403.05525,
  2024. [Online]. Available: \url{https://doi.org/10.48550/arXiv.2403.05525}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2401-04088}
\BIBentryALTinterwordspacing
A.~Q. Jiang, A.~Sablayrolles, A.~Roux, A.~Mensch, B.~Savary, C.~Bamford, D.~S.
  Chaplot, D.~de~Las~Casas, E.~B. Hanna, F.~Bressand, G.~Lengyel, G.~Bour,
  G.~Lample, L.~R. Lavaud, L.~Saulnier, M.~Lachaux, P.~Stock, S.~Subramanian,
  S.~Yang, S.~Antoniak, T.~L. Scao, T.~Gervet, T.~Lavril, T.~Wang, T.~Lacroix,
  and W.~E. Sayed, ``Mixtral of experts,'' \emph{CoRR}, vol. abs/2401.04088,
  2024. [Online]. Available: \url{https://doi.org/10.48550/arXiv.2401.04088}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2310-06825}
\BIBentryALTinterwordspacing
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot,
  D.~de~Las~Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, L.~R.
  Lavaud, M.~Lachaux, P.~Stock, T.~L. Scao, T.~Lavril, T.~Wang, T.~Lacroix, and
  W.~E. Sayed, ``Mistral 7b,'' \emph{CoRR}, vol. abs/2310.06825, 2023.
  [Online]. Available: \url{https://doi.org/10.48550/arXiv.2310.06825}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/iclr/ZengLDWL0YXZXTM23}
\BIBentryALTinterwordspacing
A.~Zeng, X.~Liu, Z.~Du, Z.~Wang, H.~Lai, M.~Ding, Z.~Yang, Y.~Xu, W.~Zheng,
  X.~Xia, W.~L. Tam, Z.~Ma, Y.~Xue, J.~Zhai, W.~Chen, Z.~Liu, P.~Zhang,
  Y.~Dong, and J.~Tang, ``{GLM-130B:} an open bilingual pre-trained model,'' in
  \emph{The Eleventh International Conference on Learning Representations,
  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}.\hskip 1em plus 0.5em minus
  0.4em\relax OpenReview.net, 2023. [Online]. Available:
  \url{https://openreview.net/forum?id=-Aw0rrrPUF}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/acl/DuQLDQY022}
\BIBentryALTinterwordspacing
Z.~Du, Y.~Qian, X.~Liu, M.~Ding, J.~Qiu, Z.~Yang, and J.~Tang, ``{GLM:} general
  language model pretraining with autoregressive blank infilling,'' in
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, S.~Muresan, P.~Nakov, and A.~Villavicencio,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2022, pp. 320--335. [Online]. Available:
  \url{https://doi.org/10.18653/v1/2022.acl-long.26}
\BIBentrySTDinterwordspacing

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{gracioli2015survey}
G.~Gracioli, A.~Alhammad, R.~Mancuso, A.~A. Fr{\"o}hlich, and R.~Pellizzoni,
  ``A survey on cache management mechanisms for real-time embedded systems,''
  \emph{ACM Computing Surveys (CSUR)}, vol.~48, no.~2, pp. 1--36, 2015.

\bibitem{podlipnig2003survey}
S.~Podlipnig and L.~B{\"o}sz{\"o}rmenyi, ``A survey of web cache replacement
  strategies,'' \emph{ACM Computing Surveys (CSUR)}, vol.~35, no.~4, pp.
  374--398, 2003.

\bibitem{DBLP:conf/cikm/LiC21}
\BIBentryALTinterwordspacing
H.~Li and L.~Chen, ``Cache-based {GNN} system for dynamic graphs,'' in
  \emph{{CIKM} '21: The 30th {ACM} International Conference on Information and
  Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5,
  2021}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2021, pp. 937--946.
  [Online]. Available: \url{https://doi.org/10.1145/3459637.3482237}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/pacmmod/LiSCY23}
\BIBentryALTinterwordspacing
Y.~Li, Y.~Shen, L.~Chen, and M.~Yuan, ``Orca: Scalable temporal graph neural
  network training with theoretical guarantees,'' \emph{Proc. {ACM} Manag.
  Data}, vol.~1, no.~1, pp. 52:1--52:27, 2023. [Online]. Available:
  \url{https://doi.org/10.1145/3588737}
\BIBentrySTDinterwordspacing

\bibitem{lin2020pagraph}
Z.~Lin, C.~Li, Y.~Miao, Y.~Liu, and Y.~Xu, ``Pagraph: Scaling gnn training on
  large graphs via computation-aware caching,'' in \emph{Proceedings of the
  11th ACM Symposium on Cloud Computing}, 2020, pp. 401--415.

\bibitem{zhuang2023survey}
B.~Zhuang, J.~Liu, Z.~Pan, H.~He, Y.~Weng, and C.~Shen, ``A survey on efficient
  training of transformers,'' \emph{arXiv preprint arXiv:2302.01107}, 2023.

\bibitem{park2024comprehensive}
S.~Park, J.~Choi, S.~Lee, and U.~Kang, ``A comprehensive survey of compression
  algorithms for language models,'' \emph{arXiv preprint arXiv:2401.15347},
  2024.

\bibitem{wang2024model}
W.~Wang, W.~Chen, Y.~Luo, Y.~Long, Z.~Lin, L.~Zhang, B.~Lin, D.~Cai, and X.~He,
  ``Model compression and efficient inference for large language models: A
  survey,'' \emph{arXiv preprint arXiv:2402.09748}, 2024.

\bibitem{ding2023efficiency}
T.~Ding, T.~Chen, H.~Zhu, J.~Jiang, Y.~Zhong, J.~Zhou, G.~Wang, Z.~Zhu,
  I.~Zharkov, and L.~Liang, ``The efficiency spectrum of large language models:
  An algorithmic survey,'' \emph{arXiv preprint arXiv:2312.00678}, 2023.

\bibitem{miao2023towards}
X.~Miao, G.~Oliaro, Z.~Zhang, X.~Cheng, H.~Jin, T.~Chen, and Z.~Jia, ``Towards
  efficient generative large language model serving: A survey from algorithms
  to systems,'' \emph{arXiv preprint arXiv:2312.15234}, 2023.

\bibitem{wan2023efficient}
Z.~Wan, X.~Wang, C.~Liu, S.~Alam, Y.~Zheng, Z.~Qu, S.~Yan, Y.~Zhu, Q.~Zhang,
  M.~Chowdhury \emph{et~al.}, ``Efficient large language models: A survey,''
  \emph{arXiv preprint arXiv:2312.03863}, vol.~1, 2023.

\bibitem{zhou2024survey}
Z.~Zhou, X.~Ning, K.~Hong, T.~Fu, J.~Xu, S.~Li, Y.~Lou, L.~Wang, Z.~Yuan, X.~Li
  \emph{et~al.}, ``A survey on efficient inference for large language models,''
  \emph{arXiv preprint arXiv:2404.14294}, 2024.

\bibitem{tang2024survey}
Y.~Tang, Y.~Wang, J.~Guo, Z.~Tu, K.~Han, H.~Hu, and D.~Tao, ``A survey on
  transformer compression,'' \emph{arXiv preprint arXiv:2402.05964}, 2024.

\bibitem{kachris2024survey}
C.~Kachris, ``A survey on hardware accelerators for large language models,''
  \emph{arXiv preprint arXiv:2401.09890}, 2024.

\bibitem{xu2023parameter}
L.~Xu, H.~Xie, S.-Z.~J. Qin, X.~Tao, and F.~L. Wang, ``Parameter-efficient
  fine-tuning methods for pretrained language models: A critical review and
  assessment,'' \emph{arXiv preprint arXiv:2312.12148}, 2023.

\bibitem{albalak2024survey}
A.~Albalak, Y.~Elazar, S.~M. Xie, S.~Longpre, N.~Lambert, X.~Wang,
  N.~Muennighoff, B.~Hou, L.~Pan, H.~Jeong \emph{et~al.}, ``A survey on data
  selection for language models,'' \emph{arXiv preprint arXiv:2402.16827},
  2024.

\bibitem{Awesome-LLM-Inference@2024}
\BIBentryALTinterwordspacing
e.~Zefan-Cai, ``Awesome-llm-kv-cache: A curated list of awesome llm inference
  papers with codes,'' 2024, open-source software available at
  https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache. [Online]. Available:
  \url{https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache}
\BIBentrySTDinterwordspacing

\bibitem{xia2024unlocking}
H.~Xia, Z.~Yang, Q.~Dong, P.~Wang, Y.~Li, T.~Ge, T.~Liu, W.~Li, and Z.~Sui,
  ``Unlocking efficiency in large language model inference: A comprehensive
  survey of speculative decoding,'' \emph{arXiv preprint arXiv:2401.07851},
  2024.

\bibitem{leviathan2023fast}
Y.~Leviathan, M.~Kalman, and Y.~Matias, ``Fast inference from transformers via
  speculative decoding,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2023, pp.
  19\,274--19\,286.

\bibitem{kim2024speculative}
S.~Kim, K.~Mangalam, S.~Moon, J.~Malik, M.~W. Mahoney, A.~Gholami, and
  K.~Keutzer, ``Speculative decoding with big little decoder,'' \emph{Advances
  in Neural Information Processing Systems}, vol.~36, 2024.

\bibitem{li2024prompt}
Z.~Li, Y.~Liu, Y.~Su, and N.~Collier, ``Prompt compression for large language
  models: A survey,'' \emph{arXiv preprint arXiv:2410.12388}, 2024.

\bibitem{shi2024keep}
L.~Shi, H.~Zhang, Y.~Yao, Z.~Li, and H.~Zhao, ``Keep the cost down: A review on
  methods to optimize llm's kv-cache consumption,'' \emph{arXiv preprint
  arXiv:2407.18003}, 2024.

\bibitem{li2024scbench}
Y.~Li, H.~Jiang, Q.~Wu, X.~Luo, S.~Ahn, C.~Zhang, A.~H. Abdi, D.~Li, J.~Gao,
  Y.~Yang \emph{et~al.}, ``Scbench: A kv cache-centric analysis of long-context
  methods,'' \emph{arXiv preprint arXiv:2412.10319}, 2024.

\bibitem{yuan2024kv}
J.~Yuan, H.~Liu, S.~Zhong, Y.-N. Chuang, S.~Li, G.~Wang, D.~Le, H.~Jin,
  V.~Chaudhary, Z.~Xu \emph{et~al.}, ``Kv cache compression, but what must we
  give in return? a comprehensive benchmark of long context capable
  approaches,'' \emph{arXiv preprint arXiv:2407.01527}, 2024.

\bibitem{bubeck2023sparks}
S.~Bubeck, V.~Chandrasekaran, R.~Eldan, J.~Gehrke, E.~Horvitz, E.~Kamar,
  P.~Lee, Y.~T. Lee, Y.~Li, S.~Lundberg \emph{et~al.}, ``Sparks of artificial
  general intelligence: Early experiments with gpt-4,'' \emph{arXiv preprint
  arXiv:2303.12712}, 2023.

\bibitem{zhao2023length}
L.~Zhao, X.~Feng, X.~Feng, B.~Qin, and T.~Liu, ``Length extrapolation of
  transformers: A survey from the perspective of position encoding,''
  \emph{arXiv preprint arXiv:2312.17044}, 2023.

\bibitem{zheng2021rethinking}
J.~Zheng, S.~Ramasinghe, and S.~Lucey, ``Rethinking positional encoding,''
  \emph{arXiv preprint arXiv:2107.02561}, 2021.

\bibitem{su2024roformer}
J.~Su, M.~Ahmed, Y.~Lu, S.~Pan, W.~Bo, and Y.~Liu, ``Roformer: Enhanced
  transformer with rotary position embedding,'' \emph{Neurocomputing}, vol.
  568, p. 127063, 2024.

\bibitem{agarap2018deep}
A.~F. Agarap, ``Deep learning using rectified linear units,'' \emph{arXiv
  preprint arXiv:1803.08375}, 2018.

\bibitem{dong2024get}
H.~Dong, X.~Yang, Z.~Zhang, Z.~Wang, Y.~Chi, and B.~Chen, ``Get more with less:
  Synthesizing recurrence with kv cache compression for efficient llm
  inference,'' \emph{arXiv preprint arXiv:2402.09398}, 2024.

\bibitem{linMatryoshkaKVAdaptiveKV2024}
\BIBentryALTinterwordspacing
B.~Lin, Z.~Zeng, Z.~Xiao, S.~Kou, T.~Hou, X.~Gao, H.~Zhang, and Z.~Deng,
  ``{{MatryoshkaKV}}: {{Adaptive KV Compression}} via {{Trainable Orthogonal
  Projection}},'' Oct. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2410.14731}
\BIBentrySTDinterwordspacing

\bibitem{liu2024unlocking}
P.~Liu, Z.-F. Gao, W.~X. Zhao, Y.~Ma, T.~Wang, and J.-R. Wen, ``Unlocking
  data-free low-bit quantization with matrix decomposition for kv cache
  compression,'' \emph{arXiv preprint arXiv:2405.12591}, 2024.

\bibitem{yu2024effectively}
H.~Yu, Z.~Yang, S.~Li, Y.~Li, and J.~Wu, ``Effectively compress kv heads for
  llm,'' \emph{arXiv preprint arXiv:2406.07056}, 2024.

\bibitem{saxena2024eigen}
U.~Saxena, G.~Saha, S.~Choudhary, and K.~Roy, ``Eigen attention: Attention in
  low-rank space for kv cache compression,'' \emph{arXiv preprint
  arXiv:2408.05646}, 2024.

\bibitem{zhang2024zero}
Z.~Zhang and H.~Shen, ``Zero-delay qkv compression for mitigating kv cache and
  network bottlenecks in llm inference,'' \emph{arXiv preprint
  arXiv:2408.04107}, 2024.

\bibitem{zhang2024lorc}
R.~Zhang, K.~Wang, L.~Liu, S.~Wang, H.~Cheng, C.~Zhang, and Y.~Shen, ``Lorc:
  Low-rank compression for llms kv cache with a progressive compression
  strategy,'' \emph{arXiv preprint arXiv:2410.03111}, 2024.

\bibitem{sun2024shadowkv}
H.~Sun, L.-W. Chang, W.~Bao, S.~Zheng, N.~Zheng, X.~Liu, H.~Dong, Y.~Chi, and
  B.~Chen, ``Shadowkv: Kv cache in shadows for high-throughput long-context llm
  inference,'' \emph{arXiv preprint arXiv:2410.21465}, 2024.

\bibitem{chang2024palu}
C.-C. Chang, W.-C. Lin, C.-Y. Lin, C.-Y. Chen, Y.-F. Hu, P.-S. Wang, N.-C.
  Huang, L.~Ceze, M.~S. Abdelfattah, and K.-C. Wu, ``Palu: Compressing kv-cache
  with low-rank projection,'' \emph{arXiv preprint arXiv:2407.21118}, 2024.

\bibitem{DBLP:journals/corr/abs-2402-17762}
\BIBentryALTinterwordspacing
M.~Sun, X.~Chen, J.~Z. Kolter, and Z.~Liu, ``Massive activations in large
  language models,'' \emph{CoRR}, vol. abs/2402.17762, 2024. [Online].
  Available: \url{https://doi.org/10.48550/arXiv.2402.17762}
\BIBentrySTDinterwordspacing

\bibitem{ashkboos2024quarot}
S.~Ashkboos, A.~Mohtashami, M.~L. Croci, B.~Li, P.~Cameron, M.~Jaggi,
  D.~Alistarh, T.~Hoefler, and J.~Hensman, ``Quarot: Outlier-free 4-bit
  inference in rotated llms,'' \emph{arXiv preprint arXiv:2404.00456}, 2024.

\bibitem{DBLP:journals/corr/abs-2405-04532}
\BIBentryALTinterwordspacing
Y.~Lin, H.~Tang, S.~Yang, Z.~Zhang, G.~Xiao, C.~Gan, and S.~Han, ``Qserve:
  {W4A8KV4} quantization and system co-design for efficient {LLM} serving,''
  \emph{CoRR}, vol. abs/2405.04532, 2024. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2405.04532}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/nips/XiLCZ23}
\BIBentryALTinterwordspacing
H.~Xi, C.~Li, J.~Chen, and J.~Zhu, ``Training transformers with 4-bit
  integers,'' in \emph{Advances in Neural Information Processing Systems 36:
  Annual Conference on Neural Information Processing Systems 2023, NeurIPS
  2023, New Orleans, LA, USA, December 10 - 16, 2023}, A.~Oh, T.~Naumann,
  A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, Eds., 2023. [Online].
  Available:
  \url{http://papers.nips.cc/paper\_files/paper/2023/hash/99fc8bc48b917c301a80cb74d91c0c06-Abstract-Conference.html}
\BIBentrySTDinterwordspacing

\bibitem{liu2024spinquant}
Z.~Liu, C.~Zhao, I.~Fedorov, B.~Soran, D.~Choudhary, R.~Krishnamoorthi,
  V.~Chandra, Y.~Tian, and T.~Blankevoort, ``Spinquant--llm quantization with
  learned rotations,'' \emph{arXiv preprint arXiv:2405.16406}, 2024.

\bibitem{lin2024duquant}
H.~Lin, H.~Xu, Y.~Wu, J.~Cui, Y.~Zhang, L.~Mou, L.~Song, Z.~Sun, and Y.~Wei,
  ``Duquant: Distributing outliers via dual transformation makes stronger
  quantized llms,'' in \emph{The Thirty-eighth Annual Conference on Neural
  Information Processing Systems}, 2024.

\bibitem{DBLP:conf/icml/XiaoLSWDH23}
\BIBentryALTinterwordspacing
G.~Xiao, J.~Lin, M.~Seznec, H.~Wu, J.~Demouth, and S.~Han, ``Smoothquant:
  Accurate and efficient post-training quantization for large language
  models,'' in \emph{International Conference on Machine Learning, {ICML} 2023,
  23-29 July 2023, Honolulu, Hawaii, {USA}}, ser. Proceedings of Machine
  Learning Research, A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato,
  and J.~Scarlett, Eds., vol. 202.\hskip 1em plus 0.5em minus 0.4em\relax
  {PMLR}, 2023, pp. 38\,087--38\,099. [Online]. Available:
  \url{https://proceedings.mlr.press/v202/xiao23c.html}
\BIBentrySTDinterwordspacing

\bibitem{wei2023outlier}
X.~Wei, Y.~Zhang, Y.~Li, X.~Zhang, R.~Gong, J.~Guo, and X.~Liu, ``Outlier
  suppression+: Accurate quantization of large language models by equivalent
  and optimal shifting and scaling,'' \emph{arXiv preprint arXiv:2304.09145},
  2023.

\bibitem{ma2024affinequant}
Y.~Ma, H.~Li, X.~Zheng, F.~Ling, X.~Xiao, R.~Wang, S.~Wen, F.~Chao, and R.~Ji,
  ``Affinequant: Affine transformation quantization for large language
  models,'' \emph{arXiv preprint arXiv:2403.12544}, 2024.

\bibitem{sun2024flatquant}
Y.~Sun, R.~Liu, H.~Bai, H.~Bao, K.~Zhao, Y.~Li, J.~Hu, X.~Yu, L.~Hou, C.~Yuan
  \emph{et~al.}, ``Flatquant: Flatness matters for llm quantization,''
  \emph{arXiv preprint arXiv:2410.09426}, 2024.

\bibitem{DBLP:conf/mlsys/0002TTYCWXDG024}
\BIBentryALTinterwordspacing
J.~Lin, J.~Tang, H.~Tang, S.~Yang, W.~Chen, W.~Wang, G.~Xiao, X.~Dang, C.~Gan,
  and S.~Han, ``{AWQ:} activation-aware weight quantization for on-device {LLM}
  compression and acceleration,'' in \emph{Proceedings of the Seventh Annual
  Conference on Machine Learning and Systems, MLSys 2024, Santa Clara, CA, USA,
  May 13-16, 2024}, P.~B. Gibbons, G.~Pekhimenko, and C.~D. Sa, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax mlsys.org, 2024. [Online]. Available:
  \url{https://proceedings.mlsys.org/paper\_files/paper/2024/hash/42a452cbafa9dd64e9ba4aa95cc1ef21-Abstract-Conference.html}
\BIBentrySTDinterwordspacing

\bibitem{shao2023omniquant}
W.~Shao, M.~Chen, Z.~Zhang, P.~Xu, L.~Zhao, Z.~Li, K.~Zhang, P.~Gao, Y.~Qiao,
  and P.~Luo, ``Omniquant: Omnidirectionally calibrated quantization for large
  language models,'' \emph{arXiv preprint arXiv:2308.13137}, 2023.

\bibitem{hooper2024kvquant}
C.~Hooper, S.~Kim, H.~Mohammadzadeh, M.~W. Mahoney, Y.~S. Shao, K.~Keutzer, and
  A.~Gholami, ``Kvquant: Towards 10 million context length llm inference with
  kv cache quantization,'' \emph{arXiv preprint arXiv:2401.18079}, 2024.

\bibitem{liu2024intactkv}
R.~Liu, H.~Bai, H.~Lin, Y.~Li, H.~Gao, Z.~Xu, L.~Hou, J.~Yao, and C.~Yuan,
  ``Intactkv: Improving large language model quantization by keeping pivot
  tokens intact,'' \emph{arXiv preprint arXiv:2403.01241}, 2024.

\bibitem{duanmu2024skvq}
H.~Duanmu, Z.~Yuan, X.~Li, J.~Duan, X.~Zhang, and D.~Lin, ``Skvq:
  Sliding-window key and value cache quantization for large language models,''
  \emph{arXiv preprint arXiv:2405.06219}, 2024.

\bibitem{liu2024kivi}
Z.~Liu, J.~Yuan, H.~Jin, S.~Zhong, Z.~Xu, V.~Braverman, B.~Chen, and X.~Hu,
  ``Kivi: A tuning-free asymmetric 2bit quantization for kv cache,''
  \emph{arXiv preprint arXiv:2402.02750}, 2024.

\bibitem{yue2024wkvquant}
Y.~Yue, Z.~Yuan, H.~Duanmu, S.~Zhou, J.~Wu, and L.~Nie, ``Wkvquant: Quantizing
  weight and key/value cache for large language models gains more,''
  \emph{arXiv preprint arXiv:2402.12065}, 2024.

\bibitem{kang2024gear}
H.~Kang, Q.~Zhang, S.~Kundu, G.~Jeong, Z.~Liu, T.~Krishna, and T.~Zhao, ``Gear:
  An efficient kv cache compression recipefor near-lossless generative
  inference of llm,'' \emph{arXiv preprint arXiv:2403.05527}, 2024.

\bibitem{yang2024no}
J.~Y. Yang, B.~Kim, J.~Bae, B.~Kwon, G.~Park, E.~Yang, S.~J. Kwon, and D.~Lee,
  ``No token left behind: Reliable kv cache compression via importance-aware
  mixed precision quantization,'' \emph{arXiv preprint arXiv:2402.18096}, 2024.

\bibitem{he2024zipvl}
Y.~He, F.~Chen, J.~Liu, W.~Shao, H.~Zhou, K.~Zhang, and B.~Zhuang, ``Zipvl:
  Efficient large vision-language models with dynamic token sparsification and
  kv cache compression,'' \emph{arXiv preprint arXiv:2410.08584}, 2024.

\bibitem{he2024zipcache}
Y.~He, L.~Zhang, W.~Wu, J.~Liu, H.~Zhou, and B.~Zhuang, ``Zipcache: Accurate
  and efficient kv cache quantization with salient token identification,''
  \emph{arXiv preprint arXiv:2405.14256}, 2024.

\bibitem{chen2024prefixquant}
M.~Chen, Y.~Liu, J.~Wang, Y.~Bin, W.~Shao, and P.~Luo, ``Prefixquant: Static
  quantization beats dynamic through prefixed outliers in llms,'' \emph{arXiv
  preprint arXiv:2410.05265}, 2024.

\bibitem{sharma2024minikvpushinglimitsllm}
\BIBentryALTinterwordspacing
A.~Sharma, H.~Ding, J.~Li, N.~Dani, and M.~Zhang, ``Minikv: Pushing the limits
  of llm inference via 2-bit layer-discriminative kv cache,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2411.18077}
\BIBentrySTDinterwordspacing

\bibitem{yao2022zeroquant}
Z.~Yao, R.~Yazdani~Aminabadi, M.~Zhang, X.~Wu, C.~Li, and Y.~He, ``Zeroquant:
  Efficient and affordable post-training quantization for large-scale
  transformers,'' \emph{Advances in Neural Information Processing Systems},
  vol.~35, pp. 27\,168--27\,183, 2022.

\bibitem{DBLP:conf/icml/0007ZYLRCLRSZ23}
\BIBentryALTinterwordspacing
Y.~Sheng, L.~Zheng, B.~Yuan, Z.~Li, M.~Ryabinin, B.~Chen, P.~Liang,
  C.~R{\'{e}}, I.~Stoica, and C.~Zhang, ``Flexgen: High-throughput generative
  inference of large language models with a single {GPU},'' in
  \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July
  2023, Honolulu, Hawaii, {USA}}, ser. Proceedings of Machine Learning
  Research, A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and
  J.~Scarlett, Eds., vol. 202.\hskip 1em plus 0.5em minus 0.4em\relax {PMLR},
  2023, pp. 31\,094--31\,116. [Online]. Available:
  \url{https://proceedings.mlr.press/v202/sheng23a.html}
\BIBentrySTDinterwordspacing

\bibitem{zandieh2024qjl}
A.~Zandieh, M.~Daliri, and I.~Han, ``Qjl: 1-bit quantized jl transform for kv
  cache quantization with zero overhead,'' \emph{arXiv preprint
  arXiv:2406.03482}, 2024.

\bibitem{zhang2024pqcache}
H.~Zhang, X.~Ji, Y.~Chen, F.~Fu, X.~Miao, X.~Nie, W.~Chen, and B.~Cui,
  ``Pqcache: Product quantization-based kvcache for long context llm
  inference,'' \emph{arXiv preprint arXiv:2407.12820}, 2024.

\bibitem{DBLP:journals/corr/abs-2405-14366}
\BIBentryALTinterwordspacing
A.~Liu, J.~Liu, Z.~Pan, Y.~He, G.~Haffari, and B.~Zhuang, ``Minicache: {KV}
  cache compression in depth dimension for large language models,''
  \emph{CoRR}, vol. abs/2405.14366, 2024. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2405.14366}
\BIBentrySTDinterwordspacing

\bibitem{yang2024kvsharerefficientinferencelayerwise}
\BIBentryALTinterwordspacing
Y.~Yang, Z.~Cao, Q.~Chen, L.~Qin, D.~Yang, H.~Zhao, and Z.~Chen, ``Kvsharer:
  Efficient inference via layer-wise dissimilar {KV} cache sharing,''
  \emph{CoRR}, vol. abs/2410.18517, 2024. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2410.18517}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/iclr/KimYYS24}
\BIBentryALTinterwordspacing
J.~Kim, J.~Yeom, S.~Yun, and H.~O. Song, ``Compressed context memory for online
  language model interaction,'' in \emph{The Twelfth International Conference
  on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11,
  2024}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2024. [Online].
  Available: \url{https://openreview.net/forum?id=64kSvC4iPg}
\BIBentrySTDinterwordspacing

\bibitem{wangLoMALosslessCompressed2024}
\BIBentryALTinterwordspacing
Y.~Wang and Z.~Xiao, ``{{LoMA}}: {{Lossless Compressed Memory Attention}},''
  Feb. 2024. [Online]. Available: \url{http://arxiv.org/abs/2401.09486}
\BIBentrySTDinterwordspacing

\bibitem{nawrotDynamicMemoryCompression2024}
\BIBentryALTinterwordspacing
P.~Nawrot, A.~Lancucki, M.~Chochowski, D.~Tarjan, and E.~M. Ponti, ``Dynamic
  {{Memory Compression}}: {{Retrofitting LLMs}} for {{Accelerated
  Inference}},'' in \emph{Forty-First {{International Conference}} on {{Machine
  Learning}}, {{ICML}} 2024, {{Vienna}}, {{Austria}}, {{July}} 21-27,
  2024}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2024. [Online].
  Available: \url{https://openreview.net/forum?id=tDRYrAkOB7}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/icml/0002DLZ00J24}
\BIBentryALTinterwordspacing
Y.~Zhang, Y.~Du, G.~Luo, Y.~Zhong, Z.~Zhang, S.~Liu, and R.~Ji, ``Cam: Cache
  merging for memory-efficient llms inference,'' in \emph{Forty-first
  International Conference on Machine Learning, {ICML} 2024, Vienna, Austria,
  July 21-27, 2024}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net,
  2024. [Online]. Available: \url{https://openreview.net/forum?id=LCTmppB165}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2406-13035}
\BIBentryALTinterwordspacing
Z.~Wan, X.~Wu, Y.~Zhang, Y.~Xin, C.~Tao, Z.~Zhu, X.~Wang, S.~Luo, J.~Xiong, and
  M.~Zhang, ``{D2O:} dynamic discriminative operations for efficient generative
  inference of large language models,'' \emph{CoRR}, vol. abs/2406.13035, 2024.
  [Online]. Available: \url{https://doi.org/10.48550/arXiv.2406.13035}
\BIBentrySTDinterwordspacing

\bibitem{zhong2024aim}
Y.~Zhong, Z.~Liu, Y.~Li, and L.~Wang, ``Aim: Adaptive inference of multi-modal
  llms via token merging and pruning,'' \emph{arXiv preprint arXiv:2412.03248},
  2024.

\bibitem{DBLP:conf/emnlp/WanWLHZJW024}
\BIBentryALTinterwordspacing
Z.~Wan, Z.~Wu, C.~Liu, J.~Huang, Z.~Zhu, P.~Jin, L.~Wang, and L.~Yuan,
  ``{LOOK-M:} look-once optimization in {KV} cache for efficient multimodal
  long-context inference,'' in \emph{Findings of the Association for
  Computational Linguistics: {EMNLP} 2024, Miami, Florida, USA, November 12-16,
  2024}, Y.~Al{-}Onaizan, M.~Bansal, and Y.~Chen, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2024, pp.
  4065--4078. [Online]. Available:
  \url{https://aclanthology.org/2024.findings-emnlp.235}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2407-08454}
\BIBentryALTinterwordspacing
Z.~Wang, B.~Jin, Z.~Yu, and M.~Zhang, ``Model tells you where to merge:
  Adaptive {KV} cache merging for llms on long-context tasks,'' \emph{CoRR},
  vol. abs/2407.08454, 2024. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2407.08454}
\BIBentrySTDinterwordspacing

\bibitem{agarwalCHAIClusteredHead2024}
\BIBentryALTinterwordspacing
S.~Agarwal, B.~Acun, B.~Hosmer, M.~Elhoushi, Y.~Lee, S.~Venkataraman,
  D.~Papailiopoulos, and C.-J. Wu, ``{{CHAI}}: {{Clustered Head Attention}} for
  {{Efficient LLM Inference}},'' Apr. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2403.08058}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2407-11550}
Y.~Feng, J.~Lv, Y.~Cao, X.~Xie, and S.~K. Zhou, ``Ada-kv: Optimizing {KV} cache
  eviction by adaptive budget allocation for efficient {LLM} inference,''
  \emph{CoRR}, vol. abs/2407.11550, 2024.

\bibitem{anonymous2024identify}
\BIBentryALTinterwordspacing
Anonymous, ``Identify critical {KV} cache in {LLM} inference from an output
  perturbation perspective,'' in \emph{Submitted to The Thirteenth
  International Conference on Learning Representations}, 2024, under review.
  [Online]. Available: \url{https://openreview.net/forum?id=lRTDMGYCpy}
\BIBentrySTDinterwordspacing

\bibitem{zhang2024unifyingkvcachecompression}
\BIBentryALTinterwordspacing
Y.~Zhang, Y.~Hu, R.~Zhao, J.~C.~S. Lui, and H.~Chen, ``Unifying kv cache
  compression for large language models with leankv,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2412.03131}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2407-15891}
H.~Tang, Y.~Lin, J.~Lin, Q.~Han, S.~Hong, Y.~Yao, and G.~Wang,
  ``Razorattention: Efficient {KV} cache compression through retrieval heads,''
  \emph{CoRR}, vol. abs/2407.15891, 2024.

\bibitem{DBLP:journals/corr/abs-2410-19258}
Y.~Fu, Z.~Cai, A.~Asi, W.~Xiong, Y.~Dong, and W.~Xiao, ``Not all heads matter:
  {A} head-level {KV} cache compression method with integrated retrieval and
  reasoning,'' \emph{CoRR}, vol. abs/2410.19258, 2024.

\bibitem{DBLP:journals/corr/abs-2410-10819}
G.~Xiao, J.~Tang, J.~Zuo, J.~Guo, S.~Yang, H.~Tang, Y.~Fu, and S.~Han,
  ``Duoattention: Efficient long-context {LLM} inference with retrieval and
  streaming heads,'' \emph{CoRR}, vol. abs/2410.10819, 2024.

\bibitem{DBLP:journals/corr/abs-2406-02069}
Z.~Cai, Y.~Zhang, B.~Gao, Y.~Liu, T.~Liu, K.~Lu, W.~Xiong, Y.~Dong, B.~Chang,
  J.~Hu, and W.~Xiao, ``Pyramidkv: Dynamic {KV} cache compression based on
  pyramidal information funneling,'' \emph{CoRR}, vol. abs/2406.02069, 2024.

\bibitem{DBLP:conf/acl/YangHGHZ024}
D.~Yang, X.~Han, Y.~Gao, Y.~Hu, S.~Zhang, and H.~Zhao, ``Pyramidinfer: Pyramid
  {KV} cache compression for high-throughput {LLM} inference,'' in
  \emph{ACL}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2024, pp. 3258--3270.

\bibitem{anonymous2024dynamickv}
\BIBentryALTinterwordspacing
Anonymous, ``Dynamic{KV}: Task-aware adaptive {KV} cache compression for long
  context {LLM}s,'' in \emph{Submitted to The Thirteenth International
  Conference on Learning Representations}, 2024, under review. [Online].
  Available: \url{https://openreview.net/forum?id=uHkfU4TaPh}
\BIBentrySTDinterwordspacing

\bibitem{wang2024prefixkvadaptiveprefixkv}
\BIBentryALTinterwordspacing
A.~Wang, H.~Chen, J.~Tan, K.~Zhang, X.~Cai, Z.~Lin, J.~Han, and G.~Ding,
  ``Prefixkv: Adaptive prefix kv cache is what vision instruction-following
  models need for efficient generation,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2412.03409}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2410-13846}
X.~Zhang, C.~Du, C.~Du, T.~Pang, W.~Gao, and M.~Lin, ``Simlayerkv: {A} simple
  framework for layer-level {KV} cache reduction,'' \emph{CoRR}, vol.
  abs/2410.13846, 2024.

\bibitem{xiao2024infllmtrainingfreelongcontextextrapolation}
\BIBentryALTinterwordspacing
C.~Xiao, P.~Zhang, X.~Han, G.~Xiao, Y.~Lin, Z.~Zhang, Z.~Liu, and M.~Sun,
  ``Infllm: Training-free long-context extrapolation for llms with an efficient
  context memory,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2402.04617}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/icml/TangZZXKH24}
J.~Tang, Y.~Zhao, K.~Zhu, G.~Xiao, B.~Kasikci, and S.~Han, ``{QUEST:}
  query-aware sparsity for efficient long-context {LLM} inference,'' in
  \emph{Forty-first International Conference on Machine Learning, {ICML} 2024,
  Vienna, Austria, July 21-27, 2024}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2024.

\bibitem{hooper2024squeezedattentionacceleratinglong}
\BIBentryALTinterwordspacing
C.~Hooper, S.~Kim, H.~Mohammadzadeh, M.~Maheswaran, J.~Paik, M.~W. Mahoney,
  K.~Keutzer, and A.~Gholami, ``Squeezed attention: Accelerating long context
  length llm inference,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2411.09688}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2409-10516}
D.~Liu, M.~Chen, B.~Lu, H.~Jiang, Z.~Han, Q.~Zhang, Q.~Chen, C.~Zhang, B.~Ding,
  K.~Zhang, C.~Chen, F.~Yang, Y.~Yang, and L.~Qiu, ``Retrievalattention:
  Accelerating long-context {LLM} inference via vector retrieval,''
  \emph{CoRR}, vol. abs/2409.10516, 2024.

\bibitem{DBLP:journals/corr/abs-2407-09450}
Z.~Fountas, M.~Benfeghoul, A.~Oomerjee, F.~Christopoulou, G.~Lampouras,
  H.~Bou{-}Ammar, and J.~Wang, ``Human-like episodic memory for infinite
  context llms,'' \emph{CoRR}, vol. abs/2407.09450, 2024.

\bibitem{liu2024clusterkv}
G.~Liu, C.~Li, J.~Zhao, C.~Zhang, and M.~Guo, ``Clusterkv: Manipulating llm kv
  cache in semantic space for recallable compression,'' \emph{arXiv preprint
  arXiv:2412.03213}, 2024.

\bibitem{DBLP:conf/nips/Zhang00CZC0TRBW23}
\BIBentryALTinterwordspacing
Z.~Zhang, Y.~Sheng, T.~Zhou, T.~Chen, L.~Zheng, R.~Cai, Z.~Song, Y.~Tian,
  C.~R{\'{e}}, C.~W. Barrett, Z.~Wang, and B.~Chen, ``{H2O:} heavy-hitter
  oracle for efficient generative inference of large language models,'' in
  \emph{Advances in Neural Information Processing Systems 36: Annual Conference
  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
  USA, December 10 - 16, 2023}, A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko,
  M.~Hardt, and S.~Levine, Eds., 2023. [Online]. Available:
  \url{http://papers.nips.cc/paper\_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html}
\BIBentrySTDinterwordspacing

\bibitem{zhao2024buzzbeehivestructuredsparsekv}
\BIBentryALTinterwordspacing
J.~Zhao, Z.~Fang, S.~Li, S.~Yang, and S.~He, ``Buzz: Beehive-structured sparse
  kv cache with segmented heavy hitters for efficient llm inference,'' 2024.
  [Online]. Available: \url{https://arxiv.org/abs/2410.23079}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2408-03675}
Y.~Chen, G.~Wang, J.~Shang, S.~Cui, Z.~Zhang, T.~Liu, S.~Wang, Y.~Sun, D.~Yu,
  and H.~Wu, ``{NACL:} {A} general and effective {KV} cache eviction framework
  for llms at inference time,'' \emph{CoRR}, vol. abs/2408.03675, 2024.

\bibitem{DBLP:conf/nips/LiuDLWXXKS23}
Z.~Liu, A.~Desai, F.~Liao, W.~Wang, V.~Xie, Z.~Xu, A.~Kyrillidis, and
  A.~Shrivastava, ``Scissorhands: Exploiting the persistence of importance
  hypothesis for {LLM} {KV} cache compression at test time,'' in \emph{Advances
  in Neural Information Processing Systems 36: Annual Conference on Neural
  Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
  December 10 - 16, 2023}, 2023.

\bibitem{DBLP:conf/mlsys/AdnanAJNSK24}
M.~Adnan, A.~Arunkumar, G.~Jain, P.~J. Nair, I.~Soloveychik, and P.~Kamath,
  ``Keyformer: {KV} cache reduction through key tokens selection for efficient
  generative inference,'' in \emph{Proceedings of the Seventh Annual Conference
  on Machine Learning and Systems, MLSys 2024, Santa Clara, CA, USA, May 13-16,
  2024}.\hskip 1em plus 0.5em minus 0.4em\relax mlsys.org, 2024.

\bibitem{chen2024sepllm}
G.~Chen, H.~Shi, J.~Li, Y.~Gao, X.~Ren, Y.~Chen, X.~Jiang, Z.~Li, W.~Liu, and
  C.~Huang, ``Sepllm: Accelerate large language models by compressing one
  segment into one separator,'' \emph{arXiv preprint arXiv:2412.12094}, 2024.

\bibitem{DBLP:conf/iclr/Ge0LZ0024}
S.~Ge, Y.~Zhang, L.~Liu, M.~Zhang, J.~Han, and J.~Gao, ``Model tells you what
  to discard: Adaptive {KV} cache compression for llms,'' in \emph{The Twelfth
  International Conference on Learning Representations, {ICLR} 2024, Vienna,
  Austria, May 7-11, 2024}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2024.

\bibitem{li2024snapkv}
Y.~Li, Y.~Huang, B.~Yang, B.~Venkitesh, A.~Locatelli, H.~Ye, T.~Cai, P.~Lewis,
  and D.~Chen, ``Snapkv: Llm knows what you are looking for before
  generation,'' \emph{arXiv preprint arXiv:2404.14469}, 2024.

\bibitem{DBLP:journals/corr/abs-2410-12876}
Z.~Zeng, B.~Lin, T.~Hou, H.~Zhang, and Z.~Deng, ``In-context kv-cache eviction
  for llms via attention-gate,'' \emph{CoRR}, vol. abs/2410.12876, 2024.

\bibitem{DBLP:conf/iclr/XiaoTCHL24}
\BIBentryALTinterwordspacing
G.~Xiao, Y.~Tian, B.~Chen, S.~Han, and M.~Lewis, ``Efficient streaming language
  models with attention sinks,'' in \emph{The Twelfth International Conference
  on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11,
  2024}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2024. [Online].
  Available: \url{https://openreview.net/forum?id=NG7sS51zVF}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/naacl/HanWPX0JW24}
C.~Han, Q.~Wang, H.~Peng, W.~Xiong, Y.~Chen, H.~Ji, and S.~Wang, ``Lm-infinite:
  Zero-shot extreme length generalization for large language models,'' in
  \emph{Proceedings of the 2024 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies
  (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21,
  2024}.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2024, pp. 3991--4008.

\bibitem{DBLP:conf/icml/RibarCHBLO24}
L.~Ribar, I.~Chelombiev, L.~Hudlass{-}Galley, C.~Blake, C.~Luschi, and D.~Orr,
  ``Sparq attention: Bandwidth-efficient {LLM} inference,'' in
  \emph{Forty-first International Conference on Machine Learning, {ICML} 2024,
  Vienna, Austria, July 21-27, 2024}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2024.

\bibitem{lee2024infinigenefficientgenerativeinference}
\BIBentryALTinterwordspacing
W.~Lee, J.~Lee, J.~Seo, and J.~Sim, ``Infinigen: Efficient generative inference
  of large language models with dynamic kv cache management,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2406.19707}
\BIBentrySTDinterwordspacing

\bibitem{xu2024recycledattentionefficientinference}
\BIBentryALTinterwordspacing
F.~Xu, T.~Goyal, and E.~Choi, ``Recycled attention: Efficient inference for
  long-context language models,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2411.05787}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2410-16179}
Z.~Chen, R.~Sadhukhan, Z.~Ye, Y.~Zhou, J.~Zhang, N.~Nolte, Y.~Tian, M.~Douze,
  L.~Bottou, Z.~Jia, and B.~Chen, ``Magicpig: {LSH} sampling for efficient
  {LLM} generation,'' \emph{CoRR}, vol. abs/2410.16179, 2024.

\bibitem{anonymous2024cake}
\BIBentryALTinterwordspacing
Anonymous, ``{CAKE}: Cascading and adaptive {KV} cache eviction with layer
  preferences,'' in \emph{Submitted to The Thirteenth International Conference
  on Learning Representations}, 2024, under review. [Online]. Available:
  \url{https://openreview.net/forum?id=EQgEMAD4kv}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2404-15574}
W.~Wu, Y.~Wang, G.~Xiao, H.~Peng, and Y.~Fu, ``Retrieval head mechanistically
  explains long-context factuality,'' \emph{CoRR}, vol. abs/2404.15574, 2024.

\bibitem{DBLP:conf/sosp/KwonLZ0ZY0ZS23}
W.~Kwon, Z.~Li, S.~Zhuang, Y.~Sheng, L.~Zheng, C.~H. Yu, J.~Gonzalez, H.~Zhang,
  and I.~Stoica, ``Efficient memory management for large language model serving
  with pagedattention,'' in \emph{Proceedings of the 29th Symposium on
  Operating Systems Principles, {SOSP} 2023, Koblenz, Germany, October 23-26,
  2023}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2023, pp. 611--626.

\bibitem{DBLP:conf/nips/DaoFERR22}
\BIBentryALTinterwordspacing
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~R{\'{e}}, ``Flashattention: Fast
  and memory-efficient exact attention with io-awareness,'' in \emph{Advances
  in Neural Information Processing Systems 35: Annual Conference on Neural
  Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,
  November 28 - December 9, 2022}, S.~Koyejo, S.~Mohamed, A.~Agarwal,
  D.~Belgrave, K.~Cho, and A.~Oh, Eds., 2022. [Online]. Available:
  \url{http://papers.nips.cc/paper\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/iclr/RaePJHL20}
\BIBentryALTinterwordspacing
J.~W. Rae, A.~Potapenko, S.~M. Jayakumar, C.~Hillier, and T.~P. Lillicrap,
  ``Compressive transformers for long-range sequence modelling,'' in \emph{8th
  International Conference on Learning Representations, {ICLR} 2020, Addis
  Ababa, Ethiopia, April 26-30, 2020}.\hskip 1em plus 0.5em minus 0.4em\relax
  OpenReview.net, 2020. [Online]. Available:
  \url{https://openreview.net/forum?id=SylKikSYDH}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/nips/Mu0G23}
\BIBentryALTinterwordspacing
J.~Mu, X.~Li, and N.~D. Goodman, ``Learning to compress prompts with gist
  tokens,'' in \emph{Advances in Neural Information Processing Systems 36:
  Annual Conference on Neural Information Processing Systems 2023, NeurIPS
  2023, New Orleans, LA, USA, December 10 - 16, 2023}, A.~Oh, T.~Naumann,
  A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, Eds., 2023. [Online].
  Available:
  \url{http://papers.nips.cc/paper\_files/paper/2023/hash/3d77c6dcc7f143aa2154e7f4d5e22d68-Abstract-Conference.html}
\BIBentrySTDinterwordspacing

\bibitem{dong2024qaq}
S.~Dong, W.~Cheng, J.~Qin, and W.~Wang, ``Qaq: Quality adaptive quantization
  for llm kv cache,'' \emph{arXiv preprint arXiv:2403.04643}, 2024.

\bibitem{liu2024cachegen}
Y.~Liu, H.~Li, Y.~Cheng, S.~Ray, Y.~Huang, Q.~Zhang, K.~Du, J.~Yao, S.~Lu,
  G.~Ananthanarayanan \emph{et~al.}, ``Cachegen: Kv cache compression and
  streaming for fast large language model serving,'' in \emph{Proceedings of
  the ACM SIGCOMM 2024 Conference}, 2024, pp. 38--56.

\bibitem{zhao2024atom}
Y.~Zhao, C.-Y. Lin, K.~Zhu, Z.~Ye, L.~Chen, S.~Zheng, L.~Ceze,
  A.~Krishnamurthy, T.~Chen, and B.~Kasikci, ``Atom: Low-bit quantization for
  efficient and accurate llm serving,'' \emph{Proceedings of Machine Learning
  and Systems}, vol.~6, pp. 196--209, 2024.

\bibitem{lin2016fixed}
D.~Lin, S.~Talathi, and S.~Annapureddy, ``Fixed point quantization of deep
  convolutional networks,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2016, pp. 2849--2858.

\bibitem{wu2020integer}
H.~Wu, P.~Judd, X.~Zhang, M.~Isaev, and P.~Micikevicius, ``Integer quantization
  for deep learning inference: Principles and empirical evaluation,''
  \emph{arXiv preprint arXiv:2004.09602}, 2020.

\bibitem{kwasniewska2019deep}
A.~Kwasniewska, M.~Szankin, M.~Ozga, J.~Wolfe, A.~Das, A.~Zajac, J.~Ruminski,
  and P.~Rad, ``Deep learning optimization for edge devices: Analysis of
  training quantization parameters,'' in \emph{IECON 2019-45th Annual
  Conference of the IEEE Industrial Electronics Society}, vol.~1.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 96--101.

\bibitem{zhou2018adaptive}
Y.~Zhou, S.-M. Moosavi-Dezfooli, N.-M. Cheung, and P.~Frossard, ``Adaptive
  quantization for deep neural network,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~32, no.~1, 2018.

\bibitem{jiang2018linear}
P.~Jiang and G.~Agrawal, ``A linear speedup analysis of distributed deep
  learning with sparse and quantized communication,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~31, 2018.

\bibitem{frantar2022gptq}
E.~Frantar, S.~Ashkboos, T.~Hoefler, and D.~Alistarh, ``Gptq: Accurate
  post-training quantization for generative pre-trained transformers,''
  \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{dettmers2024qlora}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, ``Qlora: Efficient
  finetuning of quantized llms,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~36, 2024.

\bibitem{bondarenko2023quantizable}
Y.~Bondarenko, M.~Nagel, and T.~Blankevoort, ``Quantizable transformers:
  Removing outliers by helping attention heads do nothing,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~36, pp. 75\,067--75\,096, 2023.

\bibitem{cheng2017quantized}
J.~Cheng, J.~Wu, C.~Leng, Y.~Wang, and Q.~Hu, ``Quantized cnn: A unified
  approach to accelerate and compress convolutional networks,'' \emph{IEEE
  transactions on neural networks and learning systems}, vol.~29, no.~10, pp.
  4730--4743, 2017.

\bibitem{zhou2023dataset}
D.~Zhou, K.~Wang, J.~Gu, X.~Peng, D.~Lian, Y.~Zhang, Y.~You, and J.~Feng,
  ``Dataset quantization,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2023, pp. 17\,205--17\,216.

\bibitem{jegou2010product}
H.~Jegou, M.~Douze, and C.~Schmid, ``Product quantization for nearest neighbor
  search,'' \emph{IEEE transactions on pattern analysis and machine
  intelligence}, vol.~33, no.~1, pp. 117--128, 2010.

\bibitem{dettmers2022gpt3}
T.~Dettmers, M.~Lewis, Y.~Belkada, and L.~Zettlemoyer, ``Gpt3. int8 (): 8-bit
  matrix multiplication for transformers at scale,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~35, pp. 30\,318--30\,332, 2022.

\bibitem{bondarenko2021understanding}
Y.~Bondarenko, M.~Nagel, and T.~Blankevoort, ``Understanding and overcoming the
  challenges of efficient transformer quantization,'' \emph{arXiv preprint
  arXiv:2109.12948}, 2021.

\bibitem{wei2022outlier}
X.~Wei, Y.~Zhang, X.~Zhang, R.~Gong, S.~Zhang, Q.~Zhang, F.~Yu, and X.~Liu,
  ``Outlier suppression: Pushing the limit of low-bit transformer language
  models,'' \emph{Advances in Neural Information Processing Systems}, vol.~35,
  pp. 17\,402--17\,414, 2022.

\bibitem{matsui2018survey}
Y.~Matsui, Y.~Uchida, H.~J{\'e}gou, and S.~Satoh, ``A survey of product
  quantization,'' \emph{ITE Transactions on Media Technology and Applications},
  vol.~6, no.~1, pp. 2--10, 2018.

\bibitem{zhang2023h2o}
Z.~Zhang, Y.~Sheng, T.~Zhou, T.~Chen, L.~Zheng, R.~Cai, Z.~Song, Y.~Tian,
  C.~R{\'e}, C.~Barrett \emph{et~al.}, ``H2o: Heavy-hitter oracle for efficient
  generative inference of large language models,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~36, pp. 34\,661--34\,710, 2023.

\bibitem{tao2024asymkv}
Q.~Tao, W.~Yu, and J.~Zhou, ``Asymkv: Enabling 1-bit quantization of kv cache
  with layer-wise asymmetric quantization configurations,'' \emph{arXiv
  preprint arXiv:2410.13212}, 2024.

\bibitem{kuleshov2015tensor}
V.~Kuleshov, A.~Chaganty, and P.~Liang, ``Tensor factorization via matrix
  factorization,'' in \emph{Artificial Intelligence and Statistics}.\hskip 1em
  plus 0.5em minus 0.4em\relax PMLR, 2015, pp. 507--516.

\bibitem{zhou2017tensor}
P.~Zhou, C.~Lu, Z.~Lin, and C.~Zhang, ``Tensor factorization for low-rank
  tensor completion,'' \emph{IEEE Transactions on Image Processing}, vol.~27,
  no.~3, pp. 1152--1163, 2017.

\bibitem{haeffele2015global}
B.~D. Haeffele and R.~Vidal, ``Global optimality in tensor factorization, deep
  learning, and beyond,'' \emph{arXiv preprint arXiv:1506.07540}, 2015.

\bibitem{liu2021enabling}
P.~Liu, Z.-F. Gao, W.~X. Zhao, Z.-Y. Xie, Z.-Y. Lu, and J.-R. Wen, ``Enabling
  lightweight fine-tuning for pre-trained language model compression based on
  matrix product operators,'' \emph{arXiv preprint arXiv:2106.02205}, 2021.

\bibitem{malik2018low}
O.~A. Malik and S.~Becker, ``Low-rank tucker decomposition of large tensors
  using tensorsketch,'' \emph{Advances in neural information processing
  systems}, vol.~31, 2018.

\bibitem{xuMixConHybridArchitecture2024}
\BIBentryALTinterwordspacing
X.~Xu and Z.~Lin, ``{{MixCon}}: {{A Hybrid Architecture}} for {{Efficient}} and
  {{Adaptive Sequence Modeling}},'' in \emph{Frontiers in {{Artificial
  Intelligence}} and {{Applications}}}, U.~Endriss, F.~S. Melo, K.~Bach,
  A.~{Bugar{\'i}n-Diz}, J.~M. {Alonso-Moral}, S.~Barro, and F.~Heintz,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax IOS Press, Oct. 2024. [Online].
  Available: \url{https://ebooks.iospress.nl/doi/10.3233/FAIA240593}
\BIBentrySTDinterwordspacing

\bibitem{goldsteinGoldFinchHighPerformance2024}
\BIBentryALTinterwordspacing
D.~Goldstein, F.~Obeid, E.~Alcaide, G.~Song, and E.~Cheah, ``{{GoldFinch}}:
  {{High Performance RWKV}}/{{Transformer Hybrid}} with {{Linear Pre-Fill}} and
  {{Extreme KV-Cache Compression}},'' Jul. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2407.12077}
\BIBentrySTDinterwordspacing

\bibitem{yanRecurFormerNotAll2024}
\BIBentryALTinterwordspacing
R.~Yan, L.~Zheng, X.~Du, H.~Zou, Y.~Guo, and J.~Yang, ``{{RecurFormer}}: {{Not
  All Transformer Heads Need Self-Attention}},'' Oct. 2024. [Online].
  Available: \url{http://arxiv.org/abs/2410.12850}
\BIBentrySTDinterwordspacing

\bibitem{pengRWKVReinventingRNNs2023}
\BIBentryALTinterwordspacing
B.~Peng, E.~Alcaide, Q.~Anthony, A.~Albalak, S.~Arcadinho, S.~Biderman, H.~Cao,
  X.~Cheng, M.~Chung, M.~Grella, K.~K. GV, X.~He, H.~Hou, J.~Lin, P.~Kazienko,
  J.~Kocon, J.~Kong, B.~Koptyra, H.~Lau, K.~S.~I. Mantri, F.~Mom, A.~Saito,
  G.~Song, X.~Tang, B.~Wang, J.~S. Wind, S.~Wozniak, R.~Zhang, Z.~Zhang,
  Q.~Zhao, P.~Zhou, Q.~Zhou, J.~Zhu, and R.-J. Zhu, ``{{RWKV}}: {{Reinventing
  RNNs}} for the {{Transformer Era}},'' Dec. 2023. [Online]. Available:
  \url{http://arxiv.org/abs/2305.13048}
\BIBentrySTDinterwordspacing

\bibitem{guMambaLinearTimeSequence2024}
\BIBentryALTinterwordspacing
A.~Gu and T.~Dao, ``Mamba: {{Linear-Time Sequence Modeling}} with {{Selective
  State Spaces}},'' May 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2312.00752}
\BIBentrySTDinterwordspacing

\bibitem{sunRetentiveNetworkSuccessor2023}
\BIBentryALTinterwordspacing
Y.~Sun, L.~Dong, S.~Huang, S.~Ma, Y.~Xia, J.~Xue, J.~Wang, and F.~Wei,
  ``Retentive {{Network}}: {{A Successor}} to {{Transformer}} for {{Large
  Language Models}},'' Aug. 2023. [Online]. Available:
  \url{http://arxiv.org/abs/2307.08621}
\BIBentrySTDinterwordspacing

\bibitem{yangMCSDEfficientLanguage2024}
\BIBentryALTinterwordspacing
H.~Yang, D.~Li, and S.~Li, ``{{MCSD}}: {{An Efficient Language Model}} with
  {{Diverse Fusion}},'' Jul. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2406.12230}
\BIBentrySTDinterwordspacing

\bibitem{sunYouOnlyCache2024}
\BIBentryALTinterwordspacing
Y.~Sun, L.~Dong, Y.~Zhu, S.~Huang, W.~Wang, S.~Ma, Q.~Zhang, J.~Wang, and
  F.~Wei, ``You {{Only Cache Once}}: {{Decoder-Decoder Architectures}} for
  {{Language Models}},'' May 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2405.05254}
\BIBentrySTDinterwordspacing

\bibitem{yenLongContextLanguageModeling2024a}
\BIBentryALTinterwordspacing
H.~Yen, T.~Gao, and D.~Chen, ``Long-{{Context Language Modeling}} with
  {{Parallel Context Encoding}},'' in \emph{Proceedings of the 62nd {{Annual
  Meeting}} of the {{Association}} for {{Computational Linguistics}}
  ({{Volume}} 1: {{Long Papers}}), {{ACL}} 2024, {{Bangkok}}, {{Thailand}},
  {{August}} 11-16, 2024}, L.-W. Ku, A.~Martins, and V.~Srikumar, Eds.\hskip
  1em plus 0.5em minus 0.4em\relax Association for Computational Linguistics,
  2024, pp. 2588--2610. [Online]. Available:
  \url{https://doi.org/10.18653/v1/2024.acl-long.142}
\BIBentrySTDinterwordspacing

\bibitem{monteiroXCCacheCrossAttendingCached2024}
\BIBentryALTinterwordspacing
J.~Monteiro, {\'E}.~Marcotte, P.-A. No{\"e}l, V.~Zantedeschi, D.~V{\'a}zquez,
  N.~Chapados, C.~Pal, and P.~Taslakian, ``{{XC-Cache}}: {{Cross-Attending}} to
  {{Cached Context}} for {{Efficient LLM Inference}},'' in \emph{Findings of
  the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024,
  {{Miami}}, {{Florida}}, {{USA}}, {{November}} 12-16, 2024}, Y.~{Al-Onaizan},
  M.~Bansal, and Y.-N. Chen, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Association for Computational Linguistics, 2024, pp. 15\,284--15\,302.
  [Online]. Available: \url{https://aclanthology.org/2024.findings-emnlp.896}
\BIBentrySTDinterwordspacing

\bibitem{hoBlockTransformerGlobaltoLocal2024}
\BIBentryALTinterwordspacing
N.~Ho, S.~Bae, T.~Kim, H.~Jo, Y.~Kim, T.~Schuster, A.~Fisch, J.~Thorne, and
  S.-Y. Yun, ``Block {{Transformer}}: {{Global-to-Local Language Modeling}} for
  {{Fast Inference}},'' Nov. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2406.02657}
\BIBentrySTDinterwordspacing

\bibitem{huaTransformerQualityLinear2022}
\BIBentryALTinterwordspacing
W.~Hua, Z.~Dai, H.~Liu, and Q.~V. Le, ``Transformer {{Quality}} in {{Linear
  Time}},'' in \emph{International {{Conference}} on {{Machine Learning}},
  {{ICML}} 2022, 17-23 {{July}} 2022, {{Baltimore}}, {{Maryland}}, {{USA}}},
  ser. Proceedings of {{Machine Learning Research}}, K.~Chaudhuri, S.~Jegelka,
  L.~Song, C.~Szepesv{\'a}ri, G.~Niu, and S.~Sabato, Eds., vol. 162.\hskip 1em
  plus 0.5em minus 0.4em\relax PMLR, 2022, pp. 9099--9117. [Online]. Available:
  \url{https://proceedings.mlr.press/v162/hua22a.html}
\BIBentrySTDinterwordspacing

\bibitem{munkhdalaiLeaveNoContext2024}
\BIBentryALTinterwordspacing
T.~Munkhdalai, M.~Faruqui, and S.~Gopal, ``Leave {{No Context Behind}}:
  {{Efficient Infinite Context Transformers}} with {{Infini-attention}},'' Aug.
  2024. [Online]. Available: \url{http://arxiv.org/abs/2404.07143}
\BIBentrySTDinterwordspacing

\bibitem{brandonReducingTransformerKeyValue2024}
\BIBentryALTinterwordspacing
W.~Brandon, M.~Mishra, A.~Nrusimha, R.~Panda, and J.~R. Kelly, ``Reducing
  {{Transformer Key-Value Cache Size}} with {{Cross-Layer Attention}},'' May
  2024. [Online]. Available: \url{http://arxiv.org/abs/2405.12981}
\BIBentrySTDinterwordspacing

\bibitem{wuLayerCondensedKVCache2024}
\BIBentryALTinterwordspacing
H.~Wu and K.~Tu, ``Layer-{{Condensed KV Cache}} for {{Efficient Inference}} of
  {{Large Language Models}},'' in \emph{Proceedings of the 62nd {{Annual
  Meeting}} of the {{Association}} for {{Computational Linguistics}}
  ({{Volume}} 1: {{Long Papers}}), {{ACL}} 2024, {{Bangkok}}, {{Thailand}},
  {{August}} 11-16, 2024}, L.-W. Ku, A.~Martins, and V.~Srikumar, Eds.\hskip
  1em plus 0.5em minus 0.4em\relax Association for Computational Linguistics,
  2024, pp. 11\,175--11\,188. [Online]. Available:
  \url{https://doi.org/10.18653/v1/2024.acl-long.602}
\BIBentrySTDinterwordspacing

\bibitem{liaoKVCachingShared2024}
\BIBentryALTinterwordspacing
B.~Liao and D.~V. Vargas, ``Beyond {{KV Caching}}: {{Shared Attention}} for
  {{Efficient LLMs}},'' Jul. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2407.12866}
\BIBentrySTDinterwordspacing

\bibitem{zuhriMLKVMultiLayerKeyValue2024}
\BIBentryALTinterwordspacing
Z.~M.~K. Zuhri, M.~F. Adilazuarda, A.~Purwarianti, and A.~F. Aji, ``{{MLKV}}:
  {{Multi-Layer Key-Value Heads}} for {{Memory Efficient Transformer
  Decoding}},'' Oct. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2406.09297}
\BIBentrySTDinterwordspacing

\bibitem{muCrosslayerAttentionSharing2024}
\BIBentryALTinterwordspacing
Y.~Mu, Y.~Wu, Y.~Fan, C.~Wang, H.~Li, Q.~He, M.~Yang, T.~Xiao, and J.~Zhu,
  ``Cross-layer {{Attention Sharing}} for {{Large Language Models}},'' 2024.
  [Online]. Available: \url{https://arxiv.org/abs/2408.01890}
\BIBentrySTDinterwordspacing

\bibitem{wuSystematicStudyCrossLayer2024}
\BIBentryALTinterwordspacing
Y.~Wu, H.~Wu, and K.~Tu, ``A {{Systematic Study}} of {{Cross-Layer KV Sharing}}
  for {{Efficient LLM Inference}},'' Oct. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2410.14442}
\BIBentrySTDinterwordspacing

\bibitem{yangLosslessKVCache2024}
\BIBentryALTinterwordspacing
Z.~Yang, J.~N. Han, K.~Wu, R.~Xie, A.~Wang, X.~Sun, and Z.~Kang, ``Lossless
  {{KV Cache Compression}} to 2\%,'' Oct. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2410.15252}
\BIBentrySTDinterwordspacing

\bibitem{chenDHALearningDecoupledHead2024}
\BIBentryALTinterwordspacing
Y.~Chen, L.~Zhang, J.~Shang, Z.~Zhang, T.~Liu, S.~Wang, and Y.~Sun, ``{{DHA}}:
  {{Learning Decoupled-Head Attention}} from {{Transformer Checkpoints}} via
  {{Adaptive Heads Fusion}},'' Jun. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2406.06567}
\BIBentrySTDinterwordspacing

\bibitem{zhouValueResidualLearning2024}
\BIBentryALTinterwordspacing
Z.~Zhou, T.~Wu, Z.~Jiang, and Z.~Lan, ``Value {{Residual Learning For
  Alleviating Attention Concentration In Transformers}},'' Dec. 2024. [Online].
  Available: \url{http://arxiv.org/abs/2410.17897}
\BIBentrySTDinterwordspacing

\bibitem{shazeerFastTransformerDecoding2019}
\BIBentryALTinterwordspacing
N.~Shazeer, ``Fast {{Transformer Decoding}}: {{One Write-Head}} is {{All You
  Need}},'' Nov. 2019. [Online]. Available:
  \url{http://arxiv.org/abs/1911.02150}
\BIBentrySTDinterwordspacing

\bibitem{ainslieGQATrainingGeneralized2023}
\BIBentryALTinterwordspacing
J.~Ainslie, J.~{Lee-Thorp}, M.~de~Jong, Y.~Zemlyanskiy, F.~Lebr{\'o}n, and
  S.~Sanghai, ``{{GQA}}: {{Training Generalized Multi-Query Transformer
  Models}} from {{Multi-Head Checkpoints}},'' Dec. 2023. [Online]. Available:
  \url{http://arxiv.org/abs/2305.13245}
\BIBentrySTDinterwordspacing

\bibitem{chenOptimisedGroupedQueryAttention2024a}
\BIBentryALTinterwordspacing
Y.~Chen, C.~Zhang, X.~Gao, R.~D. Mullins, G.~A. Constantinides, and Y.~Zhao,
  ``Optimised {{Grouped-Query Attention Mechanism}} for {{Transformers}},'' in
  \emph{Workshop on {{Efficient Systems}} for {{Foundation Models II}} @
  {{ICML2024}}}, Jul. 2024. [Online]. Available:
  \url{https://openreview.net/forum?id=13MMghY6Kh}
\BIBentrySTDinterwordspacing

\bibitem{chinnakonduruWeightedGroupedQuery2024}
\BIBentryALTinterwordspacing
S.~S. Chinnakonduru and A.~Mohapatra, ``Weighted {{Grouped Query Attention}} in
  {{Transformers}},'' Jul. 2024. [Online]. Available:
  \url{http://arxiv.org/abs/2407.10855}
\BIBentrySTDinterwordspacing

\bibitem{joshiQCQAQualityCapacityaware2024}
\BIBentryALTinterwordspacing
V.~Joshi, P.~Laddha, S.~Sinha, O.~J. Omer, and S.~Subramoney, ``{{QCQA}}:
  {{Quality}} and {{Capacity-aware}} grouped {{Query Attention}},'' Jun. 2024.
  [Online]. Available: \url{http://arxiv.org/abs/2406.10247}
\BIBentrySTDinterwordspacing

\bibitem{khanUniformQueryDistribution2024}
\BIBentryALTinterwordspacing
Z.~Khan, M.~Khaquan, O.~Tafveez, B.~Samiwala, and A.~A. Raza, ``Beyond
  {{Uniform Query Distribution}}: {{Key-Driven Grouped Query Attention}},''
  Aug. 2024. [Online]. Available: \url{http://arxiv.org/abs/2408.08454}
\BIBentrySTDinterwordspacing

\bibitem{javadiGQKVAEfficientPretraining2023}
\BIBentryALTinterwordspacing
F.~Javadi, W.~Ahmed, H.~Hajimolahoseini, F.~Ataiefard, M.~Hassanpour, S.~Asani,
  A.~Wen, O.~M. Awad, K.~Liu, and Y.~Liu, ``{{GQKVA}}: {{Efficient
  Pre-training}} of {{Transformers}} by {{Grouping Queries}}, {{Keys}}, and
  {{Values}},'' Dec. 2023. [Online]. Available:
  \url{http://arxiv.org/abs/2311.03426}
\BIBentrySTDinterwordspacing

\bibitem{katharopoulos2020transformers}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret, ``Transformers are rnns:
  Fast autoregressive transformers with linear attention,'' in
  \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2020, pp. 5156--5165.

\bibitem{choromanski2020rethinking}
K.~M. Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarlos,
  P.~Hawkins, J.~Q. Davis, A.~Mohiuddin, L.~Kaiser \emph{et~al.}, ``Rethinking
  attention with performers,'' in \emph{International Conference on Learning
  Representations}, 2020.

\bibitem{wang2020linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma, ``Linformer: Self-attention
  with linear complexity,'' \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{salehinejad2017recent}
H.~Salehinejad, S.~Sankar, J.~Barfett, E.~Colak, and S.~Valaee, ``Recent
  advances in recurrent neural networks,'' \emph{arXiv preprint
  arXiv:1801.01078}, 2017.

\bibitem{xu2024integrating}
X.~Xu, Y.~Liang, B.~Huang, Z.~Lan, and K.~Shu, ``Integrating mamba and
  transformer for long-short range time series forecasting,'' \emph{arXiv
  preprint arXiv:2404.14757}, 2024.

\bibitem{hasani2022liquid}
R.~Hasani, M.~Lechner, T.-H. Wang, M.~Chahine, A.~Amini, and D.~Rus, ``Liquid
  structural state-space models,'' \emph{arXiv preprint arXiv:2209.12951},
  2022.

\bibitem{smith2022simplified}
J.~T. Smith, A.~Warrington, and S.~W. Linderman, ``Simplified state space
  layers for sequence modeling,'' \emph{arXiv preprint arXiv:2208.04933}, 2022.

\bibitem{wang2022pretraining}
J.~Wang, J.~N. Yan, A.~Gu, and A.~M. Rush, ``Pretraining without attention,''
  \emph{arXiv preprint arXiv:2212.10544}, 2022.

\bibitem{xu2024survey}
R.~Xu, S.~Yang, Y.~Wang, B.~Du, and H.~Chen, ``A survey on vision mamba:
  Models, applications and challenges,'' \emph{arXiv preprint
  arXiv:2404.18861}, 2024.

\bibitem{qu2024survey}
H.~Qu, L.~Ning, R.~An, W.~Fan, T.~Derr, H.~Liu, X.~Xu, and Q.~Li, ``A survey of
  mamba,'' \emph{arXiv preprint arXiv:2408.01129}, 2024.

\bibitem{patro2024mamba}
B.~N. Patro and V.~S. Agneeswaran, ``Mamba-360: Survey of state space models as
  transformer alternative for long sequence modelling: Methods, applications,
  and challenges,'' \emph{arXiv preprint arXiv:2404.16112}, 2024.

\bibitem{guParameterizationInitializationDiagonal2022}
A.~Gu, K.~Goel, A.~Gupta, and C.~R{\'e}, ``On the {{Parameterization}} and
  {{Initialization}} of {{Diagonal State Space Models}},'' in \emph{Advances in
  {{Neural Information Processing Systems}} 35: {{Annual Conference}} on
  {{Neural Information Processing Systems}} 2022, {{NeurIPS}} 2022, {{New
  Orleans}}, {{LA}}, {{USA}}, {{November}} 28 - {{December}} 9, 2022},
  S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, Eds.\hskip
  1em plus 0.5em minus 0.4em\relax arXiv, 2022.

\bibitem{guCombiningRecurrentConvolutional2021}
\BIBentryALTinterwordspacing
A.~Gu, I.~Johnson, K.~Goel, K.~K. Saab, T.~Dao, A.~Rudra, and C.~Re,
  ``Combining {{Recurrent}}, {{Convolutional}}, and {{Continuous-time Models}}
  with {{Linear State Space Layers}},'' in \emph{Advances in {{Neural
  Information Processing Systems}}}, Nov. 2021. [Online]. Available:
  \url{https://openreview.net/forum?id=yWd42CWN3c}
\BIBentrySTDinterwordspacing

\bibitem{pan2024instinferinstorageattentionoffloading}
\BIBentryALTinterwordspacing
X.~Pan, E.~Li, Q.~Li, S.~Liang, Y.~Shan, K.~Zhou, Y.~Luo, X.~Wang, and
  J.~Zhang, ``Instinfer: In-storage attention offloading for cost-effective
  long-context llm inference,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2409.04992}
\BIBentrySTDinterwordspacing

\bibitem{jiang2024neosavinggpumemory}
\BIBentryALTinterwordspacing
X.~Jiang, Y.~Zhou, S.~Cao, I.~Stoica, and M.~Yu, ``Neo: Saving gpu memory
  crisis with cpu offloading for online llm inference,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2411.01142}
\BIBentrySTDinterwordspacing

\bibitem{he2024fastdecodehighthroughputgpuefficientllm}
\BIBentryALTinterwordspacing
J.~He and J.~Zhai, ``Fastdecode: High-throughput gpu-efficient llm serving
  using heterogeneous pipelines,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2403.11421}
\BIBentrySTDinterwordspacing

\bibitem{xu2024vtensorflexiblevirtualtensor}
\BIBentryALTinterwordspacing
J.~Xu, R.~Zhang, C.~Guo, W.~Hu, Z.~Liu, F.~Wu, Y.~Feng, S.~Sun, C.~Shao,
  Y.~Guo, J.~Zhao, K.~Zhang, M.~Guo, and J.~Leng, ``vtensor: Flexible virtual
  tensor management for efficient llm serving,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2407.15309}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2312-05516}
\BIBentryALTinterwordspacing
L.~Yu and J.~Li, ``Stateful large language model serving with pensieve,''
  \emph{CoRR}, vol. abs/2312.05516, 2023. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2312.05516}
\BIBentrySTDinterwordspacing

\bibitem{wu2024fastdistributedinferenceserving}
\BIBentryALTinterwordspacing
B.~Wu, Y.~Zhong, Z.~Zhang, S.~Liu, F.~Liu, Y.~Sun, G.~Huang, X.~Liu, and
  X.~Jin, ``Fast distributed inference serving for large language models,''
  2024. [Online]. Available: \url{https://arxiv.org/abs/2305.05920}
\BIBentrySTDinterwordspacing

\bibitem{jiang2024efficientllminferenceioaware}
\BIBentryALTinterwordspacing
C.~Jiang, L.~Gao, H.~E. Zarch, and M.~Annavaram, ``Efficient llm inference with
  i/o-aware partial kv cache recomputation,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2411.17089}
\BIBentrySTDinterwordspacing

\bibitem{athiwaratkun2024bifurcatedattentionacceleratingmassively}
\BIBentryALTinterwordspacing
B.~Athiwaratkun, S.~K. Gonugondla, S.~K. Gouda, H.~Qian, H.~Ding, Q.~Sun,
  J.~Wang, J.~Guo, L.~Chen, P.~Bhatia, R.~Nallapati, S.~Sengupta, and B.~Xiang,
  ``Bifurcated attention: Accelerating massively parallel decoding with shared
  prefixes in llms,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2403.08845}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/abs-2410-05004}
\BIBentryALTinterwordspacing
S.~Gao, Y.~Chen, and J.~Shu, ``Fast state restoration in {LLM} serving with
  hcache,'' \emph{CoRR}, vol. abs/2410.05004, 2024. [Online]. Available:
  \url{https://doi.org/10.48550/arXiv.2410.05004}
\BIBentrySTDinterwordspacing

\bibitem{jin2024computeloadkvcache}
\BIBentryALTinterwordspacing
S.~Jin, X.~Liu, Q.~Zhang, and Z.~M. Mao, ``Compute or load kv cache? why not
  both?'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2410.03065}
\BIBentrySTDinterwordspacing

\bibitem{shen2024fastswitchoptimizingcontextswitching}
\BIBentryALTinterwordspacing
A.~Shen, Z.~Li, and M.~Gao, ``Fastswitch: Optimizing context switching
  efficiency in fairness-aware large language model serving,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2411.18424}
\BIBentrySTDinterwordspacing

\bibitem{juravsky2024hydragenhighthroughputllminference}
\BIBentryALTinterwordspacing
J.~Juravsky, B.~Brown, R.~Ehrlich, D.~Y. Fu, C.~R, and A.~Mirhoseini,
  ``Hydragen: High-throughput llm inference with shared prefixes,'' 2024.
  [Online]. Available: \url{https://arxiv.org/abs/2402.05099}
\BIBentrySTDinterwordspacing

\bibitem{yao2024deftdecodingflashtreeattention}
\BIBentryALTinterwordspacing
J.~Yao, K.~Chen, K.~Zhang, J.~You, B.~Yuan, Z.~Wang, and T.~Lin, ``Deft:
  Decoding with flash tree-attention for efficient tree-structured llm
  inference,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2404.00242}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/osdi/YuJKKC22}
\BIBentryALTinterwordspacing
G.~Yu, J.~S. Jeong, G.~Kim, S.~Kim, and B.~Chun, ``Orca: {A} distributed
  serving system for transformer-based generative models,'' in \emph{16th
  {USENIX} Symposium on Operating Systems Design and Implementation, {OSDI}
  2022, Carlsbad, CA, USA, July 11-13, 2022}, M.~K. Aguilera and
  H.~Weatherspoon, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {USENIX}
  Association, 2022, pp. 521--538. [Online]. Available:
  \url{https://www.usenix.org/conference/osdi22/presentation/yu}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:conf/osdi/ZhongLCHZL0024}
\BIBentryALTinterwordspacing
Y.~Zhong, S.~Liu, J.~Chen, J.~Hu, Y.~Zhu, X.~Liu, X.~Jin, and H.~Zhang,
  ``Distserve: Disaggregating prefill and decoding for goodput-optimized large
  language model serving,'' in \emph{18th {USENIX} Symposium on Operating
  Systems Design and Implementation, {OSDI} 2024, Santa Clara, CA, USA, July
  10-12, 2024}, A.~Gavrilovska and D.~B. Terry, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax {USENIX} Association, 2024, pp. 193--210. [Online]. Available:
  \url{https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin}
\BIBentrySTDinterwordspacing

\bibitem{guldogan2024multibin}
\BIBentryALTinterwordspacing
O.~Guldogan, J.~Kunde, K.~Lee, and R.~Pedarsani, ``Multi-bin batching for
  increasing {LLM} inference throughput,'' 2024. [Online]. Available:
  \url{https://openreview.net/forum?id=WVmarX0RNd}
\BIBentrySTDinterwordspacing

\bibitem{shyam2024treeattentiontopologyawaredecoding}
\BIBentryALTinterwordspacing
V.~Shyam, J.~Pilault, E.~Shepperd, Q.~Anthony, and B.~Millidge, ``Tree
  attention: Topology-aware decoding for long-context attention on gpu
  clusters,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2408.04093}
\BIBentrySTDinterwordspacing

\bibitem{xiong2024layerkvoptimizinglargelanguage}
\BIBentryALTinterwordspacing
Y.~Xiong, H.~Wu, C.~Shao, Z.~Wang, R.~Zhang, Y.~Guo, J.~Zhao, K.~Zhang, and
  Z.~Pan, ``Layerkv: Optimizing large language model serving with layer-wise kv
  cache management,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2410.00428}
\BIBentrySTDinterwordspacing

\bibitem{gao2024costefficientlargelanguagemodel}
\BIBentryALTinterwordspacing
B.~Gao, Z.~He, P.~Sharma, Q.~Kang, D.~Jevdjic, J.~Deng, X.~Yang, Z.~Yu, and
  P.~Zuo, ``Cost-efficient large language model serving for multi-turn
  conversations with cachedattention,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2403.19708}
\BIBentrySTDinterwordspacing

\bibitem{zhao2024alisaacceleratinglargelanguage}
\BIBentryALTinterwordspacing
Y.~Zhao, D.~Wu, and J.~Wang, ``Alisa: Accelerating large language model
  inference via sparsity-aware kv caching,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2403.17312}
\BIBentrySTDinterwordspacing

\bibitem{shahout2024fastinferenceaugmentedlarge}
\BIBentryALTinterwordspacing
R.~Shahout, C.~Liang, S.~Xin, Q.~Lao, Y.~Cui, M.~Yu, and M.~Mitzenmacher,
  ``Fast inference for augmented large language models,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2410.18248}
\BIBentrySTDinterwordspacing

\bibitem{zheng2024batchllmoptimizinglargebatched}
\BIBentryALTinterwordspacing
Z.~Zheng, X.~Ji, T.~Fang, F.~Zhou, C.~Liu, and G.~Peng, ``Batchllm: Optimizing
  large batched llm inference with global prefix sharing and
  throughput-oriented token batching,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2412.03594}
\BIBentrySTDinterwordspacing

\bibitem{zheng2024sglangefficientexecutionstructured}
\BIBentryALTinterwordspacing
L.~Zheng, L.~Yin, Z.~Xie, C.~Sun, J.~Huang, C.~H. Yu, S.~Cao, C.~Kozyrakis,
  I.~Stoica, J.~E. Gonzalez, C.~Barrett, and Y.~Sheng, ``Sglang: Efficient
  execution of structured language model programs,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2312.07104}
\BIBentrySTDinterwordspacing

\bibitem{ye2024chunkattentionefficientselfattentionprefixaware}
\BIBentryALTinterwordspacing
L.~Ye, Z.~Tao, Y.~Huang, and Y.~Li, ``Chunkattention: Efficient self-attention
  with prefix-aware kv cache and two-phase partition,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2402.15220}
\BIBentrySTDinterwordspacing

\bibitem{hu2024memservecontextcachingdisaggregated}
\BIBentryALTinterwordspacing
C.~Hu, H.~Huang, J.~Hu, J.~Xu, X.~Chen, T.~Xie, C.~Wang, S.~Wang, Y.~Bao,
  N.~Sun, and Y.~Shan, ``Memserve: Context caching for disaggregated llm
  serving with elastic memory pool,'' 2024. [Online]. Available:
  \url{https://arxiv.org/abs/2406.17565}
\BIBentrySTDinterwordspacing

\bibitem{yao2024cacheblend}
J.~Yao, H.~Li, Y.~Liu, S.~Ray, Y.~Cheng, Q.~Zhang, K.~Du, S.~Lu, and J.~Jiang,
  ``Cacheblend: Fast large language model serving with cached knowledge
  fusion,'' \emph{arXiv preprint arXiv:2405.16444}, 2024.

\bibitem{zhao2022survey}
Y.~Zhao and J.~Chen, ``A survey on differential privacy for unstructured data
  content,'' \emph{ACM Computing Surveys (CSUR)}, vol.~54, no. 10s, pp. 1--28,
  2022.

\bibitem{dong2021residual}
W.~Dong and K.~Yi, ``Residual sensitivity for differentially private multi-way
  joins,'' in \emph{Proceedings of the 2021 International Conference on
  Management of Data}, 2021, pp. 432--444.

\bibitem{dong2023continual}
W.~Dong, Q.~Luo, and K.~Yi, ``Continual observation under user-level
  differential privacy,'' in \emph{2023 IEEE Symposium on Security and Privacy
  (SP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 2190--2207.

\bibitem{an_l-eval:_2023}
\BIBentryALTinterwordspacing
C.~An, S.~Gong, M.~Zhong, X.~Zhao, M.~Li, J.~Zhang, L.~Kong, and X.~Qiu,
  ``L-{Eval}: {Instituting} {Standardized} {Evaluation} for {Long} {Context}
  {Language} {Models},'' 2023. [Online]. Available:
  \url{https://arxiv.org/abs/2307.11088}
\BIBentrySTDinterwordspacing

\bibitem{kwan_m4le:_2023}
Kwan, Wai-Chung, Zeng, Xingshan, Wang, Yufei, Sun, Yusen, L.~Liangyou, Shang,
  Lifeng, Liu, Qun, Wong, and Kam-Fai, ``M4le: A multi-ability multi-range
  multi-task multi-domain long-context evaluation benchmark for large language
  models,'' \emph{arXiv preprint arXiv:2310.19240}, 2023.

\bibitem{dong2023bamboo}
Z.~Dong, T.~Tang, J.~Li, W.~X. Zhao, and J.-R. Wen, ``Bamboo: A comprehensive
  benchmark for evaluating long text modeling capacities of large language
  models,'' \emph{arXiv preprint arXiv:2309.13345}, 2023.

\bibitem{bai_longbench:_2023}
\BIBentryALTinterwordspacing
Y.~Bai, X.~Lv, J.~Zhang, H.~Lyu, J.~Tang, Z.~Huang, Z.~Du, X.~Liu, A.~Zeng,
  L.~Hou, Y.~Dong, J.~Tang, and J.~Li, ``{LongBench}: {A} {Bilingual},
  {Multitask} {Benchmark} for {Long} {Context} {Understanding},'' 2023.
  [Online]. Available: \url{https://arxiv.org/abs/2308.14508}
\BIBentrySTDinterwordspacing

\bibitem{tay_long_lra_2020}
\BIBentryALTinterwordspacing
Y.~Tay, M.~Dehghani, S.~Abnar, Y.~Shen, D.~Bahri, P.~Pham, J.~Rao, L.~Yang,
  S.~Ruder, and D.~Metzler, ``Long {Range} {Arena}: {A} {Benchmark} for
  {Efficient} {Transformers},'' 2020. [Online]. Available:
  \url{https://arxiv.org/abs/2011.04006}
\BIBentrySTDinterwordspacing

\bibitem{shaham_scrolls:_2022}
\BIBentryALTinterwordspacing
U.~Shaham, E.~Segal, M.~Ivgi, A.~Efrat, O.~Yoran, A.~Haviv, A.~Gupta, W.~Xiong,
  M.~Geva, J.~Berant, and O.~Levy, ``{SCROLLS}: {Standardized} {CompaRison}
  {Over} {Long} {Language} {Sequences},'' 2022. [Online]. Available:
  \url{https://arxiv.org/abs/2201.03533}
\BIBentrySTDinterwordspacing

\bibitem{shaham_zeroscrolls:_2023}
\BIBentryALTinterwordspacing
U.~Shaham, M.~Ivgi, A.~Efrat, J.~Berant, and O.~Levy, ``{ZeroSCROLLS}: {A}
  {Zero}-{Shot} {Benchmark} for {Long} {Text} {Understanding},'' 2023.
  [Online]. Available: \url{https://arxiv.org/abs/2305.14196}
\BIBentrySTDinterwordspacing

\bibitem{li_loogle:_2023}
\BIBentryALTinterwordspacing
J.~Li, M.~Wang, Z.~Zheng, and M.~Zhang, ``{LooGLE}: {Can} {Long}-{Context}
  {Language} {Models} {Understand} {Long} {Contexts}?'' 2023. [Online].
  Available: \url{https://arxiv.org/abs/2311.04939}
\BIBentrySTDinterwordspacing

\bibitem{longchat2023}
\BIBentryALTinterwordspacing
D.~Li*, R.~Shao*, A.~Xie, Y.~Sheng, L.~Zheng, J.~E. Gonzalez, I.~S.~X. Ma, ,
  and H.~Zhang, ``How long can open-source llms truly promise on context
  length?'' June 2023. [Online]. Available:
  \url{https://lmsys.org/blog/2023-06-29-longchat}
\BIBentrySTDinterwordspacing

\bibitem{pal2023giraffeadventuresexpandingcontext}
\BIBentryALTinterwordspacing
A.~Pal, D.~Karkhanis, M.~Roberts, S.~Dooley, A.~Sundararajan, and S.~Naidu,
  ``Giraffe: Adventures in expanding context lengths in llms,'' 2023. [Online].
  Available: \url{https://arxiv.org/abs/2308.10882}
\BIBentrySTDinterwordspacing

\bibitem{joshi_triviaqa:_2017}
\BIBentryALTinterwordspacing
M.~Joshi, E.~Choi, D.~S. Weld, and L.~Zettlemoyer, ``{TriviaQA}: {A} {Large}
  {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading}
  {Comprehension},'' 2017. [Online]. Available:
  \url{https://arxiv.org/abs/1705.03551}
\BIBentrySTDinterwordspacing

\bibitem{tseng_towards_tofelqa_2016}
\BIBentryALTinterwordspacing
B.-H. Tseng, S.-S. Shen, H.-Y. Lee, and L.-S. Lee, ``Towards {Machine}
  {Comprehension} of {Spoken} {Content}: {Initial} {TOEFL} {Listening}
  {Comprehension} {Test} by {Machine},'' Aug. 2016, arXiv:1608.06378. [Online].
  Available: \url{http://arxiv.org/abs/1608.06378}
\BIBentrySTDinterwordspacing

\bibitem{sfgram}
N.~Schaetti, ``Sfgram: a dataset containing thousands of scienc-fiction books
  and novels,'' \url{https://github.com/nschaetti/EchoTorch}, 2018.

\bibitem{hendrycks_cuad:_2021}
\BIBentryALTinterwordspacing
D.~Hendrycks, C.~Burns, A.~Chen, and S.~Ball, ``{CUAD}: {An}
  {Expert}-{Annotated} {NLP} {Dataset} for {Legal} {Contract} {Review},'' 2021.
  [Online]. Available: \url{https://arxiv.org/abs/2103.06268}
\BIBentrySTDinterwordspacing

\bibitem{kwiatkowski_nq_2019}
\BIBentryALTinterwordspacing
T.~Kwiatkowski, J.~Palomaki, O.~Redfield, M.~Collins, A.~Parikh, C.~Alberti,
  D.~Epstein, I.~Polosukhin, J.~Devlin, K.~Lee, K.~Toutanova, L.~Jones,
  M.~Kelcey, M.-W. Chang, A.~M. Dai, J.~Uszkoreit, Q.~Le, and S.~Petrov,
  ``\BIBforeignlanguage{en}{Natural {Questions}: {A} {Benchmark} for {Question}
  {Answering} {Research}},'' \emph{\BIBforeignlanguage{en}{Transactions of the
  Association for Computational Linguistics}}, vol.~7, pp. 453--466, Nov. 2019.
  [Online]. Available: \url{https://direct.mit.edu/tacl/article/43518}
\BIBentrySTDinterwordspacing

\bibitem{narrativeqa}
\BIBentryALTinterwordspacing
T.~s~Ko\v~cisk\'y, J.~Schwarz, P.~Blunsom, C.~Dyer, K.~M. Hermann, G.~Melis,
  and E.~Grefenstette, ``The {NarrativeQA} reading comprehension challenge,''
  \emph{Transactions of the Association for Computational Linguistics}, vol.
  TBD, p. TBD, 2018. [Online]. Available: \url{https://TBD}
\BIBentrySTDinterwordspacing

\bibitem{dasigi_dataset_2021_qasper}
\BIBentryALTinterwordspacing
P.~Dasigi, K.~Lo, I.~Beltagy, A.~Cohan, N.~A. Smith, and M.~Gardner, ``A
  {Dataset} of {Information}-{Seeking} {Questions} and {Answers} {Anchored} in
  {Research} {Papers},'' 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2105.03011}
\BIBentrySTDinterwordspacing

\bibitem{yang_hotpotqa:_2018}
\BIBentryALTinterwordspacing
Z.~Yang, P.~Qi, S.~Zhang, Y.~Bengio, W.~W. Cohen, R.~Salakhutdinov, and C.~D.
  Manning, ``{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop
  {Question} {Answering},'' Sep. 2018, arXiv:1809.09600. [Online]. Available:
  \url{http://arxiv.org/abs/1809.09600}
\BIBentrySTDinterwordspacing

\bibitem{pang_quality_dataset:_2021}
\BIBentryALTinterwordspacing
R.~Y. Pang, A.~Parrish, N.~Joshi, N.~Nangia, J.~Phang, A.~Chen, V.~Padmakumar,
  J.~Ma, J.~Thompson, H.~He, and S.~R. Bowman, ``{QuALITY}: {Question}
  {Answering} with {Long} {Input} {Texts}, {Yes}!'' 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2112.08608}
\BIBentrySTDinterwordspacing

\bibitem{ho_multihopqa_2020}
\BIBentryALTinterwordspacing
X.~Ho, A.-K. Duong~Nguyen, S.~Sugawara, and A.~Aizawa,
  ``\BIBforeignlanguage{en}{Constructing {A} {Multi}-hop {QA} {Dataset} for
  {Comprehensive} {Evaluation} of {Reasoning} {Steps}},'' in
  \emph{\BIBforeignlanguage{en}{Proceedings of the 28th {International}
  {Conference} on {Computational} {Linguistics}}}.\hskip 1em plus 0.5em minus
  0.4em\relax Barcelona, Spain (Online): International Committee on
  Computational Linguistics, 2020, pp. 6609--6625. [Online]. Available:
  \url{https://www.aclweb.org/anthology/2020.coling-main.580}
\BIBentrySTDinterwordspacing

\bibitem{trivedi_musique:_2021}
\BIBentryALTinterwordspacing
H.~Trivedi, N.~Balasubramanian, T.~Khot, and A.~Sabharwal, ``{MuSiQue}:
  {Multihop} {Questions} via {Single}-hop {Question} {Composition},'' 2021.
  [Online]. Available: \url{https://arxiv.org/abs/2108.00573}
\BIBentrySTDinterwordspacing

\bibitem{he_dureader:_2017}
\BIBentryALTinterwordspacing
W.~He, K.~Liu, J.~Liu, Y.~Lyu, S.~Zhao, X.~Xiao, Y.~Liu, Y.~Wang, H.~Wu,
  Q.~She, X.~Liu, T.~Wu, and H.~Wang, ``{DuReader}: a {Chinese} {Machine}
  {Reading} {Comprehension} {Dataset} from {Real}-world {Applications},'' 2017.
  [Online]. Available: \url{https://arxiv.org/abs/1711.05073}
\BIBentrySTDinterwordspacing

\bibitem{nallapati_abstractive_dailymail_2016}
\BIBentryALTinterwordspacing
R.~Nallapati, B.~Zhou, C.~Dos~Santos, C.~Gulcehre, and B.~Xiang,
  ``\BIBforeignlanguage{en}{Abstractive {Text} {Summarization} using
  {Sequence}-to-sequence {RNNs} and {Beyond}},'' in
  \emph{\BIBforeignlanguage{en}{Proceedings of {The} 20th {SIGNLL} {Conference}
  on {Computational} {Natural} {Language} {Learning}}}.\hskip 1em plus 0.5em
  minus 0.4em\relax Berlin, Germany: Association for Computational Linguistics,
  2016, pp. 280--290. [Online]. Available:
  \url{http://aclweb.org/anthology/K16-1028}
\BIBentrySTDinterwordspacing

\bibitem{narayan_dont_xsum_2018}
\BIBentryALTinterwordspacing
S.~Narayan, S.~B. Cohen, and M.~Lapata, ``Don't {Give} {Me} the {Details},
  {Just} the {Summary}! {Topic}-{Aware} {Convolutional} {Neural} {Networks} for
  {Extreme} {Summarization},'' 2018. [Online]. Available:
  \url{https://arxiv.org/abs/1808.08745}
\BIBentrySTDinterwordspacing

\bibitem{zhong_qmsum:_2021}
\BIBentryALTinterwordspacing
M.~Zhong, D.~Yin, T.~Yu, A.~Zaidi, M.~Mutuma, R.~Jha, A.~H. Awadallah,
  A.~Celikyilmaz, Y.~Liu, X.~Qiu, and D.~Radev, ``{QMSum}: {A} {New}
  {Benchmark} for {Query}-based {Multi}-domain {Meeting} {Summarization},''
  2021. [Online]. Available: \url{https://arxiv.org/abs/2104.05938}
\BIBentrySTDinterwordspacing

\bibitem{fabbri_multi-news:_2019}
\BIBentryALTinterwordspacing
A.~R. Fabbri, I.~Li, T.~She, S.~Li, and D.~R. Radev, ``Multi-{News}: a
  {Large}-{Scale} {Multi}-{Document} {Summarization} {Dataset} and
  {Abstractive} {Hierarchical} {Model},'' 2019. [Online]. Available:
  \url{https://arxiv.org/abs/1906.01749}
\BIBentrySTDinterwordspacing

\bibitem{huang_govreport_2021}
\BIBentryALTinterwordspacing
L.~Huang, S.~Cao, N.~Parulian, H.~Ji, and L.~Wang, ``Efficient {Attentions} for
  {Long} {Document} {Summarization},'' Apr. 2021, arXiv:2104.02112. [Online].
  Available: \url{http://arxiv.org/abs/2104.02112}
\BIBentrySTDinterwordspacing

\bibitem{wu_vcsum:_2023}
\BIBentryALTinterwordspacing
H.~Wu, M.~Zhan, H.~Tan, Z.~Hou, D.~Liang, and L.~Song, ``{VCSUM}: {A}
  {Versatile} {Chinese} {Meeting} {Summarization} {Dataset},'' 2023. [Online].
  Available: \url{https://arxiv.org/abs/2305.05280}
\BIBentrySTDinterwordspacing

\bibitem{chen_summscreen:_2021}
\BIBentryALTinterwordspacing
M.~Chen, Z.~Chu, S.~Wiseman, and K.~Gimpel, ``{SummScreen}: {A} {Dataset} for
  {Abstractive} {Screenplay} {Summarization},'' 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2104.07091}
\BIBentrySTDinterwordspacing

\bibitem{sharma_bigpatent:_2019}
\BIBentryALTinterwordspacing
E.~Sharma, C.~Li, and L.~Wang, ``\BIBforeignlanguage{en}{{BIGPATENT}: {A}
  {Large}-{Scale} {Dataset} for {Abstractive} and {Coherent}
  {Summarization}},'' in \emph{\BIBforeignlanguage{en}{Proceedings of the 57th
  {Annual} {Meeting} of the {Association} for {Computational}
  {Linguistics}}}.\hskip 1em plus 0.5em minus 0.4em\relax Florence, Italy:
  Association for Computational Linguistics, 2019, pp. 2204--2213. [Online].
  Available: \url{https://www.aclweb.org/anthology/P19-1212}
\BIBentrySTDinterwordspacing

\bibitem{angelidis_extractive_space_2021}
\BIBentryALTinterwordspacing
S.~Angelidis, R.~K. Amplayo, Y.~Suhara, X.~Wang, and M.~Lapata,
  ``\BIBforeignlanguage{en}{Extractive {Opinion} {Summarization} in {Quantized}
  {Transformer} {Spaces}},'' \emph{\BIBforeignlanguage{en}{Transactions of the
  Association for Computational Linguistics}}, vol.~9, pp. 277--293, Mar. 2021.
  [Online]. Available:
  \url{https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00366/98621/Extractive-Opinion-Summarization-in-Quantized}
\BIBentrySTDinterwordspacing

\bibitem{wang2022squality}
A.~Wang, R.~Y. Pang, A.~Chen, J.~Phang, and S.~R. Bowman, ``S{Q}u{ALITY}:
  Building a long-document summarization dataset the hard way,'' \emph{arXiv
  preprint 2205.11465}, 2022.

\bibitem{zhu_ncls:_2019}
\BIBentryALTinterwordspacing
J.~Zhu, Q.~Wang, Y.~Wang, Y.~Zhou, J.~Zhang, S.~Wang, and C.~Zong,
  ``\BIBforeignlanguage{en}{{NCLS}: {Neural} {Cross}-{Lingual}
  {Summarization}},'' in \emph{\BIBforeignlanguage{en}{Proceedings of the 2019
  {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}
  and the 9th {International} {Joint} {Conference} on {Natural} {Language}
  {Processing} ({EMNLP}-{IJCNLP})}}.\hskip 1em plus 0.5em minus 0.4em\relax
  Hong Kong, China: Association for Computational Linguistics, 2019, pp.
  3052--3062. [Online]. Available:
  \url{https://www.aclweb.org/anthology/D19-1302}
\BIBentrySTDinterwordspacing

\bibitem{koreeda-manning-2021-contractnli-dataset}
\BIBentryALTinterwordspacing
Y.~Koreeda and C.~Manning, ``{C}ontract{NLI}: A dataset for document-level
  natural language inference for contracts,'' in \emph{Findings of the
  Association for Computational Linguistics: EMNLP 2021}.\hskip 1em plus 0.5em
  minus 0.4em\relax Punta Cana, Dominican Republic: Association for
  Computational Linguistics, Nov. 2021, pp. 1907--1919. [Online]. Available:
  \url{https://aclanthology.org/2021.findings-emnlp.164}
\BIBentrySTDinterwordspacing

\bibitem{cobbe_traininggsm8k_2021}
\BIBentryALTinterwordspacing
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert,
  J.~Tworek, J.~Hilton, R.~Nakano, C.~Hesse, and J.~Schulman, ``Training
  {Verifiers} to {Solve} {Math} {Word} {Problems},'' 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2110.14168}
\BIBentrySTDinterwordspacing

\bibitem{petukhova_mn-ds:_2023}
\BIBentryALTinterwordspacing
A.~Petukhova and N.~Fachada, ``\BIBforeignlanguage{en}{{MN}-{DS}: {A}
  {Multilabeled} {News} {Dataset} for {News} {Articles} {Hierarchical}
  {Classification}},'' \emph{\BIBforeignlanguage{en}{Data}}, vol.~8, no.~5,
  p.~74, Apr. 2023. [Online]. Available:
  \url{https://www.mdpi.com/2306-5729/8/5/74}
\BIBentrySTDinterwordspacing

\bibitem{zhu_natural_clts_2020}
X.~Zhu, \emph{\BIBforeignlanguage{eng}{Natural {Language} {Processing} and
  {Chinese} {Computing}: 9th {CCF} {International} {Conference}, {NLPCC} 2020,
  {Zhengzhou}, {China}, {October} 14-18, 2020, {Proceedings}, {Part} {I}}},
  ser. Lecture {Notes} in {Computer} {Science} {Ser}.\hskip 1em plus 0.5em
  minus 0.4em\relax Cham: Springer International Publishing AG, 2020, no.
  v.12430.

\bibitem{rajpurkar-etal-2016-squad-F1}
\BIBentryALTinterwordspacing
P.~Rajpurkar, J.~Zhang, K.~Lopyrev, and P.~Liang, ``{SQ}u{AD}: 100,000+
  questions for machine comprehension of text,'' in \emph{Proceedings of the
  2016 Conference on Empirical Methods in Natural Language Processing}, J.~Su,
  K.~Duh, and X.~Carreras, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Austin,
  Texas: Association for Computational Linguistics, Nov. 2016, pp. 2383--2392.
  [Online]. Available: \url{https://aclanthology.org/D16-1264}
\BIBentrySTDinterwordspacing

\bibitem{guo_longcoder_lcc:_2023}
\BIBentryALTinterwordspacing
D.~Guo, C.~Xu, N.~Duan, J.~Yin, and J.~McAuley, ``{LongCoder}: {A}
  {Long}-{Range} {Pre}-trained {Language} {Model} for {Code} {Completion},''
  2023. [Online]. Available: \url{https://arxiv.org/abs/2306.14893}
\BIBentrySTDinterwordspacing

\bibitem{feng_multidoc2dial:_2021}
\BIBentryALTinterwordspacing
S.~Feng, S.~S. Patel, H.~Wan, and S.~Joshi,
  ``\BIBforeignlanguage{en}{{MultiDoc2Dial}: {Modeling} {Dialogues} {Grounded}
  in {Multiple} {Documents}},'' in \emph{\BIBforeignlanguage{en}{Proceedings of
  the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language}
  {Processing}}}.\hskip 1em plus 0.5em minus 0.4em\relax Online and Punta Cana,
  Dominican Republic: Association for Computational Linguistics, 2021, pp.
  6162--6176. [Online]. Available:
  \url{https://aclanthology.org/2021.emnlp-main.498}
\BIBentrySTDinterwordspacing

\bibitem{feng_doc2dial:_2020}
\BIBentryALTinterwordspacing
S.~Feng, H.~Wan, C.~Gunasekara, S.~S. Patel, S.~Joshi, and L.~A. Lastras,
  ``doc2dial: {A} {Goal}-{Oriented} {Document}-{Grounded} {Dialogue}
  {Dataset},'' 2020. [Online]. Available:
  \url{https://arxiv.org/abs/2011.06623}
\BIBentrySTDinterwordspacing

\bibitem{yuan_can_asap_review_2021}
\BIBentryALTinterwordspacing
W.~Yuan, P.~Liu, and G.~Neubig, ``Can {We} {Automate} {Scientific}
  {Reviewing}?'' 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2102.00176}
\BIBentrySTDinterwordspacing

\bibitem{liu_repobench:_2023}
\BIBentryALTinterwordspacing
T.~Liu, C.~Xu, and J.~McAuley, ``{RepoBench}: {Benchmarking}
  {Repository}-{Level} {Code} {Auto}-{Completion} {Systems},'' 2023. [Online].
  Available: \url{https://arxiv.org/abs/2306.03091}
\BIBentrySTDinterwordspacing

\bibitem{DBLP:journals/corr/RajpurkarZLL16}
\BIBentryALTinterwordspacing
P.~Rajpurkar, J.~Zhang, K.~Lopyrev, and P.~Liang, ``Squad: 100, 000+ questions
  for machine comprehension of text,'' \emph{CoRR}, vol. abs/1606.05250, 2016.
  [Online]. Available: \url{http://arxiv.org/abs/1606.05250}
\BIBentrySTDinterwordspacing

\bibitem{papineni-etal-2002-bleu}
\BIBentryALTinterwordspacing
K.~Papineni, S.~Roukos, T.~Ward, and W.-J. Zhu, ``{B}leu: a method for
  automatic evaluation of machine translation,'' in \emph{Proceedings of the
  40th Annual Meeting of the Association for Computational Linguistics},
  P.~Isabelle, E.~Charniak, and D.~Lin, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Philadelphia, Pennsylvania, USA: Association for Computational
  Linguistics, Jul. 2002, pp. 311--318. [Online]. Available:
  \url{https://aclanthology.org/P02-1040}
\BIBentrySTDinterwordspacing

\bibitem{post-2018-call-sacrebleu}
\BIBentryALTinterwordspacing
M.~Post, ``A call for clarity in reporting {BLEU} scores,'' in
  \emph{Proceedings of the Third Conference on Machine Translation: Research
  Papers}, O.~Bojar, R.~Chatterjee, C.~Federmann, M.~Fishel, Y.~Graham,
  B.~Haddow, M.~Huck, A.~J. Yepes, P.~Koehn, C.~Monz, M.~Negri,
  A.~N{\'e}v{\'e}ol, M.~Neves, M.~Post, L.~Specia, M.~Turchi, and K.~Verspoor,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Brussels, Belgium: Association
  for Computational Linguistics, Oct. 2018, pp. 186--191. [Online]. Available:
  \url{https://aclanthology.org/W18-6319}
\BIBentrySTDinterwordspacing

\bibitem{lin-2004-rouge}
\BIBentryALTinterwordspacing
C.-Y. Lin, ``{ROUGE}: A package for automatic evaluation of summaries,'' in
  \emph{Text Summarization Branches Out}.\hskip 1em plus 0.5em minus
  0.4em\relax Barcelona, Spain: Association for Computational Linguistics, Jul.
  2004, pp. 74--81. [Online]. Available:
  \url{https://aclanthology.org/W04-1013}
\BIBentrySTDinterwordspacing

\bibitem{denkowski-lavie-2011-meteor}
\BIBentryALTinterwordspacing
M.~Denkowski and A.~Lavie, ``Meteor 1.3: Automatic metric for reliable
  optimization and evaluation of machine translation systems,'' in
  \emph{Proceedings of the Sixth Workshop on Statistical Machine Translation},
  C.~Callison-Burch, P.~Koehn, C.~Monz, and O.~F. Zaidan, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Edinburgh, Scotland: Association for Computational
  Linguistics, Jul. 2011, pp. 85--91. [Online]. Available:
  \url{https://aclanthology.org/W11-2107}
\BIBentrySTDinterwordspacing

\bibitem{zhang2020bertscoreevaluatingtextgeneration}
\BIBentryALTinterwordspacing
T.~Zhang, V.~Kishore, F.~Wu, K.~Q. Weinberger, and Y.~Artzi, ``Bertscore:
  Evaluating text generation with bert,'' 2020. [Online]. Available:
  \url{https://arxiv.org/abs/1904.09675}
\BIBentrySTDinterwordspacing

\bibitem{chen2021evaluatinglargelanguagemodels}
\BIBentryALTinterwordspacing
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. de~Oliveira~Pinto, J.~Kaplan,
  H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, A.~Ray, R.~Puri, G.~Krueger,
  M.~Petrov, H.~Khlaaf, G.~Sastry, P.~Mishkin, B.~Chan, S.~Gray, N.~Ryder,
  M.~Pavlov, A.~Power, L.~Kaiser, M.~Bavarian, C.~Winter, P.~Tillet, F.~P.
  Such, D.~Cummings, M.~Plappert, F.~Chantzis, E.~Barnes, A.~Herbert-Voss,
  W.~H. Guss, A.~Nichol, A.~Paino, N.~Tezak, J.~Tang, I.~Babuschkin, S.~Balaji,
  S.~Jain, W.~Saunders, C.~Hesse, A.~N. Carr, J.~Leike, J.~Achiam, V.~Misra,
  E.~Morikawa, A.~Radford, M.~Knight, M.~Brundage, M.~Murati, K.~Mayer,
  P.~Welinder, B.~McGrew, D.~Amodei, S.~McCandlish, I.~Sutskever, and
  W.~Zaremba, ``Evaluating large language models trained on code,'' 2021.
  [Online]. Available: \url{https://arxiv.org/abs/2107.03374}
\BIBentrySTDinterwordspacing

\bibitem{liu2023llava}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee, ``Visual instruction tuning,'' in
  \emph{NeurIPS}, 2023.

\bibitem{MMBench}
Y.~Z. Yuan~Liu, Haodong~Duan \emph{et~al.}, ``Mmbench: Is your multi-modal
  model an all-around player?'' \emph{arXiv:2307.06281}, 2023.

\bibitem{song2024milebench}
D.~Song, S.~Chen, G.~H. Chen, F.~Yu, X.~Wan, and B.~Wang, ``Milebench:
  Benchmarking mllms in long context,'' \emph{arXiv preprint arXiv:2404.18532},
  2024.

\bibitem{MLVU}
J.~Zhou, Y.~Shu, B.~Zhao, B.~Wu, S.~Xiao, X.~Yang, Y.~Xiong, B.~Zhang,
  T.~Huang, and Z.~Liu, ``Mlvu: A comprehensive benchmark for multi-task long
  video understanding,'' \emph{arXiv preprint arXiv:2406.04264}, 2024.

\bibitem{wu2024longvideobench}
\BIBentryALTinterwordspacing
H.~Wu, D.~Li, B.~Chen, and J.~Li, ``Longvideobench: A benchmark for
  long-context interleaved video-language understanding,'' 2024. [Online].
  Available: \url{https://arxiv.org/abs/2407.15754}
\BIBentrySTDinterwordspacing

\bibitem{fu2024video}
C.~Fu, Y.~Dai, Y.~Luo, L.~Li, S.~Ren, R.~Zhang, Z.~Wang, C.~Zhou, Y.~Shen,
  M.~Zhang \emph{et~al.}, ``Video-mme: The first-ever comprehensive evaluation
  benchmark of multi-modal llms in video analysis,'' \emph{arXiv preprint
  arXiv:2405.21075}, 2024.

\bibitem{xiao2021next}
J.~Xiao, X.~Shang, A.~Yao, and T.-S. Chua, ``Next-qa: Next phase of
  question-answering to explaining temporal actions,'' in \emph{Proceedings of
  the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  June 2021, pp. 9777--9786.

\bibitem{2023videochat}
K.~Li, Y.~He, Y.~Wang, Y.~Li, W.~Wang, P.~Luo, Y.~Wang, L.~Wang, and Y.~Qiao,
  ``Videochat: Chat-centric video understanding,'' \emph{arXiv preprint
  arXiv:2305.06355}, 2023.

\bibitem{xu2017video}
D.~Xu, Z.~Zhao, J.~Xiao, F.~Wu, H.~Zhang, X.~He, and Y.~Zhuang, ``Video
  question answering via gradually refined attention over appearance and
  motion,'' in \emph{ACM Multimedia}, 2017.

\bibitem{k2012newsimilaritymeasuretaxonomy}
\BIBentryALTinterwordspacing
M.~S. K, K.~C. Shet, and U.~D. Acharya, ``A new similarity measure for taxonomy
  based on edge counting,'' 2012. [Online]. Available:
  \url{https://arxiv.org/abs/1211.4709}
\BIBentrySTDinterwordspacing

\end{thebibliography}
