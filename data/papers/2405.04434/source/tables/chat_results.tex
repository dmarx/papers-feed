\begin{table}[!t]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{@{}c c l c | cc c |c c c@{}}
    \toprule
    & \multirow{2}{*}{\centering \textbf{Benchmark}} & \multirow{2}{*}{\textbf{\# Shots}} & \textbf{DeepSeek} & \textbf{Qwen 1.5} & \textbf{LLaMA3} & \textbf{Mixtral} & \textbf{DeepSeek-V2} & \textbf{DeepSeek-V2} \\
    & & & \textbf{67B Chat} & \textbf{72B Chat} & \textbf{70B Inst.} & \textbf{8x22B Inst.} & \textbf{Chat (SFT)} & \textbf{Chat (RL)} \\
    \midrule
    & Context Length & - & 4K & 32K & 8K & 64K & 128K & 128K\\
    & Architecture & - & Dense & Dense & Dense & MoE & MoE & MoE\\
    & \# Activated Params & - & 67B & 72B & 70B & 39B & 21B & 21B \\
    & \# Total Params & - & 67B & 72B & 70B & 141B & 236B & 236B \\
    \midrule
    \multirow{8}{*}{English} & TriviaQA & 5-shot & 81.5 & 79.6 & 69.1 & 80.0 & \underline{85.4} & \textbf{86.7} \\
    & NaturalQuestions & 5-shot & 47.0 & 46.9 & 44.6 & \textbf{54.9} & 51.9 & \underline{53.4} \\
    & MMLU & 5-shot & 71.1 & 76.2 & \textbf{80.3} & 77.8 & \underline{78.4} & 77.8 \\
    & ARC-Easy & 25-shot & 96.6 & 96.8 & 96.9 & 97.1 & \underline{97.6} &  \textbf{98.1} \\
    & ARC-Challenge & 25-shot & 88.9 & \underline{91.7} & \textbf{92.6} & 90.0 & \textbf{92.5} & \textbf{92.3} \\
    & BBH & 3-shot & 71.7 & 65.9 & \underline{80.1} & 78.4 & \textbf{81.3} & 79.7 \\
    & AGIEval & 0-shot & 46.4 & \underline{62.8} & 56.6 & 41.4 & \textbf{63.2} & 61.4 \\
    & IFEval & 0-shot & 55.5 & 57.3 & \textbf{79.7} & \underline{72.1} & 64.1 & 63.8 \\
    \midrule
    \multirow{4}{*}{Code} & HumanEval & 0-shot & 73.8 & 68.9 & 76.2 & 75.0 & \underline{76.8} & \textbf{81.1} \\
    & MBPP & 3-shot & 61.4 & 52.2 & 69.8 & 64.4 & \underline{70.4} & \textbf{72.0} \\
    & CRUXEval-I-COT & 2-shot & 49.1 & 51.4 & \underline{61.1} & 59.4 & 59.5 & \textbf{61.5} \\
    & CRUXEval-O-COT & 2-shot & 50.9 & 56.5 & \textbf{63.6} & \textbf{63.6} & 60.7 & \underline{63.0}\\
    & LiveCodeBench & 0-shot & 18.3 & 18.8 & \underline{30.5} &25.0 & 28.7 &\textbf{32.5}\\
    \midrule
    \multirow{4}{*}{Math} & GSM8K & 8-shot & 84.1 & 81.9 & \textbf{93.2} & 87.9 & 90.8 & \underline{92.2} \\
    & MATH & 4-shot & 32.6 & 40.6 & 48.5 & 49.8 & \underline{52.7}& \textbf{53.9}  \\
    & CMath & 0-shot & 80.3 & \textbf{82.8} & 79.2 & 75.1 & \underline{82.0} & \underline{81.9} \\
    \midrule
    \multirow{3}{*}{Chinese} & CLUEWSC & 5-shot & 78.5 & \textbf{90.1} & 85.4 & 75.8 & \underline{88.6} & \textbf{89.9} \\
    & C-Eval & 5-shot & 65.2 & \textbf{82.2} & 67.9 & 60.0 & \underline{80.9} & 78.0 \\
    & CMMLU & 5-shot & 67.8 & \textbf{82.9} & 70.7 & 61.0 & \underline{82.4} & 81.6 \\
    \bottomrule
    \end{tabular}
    \caption{
    Comparison among \dsviisft{}, \dsviirl{}, and other representative open-source chat models. 
    Regarding TriviaQA and NaturalQuestions, it is worth noting that chat models, such as LLaMA3 70B Instruct, might not strictly adhere to the format constraints typically specified in the few-shot setting. 
    Consequently, this can lead to underestimation of certain models in our evaluation framework.
    }
    \label{tab:sft}
\end{table}