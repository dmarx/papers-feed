\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[AI@Meta(2024)]{llama3}
AI@Meta.
\newblock Llama 3 model card, 2024.
\newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebr{\'o}n, and Sanghai]{ainslie2023gqa}
J.~Ainslie, J.~Lee-Thorp, M.~de~Jong, Y.~Zemlyanskiy, F.~Lebr{\'o}n, and S.~Sanghai.
\newblock Gqa: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock \emph{arXiv preprint arXiv:2305.13245}, 2023.

\bibitem[Anthropic(2023)]{claude}
Anthropic.
\newblock Introducing {Claude}, 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{mbpp}
J.~Austin, A.~Odena, M.~Nye, M.~Bosma, H.~Michalewski, D.~Dohan, E.~Jiang, C.~Cai, M.~Terry, Q.~Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{qwen}
J.~Bai, S.~Bai, Y.~Chu, Z.~Cui, K.~Dang, X.~Deng, Y.~Fan, W.~Ge, Y.~Han, F.~Huang, B.~Hui, L.~Ji, M.~Li, J.~Lin, R.~Lin, D.~Liu, G.~Liu, C.~Lu, K.~Lu, J.~Ma, R.~Men, X.~Ren, X.~Ren, C.~Tan, S.~Tan, J.~Tu, P.~Wang, S.~Wang, W.~Wang, S.~Wu, B.~Xu, J.~Xu, A.~Yang, H.~Yang, J.~Yang, S.~Yang, Y.~Yao, B.~Yu, H.~Yuan, Z.~Yuan, J.~Zhang, X.~Zhang, Y.~Zhang, Z.~Zhang, C.~Zhou, J.~Zhou, X.~Zhou, and T.~Zhu.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{piqa}
Y.~Bisk, R.~Zellers, R.~L. Bras, J.~Gao, and Y.~Choi.
\newblock {PIQA:} reasoning about physical commonsense in natural language.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pages 7432--7439. {AAAI} Press, 2020.
\newblock \doi{10.1609/aaai.v34i05.6239}.
\newblock URL \url{https://doi.org/10.1609/aaai.v34i05.6239}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert{-}Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{codex}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. de~Oliveira~Pinto, J.~Kaplan, H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, A.~Ray, R.~Puri, G.~Krueger, M.~Petrov, H.~Khlaaf, G.~Sastry, P.~Mishkin, B.~Chan, S.~Gray, N.~Ryder, M.~Pavlov, A.~Power, L.~Kaiser, M.~Bavarian, C.~Winter, P.~Tillet, F.~P. Such, D.~Cummings, M.~Plappert, F.~Chantzis, E.~Barnes, A.~Herbert{-}Voss, W.~H. Guss, A.~Nichol, A.~Paino, N.~Tezak, J.~Tang, I.~Babuschkin, S.~Balaji, S.~Jain, W.~Saunders, C.~Hesse, A.~N. Carr, J.~Leike, J.~Achiam, V.~Misra, E.~Morikawa, A.~Radford, M.~Knight, M.~Brundage, M.~Murati, K.~Mayer, P.~Welinder, B.~McGrew, D.~Amodei, S.~McCandlish, I.~Sutskever, and W.~Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{CoRR}, abs/2107.03374, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{arc}
P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and O.~Tafjord.
\newblock Think you have solved question answering? try arc, the {AI2} reasoning challenge.
\newblock \emph{CoRR}, abs/1803.05457, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.05457}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{gsm8k}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert, J.~Tworek, J.~Hilton, R.~Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Cui et~al.(2019)Cui, Liu, Che, Xiao, Chen, Ma, Wang, and Hu]{cui-etal-2019-span}
Y.~Cui, T.~Liu, W.~Che, L.~Xiao, Z.~Chen, W.~Ma, S.~Wang, and G.~Hu.
\newblock A span-extraction dataset for {C}hinese machine reading comprehension.
\newblock In K.~Inui, J.~Jiang, V.~Ng, and X.~Wan, editors, \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 5883--5889, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1600}.
\newblock URL \url{https://aclanthology.org/D19-1600}.

\bibitem[Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu, Xie, Li, Huang, Luo, Ruan, Sui, and Liang]{deepseekmoe}
D.~Dai, C.~Deng, C.~Zhao, R.~X. Xu, H.~Gao, D.~Chen, J.~Li, W.~Zeng, X.~Yu, Y.~Wu, Z.~Xie, Y.~K. Li, P.~Huang, F.~Luo, C.~Ruan, Z.~Sui, and W.~Liang.
\newblock Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.
\newblock \emph{CoRR}, abs/2401.06066, 2024.
\newblock URL \url{https://doi.org/10.48550/arXiv.2401.06066}.

\bibitem[Dao(2023)]{dao2023flashattention2}
T.~Dao.
\newblock Flash{A}ttention-2: Faster attention with better parallelism and work partitioning, 2023.

\bibitem[DeepSeek-AI(2024)]{deepseek1}
DeepSeek-AI.
\newblock Deepseek {LLM:} scaling open-source language models with longtermism.
\newblock \emph{CoRR}, abs/2401.02954, 2024.
\newblock URL \url{https://doi.org/10.48550/arXiv.2401.02954}.

\bibitem[Dua et~al.(2019)Dua, Wang, Dasigi, Stanovsky, Singh, and Gardner]{drop}
D.~Dua, Y.~Wang, P.~Dasigi, G.~Stanovsky, S.~Singh, and M.~Gardner.
\newblock {DROP:} {A} reading comprehension benchmark requiring discrete reasoning over paragraphs.
\newblock In J.~Burstein, C.~Doran, and T.~Solorio, editors, \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)}, pages 2368--2378. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/N19-1246}.
\newblock URL \url{https://doi.org/10.18653/v1/n19-1246}.

\bibitem[Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto]{alpaca2.0}
Y.~Dubois, B.~Galambosi, P.~Liang, and T.~B. Hashimoto.
\newblock Length-controlled alpacaeval: A simple way to debias automatic evaluators.
\newblock \emph{arXiv preprint arXiv:2404.04475}, 2024.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{switch}
W.~Fedus, B.~Zoph, and N.~Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{CoRR}, abs/2101.03961, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.03961}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{pile}
L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang, H.~He, A.~Thite, N.~Nabeshima, et~al.
\newblock The {Pile}: An {800GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Google(2023)]{gemini}
Google.
\newblock Introducing gemini: our largest and most capable ai model, 2023.
\newblock URL \url{https://blog.google/technology/ai/google-gemini-ai/}.

\bibitem[Gu et~al.(2024)Gu, Rozière, Leather, Solar-Lezama, Synnaeve, and Wang]{gu2024cruxeval}
A.~Gu, B.~Rozière, H.~Leather, A.~Solar-Lezama, G.~Synnaeve, and S.~I. Wang.
\newblock Cruxeval: A benchmark for code reasoning, understanding and execution, 2024.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and J.~Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycks2021measuring}
D.~Hendrycks, C.~Burns, S.~Kadavath, A.~Arora, S.~Basart, E.~Tang, D.~Song, and J.~Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{arXiv preprint arXiv:2103.03874}, 2021.

\bibitem[High-flyer(2023)]{haillm}
High-flyer.
\newblock Hai-llm: 高效且轻量的大模型训练工具, 2023.
\newblock URL \url{https://www.high-flyer.cn/en/blog/hai-llm}.

\bibitem[Hooper et~al.(2024)Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer, and Gholami]{kv_quant}
C.~Hooper, S.~Kim, H.~Mohammadzadeh, M.~W. Mahoney, Y.~S. Shao, K.~Keutzer, and A.~Gholami.
\newblock Kvquant: Towards 10 million context length {LLM} inference with {KV} cache quantization.
\newblock \emph{CoRR}, abs/2401.18079, 2024.
\newblock URL \url{https://doi.org/10.48550/arXiv.2401.18079}.

\bibitem[Hu et~al.(2024)Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, et~al.]{minicpm}
S.~Hu, Y.~Tu, X.~Han, C.~He, G.~Cui, X.~Long, Z.~Zheng, Y.~Fang, Y.~Huang, W.~Zhao, et~al.
\newblock Minicpm: Unveiling the potential of small language models with scalable training strategies.
\newblock \emph{arXiv preprint arXiv:2404.06395}, 2024.

\bibitem[Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei, et~al.]{ceval}
Y.~Huang, Y.~Bai, Z.~Zhu, J.~Zhang, J.~Zhang, T.~Su, J.~Liu, C.~Lv, Y.~Zhang, J.~Lei, et~al.
\newblock {C-Eval}: A multi-level multi-discipline chinese evaluation suite for foundation models.
\newblock \emph{arXiv preprint arXiv:2305.08322}, 2023.

\bibitem[Jain et~al.(2024)Jain, Han, Gu, Li, Yan, Zhang, Wang, Solar-Lezama, Sen, and Stoica]{jain2024livecodebench}
N.~Jain, K.~Han, A.~Gu, W.-D. Li, F.~Yan, T.~Zhang, S.~Wang, A.~Solar-Lezama, K.~Sen, and I.~Stoica.
\newblock Livecodebench: Holistic and contamination free evaluation of large language models for code.
\newblock \emph{arXiv preprint arXiv:2403.07974}, 2024.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{joshi-etal-2017-triviaqa}
M.~Joshi, E.~Choi, D.~Weld, and L.~Zettlemoyer.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock In R.~Barzilay and M.-Y. Kan, editors, \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1601--1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1147}.
\newblock URL \url{https://aclanthology.org/P17-1147}.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, Toutanova, Jones, Kelcey, Chang, Dai, Uszkoreit, Le, and Petrov]{naturalquestions}
T.~Kwiatkowski, J.~Palomaki, O.~Redfield, M.~Collins, A.~P. Parikh, C.~Alberti, D.~Epstein, I.~Polosukhin, J.~Devlin, K.~Lee, K.~Toutanova, L.~Jones, M.~Kelcey, M.~Chang, A.~M. Dai, J.~Uszkoreit, Q.~Le, and S.~Petrov.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 7:\penalty0 452--466, 2019.
\newblock \doi{10.1162/tacl\_a\_00276}.
\newblock URL \url{https://doi.org/10.1162/tacl\_a\_00276}.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023efficient}
W.~Kwon, Z.~Li, S.~Zhuang, Y.~Sheng, L.~Zheng, C.~H. Yu, J.~E. Gonzalez, H.~Zhang, and I.~Stoica.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{race}
G.~Lai, Q.~Xie, H.~Liu, Y.~Yang, and E.~H. Hovy.
\newblock {RACE:} large-scale reading comprehension dataset from examinations.
\newblock In M.~Palmer, R.~Hwa, and S.~Riedel, editors, \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September 9-11, 2017}, pages 785--794. Association for Computational Linguistics, 2017.
\newblock \doi{10.18653/V1/D17-1082}.
\newblock URL \url{https://doi.org/10.18653/v1/d17-1082}.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{gshard}
D.~Lepikhin, H.~Lee, Y.~Xu, D.~Chen, O.~Firat, Y.~Huang, M.~Krikun, N.~Shazeer, and Z.~Chen.
\newblock Gshard: Scaling giant models with conditional computation and automatic sharding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=qrwe7XHTmYb}.

\bibitem[Li et~al.(2023)Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin]{cmmlu}
H.~Li, Y.~Zhang, F.~Koto, Y.~Yang, H.~Zhao, Y.~Gong, N.~Duan, and T.~Baldwin.
\newblock {CMMLU}: Measuring massive multitask language understanding in {Chinese}.
\newblock \emph{arXiv preprint arXiv:2306.09212}, 2023.

\bibitem[Li et~al.(2021)Li, Qi, Sun, Yi, and Zhang]{li2021ccpm}
W.~Li, F.~Qi, M.~Sun, X.~Yi, and J.~Zhang.
\newblock Ccpm: A chinese classical poetry matching dataset, 2021.

\bibitem[Liu et~al.(2023)Liu, Lei, Wang, Huang, Feng, Wen, Cheng, Ke, Xu, Tam, Zhang, Sun, Wang, Zhang, Huang, Dong, and Tang]{align_bench}
X.~Liu, X.~Lei, S.~Wang, Y.~Huang, Z.~Feng, B.~Wen, J.~Cheng, P.~Ke, Y.~Xu, W.~L. Tam, X.~Zhang, L.~Sun, H.~Wang, J.~Zhang, M.~Huang, Y.~Dong, and J.~Tang.
\newblock Alignbench: Benchmarking chinese alignment of large language models.
\newblock \emph{CoRR}, abs/2311.18743, 2023.
\newblock \doi{10.48550/ARXIV.2311.18743}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2311.18743}.

\bibitem[Loshchilov and Hutter(2017)]{adamw}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Mistral(2024)]{mixtral8x22b}
Mistral.
\newblock Cheaper, better, faster, stronger: Continuing to push the frontier of ai and making it accessible to all, 2024.
\newblock URL \url{https://mistral.ai/news/mixtral-8x22b}.

\bibitem[OpenAI(2022)]{chatgpt}
OpenAI.
\newblock Introducing {ChatGPT}, 2022.
\newblock URL \url{https://openai.com/blog/chatgpt}.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock {GPT4} technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang, S.~Agarwal, K.~Slama, A.~Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Peng et~al.(2023)Peng, Quesnelle, Fan, and Shippole]{peng2023yarn}
B.~Peng, J.~Quesnelle, H.~Fan, and E.~Shippole.
\newblock Yarn: Efficient context window extension of large language models.
\newblock \emph{arXiv preprint arXiv:2309.00071}, 2023.

\bibitem[Qi et~al.(2023)Qi, Wan, Huang, and Lin]{qi2023zero}
P.~Qi, X.~Wan, G.~Huang, and M.~Lin.
\newblock Zero bubble pipeline parallelism.
\newblock \emph{arXiv preprint arXiv:2401.10241}, 2023.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and He]{zero}
S.~Rajbhandari, J.~Rasley, O.~Ruwase, and Y.~He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, pages 1--16. IEEE, 2020.

\bibitem[Riquelme et~al.(2021)Riquelme, Puigcerver, Mustafa, Neumann, Jenatton, Pinto, Keysers, and Houlsby]{bpr}
C.~Riquelme, J.~Puigcerver, B.~Mustafa, M.~Neumann, R.~Jenatton, A.~S. Pinto, D.~Keysers, and N.~Houlsby.
\newblock Scaling vision with sparse mixture of experts.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021}, pages 8583--8595, 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html}.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2019winogrande}
K.~Sakaguchi, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale, 2019.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo]{deepseekmath}
Z.~Shao, P.~Wang, Q.~Zhu, R.~Xu, J.~Song, M.~Zhang, Y.~Li, Y.~Wu, and D.~Guo.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Shazeer(2019)]{mqa}
N.~Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{CoRR}, abs/1911.02150, 2019.
\newblock URL \url{http://arxiv.org/abs/1911.02150}.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{moe}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~V. Le, G.~E. Hinton, and J.~Dean.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1ckMDqlg}.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
J.~Su, M.~Ahmed, Y.~Lu, S.~Pan, W.~Bo, and Y.~Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Sun et~al.(2019)Sun, Yu, Yu, and Cardie]{sun2019investigating}
K.~Sun, D.~Yu, D.~Yu, and C.~Cardie.
\newblock Investigating prior knowledge for challenging chinese machine reading comprehension, 2019.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{bbh}
M.~Suzgun, N.~Scales, N.~Sch{\"a}rli, S.~Gehrmann, Y.~Tay, H.~W. Chung, A.~Chowdhery, Q.~V. Le, E.~H. Chi, D.~Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
J.~Wei, Y.~Tay, R.~Bommasani, C.~Raffel, B.~Zoph, S.~Borgeaud, D.~Yogatama, M.~Bosma, D.~Zhou, D.~Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem[Wei et~al.(2023)Wei, Luan, Liu, Dong, and Wang]{wei2023cmath}
T.~Wei, J.~Luan, W.~Liu, S.~Dong, and B.~Wang.
\newblock Cmath: Can your language model pass chinese elementary school math test?, 2023.

\bibitem[Xu et~al.(2020)Xu, Hu, Zhang, Li, Cao, Li, Xu, Sun, Yu, Yu, Tian, Dong, Liu, Shi, Cui, Li, Zeng, Wang, Xie, Li, Patterson, Tian, Zhang, Zhou, Liu, Zhao, Zhao, Yue, Zhang, Yang, Richardson, and Lan]{clue}
L.~Xu, H.~Hu, X.~Zhang, L.~Li, C.~Cao, Y.~Li, Y.~Xu, K.~Sun, D.~Yu, C.~Yu, Y.~Tian, Q.~Dong, W.~Liu, B.~Shi, Y.~Cui, J.~Li, J.~Zeng, R.~Wang, W.~Xie, Y.~Li, Y.~Patterson, Z.~Tian, Y.~Zhang, H.~Zhou, S.~Liu, Z.~Zhao, Q.~Zhao, C.~Yue, X.~Zhang, Z.~Yang, K.~Richardson, and Z.~Lan.
\newblock {CLUE:} {A} chinese language understanding evaluation benchmark.
\newblock In D.~Scott, N.~Bel, and C.~Zong, editors, \emph{Proceedings of the 28th International Conference on Computational Linguistics, {COLING} 2020, Barcelona, Spain (Online), December 8-13, 2020}, pages 4762--4772. International Committee on Computational Linguistics, 2020.
\newblock \doi{10.18653/V1/2020.COLING-MAIN.419}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.coling-main.419}.

\bibitem[Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang, et~al.]{yi}
A.~Young, B.~Chen, C.~Li, C.~Huang, G.~Zhang, G.~Zhang, H.~Li, J.~Zhu, J.~Chen, J.~Chang, et~al.
\newblock Yi: Open foundation models by 01. ai.
\newblock \emph{arXiv preprint arXiv:2403.04652}, 2024.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock {HellaSwag}: Can a machine really finish your sentence?
\newblock In A.~Korhonen, D.~R. Traum, and L.~M{\`{a}}rquez, editors, \emph{Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}, pages 4791--4800. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/v1/p19-1472}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1472}.

\bibitem[Zhao et~al.(2023)Zhao, Lin, Zhu, Ye, Chen, Zheng, Ceze, Krishnamurthy, Chen, and Kasikci]{atom}
Y.~Zhao, C.~Lin, K.~Zhu, Z.~Ye, L.~Chen, S.~Zheng, L.~Ceze, A.~Krishnamurthy, T.~Chen, and B.~Kasikci.
\newblock Atom: Low-bit quantization for efficient and accurate {LLM} serving.
\newblock \emph{CoRR}, abs/2310.19102, 2023.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.19102}.

\bibitem[Zheng et~al.(2019)Zheng, Huang, and Sun]{chid}
C.~Zheng, M.~Huang, and A.~Sun.
\newblock Chid: {A} large-scale chinese idiom dataset for cloze test.
\newblock In A.~Korhonen, D.~R. Traum, and L.~M{\`{a}}rquez, editors, \emph{Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}, pages 778--787. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/P19-1075}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1075}.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica]{mtbench}
L.~Zheng, W.-L. Chiang, Y.~Sheng, S.~Zhuang, Z.~Wu, Y.~Zhuang, Z.~Lin, Z.~Li, D.~Li, E.~P. Xing, H.~Zhang, J.~E. Gonzalez, and I.~Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan]{agieval}
W.~Zhong, R.~Cui, Y.~Guo, Y.~Liang, S.~Lu, Y.~Wang, A.~Saied, W.~Chen, and N.~Duan.
\newblock {AGIEval}: {A} human-centric benchmark for evaluating foundation models.
\newblock \emph{CoRR}, abs/2304.06364, 2023.
\newblock \doi{10.48550/arXiv.2304.06364}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2304.06364}.

\bibitem[Zhou et~al.(2024)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, et~al.]{lima}
C.~Zhou, P.~Liu, P.~Xu, S.~Iyer, J.~Sun, Y.~Mao, X.~Ma, A.~Efrat, P.~Yu, L.~Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou]{IFeval}
J.~Zhou, T.~Lu, S.~Mishra, S.~Brahma, S.~Basu, Y.~Luan, D.~Zhou, and L.~Hou.
\newblock Instruction-following evaluation for large language models.
\newblock \emph{arXiv preprint arXiv:2311.07911}, 2023.

\end{thebibliography}
