\documentclass[11pt, a4paper, logo, copyright, nonumbering]{deepseek}
\usepackage[authoryear, sort&compress, round]{natbib}
\usepackage{dblfloatfix}
\usepackage{ulem}
\usepackage{caption}
% \usepackage{subcaption}
% \usepackage{listings}
% \lstset{breaklines=true}
\usepackage{dramatist}
\usepackage{xspace}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{xltabular}
\usepackage{longtable}
\usepackage{hyperref}
\interfootnotelinepenalty=10000

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lineno}
\usepackage{multirow}
\usepackage{adjustbox}


\usepackage[bottom]{footmisc}
% \linenumbers

\usepackage{CJKutf8}
\usepackage{subfigure}
\usepackage{setspace}

% damai custom package
\usepackage{dsfont}
\usepackage{array} % 用于tabularx环境
\usepackage{tabularx} % 用于tabularx环境
\usepackage{subfigure} % 用于创建子图
\usepackage{xcolor} % 用于画有颜色的框

% ########
% 附录包
\usepackage{lipsum}  % 用于生成示例文本
\usepackage{multicol} % 引入 multicol 宏包

% \usepackage[finalizecache,cachedir=.]{minted}

\makeatletter
\def\@BTrule[#1]{%
  \ifx\longtable\undefined
    \let\@BTswitch\@BTnormal
  \else\ifx\hline\LT@hline
    \nobreak
    \let\@BTswitch\@BLTrule
  \else
     \let\@BTswitch\@BTnormal
  \fi\fi
  \global\@thisrulewidth=#1\relax
  \ifnum\@thisruleclass=\tw@\vskip\@aboverulesep\else
  \ifnum\@lastruleclass=\z@\vskip\@aboverulesep\else
  \ifnum\@lastruleclass=\@ne\vskip\doublerulesep\fi\fi\fi
  \@BTswitch}
\makeatother

\addto\extrasenglish{
    \def\chapterautorefname{Chapter}%
    \def\sectionautorefname{Section}%
    \def\paragraphautorefname{Section}
    \def\subparagraphautorefname{Section}
    \def\paragraphautorefname{Paragraph}%
    \def\tableautorefname{Table}%
    \def\equationautorefname{Equation}%
}

\newenvironment{indentchunk}[1]%
 {\begin{list}{}%
         {\setlength{\leftmargin}{#1}}%
         \item[]%
 }
 {\end{list}}
 
\bibliographystyle{abbrvnat}
% \bibliography{main}

\reportnumber{001} % Leave blank if n/a

\renewcommand{\today}{}
% DeepSeek 67B
\newcommand{\dsvi}{DeepSeek 67B}
\newcommand{\dsvic}{DeepSeek 67B Chat}
% DeepSeek-V2
\newcommand{\paramsuf}{A21B/236B}
\newcommand{\dsvii}{DeepSeek-V2}
\newcommand{\dsviilite}{DeepSeek-V2-Lite}
\newcommand{\dsviisft}{DeepSeek-V2 Chat (SFT)}
\newcommand{\dsviirl}{DeepSeek-V2 Chat (RL)}
\newcommand{\dsattn}{MLA}
\newcommand{\dsmoe}{DeepSeekMoE}

\title{\centering \dsvii{}: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}

\author[*]{
DeepSeek-AI
\\
\small
\texttt{research@deepseek.com}
}

\input{commands.tex}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\zdaxie}[1]{\textcolor[rgb]{0.545,0,0.071}{#1}}

\begin{abstract}

We present \dsvii{}, a strong Mixture-of-Experts~(MoE) language model characterized by economical training and efficient inference. 
It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. 
\dsvii{} adopts innovative architectures including Multi-head Latent Attention~(\dsattn{}) and \dsmoe{}. 
\dsattn{} guarantees efficient inference through significantly compressing the Key-Value~(KV) cache into a latent vector, while \dsmoe{} enables training strong models at an economical cost through sparse computation. 
Compared with \dsvi{}, \dsvii{} achieves significantly stronger performance, and meanwhile saves 42.5\% of training costs, reduces the KV cache by 93.3\%, and boosts the maximum generation throughput to 5.76 times. 
We pretrain \dsvii{} on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning~(SFT) and Reinforcement Learning~(RL) to fully unlock its potential. 
Evaluation results show that, even with only 21B activated parameters, \dsvii{} and its chat versions still achieve top-tier performance among open-source models.
The model checkpoints are available at \url{https://github.com/deepseek-ai/DeepSeek-V2}. 

\end{abstract}

\begin{document}
\begin{CJK*}{UTF8}{gbsn}

\maketitle

\begin{figure}[ht]
    \centering
    \subfigure[]{
        \includegraphics[width=0.54\textwidth]{figures/mmlu.pdf}
        \label{fig:mmlu}
    }
    \subfigure[]{
        \includegraphics[width=0.42\textwidth]{figures/efficiency.pdf}
        \label{fig:efficiency}
    }
    \label{fig:first_page}
    \caption{
    (a) MMLU accuracy vs. activated parameters, among different open-source models. 
    (b) Training costs and inference efficiency of \dsvi{} (Dense) and \dsvii{}.
    }
\end{figure}


\newpage

\begin{spacing}{0.9}
\tableofcontents
\end{spacing}

\newpage

\section{Introduction}

In the past few years, Large Language Models~(LLMs)~\citep{chatgpt,gpt4,claude,gemini} have undergone rapid development, offering a glimpse into the dawn of Artificial General Intelligence~(AGI). 
In general, the intelligence of an LLM tends to improve as the number of parameters increases, allowing it to exhibit emergent capabilities across various tasks~\citep{wei2022emergent}. 
However, the improvement comes at the cost of larger computing resources for training and a potential decrease in inference throughput. 
These constraints present significant challenges that impede the widespread adoption and utilization of LLMs.
In order to tackle this problem, we introduce \dsvii{}, a strong open-source Mixture-of-Experts~(MoE) language model, characterized by economical training and efficient inference through an innovative Transformer architecture. 
It is equipped with a total of 236B parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. 

We optimize the attention modules and Feed-Forward Networks~(FFNs) within the Transformer framework~\citep{transformer} with our proposed \textbf{Multi-head Latent Attention~(\dsattn{})} and \textbf{\dsmoe{}}.
(1)
In the context of attention mechanisms, the Key-Value (KV) cache of the Multi-Head Attention (MHA)~\citep{transformer} poses a significant obstacle to the inference efficiency of LLMs. 
Various approaches have been explored to address this issue, including Grouped-Query Attention (GQA)~\citep{ainslie2023gqa} and Multi-Query Attention (MQA)~\citep{mqa}. 
However, these methods often compromise performance in their attempt to reduce the KV cache. 
In order to achieve the best of both worlds, we introduce \dsattn{}, an attention mechanism equipped with low-rank key-value joint compression. 
Empirically, \dsattn{} achieves superior performance compared with MHA, and meanwhile significantly reduces the KV cache during inference, thus boosting the inference efficiency.
(2)
For Feed-Forward Networks~(FFNs), we follow the \dsmoe{} architecture~\citep{deepseekmoe}, which adopts fine-grained expert segmentation and shared expert isolation for higher potential in expert specialization. 
The \dsmoe{} architecture demonstrates great advantages compared with conventional MoE architectures like GShard~\citep{gshard}, enabling us to train strong models at an economical cost. 
As we employ expert parallelism during training, we also devise supplementary mechanisms to control communication overheads and ensure load balance. 
By combining these two techniques, \dsvii{} features strong performance (Figure~\ref{fig:mmlu}), economical training costs, and efficient inference throughput (Figure~\ref{fig:efficiency}), simultaneously. 

\begin{figure}[!t]
\centering
\includegraphics[width=0.99\linewidth]{figures/deepseekv2.pdf}
\caption{
Illustration of the architecture of \dsvii{}. 
\dsattn{} ensures efficient inference by significantly reducing the KV cache for generation, and \dsmoe{} enables training strong models at an economical cost through the sparse architecture. 
}
\label{fig:dsvii}
\end{figure}

We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens. 
Compared with the corpus used in \dsvi{}~(our previous release)~\citep{deepseek1}, this corpus features an extended amount of data, especially Chinese data, and higher data quality. 
We first pretrain \dsvii{} on the full pre-training corpus. 
Then, we collect 1.5M conversational sessions, which encompass various domains such as math, code, writing, reasoning, safety, and more, to perform Supervised Fine-Tuning~(SFT) for \dsviisft{}. 
Finally, we follow DeepSeekMath~\citep{deepseekmath} to employ Group Relative Policy Optimization~(GRPO) to further align the model with human preference and produce \dsviirl{}.

We evaluate \dsvii{} on a wide range of benchmarks in English and Chinese, and compare it with representative open-source models.
Evaluation results show that even with only 21B activated parameters, DeepSeek-V2 still achieves top-tier performance among open-source models and becomes the strongest open-source MoE language model. 
Figure~\ref{fig:mmlu} highlights that, on MMLU, \dsvii{} achieves top-ranking performance with only a small number of activated parameters. 
In addition, as shown in Figure~\ref{fig:efficiency}, compared with \dsvi{}, \dsvii{} saves 42.5\% of training costs, reduces the KV cache by 93.3\%, and boosts the maximum generation throughput to 5.76 times. 
We also evaluate \dsviisft{} and \dsviirl{} on open-ended benchmarks. 
Notably, \dsviirl{} achieves 38.9 length-controlled win rate on AlpacaEval 2.0 \citep{alpaca2.0}, 8.97 overall score on MT-Bench \citep{mtbench}, and 7.91 overall score on AlignBench \citep{align_bench}. 
The English open-ended conversation evaluations demonstrate that \dsviirl{} has top-tier performance among open-source chat models. 
In addition, the evaluation on AlignBench indicates that in Chinese, \dsviirl{} outperforms all of open-source models, and even beats most of closed-source models. 

In order to facilitate further research and development on MLA and DeepSeekMoE, we also release DeepSeek-V2-Lite, a smaller model equipped with MLA and DeepSeekMoE, for the open-source community. 
It has a total of 15.7B parameters, where 2.4B are activated for each token. 
Detailed descriptions about DeepSeek-V2-Lite can be found in Appendix~\ref{app:dsviilite}.

In the rest of this paper, we first provide a detailed description of the model architecture of \dsvii{} (Section~\ref{sec:arch}). 
Subsequently, we introduce our pre-training endeavors, including the training data construction, hyper-parameter settings, infrastructures, long context extension, and the evaluation of model performance and efficiency (Section~\ref{sec:pre-training}). 
Following this, we demonstrate our efforts in alignment, encompassing Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), the evaluation results, and other discussion (Section~\ref{sec:alignment}). 
Finally, we summarize the conclusion, deliberate on the current limitations of \dsvii{}, and outline our future work (Section~\ref{sec:conclusion}). 

\section{Architecture}
\label{sec:arch}

By and large, \dsvii{} is still in the Transformer architecture~\citep{transformer}, where each Transformer block consists of an attention module and a Feed-Forward Network~(FFN). 
However, for both the attention module and the FFN, we design and employ innovative architectures. 
For attention, we design \dsattn{}, which utilizes low-rank key-value joint compression to eliminate the bottleneck of inference-time key-value cache, thus supporting efficient inference.
For FFNs, we adopt the \dsmoe{} architecture~\citep{deepseekmoe}, a high-performance MoE architecture that enables training strong models at an economical cost. 
An illustration of the architecture of \dsvii{} is presented in Figure~\ref{fig:dsvii}, and we will introduce the details of \dsattn{} and \dsmoe{} in this section. 
For other tiny details (e.g., layer normalization and the activation function in FFNs), unless specifically stated, \dsvii{} follows the settings of \dsvi{}~\citep{deepseek1}. 

\subsection{Multi-Head Latent Attention: Boosting Inference Efficiency}

Conventional Transformer models usually adopts Multi-Head Attention~(MHA)~\citep{transformer}, but during generation, its heavy Key-Value~(KV) cache will become the bottleneck that limit the inference efficiency. 
In order to reduce the KV cache, Multi-Query Attention~(MQA)~\citep{mqa} and Grouped-Query Attention~(GQA)~\citep{ainslie2023gqa} are proposed. 
They require a smaller magnitude of KV cache, but their performance does not match MHA (we provide the ablation of MHA, GQA and MQA in Appendix~\ref{app:mha_gqa_mqa}).

For \dsvii{}, we design an innovative attention mechanism called Multi-head Latent Attention (\dsattn{}). 
Equipped with low-rank key-value joint compression, \dsattn{} achieves better performance than MHA, but requires a significantly smaller amount of KV cache. 
We introduce its architecture in the following, and also provide a comparison between \dsattn{} and MHA in Appendix~\ref{app:dsattn_mha}. 

\subsubsection{Preliminaries: Standard Multi-Head Attention}

We first introduce the standard MHA mechanism as background. 
Let $d$ be the embedding dimension, $n_h$ be the number of attention heads, $d_h$ be the dimension per head, and $\mathbf{h}_{t} \in \mathbb{R}^{d}$ be the attention input of the $t$-th token at an attention layer. 
Standard MHA first produces $\mathbf{q}_{t}, \mathbf{k}_{t}, \mathbf{v}_{t} \in \mathbb{R}^{d_h n_h}$ through three matrices $W^{Q}, W^{K}, W^{V} \in \mathbb{R}^{d_h n_h \times d}$, respectively: 
\begin{align}
    \mathbf{q}_{t} &= W^{Q} \mathbf{h}_{t}, \\
    \mathbf{k}_{t} &= W^{K} \mathbf{h}_{t}, \\
    \mathbf{v}_{t} &= W^{V} \mathbf{h}_{t},
\end{align}
Then, $\mathbf{q}_{t}, \mathbf{k}_{t}, \mathbf{v}_{t}$ will be sliced into $n_h$ heads for the multi-head attention computation: 
\begin{align}
    [\mathbf{q}_{t, 1};&\mathbf{q}_{t, 2};...;\mathbf{q}_{t, n_{h}}] = \mathbf{q}_{t}, \\
    [\mathbf{k}_{t, 1};&\mathbf{k}_{t, 2};...;\mathbf{k}_{t, n_{h}}] = \mathbf{k}_{t}, \\
    [\mathbf{v}_{t, 1};&\mathbf{v}_{t, 2};...;\mathbf{v}_{t, n_{h}}] = \mathbf{v}_{t}, \\
    \mathbf{o}_{t, i} &= \sum_{j=1}^{t} \operatorname{Softmax}_j(\frac{\mathbf{q}_{t, i}^T \mathbf{k}_{j, i}}{\sqrt{d_{h}}}) \mathbf{v}_{j, i}, \\ 
    \mathbf{u}_{t} &= W^{O} [\mathbf{o}_{t, 1};\mathbf{o}_{t, 2};...;\mathbf{o}_{t, n_{h}}],
\end{align}
where $\mathbf{q}_{t, i}, \mathbf{k}_{t, i}, \mathbf{v}_{t, i} \in \mathbb{R}^{d_h}$ denote the query, key, and value of the $i$-th attention head, respectively; 
$W^{O} \in \mathbb{R}^{d \times d_h n_h}$ denotes the output projection matrix. 
During inference, all keys and values need to be cached to accelerate inference, so MHA needs to cache $2 n_{h} d_{h} l$ elements for each token. 
In model deployment, this heavy KV cache is a large bottleneck that limits the maximum batch size and sequence length. 

\begin{figure}[!t]
\centering
\includegraphics[width=0.99\linewidth]{figures/dsattn.pdf}
\caption{
Simplified illustration of Multi-Head Attention~(MHA), Grouped-Query Attention~(GQA), Multi-Query Attention~(MQA), and Multi-head Latent Attention~(\dsattn{}).  
Through jointly compressing the keys and values into a latent vector, \dsattn{} significantly reduces the KV cache during inference. 
}
\label{fig:dsattn}
\end{figure}

\subsubsection{Low-Rank Key-Value Joint Compression}

The core of \dsattn{} is the low-rank joint compression for keys and values to reduce KV cache:
\begin{align}
    \mathbf{c}_{t}^{KV} &= W^{DKV} \mathbf{h}_{t}, \\
    \label{eq:c_to_k}
    \mathbf{k}_{t}^{C} &= W^{UK} \mathbf{c}_{t}^{KV}, \\
    \mathbf{v}_{t}^{C} &= W^{UV} \mathbf{c}_{t}^{KV},
\end{align}
where $\mathbf{c}_{t}^{KV} \in \mathbb{R}^{d_c}$ is the compressed latent vector for keys and values; 
$d_c (\ll d_h n_h)$ denotes the KV compression dimension;
$W^{DKV} \in \mathbb{R}^{d_c \times d}$ is the down-projection matrix;
and $W^{UK},W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ are the up-projection matrices for keys and values, respectively. 
During inference, \dsattn{} only needs to cache $\mathbf{c}_{t}^{KV}$, so its KV cache has only $d_{c}l$ elements, where $l$ denotes the number of layers. 
In addition, during inference, since $W^{UK}$ can be absorbed into $W^{Q}$, and $W^{UV}$ can be absorbed into $W^{O}$, we even do not need to compute keys and values out for attention. 
Figure~\ref{fig:dsattn} intuitively illustrates how the KV joint compression in \dsattn{} reduces the KV cache. 

Moreover, in order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache:
\begin{align}
    \mathbf{c}_{t}^{Q} &= W^{DQ} \mathbf{h}_{t}, \\
    \mathbf{q}_{t}^{C} &= W^{UQ} \mathbf{c}_{t}^{Q},
\end{align}
where $\mathbf{c}_{t}^{Q} \in \mathbb{R}^{d_c^{\prime}}$ is the compressed latent vector for queries; 
$d_c^{\prime} (\ll d_h n_h)$ denotes the query compression dimension; 
and $W^{DQ} \in \mathbb{R}^{d_c^{\prime} \times d}, W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c^{\prime}}$ are the down-projection and up-projection matrices for queries, respectively. 

\subsubsection{Decoupled Rotary Position Embedding}

Following \dsvi{}~\citep{deepseek1}, we intend to use the Rotary Position Embedding~(RoPE)~\citep{su2024roformer} for \dsvii{}. 
However, RoPE is incompatible with low-rank KV compression. 
To be specific, RoPE is position-sensitive for both keys and queries. 
If we apply RoPE for the keys $\mathbf{k}_{t}^{C}$, $W^{UK}$ in Equation~\ref{eq:c_to_k} will be coupled with a position-sensitive RoPE matrix. 
In this way, $W^{UK}$ cannot be absorbed into $W^{Q}$ any more during inference, since a RoPE matrix related to the currently generating token will lie between $W^{Q}$ and $W^{UK}$ and matrix multiplication does not obey a commutative law. 
As a result, we must recompute the keys for all the prefix tokens during inference, which will significantly hinder the inference efficiency. 

As a solution, we propose the decoupled RoPE strategy that uses additional multi-head queries $\mathbf{q}_{t, i}^{R} \in \mathbb{R}^{d_h^R}$ and a shared key $\mathbf{k}_{t}^{R} \in \mathbb{R}^{d_h^R}$ to carry RoPE, where $d_h^R$ denotes the per-head dimension of the decoupled queries and key. 
Equipped with the decoupled RoPE strategy, \dsattn{} performs the following computation:
\begin{align}
    [\mathbf{q}_{t, 1}^{R};\mathbf{q}_{t, 2}^{R};...;\mathbf{q}_{t, n_{h}}^{R}] = \mathbf{q}_{t}^{R} &= \operatorname{RoPE}({W^{QR}} \mathbf{c}_{t}^{Q}), \\
    \mathbf{k}_{t}^{R} &= \operatorname{RoPE}({W^{KR}} \mathbf{h}_{t}), \\
    \mathbf{q}_{t, i} &= [\mathbf{q}_{t, i}^{C}; \mathbf{q}_{t, i}^{R}], \\
    \mathbf{k}_{t, i} &= [\mathbf{k}_{t, i}^{C}; \mathbf{k}_{t}^{R}], \\
    \mathbf{o}_{t, i} &= \sum_{j=1}^{t} \operatorname{Softmax}_j(\frac{\mathbf{q}_{t, i}^T \mathbf{k}_{j, i}}{\sqrt{d_{h} + d_{h}^{R}}}) \mathbf{v}_{j, i}^{C}, \\ 
    \mathbf{u}_{t} &= W^{O} [\mathbf{o}_{t, 1};\mathbf{o}_{t, 2};...;\mathbf{o}_{t, n_{h}}],
\end{align}
where $W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c^{\prime}}$ and $W^{KR} \in \mathbb{R}^{d_h^R \times d}$ are matrices to produce the decouples queries and key, respectively; 
$\operatorname{RoPE}(\cdot)$ denotes the operation that applies RoPE matrices; 
and $[\cdot;\cdot]$ denotes the concatenation operation. 
During inference, the decoupled key should also be cached. 
Therefore, \dsvii{} requires a total KV cache containing $(d_{c} + d_h^R)l$ elements. 

In order to demonstrate the complete computation process of \dsattn{}, we also organize and provide its full formulas in Appendix~\ref{app:full_formulas}. 

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{@{}l c c@{}}
\toprule
\textbf{Attention Mechanism} & \textbf{KV Cache per Token (\# Element)} & \textbf{Capability} \\
\midrule
Multi-Head Attention (MHA) & $2 n_{h} d_{h} l$ & Strong \\
Grouped-Query Attention (GQA) & $2 n_{g} d_{h} l$ & Moderate \\
Multi-Query Attention (MQA) & ~~~~$2 d_{h} l$ & Weak \\
\midrule
\dsattn{} (Ours) & $~~~~~~~~(d_{c} + d_h^R)l \approx \frac{9}{2} d_{h} l$~~~~~~~~~~~~~~~~~~~~~~~ & Stronger \\
\bottomrule
\end{tabular}
\caption{
Comparison of the KV cache per token among different attention mechanisms. 
$n_{h}$ denotes the number of attention heads, 
$d_{h}$ denotes the dimension per attention head, 
$l$ denotes the number of layers, 
$n_{g}$ denotes the number of groups in GQA, 
and $d_{c}$ and $d_h^R$ denote the KV compression dimension and the per-head dimension of the decoupled queries and key in \dsattn{}, respectively. 
The amount of KV cache is measured by the number of elements, regardless of the storage precision.
For \dsvii{}, $d_{c}$ is set to $4d_{h}$ and $d_h^R$ is set to $\frac{d_{h}}{2}$. 
So, its KV cache is equal to GQA with only 2.25 groups, but its performance is stronger than MHA. 
}
\label{tab:kv_cache_comp}
\end{table}

\subsubsection{Comparison of Key-Value Cache}

We demonstrate a comparison of the KV cache per token among different attention mechanisms in Table~\ref{tab:kv_cache_comp}. 
\dsattn{} requires only a small amount of KV cache, equal to GQA with only 2.25 groups, but can achieve stronger performance than MHA. 

\subsection{\dsmoe{}: Training Strong Models at Economical Costs}

\subsubsection{Basic Architecture}

For FFNs, we employ the \dsmoe{} architecture~\citep{deepseekmoe}. 
\dsmoe{} has two key ideas: segmenting experts into finer granularity for higher expert specialization and more accurate knowledge acquisition, and isolating some shared experts for mitigating knowledge redundancy among routed experts. 
With the same number of activated and total expert parameters, \dsmoe{} can outperform conventional MoE architectures like GShard~\citep{gshard} by a large margin. 

Let $\mathbf{u}_{t}$ be the FFN input of the $t$-th token, we compute the FFN output $\mathbf{h}_{t}^{\prime}$ as follows:
\begin{align}
    \mathbf{h}_{t}^{\prime} & = \mathbf{u}_{t} + \sum_{i=1}^{N_{s}} {\operatorname{FFN}^{(s)}_{i}\left( \mathbf{u}_{t} \right)} + \sum_{i=1}^{N_r} {g_{i,t} \operatorname{FFN}^{(r)}_{i}\left( \mathbf{u}_{t} \right)}, \\
    g_{i,t} & = \begin{cases} 
    s_{i,t}, & s_{i,t} \in \operatorname{Topk} (\{ s_{j, t} | 1 \leq j \leq N_r \}, K_{r}), \\
    0, & \text{otherwise}, 
    \end{cases} \\
    s_{i,t} & = \operatorname{Softmax}_i \left( {\mathbf{u}_{t}}^{T} \mathbf{e}_{i} \right),
\end{align}
where $N_{s}$ and $N_r$ denote the numbers of shared experts and routed experts, respectively; 
$\operatorname{FFN}^{(s)}_{i}(\cdot)$ and $\operatorname{FFN}^{(r)}_{i}(\cdot)$ denote the $i$-th shared expert and the $i$-th routed expert, respectively; 
$K_{r}$ denotes the number of activated routed experts; 
$g_{i,t}$ is the gate value for the $i$-th expert; 
$s_{i,t}$ is the token-to-expert affinity; 
$\mathbf{e}_{i}$ is the centroid of the $i$-th routed expert in this layer; 
and $\operatorname{Topk}(\cdot, K)$ denotes the set comprising $K$ highest scores among the affinity scores calculated for the $t$-th token and all routed experts.

\subsubsection{Device-Limited Routing}

We design a device-limited routing mechanism to bound MoE-related communication costs. 
When expert parallelism is employed, the routed experts will be distributed across multiple devices. 
For each token, its MoE-related communication frequency is proportional to the number of devices covered by its target experts.
Due to the fine-grained expert segmentation in \dsmoe{}, the number of activated experts can be large, so the MoE-related communication will be more costly if we apply expert parallelism. 

For \dsvii{}, beyond the naive top-K selection of routed experts, we additionally ensure that the target experts of each token will be distributed on at most $M$ devices. 
To be specific, for each token, we first select $M$ devices that have experts with the highest affinity scores in them. 
Then, we perform top-K selection among experts on these $M$ devices. 
In practice, we find that when $M \geq 3$, the device-limited routing can achieve a good performance roughly aligned with the unrestricted top-K routing. 

\subsubsection{Auxiliary Loss for Load Balance}

We take the load balance into consideration for automatically learned routing strategies. 
Firstly, unbalanced load will raise the risk of routing collapse~\citep{moe}, preventing some experts being fully trained and utilized. 
Secondly, when expert parallelism is employed, unbalanced load will diminish computation efficiency. 
During the training of \dsvii{}, we design three kinds of auxiliary losses, for controlling expert-level load balance ($\mathcal{L}_{\mathrm{ExpBal}}$), device-level load balance ($\mathcal{L}_{\mathrm{DevBal}}$), and communication balance ($\mathcal{L}_{\mathrm{CommBal}}$), respectively. 

\paragraph{Expert-Level Balance Loss.}
We use an expert-level balance loss~\citep{switch,gshard} to mitigate the risk of routing collapse:
\begin{align}
    \mathcal{L}_{\mathrm{ExpBal}} & = \alpha_1 \sum_{i=1}^{N_r}{f_i P_i}, \\
    f_i & = \frac{N_r}{K_r T} \sum_{t=1}^{T}{ \mathds{1}( \text{Token $t$ selects Expert $i$} )}, \\
    P_i & = \frac{1}{T} \sum_{t=1}^{T}{s_{i,t}},
\end{align}
where $\alpha_1$ is a hyper-parameter called expert-level balance factor; 
$\mathds{1}(\cdot)$ denotes the indicator function; 
and $T$ denotes the number of tokens in a sequence. 

\paragraph{Device-Level Balance Loss.}
In addition to the expert-level balance loss, we additionally design a device-level balance loss to ensure balanced computation across different devices.
In the training process of \dsvii{}, we partition all routed experts into $D$ groups $\{\mathcal{E}_1, \mathcal{E}_2, ..., \mathcal{E}_D \}$, and deploy each group on a single device. 
The device-level balance loss is computed as follows:
\begin{align}
    \mathcal{L}_{\mathrm{DevBal}} & = \alpha_{2} \sum_{i=1}^{D}{f_i^{\prime} P_i^{\prime}}, \\
    f_i^{\prime} & = \frac{1}{|\mathcal{E}_i|} \sum_{j \in \mathcal{E}_i}{ f_j }, \\
    P_i^{\prime} & = \sum_{j \in \mathcal{E}_i}{ P_j },
\end{align}
where $\alpha_{2}$ is a hyper-parameter called device-level balance factor. 

\paragraph{Communication Balance Loss.}
Finally, we introduce a communication balance loss to ensure that the communication of each device is balanced. 
Although the device-limited routing mechanism guarantees that the sending communication of each device is bounded, if a certain device receives more tokens than other devices, the practical communication efficiency will also be affected. 
In order to mitigate this issue, we design a communication balance loss as follows: 
\begin{align}
    \mathcal{L}_{\mathrm{CommBal}} & = \alpha_{3} \sum_{i=1}^{D}{f_i^{\prime\prime} P_i^{\prime\prime}}, \\
    f_i^{\prime\prime} & = \frac{D}{M T} \sum_{t=1}^{T}{ \mathds{1}( \text{Token $t$ is sent to Device $i$} )}, \\
    P_i^{\prime\prime} & = \sum_{j \in \mathcal{E}_i}{ P_j },
\end{align}
where $\alpha_{3}$ is a hyper-parameter called communication balance factor. 
The device-limited routing mechanism operates on the principle of ensuring that each device transmits at most $MT$ hidden states to other devices. 
Simultaneously, the communication balance loss is employed to encourage each device to receive around $MT$ hidden states from other devices. 
The communication balance loss guarantees a balanced exchange of information among devices, promoting efficient communications. 

\subsubsection{Token-Dropping Strategy}

While balance losses aim to encourage a balanced load, it is important to acknowledge that they cannot guarantee a strict load balance. 
In order to further mitigate the computation wastage caused by unbalanced load, we introduce a device-level token-dropping strategy during training. 
This approach first computes the average computational budget for each device, which means that the capacity factor for each device is equivalent to 1.0. 
Then, inspired by \citet{bpr}, we drop tokens with the lowest affinity scores on each device until reaching the computational budget. 
In addition, we ensure that the tokens belonging to approximately 10\% of the training sequences will never be dropped. 
In this way, we can flexibly decide whether to drop tokens during inference according to the efficiency requirements, and always ensure consistency between training and inference. 

\section{Pre-Training}
\label{sec:pre-training}

\subsection{Experimental Setups}

\subsubsection{Data Construction}

While maintaining the same data processing stages as for \dsvi{}~\citep{deepseek1}, we extend the amount of data and elevate the data quality. 
In order to enlarge our pre-training corpus, we explore the potential of the internet data and optimize our cleaning processes, thus recovering a large amount of mistakenly deleted data. 
Moreover, we incorporate more Chinese data, aiming to better leverage the corpus available on the Chinese internet. 
In addition to the amount of data, we also focus on the data quality. 
We enrich our pre-training corpus with high-quality data from various sources, and meanwhile improve the quality-based filtering algorithm. 
The improved algorithm ensures that a large amount of non-beneficial data will be removed, while the valuable data will be mostly retained. 
In addition, we filter out the contentious content from our pre-training corpus to mitigate the data bias introduced from specific regional cultures. 
A detailed discussion about the influence of this filtering strategy is presented in Appendix~\ref{app:filtering_controversial_content}. 

We adopt the same tokenizer as used in \dsvi{}, which is built based on the Byte-level Byte-Pair Encoding~(BBPE) algorithm and has a vocabulary size of 100K. 
Our tokenized pre-training corpus contains 8.1T tokens, where Chinese tokens are approximately 12\% more than English ones. 

\subsubsection{Hyper-Parameters}

\paragraph{Model Hyper-Parameters.}
We set the number of Transformer layers to 60 and the hidden dimension to 5120. 
All learnable parameters are randomly initialized with a standard deviation of 0.006.
In \dsattn{}, we set the number of attention heads $n_h$ to 128 and the per-head dimension $d_h$ to 128. 
The KV compression dimension $d_c$ is set to 512, and the query compression dimension $d_c^{\prime}$ is set to 1536. 
For the decoupled queries and key, we set the per-head dimension $d_h^R$ to 64. 
Following \citet{deepseekmoe}, we substitute all FFNs except for the first layer with MoE layers. 
Each MoE layer consists of 2 shared experts and 160 routed experts, where the intermediate hidden dimension of each expert is 1536. 
Among the routed experts, 6 experts will be activated for each token. 
In addition, the low-rank compression and fine-grained expert segmentation will impact the output scale of a layer. 
Therefore, in practice, we employ additional RMS Norm layers after the compressed latent vectors, and multiply additional scaling factors at the width bottlenecks (i.e., the compressed latent vectors and the intermediate hidden states of routed experts) to ensure stable training. 
Under this configuration, \dsvii{} comprises 236B total parameters, of which 21B are activated for each token. 

\paragraph{Training Hyper-Parameters.}
We employ the AdamW optimizer~\citep{adamw} with hyper-parameters set to $\beta_1=0.9$, $\beta_2=0.95$, and $\mathrm{weight\_decay}=0.1$. 
The learning rate is scheduled using a warmup-and-step-decay strategy~\citep{deepseek1}. 
Initially, the learning rate linearly increases from 0 to the maximum value during the first 2K steps. 
Subsequently, the learning rate is multiplied by 0.316 after 
training about 60\% of tokens, and again by 0.316 after training about 90\% of tokens. 
The maximum learning rate is set to $2.4 \times 10^{-4}$, and the gradient clipping norm is set to 1.0.
We also use a batch size scheduling strategy, where the batch size is gradually increased from 2304 to 9216 in the training of the first 225B tokens, and then keeps 9216 in the remaining training. 
We set the maximum sequence length to 4K, and train \dsvii{} on 8.1T tokens. 
We leverage pipeline parallelism to deploy different layers of a model on different devices, and for each layer, the routed experts will be uniformly deployed on 8 devices ($D=8$). 
As for the device-limited routing, each token will be sent to at most 3 devices ($M=3$). 
As for balance losses, we set $\alpha_{1}$ to 0.003, $\alpha_{2}$ to 0.05, and $\alpha_{3}$ to 0.02. 
We employ the token-dropping strategy during training for acceleration, but do not drop any tokens for evaluation. 

\subsubsection{Infrastructures}

\dsvii{} is trained based on the HAI-LLM framework~\citep{haillm}, an efficient and light-weight training framework developed internally by our engineers. 
It employs a 16-way zero-bubble pipeline parallelism~\citep{qi2023zero}, an 8-way expert parallelism~\citep{gshard}, and ZeRO-1 data parallelism~\citep{zero}. 
Given that \dsvii{} has relatively few activated parameters, and a portion of the operators are recomputed to save activation memory, it can be trained without the necessity of tensor parallelism, thereby decreasing the communication overhead. 
Moreover, in order to further improve the training efficiency, we overlap the computation of shared experts with the expert parallel all-to-all communication. 
We also customize faster CUDA kernels for communications, routing algorithms, and fused linear computations across different experts.
In addition, \dsattn{} is also optimized based on an improved version of FlashAttention-2~\citep{dao2023flashattention2}.

We conduct all experiments on a cluster equipped with NVIDIA H800 GPUs. 
Each node in the H800 cluster contains 8 GPUs connected using NVLink and NVSwitch within nodes. 
Across nodes, InfiniBand interconnects are utilized to facilitate communications. 

\begin{figure}[!t]
\centering
\includegraphics[width=0.98\linewidth]{figures/needle_in_a_haystack.pdf}
\caption{
Evaluation results on the ``Needle In A Haystack'' (NIAH) tests. 
\dsvii{} performs well across all context window lengths up to 128K. 
}
\label{fig:long_context}
\end{figure}

\subsubsection{Long Context Extension}

After the initial pre-training of \dsvii{}, we employ YaRN~\citep{peng2023yarn} to extend the default context window length from 4K to 128K. 
YaRN was specifically applied to the decoupled shared key $\mathbf{k}^R_t$ as it is responsible for carrying RoPE~\citep{su2024roformer}. 
For YaRN, we set the scale $s$ to 40, $\alpha$ to 1, $\beta$ to 32, and the target maximum context length to 160K. 
Under these settings, we can expect the model to respond well for a context length of 128K. 
Slightly diverging from original YaRN, due to our distinct attention mechanism, we adjust the length scaling factor to modulate the attention entropy. 
The factor $\sqrt{t}$ is computed as $\sqrt{t} = 0.0707 \ln{s} + 1$, aiming at minimizing the perplexity.

We additionally train the model for 1000 steps, with a sequence length of 32K and a batch size of 576 sequences. 
Although the training is conducted solely at the sequence length of 32K, the model still demonstrates robust performance when being evaluated at a context length of 128K. 
As shown in Figure~\ref{fig:long_context}, the results on the ``Needle In A Haystack'' (NIAH) tests indicate that \dsvii{} performs well across all context window lengths up to 128K.

\subsection{Evaluations}

\subsubsection{Evaluation Benchmarks}

\dsvii{} is pretrained on a bilingual corpus, so we evaluate it on a series of benchmarks in English and Chinese.
Our evaluation is based on our internal evaluation framework integrated in our HAI-LLM framework. 
Included benchmarks are categorized and listed as follows, where \underline{underlined} benchmarks are in Chinese:

\textbf{Multi-subject multiple-choice} datasets include MMLU \citep{mmlu}, \underline{C-Eval} \citep{ceval}, and \underline{CMMLU} \citep{cmmlu}.

\textbf{Language understanding and reasoning} datasets include HellaSwag \citep{hellaswag}, PIQA \citep{piqa}, ARC \citep{arc}, and BigBench Hard (BBH) \citep{bbh}.

\textbf{Closed-book question answering} datasets include TriviaQA \citep{joshi-etal-2017-triviaqa} and NaturalQuestions \citep{naturalquestions}.

\textbf{Reading comprehension} datasets include RACE \cite{race}, DROP \citep{drop}, \underline{C3} \citep{sun2019investigating}, and \underline{CMRC} \citep{cui-etal-2019-span}.

\textbf{Reference disambiguation} datasets include WinoGrande \cite{sakaguchi2019winogrande} and \underline{CLUEWSC} \citep{clue}.

\textbf{Language modeling} datasets include Pile \citep{pile}.

\textbf{Chinese understanding and culture} datasets include \underline{CHID} \citep{chid} and \underline{CCPM} \citep{li2021ccpm}.

\textbf{Math} datasets include GSM8K~\citep{gsm8k}, MATH~\citep{hendrycks2021measuring}, and \underline{CMath} \citep{wei2023cmath}.

\textbf{Code} datasets include HumanEval~\citep{codex}, MBPP~\citep{mbpp}, and CRUXEval~\citep{gu2024cruxeval}.

\textbf{Standardized exams} include \underline{AGIEval} \citep{agieval}. 
Note that AGIEval includes both English and Chinese subsets.

Following our previous work~\citep{deepseek1}, we adopt perplexity-based evaluation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, ARC-Easy, ARC-Challenge, CHID, C-Eval, CMMLU, C3, and CCPM, and adopt generation-based evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, HumanEval, MBPP, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, and CMath. 
In addition, we perform language-modeling-based evaluation for Pile-test and use Bits-Per-Byte~(BPB) as the metric to guarantee fair comparison among models with different tokenizers. 

For an intuitive overview of these benchmarks, we additionally provide our evaluation formats for each benchmark in Appendix~\ref{app:evaluation_form}. 

\subsubsection{Evaluation Results}

\begin{table}[!t]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{@{}c l c | c | c c c | c@{}}
    \toprule
    & \multirow{2}{*}{\centering \textbf{Benchmark (Metric)}} & \multirow{2}{*}{\textbf{\# Shots}} & \textbf{DeepSeek} & \textbf{Qwen1.5} & \textbf{Mixtral} & \textbf{LLaMA 3} & \multirow{2}{*}{\textbf{\dsvii{}}} \\
    & & & \textbf{67B} & \textbf{72B} & \textbf{8x22B} & \textbf{70B} & \\
    \midrule
    & Architecture & - & Dense & Dense & MoE & Dense & MoE \\
    & \# Activated Params & - & 67B & 72B & 39B & 70B & 21B \\
    & \# Total Params & - & 67B & 72B & 141B & 70B & 236B \\
    \midrule
    \multirow{16}{*}{English} & Pile-test (BPB) & - & 0.642 & 0.637 & 0.623 & \textbf{0.602} & \underline{0.606} \\
    & BBH (EM) & 3-shot & 68.7 & 59.9 & \underline{78.9} & \textbf{81.0} & \underline{78.9} \\
    & MMLU (Acc.) & 5-shot & 71.3 & 77.2 & 77.6 & \textbf{78.9} & \underline{78.5} \\
    & DROP (F1) & 3-shot & 69.7 & 71.5 & \underline{80.4} & \textbf{82.5} & \underline{80.1} \\
    & ARC-Easy (Acc.) & 25-shot & 95.3 & \underline{97.1} & \underline{97.3} & \textbf{97.9} & \textbf{97.6} \\
    & ARC-Challenge (Acc.) & 25-shot & 86.4 & \underline{92.8} & 91.2 & \textbf{93.3} & 92.4 \\
    & HellaSwag (Acc.) & 10-shot & \underline{86.3} & 85.8 & \underline{86.6} & \textbf{87.9} & 84.2 \\
    & PIQA (Acc.) & 0-shot & \underline{83.6} & 83.3 & \underline{83.6} & \textbf{85.0} & \underline{83.7} \\
    & WinoGrande (Acc.) & 5-shot & \underline{84.9} & 82.4 & 83.7 & \textbf{85.7} & \underline{84.9} \\
    & RACE-Middle (Acc.) & 5-shot & \underline{69.9} & 63.4 & \textbf{73.3} & \textbf{73.3} & \textbf{73.1} \\
    & RACE-High (Acc.) & 5-shot & 50.7 & 47.0 & \underline{56.7} & \textbf{57.9} & 52.7 \\
    & TriviaQA (EM) & 5-shot & 78.9 & 73.1 & \textbf{82.1} & \underline{81.6} & 79.9 \\
    & NaturalQuestions (EM) & 5-shot & 36.6 & 35.6 & \underline{39.6} & \textbf{40.2} & 38.7 \\
    & AGIEval (Acc.) & 0-shot & 41.3 & \textbf{64.4} & 43.4 & 49.8 & \underline{51.2} \\
    \midrule
    \multirow{4}{*}{Code} & HumanEval (Pass@1) & 0-shot & 45.1 & 43.9 & \textbf{53.1} & 48.2 & \underline{48.8} \\
    & MBPP (Pass@1) & 3-shot & 57.4 & 53.6 & 64.2 & \textbf{68.6} & \underline{66.6} \\
    & CRUXEval-I (Acc.) & 2-shot & 42.5 & 44.3 & \underline{52.4} & 49.4 & \textbf{52.8} \\
    & CRUXEval-O (Acc.) & 2-shot & 41.0 & 42.3 & \underline{52.8} & \textbf{54.3} & 49.8 \\
    \midrule
    \multirow{3}{*}{Math} & GSM8K (EM) & 8-shot & 63.4 & 77.9 & \underline{80.3} & \textbf{83.0} & 79.2 \\
    & MATH (EM) & 4-shot & 18.7 & 41.4 & \underline{42.5} & \underline{42.2} & \textbf{43.6} \\
    & CMath (EM) & 3-shot & 63.0 & \underline{77.8} & 72.3 & 73.9 & \textbf{78.7} \\
    \midrule
    \multirow{7}{*}{Chinese} & CLUEWSC (EM) & 5-shot & \underline{81.0} & 80.5 & 77.5 & 78.3 & \textbf{82.2} \\
    & C-Eval (Acc.) & 5-shot & 66.1 & \textbf{83.7} & 59.6 & 67.5 & \underline{81.7} \\
    & CMMLU (Acc.) & 5-shot & \underline{70.8} & \textbf{84.3} & 60.0 & 69.3 & \textbf{84.0} \\
    & CMRC (EM) & 1-shot & \underline{73.4} & 66.6 & \underline{73.1} & \underline{73.3} & \textbf{77.5} \\
    & C3 (Acc.) & 0-shot & 75.3 & \textbf{78.2} & 71.4 & 74.0 & \underline{77.4} \\
    & CHID (Acc.)  & 0-shot & \underline{92.1} & - & 57.0 & 83.2 & \textbf{92.7} \\
    & CCPM (Acc.) & 0-shot & \underline{88.5} & 88.1 & 61.0 & 68.1 & \textbf{93.1} \\
    \bottomrule
    \end{tabular}
    \caption{
    Comparison among \dsvii{} and other representative open-source models.
    All models are evaluated in our internal framework and share the same evaluation setting.
    \textbf{Bold} denotes the best and \underline{underline} denotes the second-best. 
    Scores with a gap smaller than 0.3 are regarded as at the same level. 
    With only 21B activated parameters, \dsvii{} achieves top-tier performance among open-source models. 
    }
    \label{tab:main}
\end{table}

In Table~\ref{tab:main}, we compare \dsvii{} with several representative open-source models, including DeepSeek 67B~\citep{deepseek1} (our previous release), Qwen1.5 72B~\citep{qwen}, LLaMA3 70B~\citep{llama3}, and Mixtral 8x22B~\citep{mixtral8x22b}. 
We evaluate all these models with our internal evaluation framework, and ensure that they share the same evaluation setting. 
Overall, with only 21B activated parameters, \dsvii{} significantly outperforms \dsvi{} on almost all benchmarks, and achieves top-tier performance among open-source models. 

Further, we elaborately compare \dsvii{} with its open-source counterparts one by one. 
(1)
Compared with Qwen1.5 72B, another model that supports both Chinese and English, \dsvii{} demonstrates overwhelming advantages on the majority of English, code, and math benchmarks. 
As for Chinese benchmarks, Qwen1.5 72B shows better performance on multi-subject multiple-choice tasks while \dsvii{} is comparable or better on others. 
Note that for the CHID benchmark, the tokenizer of Qwen1.5 72B will encounter errors in our evaluation framework, so we leave the CHID score blank for Qwen1.5 72B. 
(2)
Compared with Mixtral 8x22B, \dsvii{} achieves comparable or better English performance, except for TriviaQA, NaturalQuestions, and HellaSwag, which are closely related to English commonsense knowledge. 
Notably, \dsvii{} outperforms Mixtral 8x22B on MMLU. 
On code and math benchmarks, \dsvii{} demonstrates comparable performance with Mixtral 8x22B. 
Since Mixtral 8x22B is not specifically trained on Chinese data, its Chinese capability lags far behind \dsvii{}. 
(3)
Compared with LLaMA3 70B, \dsvii{} is trained on fewer than a quarter of English tokens. 
Therefore, we acknowledge that \dsvii{} still has a slight gap in basic English capabilities with LLaMA3 70B. 
However, even with much fewer training tokens and activated parameters, \dsvii{} still demonstrates comparable code and math capability with LLaMA3 70B. 
Also, as a bilingual language model, \dsvii{} outperforms LLaMA3 70B overwhelmingly on Chinese benchmarks. 

Finally, it is worth mentioning that certain prior studies~\citep{minicpm} incorporate SFT data during the pre-training stage, whereas \dsvii{} has never been exposed to SFT data during pre-training. 

\subsubsection{Training and Inference Efficiency}

\paragraph{Training Costs.}
Since \dsvii{} activates fewer parameters for each token and requires fewer FLOPs than \dsvi{}, training \dsvii{} will be more economical than training \dsvi{} theoretically. 
Although training an MoE model will introduce additional communication overheads, through our operator and communication optimizations, the training for \dsvii{} can attain a relatively high Model FLOPs Utilization~(MFU). 
During our practical training on the H800 cluster, for training on each trillion tokens, \dsvi{} requires 300.6K GPU hours, while \dsvii{} needs only 172.8K GPU hours, i.e., sparse \dsvii{} can save 42.5\% training costs compared with dense \dsvi{}. 

\paragraph{Inference Efficiency.}
In order to efficiently deploy \dsvii{} for service, we first convert its parameters into the precision of FP8. 
In addition, we also perform KV cache quantization~\citep{kv_quant,atom} for \dsvii{} to further compress each element in its KV cache into 6 bits on average. 
Benefiting from \dsattn{} and these optimizations, actually deployed \dsvii{} requires significantly less KV cache than \dsvi{}, and thus can serve a much larger batch size. 
We evaluate the generation throughput of \dsvii{} based on the prompt and generation length distribution from the actually deployed \dsvi{} service. 
On a single node with 8 H800 GPUs, \dsvii{} achieves a generation throughput exceeding 50K tokens per second, which is 5.76 times the maximum generation throughput of \dsvi{}. 
In addition, the prompt input throughput of \dsvii{} exceeds 100K tokens per second. 

\section{Alignment}
\label{sec:alignment}

\subsection{Supervised Fine-Tuning}

Building upon our prior research~\citep{deepseek1}, we curate our instruction tuning datasets to include 1.5M instances, comprising 1.2M instances for helpfulness and 0.3M instances for safety. 
In comparison to the initial version, we improve the data quality to mitigate hallucinatory responses and enhance writing proficiency. 
We fine-tune \dsvii{} with 2 epochs, and the learning rate is set to $5 \times 10^{-6}$. 
For the evaluation of \dsviisft{}, we mainly include generation-based benchmarks, except for several representative multiple-choice tasks (MMLU and ARC). 
We also conduct an instruction-following evaluation (IFEval)~\citep{IFeval} for \dsviisft{}, using prompt-level loose accuracy as the metric. 
Moreover, we employ LiveCodeBench~\citep{jain2024livecodebench} questions from September 1st, 2023 to April 1st, 2024 to evaluate chat models. 
In addition to the standard benchmarks, we further evaluate our model on open-ended conversation benchmarks including MT-Bench~\citep{mtbench}, AlpacaEval 2.0~\citep{alpaca2.0}, and AlignBench~\citep{align_bench}. 
For comparison, we also evaluate Qwen1.5 72B Chat, LLaMA-3-70B Instruct, and Mistral-8x22B Instruct in our evaluation framework and settings. 
As for \dsvic{}, we directly refer to the evaluation results reported in our previous release. 

\subsection{Reinforcement Learning}

In order to further unlock the potential of \dsvii{} and align it with human preference, we conduct Reinforcement Learning~(RL) to adjust its preference. 

\paragraph{Reinforcement Learning Algorithm.}
In order to save the training costs of RL, we adopt Group Relative Policy Optimization~(GRPO) \citep{deepseekmath}, which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. 
Specifically, for each question $q$, GRPO samples a group of outputs $\{o_1, o_2, \cdots, o_G\}$ from the old policy $\pi_{\theta_{old}}$ and then optimizes the policy model $\pi_{\theta}$ by maximizing the following objective:
\begin{equation}
\begin{split}
    \mathcal{J}_{GRPO}(\theta) &= \mathbb{E}{[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)]}  \\
    & \frac{1}{G}\sum_{i=1}^G \left( \min \left( \frac{\pi_\theta(o_i |q)}{\pi_{\theta_{old}}(o_i |q)} A_i, \text{clip} \left( \frac{\pi_\theta(o_i |q)}{\pi_{\theta_{old}}(o_i |q)}, 1 - \epsilon, 1 + \epsilon \right)  A_i \right) - \beta \mathbb{D}_{KL}\left(\pi_{\theta} || \pi_{ref}\right)\right) ,
\end{split}
\label{eq:GRPO-obj}
\end{equation}
\begin{equation}
    \mathbb{D}_{KL}\left(\pi_{\theta} || \pi_{ref}\right) = \frac{\pi_{ref}(o_i|q)}{\pi_{\theta}(o_i|q)}- \log\frac{\pi_{ref}(o_i|q)}{\pi_{\theta}(o_i|q)} - 1,
\end{equation}
where $\epsilon$ and $\beta$ are hyper-parameters; 
and $A_i$ is the advantage, computed using a group of rewards $\{r_1, r_2, \ldots, r_G\}$ corresponding to the outputs within each group:
\begin{equation}
    A_i = \frac{r_i - {\mathrm mean(\{r_1, r_2, \cdots, r_G\})}}{{\mathrm std(\{r_1, r_2, \cdots, r_G\})}}.
\end{equation}

\paragraph{Training Strategy.}
In our preliminary experiments, we find that the RL training on reasoning data, such as code and math prompts, exhibits unique characteristics that are distinct from the training on general data. 
For example, the mathematical and coding abilities of our model can keep improving over a longer period of training steps.
Therefore, we employ a two-stage RL training strategy, which first performs reasoning alignment, and then performs human preference alignment.
In the first reasoning alignment stage, we train a reward model $RM_{reasoning}$ for code and math reasoning tasks, and optimize the policy model with the feedback of $RM_{reasoning}$:
\begin{equation}
    r_i=RM_{reasoning}(o_i). 
\end{equation}
In the second human preference alignment stage, we adopt a multi-reward framework, which acquires rewards from a helpful reward model $RM_{helpful}$, a safety reward model $RM_{safety}$, and a rule-based reward model $RM_{rule}$. 
The final reward of a response $o_i$ is
\begin{equation}
    r_i = c_1 \cdot RM_{helpful}(o_i) + c_2 \cdot RM_{safety}(o_i) + c_3 \cdot RM_{rule}(o_i), 
\end{equation}
where $c_1$, $c_2$, and $c_3$ are corresponding coefficients. 

In order to obtain reliable reward models that play crucial roles in the RL training, we carefully collect preference data, and meticulously conduct quality filtering and proportion adjustments. 
We obtain code preference data based on compiler-feedback, and mathematical preference data based on the ground-truth labels.
For reward model training, we initialize the reward models with \dsviisft{} and train them with either a point-wise or a pair-wise loss.
In our experiments, we observe that the RL training can fully tap into and activate the potential of our model, enabling it to select the correct and satisfactory answer from possible responses. 

\paragraph{Optimizations for Training Efficiency.}
Conducting RL training on extremely large models places high demands on the training framework. 
It requires careful engineering optimization to manage the GPU memory and RAM pressure, and meanwhile maintain a fast training speed.
For this goal, we implement the following engineering optimizations. 
(1) Firstly, we propose a hybrid engine that adopts different parallel strategies for training and inference respectively to achieve higher GPU utilization. 
(2) Secondly, we leverage vLLM~\citep{kwon2023efficient} with large batch sizes as our inference backend to accelerate the inference speed. 
(3) Thirdly, we carefully design a scheduling strategy for offloading models to CPUs and loading models back to GPUs, which achieves a near-optimal balance between the training speed and memory consumption. 

\subsection{Evaluation Results}

\textbf{Evaluations on Standard Benchmarks.}
Initially, we evaluate \dsviisft{} and \dsviirl{} on standard benchmarks. 
Notably, \dsviisft{} demonstrates substantial improvements in GSM8K, MATH, and HumanEval evaluations compared with its base version. 
This progress can be attributed to the inclusion of our SFT data, which comprises a considerable volume of math and code related content. 
In addition, \dsviirl{} further boosts the performance on math and code benchmarks. 
We show more code and math evaluations in Appendix~\ref{app:additional_math_code}. 

As for the comparisons with other models, we first compare \dsviisft{} with Qwen1.5 72B Chat, and find that \dsviisft{} surpasses Qwen1.5 72B Chat on almost all of English, math, and code benchmarks. 
On Chinese benchmarks, \dsviisft{} demonstrates slightly lower scores than Qwen1.5 72B Chat on multi-subject multiple-choice tasks, consistent with the performance observed from their base versions.
When compared with the state-of-the-art open-source MoE model, Mixtral 8x22B Instruct, \dsviisft{} exhibits better performance on most benchmarks, except for NaturalQuestions and IFEval. 
Furthermore, in comparison to the state-of-the-art open-source model LLaMA3 70B Chat, \dsviisft{} shows similar performance in code and math related benchmarks. 
LLaMA3 70B Chat exhibits better performance on MMLU and IFEval, while \dsviisft{} showcases stronger performance on Chinese tasks. 
Ultimately, \dsviirl{} demonstrates further enhanced performance in both mathematical and coding tasks compared with \dsviisft{}.
These comparisons highlight the strengths of \dsvii{} Chat in relation to other language models in various domains and languages.

\input{tables/chat_results.tex}

\textbf{Evaluations on Open-Ended Generation.} 
We proceed with additional evaluations of our models on open-ended conversation benchmarks. 
For English open-ended conversation generation, we utilize MT-Bench and AlpacaEval 2.0 as the benchmarks. 
Evaluation results presented in Table~\ref{tab:open} demonstrate a significant performance advantage of \dsviirl{} over \dsviisft{}. 
This outcome showcases the effectiveness of our RL training in achieving improved alignment. 
In comparison to other open-source models, \dsviirl{} demonstrates superior performance over Mistral 8x22B Instruct and Qwen1.5 72B Chat on both benchmarks. 
When compared with LLaMA3 70B Instruct, \dsviirl{} showcases competitive performance on MT-Bench and notably outperforms it on AlpacaEval 2.0. 
These results highlight the strong performance of \dsviirl{} in generating high-quality and contextually relevant responses, particularly in instruction-based conversation tasks.

\begin{table}[!t]
    \centering
    \begin{tabular}{c | c c}
    \toprule
    \textbf{Model} & \textbf{MT-Bench} & \textbf{AlpacaEval 2.0} \\
    \midrule
    DeepSeek 67B Chat & 8.35 & 16.6  \\
    Mistral 8x22B Instruct v0.1 & 8.66 & 30.9 \\
    Qwen1.5 72B Chat & 8.61 & 36.6 \\
    LLaMA3 70B Instruct & \textbf{8.95} & 34.4 \\
    \dsviisft & 8.62 & 30.0 \\
    \dsviirl & \textbf{8.97} & \textbf{38.9} \\
    \bottomrule
    \end{tabular}
    \caption{
    English open-ended conversation evaluations. 
    For AlpacaEval 2.0, we use the length-controlled win rate as the metric. 
    }
    \label{tab:open} 
\end{table}

In addition, we evaluate the Chinese open-ended generation capability based on AlignBench. 
As presented in Table~\ref{tab:align_bench_results}, \dsviirl{} exhibits a slight advantage over~\dsviisft{}. 
Notably, \dsviisft{} surpasses all open-source Chinese models by a significant margin. 
It significantly outperforms the second-best open-source model, Qwen1.5 72B Chat on both Chinese reasoning and language. 
Moreover, both \dsviisft{} and \dsviirl{} outperform GPT-4-0613 and ERNIEBot 4.0, solidifying the position of our models in the top-tier LLMs that support Chinese. 
Specifically, \dsviirl{} shows remarkable performance in Chinese language understanding, which outperforms all models including GPT-4-Turbo-1106-Preview. 
On the other hand, the reasoning capability of \dsviirl{} still lags behind giant models, such as Erniebot-4.0 and GPT-4s. 

\input{tables/alignbench.tex}

\subsection{Discussion}

\paragraph{Amount of SFT Data.}
The discussion surrounding the necessity of a large SFT corpus has been a topic of intense debate. 
Previous works~\citep{yi, lima} argue that fewer than 10K instances of SFT data are enough to produce satisfactory results. 
However, in our experiments, we observe a significant performance decline on the IFEval benchmark if we use fewer than 10K instances. 
A possible explanation is that, a language model necessitates a certain amount of data to develop specific skills. 
Although the requisite data amount may diminish with the model size increasing, it cannot be entirely eliminated. 
Our observation underscores the critical need for sufficient data to equip an LLM with desired capabilities. 
Moreover, the quality of SFT data is also crucial, especially for tasks involving writing or open-ended questions. 

\paragraph{Alignment Tax of Reinforcement Learning.}
During human preference alignment, we observe a significant performance enhancement on the open-ended generation benchmarks, in terms of the scores rated by both AI and human evaluators. 
However, we also notice a phenomenon of ``alignment tax'' \citep{ouyang2022training}, i.e., the alignment process can negatively impact the performance on some standard benchmarks such as BBH. 
In order to alleviate the alignment tax, during the RL stage, we make significant efforts in data processing and improving training strategies, finally achieving a tolerable trade-off between the performance on standard and open-ended benchmarks. 
Exploring how to align a model with human preferences without compromising its general performance presents a valuable direction for future research.

\paragraph{Online Reinforcement Learning.}
In our preference alignment experiments, we find that the online approach significantly outperforms the offline approach. 
Therefore, we invest tremendous efforts in implementing an online RL framework for aligning \dsvii{}. 
The conclusion about online or offline preference alignment can vary in different contexts, and we reserve a more thorough comparison and analysis between them for future work.

\section{Conclusion, Limitation, and Future Work}
\label{sec:conclusion}

In this paper, we introduce \dsvii{}, a large MoE language model that supports 128K context length. 
In addition to strong performance, it is also characterized by economical training and efficient inference, benefiting from its innovative architecture including \dsattn{} and \dsmoe{}. 
In practice, compared with \dsvi{}, \dsvii{} achieves significantly stronger
performance, and meanwhile saves 42.5\% of training costs, reduces the KV cache by 93.3\%, and boosts the maximum generation throughput to 5.76 times. 
Evaluation results further demonstrate that with only 21B activated parameters, \dsvii{} achieves top-tier performance among open-source models and becomes the strongest open-source MoE model. 

\dsvii{} and its chat versions share the acknowledged limitations commonly found in other LLMs, including the lack of ongoing knowledge updates after pre-training, the possibility of generating non-factual information such as unverified advice, and a chance to produce hallucinations. 
In addition, since our data primarily consist of Chinese and English content, our model may exhibit limited proficiency in other languages. 
In scenarios beyond Chinese and English, it should be used with caution.

DeepSeek will continuously invest in open-source large models with longtermism, aiming to progressively approach the goal of artificial general intelligence.
\begin{itemize}
    \item 
    In our ongoing exploration, we are dedicated to devising methods that enable further scaling up MoE models while maintaining economical training and inference costs. 
    The goal of our next step is to achieve performance on par with GPT-4 in our upcoming release.
    \item 
    Our alignment team continuously strives to enhance our models, aiming to develop a model that is not only helpful but also honest and safe for worldwide users. 
    Our ultimate objective is to align the values of our model with human values, while minimizing the need for human supervision. 
    By prioritizing ethical considerations and responsible development, we are dedicated to creating a positive and beneficial impact on society.  
    \item 
    Currently, \dsvii{} is designed to support the text modality exclusively. 
    In our forward-looking agenda, we intend to enable our model to support multiple modalities, enhancing its versatility and utility in a wider range of scenarios.
\end{itemize}

\bibliography{main}

\newpage
\appendix

\section*{Appendix}

\section{Contributions and Acknowledgments}

\begin{multicols}{2} % 开始两栏环境
\noindent
\textbf{Research \& Engineering} \\
Aixin Liu \\
Bingxuan Wang \\
Bo Liu \\
Chenggang Zhao \\
Chengqi Deng \\
Chong Ruan \\
Damai Dai \\
Daya Guo \\
Dejian Yang \\
Deli Chen \\
Erhang Li \\
Fangyun Lin \\
Fuli Luo \\
Guangbo Hao \\
Guanting Chen \\
Guowei Li \\
H. Zhang \\
Hanwei Xu \\
Hao Yang \\
Haowei Zhang \\
Honghui Ding \\
Huajian Xin \\
Huazuo Gao \\
Hui Qu \\
Jianzhong Guo \\
Jiashi Li \\
Jingyang Yuan \\
Junjie Qiu \\
Junxiao Song \\
Kai Dong \\
Kaige Gao \\
Kang Guan \\
Lean Wang \\
Lecong Zhang \\
Liang Zhao \\
Liyue Zhang \\
Mingchuan Zhang \\
Minghua Zhang \\
Minghui Tang \\
Panpan Huang \\
Peiyi Wang \\
Qihao Zhu \\
Qinyu Chen \\
Qiushi Du \\
Ruiqi Ge \\
Ruizhe Pan \\
Runxin Xu \\
Shanghao Lu \\
Shangyan Zhou \\
Shanhuang Chen \\
Shengfeng Ye \\
Shirong Ma \\
Shiyu Wang \\
Shuiping Yu \\
Shunfeng Zhou \\
Size Zheng \\
Tian Pei \\
Wangding Zeng \\
Wen Liu \\
Wenfeng Liang \\
Wenjun Gao \\
Wentao Zhang \\
Xiao Bi \\
Xiaohan Wang \\
Xiaodong Liu \\
Xiaokang Chen \\
Xiaotao Nie \\
Xin Liu \\
Xin Xie \\
Xingkai Yu \\
Xinyu Yang \\
Xuan Lu \\
Xuecheng Su \\
Y. Wu \\
Y.K. Li \\
Y.X. Wei \\
Yanhong Xu \\
Yao Li \\
Yao Zhao \\
Yaofeng Sun \\
Yaohui Wang \\
Yichao Zhang \\
Yiliang Xiong \\
Yilong Zhao \\
Ying He \\
Yishi Piao \\
Yixin Dong \\
Yixuan Tan \\
Yiyuan Liu \\
Yongji Wang \\
Yongqiang Guo \\
Yuduan Wang \\
Yuheng Zou \\
Yuxiang You \\
Yuxuan Liu \\
Z.Z. Ren \\
Zehui Ren \\
Zhangli Sha \\
Zhe Fu \\
Zhenda Xie \\
Zhewen Hao \\
Zhihong Shao \\
Zhuoshu Li \\
Zihan Wang \\
Zihui Gu \\
Zilin Li \\
Ziwei Xie \\

\noindent
\textbf{Data Annotation} \\
Bei Feng  \\
Hui Li  \\
J.L. Cai  \\
Jiaqi Ni  \\
Lei Xu  \\
Meng Li  \\
Ning Tian  \\
R.J. Chen  \\
R.L. Jin  \\
Ruyi Chen  \\
S.S. Li  \\
Shuang Zhou  \\
Tian Yuan  \\
Tianyu Sun  \\
X.Q. Li  \\
Xiangyue Jin  \\
Xiaojin Shen  \\
Xiaosha Chen  \\
Xiaowen Sun  \\
Xiaoxiang Wang  \\
Xinnan Song  \\
Xinyi Zhou  \\
Y.X. Zhu  \\
Yanhong Xu  \\
Yanping Huang  \\
Yaohui Li  \\
Yi Zheng  \\
Yuchen Zhu  \\
Yunxian Ma  \\
Zhen Huang  \\
Zhipeng Xu  \\
Zhongyu Zhang  \\

\noindent
\textbf{Business \& Compliance} \\
Bin Wang  \\
Dongjie Ji  \\
Jian Liang  \\
Jin Chen  \\
Leyi Xia  \\
Miaojun Wang  \\
Mingming Li  \\
Peng Zhang  \\
Shaoqing Wu  \\
Shengfeng Ye  \\
T. Wang  \\
W.L. Xiao  \\
Wei An  \\
Xianzu Wang  \\
Ying Tang  \\
Yukun Zha  \\
Yuting Yan  \\
Zhen Zhang  \\
Zhiniu Wen  \\

\end{multicols} % 结束两栏环境

Within each role, authors are listed alphabetically by first name.
Especially, Huazuo Gao and Wangding Zeng have made key innovations in the research of the MLA architecture.
Furthermore, we'd like to thank Jianlin Su for his helpful discussion on position embedding.
We thank all those who have contributed to DeepSeek-V2 but are not mentioned in the paper.
DeepSeek believes that innovation, novelty, and curiosity are essential in the path to AGI.

\newpage

\section{\dsviilite{}: A 16B Model Equipped with MLA and DeepSeekMoE}
\label{app:dsviilite}

\subsection{Model Description}

\paragraph{Architectures.}
\dsviilite{} has 27 layers and a hidden dimension of 2048.
It also employs MLA and has 16 attention heads, where each head has a dimension of 128.
Its KV compression dimension is 512, but slightly different from \dsvii{}, it does not compress the queries. 
For the decoupled queries and key, it has a per-head dimension of 64. 
\dsviilite{} also employs \dsmoe{}, and all FFNs except for the first layer are replaced with MoE layers. 
Each MoE layer consists of 2 shared experts and 64 routed experts, where the intermediate hidden dimension of each expert is 1408. 
Among the routed experts, 6 experts will be activated for each token. 
Under this configuration, \dsviilite{} comprises 15.7B total parameters, of which 2.4B are activated for each token. 

\begin{table}[!ht]
    \centering
    % \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{@{}c c l c c c@{}}
    \toprule
    & \textbf{Benchmark} & & \textbf{DeepSeek 7B} & \textbf{DeepSeekMoE 16B} & \textbf{DeepSeek-V2-Lite} \\
    \midrule
    & Architecture &  & MHA+Dense & MHA+MoE & MLA+MoE \\
    & Context Length & & 4K & 4K & 32K \\
    & \# Activated Params &  & 6.9B & 2.8B & 2.4B \\
    & \# Total Params &  & 6.9B & 16.4B & 15.7B \\
    & \# Training Tokens  &  & 2T & 2T & 5.7T \\
    \midrule
    \multirow{8}{*}{English} 
    & MMLU & & 48.2 & 45.0 & \textbf{58.3} \\
    & BBH & & 39.5 & 38.9 & \textbf{44.1} \\
    & TriviaQA & & 59.7 & \textbf{64.8} & 64.2 \\
    & NaturalQuestions & & 22.2 & 25.5 & \textbf{26.0} \\
    & ARC-Easy & & 67.9 & 68.1  & \textbf{70.9} \\
    & ARC-Challenge & & 48.1 &  49.8 & \textbf{51.2} \\
    & AGIEval & & 26.4 & 17.4 & \textbf{33.2} \\
    \midrule
    \multirow{3}{*}{Code} & HumanEval & & 26.2 & 26.8 & \textbf{29.9} \\
    & MBPP & & 39.0 & 39.2 & \textbf{43.2} \\
    \midrule
    \multirow{4}{*}{Math} & GSM8K & & 17.4 &  18.8 & \textbf{41.1} \\
    & MATH & & 3.3 & 4.3 & \textbf{17.1} \\
    & CMath & & 34.5 & 40.4 & \textbf{58.4} \\
    \midrule
    \multirow{3}{*}{Chinese} & CLUEWSC & & 73.1 &  72.1 & \textbf{74.3} \\
    & C-Eval & & 45.0 &  40.6 & \textbf{60.3} \\
    & CMMLU & & 47.2 &  42.5 & \textbf{64.3} \\
    \bottomrule
    \end{tabular}
    \caption{
    Performance of \dsviilite{}, DeepSeekMoE 16B, and DeepSeek 7B. 
    }
    \label{tab:dsviilite_base}
\end{table}

\paragraph{Training Details.}
\dsviilite{} is also trained from scratch on the same pre-training corpus of \dsvii{}, which is not polluted by any SFT data. 
It uses the AdamW optimizer with hyper-parameters set to $\beta_1=0.9$, $\beta_2=0.95$, and $\mathrm{weight\_decay}=0.1$. 
The learning rate is scheduled using a warmup-and-step-decay strategy. 
Initially, the learning rate linearly increases from 0 to the maximum value during the first 2K steps. 
Subsequently, the learning rate is multiplied by 0.316 after 
training about 80\% of tokens, and again by 0.316 after training about 90\% of tokens. 
The maximum learning rate is set to $4.2 \times 10^{-4}$, and the gradient clipping norm is set to 1.0.
We do not employ the batch size scheduling strategy for it, and it is trained with a constant batch size of 4608 sequences. 
During pre-training, we set the maximum sequence length to 4K, and train \dsviilite{} on 5.7T tokens. 
We leverage pipeline parallelism to deploy different layers of it on different devices, but for each layer, all experts will be deployed on the same device. 
Therefore, we only employ a small expert-level balance loss with $\alpha_{1}=0.001$, and do not employ device-level balance loss and communication balance loss for it. 
After pre-training, we also perform long context extension and SFT for \dsviilite{} and get a chat model called \dsviilite{} Chat.

\begin{table}[!ht]
    \centering
    % \footnotesize
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{@{}c c l c c c@{}}
    \toprule
    & \multirow{2}{*}{\centering \textbf{Benchmark}} & & \textbf{DeepSeek} & \textbf{DeepSeekMoE} & \textbf{DeepSeek-V2-Lite} \\
    & & & \textbf{7B Chat} & \textbf{16B Chat} & \textbf{Chat} \\
    \midrule
    & Architecture &  & MHA+Dense & MHA+MoE & MLA+MoE \\
    & Context Length & & 4K & 4K & 32K \\
    & \# Activated Params &  & 6.9B & 2.8B & 2.4B \\
    & \# Total Params &  & 6.9B & 16.4B & 15.7B \\
    & \# Training Tokens  &  & 2T & 2T & 5.7T \\
    \midrule
    \multirow{8}{*}{English} 
    & MMLU & & 49.7 &  47.2 & \textbf{55.7} \\
    & BBH & & 43.1 & 42.2 & \textbf{48.1} \\
    & TriviaQA & & 59.5 & 63.3 & \textbf{65.2} \\
    & NaturalQuestions & & 32.7 & 35.1 & \textbf{35.5} \\
    & ARC-Easy & & 70.2 &  69.9 & \textbf{74.3} \\
    & ARC-Challenge & & 50.2 &  50.0 & \textbf{51.5} \\
    & AGIEval & & 17.6 & 19.7 & \textbf{42.8} \\
    % & IFEval & & xx & xx & ?? \\
    \midrule
    \multirow{3}{*}{Code} & HumanEval & & 45.1 & 45.7 & \textbf{57.3} \\
    & MBPP & & 39.0 & \textbf{46.2} & 45.8 \\
    \midrule
    \multirow{4}{*}{Math} & GSM8K & & 62.6 &  62.2 & \textbf{72.0} \\
    & MATH & & 14.7 & 15.2 & \textbf{27.9} \\
    & CMath & & 66.4 & 67.9 & \textbf{71.7} \\
    \midrule
    \multirow{3}{*}{Chinese} & CLUEWSC & & 66.2  & 68.2 & \textbf{80.0} \\
    & C-Eval & & 44.7 &  40.0 & \textbf{60.1} \\
    & CMMLU & & 51.2  &  49.3 & \textbf{62.5} \\
    \bottomrule
    \end{tabular}
    \caption{
    Performance of \dsviilite{} Chat, DeepSeekMoE 16B Chat, and DeepSeek 7B Chat. 
    }
    \label{tab:dsviilite_chat}
\end{table}

\subsection{Performance Evaluation}

\paragraph{Base Model.}
We evaluate the performance of \dsviilite{} and compare it with our previous small-size base models in Table~\ref{tab:dsviilite_base}. 
\dsviilite{} exhibits overwhelming performance advantages, especially in reasoning, coding, and math. 

\paragraph{Chat Model.}
We evaluate the performance of \dsviilite{} Chat and compare it with our previous small-size chat models in Table~\ref{tab:dsviilite_chat}. 
\dsviilite{} also outperforms our previous small-size chat models by a large margin. 

\section{Full Formulas of \dsattn{}}
\label{app:full_formulas}

In order to demonstrate the complete computation process of \dsattn{}, we provide its full formulas in the following: 
\begin{align}
    \mathbf{c}_{t}^{Q} &= W^{DQ} \mathbf{h}_{t}, \\
    [\mathbf{q}_{t, 1}^{C};\mathbf{q}_{t, 2}^{C};...;\mathbf{q}_{t, n_{h}}^{C}] = \mathbf{q}_{t}^{C} &= W^{UQ} \mathbf{c}_{t}^{Q}, \\
    [\mathbf{q}_{t, 1}^{R};\mathbf{q}_{t, 2}^{R};...;\mathbf{q}_{t, n_{h}}^{R}] = \mathbf{q}_{t}^{R} &= \operatorname{RoPE}({W^{QR}} \mathbf{c}_{t}^{Q}), \\
    \mathbf{q}_{t, i} &= [\mathbf{q}_{t, i}^{C}; \mathbf{q}_{t, i}^{R}], \\
    \boxed{\color{blue} \mathbf{c}_{t}^{KV}} &= W^{DKV} \mathbf{h}_{t}, \\
    [\mathbf{k}_{t, 1}^{C};\mathbf{k}_{t, 2}^{C};...;\mathbf{k}_{t, n_{h}}^{C}] = \mathbf{k}_{t}^{C} &= W^{UK} \mathbf{c}_{t}^{KV}, \\
    \boxed{\color{blue}\mathbf{k}_{t}^{R}} &= \operatorname{RoPE}({W^{KR}} \mathbf{h}_{t}), \\
    \mathbf{k}_{t, i} &= [\mathbf{k}_{t, i}^{C}; \mathbf{k}_{t}^{R}], \\
    [\mathbf{v}_{t, 1}^{C};\mathbf{v}_{t, 2}^{C};...;\mathbf{v}_{t, n_{h}}^{C}] = \mathbf{v}_{t}^{C} &= W^{UV} \mathbf{c}_{t}^{KV}, \\
    \mathbf{o}_{t, i} &= \sum_{j=1}^{t} \operatorname{Softmax}_j(\frac{\mathbf{q}_{t, i}^T \mathbf{k}_{j, i}}{\sqrt{d_{h} + d_{h}^{R}}}) \mathbf{v}_{j, i}^{C}, \\
    \mathbf{u}_{t} &= W^{O} [\mathbf{o}_{t, 1};\mathbf{o}_{t, 2};...;\mathbf{o}_{t, n_{h}}],
\end{align}
where the boxed vectors in blue need to be cached for generation. 
During inference, the naive formula needs to recover $\mathbf{k}_{t}^{C}$ and $\mathbf{v}_{t}^{C}$ from $\mathbf{c}_{t}^{KV}$ for attention. 
Fortunately, due to the associative law of matrix multiplication, we can absorb $W^{UK}$ into $W^{UQ}$, and $W^{UV}$ into $W^{O}$. 
Therefore, we do not need to compute keys and values out for each query. 
Through this optimization, we avoid the computational overhead for recomputing $\mathbf{k}_{t}^{C}$ and $\mathbf{v}_{t}^{C}$ during inference. 

\section{Ablation of Attention Mechanisms}

\subsection{Ablation of MHA, GQA, and MQA}
\label{app:mha_gqa_mqa}

We show the evaluation results for 7B dense models with MHA, GQA, and MQA on four hard benchmarks in Table~\ref{tab:mha_gqa_mqa}. 
All of these three models are trained on 1.33T tokens, and share the same architecture except for the attention mechanisms. 
In addition, for a fair comparison, we align the number of parameters of them to around 7B by adjusting the number of layers. 
From the table, we can find that MHA demonstrates significant advantages over GQA and MQA on these benchmarks. 

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}l c | c c c@{}}
    \toprule
    \multirow{2}{*}{\centering \textbf{Benchmark (Metric)}} & \multirow{2}{*}{\textbf{\# Shots}} & \textbf{Dense 7B} & \textbf{Dense 7B} & \textbf{Dense 7B} \\
     & & \textbf{w/ MQA} & \textbf{w/ GQA (8 Groups)} & \textbf{w/ MHA} \\
    \midrule
    \# Params & - & 7.1B & 6.9B & 6.9B \\
    \midrule
    BBH (EM) & 3-shot & 33.2 & 35.6 & \textbf{37.0} \\
    MMLU (Acc.) & 5-shot & 37.9 & 41.2 & \textbf{45.2} \\
    C-Eval (Acc.) & 5-shot & 30.0 & 37.7 & \textbf{42.9} \\
    CMMLU (Acc.) & 5-shot & 34.6 & 38.4 & \textbf{43.5} \\
    \bottomrule
    \end{tabular}
    \caption{
    Comparison among 7B dense models with MHA, GQA, and MQA, respectively. 
    MHA demonstrates significant advantages over GQA and MQA on hard benchmarks. 
    }
    \label{tab:mha_gqa_mqa}
\end{table}

\subsection{Comparison Between \dsattn{} and MHA}
\label{app:dsattn_mha}

In Table~\ref{tab:dsattn_mha}, we show the evaluation results for MoE models equipped with \dsattn{} and MHA, respectively, on four hard benchmarks. 
For a solid conclusion, we train and evaluate models across two scales. 
Two small MoE models comprise about 16B total parameters, and we train them on 1.33T tokens. 
Two large MoE models comprise about 250B total parameters, and we train them on 420B tokens. 
Also, two small MoE models and two large MoE models respectively share the same architecture except for the attention mechanisms. 
From the table, we can observe that \dsattn{} shows better performance than MHA. 
More importantly, \dsattn{} requires a significantly smaller amount of KV cache (14\% for small MoE models and 4\% for large MoE models) than MHA. 

\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{@{}l c | c c | c c@{}}
    \toprule
    \multirow{2}{*}{\centering \textbf{Benchmark (Metric)}} & \multirow{2}{*}{\textbf{\# Shots}} & \textbf{Small MoE} & \textbf{Small MoE} & \textbf{Large MoE} & \textbf{Large MoE} \\
     & & \textbf{w/ MHA} & \textbf{w/ \dsattn{}} & \textbf{w/ MHA} & \textbf{w/ \dsattn{}} \\
    \midrule
    \# Activated Params & - & 2.5B & 2.4B & 25.0B & 21.5B \\
    \# Total Params & - & 15.8B & 15.7B & 250.8B & 247.4B \\
    KV Cache per Token (\# Element) & - & 110.6K & 15.6K & 860.2K & 34.6K \\
    \midrule
    BBH (EM) & 3-shot & 37.9 & \textbf{39.0} & 46.6 & \textbf{50.7} \\
    MMLU (Acc.) & 5-shot & 48.7 & \textbf{50.0} & 57.5 & \textbf{59.0} \\
    C-Eval (Acc.) & 5-shot & \textbf{51.6} & 50.9 & 57.9 & \textbf{59.2} \\
    CMMLU (Acc.) & 5-shot & 52.3 & \textbf{53.4} & 60.7 & \textbf{62.5} \\
    \bottomrule
    \end{tabular}
    \caption{
    Comparison between \dsattn{} and MHA on hard benchmarks. 
    \dsvii{} shows better performance than MHA, but requires a significantly smaller amount of KV cache. 
    }
    \label{tab:dsattn_mha}
\end{table}

\section{Discussion About Pre-Training Data Debiasing}
\label{app:filtering_controversial_content}

During pre-training data preparation, we identify and filter out contentious content, such as values influenced by regional cultures, to avoid our model exhibiting unnecessary subjective biases on these controversial topics.
Consequently, we observe that \dsvii{} performs slightly worse on the test sets that are closely associated with specific regional cultures. 
For example, when evaluated on MMLU, although \dsvii{} achieves comparable or superior performance on the majority of testsets compared with its competitors like Mixtral 8x22B, it still lags behind on the Humanity-Moral subset, which is mainly associated with American values. 

Further, we conduct a manual analysis on this subset. 
Three well-educated human annotators conduct independent annotations on 420 moral scenarios from the MMLU Humanity-Moral subset. 
Then, we compute the agreement among their annotations and the ground-truth label. 
As shown in Table~\ref{tab:filtering_data}, three human annotators and the ground-truth label exhibit a low agreement with each other. 
Therefore, we attribute the abnormal performance of \dsvii{} on these value-sensitive test sets to our efforts in debiasing the pre-training corpus. 

\input{tables/filtering_data.tex}

\section{Additional Evaluations on Math and Code}
\label{app:additional_math_code}

\input{tables/math.tex}

\section{Evaluation Formats}
\label{app:evaluation_form}

We present our evaluation formats for each benchmark in Table~\ref{tab:agieval_eval_format_example}-\ref{tab:CRUXEval_O_eval_format_example}, respectively. 

\input{tables/evaluation_format}

\end{CJK*}
\end{document} 

