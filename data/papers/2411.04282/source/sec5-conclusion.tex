
\section{Conclusion}
\label{sec:conclusion}


In conclusion, this work introduces LaTRO, a principled framework for optimizing language models' reasoning capabilities without external feedback or reward models. By formulating reasoning as sampling from a latent distribution and leveraging self-rewarding, LaTRO enables models to concurrently improve both their reasoning process and ability to evaluate reasoning quality. Our extensive experiments across multiple model architectures and tasks demonstrate significant performance gains, with LaTRO outperforming baseline models and supervised fine-tuning approaches. These findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked through our proposed optimization approach, representing a significant step towards creating more intelligent systems that can self-evolve their problem-solving capabilities.

While LaTRO shows promising results, there are some limitations to consider. The computational cost of sampling multiple rationales during training could be prohibitive for very large models. Future work could explore ways to reduce this computational overhead, such as using more efficient sampling techniques or adaptive rationale generation. Other promising directions include investigating the applicability of LaTRO to a wider range of reasoning tasks beyond math and science problems, and exploring how to 
conduct multi-step reasoning learning to to enhance reasoning capabilities further. Despite these limitations, our contributions advance both the state-of-the-art in LLM reasoning capabilities and provide valuable insights into the nature of LLM alignment and its potential for self-improvement.

\iffalse
\paragraph{Limitation}
\label{sec:limitaion}

\paragraph{Future work}
\label{sec:future_work}

\begin{itemize}[left=4pt]
    % \item \textbf{Full Fine-tuning}: Choose both $q_\phi$ and $p_\theta$ to be the current LLM for optimization as $\pi_\theta$, then we can recover the original objective \labelcref{eqn:latro_problem}, and $p_0$ can be the previous LLM to make sure we are not optimizing too far.
    \item \textbf{Adapter Optimization}: Choose $q_\phi$ to be a lightweight adapter, e.g. LoRA \citep{DBLP:conf/iclr/HuSWALWWC22} that controls the reasoning process, or even an implicit reasoning adapter \citep{DBLP:journals/corr/abs-2311-01460}, and $p_\theta = p_0 = \llm$ to be a fixed LLM.
    \item \textbf{Sampling Control}: Choose $q = p_\theta = p_0 = \llm$ to be the same, fixed LLM while $\phi$ denotes the parameters of an inference-time algorithm to control the sampling process.
    \item \textbf{Prompt Optimization}: Fix all $q_\phi = p_\theta = p_0 = \llm$ while optimizing the prompt $\reason$.
    \item \textbf{Teacher Model} $p$: since $p$ can be arbitrary distribution, we can use a stronger teacher model that is better at reasoning instead of the original LLM.
\end{itemize}

\fi 

% \clearpage