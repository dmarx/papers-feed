% All related keywords:
% Large Language Model;
% Optimizing reasoning capabilities;
% Self-improvement;
% Reinforcement Learning;
% Reward model-free optimization;\haolincomment{should we term 'alignment' or optimization?};

% uncover and activate the LLM internal reasoning capability;
% self-synthesis high-quality reasoning paths (the lack of existing reasoning dataset); 
% maintain the diversity of reasoning paths (compared to sft); 
% training-time self-consistency;

% \zuxin{Maybe we can use the words "The Hidden Reasoner" to describe the main topic of this paper. Like: 1) Unveiling the Hidden Reasoner: Unlocking Latent Reasoning Capabilities in Large Language Models. 2) Your Language Model is Secretly a Hidden Reasoner: Unlocking Latent Reasoning Capabilities in Large Language Models}

% Story-telling Outline:

% 1. Importance of reasoning capability in LLMs \\
% 2. Brief introduction to LLM reasoning capabilities \\
% 3. Challenges of direct answer generation for complex problems \\
% 4. The concept of reasoning paths and their importance \\
% 5. Potential of inference scaling (e.g., OpenAI's o1 model) \\
% 6. Lack of research on improving reasoning during training \\
% 7. Challenges in training-time reasoning improvement:
%    a. Lack of high-quality reasoning data for complex problems
%    b. Issues with supervised fine-tuning (SFT) approaches
%    c. Difficulties in reward modeling for RLHF approaches \\
% 8. Key questions to address:
%    a. Existence and activation of innate reasoning capabilities in LLMs
%    b. Methods to invoke reasoning without external models or feedback
%    c. Generating high-quality, diverse reasoning paths
%    d. Principled approach to shift inference-time scaling to training time \\
% 9. Surprising findings and potential of the proposed approach

% Draft Introduction:

% Reasoning is a cornerstone of human intelligence, enabling the resolution of complex problems, the formation of sound decisions, and the navigation in the intricacies of the world \needscitation.

As we strive to develop more capable and aligned large language models (LLMs), enhancing their reasoning capabilities has become a critical area of research. 
However, the inherent next-token prediction nature of LLMs presents a challenge for them to directly generate answers of complex problems that require multiple reasoning steps \citep{wang2022self,huang2023large}. For example, LLMs often struggle to directly provide precise answers to math problems or even simple puzzles like ``How many `r's are in the word 'strawberry'?".
This limitation has driven researchers to investigate various prompting approaches that encourage models to generate reasoning trajectories or rationalesâ€”sequences of tokens that provide additional context and step-by-step progression toward the final answer, such as Chain-of-Thought (CoT) \citep{DBLP:conf/nips/Wei0SBIXCLZ22}, Tree-of-Though (ToT) \citep{yao2024tree}, and Program-of-Thought (PoT) \citep{DBLP:journals/tmlr/ChenM0C23} prompting methods.


The recent advancements also show inference-time techniques to improve the reasoning capability of LLMs \citep{wu2024empirical,brown2024large}, as evidenced by the OpenAI o1 model\citep{openai_learning_2024}, which have demonstrated strong ability in many reasoning tasks, including math \citep{cobbe2021training,trinh2024solving,luo2024improve}, coding \citep{jimenez2023swe,guo2024deepseek,zhang2024diversity}, and science problems \citep{rein2023gpqa}. 
Among these approaches, CoT with Self-Consistency (CoT-SC) \citep{DBLP:conf/iclr/0002WSLCNCZ23} and CoT-Decoding \citep{wang2024chain} extend the CoT approach at inference time by generating multiple reasoning paths and selecting the most consistent one. Additionally, methods like ReAct \citep{DBLP:conf/iclr/YaoZYDSN023} and Reflexion \citep{DBLP:conf/nips/ShinnCGNY23} integrate reasoning into LLM agent loops, further enhancing problem-solving capabilities.

These approaches demonstrate great promise to elicit the reasoning capabilities in during the inference time \citep{snell2024scaling}. However, how to explicitly improve the reasoning abilities during the training phase of LLMs still remains a challenging problem. Several challenges impede progress in this direction. Firstly, there is a scarcity of high-quality reasoning data for complex problems, making it difficult to apply traditional supervised fine-tuning (SFT) approaches. Furthermore, even when such data is available, SFT on deterministic reasoning paths may lead to a lack of diversity in problem-solving strategies, potentially causing over-confidence issues and performance degradation \cite{cobbe2021training}. This is particularly concerning in domains requiring multiple valid approaches, such as mathematical proofs or ethical reasoning.

Another type of approach, which improving reasoning through reinforcement learning from human feedback (RLHF) presents its own set of challenges. Developing a reward model capable of accurately evaluating the quality and validity of reasoning paths is a formidable task, possibly subject to distribution shifts and biased evaluations.

Given these challenges, our research aims to address several fundamental questions:
\begin{enumerate}
    \item Do pretrained LLMs\haolincomment{We actually conducted experiments on instruction fine-tuned versions} already possess innate reasoning capabilities that have not been fully activated or utilized?
    \item If so, how can we invoke and enhance these reasoning abilities without relying on external models or additional human feedback?
    \item Is it possible to generate high-quality reasoning paths while maintaining diversity in problem-solving approaches? \haolincomment{We may need to show this empirically later on otherwise removing this part.}
    % \item Can we develop a principled method to shift the computational burden from inference-time scaling to the training phase, potentially leading to better trade-offs in model performance and efficiency? 
\end{enumerate}

\yy{consider move this to the main method and add a subsection as motivation. }

Our investigations have yielded promising results:
\begin{enumerate}
    \item We find evidence (\Cref{sec:motivating_example}) suggesting that pre-trained LLMs are not only capable reasoners, but also possess the potential to act as explicit reward models for evaluating reasoning paths. We call utilizing such explicit reward functions induced by LLM themselves \emph{self-rewarding}.
    \item Inspired by this preliminary finding, we propose a novel formulation of the reasoning process in LLMs as sampling from a latent distribution (\Cref{sec:formulation}).
    \item We develop latent reasoning optimization, LaTRO, a principled framework that aims to uncover and activate the internal reasoning capabilities of LLMs without the need for extra reward model training or external feedback (\Cref{sec:latro}).
    \item Our experiments show that empirically, by optimizing the rationales, LaTRO outperforms both the base model and the SFT baselines in GSM8K. In addition, our study also suggests that LaTRO is capable of compressing the reasoning process and further moving the compute burden of the inference time to the training time (\Cref{sec:results,sec:ablation}). \needsreview
\end{enumerate}

\haolincomment{Need a conclude of introduction here.}


%\yy{I feel this paragraph we should start with introducing several prompt based reasoning method (cot, self-consistent cot) and how exactly they are used, and then introduce the drawback of these methods.}

% These approaches of generating reasoning trajectories show great promise, with recent work by the OpenAI o1 model \citep{openai_learning_2024}\haolincomment{Not sure if here we should use a citation or a url in the footnote} demonstrating that inference-time scaling can significantly enhance LLM reasoning capabilities. However, a critical gap in the current research community lies in improving these reasoning abilities during training time, rather than relying solely on inference-time techniques.

% Several challenges impede progress in this direction. First, there is a scarcity of high-quality reasoning data for complex problems, making it difficult to apply traditional supervised fine-tuning (SFT) approaches. Moreover, even if such data were available, SFT on deterministic reasoning paths could lead to a lack of diversity in model problem-solving strategies, thus causing the over-confidence issues of the model and lead to potential performance degradation \citep{cobbe2021training}. This is particularly problematic in domains where there are multiple valid approaches, such as mathematical proofs or ethical reasoning.

% Second, the application of reinforcement learning from human feedback (RLHF) to improve reasoning faces its own set of obstacles. Developing a reward model capable of accurately judging the quality and validity of reasoning paths is a formidable task, potentially subject to issues of distribution shift and biased evaluation.


\yy{I feel the intro is too lengthy, maybe a better structure is 1) introduce reasoning; 2) introduce prompt based method and its drawback 3) training based and the challenge 4) several sentences to introduce our main idea and a summary of the content. and control the overall part less than the first two pages}

\input{figures/figure-intro-overview-v2}

\section{Related work}
\paragraph{Reasoning and Math Problem Solving of LLMs}
Prompt-based reasoning methods prove to be effective in many areas like math problem solving, logical reasoning and various kinds of agentic AI tasks. Chain-of-Thoughts or CoT~\citep{wei2022chain} is the pioneering work that prompts the agent to decompose challenging tasks into smaller, more manageable reasoning steps. Tree-of-Thoughts~\citep{yao2024tree}

\paragraph{Variational Bayesian Methods in LLMs}


\paragraph{Self-Improving of LLMs}
\paragraph{Reward Model Free Fine-tuning}
