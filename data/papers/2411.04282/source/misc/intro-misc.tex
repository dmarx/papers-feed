As we strive to create more capable and aligned artificial intelligence systems, endowing large language models (LLMs) with robust reasoning capabilities has become a critical area of research. Cutting-edge developments in LLMs have showcased their aptitude for diverse reasoning tasks, from logical deduction\needscitation to mathematical problem-solving\needscitation and even creative ideation.\needscitation Nonetheless, the journey towards consistently reliable and diverse reasoning in these models remains challenging and largely uncharted.

The inherent nature of LLMs as next-token prediction engines presents a significant hurdle when tasked with directly generating answers to complex problems. When confronted with a multistep mathematical question or even a tricky word problem, for instance ``how many r's in the word strawberry?", an LLM often finds it difficult to yield a coherent and precise answer without undertaking intermediate stages. This limitation has motivated researchers to explore alternative approaches, such as prompting models to generate reasoning trajectories or rationales, that is, sequences of tokens that provide additional context and step-by-step progression toward the final answer. Along this line of research, there are many effective approaches: Chain-of-Thought (CoT, \citet{DBLP:conf/nips/Wei0SBIXCLZ22}) guides LLM to think by prompting ``Let's think step by step", and Program-of-Thought (PoT, \citet{DBLP:journals/tmlr/ChenM0C23}) offers an alternative reasoning by prompting LLMs to code. CoT with self-consistency (CoT-SC, \citet{DBLP:conf/iclr/0002WSLCNCZ23}) scales CoT at inference time by generating multiple CoT responses and taking the major votes. ReAct \citep{DBLP:conf/iclr/YaoZYDSN023} and Reflexion \citep{DBLP:conf/nips/ShinnCGNY23} bring the reasoning into the LLM agent loops.
\yy{I feel this paragraph we should start with introducing several prompt based reasoning method (cot, self-consistent cot) and how exactly they are used, and then introduce the drawback of these methods.}

These approaches of generating reasoning trajectories show great promise, with recent work by the OpenAI o1 model \citep{openai_learning_2024}\haolincomment{Not sure if here we should use a citation or a url in the footnote} demonstrating that inference-time scaling can significantly enhance LLM reasoning capabilities. However, a critical gap in the current research community lies in improving these reasoning abilities during training time, rather than relying solely on inference-time techniques.

Several challenges impede progress in this direction. First, there is a scarcity of high-quality reasoning data for complex problems, making it difficult to apply traditional supervised fine-tuning (SFT) approaches. Moreover, even if such data were available, SFT on deterministic reasoning paths could lead to a lack of diversity in model problem-solving strategies, thus causing the over-confidence issues of the model and lead to potential performance degradation \citep{cobbe2021training}. This is particularly problematic in domains where there are multiple valid approaches, such as mathematical proofs or ethical reasoning.

Second, the application of reinforcement learning from human feedback (RLHF) to improve reasoning faces its own set of obstacles. Developing a reward model capable of accurately judging the quality and validity of reasoning paths is a formidable task, potentially subject to issues of distribution shift and biased evaluation.

Given these challenges, our research aims to address several fundamental questions:
\begin{enumerate}
    \item Do pretrained LLMs\haolincomment{We actually conducted experiments on instruction fine-tuned versions} already possess innate reasoning capabilities that have not been fully activated or utilized?
    \item If so, how can we invoke and enhance these reasoning abilities without relying on external models or additional human feedback?
    \item Is it possible to generate high-quality reasoning paths while maintaining diversity in problem-solving approaches? \haolincomment{We may need to show this empirically later on otherwise removing this part.}
    % \item Can we develop a principled method to shift the computational burden from inference-time scaling to the training phase, potentially leading to better trade-offs in model performance and efficiency? 
\end{enumerate}

\yy{consider move this to the main method and add a subsection as motivation. }

Our investigations have yielded promising results:
\begin{enumerate}
    \item We find evidence (\Cref{sec:motivating_example}) suggesting that pre-trained LLMs are not only capable reasoners, but also possess the potential to act as explicit reward models for evaluating reasoning paths. We call utilizing such explicit reward functions induced by LLM themselves \emph{self-rewarding}.
    \item Inspired by this preliminary finding, we propose a novel formulation of the reasoning process in LLMs as sampling from a latent distribution (\Cref{sec:formulation}).
    \item We develop latent reasoning optimization, LaTRO, a principled framework that aims to uncover and activate the internal reasoning capabilities of LLMs without the need for extra reward model training or external feedback (\Cref{sec:latro}).
    \item Our experiments show that empirically, by optimizing the rationales, LaTRO outperforms both the base model and the SFT baselines in GSM8K. In addition, our study also suggests that LaTRO is capable of compressing the reasoning process and further moving the compute burden of the inference time to the training time (\Cref{sec:results,sec:ablation}). \needsreview
\end{enumerate}






-----------------------



As we strive to create more capable and aligned artificial intelligence systems, the development of robust reasoning capabilities in large language models (LLMs) has emerged as a critical area of research. Recent advancements in LLMs have demonstrated their proficiency in diverse reasoning tasks, including logical deduction, mathematical problem-solving, and creative ideation. Despite these advancements, consistently reliable and diverse reasoning in these models remains a challenging and largely unexplored area.

The inherent nature of LLMs, which function as next-token prediction engines, presents a significant challenge when directly generating answers to complex problems. For example, LLMs often struggle to provide coherent and precise answers to multistep questions, such as intricate mathematical problems or even simple puzzles like "How many 'r's are in the word 'strawberry'?" without intermediate stages. This limitation has driven researchers to investigate alternative approaches, particularly those that prompt models to generate reasoning trajectories or rationalesâ€”sequences of tokens that provide additional context and step-by-step progression toward the final answer.

Among these approaches, Chain-of-Thought (CoT) prompting \cite{DBLP:conf/nips/Wei0SBIXCLZ22} encourages LLMs to think through problems step-by-step. Program-of-Thought (PoT) \cite{DBLP:journals/tmlr/ChenM0C23} offers an alternative by prompting LLMs to generate code as a form of reasoning. CoT with Self-Consistency (CoT-SC) \cite{DBLP:conf/iclr/0002WSLCNCZ23} scales the CoT approach at inference time by generating multiple reasoning paths and selecting the most consistent one. Additionally, methods like ReAct \cite{DBLP:conf/iclr/YaoZYDSN023} and Reflexion \cite{DBLP:conf/nips/ShinnCGNY23} integrate reasoning into LLM agent loops, further enhancing problem-solving capabilities.

These approaches demonstrate significant promise, as evidenced by the recent work using the OpenAI o1 model, which illustrates that inference-time scaling can markedly enhance LLM reasoning capabilities. However, a critical gap in the current research is the improvement of these reasoning abilities during the training phase, rather than relying solely on inference-time techniques.

Several challenges impede progress in this direction. Firstly, there is a scarcity of high-quality reasoning data for complex problems, making it difficult to apply traditional supervised fine-tuning (SFT) approaches. Furthermore, even when such data is available, SFT on deterministic reasoning paths may lead to a lack of diversity in problem-solving strategies, potentially causing over-confidence issues and performance degradation \cite{cobbe2021training}. This is particularly concerning in domains requiring multiple valid approaches, such as mathematical proofs or ethical reasoning.

Secondly, improving reasoning through reinforcement learning from human feedback (RLHF) presents its own set of challenges. Developing a reward model capable of accurately evaluating the quality and validity of reasoning paths is a formidable task, possibly subject to distribution shifts and biased evaluations.

Given these challenges, our research aims to address several fundamental questions:
1. Do pretrained LLMs possess latent reasoning capabilities that have not been fully activated or utilized?
2. If so, how can these reasoning abilities be invoked and enhanced without relying on external models or additional human feedback?
3. Is it feasible to generate high-quality reasoning paths while maintaining diversity in problem-solving approaches?

Our investigations have yielded promising results. Firstly, we find evidence suggesting that pretrained LLMs are not only capable reasoners but also possess the potential to act as explicit reward models for evaluating reasoning paths. We term this utilization of explicit reward functions induced by LLMs themselves as \emph{self-rewarding}. Inspired by this preliminary finding, we propose a novel formulation of the reasoning process in LLMs as sampling from a latent distribution. We develop Latent Reasoning Optimization (LaTRO), a principled framework aiming to uncover and activate the internal reasoning capabilities of LLMs without the need for additional reward model training or external feedback.

Empirically, our experiments show that by optimizing the rationales, LaTRO outperforms both the base model and SFT baselines on the GSM8K dataset. Moreover, our study suggests that LaTRO can compress the reasoning process, shifting the computational burden from inference time to the training phase, thereby leading to more efficient and effective model performance.

In summary, our research contributes novel insights and methodologies for enhancing the reasoning capabilities of LLMs during training, offering a promising direction for future advancements in AI reasoning and problem-solving.