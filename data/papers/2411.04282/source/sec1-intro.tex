The development of large language models (LLMs) with enhanced reasoning capabilities has emerged as a crucial area of research. Despite their impressive advances, the inherent next-token prediction mechanism of LLMs makes it challenging for these models to solve complex problems requiring multiple reasoning steps \citep{wang2022self, huang2023large}. For instance, LLMs often struggle to directly provide accurate solutions to mathematical problems or even simple puzzles like counting specific letters in a word.
Consequently, researchers have explored various prompting strategies that guide LLMs to generate reasoning trajectories or rationalesâ€”sequences of tokens that build a step-by-step progression toward an answer. Techniques such as Chain-of-Thought (CoT) \citep{DBLP:conf/nips/Wei0SBIXCLZ22}, Tree-of-Thought (ToT) \citep{yao2024tree}, and Program-of-Thought (PoT) \citep{DBLP:journals/tmlr/ChenM0C23} prompting methods exemplify this approach.

\input{figures/figure-intro-overview-v2}

Recent progress has also focused on inference-time techniques to enhance the reasoning abilities of LLMs \citep{wu2024empirical, brown2024large}, as observed in the OpenAI o1 model \citep{openai_learning_2024}. These methods have demonstrated remarkable performance in diverse reasoning tasks, including mathematics \citep{cobbe2021training, trinh2024solving, luo2024improve}, coding \citep{jimenez2023swe, guo2024deepseek, zhang2024diversity}, and scientific problem-solving \citep{rein2023gpqa}. Notable inference-time methods, such as CoT with Self-Consistency (CoT-SC) \citep{DBLP:conf/iclr/0002WSLCNCZ23} and CoT-Decoding \citep{wang2024chain}, extend the CoT approach by generating multiple reasoning paths and selecting the most consistent one. Additionally, techniques like ReAct \citep{DBLP:conf/iclr/YaoZYDSN023} and Reflexion \citep{DBLP:conf/nips/ShinnCGNY23} integrate reasoning into LLM agent loops, further enhancing their problem-solving capabilities.

Despite the promising results at inference time, improving the reasoning abilities of LLMs during their training phase remains a challenging problem. Several obstacles impede progress in this area. Firstly, there is a scarcity of high-quality reasoning data for complex problems, limiting the applicability of traditional supervised fine-tuning (SFT) approaches \citep{zelikman2022star}. Moreover, when such data is available, SFT on deterministic reasoning paths may result in a lack of diversity in problem-solving strategies, potentially causing over-confidence issues and performance degradation \citep{cobbe2021training}, especially in domains needing multiple valid approaches, such as mathematical proofs and coding.
Alternatively, improving reasoning through reinforcement learning from human feedback (RLHF) presents its own challenges \citep{havrilla2024teaching,luo2024improve}. Developing a reward model that accurately evaluates the quality and validity of reasoning paths is a formidable task, susceptible to distribution shifts and biased evaluations. 

Self-improvement approaches like STaR (Self-Taught Reasoner) \citep{zelikman2022star} and Quiet-STaR \citep{zelikman2024quiet} have shown promise in enhancing language models' reasoning capabilities without external feedback. 
However, STaR relies on task-specific few-shot examples to bootstrap its reasoning process, which can limit its generalizability across diverse tasks. While Quiet-STaR attempts to overcome this by inferring implicit rationales across arbitrary text, it does not directly optimize the reasoning process itself. Through these findings, we observe that \textit{pretrained LLMs already possess innate reasoning capabilities but just have not been fully activated or utilized}, inspiring us to propose our approach.

Our proposed method, \methodfull, addresses the limitations of previous approaches by formulating reasoning as sampling from a latent distribution and optimizing it through a principled variational framework. As illustrated in Fig. \ref{fig:overview}, LaTRO enables language models to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring task-specific few-shot examples or external reward models.
Key contributions of LaTRO include:
\begin{enumerate}[left=4pt]
\item A theoretical formulation connecting LLM reasoning optimization to latent variable models;
\item A self-rewarding mechanism leveraging the model's own probability estimates;
% \item The ability to compress long reasoning processes to shorter ones during training
\item Significant performance gains across multiple model architectures and reasoning tasks, demonstrating LaTRO's effectiveness in unlocking latent reasoning capabilities of language models.
\end{enumerate}
Our findings suggest that pre-trained LLMs are not only capable reasoners but also possess the potential to act as explicit reward models for evaluating reasoning paths. We term this approach of utilizing explicit reward functions induced by LLMs themselves as "self-rewarding." Empirically, LaTRO outperforms both baseline models and supervised fine-tuning approaches on reasoning tasks like GSM8K, while also demonstrating the capacity to compress reasoning processes and shift computational burdens from inference to training time.


\section{Related work}
\paragraph{Prompt-based LLM Reasoning}
Prompt-based reasoning methods prove to be effective across various domains, 
such as math problem-solving~\citep{polu2020generative,hendrycks2021measuring,DBLP:journals/corr/abs-2110-14168}, logical reasoning~\citep{sprague2024cot} and agentic tasks~\citep{DBLP:conf/iclr/YaoZYDSN023, DBLP:conf/nips/ShinnCGNY23,yao2023retroformer}. Chain-of-Thoughts or CoT~\citep{DBLP:conf/nips/Wei0SBIXCLZ22} is the pioneering work that prompts LLMs to decompose challenging tasks into smaller reasoning steps. After that, two primary research directions further improved reasoning capabilities during inference. One direction searched over the reasoning trajectories against a process-based verifier, or reward model~\citep{yao2024tree,DBLP:conf/aaai/BestaBKGPGGLNNH24,lightman2023let}. For example, tree-of-thoughts~\citep{yao2024tree} explored over thoughts by depth-first search (DFS), breadth-first search (BFS) or beam search. The other approach used a critic model to provide verbal feedback, iteratively refining the responses with that feedback~\citep{saunders2022self,DBLP:conf/nips/ShinnCGNY23,yao2023retroformer,NEURIPS2023_91edff07}. 

\paragraph{Self-Rewarding for LLM Reasoning}

Reasoning capabilities in LLMs can be enhanced in post-training through self-rewarding and reinforcement learning. The Self-Taught Reasoner, or STaR~\citep{zelikman2022star} introduced a bootstrapping technique that allows LLMs to generate rationales and fine-tune itself with self-generated reasoning paths. Quiet-STaR~\citep{zelikman2024quiet} extended this by training LLMs to infer implicit rationales across arbitrary text, enhancing both reasoning and predictive abilities without task-specific fine-tuning. Reinforced Fine-Tuning, or ReFT~\citep{trung2024reft} took this further by leveraging reinforcement learning to improve generalization in reasoning tasks like math problem-solving, enabling LLMs to learn from multiple reasoning paths. Self-correction capabilities in LLMs can also be reinforced through self-generated data~\citep{kumar2024training}. Lastly, \citet{hoffman2024training,hu2023amortizing} formulated the reasoning process as latent variable models, aligning LLMs towards more accurate reasoning with fewer annotated data.
