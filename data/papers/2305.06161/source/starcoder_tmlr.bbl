\begin{thebibliography}{118}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmad et~al.(2021)Ahmad, Chakraborty, Ray, and Chang]{ahmad2021plbart}
Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
\newblock Unified pre-training for program understanding and generation.
\newblock In \emph{Proceedings of NAACL}, 2021.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.211}.

\bibitem[Akiki et~al.(2022)Akiki, Pistilli, Mieskes, Gall{\'{e}}, Wolf, Ilic,
  and Jernite]{akiki-bigscience-22}
Christopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Gall{\'{e}}, Thomas
  Wolf, Suzana Ilic, and Yacine Jernite.
\newblock {BigScience:} a case study in the social construction of a
  multilingual large language model.
\newblock \emph{CoRR}, abs/2212.04960, 2022.
\newblock \doi{10.48550/arXiv.2212.04960}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2212.04960}.

\bibitem[Akiki et~al.(2023)Akiki, Ogundepo, Piktus, Zhang, Oladipo, Lin, and
  Potthast]{akiki-spacerini-search}
Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde
  Oladipo, Jimmy Lin, and Martin Potthast.
\newblock Spacerini: Plug-and-play search engines with {Pyserini} and {Hugging
  Face}.
\newblock \emph{CoRR}, abs/2302.14534, 2023.
\newblock \doi{10.48550/arXiv.2302.14534}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.14534}.

\bibitem[{Andersen et al v. Stability AI et al}(2023)]{stablediffusion_lawsuit}
{Andersen et al v. Stability AI et al}.
\newblock \emph{3:23-cv-00201 N.D. Cal.}
\newblock 2023.

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones,
  Joseph, Mann, DasSarma, Elhage, Hatfield-Dodds, Hernandez, Kernion, Ndousse,
  Olsson, Amodei, Brown, Clark, McCandlish, Olah, and
  Kaplan]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
  Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac
  Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
  Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and
  Jared Kaplan.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, and Sutton]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles
  Sutton.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Bahl et~al.(1983)Bahl, Jelinek, and Mercer]{bahl1983perplexity}
Lalit Bahl, Frederick Jelinek, and Robert Mercer.
\newblock A maximum likelihood approach to continuous speech recognition.
\newblock \emph{Pattern Analysis and Machine Intelligence, IEEE Transactions
  on}, PAMI-5:\penalty0 179 -- 190, 04 1983.
\newblock \doi{10.1109/TPAMI.1983.4767370}.

\bibitem[Bavarian et~al.(2022)Bavarian, Jun, Tezak, Schulman, McLeavey, Tworek,
  and Chen]{bavarian2022fim}
Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine
  McLeavey, Jerry Tworek, and Mark Chen.
\newblock Efficient training of language models to fill in the middle.
\newblock \emph{arXiv preprint arXiv:2207.14255}, 2022.
\newblock \doi{10.48550/ARXIV.2207.14255}.
\newblock URL \url{https://arxiv.org/abs/2207.14255}.

\bibitem[BBC(2023)]{bbc2023chatgpt_ban}
BBC.
\newblock {ChatGPT} accessible again in {Italy}.
\newblock \url{https://www.bbc.com/news/technology-65431914}, 2023.

\bibitem[Ben~Allal et~al.(2022)Ben~Allal, Muennighoff, Kumar~Umapathi, Lipkin,
  and Von~Werra]{bigcode-evaluation-harness}
Loubna Ben~Allal, Niklas Muennighoff, Logesh Kumar~Umapathi, Ben Lipkin, and
  Leandro Von~Werra.
\newblock A framework for the evaluation of code generation models.
\newblock \url{https://github.com/bigcode-project/bigcode-evaluation-harness},
  December 2022.

\bibitem[Ben~Allal et~al.(2023)Ben~Allal, Li, Kocetkov, Mou, Akiki, Ferrandis,
  Muennighoff, Mishra, Gu, Dey, Umapathi, Anderson, Zi, Poirier, Schoelkopf,
  Troshin, Abulkhanov, Romero, Lappert, De~Toni, del Río, Liu, Bose,
  Bhattacharyya, Zhuo, Yu, Villegas, Zocca, Mangrulkar, Lansky, Nguyen,
  Contractor, Villa, Li, Bahdanau, Jernite, Hughes, Fried, Guha, de~Vries, and
  von Werra]{allal2023santacoder}
Loubna Ben~Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki,
  Carlos~Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan
  Dey, Logesh~Kumar Umapathi, Carolyn~Jane Anderson, Yangtian Zi, Joel~Lamy
  Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero,
  Michael Lappert, Francesco De~Toni, Bernardo~García del Río, Qian Liu,
  Shamik Bose, Urvashi Bhattacharyya, Terry~Yue Zhuo, Ian Yu, Paulo Villegas,
  Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor,
  Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel
  Fried, Arjun Guha, Harm de~Vries, and Leandro von Werra.
\newblock {SantaCoder:} don't reach for the stars!
\newblock In \emph{Deep Learning for Code Workshop (DL4C)}, 2023.

\bibitem[Bengio et~al.(2000)Bengio, Ducharme, and Vincent]{bengio2000neural}
Yoshua Bengio, R\'{e}jean Ducharme, and Pascal Vincent.
\newblock A neural probabilistic language model.
\newblock In T.~Leen, T.~Dietterich, and V.~Tresp (eds.), \emph{Advances in
  Neural Information Processing Systems}, volume~13. MIT Press, 2000.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html}.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien,
  Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der
  Wal]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
  O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai
  Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der
  Wal.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock \emph{arXiv preprint arXiv:2304.01373}, 2023.

\bibitem[{BigScience Workshop}(2022)]{bigscience_workshop_2022}
{BigScience Workshop}.
\newblock {BLOOM} (revision 4ab0472), 2022.
\newblock URL \url{https://huggingface.co/bigscience/bloom}.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang,
  and Weinbach]{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
  USVSN~Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
  Wang, and Samuel Weinbach.
\newblock {GPT-NeoX-20B:} an open-source autoregressive language model.
\newblock \emph{arXiv preprint arXiv:2204.06745}, 2022.

\bibitem[Bloom(1970)]{bloom-filters-10.1145/362686.362692}
Burton~H. Bloom.
\newblock Space/time trade-offs in hash coding with allowable errors.
\newblock \emph{Commun. ACM}, 13\penalty0 (7):\penalty0 422–426, jul 1970.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/362686.362692}.
\newblock URL \url{https://doi.org/10.1145/362686.362692}.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill, Brynjolfsson, Buch, Card,
  Castellon, Chatterji, Chen, Creel, Davis, Demszky, Donahue, Doumbouya,
  Durmus, Ermon, Etchemendy, Ethayarajh, Fei{-}Fei, Finn, Gale, Gillespie,
  Goel, Goodman, Grossman, Guha, Hashimoto, Henderson, Hewitt, Ho, Hong, Hsu,
  Huang, Icard, Jain, Jurafsky, Kalluri, Karamcheti, Keeling, Khani, Khattab,
  Koh, Krass, Krishna, Kuditipudi, and et~al.]{stanford2022foundation}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,
  Niladri~S. Chatterji, Annie~S. Chen, Kathleen Creel, Jared~Quincy Davis,
  Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano
  Ermon, John Etchemendy, Kawin Ethayarajh, Li~Fei{-}Fei, Chelsea Finn, Trevor
  Gale, Lauren Gillespie, Karan Goel, Noah~D. Goodman, Shelby Grossman, Neel
  Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel~E. Ho, Jenny
  Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky,
  Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar
  Khattab, Pang~Wei Koh, Mark~S. Krass, Ranjay Krishna, Rohith Kuditipudi, and
  et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{CoRR}, abs/2108.07258, 2021.
\newblock URL \url{https://arxiv.org/abs/2108.07258}.

\bibitem[Brants et~al.(2007)Brants, Popat, Xu, Och, and Dean]{brants2007large}
Thorsten Brants, Ashok~C. Popat, Peng Xu, Franz~J. Och, and Jeffrey Dean.
\newblock Large language models in machine translation.
\newblock In \emph{Proceedings of the 2007 Joint Conference on Empirical
  Methods in Natural Language Processing and Computational Natural Language
  Learning ({EMNLP}-{C}o{NLL})}, pp.\  858--867, Prague, Czech Republic, June
  2007. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/D07-1090}.

\bibitem[Broder(2000)]{broder2000identifying}
Andrei~Z. Broder.
\newblock Identifying and filtering near-duplicate documents.
\newblock In \emph{Annual symposium on combinatorial pattern matching}, pp.\
  1--10. Springer, 2000.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Buck et~al.(2014)Buck, Heafield, and van Ooyen]{buck2014n}
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
\newblock {N}-gram counts and language models from the {C}ommon {C}rawl.
\newblock In \emph{Proceedings of the Ninth International Conference on
  Language Resources and Evaluation ({LREC}'14)}, pp.\  3579--3584, Reykjavik,
  Iceland, May 2014. European Language Resources Association (ELRA).
\newblock URL
  \url{http://www.lrec-conf.org/proceedings/lrec2014/pdf/1097_Paper.pdf}.

\bibitem[Butterick(2022)]{butterick2022copilot}
Matthew Butterick.
\newblock This {CoPilot} is stupid and wants to kill me.
\newblock
  \url{https://matthewbutterick.com/chron/this-copilot-is-stupid-and-wants-to-kill-me.html},
  2022.

\bibitem[Cassano et~al.(2023)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin,
  Pinckney, Yee, Zi, Anderson, Feldman, Guha, Greenberg, and
  Jangda]{cassano2022multiple}
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna
  Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane
  Anderson, Molly~Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.
\newblock {MultiPL-E:} a scalable and polyglot approach to benchmarking neural
  code generation.
\newblock \emph{IEEE Transactions on Software Engineering}, pp.\  1--17, 2023.
\newblock \doi{10.1109/TSE.2023.3267446}.
\newblock URL \url{https://arxiv.org/abs/2208.08227}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan,
  Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry,
  Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet,
  Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol,
  Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike,
  Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder,
  McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde
  de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
  Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy
  Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
  Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens
  Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias
  Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,
  William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
  Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
  Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
  Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
  Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
  Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur{-}Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier{-}Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur{-}Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier{-}Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah
  Fiedel.
\newblock {PaLM:} scaling language modeling with pathways.
\newblock \emph{CoRR}, abs/2204.02311, 2022.
\newblock \doi{10.48550/arXiv.2204.02311}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2204.02311}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with
  {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[{DOE 1 v. and GitHub, Inc.}(2022)]{DOE1vGitHub}
{DOE 1 v. and GitHub, Inc.}
\newblock \emph{4:22-cv-06823 N.D. Cal.}
\newblock 2022.

\bibitem[Eloundou et~al.(2023)Eloundou, Manning, Mishkin, and
  Rock]{eloundou2023gpts}
Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock.
\newblock {GPTs} are {GPTs}: An early look at the labor market impact potential
  of large language models.
\newblock \emph{arXiv preprint arXiv:2303.10130}, 2023.

\bibitem[Euronews(2023)]{euronews2023copilot}
Euronews.
\newblock Microsoft attracting users to its code-writing, generative {AI}
  software.
\newblock \url{https://www.euronews.com/next/2023/01/25/microsoft-results-ai},
  2023.

\bibitem[{European Council}(2018)]{eu2018gdpr}
{European Council}.
\newblock The general data protection regulation.
\newblock
  \url{https://www.consilium.europa.eu/en/policies/data-protection/data-protection-regulation/},
  2018.

\bibitem[Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu,
  Jiang, and Zhou]{feng2020codebert}
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun
  Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
\newblock Codebert: A pre-trained model for programming and natural languages.
\newblock \emph{arXiv preprint arXiv:2002.08155}, 2020.
\newblock \doi{10.48550/ARXIV.2002.08155}.
\newblock URL \url{https://arxiv.org/abs/2002.08155}.

\bibitem[Fried et~al.(2022)Fried, Aghajanyan, Lin, Wang, Wallace, Shi, Zhong,
  Yih, Zettlemoyer, and Lewis]{fried2022incoder}
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,
  Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
\newblock {InCoder:} a generative model for code infilling and synthesis.
\newblock \emph{arXiv preprint arXiv:2204.05999}, 2022.
\newblock \doi{10.48550/ARXIV.2204.05999}.
\newblock URL \url{https://arxiv.org/abs/2204.05999}.

\bibitem[Gao et~al.(2021{\natexlab{a}})Gao, Biderman, Black, Golding, Hoppe,
  Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
  and Connor Leahy.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2021{\natexlab{a}}.

\bibitem[Gao et~al.(2021{\natexlab{b}})Gao, Tow, Biderman, Black, DiPofi,
  Foster, Golding, Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite,
  Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
  and Andy Zou.
\newblock A framework for few-shot language model evaluation, September
  2021{\natexlab{b}}.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Gao et~al.(2022)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and
  Neubig]{gao2022pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie
  Callan, and Graham Neubig.
\newblock {PAL}: Program-aided language models.
\newblock \emph{arXiv preprint arXiv:2211.10435}, 2022.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman_realtoxicityprompts_2020}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
\newblock {RealToxicityPrompts}: {Evaluating} {Neural} {Toxic} {Degeneration}
  in {Language} {Models}.
\newblock In \emph{Findings of the {Association} for {Computational}
  {Linguistics}: {EMNLP} 2020}, pp.\  3356--3369, Online, November 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.301}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.301}.

\bibitem[Heafield et~al.(2013)Heafield, Pouzyrevsky, Clark, and
  Koehn]{heafield2013scalable}
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan~H. Clark, and Philipp Koehn.
\newblock Scalable modified {Kneser-Ney} language model estimation.
\newblock In \emph{Proceedings of the 51st Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pp.\  690--696,
  Sofia, Bulgaria, August 2013. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/P13-2121}.

\bibitem[Henderson et~al.(2023)Henderson, Li, Jurafsky, Hashimoto, Lemley, and
  Liang]{henderson2023foundation}
Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark~A Lemley,
  and Percy Liang.
\newblock Foundation models and fair use.
\newblock \emph{arXiv preprint arXiv:2303.15715}, 2023.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2020mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hindle et~al.(2012)Hindle, Barr, Su, Gabel, and
  Devanbu]{hindle2012naturalness}
Abram Hindle, Earl~T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
\newblock On the naturalness of software.
\newblock In \emph{2012 34th International Conference on Software Engineering
  (ICSE)}, pp.\  837--847. IEEE, 2012.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland,
  Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae,
  Vinyals, and Sifre]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den
  Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
  Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2020curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[Husain et~al.(2019)Husain, Wu, Gazit, Allamanis, and
  Brockschmidt]{husain2019codesearchnet}
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
  Brockschmidt.
\newblock {CodeSearchNet} challenge: Evaluating the state of semantic code
  search.
\newblock \emph{arXiv preprint arXiv:1909.09436}, 2019.

\bibitem[Hutchinson et~al.(2020)Hutchinson, Prabhakaran, Denton, Webster,
  Zhong, and Denuyl]{hutchinson_social_2020}
Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu~Zhong,
  and Stephen Denuyl.
\newblock Social biases in {NLP} models as barriers for persons with
  disabilities.
\newblock In \emph{Proceedings of the 58th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics}}, pp.\  5491--5501, Online,
  July 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.487}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.487}.

\bibitem[Jozefowicz et~al.(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and
  Wu]{jozefowicz2016exploring}
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
\newblock Exploring the limits of language modeling.
\newblock \emph{arXiv preprint arXiv:1602.02410}, 2016.

\bibitem[Kanade et~al.(2020)Kanade, Maniatis, Balakrishnan, and
  Shi]{kanade2020embeddings}
Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi.
\newblock Learning and evaluating contextual embedding of source code.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Khlaaf et~al.(2022)Khlaaf, Mishkin, Achiam, Krueger, and
  Brundage]{khlaaf2022hazard}
Heidy Khlaaf, Pamela Mishkin, Joshua Achiam, Gretchen Krueger, and Miles
  Brundage.
\newblock A hazard analysis framework for code synthesis large language models.
\newblock \emph{arXiv preprint arXiv:2207.14157}, 2022.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{DBLP:journals/corr/KingmaB14}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Kocetkov et~al.(2022)Kocetkov, Li, Ben~Allal, Li, Mou,
  Muñoz~Ferrandis, Jernite, Mitchell, Hughes, Wolf, Bahdanau, von Werra, and
  de~Vries]{Kocetkov2022TheStack}
Denis Kocetkov, Raymond Li, Loubna Ben~Allal, Jia Li, Chenghao Mou, Carlos
  Muñoz~Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas
  Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de~Vries.
\newblock The {S}tack: 3 {TB} of permissively licensed source code.
\newblock \emph{Preprint}, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.15533}.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2023large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{arXiv preprint arXiv:2205.11916}, 2022.

\bibitem[Kuhn(2022)]{kuhn2022copilot}
Bradley~M. Kuhn.
\newblock If software is my copilot, who programmed my software?
\newblock
  \url{https://sfconservancy.org/blog/2022/feb/03/github-copilot-copyleft-gpl/},
  2022.

\bibitem[Kurita et~al.(2019)Kurita, Vyas, Pareek, Black, and
  Tsvetkov]{kurita_measuring_2019}
Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan~W Black, and Yulia Tsvetkov.
\newblock Measuring bias in contextualized word representations.
\newblock In \emph{Proceedings of the {First} {Workshop} on {Gender} {Bias} in
  {Natural} {Language} {Processing}}, pp.\  166--172, Florence, Italy, August
  2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-3823}.
\newblock URL \url{https://www.aclweb.org/anthology/W19-3823}.

\bibitem[Lacoste et~al.(2019)Lacoste, Luccioni, Schmidt, and
  Dandres]{lacoste2019quantifying}
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.
\newblock Quantifying the carbon emissions of machine learning.
\newblock \emph{arXiv preprint arXiv:1910.09700}, 2019.

\bibitem[Lai et~al.(2022)Lai, Li, Wang, Zhang, Zhong, Zettlemoyer, tau Yih,
  Fried, Wang, and Yu]{Lai2022DS1000}
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke
  Zettlemoyer, Scott~Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
\newblock {DS-1000:} a natural and reliable benchmark for data science code
  generation.
\newblock \emph{ArXiv}, abs/2211.11501, 2022.

\bibitem[Lee(2013)]{lee2013pseudo}
Dong-Hyun Lee.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{Workshop on challenges in representation learning, ICML},
  number~2, pp.\  896, 2013.

\bibitem[Leinonen et~al.(2023)Leinonen, Denny, MacNeil, Sarsa, Bernstein, Kim,
  Tran, and Hellas]{leinonen2023comparing}
Juho Leinonen, Paul Denny, Stephen MacNeil, Sami Sarsa, Seth Bernstein, Joanne
  Kim, Andrew Tran, and Arto Hellas.
\newblock Comparing code explanations created by students and large language
  models, 2023.

\bibitem[Lemley \& Casey(2020)Lemley and Casey]{lemley2020fair}
Mark~A Lemley and Bryan Casey.
\newblock Fair learning.
\newblock \emph{Tex. L. Rev.}, 99:\penalty0 743, 2020.
\newblock URL \url{https://texaslawreview.org/fair-learning/}.

\bibitem[Levendowski(2018)]{levendowski2018copyright}
Amanda Levendowski.
\newblock How copyright law can fix artificial intelligence's implicit bias
  problem.
\newblock \emph{Wash. L. Rev.}, 93:\penalty0 579, 2018.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond,
  Eccles, Keeling, Gimeno, Dal~Lago, Hubert, Choy, de~Masson~d'Autume,
  Babuschkin, Chen, Huang, Welbl, Gowal, Cherepanov, Molloy, Mankowitz,
  Sutherland~Robson, Kohli, de~Freitas, Kavukcuoglu, and
  Vinyals]{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago,
  Thomas Hubert, Peter Choy, Cyprien de~Masson~d'Autume, Igor Babuschkin,
  Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov,
  James Molloy, Daniel Mankowitz, Esme Sutherland~Robson, Pushmeet Kohli, Nando
  de~Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
\newblock Competition-level code generation with alphacode.
\newblock \emph{arXiv preprint arXiv:2203.07814}, 2022.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022helm}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2211.09110}, 2022.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa:} a robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Lomas(2022)]{lomas2023gdpr_llms}
Natasha Lomas.
\newblock Unpicking the rules shaping generative {AI}.
\newblock
  \url{https://techcrunch.com/2023/04/13/generative-ai-gdpr-enforcement/},
  2022.

\bibitem[Lu et~al.(2021)Lu, Guo, Ren, Huang, Svyatkovskiy, Blanco, Clement,
  Drain, Jiang, Tang, Li, Zhou, Shou, Zhou, Tufano, Gong, Zhou, Duan,
  Sundaresan, Deng, Fu, and Liu]{lu2021codexglue}
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
  Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge~Li, Lidong
  Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan,
  Neel Sundaresan, Shao~Kun Deng, Shengyu Fu, and Shujie Liu.
\newblock {CodeXGLUE}: A machine learning benchmark dataset for code
  understanding and generation.
\newblock \emph{arXiv preprint arXiv:2102.04664}, 2021.

\bibitem[Marone \& {Van Durme}(2023)Marone and {Van
  Durme}]{marone-data-portraits-search}
Marc Marone and Benjamin {Van Durme}.
\newblock Data portraits: Recording foundation model training data.
\newblock \emph{CoRR}, abs/2303.03919, 2023.
\newblock \doi{10.48550/arXiv.2303.03919}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.03919}.

\bibitem[May et~al.(2019)May, Wang, Bordia, Bowman, and
  Rudinger]{may_measuring_2019}
Chandler May, Alex Wang, Shikha Bordia, Samuel~R. Bowman, and Rachel Rudinger.
\newblock On measuring social biases in sentence encoders.
\newblock In \emph{Proceedings of the 2019 {Conference} of the {North}
  {American} {Chapter} of the {Association} for {Computational} {Linguistics}:
  {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
  pp.\  622--628, Minneapolis, Minnesota, June 2019. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1063}.
\newblock URL \url{https://www.aclweb.org/anthology/N19-1063}.

\bibitem[Meade et~al.(2022)Meade, Poole-Dayan, and Reddy]{meade_empirical_2022}
Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy.
\newblock An empirical survey of the effectiveness of debiasing techniques for
  pre-trained language models.
\newblock In \emph{Proceedings of the 60th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics} ({Volume} 1: {Long}
  {Papers})}, pp.\  1878--1898, Dublin, Ireland, May 2022. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.132}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.132}.

\bibitem[Meade et~al.(2023)Meade, Gella, Hazarika, Gupta, Jin, Reddy, Liu, and
  Hakkani-Tür]{meade_using_2023}
Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di~Jin, Siva
  Reddy, Yang Liu, and Dilek Hakkani-Tür.
\newblock Using in-context learning to improve dialogue safety, February 2023.
\newblock URL \url{http://arxiv.org/abs/2302.00871}.
\newblock arXiv:2302.00871 [cs].

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'{a}}t, Burget, Cernock{\'{y}},
  and Khudanpur]{mikolov2010recurrent}
Tom{\'{a}}s Mikolov, Martin Karafi{\'{a}}t, Luk{\'{a}}s Burget, Jan
  Cernock{\'{y}}, and Sanjeev Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura (eds.),
  \emph{{INTERSPEECH} 2010, 11th Annual Conference of the International Speech
  Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010},
  pp.\  1045--1048. {ISCA}, 2010.
\newblock URL
  \url{http://www.isca-speech.org/archive/interspeech\_2010/i10\_1045.html}.

\bibitem[Mitchell et~al.(2019)Mitchell, Wu, Zaldivar, Barnes, Vasserman,
  Hutchinson, Spitzer, Raji, and Gebru]{mitchell2019modelcard}
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
  Ben Hutchinson, Elena Spitzer, Inioluwa~Deborah Raji, and Timnit Gebru.
\newblock Model cards for model reporting.
\newblock In danah boyd and Jamie~H. Morgenstern (eds.), \emph{Proceedings of
  the Conference on Fairness, Accountability, and Transparency, FAT* 2019,
  Atlanta, GA, USA, January 29-31, 2019}, pp.\  220--229. {ACM}, 2019.
\newblock \doi{10.1145/3287560.3287596}.
\newblock URL \url{https://doi.org/10.1145/3287560.3287596}.

\bibitem[Mitchell et~al.(2022)Mitchell, Luccioni, Lambert, Gerchick,
  McMillan{-}Major, Ozoani, Rajani, Thrush, Jernite, and
  Kiela]{mitchell-measuring-data}
Margaret Mitchell, Alexandra~Sasha Luccioni, Nathan Lambert, Marissa Gerchick,
  Angelina McMillan{-}Major, Ezinwanne Ozoani, Nazneen Rajani, Tristan Thrush,
  Yacine Jernite, and Douwe Kiela.
\newblock Measuring data.
\newblock \emph{CoRR}, abs/2212.05129, 2022.
\newblock \doi{10.48550/arXiv.2212.05129}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2212.05129}.

\bibitem[MOI et~al.(2022)MOI, Patry, Cistac, Pete, Morgan, Pütz, Mishig,
  Johansen, Wolf, Gugger, Clement, Chaumond, Debut, Garillot, Georges, dctelus,
  Louis, MarcusGrass, Peyash, 0xflotus, deLevie, Mamaev, Arthur, Cameron,
  Clement, Moges, Hewitt, Zolotukhin, and
  Thomas]{anthony_moi_2022_hftokenizers}
Anthony MOI, Nicolas Patry, Pierric Cistac, Pete, Funtowicz Morgan, Sebastian
  Pütz, Mishig, Bjarte Johansen, Thomas Wolf, Sylvain Gugger, Clement, Julien
  Chaumond, Lysandre Debut, François Garillot, Luc Georges, dctelus, JC~Louis,
  MarcusGrass, Taufiquzzaman Peyash, 0xflotus, Alan deLevie, Alexander Mamaev,
  Arthur, Cameron, Colin Clement, Dagmawi Moges, David Hewitt, Denis
  Zolotukhin, and Geoffrey Thomas.
\newblock huggingface/tokenizers: Rust 0.13.2, November 2022.
\newblock URL \url{https://doi.org/10.5281/zenodo.7298413}.

\bibitem[Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts,
  Biderman, Scao, Bari, Shen, Yong, Schoelkopf, Tang, Radev, Aji, Almubarak,
  Albanie, Alyafeai, Webson, Raff, and Raffel]{muennighoff2022crosslingual}
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella
  Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
  Schoelkopf, Xiangru Tang, Dragomir Radev, Alham~Fikri Aji, Khalid Almubarak,
  Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel.
\newblock Crosslingual generalization through multitask finetuning.
\newblock \emph{arXiv preprint arXiv:2211.01786}, 2022.

\bibitem[Nadeem et~al.(2021)Nadeem, Bethke, and Reddy]{nadeem_stereoset_2021}
Moin Nadeem, Anna Bethke, and Siva Reddy.
\newblock {StereoSet}: {Measuring} stereotypical bias in pretrained language
  models.
\newblock In \emph{Proceedings of the 59th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics} and the 11th {International}
  {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long}
  {Papers})}, pp.\  5356--5371, Online, August 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.416}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.416}.

\bibitem[Nangia et~al.(2020)Nangia, Vania, Bhalerao, and
  Bowman]{nangia_crows-pairs_2020}
Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel~R. Bowman.
\newblock {CrowS}-{Pairs:} a challenge dataset for measuring social biases in
  masked language models.
\newblock \emph{arXiv:2010.00133 [cs]}, September 2020.
\newblock URL \url{http://arxiv.org/abs/2010.00133}.
\newblock arXiv: 2010.00133.

\bibitem[Nijkamp et~al.(2023)Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese,
  and Xiong]{nijkamp:codegen}
Erik Nijkamp, Bo~Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio
  Savarese, and Caiming Xiong.
\newblock {CodeGen:} an open large language model for code with multi-turn
  program synthesis.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=iaYcJKpY2B_}.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez,
  Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
  Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
  Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
  Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem[OpenAI(2023{\natexlab{a}})]{openai2023gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2023{\natexlab{a}}.

\bibitem[OpenAI(2023{\natexlab{b}})]{openai2023systemcard}
OpenAI.
\newblock {GPT-4} system card.
\newblock \url{https://cdn.openai.com/papers/gpt-4-system-card.pdf},
  2023{\natexlab{b}}.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock {B}leu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  311--318, Philadelphia, Pennsylvania,
  USA, July 2002. Association for Computational Linguistics.
\newblock \doi{10.3115/1073083.1073135}.
\newblock URL \url{https://aclanthology.org/P02-1040}.

\bibitem[Pearce et~al.(2022)Pearce, Ahmad, Tan, Dolan-Gavitt, and
  Karri]{pearce2022copilotsec}
Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh
  Karri.
\newblock Asleep at the keyboard? {Assessing} the security of {GitHub
  Copilot}'s code contributions.
\newblock In \emph{IEEE Symposium on Security and Privacy}, San Francisco, CA,
  2022.
\newblock URL \url{https://arxiv.org/abs/2108.09293}.

\bibitem[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese,
  McAleese, and Irving]{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John
  Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock \emph{arXiv preprint arXiv:2202.03286}, 2022.

\bibitem[Piktus et~al.(2023)Piktus, Akiki, Villegas, Lauren{\c{c}}on, Dupont,
  Luccioni, Jernite, and Rogers]{piktus-roots-search}
Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Lauren{\c{c}}on,
  G{\'{e}}rard Dupont, Alexandra~Sasha Luccioni, Yacine Jernite, and Anna
  Rogers.
\newblock The {ROOTS} search tool: Data transparency for {LLMs}.
\newblock \emph{CoRR}, abs/2302.14035, 2023.
\newblock \doi{10.48550/arXiv.2302.14035}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.14035}.

\bibitem[Pradel et~al.(2020)Pradel, Gousios, Liu, and
  Chandra]{pradel:typewriter}
Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra.
\newblock {{TypeWriter}}: {{Neural Type Prediction}} with {{Search-Based
  Validation}}.
\newblock In \emph{ACM Joint Meeting on European Software Engineering
  Conference and Symposium on the Foundations of Software Engineering}, 2020.
\newblock \doi{10.1145/3368089.3409715}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri,
  Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy,
  Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart,
  Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis,
  Kavukcuoglu, and Irving]{rae2021scaling}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
  Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
  Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
  de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
  Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris Jones, James
  Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel,
  William Isaac, Ed~Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
  Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray
  Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis \& insights from training
  {Gopher}.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Reddy et~al.(2019)Reddy, Chen, and Manning]{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D. Manning.
\newblock {CoQA}: A conversational question answering challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 249--266, 2019.
\newblock \doi{10.1162/tacl_a_00266}.
\newblock URL \url{https://aclanthology.org/Q19-1016}.

\bibitem[Rothchild \& Rothchild(2022)Rothchild and
  Rothchild]{rothchild2022copyright}
John~A. Rothchild and Daniel Rothchild.
\newblock Copyright implications of the use of code repositories to train a
  machine learning model.
\newblock
  \url{https://www.fsf.org/licensing/copilot/copyright-implications-of-the-use-of-code-repositories-to-train-a-machine-learning-model},
  2022.

\bibitem[Sandoval et~al.(2023)Sandoval, Pearce, Nys, Karri, Garg, and
  Dolan-Gavitt]{sandoval2023lost}
Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, and
  Brendan Dolan-Gavitt.
\newblock Lost at {C}: A user study on the security implications of large
  language model code assistants, 2023.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow,
  Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock {BLOOM:} a {176B}-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Shazeer(2019)]{shazeer2019mqa}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{CoRR}, abs/1911.02150, 2019.
\newblock URL \url{http://arxiv.org/abs/1911.02150}.

\bibitem[Smith(2016)]{smith2016bigquery}
Arfon Smith.
\newblock Kernel description.
\newblock
  \url{https://github.blog/2016-06-29-making-open-source-data-more-available/},
  2016.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zhang, Child, Aminabadi,
  Bernauer, Song, Shoeybi, He, Houston, Tiwary, and Catanzaro]{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, Elton Zhang, Rewon Child, Reza~Yazdani Aminabadi, Julie
  Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh
  Tiwary, and Bryan Catanzaro.
\newblock Using {DeepSpeed} and {Megatron} to train {Megatron-Turing NLG 530B},
  a large-scale generative language model.
\newblock \emph{arXiv preprint arXiv:2201.11990}, 2022.

\bibitem[Solaiman(2023)]{solaiman2023gradient}
Irene Solaiman.
\newblock The gradient of generative {AI} release: Methods and considerations.
\newblock \emph{arXiv preprint arXiv:2302.04844}, 2023.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Tran, Garcia, Bahri, Schuster, Zheng,
  Houlsby, and Metzler]{tay2022unifying}
Yi~Tay, Mostafa Dehghani, Vinh~Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster,
  Huaixiu~Steven Zheng, Neil Houlsby, and Donald Metzler.
\newblock Unifying language learning paradigms.
\newblock \emph{arXiv preprint arXiv:2205.05131}, 2022.

\bibitem[Thompson(2022)]{thompson-2022-copilot-stats}
Clive Thompson.
\newblock How an ai became my code-writing genie, Mar 2022.
\newblock URL
  \url{https://www.wired.com/story/openai-copilot-autocomplete-for-code/}.

\bibitem[Thoppilan et~al.(2022)Thoppilan, Freitas, Hall, Shazeer, Kulshreshtha,
  Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali, Huang,
  Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhao, Zhou, Chang,
  Krivokon, Rusch, Pickett, Srinivasan, Man, Meier-Hellstern, Morris, Doshi,
  Santos, Duke, Soraker, Zevenbergen, Prabhakaran, Diaz, Hutchinson, Olson,
  Molina, Hoffman-John, Lee, Aroyo, Rajakumar, Butryna, Lamm, Kuzmina, Fenton,
  Cohen, Bernstein, Kurzweil, Aguera-Arcas, Cui, Croak, Chi, and
  Le]{thoppilan2022lamda}
Romal Thoppilan, Daniel~De Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo
  Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
  Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao,
  Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,
  Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith~Ringel
  Morris, Tulsee Doshi, Renelito~Delos Santos, Toju Duke, Johnny Soraker, Ben
  Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen
  Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi
  Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron
  Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,
  Marian Croak, Ed~Chi, and Quoc Le.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Togelius \& Yannakakis(2023)Togelius and
  Yannakakis]{togelius2023choose}
Julian Togelius and Georgios~N. Yannakakis.
\newblock Choose your weapon: Survival strategies for depressed {AI} academics.
\newblock \emph{arXiv preprint arXiv:2304.06035}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample.
\newblock {LLaMA:} open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[Vidgen et~al.(2021)Vidgen, Thrush, Waseem, and
  Kiela]{vidgen_learning_2021}
Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela.
\newblock Learning from the worst: Dynamically generated datasets to improve
  online hate detection.
\newblock In \emph{Proceedings of the 59th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics} and the 11th {International}
  {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long}
  {Papers})}, pp.\  1667--1682, Online, August 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.132}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.132}.

\bibitem[Wan et~al.(2023)Wan, Wallace, Shen, and Klein]{wan2023poisoning}
Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein.
\newblock Poisoning language models during instruction tuning, 2023.

\bibitem[Wang \& Komatsuzaki(2021)Wang and Komatsuzaki]{wang2021gpt}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B:} a 6 billion parameter autoregressive language model,
  2021.

\bibitem[Wang et~al.(2021)Wang, Wang, Joty, and Hoi]{wang-etal-2021-codet5}
Yue Wang, Weishi Wang, Shafiq Joty, and Steven~C.H. Hoi.
\newblock {C}ode{T}5: Identifier-aware unified pre-trained encoder-decoder
  models for code understanding and generation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  8696--8708, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.685}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.685}.

\bibitem[Wang et~al.(2022)Wang, Zhou, Fried, and Neubig]{wang2022execution}
Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig.
\newblock Execution-based evaluation for open-domain code generation.
\newblock \emph{arXiv preprint arXiv:2212.10481}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi,
  Le, and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
  Ed~H. Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_VjQlMeSB_J}.

\bibitem[Woelfle et~al.(2011)Woelfle, Olliaro, and Todd]{open-science}
Michael Woelfle, Piero Olliaro, and Matthew~H. Todd.
\newblock Open science is a research accelerator.
\newblock \emph{Nature Chemistry}, 3\penalty0 (10):\penalty0 745--748, October
  2011.
\newblock ISSN 1755-4349.
\newblock \doi{10.1038/nchem.1149}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[{World Economic Forum}(2023)]{wef2023futureofjobs}
{World Economic Forum}.
\newblock Future of jobs report.
\newblock \url{https://www3.weforum.org/docs/WEF_Future_of_Jobs_2023.pdf},
  2023.

\bibitem[Xu et~al.(2022)Xu, Alon, Neubig, and
  Hellendoorn]{xu2022systematicevaluation}
Frank~F. Xu, Uri Alon, Graham Neubig, and Vincent~Josua Hellendoorn.
\newblock A systematic evaluation of large language models of code.
\newblock In \emph{Proceedings of the 6th ACM SIGPLAN International Symposium
  on Machine Programming}, MAPS 2022, pp.\  1–10, New York, NY, USA, 2022.
  Association for Computing Machinery.
\newblock ISBN 9781450392730.
\newblock \doi{10.1145/3520312.3534862}.
\newblock URL \url{https://doi.org/10.1145/3520312.3534862}.

\bibitem[Yee \& Guha(2023)Yee and Guha]{yee:typeweaver}
Ming-Ho Yee and Arjun Guha.
\newblock Do machine learning models produce {TypeScript} types that type
  check?
\newblock In \emph{European Conference on Object-Oriented Programming (ECOOP)},
  2023.

\bibitem[Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia,
  Tam, Ma, Xue, Zhai, Chen, Zhang, Dong, and Tang]{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue,
  Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang.
\newblock {GLM-130B:} an open bilingual pre-trained model.
\newblock \emph{arXiv preprint arXiv:2210.02414}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock {OPT:} open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zheng et~al.(2023)Zheng, Xia, Zou, Dong, Wang, Xue, Wang, Shen, Wang,
  Li, Su, Yang, and Tang]{qinkai:codegeex}
Qinkai Zheng, Xiao Xia, Xu~Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang,
  Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang.
\newblock {CodeGeeX}: A pre-trained model for code generation with multilingual
  evaluations on {HumanEval-X}.
\newblock \emph{arXiv preprint arXiv:2303.17568}, 2023.
\newblock \doi{10.48550/arXiv.2303.17568}.

\bibitem[Zhou et~al.(2022)Zhou, Schärli, Hou, Wei, Scales, Wang, Schuurmans,
  Cui, Bousquet, Le, and Chi]{zhou2023leasttomost}
Denny Zhou, Nathanael Schärli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang,
  Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed~Chi.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2205.10625}, 2022.

\end{thebibliography}
