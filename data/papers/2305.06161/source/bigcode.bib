@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = 2019,
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@article{liu2019roberta,
  title={{RoBERTa:} A Robustly Optimized {BERT} Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{gao2021simcse,
  title={Simcse: Simple contrastive learning of sentence embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  journal={arXiv preprint arXiv:2104.08821},
  year={2021}
}

@InProceedings{radford2021learning,  
title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},  
booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},  
year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},  
volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},  month = 	 {18--24 Jul},
  publisher =    {PMLR},  
pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html}
}

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year={2021},
  eprint={2107.03374},
  journal={arXiv preprint},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@inproceedings{nijkamp:codegen,
title={{CodeGen:} An Open Large Language Model for Code with Multi-Turn Program Synthesis},
author={Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=iaYcJKpY2B_}
}
@article{qinkai:codegeex,
  title = {{CodeGeeX}: A Pre-Trained Model for Code Generation with Multilingual Evaluations on {HumanEval-X}},
  author = {Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and Su, Teng and Yang, Zhilin and Tang, Jie},
  year = {2023},
  journal={arXiv preprint arXiv:2303.17568},
  doi = {10.48550/arXiv.2303.17568}
}


@article{lu2021codexglue,
  title={{CodeXGLUE}: A machine learning benchmark dataset for code understanding and generation},
  author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},
  journal={arXiv preprint arXiv:2102.04664},
  year={2021}
}

@article{scao2022bloom,
  title={{BLOOM:} A {176B}-Parameter Open-Access Multilingual Language Model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{zhang2022opt,
  title={{OPT:} Open pre-trained transformer language models},
  author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{zeng2022glm,
  title={{GLM-130B:} An Open Bilingual Pre-trained Model},
  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Peng Zhang and Yuxiao Dong and Jie Tang},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}

@article{fried2022incoder,
  doi = {10.48550/ARXIV.2204.05999},
  journal={arXiv preprint arXiv:2204.05999},
  
  url = {https://arxiv.org/abs/2204.05999},
  
  author = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  
  keywords = {Software Engineering (cs.SE), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{InCoder:} A Generative Model for Code Infilling and Synthesis},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}
@article{christopolou2022pangucoder,
  doi = {10.48550/ARXIV.2207.11280},
  journal={arXiv preprint arXiv:2207.11280},
  
  url = {https://arxiv.org/abs/2207.11280},
  
  author = {Christopoulou, Fenia and Lampouras, Gerasimos and Gritta, Milan and Zhang, Guchun and Guo, Yinpeng and Li, Zhongqi and Zhang, Qi and Xiao, Meng and Shen, Bo and Li, Lin and Yu, Hao and Yan, Li and Zhou, Pingyi and Wang, Xin and Ma, Yuchi and Iacobacci, Ignacio and Wang, Yasheng and Liang, Guangtai and Wei, Jiansheng and Jiang, Xin and Wang, Qianxiang and Liu, Qun},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Programming Languages (cs.PL), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{PanGu-Coder:} Program Synthesis with Function-Level Language Modeling},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{li2022competition,
  title={Competition-Level Code Generation with AlphaCode},
    author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and
    Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and
    Keeling, James and Gimeno, Felix and Dal Lago, Agustin and
    Hubert, Thomas and Choy, Peter and de Masson d'Autume, Cyprien and
    Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and
    Gowal, Sven and Cherepanov, Alexey and Molloy, James and
    Mankowitz, Daniel and Sutherland Robson, Esme and Kohli, Pushmeet and
    de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}
@article{stanford2022foundation,
  author    = {Rishi Bommasani and
               Drew A. Hudson and
               Ehsan Adeli and
               Russ Altman and
               Simran Arora and
               Sydney von Arx and
               Michael S. Bernstein and
               Jeannette Bohg and
               Antoine Bosselut and
               Emma Brunskill and
               Erik Brynjolfsson and
               Shyamal Buch and
               Dallas Card and
               Rodrigo Castellon and
               Niladri S. Chatterji and
               Annie S. Chen and
               Kathleen Creel and
               Jared Quincy Davis and
               Dorottya Demszky and
               Chris Donahue and
               Moussa Doumbouya and
               Esin Durmus and
               Stefano Ermon and
               John Etchemendy and
               Kawin Ethayarajh and
               Li Fei{-}Fei and
               Chelsea Finn and
               Trevor Gale and
               Lauren Gillespie and
               Karan Goel and
               Noah D. Goodman and
               Shelby Grossman and
               Neel Guha and
               Tatsunori Hashimoto and
               Peter Henderson and
               John Hewitt and
               Daniel E. Ho and
               Jenny Hong and
               Kyle Hsu and
               Jing Huang and
               Thomas Icard and
               Saahil Jain and
               Dan Jurafsky and
               Pratyusha Kalluri and
               Siddharth Karamcheti and
               Geoff Keeling and
               Fereshte Khani and
               Omar Khattab and
               Pang Wei Koh and
               Mark S. Krass and
               Ranjay Krishna and
               Rohith Kuditipudi and
               et al.},
  title     = {On the Opportunities and Risks of Foundation Models},
  journal   = {CoRR},
  volume    = {abs/2108.07258},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.07258},
  eprinttype = {arXiv},
  eprint    = {2108.07258},
  timestamp = {Tue, 04 Jan 2022 14:40:20 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-07258.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{brown2020language,
  added-at = {2020-07-28T16:09:05.000+0200},
  author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  biburl = {https://www.bibsonomy.org/bibtex/27a2a9aee490ff30dd5b4d0470a8be8d8/albinzehe},
  interhash = {c02cbc3bfa91c08710d0db948c927dad},
  intrahash = {7a2a9aee490ff30dd5b4d0470a8be8d8},
  journal = {arXiv preprint arXiv:2005.14165},
  keywords = {gpt-3 kallimachos languagemodels proposal-knowledge transformer},
  timestamp = {2020-07-28T16:09:05.000+0200},
  title = {Language models are few-shot learners},
  year = 2020
}


@inproceedings{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{chowdhery2022palm,
  author    = {Aakanksha Chowdhery and
               Sharan Narang and
               Jacob Devlin and
               Maarten Bosma and
               Gaurav Mishra and
               Adam Roberts and
               Paul Barham and
               Hyung Won Chung and
               Charles Sutton and
               Sebastian Gehrmann and
               Parker Schuh and
               Kensen Shi and
               Sasha Tsvyashchenko and
               Joshua Maynez and
               Abhishek Rao and
               Parker Barnes and
               Yi Tay and
               Noam Shazeer and
               Vinodkumar Prabhakaran and
               Emily Reif and
               Nan Du and
               Ben Hutchinson and
               Reiner Pope and
               James Bradbury and
               Jacob Austin and
               Michael Isard and
               Guy Gur{-}Ari and
               Pengcheng Yin and
               Toju Duke and
               Anselm Levskaya and
               Sanjay Ghemawat and
               Sunipa Dev and
               Henryk Michalewski and
               Xavier Garcia and
               Vedant Misra and
               Kevin Robinson and
               Liam Fedus and
               Denny Zhou and
               Daphne Ippolito and
               David Luan and
               Hyeontaek Lim and
               Barret Zoph and
               Alexander Spiridonov and
               Ryan Sepassi and
               David Dohan and
               Shivani Agrawal and
               Mark Omernick and
               Andrew M. Dai and
               Thanumalayan Sankaranarayana Pillai and
               Marie Pellat and
               Aitor Lewkowycz and
               Erica Moreira and
               Rewon Child and
               Oleksandr Polozov and
               Katherine Lee and
               Zongwei Zhou and
               Xuezhi Wang and
               Brennan Saeta and
               Mark Diaz and
               Orhan Firat and
               Michele Catasta and
               Jason Wei and
               Kathy Meier{-}Hellstern and
               Douglas Eck and
               Jeff Dean and
               Slav Petrov and
               Noah Fiedel},
  title     = {{PaLM:} Scaling Language Modeling with Pathways},
  journal   = {CoRR},
  volume    = {abs/2204.02311},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2204.02311},
  doi       = {10.48550/arXiv.2204.02311},
  eprinttype = {arXiv},
  eprint    = {2204.02311},
  timestamp = {Tue, 16 Aug 2022 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2204-02311.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{jernite2022datagovernance,
author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and Dupont, Gerard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Radev, Dragomir and Gokaslan, Aaron and Nikpoor, Somaieh and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
title = {Data Governance in the Age of Large-Scale Data-Driven Language Technology},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534637},
doi = {10.1145/3531146.3534637},
abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2206–2222},
numpages = {17},
keywords = {technology governance, language data, datasets, data rights},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}
@inproceedings{izadi2022codefill,
author = {Izadi, Maliheh and Gismondi, Roberta and Gousios, Georgios},
title = {{CodeFill:} Multi-Token Code Completion by Jointly Learning from Structure and Naming Sequences},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510172},
doi = {10.1145/3510003.3510172},
abstract = {Code completion is an essential feature of IDEs, yet current auto-completers are restricted to either grammar-based or NLP-based single token completions. Both approaches have significant drawbacks: grammar-based autocompletion is restricted in dynamically-typed language environments, whereas NLP-based autocompleters struggle to understand the semantics of the programming language and the developer's code context.In this work, we present CodeFill, a language model for autocompletion that combines learned structure and naming information. Using a parallel Transformer architecture and multi-task learning, CodeFill consumes sequences of source code token names and their equivalent AST token types. Uniquely, CodeFill is trained both for single-token and multi-token (statement) prediction, which enables it to learn long-range dependencies among grammatical and naming elements. We train CodeFill on two datasets, consisting of 29M and 425M lines of code, respectively. To make the evaluation more realistic, we develop a method to automatically infer points in the source code at which completion matters. We compare CodeFill against four baselines and two state-of-the-art models, GPT-C and TravTrans+. CodeFill surpasses all baselines in single token prediction (MRR: 70.9% vs. 66.2% and 67.8%) and outperforms the state of the art for multi-token prediction (ROUGE-L: 63.7% vs. 52.4% and 59.2%, for n = 4 tokens). We publicly release our source code and datasets.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {401–412},
numpages = {12},
keywords = {transformers, types, dynamically-typed languages, multi-task learning, automatic code completion},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}
@inproceedings{xu2022systematicevaluation,
author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
title = {A Systematic Evaluation of Large Language Models of Code},
year = {2022},
isbn = {9781450392730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520312.3534862},
doi = {10.1145/3520312.3534862},
abstract = {Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.},
booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
pages = {1–10},
numpages = {10},
keywords = {code generation, evaluation, pretraining, code language model, open-source},
location = {San Diego, CA, USA},
series = {MAPS 2022}
}
@inproceedings{ahmad-etal-2021-unified,
    title = "Unified Pre-training for Program Understanding and Generation",
    author = "Ahmad, Wasi  and
      Chakraborty, Saikat  and
      Ray, Baishakhi  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.211",
    pages = "2655--2668"
}
@inproceedings{wang-etal-2021-codet5,
    title = "{C}ode{T}5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
    author = "Wang, Yue  and
      Wang, Weishi  and
      Joty, Shafiq  and
      Hoi, Steven C.H.",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.685",
    doi = "10.18653/v1/2021.emnlp-main.685",
    pages = "8696--8708",
    abstract = "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.",
}
@article{roziere2021dobf,
  title={DOBF: A Deobfuscation Pre-Training Objective for Programming Languages},
  author={Roziere, Baptiste and Lachaux, Marie-Anne and Szafraniec, Marc and Lample, Guillaume},
  journal={arXiv preprint arXiv:2102.07492},
  year={2021}
}
@misc{bigcode-evaluation-harness,
  author = {Ben Allal, Loubna and
                  Muennighoff, Niklas and
                  Kumar Umapathi, Logesh and
                  Lipkin, Ben and
                  Von Werra, Leandro},
  title = {A framework for the evaluation of code generation models},
  howpublished = {\url{https://github.com/bigcode-project/bigcode-evaluation-harness}},
  year = 2022,
  month = dec
}
@misc{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}
@inproceedings{kanade2020embeddings,
author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
title = {Learning and Evaluating Contextual Embedding of Source Code},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as BERT, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4M Python files from GitHub, which we use to pre-train CuBERT, an open-sourced code-understanding BERT model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune CuBERT on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, BiLSTM and Transformer models, as well as published state-of-the-art models, showing that CuBERT outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against CuBERT models as a strong baseline.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {474},
numpages = {12},
series = {ICML'20}
}
@inproceedings{feng-etal-2020-codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.139",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
    abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
}
@article{bavarian2022fim,
  doi = {10.48550/ARXIV.2207.14255},
  
  journal={arXiv preprint arXiv:2207.14255},
  url = {https://arxiv.org/abs/2207.14255},
  
  author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Efficient Training of Language Models to Fill in the Middle},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@article{gao2020pile,
      title={The {P}ile: An 800{GB} Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2021},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
  journal={arXiv preprint arXiv:2101.00027},
}
@article{husain2019codesearchnet,
  title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}
@inproceedings{lee-etal-2022-deduplicating,
    title = "Deduplicating Training Data Makes Language Models Better",
    author = "Lee, Katherine  and
      Ippolito, Daphne  and
      Nystrom, Andrew  and
      Zhang, Chiyuan  and
      Eck, Douglas  and
      Callison-Burch, Chris  and
      Carlini, Nicholas",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.577",
    doi = "10.18653/v1/2022.acl-long.577",
    pages = "8424--8445",
    abstract = "We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1{\%} of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets{---}for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4{\%} of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.",
}

@inproceedings{carlini21extracting,
 author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
title = {Extracting Training Data from Large Language Models},
booktitle = {USENIX Security Symposium},
year = {2021},
howpublished = {arXiv preprint arXiv:2012.07805},
url = {https://arxiv.org/abs/2012.07805}
}
@article{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  journal={arXiv preprint arXiv:2202.06539},
  year={2022}
}
@article{khlaaf2022hazard,
  title={A Hazard Analysis Framework for Code Synthesis Large Language Models},
  author={Khlaaf, Heidy and Mishkin, Pamela and Achiam, Joshua and Krueger, Gretchen and Brundage, Miles},
  journal={arXiv preprint arXiv:2207.14157},
  year={2022}
}
@inproceedings{allamanis2019duplication,
author = {Allamanis, Miltiadis},
title = {The Adverse Effects of Code Duplication in Machine Learning Models of Code},
year = {2019},
isbn = {9781450369954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359591.3359735},
doi = {10.1145/3359591.3359735},
abstract = {The field of big code relies on mining large corpora of code to perform some learning task towards creating better tools for software engineers. A significant threat to this approach was recently identified by Lopes et al. (2017) who found a large amount of near-duplicate code on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this work, we explore the effects of code duplication on machine learning models showing that reported performance metrics are sometimes inflated by up to 100\% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present a duplication index for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them. Finally, we release tools to help the community avoid this problem in future research.},
booktitle = {Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {143–153},
numpages = {11},
keywords = {dataset collection, big code, code naturalness, machine learning, duplication},
location = {Athens, Greece},
series = {Onward! 2019}
}
@article{lee2021deduplicating,
  title={Deduplicating training data makes language models better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2107.06499},
  year={2021}
}
@inproceedings{allamanis2019adverse,
  title={The adverse effects of code duplication in machine learning models of code},
  author={Allamanis, Miltiadis},
  booktitle={Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
  pages={143--153},
  year={2019}
}
@article{lopes2017dejavu,
  title={D{\'e}j{\`a}Vu: a map of code duplicates on GitHub},
  author={Lopes, Cristina V and Maj, Petr and Martins, Pedro and Saini, Vaibhav and Yang, Di and Zitny, Jakub and Sajnani, Hitesh and Vitek, Jan},
  journal={Proceedings of the ACM on Programming Languages},
  volume={1},
  number={OOPSLA},
  pages={1--28},
  year={2017},
  publisher={ACM New York, NY, USA}
}
@article{hernandez2022scaling,
  title={Scaling Laws and Interpretability of Learning from Repeated Data},
  author={Danny Hernandez and Tom Brown and Tom Conerly and Nova DasSarma and Dawn Drain and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Tom Henighan and Tristan Hume and Scott Johnston and Ben Mann and Chris Olah and Catherine Olsson and Dario Amodei and Nicholas Joseph and Jared Kaplan and Sam McCandlish},
  journal={arXiv preprint arXiv:2205.10487},
  year={2022}
}
@inproceedings{broder2000identifying,
  title={Identifying and filtering near-duplicate documents},
  author={Broder, Andrei Z.},
  booktitle={Annual symposium on combinatorial pattern matching},
  pages={1--10},
  year={2000},
  organization={Springer}
}
@article{har2012approximate,
  title={Approximate nearest neighbor: Towards removing the curse of dimensionality},
  author={Har-Peled, Sariel and Indyk, Piotr and Motwani, Rajeev},
  year={2012},
  publisher={Theory of Computing Exchange}
}
@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
@misc {bigscience_workshop_2022,
	author       = { {BigScience Workshop} },
	title        = { {BLOOM} (Revision 4ab0472) },
	year         = 2022,
	url          = { https://huggingface.co/bigscience/bloom },
	doi          = { 10.57967/hf/0003 },
	publisher    = { Hugging Face }
}
@inproceedings{
laurencon2022roots,
title={The {BigScience} {ROOTS} Corpus: A 1.6{TB} Composite Multilingual Dataset},
author={Hugo Lauren{\c{c}}on and Lucile Saulnier and Thomas Wang and Christopher Akiki and Albert Villanova del Moral and Teven Le Scao and Leandro Von Werra and Chenghao Mou and Eduardo Gonz{\'a}lez Ponferrada and Huu Nguyen and J{\"o}rg Frohberg and Mario {\v{S}}a{\v{s}}ko and Quentin Lhoest and Angelina McMillan-Major and G{\'e}rard Dupont and Stella Biderman and Anna Rogers and Loubna Ben allal and Francesco De Toni and Giada Pistilli and Olivier Nguyen and Somaieh Nikpoor and Maraim Masoud and Pierre Colombo and Javier de la Rosa and Paulo Villegas and Tristan Thrush and Shayne Longpre and Sebastian Nagel and Leon Weber and Manuel Romero Mu{\~n}oz and Jian Zhu and Daniel Van Strien and Zaid Alyafeai and Khalid Almubarak and Vu Minh Chien and Itziar Gonzalez-Dios and Aitor Soroa and Kyle Lo and Manan Dey and Pedro Ortiz Suarez and Aaron Gokaslan and Shamik Bose and David Ifeoluwa Adelani and Long Phan and Hieu Tran and Ian Yu and Suhas Pai and Jenny Chim and Violette Lepercq and Suzana Ilic and Margaret Mitchell and Sasha Luccioni and Yacine Jernite},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=UoEw6KigkUn}
}
@misc{kuhn2022copilot,
  author = {Bradley M. Kuhn},
  title = {If Software is My Copilot, Who Programmed My Software?},
  howpublished = "\url{https://sfconservancy.org/blog/2022/feb/03/github-copilot-copyleft-gpl/}",
  year = {2022}, 
}
@misc{reda2021copilot,
  author = {Felix Reda},
  title = {{GitHub Copilot} is not infringing your copyright},
  howpublished = "\url{https://felixreda.eu/2021/07/github-copilot-is-not-infringing-your-copyright/}",
  year = {2022}, 
}
@misc{rothchild2022copyright,
  author = {Rothchild, John A. and Rothchild, Daniel},
  title = {Copyright Implications of the Use of Code Repositories to Train a Machine Learning Model},
  howpublished = "\url{https://www.fsf.org/licensing/copilot/copyright-implications-of-the-use-of-code-repositories-to-train-a-machine-learning-model}",
  year = {2022}, 
}
@misc{butterick2022copilot,
  author = {Butterick, Matthew},
  title = {This {CoPilot} is stupid and wants to kill me},
  howpublished = "\url{https://matthewbutterick.com/chron/this-copilot-is-stupid-and-wants-to-kill-me.html}",
  year = {2022}, 
}
@article{klaaf2022hazard,
  doi = {10.48550/ARXIV.2207.14157},
  
  journal={arXiv preprint arXiv:2207.14157},
  url = {https://arxiv.org/abs/2207.14157},
  
  author = {Khlaaf, Heidy and Mishkin, Pamela and Achiam, Joshua and Krueger, Gretchen and Brundage, Miles},
  
  keywords = {Software Engineering (cs.SE), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Hazard Analysis Framework for Code Synthesis Large Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}
@article{bender-friedman-2018-data,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1041",
    doi = "10.1162/tacl_a_00041",
    pages = "587--604",
    abstract = "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
}
@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{Kocetkov2022TheStack,
  title={The {S}tack: 3 {TB} of permissively licensed source code},
  author={Kocetkov, Denis and Li, Raymond and Ben Allal, Loubna and Li, Jia and Mou,Chenghao and Muñoz Ferrandis, Carlos and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and von Werra, Leandro and de Vries, Harm},
  journal={Preprint},
  year={2022},
  url={https://arxiv.org/abs/2211.15533}
}

@inproceedings{yee:typeweaver,
  author = "Ming-Ho Yee and Arjun Guha",
  title = "Do Machine Learning Models Produce {TypeScript} Types that Type Check?",
  booktitle = "European Conference on Object-Oriented Programming (ECOOP)",
  year = 2023
}

@article{cassano2022multiple,
  author={Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
  journal={IEEE Transactions on Software Engineering}, 
  title={{MultiPL-E:} A Scalable and Polyglot Approach to Benchmarking Neural Code Generation}, 
  year={2023},
  volume={},
  number={},
  pages={1-17},
  doi={10.1109/TSE.2023.3267446},
  url={https://arxiv.org/abs/2208.08227}
  }

  
@article{shazeer2019mqa,
  author    = {Noam Shazeer},
  title     = {Fast Transformer Decoding: One Write-Head is All You Need},
  journal   = {CoRR},
  volume    = {abs/1911.02150},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02150},
  eprinttype = {arXiv},
  eprint    = {1911.02150},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02150.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{vaswani2017attention,
  added-at = {2019-01-14T18:39:11.000+0100},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  biburl = {https://www.bibsonomy.org/bibtex/2a08c93d224dfcfb83550246c3d6a178f/stefan.ernst},
  booktitle = {Advances in Neural Information Processing Systems},
  description = {Aktuelleres Paper zur Verwendung von Attention für die Neural Machine Translation},
  interhash = {c9bf08cbcb15680c807e12a01dd8c929},
  intrahash = {a08c93d224dfcfb83550246c3d6a178f},
  keywords = {final thema:attention},
  pages = {5998--6008},
  timestamp = {2019-01-14T18:39:11.000+0100},
  title = {Attention is all you need},
  year = 2017
}

@inproceedings{kulal2019spoc,
 author = {Kulal, Sumith and Pasupat, Panupong and Chandra, Kartik and Lee, Mina and Padon, Oded and Aiken, Alex and Liang, Percy S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SPoC: Search-based Pseudocode to Code},
 url = {https://proceedings.neurips.cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{yu-etal-2018-spider,
    title = "{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task",
    author = "Yu, Tao  and
      Zhang, Rui  and
      Yang, Kai  and
      Yasunaga, Michihiro  and
      Wang, Dongxu  and
      Li, Zifan  and
      Ma, James  and
      Li, Irene  and
      Yao, Qingning  and
      Roman, Shanelle  and
      Zhang, Zilin  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1425",
    doi = "10.18653/v1/D18-1425",
    pages = "3911--3921",
}

@article{iyer2018mapping,
  title={Mapping language to code in programmatic context},
  author={Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1808.09588},
  year={2018}
}

@article{hendrycks2021measuring,
  doi = {10.48550/ARXIV.2105.09938},
  url = {https://arxiv.org/abs/2105.09938},
  author = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and Steinhardt, Jacob},
  title = {Measuring Coding Challenge Competence With {APPS}},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2105.09938},
  year = {2021}
}

@article{wikisql,
  doi = {10.48550/ARXIV.1709.00103},
  journal={arXiv preprint arXiv:1709.00103},
  url = {https://arxiv.org/abs/1709.00103},
  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {{Seq2SQL:} Generating Structured Queries from Natural Language using Reinforcement Learning},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{spidersql,
  doi = {10.48550/ARXIV.1809.08887},
  
  journal={arXiv preprint arXiv:1809.08887},
  url = {https://arxiv.org/abs/1809.08887},
  
  author = {Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and Zhang, Zilin and Radev, Dragomir},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{mbxp,
  doi = {10.48550/ARXIV.2210.14868},  
  journal={arXiv preprint arXiv:2210.14868},
  
  url = {https://arxiv.org/abs/2210.14868},
  
  author = {Athiwaratkun, Ben and Gouda, Sanjay Krishna and Wang, Zijian and Li, Xiaopeng and Tian, Yuchen and Tan, Ming and Ahmad, Wasi Uddin and Wang, Shiqi and Sun, Qing and Shang, Mingyue and Gonugondla, Sujan Kumar and Ding, Hantian and Kumar, Varun and Fulton, Nathan and Farahani, Arash and Jain, Siddhartha and Giaquinto, Robert and Qian, Haifeng and Ramanathan, Murali Krishna and Nallapati, Ramesh and Ray, Baishakhi and Bhatia, Parminder and Sengupta, Sudipta and Roth, Dan and Xiang, Bing},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi-lingual Evaluation of Code Generation Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Lai2022DS1000,
  title={{DS-1000:} A Natural and Reliable Benchmark for Data Science Code Generation},
  author={Yuhang Lai and Chengxi Li and Yiming Wang and Tianyi Zhang and Ruiqi Zhong and Luke Zettlemoyer and Scott Wen-tau Yih and Daniel Fried and Sida Wang and Tao Yu},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.11501}
}

@article{feng2020codebert,
  doi = {10.48550/ARXIV.2002.08155},
  url = {https://arxiv.org/abs/2002.08155},
  author = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2002.08155},
  year = {2020}
}

@article{ren2020codebleu,
  doi = {10.48550/ARXIV.2009.10297},
  url = {https://arxiv.org/abs/2009.10297},
  journal={arXiv preprint arXiv:2009.10297},
  author = {Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Sundaresan, Neel and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
  title = {{CodeBLEU:} a Method for Automatic Evaluation of Code Synthesis},
  publisher = {arXiv},
  year = {2020}
}

@article{tufano2020unit,
  doi = {10.48550/ARXIV.2009.05617},
  url = {https://arxiv.org/abs/2009.05617},
  author = {Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Deng, Shao Kun and Sundaresan, Neel},
  title = {Unit Test Case Generation with Transformers and Focal Context},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2009.10297},
  year = {2020}
}

@inproceedings{villmow-etal-2021-contest,
    title = "{C}on{T}est: A Unit Test Completion Benchmark featuring Context",
    author = "Villmow, Johannes  and
      Depoix, Jonas  and
      Ulges, Adrian",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nlp4prog-1.2",
    doi = "10.18653/v1/2021.nlp4prog-1.2",
    pages = "17--25",
}

@inproceedings{ahmed2022multilingual,
	doi = {10.1145/3510003.3510049},
	year = 2022,
	publisher = {ACM},
	author = {Toufique Ahmed and Premkumar Devanbu},
	title = {Multilingual training for software engineering},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering}
}

@inproceedings{hellendoorn:dl-ti,
  title = {Deep {{Learning Type Inference}}},
  booktitle = {Fse},
  author = {Hellendoorn, Vincent J. and Bird, Christian and Barr, Earl T. and Allamanis, Miltiadis},
  year = {2018}
}

@inproceedings{wei:lambdanet,
  title = {{{LambdaNet}}: {{Probabilistic Type Inference}} Using {{Graph Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Wei, Jiayi and Goyal, Maruth and Durrett, Greg and Dillig, Isil},
  year = {2020},
  file = {C\:\\Users\\arjun\\Zotero\\storage\\SKRJCV7N\\Wei et al. - 2020 - LambdaNet Probabilistic Type Inference using Grap.pdf}
}

@inproceedings{pradel:typewriter,
  title = {{{TypeWriter}}: {{Neural Type Prediction}} with {{Search-Based Validation}}},
  booktitle = " ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
  doi = {10.1145/3368089.3409715},
  author = {Pradel, Michael and Gousios, Georgios and Liu, Jason and Chandra, Satish},
  year = {2020}
}

@misc{anthony_moi_2022_hftokenizers,
  author       = {Anthony MOI and
                  Nicolas Patry and
                  Pierric Cistac and
                  Pete and
                  Funtowicz Morgan and
                  Sebastian Pütz and
                  Mishig and
                  Bjarte Johansen and
                  Thomas Wolf and
                  Sylvain Gugger and
                  Clement and
                  Julien Chaumond and
                  Lysandre Debut and
                  François Garillot and
                  Luc Georges and
                  dctelus and
                  JC Louis and
                  MarcusGrass and
                  Taufiquzzaman Peyash and
                  0xflotus and
                  Alan deLevie and
                  Alexander Mamaev and
                  Arthur and
                  Cameron and
                  Colin Clement and
                  Dagmawi Moges and
                  David Hewitt and
                  Denis Zolotukhin and
                  Geoffrey Thomas},
  title        = {huggingface/tokenizers: Rust 0.13.2},
  month        = nov,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v0.13.2},
  doi          = {10.5281/zenodo.7298413},
  url          = {https://doi.org/10.5281/zenodo.7298413}
}
@inproceedings{rust-etal-2021-good,
    title = "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models",
    author = "Rust, Phillip  and
      Pfeiffer, Jonas  and
      Vuli{\'c}, Ivan  and
      Ruder, Sebastian  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.243",
    doi = "10.18653/v1/2021.acl-long.243",
    pages = "3118--3135",
    abstract = "In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model{'}s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.",
}

@BOOK{Tunstall2022-nw,
  title     = "Natural language processing with transformers, revised edition",
  author    = "Tunstall, Lewis and Von Werra, Leandro and Wolf, Thomas",
  publisher = "O'Reilly Media",
  month     =  jun,
  year      =  2022,
  address   = "Sebastopol, CA"
}

@inproceedings{hu2019code,
  title={Code generation from supervised code embeddings},
  author={Hu, Han and Chen, Qiuyuan and Liu, Zhaoyi},
  booktitle={Neural Information Processing: 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12--15, 2019, Proceedings, Part IV 26},
  pages={388--396},
  year={2019},
  organization={Springer}
}


@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}


@inproceedings{allal2023santacoder,
  author = {Ben Allal, Loubna  and Li, Raymond and Kocetkov, Denis and 
    Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and 
    Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and
    Umapathi, Logesh Kumar and Anderson, Carolyn Jane and Zi, Yangtian and
    Poirier, Joel Lamy and Schoelkopf, Hailey and Troshin, Sergey and
    Abulkhanov, Dmitry and Romero, Manuel and Lappert, Michael and
    De Toni, Francesco and del Río, Bernardo García and Liu, Qian and
    Bose, Shamik and Bhattacharyya, Urvashi and Zhuo, Terry Yue and
    Yu, Ian and Villegas, Paulo and Zocca, Marco and Mangrulkar, Sourab and
    Lansky, David and Nguyen, Huu and Contractor, Danish and Villa, Luis and
    Li, Jia and Bahdanau, Dzmitry and Jernite, Yacine and Hughes, Sean and
    Fried, Daniel and Guha, Arjun and de Vries, Harm and von Werra, Leandro},
  title = {{SantaCoder:} don't reach for the stars!},
  booktitle = "Deep Learning for Code Workshop (DL4C)",
  year = {2023},
}

@article{eloundou2023gpts,
  title={{GPTs} are {GPTs}: An early look at the labor market impact potential of large language models},
  author={Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  journal={arXiv preprint arXiv:2303.10130},
  year={2023}
}
@article{henderson2023foundation,
  title={Foundation Models and Fair Use},
  author={Henderson, Peter and Li, Xuechen and Jurafsky, Dan and Hashimoto, Tatsunori and Lemley, Mark A and Liang, Percy},
  journal={arXiv preprint arXiv:2303.15715},
  year={2023}
}
@misc{openai2023systemcard,
  author = {OpenAI},
  title = {{GPT-4} System Card},
  howpublished = "\url{https://cdn.openai.com/papers/gpt-4-system-card.pdf}",
  year = {2023}, 
}
@article{lemley2020fair,
  title={Fair learning},
  author={Lemley, Mark A and Casey, Bryan},
  journal={Tex. L. Rev.},
  volume={99},
  pages={743},
  year={2020},
  publisher={HeinOnline},
  url={https://texaslawreview.org/fair-learning/}
}
@article{levendowski2018copyright,
  title={How copyright law can fix artificial intelligence's implicit bias problem},
  author={Levendowski, Amanda},
  journal={Wash. L. Rev.},
  volume={93},
  pages={579},
  year={2018},
  publisher={HeinOnline}
}
@article{touvron2023llama,
  title={{LLaMA:} Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{solaiman2023gradient,
  title={The Gradient of Generative {AI} Release: Methods and Considerations},
  author={Solaiman, Irene},
  journal={arXiv preprint arXiv:2302.04844},
  year={2023}
}
@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun},
  booktitle={Workshop on challenges in representation learning, ICML},
  number={2},
  pages={896},
  year={2013}
}
@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}
@article{black2022gpt,
  title={{GPT-NeoX-20B:} An open-source autoregressive language model},
  author={Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}
@article{chung2022scaling,
    title={Scaling Instruction-Finetuned Language Models},
    author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
    year={2022},
  journal={arXiv preprint arXiv:2210.11416},
    eprint={2210.11416},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2210.11416}
}
@article{togelius2023choose,
  title={Choose Your Weapon: Survival Strategies for Depressed {AI} Academics},
  author={Togelius, Julian and Yannakakis, Georgios N.},
  journal={arXiv preprint arXiv:2304.06035},
  year={2023}
}
@misc{wang2021gpt,
  title={{GPT-J-6B:} A 6 billion parameter autoregressive language model},
  author={Wang, Ben and Komatsuzaki, Aran},
  year={2021},
  publisher={Software}
}
@article{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
  journal={arXiv preprint arXiv:2304.01373},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{tay2022unifying,
  title={Unifying language learning paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}
@inproceedings{brants2007large,
    title = "Large Language Models in Machine Translation",
    author = "Brants, Thorsten  and
      Popat, Ashok C.  and
      Xu, Peng  and
      Och, Franz J.  and
      Dean, Jeffrey",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = 2007,
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1090",
    pages = "858--867",
}
@inproceedings{heafield2013scalable,
    title = "Scalable Modified {Kneser-Ney} Language Model Estimation",
    author = "Heafield, Kenneth  and
      Pouzyrevsky, Ivan  and
      Clark, Jonathan H.  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = 2013,
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-2121",
    pages = "690--696",
}
@inproceedings{buck2014n,
    title = "{N}-gram Counts and Language Models from the {C}ommon {C}rawl",
    author = "Buck, Christian  and
      Heafield, Kenneth  and
      van Ooyen, Bas",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/1097_Paper.pdf",
    pages = "3579--3584",
    abstract = "We contribute 5-gram counts and language models trained on the Common Crawl corpus, a collection over 9 billion web pages. This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate. By preserving singletons, we were able to use Kneser-Ney smoothing to build large language models. This paper describes how the corpus was processed with emphasis on the problems that arise in working with data at this scale. Our unpruned Kneser-Ney English {\$}5{\$}-gram language model, built on 975 billion deduplicated tokens, contains over 500 billion unique n-grams. We show gains of 0.5-1.4 BLEU by using large language models to translate into various languages.",
}
@inproceedings{bengio2000neural,
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {A Neural Probabilistic Language Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html},
 volume = {13},
 year = {2000}
}
@inproceedings{mikolov2010recurrent,
  author       = {Tom{\'{a}}s Mikolov and
                  Martin Karafi{\'{a}}t and
                  Luk{\'{a}}s Burget and
                  Jan Cernock{\'{y}} and
                  Sanjeev Khudanpur},
  editor       = {Takao Kobayashi and
                  Keikichi Hirose and
                  Satoshi Nakamura},
  title        = {Recurrent neural network based language model},
  booktitle    = {{INTERSPEECH} 2010, 11th Annual Conference of the International Speech
                  Communication Association, Makuhari, Chiba, Japan, September 26-30,
                  2010},
  pages        = {1045--1048},
  publisher    = {{ISCA}},
  year         = {2010},
  url          = {http://www.isca-speech.org/archive/interspeech\_2010/i10\_1045.html},
  timestamp    = {Tue, 16 Nov 2021 11:37:23 +0100},
  biburl       = {https://dblp.org/rec/conf/interspeech/MikolovKBCK10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}

@article{jozefowicz2016exploring,
  title={Exploring the limits of language modeling},
  author={Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  journal={arXiv preprint arXiv:1602.02410},
  year={2016}
}
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}
@inproceedings{pennington2014glove,
    title = "{GloVe:} Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}
@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training {Gopher}},
  author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}
@article{smith2022using,
  title={Using {DeepSpeed} and {Megatron} to train {Megatron-Turing NLG 530B}, a large-scale generative language model},
  author={Shaden Smith and Mostofa Patwary and Brandon Norick and Patrick LeGresley and Samyam Rajbhandari and Jared Casper and Zhun Liu and Shrimai Prabhumoye and George Zerveas and Vijay Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and Julie Bernauer and Xia Song and Mohammad Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}
@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}
@article{openai2023gpt4,
      title={{GPT-4} Technical Report}, 
      author={OpenAI},
      year={2023},
  journal={arXiv preprint arXiv:2009.03300},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{hendrycks2020mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{reddy2019coqa,
    title = "{CoQA}: A Conversational Question Answering Challenge",
    author = "Reddy, Siva  and
      Chen, Danqi  and
      Manning, Christopher D.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1016",
    doi = "10.1162/tacl_a_00266",
    pages = "249--266",
}
@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}
@article{gao2022pal,
    title={{PAL}: Program-aided Language Models},
    author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
    journal={arXiv preprint arXiv:2211.10435},
    year={2022}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
    author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@inproceedings{
lewkowycz2022solving,
title={Solving Quantitative Reasoning Problems with Language Models},
author={Aitor Lewkowycz and Anders Johan Andreassen and David Dohan and Ethan Dyer and Henryk Michalewski and Vinay Venkatesh Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=IFXTZERXdM7}
}
@misc{kalliamvakou2022copilot,
  author = {Eirini Kalliamvakou},
  title = {Research: quantifying {GitHub Copilot}’s impact on developer productivity and happiness},
  howpublished = "\url{https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/}",
  year = {2022}, 
}
@misc{wef2023futureofjobs,
  author = {{World Economic Forum}},
  title = {Future of Jobs Report},
  howpublished = "\url{https://www3.weforum.org/docs/WEF_Future_of_Jobs_2023.pdf}",
  year = {2023}, 
}
@misc{bbc2023chatgpt_ban,
  author = {BBC},
  title = {{ChatGPT} accessible again in {Italy}},
  howpublished = "\url{https://www.bbc.com/news/technology-65431914}",
  year = {2023}, 
}
@misc{euronews2023copilot,
  author = {Euronews},
  title = {Microsoft attracting users to its code-writing, generative {AI} software},
  howpublished = "\url{https://www.euronews.com/next/2023/01/25/microsoft-results-ai}",
  year = {2023}, 
}
@misc{guardian2023chatgpt,
  author = {{The Guardian}},
  title = {{ChatGPT} reaches 100 million users two months after launch},
  howpublished = "\url{https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app}",
  year = {2023}, 
}
@misc{lomas2023gdpr_llms,
  author = {Natasha Lomas},
  title = {Unpicking the rules shaping generative {AI}},
  howpublished = "\url{https://techcrunch.com/2023/04/13/generative-ai-gdpr-enforcement/}",
  year = {2022}, 
}
@misc{eu2018gdpr,
  author = {{European Council}},
  title = {The general data protection regulation},
  howpublished = "\url{https://www.consilium.europa.eu/en/policies/data-protection/data-protection-regulation/}",
  year = {2018}, 
}
@book{DOE1vGitHub,
  title         = {4:22-cv-06823 N.D. Cal.},
  author        =  {{DOE 1 v. and GitHub, Inc.}},
  year         = {2022},
}
@book{stablediffusion_lawsuit,
  title = {3:23-cv-00201 N.D. Cal.},
  author = {{Andersen et al v. Stability AI et al}},
  year = {2023},
}
@article{kojima2023large,
      title={Large Language Models are Zero-Shot Reasoners}, 
      author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
      year={2022},
      eprint={2205.11916},
      archivePrefix={arXiv},
  journal={arXiv preprint arXiv:2205.11916},
      primaryClass={cs.CL}
}

@article{zhou2023leasttomost,
      title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models}, 
      author={Denny Zhou and Nathanael Schärli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc Le and Ed Chi},
      year={2022},
      eprint={2205.10625},
      archivePrefix={arXiv},
  journal={arXiv preprint arXiv:2205.10625},
      primaryClass={cs.AI}
}

@article{askell2021general,
      title={A General Language Assistant as a Laboratory for Alignment}, 
      author={Amanda Askell and Yuntao Bai and Anna Chen and Dawn Drain and Deep Ganguli and Tom Henighan and Andy Jones and Nicholas Joseph and Ben Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Jackson Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Jared Kaplan},
      year={2021},
      eprint={2112.00861},
  journal={arXiv preprint arXiv:2112.00861},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{christiano2017rlhf,
 author = {Christiano, Paul F. and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Reinforcement Learning from Human Preferences},
 url = {https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html},
 volume = {30},
 year = {2017}
}
@inproceedings{stiennon2020learning,
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {3008--3021},
	publisher = {Curran Associates, Inc.},
	title = {Learning to summarize with human feedback},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html},
	volume = {33},
	year = {2020}
}
@inproceedings{ouyang2022instructgpt,
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {27730--27744},
	publisher = {Curran Associates, Inc.},
	title = {Training language models to follow instructions with human feedback},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
	volume = {35},
	year = {2022}
}
@article{bahl1983perplexity,
    author = {Bahl, Lalit and Jelinek, Frederick and Mercer, Robert},
    year = {1983},
    month = {04},
    pages = {179 - 190},
    title = {A Maximum Likelihood Approach to Continuous Speech Recognition},
    volume = {PAMI-5},
    isbn = {9781558601246},
    journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
    doi = {10.1109/TPAMI.1983.4767370}
}

@article{akiki-bigscience-22,
  author       = {Christopher Akiki and
                  Giada Pistilli and
                  Margot Mieskes and
                  Matthias Gall{\'{e}} and
                  Thomas Wolf and
                  Suzana Ilic and
                  Yacine Jernite},
  title        = {{BigScience:} A Case Study in the Social Construction of a Multilingual Large Language Model},
  journal      = {CoRR},
  volume       = {abs/2212.04960},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2212.04960},
  doi          = {10.48550/arXiv.2212.04960},
  eprinttype    = {arXiv},
  eprint       = {2212.04960},
  timestamp    = {Mon, 02 Jan 2023 15:09:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-04960.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{piktus-roots-search,
  author       = {Aleksandra Piktus and
                  Christopher Akiki and
                  Paulo Villegas and
                  Hugo Lauren{\c{c}}on and
                  G{\'{e}}rard Dupont and
                  Alexandra Sasha Luccioni and
                  Yacine Jernite and
                  Anna Rogers},
  title        = {The {ROOTS} Search Tool: Data Transparency for {LLMs}},
  journal      = {CoRR},
  volume       = {abs/2302.14035},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.14035},
  doi          = {10.48550/arXiv.2302.14035},
  eprinttype    = {arXiv},
  eprint       = {2302.14035},
  timestamp    = {Tue, 28 Feb 2023 14:02:05 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-14035.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{akiki-spacerini-search,
  author       = {Christopher Akiki and
                  Odunayo Ogundepo and
                  Aleksandra Piktus and
                  Xinyu Zhang and
                  Akintunde Oladipo and
                  Jimmy Lin and
                  Martin Potthast},
  title        = {Spacerini: Plug-and-play Search Engines with {Pyserini} and {Hugging
                  Face}},
  journal      = {CoRR},
  volume       = {abs/2302.14534},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.14534},
  doi          = {10.48550/arXiv.2302.14534},
  eprinttype    = {arXiv},
  eprint       = {2302.14534},
  timestamp    = {Thu, 02 Mar 2023 10:23:33 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-14534.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{marone-data-portraits-search,
  author       = {Marc Marone and
                  Benjamin {Van Durme}},
  title        = {Data Portraits: Recording Foundation Model Training Data},
  journal      = {CoRR},
  volume       = {abs/2303.03919},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.03919},
  doi          = {10.48550/arXiv.2303.03919},
  eprinttype    = {arXiv},
  eprint       = {2303.03919},
  timestamp    = {Wed, 15 Mar 2023 17:23:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-03919.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mitchell-measuring-data,
  author       = {Margaret Mitchell and
                  Alexandra Sasha Luccioni and
                  Nathan Lambert and
                  Marissa Gerchick and
                  Angelina McMillan{-}Major and
                  Ezinwanne Ozoani and
                  Nazneen Rajani and
                  Tristan Thrush and
                  Yacine Jernite and
                  Douwe Kiela},
  title        = {Measuring Data},
  journal      = {CoRR},
  volume       = {abs/2212.05129},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2212.05129},
  doi          = {10.48550/arXiv.2212.05129},
  eprinttype    = {arXiv},
  eprint       = {2212.05129},
  timestamp    = {Mon, 02 Jan 2023 15:09:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2212-05129.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{papineni2002bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{ahmad2021plbart,
    title = "Unified Pre-training for Program Understanding and Generation",
    author = "Ahmad, Wasi  and
      Chakraborty, Saikat  and
      Ray, Baishakhi  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.211",
}

@misc{smith2016bigquery,
year=2016,
author="Smith, Arfon",
  title = { Kernel Description},
  howpublished = {\url{https://github.blog/2016-06-29-making-open-source-data-more-available/}},
}

@inproceedings{pearce2022copilotsec,
    Author = {Hammond Pearce and Baleegh Ahmad and Benjamin Tan and Brendan Dolan-Gavitt and Ramesh Karri},
    year = {2022},
    booktitle = {IEEE Symposium on Security and Privacy},
    Url = {https://arxiv.org/abs/2108.09293},
    address = {San Francisco, CA},
    Title = {Asleep at the Keyboard? {Assessing} the Security of {GitHub Copilot}'s Code Contributions},
}

@article{bloom-filters-10.1145/362686.362692,
    author = {Bloom, Burton H.},
    title = {Space/Time Trade-Offs in Hash Coding with Allowable Errors},
    year = {1970},
    issue_date = {July 1970},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {13},
    number = {7},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/362686.362692},
    doi = {10.1145/362686.362692},
    journal = {Commun. ACM},
    month = {jul},
    pages = {422–426},
    numpages = {5},
    keywords = {scatter storage, hash addressing, hash coding, storage layout, retrieval trade-offs, searching, retrieval efficiency, storage efficiency}
}

@inproceedings{gehman_realtoxicityprompts_2020,
	address = {Online},
	title = {{RealToxicityPrompts}: {Evaluating} {Neural} {Toxic} {Degeneration} in {Language} {Models}},
	shorttitle = {{RealToxicityPrompts}},
	url = {https://aclanthology.org/2020.findings-emnlp.301},
	doi = {10.18653/v1/2020.findings-emnlp.301},
	abstract = {Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.},
	urldate = {2023-04-05},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
	month = nov,
	year = {2020},
	pages = {3356--3369},
}

@inproceedings{vidgen_learning_2021,
	address = {Online},
	title = {Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},
	shorttitle = {Learning from the Worst},
	url = {https://aclanthology.org/2021.acl-long.132},
	doi = {10.18653/v1/2021.acl-long.132},
	abstract = {We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54\% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Vidgen, Bertie and Thrush, Tristan and Waseem, Zeerak and Kiela, Douwe},
	month = aug,
	year = {2021},
	pages = {1667--1682},
}

@inproceedings{may_measuring_2019,
	address = {Minneapolis, Minnesota},
	title = {On Measuring Social Biases in Sentence Encoders},
	url = {https://www.aclweb.org/anthology/N19-1063},
	doi = {10.18653/v1/N19-1063},
	abstract = {The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test's assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.},
	urldate = {2020-09-21},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R. and Rudinger, Rachel},
	month = jun,
	year = {2019},
	pages = {622--628},
}

@article{webster_measuring_2020,
	title = {Measuring and {Reducing} {Gendered} {Correlations} in {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2010.06032},
	abstract = {Pre-trained models have revolutionized natural language understanding. However, researchers have found they can encode artifacts undesired in many applications, such as professions correlating with one gender more than another. We explore such gendered correlations as a case study for how to address unintended correlations in pre-trained models. We define metrics and reveal that it is possible for models with similar accuracy to encode correlations at very different rates. We show how measured correlations can be reduced with general-purpose techniques, and highlight the trade offs different strategies have. With these results, we make recommendations for training robust models: (1) carefully evaluate unintended correlations, (2) be mindful of seemingly innocuous configuration differences, and (3) focus on general mitigations.},
	urldate = {2020-10-27},
	journal = {arXiv:2010.06032 [cs]},
	author = {Webster, Kellie and Wang, Xuezhi and Tenney, Ian and Beutel, Alex and Pitler, Emily and Pavlick, Ellie and Chen, Jilin and Petrov, Slav},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.06032},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{hutchinson_social_2020,
	address = {Online},
	title = {Social Biases in {NLP} Models as Barriers for Persons with Disabilities},
	url = {https://aclanthology.org/2020.acl-main.487},
	doi = {10.18653/v1/2020.acl-main.487},
	abstract = {Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hutchinson, Ben and Prabhakaran, Vinodkumar and Denton, Emily and Webster, Kellie and Zhong, Yu and Denuyl, Stephen},
	month = jul,
	year = {2020},
	pages = {5491--5501},
}

@inproceedings{nadeem_stereoset_2021,
	address = {Online},
	title = {{StereoSet}: {Measuring} stereotypical bias in pretrained language models},
	shorttitle = {{StereoSet}},
	url = {https://aclanthology.org/2021.acl-long.416},
	doi = {10.18653/v1/2021.acl-long.416},
	abstract = {A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.},
	urldate = {2022-03-14},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Nadeem, Moin and Bethke, Anna and Reddy, Siva},
	month = aug,
	year = {2021},
	pages = {5356--5371},
}

@article{dziri_faithdial_2022,
  title = "{FaithDial: A Faithful Benchmark for Information-Seeking Dialogue}",
  author = {Dziri, Nouha and Kamalloo, Ehsan and Milton, Sivan and Zaiane, Osmar and Yu, Mo and Ponti, Edoardo M and Reddy, Siva},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {1473--1490},
  year = {2022},
  month = {12},
  publisher = {MIT Press},
  doi={10.1162/tacl_a_00529}
}

@inproceedings{dziri-etal-2022-origin,
    title = "On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?",
    author = "Dziri, Nouha  and
      Milton, Sivan  and
      Yu, Mo  and
      Zaiane, Osmar  and
      Reddy, Siva",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.387",
    doi = "10.18653/v1/2022.naacl-main.387",
    pages = "5271--5285",
    abstract = "Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination. In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models? We conduct a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models. Our study reveals that the standard benchmarks consist of {\textgreater} 60{\%} hallucinated responses, leading to models that not only hallucinate but even amplify hallucinations. Our findings raise important questions on the quality of existing datasets and models trained using them. We make our annotations publicly available for future research.",
}

@inproceedings{dinan_wizard_2019,
    author = {Emily Dinan and
    Stephen Roller and
    Kurt Shuster and
    Angela Fan and
    Michael Auli and
    Jason Weston},
    title        = {Wizard of Wikipedia: Knowledge-Powered Conversational Agents},
    booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
    New Orleans, LA, USA, May 6-9, 2019},
    publisher    = {OpenReview.net},
    year         = {2019},
    url          = {https://openreview.net/forum?id=r1l73iRqKm},
    timestamp    = {Thu, 30 Jul 2020 17:57:23 +0200},
}

@inproceedings{meade_empirical_2022,
	address = {Dublin, Ireland},
	title = {An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models},
	url = {https://aclanthology.org/2022.acl-long.132},
	doi = {10.18653/v1/2022.acl-long.132},
	abstract = {Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model's language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.},
	urldate = {2022-11-23},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Meade, Nicholas and Poole-Dayan, Elinor and Reddy, Siva},
	month = may,
	year = {2022},
	pages = {1878--1898},
}

@inproceedings{kurita_measuring_2019,
	address = {Florence, Italy},
	title = {Measuring Bias in Contextualized Word Representations},
	url = {https://www.aclweb.org/anthology/W19-3823},
	doi = {10.18653/v1/W19-3823},
	abstract = {Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.},
	urldate = {2020-09-21},
	booktitle = {Proceedings of the {First} {Workshop} on {Gender} {Bias} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kurita, Keita and Vyas, Nidhi and Pareek, Ayush and Black, Alan W and Tsvetkov, Yulia},
	month = aug,
	year = {2019},
	pages = {166--172},
}

@article{nangia_crows-pairs_2020,
	title = {{CrowS}-{Pairs:} A Challenge Dataset for Measuring Social Biases in Masked Language Models},
	shorttitle = {{CrowS}-{Pairs}},
	url = {http://arxiv.org/abs/2010.00133},
	abstract = {Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.},
	urldate = {2020-10-08},
	journal = {arXiv:2010.00133 [cs]},
	author = {Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R.},
	month = sep,
	year = {2020},
	note = {arXiv: 2010.00133},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: EMNLP 2020},
}

@misc{meade_using_2023,
	title = {Using In-Context Learning to Improve Dialogue Safety},
	url = {http://arxiv.org/abs/2302.00871},
	doi = {10.48550/arXiv.2302.00871},
	abstract = {While large neural-based conversational models have become increasingly proficient as dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based framework for reducing bias and toxicity in responses generated from neural-based chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe model responses to similar dialogue contexts. We find our proposed approach performs competitively with strong baselines which use fine-tuning. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 2.92\% more than our approach. Finally, we also propose a straightforward re-ranking procedure which can further improve response safeness.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Meade, Nicholas and Gella, Spandana and Hazarika, Devamanyu and Gupta, Prakhar and Jin, Di and Reddy, Siva and Liu, Yang and Hakkani-Tür, Dilek},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00871 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{rashkin_increasing_2021,
	address = {Online},
	title = {Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features},
	url = {https://aclanthology.org/2021.acl-long.58},
	doi = {10.18653/v1/2021.acl-long.58},
	abstract = {Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text. We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence. Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses. We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity. At training time, additional inputs based on these evaluation measures are given to the dialogue model. At generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence. We also investigate the usage of additional controls at decoding time using resampling techniques. In addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rashkin, Hannah and Reitter, David and Tomar, Gaurav Singh and Das, Dipanjan},
	month = aug,
	year = {2021},
	pages = {704--718},
}

@article{ji_survey_2023,
	title = {Survey of Hallucination in Natural Language Generation},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	number = {12},
	urldate = {2023-05-02},
	journal = {ACM Computing Surveys},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	month = mar,
	year = {2023},
	keywords = {consistency in NLG, extrinsic hallucination, factuality in NLG, faithfulness in NLG, Hallucination, intrinsic hallucination},
	pages = {248:1--248:38},
}

@misc{liu_evaluating_2023,
	title = {Evaluating Verifiability in Generative Search Engines},
	url = {http://arxiv.org/abs/2304.09848},
	doi = {10.48550/arXiv.2304.09848},
	abstract = {Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5\% of generated sentences are fully supported by citations and only 74.5\% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Zhang, Tianyi and Liang, Percy},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09848 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: 25 pages, 12 figures},
}

@article{liang2022helm,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@misc{leinonen2023comparing,
      title={Comparing Code Explanations Created by Students and Large Language Models}, 
      author={Juho Leinonen and Paul Denny and Stephen MacNeil and Sami Sarsa and Seth Bernstein and Joanne Kim and Andrew Tran and Arto Hellas},
      year={2023},
      eprint={2304.03938},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{sandoval2023lost,
      title={Lost at {C}: A User Study on the Security Implications of Large Language Model Code Assistants}, 
      author={Gustavo Sandoval and Hammond Pearce and Teo Nys and Ramesh Karri and Siddharth Garg and Brendan Dolan-Gavitt},
      year={2023},
      eprint={2208.09727},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{wan2023poisoning,
      title={Poisoning Language Models During Instruction Tuning}, 
      author={Alexander Wan and Eric Wallace and Sheng Shen and Dan Klein},
      year={2023},
      eprint={2305.00944},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{nair2023dera,
      title={DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents}, 
      author={Varun Nair and Elliot Schumacher and Geoffrey Tso and Anitha Kannan},
      year={2023},
      eprint={2303.17071},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2023teaching,
      title={Teaching Large Language Models to Self-Debug}, 
      author={Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},
      year={2023},
      eprint={2304.05128},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhuo2023large,
      title={Large Language Models Are State-of-the-Art Evaluators of Code Generation}, 
      author={Terry Yue Zhuo},
      year={2023},
      journal={arXiv preprint arXiv:2304.14317},
}


@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@inproceedings{holtzman2020curious,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}
@article{lacoste2019quantifying,
  title={Quantifying the carbon emissions of machine learning},
  author={Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  journal={arXiv preprint arXiv:1910.09700},
  year={2019}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{liu2023correct,
    title={Is Your Code Generated by {ChatGPT} Really Correct? {Rigorous} Evaluation of Large Language Models for Code Generation},
    author={Jiawei Liu and Chunqiu Steven Xia and Yuyao Wang and Lingming Zhang},
    year={2023},
    eprint={2305.01210},
    archivePrefix={arXiv},
    primaryClass={cs.SE},
    url={https://arxiv.org/abs/2305.01210},
  journal={arXiv preprint arXiv:2305.01210},
}

@inproceedings{mitchell2019modelcard,
  author       = {Margaret Mitchell and
                  Simone Wu and
                  Andrew Zaldivar and
                  Parker Barnes and
                  Lucy Vasserman and
                  Ben Hutchinson and
                  Elena Spitzer and
                  Inioluwa Deborah Raji and
                  Timnit Gebru},
  editor       = {danah boyd and
                  Jamie H. Morgenstern},
  title        = {Model Cards for Model Reporting},
  booktitle    = {Proceedings of the Conference on Fairness, Accountability, and Transparency,
                  FAT* 2019, Atlanta, GA, USA, January 29-31, 2019},
  pages        = {220--229},
  publisher    = {{ACM}},
  year         = {2019},
  url          = {https://doi.org/10.1145/3287560.3287596},
  doi          = {10.1145/3287560.3287596},
  timestamp    = {Fri, 26 Mar 2021 16:46:37 +0100},
  biburl       = {https://dblp.org/rec/conf/fat/MitchellWZBVHSR19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}

@misc{thompson-2022-copilot-stats,
title={How an AI became my code-writing Genie},
url={https://www.wired.com/story/openai-copilot-autocomplete-for-code/},
journal={Wired},
publisher={Conde Nast},
author={Thompson, Clive},
year={2022},
month={Mar}}

@inproceedings{hindle2012naturalness,
  title={On the naturalness of software},
  author={Hindle, Abram and Barr, Earl T and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
  booktitle={2012 34th International Conference on Software Engineering (ICSE)},
  pages={837--847},
  year={2012},
  organization={IEEE}
}

@inproceedings{kanade2020learning,
  title={Learning and evaluating contextual embedding of source code},
  author={Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
  booktitle={International conference on machine learning},
  pages={5110--5121},
  year={2020},
  organization={PMLR}
}

@article{open-science,
  title = {Open Science Is a Research Accelerator},
  author = {Woelfle, Michael and Olliaro, Piero and Todd, Matthew H.},
  year = {2011},
  month = oct,
  journal = {Nature Chemistry},
  volume = {3},
  number = {10},
  pages = {745--748},
  publisher = {{Nature Publishing Group}},
  issn = {1755-4349},
  doi = {10.1038/nchem.1149},
  urldate = {2023-06-17},
  copyright = {2011 Springer Nature Limited},
  langid = {english},
  keywords = {Medicinal chemistry,Total synthesis}
}

@article{wang2022execution,
  title={Execution-based evaluation for open-domain code generation},
  author={Wang, Zhiruo and Zhou, Shuyan and Fried, Daniel and Neubig, Graham},
  journal={arXiv preprint arXiv:2212.10481},
  year={2022}
}

@article{muennighoff2023octopack,
  title={OctoPack: Instruction Tuning Code Large Language Models},
  author={Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and von Werra, Leandro and Longpre, Shayne},
  journal={arXiv preprint arXiv:2308.07124},
  year={2023}
}