%Please add the following packages if necessary:
%\usepackage{booktabs, multirow} % for borders and merged ranges
%\usepackage{soul}% for underlines
%\usepackage[table]{xcolor} % for cell colors
%\usepackage{changepage,threeparttable} % for wide tables
%If the table is too wide, replace \begin{table}[!htp]...\end{table} with
%\begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering\begin{threeparttable}[!htb]...\end{threeparttable}\end{adjustwidth}

% without average rank column
\begin{table}
\centering
% \caption{Generated by Spread-LaTeX}\label{tab: }
% https://docs.google.com/spreadsheets/d/1Kra9d2UdfvdW3AoO3oqdAlOgQocWXqpqf3jtn20BvoU/edit?usp=sharing
\scriptsize
% \begin{tabularx}{1.06\linewidth}{Xccccccccccc}
\begin{tabular}{p{0.15\textwidth}ccccccccccc}
\toprule
% &\multirow{3}{*}{\textbf{Synthetic\\Reasoning:\\Abstract}} &\multirow{3}{*}{\textbf{Synthetic\\Reasoning:\\NL}} &\textbf{bAbI} &\textbf{Dyck} &\textbf{GSM8K} &\textbf{MATH} & \multirow{3}{*}{\textbf{MATH (chain-of-thought)}} &\textbf{LSAT - EM} &\textbf{LegalSupport} &Average Rank \\
% &\thead{\textbf{Synthetic\\Reasoning:\\Abstract}} &\thead{\textbf{Synthetic\\Reasoning:\\NL}} &\textbf{bAbI} &\textbf{Dyck} &\textbf{GSM8K} &\textbf{MATH} & \thead{\textbf{MATH\\(CoT)}} &\thead{\textbf{LSAT\\EM}} &\thead{\textbf{Legal\\Support}} & \thead{Average\\Rank} \\
\textbf{Model} & \thead{\textbf{Weights}\\\textbf{Released}} &\thead{\textbf{Synth.}\\\textbf{Reason.}\\\textbf{AS}} &\thead{\textbf{Synth.}\\\textbf{Reason:}\\\textbf{NL}} &\thead{\textbf{bAbI}} &\thead{\textbf{Dyck}} &\thead{\textbf{GSM8K}} &\thead{\textbf{MATH}} & \thead{\textbf{MATH}\\\textbf{(CoT)}} &\thead{\textbf{LSAT}} &\thead{\textbf{Legal}\\\textbf{Support}} \\
%\cmidrule(r){1-1} \cmidrule(l){2-11}
\midrule
code-davinci-002 & &\textbf{0.54} &0.684 &\textbf{0.686} &0.805 &\textbf{0.568} &\textbf{0.41} &0.433 &--- &--- \\
text-davinci-003 & &0.502 &\textbf{0.734} &0.653 &0.751 &0.506 &0.39 &\textbf{0.449} &\textbf{0.233} &\textbf{0.622} \\
Luminous Supreme (70B) & &0.312 &--- &0.504 &0.729 &0.112 &0.149 &0.057 &0.212 &0.53 \\
StarCoderBase (15.5B) & \checkmark &0.44 &0.21 &0.504 &\textbf{0.854} &0.084 &0.151 &0.07 &0.19 &0.532 \\
Cohere Command Beta (52.4B) & &0.243 &0.245 &0.473 &0.421 &0.138 &0.133 &0.075 &0.229 &0.606 \\
J1-Jumbo v1 (178B) & &0.263 &0.174 &0.543 &0.445 &0.054 &0.089 &0.033 &0.232 &0.484 \\
J1-Grande v2 beta (17B) & &0.286 &0.139 &0.47 &0.617 &0.096 &0.127 &0.068 &0.191 &0.562 \\
code-cushman-001 & &0.341 &0.164 &0.481 &0.451 &0.049 &0.099 &0.072 &--- &--- \\
OPT (175B) & \checkmark &0.225 &0.248 &0.507 &0.494 &0.04 &0.065 &0.026 &0.22 &0.532 \\
GPT-NeoX (20B) & \checkmark & 0.204 &0.167 &0.468 &0.747 &0.053 &0.141 &0.071 &0.191 &0.515 \\
BLOOM (176B) & \checkmark &0.304 &0.197 &0.447 &0.545 &0.095 &0.043 &0.055 &0.209 &0.543 \\
GLM (130B) & \checkmark &0.252 &0.254 &0.443 &0.549 &0.061 &0 &0.059 &0.193 &0.451 \\
UL2 (20B) & \checkmark &0.205 &0.217 &0.501 &0.14 &0.024 &0 &0 &0.207 &0.506 \\
OPT (66B) & \checkmark &0.193 &0.213 &0.408 &0.471 &0.018 &0.048 &0.029 &0.175 &0.527 \\
YaLM (100B) & \checkmark &0.056 &0.061 &0.346 &0.633 &0 &0 &0 &0.23 &0.484 \\
T5 (11B) & \checkmark &0.196 &0.101 &0.412 &0.347 &0.023 &0 &0 &0.159 &0.558 \\
\bottomrule
% \end{tabularx}
\end{tabular}
\caption{Model results on natural language reasoning tasks in the HELM benchmark, with models ordered by their average rank on the tasks. We use ``---'' when a model was not evaluated on a given metric, or has runtime errors logged in HELM (e.g., ``unmapped prediction'' for the code-davinci-002 and code-cushman-001 models on LSAT and Legal Support). StarCoder generally substantially outperforms other models with released weights, and often outperforms much larger models.}
\label{tab:helm_results}
\end{table}