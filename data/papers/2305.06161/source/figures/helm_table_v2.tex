%Please add the following packages if necessary:
%If the table is too wide, replace \begin{table}[!htp]...\end{table} with
%\begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering\begin{threeparttable}[!htb]...\end{threeparttable}\end{adjustwidth}

% without average rank column
\begin{table}
\centering
% \caption{Generated by Spread-LaTeX}\label{tab: }
% https://docs.google.com/spreadsheets/d/1Kra9d2UdfvdW3AoO3oqdAlOgQocWXqpqf3jtn20BvoU/edit?usp=sharing
\scriptsize
\setlength{\tabcolsep}{3pt}
% \begin{tabularx}{1.06\linewidth}{Xccccccccccc}
\resizebox{\textwidth}{!}{
\begin{tabular}{p{0.14\textwidth}rcccccccccccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Size}} &
% \thead{\textbf{Weights}\\\textbf{Released}}
\thead{\textbf{Open}\\\textbf{Access}}
&\thead{\textbf{Synth.}\\\textbf{Reason.}\\\textbf{(AS)}} &\thead{\textbf{Synth.}\\\textbf{Reason.}\\\textbf{(NL)}} &\thead{\textbf{bAbI}} &\thead{\textbf{Dyck}} &\thead{\textbf{GSM8K}} & {\textbf{MATH}} & \thead{\textbf{MATH}\\\textbf{(CoT)}} &\thead{\textbf{LSAT}} &\thead{\textbf{Legal}\\\textbf{Support}} \\
\midrule
code-davinci-002 & 175B & &\textbf{54.0} &68.4 &\textbf{68.6} &80.5 &\textbf{56.8} &\textbf{41.0} &43.3 &--- &--- \\
text-davinci-003 & 175B & & 50.2 &\textbf{73.4} &65.3 &75.1 &50.6 &39.0 &\textbf{44.9} &\textbf{23.3} &\textbf{62.2} \\
Luminous Supreme & 70B & & 31.2 &--- &50.4 &72.9 &11.2 &14.9 &5.7 &21.2 &53.0 \\
StarCoderBase & 15.5B & \checkmark &44.0 &21.0 &50.4 &\textbf{85.4} & 8.4 &15.1 &7.0 &19.0 &53.2 \\
Cohere Command Beta & 52.4B & &24.3 &24.5 &47.3 &42.1 &13.8 &13.3 &7.5 &22.9 &60.6 \\
J1-Jumbo v1 & 178B & &26.3 &17.4 &54.3 &44.5 &5.4 &8.9 &3.3 &23.2 &48.4 \\
J1-Grande v2 beta & 17B & &28.6 &13.9 &47.0 &61.7 &9.6 &12.7 &6.8 &19.1 &56.2 \\
code-cushman-001 & 12B & &34.1 &16.4 &48.1 &45.1 &4.9 &9.9 &7.2 &--- &--- \\
OPT & 175B & \checkmark &22.5 &24.8 &50.7 &49.4 &4.0 &6.5 &2.6 &22.0 &53.2 \\
GPT-NeoX & 20B & \checkmark & 20.4 &16.7 &46.8 &74.7 &5.3 &14.1 &7.1 &19.1 &51.5 \\
BLOOM & 176B & \checkmark &30.4 &19.7 &44.7 &54.5 &9.5 &4.3 & 5.5 &20.9 &54.3 \\
GLM  & 130B & \checkmark &25.2 &25.4 &44.3 &54.9 &6.1 &0 &5.9 &19.3 &45.1 \\
UL2  & 20B & \checkmark &20.5 &21.7 &50.1 &14.0 &2.4 &0 &0 &20.7 &50.6 \\
OPT  & 66B & \checkmark &19.3 &21.3 &40.8 &47.1 &1.8 &4.8 &2.9 &17.5 &52.7 \\
YaLM & 100B & \checkmark &5.6 &6.1 &34.6 &63.3 &0 &0 &0 &2.3 &48.4 \\
T5  & 11B & \checkmark &19.6 &10.1 &41.2 &34.7 &2.3 &0 &0 &15.9 &55.8 \\
\bottomrule
\end{tabular}
}
% qian: change weights released to open access to keep consistency with previous mention
\caption{Model results on natural language reasoning tasks in the HELM benchmark, with models ordered by their average rank on the tasks. We use ``---'' when a model was not evaluated on a given metric, or has runtime errors logged in HELM (e.g., ``unmapped prediction'' for the code-davinci-002 and code-cushman-001 models on LSAT and Legal Support). StarCoder generally substantially outperforms other open-access models, and often outperforms much larger models.}
\label{tab:helm_results}
\end{table}