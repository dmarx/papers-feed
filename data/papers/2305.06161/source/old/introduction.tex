\section{Introduction}

Large Language Models \citep[LLMs;][]{brown2020language, chen2021codex,chowdhery2022palm,zhang2022opt,openai2023gpt4}, such as OpenAI's ChatGPT, are taking the world by storm. Within a mere two months of launching, ChatGPT has amassed over 100~million users, making it the fastest-growing internet application in history~\citep{guardian2023chatgpt}. Similarly, Microsoftâ€™s Copilot, an LLM designed for coding applications, has attracted over 1~million~professional developers~\citep{euronews2023copilot} and can accelerate coding tasks by up to~55\%~\citep{kalliamvakou2022copilot}. 
Generative AI models, and LLMs in particular, are exciting new productivity-boosting tools, which are predicted to significantly impact the workforce in the coming years~\citep{eloundou2023gpts, stanford2022foundation,wef2023futureofjobs}. Although this work focuses on LLMs trained on code (Code LLMs), many growing techno-social concerns are being raised within the broader context of generative AI technology.  

Some safety concerns, such as generating false information or amplifying existing biases, are being addressed after training by using various techniques to better ``align'' the LLM with human values~\citep{stiennon2020learning,bai2022training,perez2022red}. Other legal and ethical concerns already arise during the pre-training phase, specifically regarding the rights of content creators whose public data is used to train the language model. This data is subject to copyright laws in many jurisdictions, including the U.S.\ and E.U. It has been questioned whether machine learning models trained on such data fall under exemptions such as the fair-use doctrine in the U.S.~\citep{kuhn2022copilot,butterick2022copilot,rothchild2022copyright}. It is likely considered fair use when a model generates novel content that is not in the training set, as it is a transformative use of the copyrighted material~\citep{lemley2020fair}. However, if the model produces output similar to copyrighted data, particularly in scenarios that affect the economic market
of the content creators, fair use may no longer apply~\citep{levendowski2018copyright}. \citet{henderson2023foundation}, therefore, suggest LLM developers should provide additional tools to ensure these models comply with current copyright laws. It is important to mention that these legal issues are not only the subject of scholarly debates: lawsuits have already been filed against GitHub Copilot~\citep{DOE1vGitHub} as well as Stable Diffusion~\citep{stablediffusion_lawsuit}, a text-to-image tool from Stability~AI. 

Concerns about personal information led Italy to temporarily ban ChatGPT and launch an ongoing investigation into OpenAI's compliance with the E.U.'s General Data Protection Regulation~(GDPR)~\citep{bbc2023chatgpt_ban}. According to these regulations~\citep{eu2018gdpr,lomas2023gdpr_llms}, organizations that process personal information must have a valid legal basis. These laws could potentially affect LLM developers who gather vast amounts of public data from the internet, which may include personal information. Obtaining explicit consent from data creators is difficult at this scale, and it is uncertain whether other legal grounds exist for processing this personal information. Moreover, even with a valid legal basis, GDPR mandates that data processors inform individuals as to how their data is being processed and provide data access controls, such as the right to have your data deleted or to modify erroneous data. This would require LLM providers to be transparent about the data they have collected and provide tooling for individuals to inspect their data and have the possibility to delete it.    

The lack of transparency and openness surrounding the development processes of generative AI models has also raised concerns in the scientific community. Some of the best-performing LLMs, such as Google's PaLM~\citep{chowdhery2022palm} and DeepMind's Chinchilla~\citep{hoffmann2022training}, were developed in a completely closed manner, restricting model access to researchers within their respective organizations. Although OpenAI and other AI startups have made their LLMs available to the general public, they have done so through a paid API service and without sharing all the details regarding the development process. While API access allows researchers to experiment with these models, it limits their ability to research LLM safety and alignment~\citep{perez2022red} and inspect the models' inner workings~\citep{olsson2022context}. Additionally, the high development costs make it nearly impossible for academic institutions to develop these models from scratch, which has created anxiety among academic researchers about whether they can meaningfully contribute to new AI breakthroughs~\citep{togelius2023choose}.

% dfried: move this to related work
Other research projects have shared their developed LLMs publicly. In this work, we refer to open-access LLMs when the model weights are publicly available. However, we stress that the level of openness still varies across these open-access models, specifically regarding the extent to which they have made their training data and filtering methods available. For example, Google released the model weights of T5~\citep{raffel2020exploring} and UL2~\citep{tay2022unifying} under a permissive license and released the corresponding C4 training set~\citep{raffel2020exploring}. On the other hand, Salesforce released the model weights of CodeGen-Mono~\citep{nijkamp:codegen} under a permissive license, but they did not make the proprietary Python training set available. OPT~\citep{zhang2022opt} and LLaMA~\citep{touvron2023llama} also do not provide access to the training data and place additional restrictions on the distribution of the model weights (non-commercial use only). Note that the development of these models all took place within their respective companies and was not accessible to external researchers. 

In contrast, other LLM projects have embraced a fully open approach. The BigScience research workshop~\citep{bigscience_workshop_2022} was an open scientific collaboration~\citep{akiki-bigscience-22} with hundreds of researchers from diverse backgrounds and countries working together to release BLOOM, a multi-lingual LLM~\citep{scao2022bloom,muennighoff2022crosslingual}. Similarly, EleutherAI, a grassroots-turned-nonprofit research initiative, has released several open-access LLMs, including GPT-NeoX~\citep{black2022gpt}, GPT-J~\citep{wang2021gpt}, and Pythia~\citep{biderman2023pythia}, as well as the associated training data~\citep{gao2020pile}. 


% dfried: shorten, with full text in discussion or related work?
\citet{solaiman2023gradient} explains how the degree of openness in the LLM development process is connected to the potential risks associated with a model release. When systems are developed in a fully closed manner, it's more likely that power gets concentrated among high-resourced organizations, and the small development team may not fully comprehend the impact and long-term consequences of the model being deployed. In addition, closed-development systems are often less auditable by external experts and can impede scientific progress since researchers cannot build upon each other's work. On the other hand, fully open development allows for community research, democratizes access to the models, and enables full audits throughout the whole development process. However, without appropriate guardrails, open LLM development poses a higher risk of misuse, as increased model access also increases the likelihood of harm caused by the model. Even though a released API can be shut down, once the model weights are released, it is nearly impossible to retract them.  Therefore, it is crucial to discuss and develop responsible AI practices as part of the open development of LLMs.

In light of these considerations, the BigCode project was launched as an open-scientific collaboration focusing on the responsible development of LLMs for code. BigCode is co-stewarded by Hugging Face and ServiceNow and has brought together more than 600 members from diverse academic institutes and industry labs. The community has several working groups that focus on topics such as collecting datasets, implementing fast inference methods, creating an evaluation suite, and developing ethical best practices for these models. The community previously released The Stack~\citep{Kocetkov2022TheStack}, a 6.4~TB dataset of permissively licensed source code in 384~programming languages, and included 54~GB of GitHub issues and repository-level metadata in the v1.2~version of the dataset. The Stack comes with ``Am I in The Stack'', a governance tool for developers to check whether their source code is part of the dataset, and an opt-out process for those who wish to have their code removed from the dataset. In December 2022, the BigCode community also released SantaCoder~\citep{allal2023santacoder}, a strong-performing 1.1B~parameter model trained on Java, JavaScript, and Python code from The Stack. 

In this technical report, we describe our efforts to develop StarCoder and StarCoderBase, two 15.5B~parameter models trained on permissively licensed data from The Stack. We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub issues, Git commits, and Jupyter notebooks. We fine-tuned StarCoderBase on another 35B Python tokens, leading to the StarCoder model. Both StarCoder models come with a novel combination of architectural features, such as an 8K context length~\citep{dao2022flashattention}, infilling capabilities through Fill-in-the-Middle~\citep[FIM;][]{bavarian2022fim}, and fast large-batch inference through Multi-Query-Attention~\citep[MQA;][]{shazeer2019mqa}. We present an extensive evaluation of the StarCoder models and release a demo along with an integrated attribution tool that can help users locate model generations that may have been copied from the training set. Overall, our contributions can be summarized as follows.
\begin{itemize}
    \item We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming languages that support a novel combination of capabilities and architectural features unavailable in other open Code LLMs.
    \item We perform the most comprehensive evaluation of Code LLMs to date using a diverse set of benchmarks~\citep{Lai2022DS1000,cassano2022multiple,pearce2022copilotsec,fried2022incoder,yee:typeweaver,austin2021program,chen2021codex,bigcode-evaluation-harness,hendrycks2020mmlu,reddy2019coqa,cobbe2021training, nadeem_stereoset_2021,gehman_realtoxicityprompts_2020,liang2022helm}, and show that:
    \begin{itemize}
        \item \emph{StarCoder outperforms every open LLM for code that supports multiple programming languages}~\citep{nijkamp:codegen,qinkai:codegeex};
        \item \emph{StarCoder matches or outperforms the OpenAI code-cushman-001 model};
        \item When fine-tuned on Python, \emph{StarCoder substantially outperforms existing LLMs that are also fine-tuned on Python}; and
        \item Leveraging its 8K token context, StarCoder can be prompted to behave as a \emph{virtual technical assistant without instruction-tuning or RLHF}.
    \end{itemize}
    \item We take important steps towards a safe open model release: 
    \begin{itemize}
        \item We release StarCoder under an \emph{OpenRAIL-M license agreement}, which enables royalty-free access, use, and distribution of the model while embedding a set of use restrictions in identified critical scenarios. We have worked on a version of the license agreement that: (i) is more commercially viable for companies wishing to use and distribute the model and (ii) promotes transparency and understanding through the sharing of AI documentation such as model cards~\citep{mitchell2019modelcard};
        \item We incorporate a \emph{new attribution tool into the VSCode demo that can help users detect and locate model generations that may have been copied from the training set}. This is achieved through a two-step process that involves a lightweight membership check followed by a search over a BM25 index (Section \ref{sec:attribution_tool}); and 
        \item \emph{We have significantly improved the PII redaction pipeline by collecting a PII dataset containing 12,000 files with 22,950 annotated entities}. We fine-tuned our own encoder model (StarEncoder) on this dataset, resulting in a robust PII detection model (Section \ref{sec:PII}). 
        % \item We introduce a novel governance card summarizing all our governance efforts for StarCoder. 
    \end{itemize}
\end{itemize}