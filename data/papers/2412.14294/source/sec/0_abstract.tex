\input{preamble}
% \setlength{\parskip}{0pt}

\begin{abstract}

\vspace{-3mm}
\noindent We propose a novel block for video modelling. It relies on a time--space--channel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture \emph{\ssm} performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, our model is causal and outperforms or is on par with a pure attention model ViViT\nobreakdash-L on large scale video datasets (SSv2, Kinetics400), while having $3\times$ less parameters, $12\times$ smaller memory footprint, and $5\times$ lower FLOPs count.
%
Code and checkpoints are available online.\footnote{ \url{https://github.com/google-deepmind/trecvit}}
\end{abstract}