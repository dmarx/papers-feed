\input{preamble}

\section{Related work}
\label{sec:related}

\textbf{Transformers for Video.}
Proposed initially as language models, transformers~\cite{vaswani2017attention} have quickly become the dominant architecture across multiple modalities (images, audio, video).  Transformer blocks alternate between a spatial mixing block represented by self-attention and a (feature) channel mixing block, represented by a gated MLP. Given that the self-attention layer treats the input tokens as \emph{a set}, positional encodings must be used in order to specify the location of each token. It also means that no parsing order is needed, unlike the case with RNNs. Vision transformers (ViT)~\cite{dosovitskiy2021an,Liu_2021_ICCV} split images into a fixed number of patches that are projected into an embedding space to obtain \textit{tokens} and these are then processed by a regular transformer. 
Several works extended ViT to video, \eg by replacing the regular image patches with spatio-temporal ones. The main challenge with transformers, particularly for video, is the quadratic complexity in the number of input tokens. Multiple approaches have been proposed to address this: \eg factorisations of the self-attention operation~\citep{vivit,pmlr-v139-bertasius21a}, iterative attention~\cite{jaegle2022perceiver}, sparse sampling of the input frames~\cite{tubevit}, and  distributed self-attention operations across different devices~\cite{liu2024ringattention}. 
Our proposed model uses a novel space-time factorisation, where the temporal dimension is handled by LRUs and the spatial dimension by self-attention. 

As these models scale successfully to large number of parameters, their data needs are efficiently met by using self-supervised pre-training like masked autoencoding (MAE)~\cite{tong2022videomae} or contrastive learning~\cite{pmlr-v235-zhao24f}. Due to the factorisation used in our architecture, using such pre-training strategies is straightforward and we include successful experiments with MAE pre-training in Section~\ref{sec:experiments}.

\vspace{1mm}

\noindent\textbf{SSM, a type of Linear Recurrent Model.} While transformers~\cite{vaswani2017attention} can be efficiently parallelised during training, at inference they need to pay a quadratic cost in the sequence length. On the other hand, recurrent networks~\cite{elman1990finding,hochreiter1997long,Mikolov2010,bahdanau2014neural,sutskever2014sequence} are compact and efficient at inference but slow at training. State Space Models (SSMs)~\cite{gu2020hippo, gu2021efficiently, orvieto2023resurrecting}, a particular type of linear recurrent networks, have recently been proposed as an answer to the scalability problem of RNNs, and have shown strong performance in language and other long-range dependencies tasks~\cite{de2024griffinmixinggatedlinear, gu2023mamba}. 

SSMs, like S4~\cite{gu2021efficiently}, S4D~\cite{gu2022parameterization}, or Mamba~\cite{gu2023mamba} have been introduced as particular discretizations of a continuous time linear system. On the other hand, the linear recurrent unit (LRU)~\cite{orvieto2023resurrecting} was designed by identifying the minimal set of changes to a vanilla RNN~\cite{elman1990finding} that allows it to obtain the same key properties as the S4D architecture~\citep{gu2021efficiently}; we discuss the LRU in more detail in Section~\ref{sec:model}. Improving on the LRU, the gated LRU~\cite{de2024griffinmixinggatedlinear} introduces gating mechanisms similar to LSTM or GRU architectures, to filter the input sequence, while the recurrent gate controls the rate of the information decay. Importantly, different from LSTM or GRU, these gates do not depend on the previous state, which would prevent parallelisation at training time.
In our work, we use gated LRUs, but we expect similar results to be obtained when using other gated SSM blocks like Mamba within our factorisation.

\vspace{1mm}


\noindent\textbf{SSMs for Video.} While SSMs have mostly been explored in language, several architectures like S4 and Mamba have also been adapted to image and video modalities~\cite{surveyvmamba}. ViS4mer~\cite{Islam2022LongMC} uses a ViT image encoder to process videos frame by frame, and integrates their representations over time using S4 blocks at the top.  TranS4mer~\cite{10204597} uses self-attention over short clips and integrates these with gated S4 blocks. More recently, the Mamba architecture was extended to images and videos by having it process a flattened 1D sequence of image or video patches. This requires defining a processing order for the patches, and different orders have been proposed, \eg bidirectional and following a column or row order~\cite{zhu2024vision,li2024videomambastatespacemodel}. As opposed to these Mamba-based architectures, our factorisation naturally uses the arrow-of-time to decide the scanning order, resulting in a causal model. Another important benefit of our hybrid architecture is that we can initialise the ViT blocks with strong existing pre-trained weights. This leads to strong performance even at larger scale, as opposed to VideoMamba~\cite{li2024videomambastatespacemodel} where the authors report severe overfitting issues, requiring distillation from smaller models when training in a supervised fashion or distillation from CLIP features~\cite{clip} for self-supervised training.    