\vspace{-10mm}
\section{Introduction}
\label{sec:intro}
\vspace{-2mm}

Video understanding requires low-level scene understanding (e.g. how objects move) and high-level reasoning (e.g. causal relations between events) over a signal that is high-dimensional, can be noisy, and contains high correlations and redundancies in both spatial and temporal dimensions. Efficient video modelling needs high-capacity models that can represent the sheer diversity and richness of real-world videos, while having reasonable compute and memory footprint both at training and during inference time. Convolutional neural networks~\citep{i3d,slowfast} have been a successful family of models for video, but their scaling capabilities (in both data and parameters) are limited due to their inductive biases (locality, invariance). Recurrent neural networks, e.g.~\citep{SrivastavaLSTM,patraucean2015spatio} have some desirable properties for video modelling (constant inference cost per timestep independent of the length of the video), but they are slow to train due to their sequential nature and have difficulties in learning over long complex sequences. Transformers~\citep{vaswani2017attention} have emerged as a very powerful family of models for all modalities, with impressive scaling capabilities. However, they have a significant memory footprint and latency due to the quadratic complexity of the self-attention operation.
%
Recently, a new family of linear recurrent networks~\citep{gu2020hippo,gu2023mamba, orvieto2023resurrecting,Beck2024xLSTM}, referred to as State Space Models (SSMs), has emerged as an answer to the quadratic complexity of self-attention and the slow training of RNNs, with promising results for vision and language ~\cite{de2024griffinmixinggatedlinear,li2024videomambastatespacemodel}. 

In this paper, we propose a hybrid architecture that combines the best of all worlds. It alternates gated linear recurrent units (LRUs)~\citep{de2024griffinmixinggatedlinear} applied over time, with self-attention blocks over space, and MLP over feature channels. As opposed to space and channels, time has a natural order (\textit{"arrow-of-time"}) that LRUs can implicitly and efficiently model with $O(N)$ complexity in the number of input frames at training time and $O(1)$ complexity at inference time, making it possible to process videos that extend even indefinitely. Space, on the other hand, has a fixed limited dimension, for which the quadratic cost of self-attention is more accessible. From a practical perspective, using self-attention over space allows us to naturally process in parallel all the pixels of a given frame, without having to commit to a particular scanning order~\cite{li2024videomambastatespacemodel}, making better use of  hardware when parallel resources are available.
 
To further limit the self-attention cost, we use spatial patches  as introduced in the successful ViT~\citep{dosovitskiy2021an} model. But, compared to existing video transformer models, \eg ViViT~\citep{vivit}, the patches do not have a fixed temporal extent. Instead, the embeddings of the spatial patches are integrated continuously into the hidden state of the gated LRUs, providing \emph{persistent} memory of 
the entire temporal sequence up to the current frame. Furthermore, similar to convolutional networks, the parameters of the LRUs are shared over space, preventing the number of parameters from exploding as the resolution of the video increases. 

We refer to the resulting model as \emph{T}emporal \emph{Rec}urrent \emph{Vi}deo \emph{T}ransformer~(\ssm).  \ssm\ is highly flexible and can address various video understanding tasks, both sparse (\eg video classification) and dense (\eg point tracking), trained in a supervised or self-supervised manner, \eg using masked auto-encoding. In all our experiments, we use a causal setup that respects the arrow of time, so the model is suitable for any downstream applications, from \eg video classification where we have offline access to the videos, to \eg robotics, where online processing is required. Overall, our model is significantly more efficient in both memory footprint and FLOPs compared to vanilla transformers.   

\par \noindent \textbf{Paper structure}: We discuss related works in more depth in section~\ref{sec:related} and we introduce the proposed model in section~\ref{sec:model}. We discuss training regimes and analyse efficiency when comparing to baselines in section~\ref{sec:training}. In section~\ref{sec:experiments}, we present extensive experiments for various training regimes, different tasks and datasets. We conclude in section~\ref{sec:conclusion} with a discussion of the limitations of the proposed approach and directions for future work.  
