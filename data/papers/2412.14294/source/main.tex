% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{6504} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\title{\ssm: A Recurrent Video Transformer}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Viorica P\u{a}tr\u{a}ucean$^{*}$ \and Xu Owen He$^{*}$ \and Joseph Heyward$^{*}$ \and Chuhan Zhang$^{*}$ \and Mehdi S.\ M.\ Sajjadi \and George-Cristian Muraru \and Artem Zholus \and Mahdi Karami \and Ross Goroshin \and Yutian Chen \and Simon Osindero \and Jo√£o Carreira \and Razvan Pascanu \\[3mm]
\small{Google DeepMind} \\
\small{Corresponding author: \texttt{viorica@google.com}, $^{*}$core contributor}
}


\begin{document}
% \maketitle
\vspace{-2em}
\vspace{-2mm}
% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
% % \begin{figure*}[!ht]
% \begin{subfigure}{0.6\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{img/revitmodel.pdf}
%     % \caption{\ssm\ architecture}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{0.34\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{img/revitblock_tall.pdf} 
%     % \caption{\ssm\ block}
% \end{subfigure}
% % \vspace{-1mm}
% \captionof{figure}{\textbf{Left:} \ssm\ architecture. Each video frame is divided into non-overlapping patches that are linearly projected into a token embedding space. We then add a learnt spatial positional encoding. The tokens are passed through gated linear recurrent units (LRUs) that share parameters across space. The outputs of the recurrent blocks are then processed by a ViT block. The recurrent operation followed by ViT is repeated N times. \textbf{Right:} \ssm\ block. The input is a batch of videos, each frame with N tokens. We apply recurrent units over \textit{temporal tubes} to integrate information over time, and self-attention and MLP across tokens within each frame. Note that the recurrent units share parameters, but the information is not mixed across temporal tubes. Similarly, the ViT blocks share parameters, but the information is not mixed across frames.}
% \vspace{1em}
% \vspace{2mm}
% \label{fig:ssmvit}
% % \end{figure*}
% }]
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center} % Center the minipages
\begin{minipage}{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/revitmodel.pdf}
\end{minipage}%
\hfill
\begin{minipage}{0.34\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/revitblock_tall.pdf}
\end{minipage}
\vspace{-2mm}
\captionof{figure}{\textbf{Left:} \ssm\ architecture. Each video frame is divided into non-overlapping patches that are linearly projected into a token embedding space. We then add a learnt spatial positional encoding. The tokens are passed through gated linear recurrent units (LRUs) that share parameters across space. The outputs of the recurrent blocks are then processed by a ViT block. The recurrent operation followed by ViT is repeated N times. \textbf{Right:} \ssm\ block. The input is a batch of videos, each frame with N tokens. We apply recurrent units over \textit{temporal tubes} to integrate information over time, and self-attention and MLP across tokens within each frame. Note that the recurrent units share parameters, but the information is not mixed across temporal tubes. Similarly, the ViT blocks share parameters, but the information is not mixed across frames.}
% \vspace{1em}
% \vspace{2mm}
\label{fig:ssmvit}
\end{center}

\vspace{1em} 
}] 


\input{sec/0_abstract}    
\input{sec/1_intro}
\input{sec/2_relatedwork}
\input{sec/3_method}
\input{sec/4_experiments}
\input{sec/5_conclusion}
\section*{Acknowledgements}
We would like to thank Caglar Gulcehre, Daniel Zoran, Dima Damen, and Andrew Zisserman for their insightful feedback throughout this project.
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
\input{sec/X_suppl}

\end{document}
