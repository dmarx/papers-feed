\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arnab et~al.(2021)Arnab, Dehghani, Heigold, Sun, Lučić, and
  Schmid]{vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and
  Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock In \emph{2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 6816--6826, 2021.

\bibitem[Authors()]{jaxstatix}
The~Jax Authors.
\newblock Jax documentation.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Beck et~al.(2024)Beck, Pöppel, Spanring, Auer, Prudnikova, Kopp,
  Klambauer, Brandstetter, and Hochreiter]{Beck2024xLSTM}
Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra
  Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp
  Hochreiter.
\newblock xlstm: Extended long short-term memory.
\newblock \emph{arXiv preprint arXiv:2405.04517}, 2024.

\bibitem[Bertasius et~al.(2021)Bertasius, Wang, and
  Torresani]{pmlr-v139-bertasius21a}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pages 813--824. PMLR, 2021.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem[Carreira and Zisserman(2017{\natexlab{a}})]{Carreira_2017_CVPR}
Joao Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2017{\natexlab{a}}.

\bibitem[Carreira and Zisserman(2017{\natexlab{b}})]{i3d}
João Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In \emph{2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 4724--4733, 2017{\natexlab{b}}.

\bibitem[De et~al.(2024)De, Smith, Fernando, Botev, Cristian-Muraru, Gu,
  Haroun, Berrada, Chen, Srinivasan, Desjardins, Doucet, Budden, Teh, Pascanu,
  Freitas, and Gulcehre]{de2024griffinmixinggatedlinear}
Soham De, Samuel~L. Smith, Anushan Fernando, Aleksandar Botev, George
  Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen,
  Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden,
  Yee~Whye Teh, Razvan Pascanu, Nando~De Freitas, and Caglar Gulcehre.
\newblock Griffin: Mixing gated linear recurrences with local attention for
  efficient language models, 2024.

\bibitem[Doersch et~al.(2022)Doersch, Gupta, Markeeva, Continente, Smaira,
  Aytar, Carreira, Zisserman, and Yang]{doersch2022tapvid}
Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria~Recasens Continente, Lucas
  Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang.
\newblock {TAP}-vid: A benchmark for tracking any point in a video.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem[Doersch et~al.(2023)Doersch, Yang, Vecerík, Gokay, Gupta, Aytar,
  Carreira, and Zisserman]{DoerschYVG0ACZ23}
Carl Doersch, Yi Yang, Mel Vecerík, Dilara Gokay, Ankush Gupta, Yusuf Aytar,
  João Carreira, and Andrew Zisserman.
\newblock Tapir: Tracking any point with per-frame initialization and temporal
  refinement.
\newblock In \emph{ICCV}, pages 10027--10038, 2023.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Elman(1990)]{elman1990finding}
Jeffrey~L Elman.
\newblock Finding structure in time.
\newblock \emph{Cognitive Science}, 14\penalty0 (2):\penalty0 179--211, 1990.

\bibitem[Feichtenhofer et~al.(2019)Feichtenhofer, Fan, Malik, and He]{slowfast}
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
\newblock Slowfast networks for video recognition.
\newblock In \emph{2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 6201--6210, 2019.

\bibitem[Goyal et~al.(2017)Goyal, Ebrahimi~Kahou, Michalski, Materzynska,
  Westphal, Kim, Haenel, Fruend, Yianilos, Mueller-Freitag,
  et~al.]{goyal2017something}
Raghav Goyal, Samira Ebrahimi~Kahou, Vincent Michalski, Joanna Materzynska,
  Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos,
  Moritz Mueller-Freitag, et~al.
\newblock The" something something" video database for learning and evaluating
  visual common sense.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 5842--5850, 2017.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1474--1487, 2020.

\bibitem[Gu et~al.(2021)Gu, Goel, and R{\'e}]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{arXiv preprint arXiv:2111.00396}, 2021.

\bibitem[Gu et~al.(2022)Gu, Gupta, Goel, and R{\'e}]{gu2022parameterization}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher R{\'e}.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock \emph{arXiv preprint arXiv:2206.11893}, 2022.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Islam and Bertasius(2022)]{Islam2022LongMC}
Md.~Mohaiminul Islam and Gedas Bertasius.
\newblock Long movie clip classification with state-space video models.
\newblock In \emph{European Conference on Computer Vision}, 2022.

\bibitem[Islam et~al.(2023)Islam, Hasan, Athrey, Braskich, and
  Bertasius]{10204597}
Md~Mohaiminul Islam, Mahmudul Hasan, Kishan~Shamsundar Athrey, Tony Braskich,
  and Gedas Bertasius.
\newblock Efficient movie scene detection using state-space transformers.
\newblock In \emph{2023 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 18749--18758, 2023.

\bibitem[Jaegle et~al.(2022)Jaegle, Borgeaud, Alayrac, Doersch, Ionescu, Ding,
  Koppula, Zoran, Brock, Shelhamer, Henaff, Botvinick, Zisserman, Vinyals, and
  Carreira]{jaegle2022perceiver}
Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin
  Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan
  Shelhamer, Olivier~J Henaff, Matthew Botvinick, Andrew Zisserman, Oriol
  Vinyals, and Joao Carreira.
\newblock Perceiver {IO}: A general architecture for structured inputs \&
  outputs.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Jelassi et~al.(2024)Jelassi, Brandfonbrener, Kakade, and
  Malach]{jelassi2024repeat}
Samy Jelassi, David Brandfonbrener, Sham~M Kakade, and Eran Malach.
\newblock Repeat after me: Transformers are better than state space models at
  copying.
\newblock \emph{arXiv preprint arXiv:2402.01032}, 2024.

\bibitem[Kundu et~al.(2022)Kundu, Tagliasacchi, Mak, Stone, Doersch, Oztireli,
  Herrmann, Gnanapragasam, Duckworth, Rebain, Fleet, Sun, Nowrouzezahrai,
  Lagun, Pot, Zhong, Golemo, Belletti, Meyer, Liu, Laradji, Greff, Yi, Beyer,
  Sela, Sajjadi, Radwan, Sabour, Vora, Kipf, Wu, Sitzmann, Du, and Miao]{movie}
Abhijit Kundu, Andrea Tagliasacchi, Anissa~Yuenming Mak, Austin Stone, Carl
  Doersch, Cengiz Oztireli, Charles Herrmann, Dan Gnanapragasam, Daniel
  Duckworth, Daniel Rebain, David~James Fleet, Deqing Sun, Derek
  Nowrouzezahrai, Dmitry Lagun, Etienne Pot, Fangcheng Zhong, Florian Golemo,
  Francois Belletti, Henning Meyer, Hsueh-Ti~(Derek) Liu, Issam Laradji, Klaus
  Greff, Kwang~Moo Yi, Lucas Beyer, Matan Sela, Mehdi S.~M. Sajjadi, Noha
  Radwan, Sara Sabour, Suhani Vora, Thomas Kipf, Tianhao Wu, Vincent Sitzmann,
  Yilun Du, and Yishu Miao, editors.
\newblock \emph{Kubric: A scalable dataset generator}, 2022.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and M{\"u}ller]{LeCun2012}
Yann~A. LeCun, L{\'e}on Bottou, Genevieve~B. Orr, and Klaus-Robert M{\"u}ller.
\newblock \emph{Efficient BackProp}, pages 9--48.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.

\bibitem[Li et~al.(2024)Li, Li, Wang, He, Wang, Wang, and
  Qiao]{li2024videomambastatespacemodel}
Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao.
\newblock Videomamba: State space model for efficient video understanding,
  2024.

\bibitem[Liu et~al.(2024)Liu, Zaharia, and Abbeel]{liu2024ringattention}
Hao Liu, Matei Zaharia, and Pieter Abbeel.
\newblock Ringattention with blockwise transformers for near-infinite context.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{Liu_2021_ICCV}
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 10012--10022, 2021.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'{a}}t, Burget, Cernock{\'{y}},
  and Khudanpur]{Mikolov2010}
Tom{\'{a}}s Mikolov, Martin Karafi{\'{a}}t, Luk{\'{a}}s Burget, Jan
  Cernock{\'{y}}, and Sanjeev Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{{INTERSPEECH} 11th Annual Conference of the International
  Speech Communication Association}, pages 1045--1048, 2010.

\bibitem[Orvieto et~al.(2023{\natexlab{a}})Orvieto, De, Gulcehre, Pascanu, and
  Smith]{orvieto2023universality}
Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel~L Smith.
\newblock On the universality of linear recurrences followed by nonlinear
  projections.
\newblock \emph{arXiv preprint arXiv:2307.11888}, 2023{\natexlab{a}}.

\bibitem[Orvieto et~al.(2023{\natexlab{b}})Orvieto, Smith, Gu, Fernando,
  Gulcehre, Pascanu, and De]{orvieto2023resurrecting}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,
  Razvan Pascanu, and Soham De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock \emph{arXiv preprint arXiv:2303.06349}, 2023{\natexlab{b}}.

\bibitem[Patraucean et~al.(2016)Patraucean, Handa, and
  Cipolla]{patraucean2015spatio}
Viorica Patraucean, Ankur Handa, and Roberto Cipolla.
\newblock Spatio-temporal video autoencoder with differentiable memory.
\newblock In \emph{2016 International Conference on Learning Representations
  (ICLR) - Workshop track}, 2016.

\bibitem[Piergiovanni et~al.(2023)Piergiovanni, Kuo, and Angelova]{tubevit}
A.~J. Piergiovanni, Weicheng Kuo, and Anelia Angelova.
\newblock Rethinking video vits: Sparse video tubes for joint image and video
  learning.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2023, Vancouver, BC, Canada, June 17-24, 2023}, pages
  2214--2224. {IEEE}, 2023.

\bibitem[Pont-Tuset et~al.(2017)Pont-Tuset, Perazzi, Caelles, Arbel\'aez,
  Sorkine-Hornung, and {Van Gool}]{davis2017}
Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\'aez, Alexander
  Sorkine-Hornung, and Luc {Van Gool}.
\newblock The 2017 davis challenge on video object segmentation.
\newblock \emph{arXiv:1704.00675}, 2017.

\bibitem[Pătrăucean et~al.(2023)Pătrăucean, Smaira, Gupta, Continente,
  Markeeva, Banarse, Koppula, Heyward, Malinowski, Yang, Doersch, Matejovicova,
  Sulsky, Miech, Frechette, Klimczak, Koster, Zhang, Winkler, Aytar, Osindero,
  Damen, Zisserman, and Carreira]{patraucean2023perception}
Viorica Pătrăucean, Lucas Smaira, Ankush Gupta, Adrià~Recasens Continente,
  Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz
  Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine
  Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang,
  Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman,
  and João Carreira.
\newblock Perception test: A diagnostic benchmark for multimodal video models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, pages 8748--8763.
  {PMLR}, 2021.

\bibitem[Srivastava et~al.(2015)Srivastava, Mansimov, and
  Salakhutdinov]{SrivastavaLSTM}
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov.
\newblock Unsupervised learning of video representations using lstms.
\newblock In \emph{Proceedings of the 32nd International Conference on
  International Conference on Machine Learning - Volume 37}, page 843–852.
  JMLR.org, 2015.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3104--3112, 2014.

\bibitem[Tong et~al.(2022)Tong, Song, Wang, and Wang]{tong2022videomae}
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
\newblock Video{MAE}: Masked autoencoders are data-efficient learners for
  self-supervised video pre-training.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[van Steenkiste et~al.(2024)van Steenkiste, Zoran, Yang, Rubanova,
  Kabra, Doersch, Gokay, Heyward, Pot, Greff, Hudson, Keck, Carreira,
  Dosovitskiy, Sajjadi, and Kipf]{steenkiste2024moving}
Sjoerd van Steenkiste, Daniel Zoran, Yi Yang, Yulia Rubanova, Rishabh Kabra,
  Carl Doersch, Dilara Gokay, Joseph Heyward, Etienne Pot, Klaus Greff, Drew~A.
  Hudson, Thomas~Albert Keck, Joao Carreira, Alexey Dosovitskiy, Mehdi S.~M.
  Sajjadi, and Thomas Kipf.
\newblock Moving off-the-grid: Scene-grounded video representations.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information
  Processing Systems}, 2024.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Venkataramanan et~al.(2024)Venkataramanan, Rizve, Carreira, Asano, and
  Avrithis]{venkataramanan2023imagenet}
Shashanka Venkataramanan, Mamshad~Nayeem Rizve, Jo{\~a}o Carreira, Yuki~M
  Asano, and Yannis Avrithis.
\newblock Is imagenet worth 1 video? learning strong image encoders from 1 long
  unlabelled video.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Zhang et~al.(2024)Zhang, Zhu, Wang, Zhang, Chen, Wang, and
  Ye]{surveyvmamba}
Hanwei Zhang, Ying Zhu, Dan Wang, Lijun Zhang, Tianxiang Chen, Ziyang Wang, and
  Zi Ye.
\newblock A survey on visual mamba.
\newblock \emph{Applied Sciences}, 14\penalty0 (13), 2024.

\bibitem[Zhao et~al.(2024)Zhao, Gundavarapu, Yuan, Zhou, Yan, Sun, Friedman,
  Qian, Weyand, Zhao, Hornung, Schroff, Yang, Ross, Wang, Adam, Sirotenko, Liu,
  and Gong]{pmlr-v235-zhao24f}
Long Zhao, Nitesh~Bharadwaj Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan,
  Jennifer~J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel
  Hornung, Florian Schroff, Ming-Hsuan Yang, David~A Ross, Huisheng Wang,
  Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong.
\newblock {V}ideo{P}rism: A foundational visual encoder for video
  understanding.
\newblock In \emph{Proceedings of the 41st International Conference on Machine
  Learning}, pages 60785--60811. PMLR, 2024.

\bibitem[Zhu et~al.(2024)Zhu, Liao, Zhang, Wang, Liu, and Wang]{zhu2024vision}
Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang
  Wang.
\newblock Vision mamba: Efficient visual representation learning with
  bidirectional state space model.
\newblock In \emph{Forty-first International Conference on Machine Learning},
  2024.

\end{thebibliography}
