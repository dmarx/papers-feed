\epiloguechapter[Epilogue: Model Complexity from the Macroscopic Perspective]{Model Complexity from the Macroscopic Perspective}\label{epi:overparameterization}
\index{duality!microscopic-macroscopic} 
\epigraph{According to the \terminate{hype} of 1987, neural networks\index{neural network} were meant to be intelligent models that discovered features\index{feature} and patterns in data. Gaussian processes in contrast are simply smoothing devices. How can Gaussian processes possibly replace neural networks?\index{neural network} Were neural networks over-hyped, or have we underestimated the power of smoothing methods?}{David MacKay \cite{mackay2003information}.\index{MacKay, David}}



\noindent{}Throughout this book, we've focused on deep-learning models that are very wide but also deep. Our reason for this focus is that such large neural networks with many many \terminate{model parameters} work extremely well in practice and thus form 
the 
foundation
of the modern approach to artificial intelligence. 


The success of these \textbf{overparameterized}\index{overparameterization|textbf} models with far more parameters than training data has led many to simply conjecture that ``more is better'' when it comes to \terminate{deep learning}.
In a 
more refined sense, there's mounting empirical evidence that a \textbf{scaling hypothesis}\index{scaling hypothesis} can accurately capture the behavior of deep neural networks, and its associated \emph{scaling laws}\index{scaling law} overall point towards the optimality of the overparameterized regime.\footnote{
    See e.g.~\cite{kaplan2020scaling} for an empirical study of scaling laws\index{scaling law} in deep learning language models\index{natural language processing} based on the \terminate{transformer} architecture. Empirically, it's observed that overparameterization is good; the optimal growth of the number of training samples $\NR$  scales sublinearly with the growth in parameters $P$, though importantly they should still scale together with a power law: $\NR \propto P^\alpha$ for $0 <\alpha < 1$. 
}
The simplicity of these empirical laws recalls an earlier period in statistical physics, when
a similar scaling hypothesis was
conjectured
to govern the behavior of certain complicated systems
in statistical mechanics\index{statistical physics}.\footnote{
    In physics, such scaling laws are an example of the phenomenon of \neo{universality}, the fact that when a system has many elementary components, it can often be described by a very simple \neo{effective theory} that's independent of the microscopic details of the underlying system \cite{Kadanoff:1971pc}. The framework of \emph{renormalization group}\index{renormalization group flow} then offers an explanation for how this universality arises by characterizing the flow from the microscopic to the macroscopic \cite{PhysRevB.4.3174,PhysRevB.4.3184}. This perhaps suggests that
    the 
    analogous
    notion of \neo{representation group flow} (cf.~\S\ref{sec:marginalization-group-flow})
    may be able
    to explain the neural scaling laws\index{scaling law} of \cite{kaplan2020scaling}.
}




However, the practical success of overparameterized models in deep learning appears to be in tension with orthodox \terminate{machine learning} and classic statistical theory\index{statistics (branch of mathematics)}. 
Heuristically, the \neo{Occam's razor} principle of sparsity
posits that we should favor the simplest hypothesis that explains our observations:
in the context of  
\terminate{machine learning}, this is usually interpreted to mean that we should prefer models with fewer parameters when comparing models 
performing the same tasks.
More quantitatively, we expect that models with fewer parameters will have smaller \emph{generalization errors}\index{generalization error},
$\gen \equiv \L_\B - \L_\A$, %
and will be less prone to overfit\index{overfitting} their \terminate{training set} $\A$.
Vice versa, we should naively expect that overparameterized models 
\emph{will} overfit\index{overfitting} their training data and generalize poorly. Thus, this orthodoxy is in direct conflict with the empirical success of overparameterized neural networks and 
is a big
theoretical puzzle in understanding modern deep learning.\footnote{
    See, e.g., the extensive discussion in \cite{zhang2016understanding} on the difficulty of trying to understand why large neural networks generalize so well according to traditional measures of model complexity.
}









In this 
brief
\terminate{epilogue}, we're going to
offer a resolution of this puzzle.
The crux of the matter hinges on the notion of \term{model complexity}. 
On the one hand, our orthodox discussion of
generalization above took a \term{microscopic perspective} -- focusing on how a network works in terms of its explicit low-level components -- and wanted to 
identify model complexity with model parameters.
On the other hand, in this book 
we integrated out\index{integrating out} the model parameters and
developed 
a \term{macroscopic perspective}  -- providing an effective theory description of 
the predictions of
realistic fully-trained networks -- for which this notion of model complexity is completely reversed.


Indeed,
we 
motivated our effective theory approach in \S\ref{ch:introduction} on the basis that we would be able to find simplicity in the limit of a large number of model parameters, and from \S\ref{ch:tools} to \S\ref{ch:eot}  we've now seen how this hypothesis has been borne out again and again for 
realistic large-but-finite-width networks.
Having finished all the technical calculations (outside of the appendices), we can see now
that it's the depth-to-width aspect ratio,
\be
r \equiv L/n \, ,
\ee
that controls the model complexity of overparameterized neural networks.
To understand why, recall that 
this ratio emerged from our calculations as the expansion parameter or \emph{cutoff} of our effective theory and determined how we could \emph{truncate}
the series expansion of the 
fully-trained distribution
while still approximating the true behavior of networks with minimal error. 
This means that
it's the number of data-dependent couplings\index{data-dependent coupling} of the truncated nearly-Gaussian distribution -- and \emph{not} the 
number of 
model parameters -- that ultimately define the model complexity in deep learning. From this macroscopic perspective, there's absolutely no conflict between the 
sparse 
intuition of \terminate{Occam's razor} in theory and the 
simplicity of 
the \terminate{scaling hypothesis} in practice.














To see how this works in even greater detail, let us
recall the three main problems 
we discussed at the beginning of the book 
in \S\ref{sec:why-it-works}, 
and then review how 
the principle of sparsity\index{sparsity, principle of}
enabled us to solve them.\footnote{In this epilogue, we'll drop layer indices on our variables to ease the notation, as everything is evaluated at the output layer; we'll also sometimes drop the neural indices when they are unimportant.}
Taylor expanding the trained network output around the network's initialization~\eqref{eq:proto-dynamics}, 
\begin{align}\label{eq:proto-dynamics-reprint}
z(x_{\delta};\theta^\star) =&z(x_{\delta};\theta) +\sum_{\mu=1}^P \dthetaI_\mu \frac{dz_{\delta}}{d\theta_\mu } +\frac{1}{2} \sum_{\mu, \nu=1}^P\dthetaI_\mu \dthetaI_\nu   \frac{d^2z_{\delta} }{d\theta_\mu d\theta_\nu}+\ldots\,,  
\end{align}
we illustrated \textbf{Problem 1}, \eqref{eq:grand-probelm-1}, that we might have to 
compute
an infinite number of terms,
\be\label{eq:grand-probelm-1-reprint}
z\, ,\quad \frac{dz}{d\theta }\, ,\quad \frac{d^2z }{d\theta^2}\, ,\quad \frac{d^3z }{d\theta^3}\,,\quad \frac{d^4z }{d\theta^4}\,,\quad \dots \, , %
\ee
\textbf{Problem 2}, \eqref{eq:grand-probelm-2}, that we have to determine the map from the \terminate{initialization distribution} over the model parameters to the induced initialization distribution over the network output and its derivatives,
\be\label{eq:grand-probelm-2-reprint}
p(\theta)\to p\!\le(z, \frac{dz}{d\theta }, \frac{d^2z }{d\theta^2},\, \dots  \ri) \, ,
\ee
and \textbf{Problem 3},\eqref{eq:grand-probelm-3}, that we have to solve the training dynamics, 
which can depend on \emph{everything},
\be\label{eq:grand-probelm-3-reprint}
\theta^\star \equiv\le[\theta^{\star}\ri]\!\le(\theta,\, z,\, \frac{dz}{d\theta},\, \frac{d^2 z}{d\theta^2},\, \ldots;\, \text{learning algorithm};\, \text{training data}\ri)\, .
\ee
Let us now give our detailed solutions to this problem set -- expanding on the schematic explanations that we gave in \S\ref{sec:why-it-works} -- and then carefully examine them through the lens of \neo{model complexity}. We'll begin first with the simple infinite-width limit and then discuss the nearly-simple-but-realistic $1/n$ truncation.


\subsubsection{Sparsity at Infinite Width}
In the infinite-width limit, we can now understand our solutions as follows:
\bi
\item Addressing \textbf{Problem 1}, \eqref{eq:example-of-sparsity-1}, all the higher derivative terms vanish, and we only need to keep track of two statistical variables:
\be\label{eq:example-of-sparsity-1-reprint}
z\, , \quad \frac{dz}{d\theta} \quad \implies \quad z_{\delta} \, , \quad \NTK_{\delta_1\delta_2}\, .
\ee
Note that this first derivative 
gives
the \emph{random features}\index{feature function!random} of the \terminate{linear model} description, cf.~\eqref{eq:feature-function-stochastic}, and the kernel
associated with these features is 
just 
the
NTK.
\item Addressing \textbf{Problem 2}, we found that the network output and its first derivative are statistically independent, each governed by the simple distribution \eqref{eq:example-of-sparsity-2}:
\be\label{eq:example-of-sparsity-2-reprint}
\lim_{n \to \infty}  p\!\le(z, \frac{dz}{d\theta}, \frac{d^2z}{d\theta^2},\, \dots  \ri)=p\!\le(z_{\delta}\ri) \delta\!\le( \sum_{\mu,\nu} \lambda_{\mu\nu} \frac{dz_{\delta_1}}{d\theta_\mu}\frac{dz_{\delta_2}}{d\theta_\nu} -\NTKI_{\delta_1 \delta_2} \ri) \, ,
\ee 
where on the right-hand side, $p(z_{\delta})$ is a zero-mean Gaussian distribution
with its variance given by the kernel $\ker_{\delta_1\delta_2}$~\eqref{eq:definition-of-kernel-first}, and the second factor is a \terminate{Dirac delta function} distribution 
that fixes the contraction of first derivatives that make up the  NTK $\NTK_{\delta_1\delta_2}$ 
to be deterministically given by the
frozen NTK $\NTKI_{\delta_1\delta_2}$ \eqref{eq:frozen-NTK}.
\item Addressing \textbf{Problem 3},
we obtained a solution for the trained model parameters $\theta^\star$ in a \emph{closed form} \eqref{eq:example-of-sparsity-3}:
\be\label{eq:example-of-sparsity-3-reprint}
\lim_{n \to \infty}\theta_\mu^\star = \theta_\mu(t=0)  - \sum_{\nu,\tra_1,\tra_2,i}\lambda_{\mu\nu} \frac{\td z_{i;\tra_1}}{\td \theta_\nu} \NTKIsub^{\tra_1 \tra_2}\le(z_{i;\tra_2}-y_{i;\tra_2}\ri) \, ,
\ee
with the associated fully-trained network outputs $z_{\delta}(T)=z(x_{\delta};\theta^{\star})$ given in \eqref{eq:network-step-general}. 
We further showed in \S\ref{subsec:algorithmic-independence-at-infinity} that this fully-trained solution is independent of the algorithm used to train the network. 
\ei
Combining these insights, we found that the fully-trained distribution,
\be\label{eq:infinite-width-correct-conditioning}
\lim_{n \to \infty}p\Big(z(T)\Big) \equiv p\!\le(z(T)\Big\vert y_{\tra}, \ker_{\delta_1 \delta_2}, \NTKI_{\delta_1 \delta_2} \ri) \, ,
\ee
is a \neo{Gaussian distribution}; the reason for writing it as a conditional distribution in this way is that the mean, \eqref{eq:GD-frozen-mean}, is only a function of vector of training set labels, $y_{\tra}$, and the frozen NTK matrix, $\NTKI^{(L)}_{\delta_1 \delta_2}$, while the variance, \eqref{eq:generalized-posterior-variance}, is only a function of  the kernel matrix, $\ker^{(L)}_{\delta_1 \delta_2}$, and the frozen NTK matrix, $\NTKI^{(L)}_{\delta_1 \delta_2}$. 
In other words,
the distribution of 
predictions on a test sample, $x_{\tea}$,  
will depend on the test sample together with all of the training data, $\D = \{\tea\} \cup \A$, and 
the shape of that
data dependence is governed  by the specific functional forms of the data-dependent couplings\index{data-dependent coupling}, i.e.~the output-layer kernel  $\ker(x_{\delta_1}, x_{\delta_2})$  and output-layer frozen NTK 
$\NTKI(x_{\delta_1}, x_{\delta_2})$.
Thus, the infinite-width solution \eqref{eq:infinite-width-correct-conditioning} allows for a very sparse description, depending only on a few objects in a simple way.











\subsubsection{Near-Sparsity at Finite Width}\index{sparsity, principle of!near-sparsity at finite width}\index{near-sparsity|see{sparsity, principle of}}
Similarly, at large-but-finite width, we can now understand our solutions as follows:
\bi
\item Addressing \textbf{Problem 1}, \eqref{eq:example-of-sparsity-finite-n-1}, all derivatives $d^k f/d\theta^k$ for $k\geq 4$ are $\o{1/n^2}$, and so we only need to keep track of the statistical variables up to the third derivative:
\be\label{eq:example-of-sparsity-finite-n-1-reprint}
z\, , \quad \frac{dz}{d\theta}\, , \quad \frac{d^2z}{d\theta^2}\,, \quad \frac{d^3z}{d\theta^3} \quad \implies \quad z_\delta \, ,  \NTK_{\delta_1 \delta_2} \, ,  \dNTK_{\delta_0 \delta_1 \delta_2}\, ,  \ddNTK_{\delta_0 \delta_1 \delta_2 \delta_3}\, ,  \ddNTKII_{\delta_1 \delta_2 \delta_3 \delta_4} \, .
\ee
Here, we note that the NTK, dNTK, and ddNTKs capture all the terms up to the third derivative in our Taylor expansion \eqref{eq:proto-dynamics-reprint} for a gradient-based learning update, cf.~\eqref{eq:preactivation-updated-finite-width-refined}, \eqref{eq:NTK-updated-finite-width-refined}, and \eqref{eq:dNTK-updated-finite-width}.
\item Addressing \textbf{Problem 2}, \eqref{eq:example-of-sparsity-finite-n-2}, we evaluated the distribution of all these statistical variables, and found that its joint distribution is nearly-Gaussian:
\be\label{eq:example-of-sparsity-finite-n-2-reprint}
 p\!\le(z, \frac{dz}{d\theta}, \frac{d^2z}{d\theta^2},
 \, \dots 
 \ri) = p\!\le(z, \, \NTK,\, \dNTK,\, \ddNTK, \,\ddNTKII \ri) + \o{\frac{1}{n^2}}\, .
\ee
\item Addressing \textbf{Problem 3}, \eqref{eq:example-of-sparsity-finite-n-3}, we were able to use \terminate{perturbation theory} to solve the  nonlinear training dynamics and evaluate the predictions of fully-trained networks at finite width:
\be\label{eq:example-of-sparsity-finite-n-3-reprint}
z(x_{\delta};\theta^\star)=\le[z(x_{\delta};\theta^\star)\ri]\le(z, \, \NTK,\, \dNTK,\, \ddNTK, \,\ddNTKII;\,  \text{algorithm projectors}\ri)\, .
\ee
Here, the details of the \neo{algorithm dependence} of the  
prediction is manifest, captured entirely by a handful of algorithm projectors, cf.~\eqref{eq:implicit-ZA-tensor}--\eqref{eq:implicit-ZB-tensor-II}.
\ei
Combining these insights, we found that the fully-trained distribution,
\be\label{eq:finite-width-correct-conditioning}
p\Big(z(T) \Big) \equiv p\!\le(z(T) \Big\vert y, G, \NTKM,V,A, B, D, F, P, Q, \ddNTKRS, \ddNTKSS, \ddNTKTS, \ddNTKUS\ri)
+ \o{\frac{1}{n^2}}
\, ,
\ee
is a \neo{nearly-Gaussian distribution}; its statistics are entirely described by the conditional variables listed here, though we've  suppressed the sample indices in order to fit them all on one line.\footnote{
    Note that we've also suppressed the algorithm projectors\index{algorithm projector} in this conditioning, presuming that we are considering a fixed \terminate{learning algorithm}: once an algorithm is fixed, the projectors
    can only be 
    fixed functions of any training set tensors that contain $\o{1}$ terms -- i.e.~the metric submatrix and the NTK mean submatrix, both evaluated on the training set only, $\widetilde{G}_{\tra_1\tra_2}$ and  $\widetilde{\NTKM}_{\tra_1\tra_2}$ -- and
    of the global learning rate\index{learning rate!global}, $\eta$,
    see e.g.~\eqref{eq:implicit-ZA-tensor}--\eqref{eq:implicit-ZB-tensor-II} for \terminate{gradient descent}.
    Thus, the algorithm dependence is taken care of entirely by the tensors we're conditioning on already.
}
In addition to the metric $G$ and the NTK mean $H$, in this conditioning we're accounting for the finite-width data-dependent couplings\index{data-dependent coupling} arising from our decompositions\index{tensor decomposition!giving data-dependent couplings} of the four-point connected correlator of preactivations $\E{zzzz}_{\text{connected}}$, \eqref{eq:C4_MLPH},  the NTK-preactivation cross correlator $\E{\DNTKS z z}$, \eqref{eq:D-F-decomposition-general-layer}, the NTK variance $\E{\DNTKS^2}$, \eqref{eq:NTH-variance-decomposition},
the dNTK-preactivation cross correlator $\E{\dNTK z}$, \eqref{eq:dntk-ell-layer-decomposition},
and the means of the ddNTKs $\E{\ddNTK}$ and $\E{\ddNTKII}$, \eqref{eq:decomposition-ddNTK} and \eqref{eq:decomposition-ddNTK-II}.
Importantly, all of these finite-width tensors at $\o{1/n}$ are functions of exactly four input samples each, e.g.~for the four-point vertex we have $V_{(\delta_1 \delta_2)(\delta_3\delta_4)} \equiv V\!\le(x_{\delta_1},  x_{\delta_2},x_{\delta_3},x_{\delta_4}\ri)$, and the specific functional forms of these data-dependent couplings\index{data-dependent coupling} determine the 
overall data dependence of the distribution.
Thus, though 
slightly more complicated than the infinite-width description \eqref{eq:infinite-width-correct-conditioning}, the solution truncated to $\o{1/n}$, \eqref{eq:finite-width-correct-conditioning}, is 
a nearly-sparse\index{sparsity, principle of!near-sparsity at finite width} description, depending only on two-hands-full of objects in a nearly-simple way.













\subsubsection{Model Complexity of Fully-Trained Neural Networks}
These effective theory results, \eqref{eq:infinite-width-correct-conditioning} and \eqref{eq:finite-width-correct-conditioning}, should make it clear that for overparameterized\index{overparameterization} neural networks, 
it is no longer appropriate to identify
the number of \terminate{model parameters} with the
\terminate{model complexity}.
Consider a fixed combined training and test dataset\index{input data} of size $\ND$:
\bi
\item For the %
\terminate{Gaussian distribution}, \eqref{eq:infinite-width-correct-conditioning}, that describes an ensemble of fully-trained infinite-width networks, we only need
\be
n_{\text{out}}\NR+\le[\frac{\ND(\ND+1)}{2}\ri] + \le[\frac{\ND(\ND+1)}{2}\ri] =\o{\ND^2} \, 
\ee
numbers in order to completely specify the distribution, with each term corresponding to the numbers needed to enumerate $y_{i;\tra}$, $\ker_{\delta_1 \delta_2}$, and $\NTKI_{\delta_1 \delta_2}$, respectively.
\item For the \terminate{nearly-Gaussian distribution}, \eqref{eq:finite-width-correct-conditioning}, that describes an ensemble of 
fully-trained
finite-width networks with small-but-nonzero aspect ratios, $0 < r \ll 1$, we now need 
\be
\o{\ND^4} \,
\ee
numbers in order to completely specify the distribution, with the counting dominated by the finite-width tensors, each of which having exactly four sample indices. 
\ei\index{macroscopic perspective}\index{microscopic perspective}
Thus, while each infinite-width network has an \emph{infinite} number of microscopic model parameters, its macroscopic data-dependent couplings are only \emph{quadratic} in samples. 
Meanwhile, our finite-width networks have less model parameters than our infinite-width network -- i.e.~finite
$<$ infinite -- but their macroscopic effective description is 
more complicated!
What we have found here is the manifestation of the \textbf{microscopic-macroscopic duality}\index{duality!microscopic-macroscopic}\index{duality|textbf}: 
under this duality,
complexity in \neo{parameter space} is transformed into simplicity in \neo{sample space}, 
and density in \neo{model parameters} is exchanged for sparsity in \emph{data-dependent couplings}\index{data-dependent coupling}. In the overparameterized regime\index{overparameterization},
this duality indicates that 
we 
really should
identify the \terminate{model complexity} with 
the data-dependent couplings rather than 
the model parameters.







To further elaborate on this general point, we could imagine carrying out our finite-width \terminate{$1/n$ expansion},
 \eqref{eq:finite-width-limit-distribution-truncated}, to higher orders as
\be\label{eq:finite-width-limit-distribution-reprint}
p\Big(z(T)\Big)=p^{\{0\}}\Big(z(T)\Big) + \frac{p^{\{1\}}\Big(z(T)\Big)}{n} + \frac{p^{\{2\}}\Big(z(T)\Big)}{n^2} + \o{ \frac{1}{n^3} }\, .
\ee
To begin,
now the preactivation distribution at initialization,
\be\label{eq:marginal-distribution-action-ansatz-reprint}
p\!\le(z \Big\vert\D\ri) = \frac{1}{Z}  e^{-\ac(z) } + \o{\frac{1}{n^3}} \, ,
\ee
will be effectively described in terms of a \emph{sextic action}\index{action!sextic},
\begin{align}\label{eq:general-L-action-sixth}
\ac(z)
\equiv&\frac{1}{2}\sum_{i=1}^{n_{L}}\sum_{\delta_1,\delta_2\in\D} g^{\delta_1\delta_2} \zNL{i}{\delta_1}\zNL{i}{\delta_2}\, \\
&-\frac{1}{8}\sum_{i_1,i_2=1}^{n_{L}}\sum_{\delta_1,\ldots,\delta_4\in\D}v^{(\delta_1\delta_2)(\delta_3\delta_4)} \zNL{i_1}{\delta_1}\zNL{i_1}{\delta_2}\, \zNL{i_2}{\delta_3}\zNL{i_2}{\delta_4}
\nonumber \\
&+\frac{1}{24}\sum_{i_1,i_2,i_3=1}^{n_{L}}\sum_{\delta_1,\ldots,\delta_6\in\D}\SPC^{(\delta_1\delta_2)(\delta_3\delta_4)(\delta_5\delta_6)} \zNL{i_1}{\delta_1}\zNL{i_1}{\delta_2}\, \zNL{i_2}{\delta_3}\zNL{i_2}{\delta_4}\,\zNL{i_3}{\delta_5}\zNL{i_3}{\delta_6}\, . \notag
\end{align}
In this action, 
the \emph{sextic coupling}\index{coupling!sextic} scales as
\be
\SPC^{(\delta_1\delta_2)(\delta_3\delta_4)(\delta_5\delta_6)} = \o{\frac{1}{n^2}} \, ,
\ee 
and leads to a nontrivial  connected six-point correlator characterized by a \neo{six-point vertex}\index{six-point vertex|seealso{data-dependent coupling}}: $\SPV_{(\delta_1\delta_2)(\delta_3\delta_4)(\delta_5\delta_6)}$.\footnote{We computed this vertex in the second layer
in footnote \ref{foot:second-layer-hierarchy} of \S\ref{sec:second-layer-non-gaussian}, and  we've further taught you everything that you need to know in order to extend the computation of such higher-point correlators to deeper layers, and then analyze their scaling at criticality.
As a \terminate{checkpoint} for your algebra, the single-input recursion for the six-point vertex specialized to the $\relu$ activation function is
\be
\SPV^{(\ell+1)}= \frac{1}{8}\left[C_W^{(\ell+1)}\right]^3 \le[\SPV^{(\ell)} + 
30 V^{(\ell)} \ker^{(\ell)}
+44 \le(\ker^{(\ell)}\ri)^3
\ri]\, ,
\ee
which at criticality has a solution
\be
\frac{\SPV^{(\ell)}}{n^2 \le(K^{(\ell)}\ri)^3} = 75 \frac{\ell^2}{n^2} + \dots \,.
\ee
} At criticality, this connected correlator scales as
\be
\frac{1}{n^2} \SPV_{(\delta_1\delta_2)(\delta_3\delta_4)(\delta_5\delta_6)} \propto \o{\frac{L^2}{n^2}} \, ,
\ee
consistent with the expectations of our \terminate{effective theory} cutoff\index{cutoff, effective theory}.
Thus, we expect that the refined description, \eqref{eq:finite-width-limit-distribution-reprint}, is accurate to order $L^2/n^2$, but at the cost of a significant increase in the \terminate{model complexity}:
the counting will be dominated by the $1/n^2$ finite-width tensors -- each of which has six sample indices -- and so we now
require 
\be
\o{\ND^6}
\ee 
numbers in order to specify all the data-dependent couplings\index{data-dependent coupling} of the distribution.
In general, to achieve an accuracy of order $L^k/n^k$, we expect that a
macroscopic
description
\be\label{eq:finite-width-limit-distribution-reprint-more-terms}
p\Big(z(T)\Big) =\sum_{m=0}^{k} \frac{p^{\{m\}}\Big(z(T)\Big)}{n^m}+ \o{\frac{L^{k+1}}{n^{k+1}}}\, ,
\ee
will have its \terminate{model complexity} dominated by data-dependent couplings\index{data-dependent coupling} with $2k$-sample indices, requiring 
\be
\o{\ND^{2k}}
\ee
numbers.
In this way, the \textbf{1/\emph{n} expansion}\index{$1/n$ expansion|textbf} gives a sequence of effective theories with increasing accuracy at the cost of increasing complexity.\footnote{
    Note here that this counting is for the union of the \terminate{training set} and the \terminate{test set}, $\D = \A \cup \B$, rather than just for the training set $\A$; in other words, the stochastic predictions made on a test sample, $x_{\tea}$, will necessarily depend on that test sample.
In particular, every time you want to make a prediction on a new sample that you haven't predicted before, $\ND$ increases in size, and so does your description of the joint distribution -- \eqref{eq:infinite-width-correct-conditioning} and \eqref{eq:finite-width-correct-conditioning} -- over the entire dataset\index{input data} $\D$. Statistical models that have this property are sometimes called \textbf{non-parametric models}\index{non-parametric model|textbf}, since the ``parameters'' of the full distribution -- i.e.~the \emph{data-dependent couplings}\index{data-dependent coupling} --  depend on data points that you may not have even seen yet. 



From the \terminate{macroscopic perspective}, the description of any \emph{single} prediction scales with the size of the training set as $O\big((\NR + 1)^p\big) \approx \o{\NR^p}$; in principle you can just plug $x_{\tea}$ into a prediction formula such as \eqref{eq:very-general-finite-width-solution-DONT-CHANGE}.
From the \terminate{microscopic perspective}, if we train a model and find a solution $\theta^\star \equiv \theta^\star(\A)$ for the model parameters given a training set $\A$, then in practice we can then forget about that training set and simply make predictions as $z(x_{\tea}; \theta^\star)$ --
paying only the computation complexity cost of using the model to make a prediction. %
Thus, in deep learning we can think of this non-parametric growth of macroscopic model complexity with the size of the test set as similar in nature to the microscopic $\o{P}$ complexity of the forward pass needed to make a new prediction.


Potentially it may have been useful to tell you -- at the very least in order to understand this \terminate{epilogue}'s \terminate{epigraph} --  that a non-parametric model based on a Gaussian distribution\index{Gaussian distribution!as a Gaussian process}
 is called a \textbf{Gaussian process}\index{Gaussian process|see{Gaussian distribution}}, and accordingly people sometimes say that neural networks in the \terminate{infinite-width limit} are Gaussian processes. Similarly, if you wanted to talk about the distribution over finite-width networks in the context of non-parametric statistics, you might call it a \textbf{nearly-Gaussian process}\index{nearly-Gaussian process|see{nearly-Gaussian distribution}}.\index{nearly-Gaussian distribution!as a nearly-Gaussian process} These processes are distributions over functions $z(x)$, where e.g.~$z$ is the trained model output function.
However, the reason why we haven't brought this up before is that we find this terminology
unnecessary: for any fixed dataset\index{input data} $\D$, we don't have to worry about distributions over \emph{functions} and instead can just think about a joint distribution over the finite \emph{sets} of outputs evaluated on the dataset.
}



Importantly, for any particular network architecture that we want to describe, as the depth-to-width ratio $r\equiv L/n$ increases, we'll in principle need to include more and more of these higher-order terms, making our macroscopic \terminate{effective theory} more and more complex:
\bi
\item In the strict limit  $r \to 0$, the interactions between neurons turn off, and the sparse $\o{\ND^2}$ \emph{Gaussian}\index{Gaussian distribution} description of the \terminate{infinite-width limit}, \eqref{eq:infinite-width-correct-conditioning},  will be accurate. Such networks are not really deep, as $L/n=0$, and they do not learn representations (\S\ref{subsec:linear-at-infinity}).\index{representation learning}
\item In the regime $0 < r \ll 1$, there are small nontrivial \terminate{interactions} between neurons, and the nearly-sparse\index{sparsity, principle of!near-sparsity at finite width} $\o{\ND^4}$ \emph{nearly-Gaussian}\index{nearly-Gaussian distribution} description of the finite-width effective theory truncated at order $1/n$,  \eqref{eq:finite-width-correct-conditioning},  will be accurate. Such networks are wide while at the same time having nontrivial depth, $L/n\neq 0$,  and they do learn representations (\S\ref{subsec:nonlinear-at-finite}).\index{representation learning}
\item For larger $r$, the neurons are strongly-coupled\index{interactions!strong coupling}, and a more generic $O\big(\ND^{2k}\big)$ \emph{non-Gaussian}\index{non-Gaussian distribution} description, \eqref{eq:finite-width-limit-distribution-reprint-more-terms}, would in principle be necessary. However, in this case the 
the \terminate{macroscopic perspective} leads to an \emph{ineffective} description that is not tractable, and relatedly, we do not expect such networks to be practically useful for machine learning tasks (\S\ref{sec:solution_DLN}).\footnote{
In \S\ref{sec:solution_DLN}, we were able to access this regime in the special case of \emph{deep linear networks}\index{deep linear network}. There, we saw that 
higher-point connected correlators\index{connected correlator!} can grow uncontrollably even when the network is tuned to \terminate{criticality}, and there's no reason to expect that this would be any more favorable for nonlinear activation functions.
Moreover, as we discussed in \S\ref{sec:signal_prop_finite_width}, the growth of these higher-order correlators in networks for all choices of activation functions will lead to large  \terminate{fluctuations} from instantiation-to-instantiation, meaning that the ensemble description \eqref{eq:finite-width-limit-distribution-reprint-more-terms} can no longer be trusted for any \emph{particular} network.
Altogether, this suggests that networks of an aspect ratio $r$ that require large sample-space complexity, $O\big(\ND^{2k}\big)$ with large $k$, will generically exhibit strongly-coupled \terminate{chaos}; we do not expect such networks  to be 
effectively describable or practically trainable.

Even if we could find a workable network with such a large complexity, would we ever need it?
As a \neo{gedanken model},
let's consider an ineffective description with almost no truncation, say $k\sim \ND$, which comes with an exponential number of data-dependent couplings, $O\big(\ND^{\ND}\big)$.
Such an exponentially complex description would  only really be appropriate when we have \neo{unstructured data}: e.g.~if we have a binary labeling $f(x) = \{0,1\}$ for which each label is chosen \emph{randomly}, then the number of possible functions for $\ND$ uncorrelated data points is $2^{\ND}$; each time we want to incorporate a new input-output pair into our dataset, we'd have to \emph{double} the complexity of our description.
Such unstructured data is at odds with one of the main purposes of \terminate{machine learning}: recognizing patterns in the data -- i.e.~correlations -- which allow for the learning of representations and the finding of sparse descriptions. In fact, as there's no efficient way to learn such unstructured datasets -- see e.g.~the \neo{no-free-lunch theorem} of \cite{wolpert1996lack,wolpert1997no} -- in practice we cannot possibly require these ineffective descriptions
for any realistic machine learning scenarios.
\label{footnote:epilogue-chaos}
}
\ei
In this way, our effective theory cutoff scale $r$ governs \terminate{model complexity} of the statistics needed to faithfully describe the behavior of different neural networks. The simplicity of the \neo{macroscopic perspective} only emerges for small values of the cutoff $r$.


















With that in mind, the practical success of \terminate{deep learning} in the overparameterized\index{overparameterization} regime and the empirical accuracy of a simple \terminate{scaling hypothesis} is really telling us that useful neural networks\index{neural network} should be \emph{sparse}  -- hence the preference for larger and larger models -- but not too sparse -- so that they are also \emph{deep}.
Thus, from the \terminate{macroscopic perspective}, a \textbf{nearly-sparse}\index{sparsity, principle of} \terminate{model complexity} is perhaps the most important
\terminate{inductive bias}\index{inductive bias!of sparsity in deep learning} of \terminate{deep learning}.

\sbreak


For 
an \emph{information-theoretic}\index{information theory} estimate of the depth-to-width ratio $r^\star$
for which the wide attraction of simplicity and the deep need of complexity are balanced to the end of \emph{near-sparsity}\index{sparsity, principle of!near-sparsity at finite width},
please 
feel free to flip the page and make your way through Appendix \ref{app:mi-stuff}.




















