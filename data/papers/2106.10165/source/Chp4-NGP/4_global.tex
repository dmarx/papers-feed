
\chapter{RG Flow of Preactivations}\index{representation group flow!of preactivations}
\label{ch:ngp}


\epigraph{``You can hide a lot in a large-$N$ matrix.'' -- Steve Shenker\index{Shenker, Stephen} \emph{-- John McGreevy~\cite{mcgreevy2010holographic}.\index{McGreevy, John}} }{}

\noindent{}At the end of the last chapter,
we computed the statistics of preactivations for deep linear networks at initialization
and saw them \emph{run} as a function of the network depth. For that toy model, using a handful of
Wick contractions
and the recursive structure of the network architecture, we were able to fully understand the effects of the network's hyperparameters -- its initialization scheme, width, and depth -- on preactivation correlators. This exercise in particular highlighted the importance of \emph{critical initialization hyperparameters} and sufficiently small \emph{depth-to-width ratio}
in order for the network outputs to be well-behaved, theoretically and practically. 
To extend these insights beyond deep linear networks, we need to develop an \terminate{effective theory of deep learning} for networks with any activation function.

While ultimately the goal of our effective theory is to explain how a \emph{particular} neural network learns from a given dataset, our immediate goal in~\S\ref{ch:ngp} and~\S\ref{ch:signalprop}
will be to understand how an \emph{ensemble} of neural networks at initialization behaves as a function of data.
In
\S\ref{ch:NTHb}, \S\ref{ch:features}, and \S\ref{ch:eot},
we'll find that these goals are closely tied together:
through the judicious study of the ensemble,
we can systematically evaluate the \emph{typical}\index{typicality} behavior of trained
networks
as well as
how any particular network may \emph{fluctuate} away from typicality.
Our starting point will thus be a study of the statistics of neural-network preactivations with Gaussian-initialized biases and weights. 
All in all, the formalism developed in this chapter for analyzing the ensemble of networks at initialization will be the key to a principled understanding of deep learning.

As stressed in the introduction,~\S\ref{ch:introduction}, our focus will always be on describing real finite-width networks, since a lot is lost in idealized infinite-width networks.
One salient phenomenon lost in the infinite-width limit is the increasing non-Gaussianity in the preactivation distributions of deeper layers.
Such non-Gaussianity makes the behavior of finite-width networks much richer but more complicated to analyze.
In order to tame these complications, we'll
need to
borrow
some tools from theoretical physics\index{physics}. 
In particular, physicists have a long tradition of finding simple descriptions of complicated systems in the limit of a large number of degrees of freedom, while keeping in mind the true goal of modeling real systems. In our context, this hints at tractability and simplification in the regime where networks become very wide, though not infinitely so.
To make this precise, in this chapter we introduce the \emph{large-}$n$ expansion or  \terminate{$1/n$ expansion} in order to perform perturbative expansions when hidden-layer width $n$ becomes parametrically big.
With this tool, we'll be able to systematically study the preactivation distributions of finite neural networks to arbitrary precision.\footnote{
Back in 1996, Neal introduced the \neo{infinite-width limit} in a seminal work \cite{neal1996priors}, focusing on single-hidden-layer networks.  Much later, this program was continued in \cite{lee2018deep,matthews2018gaussian}, extending the infinite-width limit to deeper networks, and then was extended further by Yaida   
in \cite{Yaida2019} to \emph{finite-width networks}. A large part of this chapter is focused on reproducing the recursions first derived in \cite{Yaida2019}.

However, our perspective here is different than the one taken in this prior work. In particular, our main motivation is in computing the  distribution of preactivations at initialization, with an eye towards ultimately understanding gradient-based training (\S\ref{ch:NTHb}, \S\ref{ch:features}, \S\ref{ch:eot}), rather than providing a starting point for Bayesian inference. (We will give our own perspective on Bayesian learning for deep learning in \S\ref{ch:bayesian-inference}.) Additionally, in contrast to \cite{Yaida2019}, our results here are derived by first focusing on the couplings in the \neo{action}, rather than directly on the correlators of the distribution. This method is more intuitive and can be more easily extended.
}






As we did for deep linear networks, we will proceed recursively, investigating how the
distribution of preactivations changes from layer to layer by following the transformation of inputs
via the iterative MLP forward-pass equation.
We start in \S\ref{sec:first-layer-gaussian} by computing the distribution of preactivations in the first layer, integrating out the first set of weights and biases. This procedure recovers a well-known result that the distribution of the first-layer preactivations is Gaussian.
Since this calculation is so central to the rest of the chapter, we'll present two different derivations: a combinatorial derivation in terms of Wick contractions and an algebraic derivation using the Hubbard-Stratonovich transformation.


Next, in \S\ref{sec:second-layer-non-gaussian}, we'll consider the
distribution of preactivations in the second layer and see the emergence of non-Gaussianity in four-point and higher-point connected correlators.
The magnitude of these correlators is
suppressed when the network is very wide, vanishing in the strict infinite-width limit.
This suppression for wide networks in turn enables us to write down an action describing the preactivation distribution, building on the correspondence explored in~\S\ref{ch:tools} between such connected correlators and the couplings in the action.
In particular, the large-$n$ expansion lets us start with the quadratic action describing the Gaussian distribution in the infinite-with limit and then perturbatively expand around it in a series of the inverse width, $1/n$, to arbitrary desired precision.
Given the importance of this result, we again provide two derivations, one based on Wick contractions and the other based on expanding the stochastic metric.

Finally, in~\S\ref{sec:deeper-layer-accumulation}, we'll analyze the
distribution of preactivations at any depth.
At this point we can simply repurpose the calculations from the preceding sections to see how the distribution of preactivations recursively transforms from the $\ell$-th layer to the $(\ell+1)$-th layer.
In particular, keeping the leading finite-width $1/n$ corrections, we'll obtain recursion equations for the two-point and 
four-point correlators,
encoding how these observables evolve with increasing depth.
We'll see that the preactivation distribution of the $(\ell+1)$-th layer contains a nearly-Gaussian piece inherited from the $\ell$-th layer as well as an additional near-Gaussianity generated in the transition from the $\ell$-th to $(\ell+1)$-th layer.
In the next chapter, \S\ref{ch:signalprop}, we'll see in detail how the near-Gaussianity accumulates with depth by explicitly solving these recursions and analyzing their solutions, which extends the notion of criticality and emergence of the depth-to-width ratio to networks with general activation functions.

After a short clarifying section on some implications of marginalization (\S\ref{sec:sum-rule}) and a section on subleading corrections (\S\ref{sec:loop-correction}),
we take a step back in \S\ref{sec:marginalization-group-flow}  in order to
draw a parallel between our formalism and 
 the \emph{renormalization group} in theoretical physics\index{physics}. 
Renormalization group is a powerful recursive method for understanding complicated interacting systems, capturing how the effective interactions between the constituents of a system change when the scale at which they are measured changes from microscopic to macroscopic.
Specifically, renormalization marginalizes over the microscopic degrees of freedom in the system to yield an effective \emph{coarse-grained}\index{coarse-graining} description at long distances.\index{coarse-graining|seealso{representation group flow}}\index{coarse-graining|seealso{renormalization group flow}}
This is analogous to the way we recursively marginalize over preactivations in previous layers to obtain an effective description of a \neo{representation}\index{representation|seealso{feature}} at the current layer,
in our case capturing how the interactions between neurons change with depth.
In both cases the flow of the distributions is created by the marginalization of fine-grained information. Given the complete parallel, we will call our flow \emph{representation group (RG) flow}.\index{representation group flow}



If this sounds like a popular heuristic explanation for what deep neural networks do -- transforming fine-grained information at the input level into coarser information at the feature levels and finally into fully coarse-grained representation at the output level
-- that's because our formalism makes this heuristic picture of representation coarse-graining concrete.\footnote{There have been many formal and informal comments on the connection between renormalization and deep learning, but the relationship has never before been made precise.}
Our formalism will further let us directly probe the effect of the \emph{deep} in \emph{deep learning} by tracking the change in preactivation distributions as we increase the number of layers.
Thus, it is the starting point for an \terminate{effective theory of deep learning}, which we will continue to develop throughout the book.













































\section{First Layer: Good-Old Gaussian}
\label{sec:first-layer-gaussian}

Given a dataset
\be
\D=\le\{\x{i}{\alpha}\ri\}_{i=1,\ldots,n_0;\, \alpha=1,\ldots,\ND}
\ee
containing $\ND$ inputs of $n_0$-dimensional vectors, the preactivations in the first layer are given by
\be\label{eq:first-layer-preactivation-def}
\z{i}{\alpha}{1} \equiv z_i^{(1)}(x_\alpha)=\bias{i}{1}+\sum_{j=1}^{n_{0}}\W{ij}{1}\x{j}{\alpha}\,,  \quad \text{for} \quad i=1,\ldots,n_1\, .
\ee
At initialization the biases $b^{(1)}$ and weights $W^{(1)}$ are independently distributed according to mean-zero Gaussian distributions with variances
\begin{align}
\label{eq:bias-variance-def-first}
\mathbb{E}\le[b^{(1)}_{i}b^{(1)}_{j}\ri]&=\delta_{ij} C_{b}^{(1)}\, ,\\ %
\label{eq:weight-variance-def-first}
\mathbb{E}\le[W^{(1)}_{i_1 j_1}W^{(1)}_{i_2 j_2}\ri]&=\delta_{i_1 i_2} \delta_{j_1 j_2}\frac{C_{W}^{(1)}}{n_{0}}\, .%
\end{align}
The first-layer preactivations $z^{(1)}=\z{i}{\alpha}{1}$ form an $(n_1\ND)$-dimensional vector, and we are interested in its distribution at initialization, 
\be\label{eq:first-layer-distribution}
p\!\le(z^{(1)}\Big\vert\D\ri)= p\!\le(z^{(1)}\le(x_1\ri), \ldots, z^{(1)}\le(x_{\ND}\ri) \ri)\, .%
\ee
Note how this distribution depends conditionally on the input data, representing the fact that the preactivations are functions of the input.

Now, let us compute the distribution of the first-layer preactivations at initialization.
Since this will be so important, we give two derivations, one combinatorial and one algebraic.



\subsubsection{Wick this way: combinatorial derivation via correlators}
The first derivation involves direct application of Wick contractions\index{Wick contraction} to compute correlators of the first-layer distribution~\eqref{eq:first-layer-distribution}.
Starting with the one-point correlator, simply inserting the definition of the first-layer preactivations~\eqref{eq:first-layer-preactivation-def} gives
\be
\E{\z{i}{\alpha}{1}} = \E{\bias{i}{1}+\sum_{j=1}^{n_{0}}\W{ij}{1}\x{j}{\alpha_1}} = 0\,,
\ee
since $\E{\bias{i}{1}}=\E{\W{ij}{1}}=0$. In fact, it's easy to see that all the odd-point correlators of $p\!\le(z^{(1)}\Big\vert\D\ri)$ vanish because there always is an odd number of either biases $b^{(1)}$ or weights $W^{(1)}$ left unpaired under Wick contractions.

Next for the two-point correlator, again inserting the definition~\eqref{eq:first-layer-preactivation-def}, we see
\begin{align}\label{eq:first-layer-second-moment}
\E{\z{i_1}{\alpha_1}{1}\z{i_2}{\alpha_2}{1}}&=\E{\le(\bias{i_1}{1}+\sum_{j_1=1}^{n_{0}}\W{i_1j_1}{1}\x{j_1}{\alpha_1}\ri)\le(\bias{i_2}{1}+\sum_{j_2=1}^{n_{0}}\W{i_2 j_2}{1}\x{j_2}{\alpha_2}\ri)}\,  \\
&=\delta_{i_1 i_2}\le(\Cb{1}+\CW{1}\frac{1}{n_0}\sum_{j=1}^{n_0}\x{j}{\alpha_1}\x{j}{\alpha_2} \ri)=\delta_{i_1 i_2} \Ti{G}{\alpha_1 \alpha_2}{1}\, , \nonumber
\end{align}
where to get to the second line we Wick-contracted the biases and weights using \eqref{eq:bias-variance-def-first} and \eqref{eq:weight-variance-def-first}. We also introduced the first-layer \term{metric}\index{metric|seealso{kernel}}\index{metric|seealso{data-dependent coupling}}\index{metric!first-layer}
\be\label{eq:first-layer-metric}
\Ti{G}{\alpha_1 \alpha_2}{1}\equiv\Cb{1}+\CW{1}\frac{1}{n_0}\sum_{j=1}^{n_0}\x{j}{\alpha_1}\x{j}{\alpha_2}\,  ,
\ee
which is a function of the two samples, $\Ti{G}{\alpha_1\alpha_2}{1}=G^{(1)}(x_{\alpha_1}, x_{\alpha_2})$, and represents the two-point correlation of preactivations in the first layer between different samples. 

The higher-point correlators can be obtained similarly. For instance, the full four-point correlation can be obtained by inserting the definition~\eqref{eq:first-layer-preactivation-def} four times and Wick-contracting the biases and weights, yielding
\begin{align}\label{eq:first-layer-fourh-moment}
&\E{\z{i_1}{\alpha_1}{1}\z{i_2}{\alpha_2}{1}\z{i_3}{\alpha_3}{1}\z{i_4}{\alpha_4}{1}}\, \\
=&\delta_{i_1 i_2} \delta_{i_3 i_4} \Ti{G}{\alpha_1 \alpha_2}{1}\Ti{G}{\alpha_3 \alpha_4}{1}+\delta_{i_1 i_3} \delta_{i_2 i_4}\Ti{G}{\alpha_1 \alpha_3}{1}\Ti{G}{\alpha_2 \alpha_4}{1}+\delta_{i_1 i_4}\delta_{i_2 i_3} \Ti{G}{\alpha_1 \alpha_4}{1}\Ti{G}{\alpha_2 \alpha_3}{1}\,   \nonumber\\
=&\E{\z{i_1}{\alpha_1}{1}\z{i_2}{\alpha_2}{1}}\E{\z{i_3}{\alpha_3}{1}\z{i_4}{\alpha_4}{1}}+\E{\z{i_1}{\alpha_1}{1}\z{i_3}{\alpha_3}{1}}\E{\z{i_2}{\alpha_2}{1}\z{i_4}{\alpha_4}{1}}\, \nonumber\\
&+\E{\z{i_1}{\alpha_1}{1}\z{i_4}{\alpha_4}{1}}\E{\z{i_2}{\alpha_2}{1}\z{i_3}{\alpha_3}{1}}\,  .\nonumber
\end{align}
Note that the end result is same as Wick-contracting $z^{(1)}$'s with the variance given by~\eqref{eq:first-layer-second-moment}.  As we recall from \S\ref{ch:tools}, this can compactly be summarized by saying that the \emph{connected} four-point correlator vanishes,
\be
\E{\z{i_1}{\alpha_1}{1}\z{i_2}{\alpha_2}{1}\z{i_3}{\alpha_3}{1}\z{i_4}{\alpha_4}{1}}\Big\vert_{\text{connected}} = 0 \, .
\ee
Similar Wick combinatorics shows that all the full higher-point correlators can be obtained simply by Wick-contracting $z^{(1)}$'s with the variance given by~\eqref{eq:first-layer-second-moment}, and hence all the connected higher-point correlators vanish. 
This means that all correlators can be generated from a Gaussian distribution with zero mean and the variance \eqref{eq:first-layer-second-moment}.%


Then, in order to write down the first-layer action, all we need is the inverse of this variance, given by a matrix $\delta_{i_1i_2}  \Kinv{\alpha_1\alpha_2}{1}$ that satisfies
\be
\sum_{j =1}^{n_1}\sum_{\beta\in\D}\le(\delta_{i_1 j}  \Kinv{\alpha_1\beta}{1} \ri)\le(\delta_{j i_2} \Ti{G}{\beta\alpha_2}{1}\ri)=\delta_{i_1 i_2}\delta^{\alpha_1}_{\ \alpha_2}\, ,
\ee
with the inverse of the first-layer metric\index{metric!inverse} $\Ti{G}{\alpha_1\alpha_2}{1}$ denoted as $ \Kinv{\alpha_1\alpha_2}{1}$ and defined by
\be
\sum_{\beta\in \D}\Kinv{\alpha_1\beta}{1} \Ti{G}{\beta\alpha_2}{1}=\delta^{\alpha_1}_{\ \alpha_2}\, .
\ee
Just as in~\S\ref{ch:tools}, we
follow the conventions of \emph{general relativity} and suppress the superscript ``$-1$'' for the inverse metric, distinguishing the metric $\Ti{G}{\alpha_1\alpha_2}{1}$ and the inverse metric $\Kinv{\alpha_1\alpha_2}{1}$  by whether sample indices are lowered or raised.
With this notation, the Gaussian distribution for the first-layer preactivations is expressed as
\be
p\!\le(z^{(1)}\Big\vert\D\ri)=\frac{1}{Z} e^{-\EFT{1}} \, ,
\ee
with the quadratic action\index{action!quadratic}\index{Gaussian distribution!action}
\be\label{eq:Gauss-action}
\ac\!\le(z^{(1)}\ri)=\frac{1}{2} \sum_{i=1}^{n_1}\sum_{\alpha_1,\alpha_2\in\D}\Kinv{\alpha_1\alpha_2}{1} \z{i}{\alpha_1}{1} \z{i}{\alpha_2}{1} \, ,
\ee
and the \terminate{partition function}
\be
Z=\int \le[\prod_{i,\alpha}d\z{i}{\alpha}{1}\ri] e^{-\EFT{1}}=\dete{2\pi G^{(1)}}^{\frac{n_1}{2}}\, ,
\ee
where $\dete{2\pi G^{(1)}}$ is the determinant of the  $\ND$-by-$\ND$ matrix $2\pi G^{(1)}_{\alpha_1 \alpha_2}$ and, whenever we write out a determinant involving the metric, it will always be that of the metric and \emph{not} of the inverse metric.\footnote{N.B.~compared to the generic quadratic action introduced in \eqref{eq:intro-quadratic-action-reprint} where the random variable $z_\mu$ was a \emph{vector} with a general index $\mu$, here in \eqref{eq:Gauss-action} we've subdivided the general index into a pair of indices, $\mu \to (i, \alpha)$, so that the first-layer preactivation $\z{i}{\alpha}{1}$ is a \neo{tensor} with a neural index\index{neural indices} $i$ and a sample index\index{sample indices} $\alpha$.}


\subsubsection{Hubbard-Stratonovich this way: algebraic derivation via action}
Rather than first computing correlators and then backing out the distribution that generates them, we can instead work with the distribution directly. Let's start with the formal expression for the preactivation distribution~\eqref{eq:first-layer-formal-expression-first-encounter} worked out in the last chapter\footnote{For architectures other than MLPs, the expression inside the \terminate{Dirac delta function} would be different, but we expect much of the following to hold so long as the parameters are sampled from simple distributions.\index{multilayer perceptron}}
\be\label{eq:first-layer-formal-expression}
p\!\le(z\big\vert\D\ri)=\int \le[  \prod_{i} d b_{i}\ p\!\le(b_{i}\ri) \ri]  \le[\prod_{i,j} d W_{ij}\ p\!\le(W_{ij}\ri) \ri] \prod_{i,\alpha} \delta\!\le(z_{i;\alpha}-b_i-\sum_{j}W_{ij}\x{j}{\alpha}\ri)\, ,
\ee 
where we have momentarily suppressed the layer superscripts ``${(1)}$'' because it is distracting. At this point, we could try to eliminate some of the integrals over the model parameters against the constraints imposed by the Dirac delta functions, but it's easy to get confused by the different numbers of model-parameter integrals and delta-function constraints.\index{Dirac delta function}

\index{Dirac delta function!integral representation}
To clarify matters, we import a neat trick from theoretical physics\index{physics} called
the \term{Hubbard-Stratonovich transformation}.
Specifically, using the following \terminate{integral representation} of the Dirac delta function~\eqref{eq:integral-form-delta-function}
\be\label{eq:integral-form-delta-function-reprint}
\delta(z-a)=\int \frac{d\Lambda}{2\pi} e^{i \Lambda (z-a)}\,   
\ee
for each constraint and also plugging in explicit expressions for the Gaussian distributions over the parameters,  we obtain
\begin{align}
p\!\le(z\big\vert\D\ri)&= \int \le[\prod_{i} \frac{d b_{i}}{\sqrt{2\pi C_b}}\ri]  \le[\prod_{i,j} \frac{d W_{ij}}{\sqrt{2\pi C_W/n_0}} \ri]\le[\prod_{i,\alpha} \frac{d \HS_{i}^{\ \alpha}}{2\pi}\ri]\, \\
&\quad \quad \times\exp\!\le[-\sum_{i} \frac{b_i^2}{2C_b}-n_0\sum_{i,j} \frac{W_{ij}^2}{2C_W}+i\sum_{i,\alpha} \HS_{i}^{\ \alpha}\le(z_{i;\alpha}-b_i-\sum_{j}W_{ij}\x{j}{\alpha}\ri)\ri]\, .  \nonumber
\end{align}
Completing the square in the exponential for both the biases $b$ and weights $W$, we see that the action is quadratic in the model parameters
\begin{align}
&- \sum_{i}\frac{b_i^2}{2C_b}-n_0\sum_{i,j} \frac{W_{ij}^2}{2C_W}+i\sum_{i,\alpha} \HS_{i}^{\ \alpha}\le(z_{i;\alpha}-b_i-\sum_{j}W_{ij}\x{j}{\alpha}\ri)\, \\
=&-\frac{1}{2C_b}\sum_{i}\le(b_i+iC_b \sum_{\alpha}\HS_{i}^{\ \alpha}\ri)^2-\frac{C_b}{2}\sum_{i}\le(\sum_{\alpha}\HS_{i}^{\ \alpha}\ri)^2\, \nonumber\\
&- \frac{n_0}{2C_W}\sum_{i,j}\le(W_{ij}+i\frac{C_W}{n_0} \sum_{\alpha}\HS_{i}^{\ \alpha}\x{j}{\alpha}\ri)^2-\frac{C_W}{2n_0}\sum_{i,j}\le(\sum_{\alpha}\HS_{i}^{\ \alpha}\x{j}{\alpha}\ri)^2+i\sum_{i,\alpha} \HS_{i}^{\ \alpha} z_{i;\alpha}\, .\nonumber
\end{align}
The biases and weights can then be integrated out, yielding
an integral representation for the first-layer distribution $p(z)$ as
\be\label{eq:to-be-referenced-in-interlayer-part-far-ahead-in-the-future}
\int \le[\prod_{i,\alpha} \frac{d \HS_{i}^{\ \alpha}}{2\pi}\ri]\ \exp\!\le[-\frac{1}{2}\sum_{i,\alpha_1,\alpha_2}\HS_{i}^{\ \alpha_1}\HS_{i}^{\ \alpha_2} \le(C_b+C_W\sum_{j}\frac{\x{j}{\alpha_1}\x{j}{\alpha_2}}{n_0} \ri)+i\sum_{i,\alpha} \HS_{i}^{\ \alpha} z_{i;\alpha}\ri]\, .
\ee
In essence, we've so far traded the delta-function constraints and the model parameters for the auxiliary Hubbard-Stratonovich variables $\HS_{i}^{\ \alpha}$, which have quadratic action and a simple linear interaction with the preactivations $z_{i;\alpha}$.

Note that  the inverse variance for the Hubbard-Stratonovich variables $\HS_{i}^{\ \alpha}$ is just the first-layer metric~\eqref{eq:first-layer-metric} we introduced in the Wick-contraction derivation,
\be
\Cb{1}+\CW{1}\sum_{j}\frac{\x{j}{\alpha_1}\x{j}{\alpha_2}}{n_0}=\Ti{G}{\alpha_1\alpha_2}{1}\, ,
\ee
where by now enough dust has settled that layer superscripts  ``$(1)$'' have been restored.
Once again completing the square,
the argument of the exponential becomes
\be
-\frac{1}{2}\sum_{i,\alpha_1,\alpha_2}\le[\Ti{G}{\alpha_1\alpha_2}{1}\le(\HS_{i}^{\ \alpha_1}-i \sum_{\beta_1} \Kinv{\alpha_1 \beta_1}{1}\z{i}{\beta_1}{1}\ri)\le(\HS_{i}^{\ \alpha_2}-i \sum_{\beta_2} \Kinv{\alpha_2 \beta_2}{1}\z{i}{\beta_2}{1}\ri)+\Kinv{\alpha_1 \alpha_2}{1}\z{i}{\alpha_1}{1}\z{i}{\alpha_2}{1}\ri]\, ,
\ee
which finally lets us integrate out the Hubbard-Stratonovich variables $\HS_{i}^{\ \alpha}$ and recover our previous result
\be\label{eq:first-layer-distribution-HS-derivation}
p\!\le(z^{(1)}\Big\vert\D\ri)= \frac{1}{\dete{2\pi G^{(1)}}^{\frac{n_1}{2}}}\exp\!\le(-\frac{1}{2}\sum_{i=1}^{n_1}\sum_{\alpha_1,\alpha_2\in\D}\Kinv{\alpha_1 \alpha_2}{1}\z{i}{\alpha_1}{1}\z{i}{\alpha_2}{1}\ri)\, .
\ee
As before, $\dete{2\pi G^{(1)}}$ represents the determinant of the  matrix $2\pi G^{(1)}_{\alpha_1 \alpha_2}$.
The first-layer distribution is Gaussian with each neuron independent, and correlations between preactivations for different samples are encoded entirely in the metric $\Ti{G}{\alpha_1\alpha_2}{1}$. 

\subsubsection{Gaussian action in action}
Now that we've obtained an action representation for the distribution of the first-layer preactivations in two different ways, let's get a feel for how to compute with it. We'll start by computing the expectation of some quantities that will be needed in~\S\ref{sec:second-layer-non-gaussian}: the expectation of two activations on the same neuron, $\E{\sigma\!\le(\z{i_1}{\alpha_1}{1}\ri)\sigma\!\le(\z{i_1}{\alpha_2}{1}\ri)}$,  and the expectation of four activations, $\E{\sigma\!\le(\z{i_1}{\alpha_1}{1}\ri)\sigma\!\le(\z{i_1}{\alpha_2}{1}\ri)\sigma\!\le(\z{i_2}{\alpha_3}{1}\ri)\sigma\!\le(\z{i_2}{\alpha_4}{1}\ri)}$, either with all four on the same neuron $i_1=i_2$ or with each pair on two separate neurons $i_1\ne i_2$.


Let's start with the two-point correlator of activations. Using the definition of the expectation and inserting the action representation of the distribution~\eqref{eq:first-layer-distribution-HS-derivation}, we get
\begin{align}
&\E{\sigma\!\le(\z{i_1}{\alpha_1}{1}\ri)\sigma\!\le(\z{i_1}{\alpha_2}{1}\ri)}\, \\
=&\int \le[ \prod_{i=1}^{n_1} \frac{\prod_{\alpha\in\D} d z_{i;\alpha}}{\sqrt{\dete{2\pi G^{(1)}}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{j=1}^{n_1}\sum_{\beta_1,\beta_2\in\D}\Kinv{\beta_1 \beta_2}{1}z_{j;\beta_1}z_{j;\beta_2}\ri)\sigma\!\le(z_{i_1;\alpha_1}\ri)\sigma\!\le(z_{i_1;\alpha_2}\ri)\,  \nonumber\\
=&\le\{\prod_{i\ne i_1}\int \le[ \frac{ \prod_{\alpha\in\D} d z_{i;\alpha}}{\sqrt{\dete{2\pi G^{(1)}}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{\beta_1,\beta_2\in\D}\Kinv{\beta_1 \beta_2}{1}z_{i;\beta_1}z_{i;\beta_2}\ri)\ri\}\, \nonumber\\
&\times\int \le[ \frac{\prod_{\alpha\in\D} d z_{i_1;\alpha}}{\sqrt{\dete{2\pi G^{(1)}}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{\beta_1,\beta_2\in\D}\Kinv{\beta_1 \beta_2}{1}z_{i_1;\beta_1}z_{i_1;\beta_2}\ri)\sigma\!\le(z_{i_1;\alpha_1}\ri)\sigma\!\le(z_{i_1;\alpha_2}\ri)\, \nonumber\\
=&\{1\}\times \le[\int \frac{\prod_{\alpha\in\D}  d z_{\alpha}}{\sqrt{\dete{2\pi G^{(1)}}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{\beta_1,\beta_2\in\D}\Kinv{\beta_1 \beta_2}{1}z_{\beta_1}z_{\beta_2}\ri)\sigma\!\le(z_{\alpha_1}\ri)\sigma\!\le(z_{\alpha_2}\ri)\, \nonumber\\
\equiv& \bra \sigma\!\le(z_{\alpha_1}\ri)\sigma\!\le(z_{\alpha_2}\ri)\ket_{G^{(1)}}\, .\nonumber
\end{align}
The second equality states that the probability distribution factorizes for each neuron due to the relation $e^{x+y}=e^{x}e^{y}$. To go from the second equality to the third, we compute the integrals for the neurons with $i \neq i_1$, which are all trivial, and we also rename the dummy integral variable $z_{i_1;\alpha}$ to $z_{\alpha}$. The final equality reintroduces the notation \eqref{eq:gauss-braket}
\be\label{eq:gaussian-integration-metric-g}
\bra F\le(z_{\alpha_1},\ldots, z_{\alpha_m}\ri)\ket_{g}\equiv \int \le[\frac{\prod_{\alpha\in \D} dz_{\alpha}}{\sqrt{\dete{2\pi g}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{\beta_1,\beta_2\in\D}g^{\beta_1\beta_2}z_{\beta_1}z_{\beta_2}\ri) F\!\le(z_{\alpha_1},\ldots, z_{\alpha_m}\ri)\, 
\ee
to describe a Gaussian expectation with variance $g$ and an arbitrary function $F\!\le(z_{\alpha_1},\ldots, z_{\alpha_m}\ri)$ over variables with sample indices \emph{only}. In other parts of this book we'll explicitly evaluate this type of Gaussian expectation in various setups for concrete choices of activation functions, but for the purpose of this chapter we will view computations as complete when they are reduced to such Gaussian expectations\index{Gaussian expectation} without any neural indices. %
Introducing further the simplifying notation
\be
\sigma_{\alpha}\equiv \sigma\!\le(z_{\alpha}\ri)\, ,
\ee
the result of the computation above can be succinctly summarized as
\be\label{eq:two-activations-Gauss}
\E{\sigma\!\le(\z{i_1}{\alpha_1}{1}\ri)\sigma\!\le(\z{i_1}{\alpha_2}{1}\ri)}= \bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(1)}}\, .
\ee

It's easy to generalize this to correlators of more than two activations. For instance, for four activations on the same neuron $i_1=i_2$, we have by the exact same manipulations
\be\label{eq:four-activations-one-neuron-Gauss} 
\E{\sigma\!\le(\z{i_1}{\alpha_1}{1}\ri)\sigma\!\le(\z{i_1}{\alpha_2}{1}\ri)\sigma\!\le(\z{i_1}{\alpha_3}{1}\ri)\sigma\!\le(\z{i_1}{\alpha_4}{1}\ri)}=\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{G^{(1)}}\, ,
\ee
and for each pair on two different neurons $i_1\ne i_2$, we have
\begin{align}\label{eq:four-activations-two-neurons-Gauss} 
&\E{\sigma\!\le(\z{i_1}{\alpha_1}{1}\ri)\sigma\!\le(\z{i_1}{\alpha_2}{1}\ri)\sigma\!\le(\z{i_2}{\alpha_3}{1}\ri)\sigma\!\le(\z{i_2}{\alpha_4}{1}\ri)}\, \\
=&\le\{\prod_{  i\notin \{i_1,i_2\} }\int \le[ \frac{ \prod_{\alpha\in\D} d z_{i;\alpha}}{\sqrt{\dete{2\pi G^{(1)}}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{\beta_1,\beta_2\in\D}\Kinv{\beta_1 \beta_2}{1}z_{i;\beta_1}z_{i;\beta_2}\ri)\ri\}\, \nonumber\\
&\times\int \le[ \frac{\prod_{\alpha\in\D} d z_{i_1;\alpha}}{\sqrt{\dete{2\pi G^{(1)}}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{\beta_1,\beta_2\in\D}\Kinv{\beta_1 \beta_2}{1}z_{i_1;\beta_1}z_{i_1;\beta_2}\ri)\sigma\!\le(z_{i_1;\alpha_1}\ri)\sigma\!\le(z_{i_1;\alpha_2}\ri)\, \nonumber\\
&\times\int \le[ \frac{\prod_{\alpha\in\D} d z_{i_2;\alpha}}{\sqrt{\dete{2\pi G^{(1)}}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{\beta_1,\beta_2\in\D}\Kinv{\beta_1 \beta_2}{1}z_{i_2;\beta_1}z_{i_2;\beta_2}\ri)\sigma\!\le(z_{i_2;\alpha_3}\ri)\sigma\!\le(z_{i_2;\alpha_4}\ri)\,  \nonumber\\
&=\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(1)}}\bra \sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{G^{(1)}}\, , \nonumber
\end{align}
where it's clear each neuron factorizes and gives separate Gaussian integrals.
This illustrates the fact that neurons are independent, and thus there is no interaction among different neurons in the first layer.
In deeper layers, the preactivation distributions are nearly-Gaussian\index{nearly-Gaussian distribution} and things will be a bit more complicated. %



\section{Second Layer: Genesis of Non-Gaussianity}
\label{sec:second-layer-non-gaussian}
In this section, we'll move onto evaluating the distribution of preactivations in the second layer of an MLP.
The second-layer preactivations are defined via
\be\label{eq:second-layer-preactivations}
\z{i}{\alpha}{2} \equiv z_i^{(2)}(x_\alpha)=
\bias{i}{2}+\sum_{j=1}^{n_{1}}\W{ij}{2}\s{j}{\alpha}{1}\,,  \quad \text{for} \quad i=1,\ldots,n_2\, ,
\ee
with the first-layer activations denoted as
\be
\s{i}{\alpha}{1}\equiv \sigma\!\le(\z{i}{\alpha}{1}\ri)\, ,
\ee
and the biases $b^{(2)}$ and weights $W^{(2)}$ sampled from Gaussian distributions.

The joint distribution of preactivations in the first and second layers can be factorized as
\be
p\!\le(z^{(2)},z^{(1)}\Big\vert\D\ri)=p\!\le(z^{(2)}\Big\vert z^{(1)}\ri) p\!\le( z^{(1)}\Big\vert\D\ri)\, .
\ee
Here the first-layer marginal distribution $p\!\le( z^{(1)}\Big\vert\D\ri)$ was evaluated in the last section, \S\ref{sec:first-layer-gaussian}, to be a Gaussian distribution~\eqref{eq:first-layer-distribution-HS-derivation} with the variance given in terms of the first-layer metric $\Ti{G}{\alpha_1\alpha_2}{1}$. As for the conditional distribution, we know that it can be expressed as\footnote{Again, the expression in the \terminate{Dirac delta function} is specific to \terminate{multilayer perceptron} architectures, but this formalism can easily be adapted for other architectures.}
\begin{align}\label{eq:second-conditioned-first}
&p\!\le(z^{(2)}\Big\vert z^{(1)}\ri)\, \\
=&\int \le[\prod_{i} d b_{i}^{(2)}\ p\!\le(b_{i}^{(2)}\ri)  \ri]\le[ \prod_{i,j} d W_{ij}^{(2)}\ p\!\le(W_{ij}^{(2)}\ri) \ri]\prod_{i,\alpha} \delta\!\le(\z{i}{\alpha}{2}-b_i^{(2)}-\sum_{j}W_{ij}^{(2)}\s{j}{\alpha}{1}\ri)\, ,\nonumber
\end{align}
from the formal expression~\eqref{eq:deeper-layer-formal-expression-first-encounter} for the preactivation distribution conditioned on the activations in the previous layer.
The marginal distribution of the second-layer preactivations can then be obtained by \term{marginalizing over}\index{marginalizing over|seealso{integrating out}} or \term{integrating out}\index{integrating out|seealso{marginalizing over}} the first-layer preactivations as
\be\label{eq:marginalization-first-layer}
p\!\le(z^{(2)}\Big\vert\D\ri)=\int \le[\prod_{i,\alpha}  d\z{i}{\alpha}{1}\ri]\ p\!\le(z^{(2)}\Big\vert z^{(1)}\ri)  p\!\le( z^{(1)}\Big\vert\D\ri)\, .
\ee
To evaluate this expression for the marginal distribution $p\!\le( z^{(2)}\Big\vert\D\ri)$, first we'll discuss how to treat the conditional distribution $p\!\le(z^{(2)}\Big\vert z^{(1)}\ri)$, and then we'll explain how to integrate over the first-layer preactivations $z^{(1)}$ governed by the Gaussian distribution~\eqref{eq:first-layer-distribution-HS-derivation}.

\subsubsection{Second-layer conditional distribution}
The conditional distribution~\eqref{eq:second-conditioned-first} can be evaluated exactly in the same way as we evaluated the first-layer distribution~\eqref{eq:first-layer-formal-expression} conditioned on the inputs, with the simple replacement of the layer indices $\ell$ as $1\to 2$ and exchanging the network input for the first-layer preactivation as $\x{j}{\alpha}\to \s{j}{\alpha}{1}$. Giving you a moment to flip back to \eqref{eq:first-layer-formal-expression} to make these substitutions and then remind yourself of the answer $\eqref{eq:first-layer-distribution-HS-derivation}$, it's easy to see that this evaluation yields
\be\label{eq:second-layer-conditional}
p\!\le(z^{(2)}\Big\vert z^{(1)}\ri)= \frac{1}{\sqrt{\dete{2\pi \widehat{G}^{(2)}}^{n_2}}} \exp\!\le(-\frac{1}{2}\sum_{i=1}^{n_2}\sum_{\alpha_1,\alpha_2\in\D}\SKinv{\alpha_1 \alpha_2}{2}\z{i}{\alpha_1}{2}\z{i}{\alpha_2}{2}\ri)\, ,
\ee
where we have defined the \emph{stochastic} second-layer metric\index{metric!stochastic}\index{metric!second-layer}
\be
\Ti{\widehat{G}}{\alpha_1 \alpha_2}{2}\equiv  \Cb{2}+\CW{2}\frac{1}{n_1}\sum_{j=1}^{n_1}\s{j}{\alpha_1}{1}\s{j}{\alpha_2}{1}\, ,
\ee
with a hat to emphasize that it is a random variable that depends on the stochastic variable $z^{(1)}$ through $\sigma^{(1)} \equiv \sigma\!\le(z^{(1)}\ri)$. Thus, we see that the second-layer conditional distribution \eqref{eq:second-layer-conditional} is a Gaussian whose variance itself is a random variable.
In particular, the stochastic second-layer metric fluctuates around the \emph{mean} second-layer metric\index{metric!mean}
\begin{align}\label{eq:second-layer-metric-mean}
\Ti{G}{\alpha_1 \alpha_2}{2}\equiv\E{\Ti{\widehat{G}}{\alpha_1 \alpha_2}{2}}&=\Cb{2}+\CW{2}\frac{1}{n_1}\sum_{j=1}^{n_1}\E{\s{j}{\alpha_1}{1}\s{j}{\alpha_2}{1}}\,   \\
&=\Cb{2}+\CW{2}\bra \sigma_{\alpha_1} \sigma_{\alpha_2} \ket_{G^{(1)}}\, , \nonumber
\end{align}
where in the last step we recalled the result~\eqref{eq:two-activations-Gauss} for evaluating the two-point correlator of the first-layer activations on the same neuron.

Around this mean, we define the fluctuation of the second-layer metric as
\be\label{eq:second-layer-mean-fluctuation}
\dKi{\alpha_1\alpha_2}{2}\equiv \Ti{\widehat{G}}{\alpha_1 \alpha_2}{2}-\Ti{G}{\alpha_1 \alpha_2}{2}=\CW{2}\frac{1}{n_1}\sum_{j=1}^{n_1}\le(\s{j}{\alpha_1}{1}\s{j}{\alpha_2}{1}-\bra \sigma_{\alpha_1} \sigma_{\alpha_2} \ket_{G^{(1)}}\ri)\, ,
\ee
which by construction has the mean zero when averaged over the first-layer preactivations,
\be\label{eq:meanzero}
\E{\dKi{\alpha_1\alpha_2}{2}}=0 \, .
\ee
The typical size of the fluctuations is given by its two-point correlator. Recalling the expressions we derived for Gaussian integrals~\eqref{eq:two-activations-Gauss} and~\eqref{eq:four-activations-one-neuron-Gauss} of two and four activations on the same neuron and their factorization property on separate neurons~\eqref{eq:four-activations-two-neurons-Gauss}, we obtain
\begin{align}\label{eq:second-layer-metric-fluctuation-two-point-function}
&\E{\dKi{\alpha_1\alpha_2}{2}\dKi{\alpha_3\alpha_4}{2}}\, \\
=&\le(\frac{\CW{2}}{n_1}\ri)^2  \sum_{j,k=1}^{n_1} \E{\le(\s{j}{\alpha_1}{1}\s{j}{\alpha_2}{1}-  \E{ \s{j}{\alpha_1}{1}\s{j}{\alpha_2}{1}}\ri) \le(\s{k}{\alpha_3}{1}\s{k}{\alpha_4}{1}-  \E{ \s{k}{\alpha_3}{1}\s{k}{\alpha_4}{1}}\ri) } \,  \nonumber\\
=&\le(\frac{\CW{2}}{n_1}\ri)^2  \sum_{j=1}^{n_1}  \le\{\E{ \s{j}{\alpha_1}{1}\s{j}{\alpha_2}{1}\s{j}{\alpha_3}{1}\s{j}{\alpha_4}{1} }  - \E{ \s{j}{\alpha_1}{1}\s{j}{\alpha_2}{1}}\E{\s{j}{\alpha_3}{1}\s{j}{\alpha_4}{1} }\ri\} \,  \nonumber\\
=& \frac{1}{n_1} \le(\CW{2}\ri)^2  \le[ \bra\sigma_{\alpha_1} \sigma_{\alpha_2} \sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(1)}}  - \bra\sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(1)}}\bra\sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(1)}} \ri]\,   \notag \\
\equiv&\frac{1}{n_1}V^{(2)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}\,, \nonumber
\end{align}
where at the end we introduced the second-layer \term{four-point vertex}\index{four-point vertex|seealso{data-dependent coupling}} $V^{(2)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}=V\le(x_{\alpha_1},x_{\alpha_2}; x_{\alpha_3},x_{\alpha_4}\ri)$, which depends on four input data points and is symmetric under the exchanges of sample indices $\alpha_1\leftrightarrow\alpha_2$, $\alpha_3\leftrightarrow\alpha_4$, and $(\alpha_1,\alpha_2)\leftrightarrow(\alpha_3,\alpha_4)$. We will understand the significance of this quantity soon in a future equation, ~\eqref{eq:C4_MLP2}.


Here, we also see our first hint of simplification in the wide regime $n_1\gg 1$: since the four-point vertex here is manifestly of order one, we see that the metric fluctuation will be suppressed in that regime.
Essentially, as the number of neurons in the first layer grows, the metric fluctuation becomes more and more Gaussian due to the central limit theorem. In the strict limit of infinite $n_1$, the metric would \emph{self-average}\index{self-averaging}, meaning that the fluctuation would vanish.


Now that we have a feel for the distribution of metric fluctuations, we are only too ready to actually integrate out the first-layer preactivations $z^{(1)}$ and obtain the marginal distribution of the second-layer preactivations $p\!\le(z^{(2)}\Big\vert\D\ri)$. We again provide two derivations, one brute-force and the other clever.

\subsubsection{Wick Wick Wick: combinatorial derivation}
The correlators of the second-layer preactivations can be written nicely in terms of the expectations of the stochastic metric that we just computed. In order to compute the correlators, first we use the fact that the conditional distribution $p\!\le(z^{(2)}\Big\vert z^{(1)}\ri)$ is Gaussian \eqref{eq:second-layer-conditional} to Wick contract the second-layer preactivations $z^{(2)}$, resulting in expressions involving expectations of the stochastic metric $\Ti{\widehat{G}}{\alpha_1\alpha_2}{2}$; we then insert expressions  for the expectations of the stochastic metric obtained above.

With this in mind, the two-point correlator of the second-layer preactivations is given by
\be\label{eq:C2_MLP2}
\E{\z{i_1}{\alpha_1}{2}\z{i_2}{\alpha_2}{2}}=\delta_{i_1i_2}\E{\Ti{\widehat{G}}{\alpha_1 \alpha_2}{2}}=\delta_{i_1i_2}\Ti{G}{\alpha_1 \alpha_2}{2}=\delta_{i_1i_2}\le(\Cb{2}+\CW{2} \bra\sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(1)}}\ri)\, ,
\ee
where to be clear we first used \eqref{eq:second-layer-conditional} to do the single Wick contraction and then inserted the expression \eqref{eq:second-layer-metric-mean} for the mean of the stochastic metric.

Similarly, the full four-point function can be evaluated as
\begin{align}\label{eq:M4_MLP2}
&\E{\z{i_1}{\alpha_1}{2}\z{i_2}{\alpha_2}{2}\z{i_3}{\alpha_3}{2}\z{i_4}{\alpha_4}{2}}\, \\
=&\delta_{i_1i_2}\delta_{i_3 i_4} \E{\Ti{\widehat{G}}{\alpha_1 \alpha_2}{2}\Ti{\widehat{G}}{\alpha_3 \alpha_4}{2}}+\delta_{i_1i_3}\delta_{i_2 i_4} \E{\Ti{\widehat{G}}{\alpha_1 \alpha_3}{2}\Ti{\widehat{G}}{\alpha_2 \alpha_4}{2}}+\delta_{i_1i_4}\delta_{i_2 i_3} \E{\Ti{\widehat{G}}{\alpha_1 \alpha_4}{2}\Ti{\widehat{G}}{\alpha_2 \alpha_3}{2}}\,, \nonumber\\
=&\delta_{i_1i_2}\delta_{i_3 i_4} \Ti{G}{\alpha_1 \alpha_2}{2}\Ti{G}{\alpha_3 \alpha_4}{2}+\delta_{i_1i_3}\delta_{i_2 i_4} \Ti{G}{\alpha_1 \alpha_3}{2}\Ti{G}{\alpha_2 \alpha_4}{2}+\delta_{i_1i_4}\delta_{i_2 i_3} \Ti{G}{\alpha_1 \alpha_4}{2}\Ti{G}{\alpha_2 \alpha_3}{2}\, \nonumber\\
&+\frac{1}{n_1}\le[\delta_{i_1i_2}\delta_{i_3 i_4}V^{(2)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}+\delta_{i_1i_3}\delta_{i_2 i_4}V^{(2)}_{(\alpha_1\alpha_3)(\alpha_2\alpha_4)}+\delta_{i_1i_4}\delta_{i_2 i_3}V^{(2)}_{(\alpha_1\alpha_4)(\alpha_2\alpha_3)} \ri]\, ,\nonumber
\end{align}
where in the first line we made three Wick contractions of the four second-layer preactivations $z^{(2)}$'s using the Gaussian distribution~\eqref{eq:second-layer-conditional}, and then in the second line we recalled~\eqref{eq:meanzero} and~\eqref{eq:second-layer-metric-fluctuation-two-point-function} for the expectations of the stochastic metric $ \Ti{\widehat{G}}{\alpha_1 \alpha_2}{2}=\Ti{G}{\alpha_1 \alpha_2}{2}+\dKi{\alpha_1\alpha_2}{2}$ over the first-layer preactivations $z^{(1)}$.
This means that the \emph{connected} four-point correlator -- recall~\eqref{eq:C4} -- after subtracting the contributions from the two-point correlators of the second-layer preactivations is given by
\begin{align}\label{eq:C4_MLP2}
&\E{\z{i_1}{\alpha_1}{2}\z{i_2}{\alpha_2}{2}\z{i_3}{\alpha_3}{2}\z{i_4}{\alpha_4}{2}}\Big\vert_{\text{connected}}\, \\
=&\frac{1}{n_1}\le[\delta_{i_1i_2}\delta_{i_3 i_4}V^{(2)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}+\delta_{i_1i_3}\delta_{i_2 i_4}V^{(2)}_{(\alpha_1\alpha_3)(\alpha_2\alpha_4)}+\delta_{i_1i_4}\delta_{i_2 i_3}V^{(2)}_{(\alpha_1\alpha_4)(\alpha_2\alpha_3)} \ri]\, . \nonumber
\end{align}
Here we see the true importance of the four-point vertex we introduced in~\eqref{eq:second-layer-metric-fluctuation-two-point-function}; it gives the connected second-layer four-point correlator and controls the near-Gaussianity of the second-layer preactivation distribution. Thus, we see that this connected correlator is suppressed in the wide regime of $n_1\gg1$, suggesting that the preactivation distribution will become more and more Gaussian 
as the network gets wider and wider. 
Given this, we see that the second-layer preactivation distribution $p\!\le(z^{(2)}\Big\vert\D\ri)$ is in general \emph{non-Gaussian}
but also simplifies significantly in the large-$n_1$ regime, becoming Gaussian in the strict $n_1=\infty$ limit and with the four-point vertex $V^{(2)}_{(\alpha_1\alpha_3)(\alpha_2\alpha_4)}$ measuring the leading deviation from Gaussianity.




To complete our combinatorial derivation, we need to find an action that generates correlations~\eqref{eq:C2_MLP2} and~\eqref{eq:C4_MLP2}.
As we know, a quadratic action cannot generate non-Gaussian distributions with nontrivial connected four-point correlators, so we need a different action that's appropriate for a \terminate{nearly-Gaussian distribution}. Intuition from single-variable non-Gaussian integrals in~\S\ref{sec:not-Gauss} suggests that we could perhaps generate the requisite correlations by including a quartic term in the action.





With that in mind, let's start with a quartic action for an $(n \ND)$-dimensional random variable $z$
\begin{align}\label{eq:generic-quartic}
S\!\le[z\ri]&=\frac{1}{2}\sum_{\alpha_1,\alpha_{2}\in\D}g^{\alpha_1\alpha_2}\sum_{i=1}^{n} z_{i;\alpha_1}z_{i;\alpha_2}\, \nonumber\\
&-\frac{1}{8}\sum_{\alpha_1,\ldots,\alpha_4\in\D}v^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}\sum_{i_1,i_2=1}^{n} z_{i_1;\alpha_1}z_{i_1;\alpha_2} \, z_{i_2;\alpha_3}z_{i_2;\alpha_4} \, ,
\end{align}
with undetermined couplings $g$ and $v$.
We will treat the quartic coupling $v$ perturbatively, an assumption that we will justify later by relating the quartic coupling $v$ to the $1/n_1$-suppressed connected four-point correlator.
Note that by construction the quartic coupling $v^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}$ has the same symmetric structure as the four-point vertex $V^{(2)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}$ with respect to the sample indices.\footnote{The conventional factor of $1/8$ in \eqref{eq:generic-quartic} is to account for this symmetry.}
Using this action, we can compute to the first order in $v$ the two-point and four-point correlators. Then, by matching with the expressions~\eqref{eq:C2_MLP2} and~\eqref{eq:C4_MLP2} for these quantities, we'll learn how to adjust the couplings $g$ and $v$ to reproduce the right statistics of second-layer preactivations in the wide regime.

Before proceeding further, it is convenient to introduce some notation. 
In~\eqref{eq:gaussian-integration-metric-g}, we defined $\bra F\!\le(z_{\alpha_1},\ldots, z_{\alpha_m}\ri) \ket_{g}$
for the average of an arbitrary function $F$ over a Gaussian distribution with variance $g$, where preactivation variables $z_{\alpha}$ have sample indices \emph{only}. 
In addition, we here define
\begin{align}\label{eq:many-neuron-gaussian-notation}
&\bra\!\bra F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_{m};\alpha_m}\ri)\ket\!\ket_{g}\, \\
\equiv& \int \le[\prod_{i=1}^{n}\frac{\prod_{\alpha\in\D} dz_{i;\alpha}}{\sqrt{\dete{2\pi g}}}\ri]\exp\!\le(-\frac{1}{2}\sum_{j=1}^n\sum_{\beta_1,\beta_2\in\D}g^{\beta_1 \beta_2}z_{j;\beta_1}z_{j;\beta_2}\ri) F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_m;\alpha_m}\ri)\,, \nonumber
\end{align}
which now includes neural indices. As we saw while working through~\eqref{eq:two-activations-Gauss} and~\eqref{eq:four-activations-two-neurons-Gauss}, this type of average factorizes into integrals of the form~\eqref{eq:gaussian-integration-metric-g} for each neuron.


With this notation in hand, the expectation of an arbitrary function $F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_{m};\alpha_m}\ri)$ against a distribution with the quartic action \eqref{eq:generic-quartic} can be rewritten in terms of Gaussian expectations, enabling the perturbative expansion in the coupling $v$ as
\begin{align}\label{eq:arbitrary-function-quartic-expectation}
&\E{F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_{m};\alpha_m}\ri)}\, \\
=&\frac{\int \le[\prod_{i,\alpha} d z_{i;\alpha}\ri] e^{-\ac\le(z\ri)}F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_{m};\alpha_m}\ri)}{\int \le[\prod_{i,\alpha} d z_{i;\alpha}\ri] e^{-\ac\le(z\ri)}}\,  \nonumber \\
=&\frac{\bra\!\!\bra \exp\!\le\{\frac{1}{8}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}\sum_{j_1,j_2=1}^{n} z_{j_1;\beta_1}z_{j_1;\beta_2}\, z_{j_2;\beta_3}z_{j_2;\beta_4} \ri\}\,F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_{m};\alpha_m}\ri)\ket\!\!\ket_{g}}{\bra\!\!\bra \exp\!\le\{\frac{1}{8}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}\sum_{j_1,j_2=1}^{n} z_{j_1;\beta_1}z_{j_1;\beta_2}\, z_{j_2;\beta_3}z_{j_2;\beta_4} \ri\}\ket\!\!\ket_{g}}\,  \nonumber\\
=&\bra\!\bra F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_{m};\alpha_m}\ri)\ket\!\ket_{g}\, \nonumber\\
&+\frac{1}{8}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}\sum_{j_1,j_2=1}^{n}\Big[\bra\!\bra z_{j_1;\beta_1}z_{j_1;\beta_2}\, z_{j_2;\beta_3}z_{j_2;\beta_4} F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_{m};\alpha_m}\ri)  \ket\!\ket_{g}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \ -\bra\!\bra z_{j_1;\beta_1}z_{j_1;\beta_2}\, z_{j_2;\beta_3}z_{j_2;\beta_4} \ket\!\ket_{g}\bra\!\bra F\!\le(z_{i_1;\alpha_1},\ldots, z_{i_{m};\alpha_m}\ri)\ket\!\ket_{g}\Big]\, \nonumber\\
&+\o{v^2}\, ,\nonumber
\end{align}
where in the first line we used the definition of the expectation, in the second line we rewrote the numerator and denominator using the notation~\eqref{eq:many-neuron-gaussian-notation} that we just introduced, and in the third line we expanded the exponential in the coupling $v$, both in the denominator and numerator. %
In short, this tells us how to perturbatively express an expectation against the full distribution with the quartic action~\eqref{eq:generic-quartic}  in terms of the leading Gaussian expectation and perturbative corrections; these perturbative contributions nonetheless involve only Gaussian expectations and hence are easy to evaluate.

With this in mind, let's consider some particular choices for $F$. 
Starting with the two-point correlator, we get
\begin{align}\label{eq:second-moment-from-action}
&\E{z_{i_1;\alpha_1} z_{i_2;\alpha_2}}\, \\%=\frac{1}{Z}\int \le[\prod_{i,\alpha} d z_{i;\alpha}\ri] e^{-S\le(z\ri)}z_{i_1;\alpha_1} z_{i_2;\alpha_2}\, \\
=&\delta_{i_1 i_2}\le[g_{\alpha_1\alpha_2}+\frac{1}{2}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}\le(n g_{\alpha_1\beta_1}g_{\alpha_2\beta_2}g_{\beta_3\beta_4}+2g_{\alpha_1\beta_1}g_{\alpha_2\beta_3}g_{\beta_2\beta_4}\ri)\ri]+\o{v^2}\, .\nonumber
\end{align}
Here the variance $g_{\alpha_1\alpha_2}$ is the inverse of the quadratic coupling, with $\sum_{\beta }g_{\alpha_1\beta}g^{\beta\alpha_2}=\delta_{\alpha_1}^{\ \alpha_2}$.
Similarly, we find that the connected four-point correlator evaluates to
\begin{align}\label{eq:fourth-cumulant-from-action}
&\E{z_{i_1;\alpha_1} z_{i_2;\alpha_2}z_{i_3;\alpha_3} z_{i_4;\alpha_4}}\Big\vert_{\text{connected}}\, \\
\equiv&\E{z_{i_1;\alpha_1} z_{i_2;\alpha_2}z_{i_3;\alpha_3} z_{i_4;\alpha_4}}-\E{z_{i_1;\alpha_1} z_{i_2;\alpha_2}}\E{z_{i_3;\alpha_3} z_{i_4;\alpha_4}}\, \nonumber\\
&-\E{z_{i_1;\alpha_1} z_{i_3;\alpha_3}}\E{z_{i_2;\alpha_2} z_{i_4;\alpha_4}}-\E{z_{i_1;\alpha_1} z_{i_4;\alpha_4}}\E{z_{i_2;\alpha_2} z_{i_3;\alpha_3}}\,  \nonumber\\
=&\delta_{i_1 i_2}\delta_{i_3 i_4}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}g_{\alpha_1\beta_1}g_{\alpha_2\beta_2}g_{\alpha_3\beta_3}g_{\alpha_4\beta_4}\, \nonumber\\
&+\delta_{i_1 i_3}\delta_{i_2 i_4}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_3)(\beta_2\beta_4)}g_{\alpha_1\beta_1}g_{\alpha_3\beta_3}g_{\alpha_2\beta_2}g_{\alpha_4\beta_4}\, \nonumber\\
&+\delta_{i_1 i_4}\delta_{i_2 i_3}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_4)(\beta_2\beta_3)}g_{\alpha_1\beta_1}g_{\alpha_4\beta_4}g_{\alpha_2\beta_2}g_{\alpha_3\beta_3}+\o{v^2}\, .\nonumber
\end{align}
Comparing these expressions,~\eqref{eq:second-moment-from-action} and \eqref{eq:fourth-cumulant-from-action}, with correlators in the second layer,~\eqref{eq:C2_MLP2} and~\eqref{eq:C4_MLP2}, it's easy to see that setting the couplings as
\begin{align}
g^{\alpha_1\alpha_2}&=\TI{G}{\alpha_1\alpha_2}{2}+\o{\frac{1}{n_1}}\, ,\label{eq:second-layer-quadratic-coupling}\\
v^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}&=\frac{1}{n_1}V_{(2)}^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}+\o{\frac{1}{n_1^2}}\, ,\label{eq:second-layer-quartic-coupling}
\end{align}
reproduces the second-layer preactivation correlators to the leading order in $1/n_1$, with the marginal distribution
\be
p\!\le(z^{(2)}\Big\vert\D\ri)=\frac{1}{Z} e^{-\EFT{2}} \, 
\ee
and quartic action \eqref{eq:generic-quartic}.
Here for convenience we have defined a version of the four-point vertex with indices \emph{raised} by the inverse of the second-layer mean metric
\be
V_{(2)}^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}\equiv\sum_{\beta_1,\ldots,\beta_4} \TI{G}{\alpha_1\beta_1}{2}\TI{G}{\alpha_2\beta_2}{2}\TI{G}{\alpha_3\beta_3}{2}\TI{G}{\alpha_4\beta_4}{2}V^{(2)}_{(\beta_1\beta_2)(\beta_3\beta_4)} \, .
\ee
Note that the quartic coupling $v$ is $\o{1/n_1}$, justifying our earlier perturbative treatment of the coupling for wide networks.
Note also that these couplings -- the inverse metric $\TI{G}{\alpha_1\alpha_2}{2}$ and the quartic coupling $V_{(2)}^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}$ -- are input-dependent.
In particular, the effective strength of interaction between neurons is set by the particular set of inputs to the network. 

This completes our first combinatorial derivation of the second-layer preactivation distribution.

\subsubsection{Schwinger-Dyson this way: algebraic derivation}
Here is a neat way to derive the action for the second-layer preactivation distribution. Plugging the conditional distribution~\eqref{eq:second-layer-conditional} into the marginalization equation~\eqref{eq:marginalization-first-layer}, the second-layer marginal distribution becomes
\be\label{eq:stochastic-marginalization-second}
p\!\le(z^{(2)}\Big\vert\D\ri)=\int \le[\prod_{i,\alpha}  d\z{i}{\alpha}{1}\ri] p\!\le(z^{(1)}\Big\vert\D\ri)\frac{ \exp\!\le(-\frac{1}{2}\sum_{j=1}^{n_2}\sum_{\alpha_1,\alpha_2\in\D}\SKinv{\alpha_1 \alpha_2}{2}\z{j}{\alpha_1}{2}\z{j}{\alpha_2}{2}\ri)}{\sqrt{\dete{2\pi \widehat{G}^{(2)}}^{n_2}}}\, .\\
\ee
We saw that the stochastic metric has a natural decomposition into mean and fluctuating parts as\index{tensor decomposition!metric mean and fluctuation}
\be
\Ti{\widehat{G}}{\alpha_1 \alpha_2}{2}=\Ti{G}{\alpha_1 \alpha_2}{2}+\dKi{\alpha_1\alpha_2}{2}\, .
\ee
Inverting this matrix to the second order in the fluctuation around the mean, we get the inverse stochastic metric\footnote{This together with the defining equation for the metric fluctuation \eqref{eq:second-layer-mean-fluctuation} are sometimes called the \terminate{Schwinger-Dyson equations} \cite{DysonEq,Schwinger452} from which this subsubsection takes its title.}
\begin{align}\label{eq:stochastic-metric-inversion}
\SKinv{\alpha_1\alpha_2}{2}=&\Kinv{\alpha_1\alpha_2}{2}-\sum_{\beta_1,\beta_{2}\in\D}\Kinv{\alpha_1\beta_1}{2}\dKi{\beta_1\beta_2}{2}\Kinv{\beta_2\alpha_2}{2}\, \\
&+\sum_{\beta_1,\ldots,\beta_{4}\in\D}\Kinv{\alpha_1\beta_1}{2}\dKi{\beta_1\beta_2}{2}\Kinv{\beta_2\beta_3}{2} \dKi{\beta_{3}\beta_{4}}{2} \Kinv{\beta_{4}\alpha_2}{2}+\o{\Delta^3}\, .\nonumber
\end{align}
Putting this into the exponential that appears in the integrand of the marginal distribution~\eqref{eq:stochastic-marginalization-second} and Taylor-expanding in the fluctuation $\dKi{\alpha_1\alpha_2}{2}$, we find
\begin{align}\label{eq:second-layer-stochastic-exponential}
&\exp\!\le(-\frac{1}{2}\sum_{j=1}^{n_2}\sum_{\alpha_1,\alpha_2\in\D}\SKinv{\alpha_1 \alpha_2}{2}\z{j}{\alpha_1}{2}\z{j}{\alpha_2}{2}\ri)\, \\
=&\exp\!\le(-\frac{1}{2}\sum_{j=1}^{n_2}\sum_{\alpha_1,\alpha_2\in\D}\Kinv{\alpha_1 \alpha_2}{2}\z{j}{\alpha_1}{2}\z{j}{\alpha_2}{2}\ri)\, \nonumber\\
&\times\Bigg\{ 1+\frac{1}{2}\sum_{i=1}^{n_2}\sum_{\alpha_1,\alpha_{2}\in\D} \le(\sum_{\beta_1,\beta_{2}\in\D}\Kinv{\alpha_1\beta_1}{2}\dKi{\beta_1\beta_2}{2}\Kinv{\beta_2\alpha_2}{2}\ri)\z{i}{\alpha_1}{2}\z{i}{\alpha_2}{2}\, \nonumber\\
&\quad -\frac{1}{2}\sum_{i=1}^{n_2}\sum_{\alpha_1,\alpha_{2}\in\D} \le(\sum_{\beta_1,\ldots,\beta_{4}\in\D}\Kinv{\alpha_1\beta_1}{2}\dKi{\beta_1\beta_2}{2}\Kinv{\beta_2\beta_3}{2} \dKi{\beta_{3}\beta_{4}}{2} \Kinv{\beta_{4}\alpha_2}{2}\ri)\z{i}{\alpha_1}{2}\z{i}{\alpha_2}{2}\, \nonumber\\
&\quad + \frac{1}{2!}\le(\frac{1}{2}\ri)^2\sum_{i_1,i_2=1}^{n_2}\sum_{\alpha_1,\ldots,\beta_{4}\in\D}\Kinv{\alpha_1\beta_1}{2}\cdots\Kinv{\alpha_4\beta_4}{2} \dKi{\beta_1\beta_2}{2} \dKi{\beta_3\beta_4}{2} \z{i_1}{\alpha_1}{2}\z{i_1}{\alpha_2}{2}\z{i_2}{\alpha_{3}}{2}\z{i_2}{\alpha_{4}}{2}+\o{\Delta^3}\Bigg\}\, .\nonumber
\end{align}
Using this expression, the determinant in the denominator becomes
\begin{align}\label{eq:second-layer-stochastic-determinant}
&\sqrt{\dete{2\pi \widehat{G}^{(2)}}^{n_2}}=\int \le[\prod_{i,\alpha}  d\z{i}{\alpha}{2}\ri]\exp\!\le(-\frac{1}{2}\sum_{j=1}^{n_2}\sum_{\alpha_1,\alpha_2\in\D}\SKinv{\alpha_1 \alpha_2}{2}\z{j}{\alpha_1}{2}\z{j}{\alpha_2}{2}\ri)\, \\
=&\sqrt{\dete{2\pi G^{(2)}}^{n_2}}\Bigg[1+\frac{n_2}{2}\sum_{\beta_1,\beta_2\in\D}\dKi{\beta_1\beta_2}{2}\Kinv{\beta_1\beta_2}{2}\, \nonumber\\
&\quad+\sum_{\beta_1,\ldots,\beta_4\in\D}\dKi{\beta_1\beta_2}{2}\dKi{\beta_3\beta_4}{2}\le(\frac{n_2^2}{8}\Kinv{\beta_1\beta_2}{2}\Kinv{\beta_3\beta_4}{2}-\frac{n_2}{4}\Kinv{\beta_1\beta_3}{2}\Kinv{\beta_2\beta_4}{2}\ri)+\o{\Delta^3}\Bigg]\, ,\nonumber
\end{align}
where on the first line we re-expressed the determinant as a Gaussian integral, and on the subsequent line we plugged in \eqref{eq:second-layer-stochastic-exponential} and integrated over the second-layer preactivations $z^{(2)}$.

Next, plugging these two expressions~\eqref{eq:second-layer-stochastic-exponential} and~\eqref{eq:second-layer-stochastic-determinant} back into our expression for the second-layer distribution~\eqref{eq:stochastic-marginalization-second}, we can now integrate out the first-layer preactivations, giving
\begin{align}
p\!\le(z^{(2)}\Big\vert\D\ri)=&\frac{1}{\sqrt{\dete{2\pi G^{(2)}}^{n_2}}}\exp\!\le(-\frac{1}{2}\sum_{j=1}^{n_2}\sum_{\alpha_1,\alpha_2\in\D}\Kinv{\alpha_1 \alpha_2}{2}\z{j}{\alpha_1}{2}\z{j}{\alpha_2}{2}\ri)\, \\
&\times\Bigg\{ \le[1+\o{\frac{1}{n_1}}\ri]+\sum_{i=1}^{n_2}\sum_{\alpha_1,\alpha_{2}\in\D}\le[\o{\frac{1}{n_1}}\ri]\z{i_1}{\alpha_1}{2}\z{i_1}{\alpha_2}{2}\,\nonumber\\
&\quad +\frac{1}{8n_1}\sum_{i_1,i_2=1}^{n_2}\sum_{\alpha_1,\ldots,\alpha_{4}\in\D}\TI{V}{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}{2} \z{i_1}{\alpha_1}{2}\z{i_1}{\alpha_2}{2}\z{i_2}{\alpha_{3}}{2}\z{i_2}{\alpha_{4}}{2}\Bigg\}+\o{\frac{1}{n_1^2}}\,, \nonumber
\end{align}
where we have used the fact that expectations of the metric fluctuation are given by $\E{\dKi{\beta_1\beta_2}{2}}=0$ and $\E{\dKi{\beta_1\beta_2}{2}\dKi{\beta_3\beta_4}{2}}=\frac{1}{n_1}\Ti{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{2}$.\footnote{We tacitly assumed that the expectation of $\widehat{\Delta G}^{m\geq3}$ are of order $\o{1/n_1^2}$ or greater.\label{foot:second-layer-hierarchy}
For instance, you can follow exactly the same steps as in \eqref{eq:second-layer-metric-fluctuation-two-point-function} and compute
\begin{align}
&\E{\dKi{\beta_1\beta_2}{2}\dKi{\beta_3\beta_4}{2}\dKi{\beta_5\beta_6}{2}}\, \\
=&\frac{1}{n_1^2}\le(\CW{2}\ri)^3\Big[\bra\sigma_{\alpha_1} \sigma_{\alpha_2} \sigma_{\alpha_3} \sigma_{\alpha_4}\sigma_{\alpha_5} \sigma_{\alpha_6}\ket_{G^{(1)}}  - \bra\sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(1)}}\bra\sigma_{\alpha_3} \sigma_{\alpha_4}\sigma_{\alpha_5} \sigma_{\alpha_6}\ket_{G^{(1)}}\, \notag\\
&\qquad\qquad\qquad- \bra\sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(1)}}\bra\sigma_{\alpha_5} \sigma_{\alpha_6}\sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(1)}}- \bra\sigma_{\alpha_5} \sigma_{\alpha_6}\ket_{G^{(1)}}\bra\sigma_{\alpha_1} \sigma_{\alpha_2}\sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(1)}}\, \notag\\
&\qquad\qquad\qquad+2\bra\sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(1)}}\bra\sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(1)}}\bra\sigma_{\alpha_5} \sigma_{\alpha_6}\ket_{G^{(1)}}\Big]\, .\notag
\end{align}
Just as in the middle step of \eqref{eq:second-layer-metric-fluctuation-two-point-function}, here again you've likely noticed that nonzero contributions arise only when all the neural indices coincide. You can further use that same insight to show that $\E{\le(\widehat{\Delta G}^{(2)}\ri)^{m}}=\o{1/n^{m-1}_1}$.
}
Taking the logarithm to isolate the action and absorbing the irrelevant constant terms into the partition function, we arrive at the correct expression for the second-layer quartic action to leading order in the first layer width
\begin{align}\label{eq:second-layer-quartic-action-in-SD-subsubsection}
\ac\!\le(z\ri)=&\frac{1}{2}\sum_{\alpha_1,\alpha_{2}\in\D}\le[G_{(2)}^{\alpha_1\alpha_2}+\o{\frac{1}{n_1}}\ri]\sum_{i=1}^{n_2} z_{i;\alpha_1}z_{i;\alpha_2}\, \\
&-\frac{1}{8}\sum_{\alpha_1,\ldots,\alpha_4\in\D}\frac{1}{n_1}V_{(2)}^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}\sum_{i_1,i_2=1}^{n_2} z_{i_1;\alpha_1}z_{i_1;\alpha_2}z_{i_2;\alpha_3}z_{i_2;\alpha_4}+\o{\frac{1}{n_1^2}}\, .\nonumber
\end{align}

Here, a prudent reader might wonder about our dropping of the $1/n_1$ correction to the quadratic coupling, while keeping the quartic coupling despite being of the same order. The main reason for this is that such a correction is a \emph{subleading} contribution to the two-point correlator, while the quartic coupling gives the \emph{leading} contribution to the connected 
four-point correlator. Indeed, we shall encounter various observables whose leading %
contributions stem solely from the nontrivial neuron-neuron interaction induced by the quartic coupling. By contrast, the correction to the quadratic coupling at finite-width is just a small quantitative effect. Nevertheless, we will compute this subleading correction in \S\ref{sec:loop-correction} for completeness.\footnote{It will also turn out (\S\ref{sec:signal_prop_finite_width}) that by fine-tuning the initialization hyperparameters such subleading corrections are suppressed with depth in comparison to nearly-Gaussian corrections, so in a sense this subleading correction to the quadratic coupling can be doubly ignored.}













\subsubsection{Nearly-Gaussian action in action}\index{nearly-Gaussian distribution!action}
Having completed the two derivations, before moving on to the next section, let's use this opportunity to get a bit more of a feel for how to compute with a \terminate{nearly-Gaussian distribution}. Paralleling what we did with the Gaussian action in the last section, let's evaluate the expectation of two activations on the same neuron
and four activations,
with all four on the same neuron or pairs on separate neurons.
The resulting expressions will enable us to obtain the distributions of the preactivations in deeper layers.


In the following, we are just applying the formula~\eqref{eq:arbitrary-function-quartic-expectation} for the expectation of a general function.
These expressions will be valid for any layer $\ell > 1$.
First, for two activations on the same neuron, we find %
\begin{align}\label{eq:two-activations-deep}
&\E{\sigma\!\le(z_{i_1;\alpha_1}\ri)\sigma\!\le(z_{i_1;\alpha_2}\ri)}\, \\
=&\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{g}+\frac{1}{8}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}\, \nonumber\\
&\quad \quad \quad \quad \quad \times\Big[\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1} z_{\beta_2}-g_{\beta_1\beta_2}\ri)\le(z_{\beta_3} z_{\beta_4}-g_{\beta_3\beta_4}\ri)\ket_{g}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad +2n \bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1} z_{\beta_2}-g_{\beta_1\beta_2}\ri)\ket_{g} g_{\beta_3\beta_4}-2\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{g}g_{\beta_1\beta_3}g_{\beta_2\beta_4}\Big]+\o{v^2}\, , \nonumber
\end{align}
where we assume the reader is by now familiar enough with Gaussian integrals and factorization into separate neurons so as not to include the middle steps. This result
highlights that the addition of the quartic coupling $v$ has a nontrivial effect even on the two-point correlator of same-neuron activations. 
We can similarly compute the expectation of four activations on the same neuron, but we'll need only the leading Gaussian contribution, namely
\begin{align}\label{eq:four-activations-same-deep}
&\E{\sigma\!\le(z_{i_1;\alpha_1}\ri)\sigma\!\le(z_{i_1;\alpha_2}\ri)\sigma\!\le(z_{i_1;\alpha_3}\ri)\sigma\!\le(z_{i_1;\alpha_4}\ri)}-\E{\sigma\!\le(z_{i_1;\alpha_1}\ri)\sigma\!\le(z_{i_1;\alpha_2}\ri)}\E{\sigma\!\le(z_{i_1;\alpha_3}\ri)\sigma\!\le(z_{i_1;\alpha_4}\ri)}\, \\
=&\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{g}-\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{g}\bra\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{g}+\o{v}\, ,\nonumber
\end{align}
where we subtracted off the contribution from the two-point correlators as that's what'll appear in the next section.
Finally, the similar expectation of four activations on two different pairs of neurons $i_1 \neq i_2$ can be evaluated by the application of the formula~\eqref{eq:arbitrary-function-quartic-expectation} and neuron factorizations in Gaussian expectations, yielding 
\begin{align}\label{eq:four-activations-different-deep-connected}
&\E{\sigma\!\le(z_{i_1;\alpha_1}\ri)\sigma\!\le(z_{i_1;\alpha_2}\ri)\sigma\!\le(z_{i_2;\alpha_3}\ri)\sigma\!\le(z_{i_2;\alpha_4}\ri)}-\E{\sigma\!\le(z_{i_1;\alpha_1}\ri)\sigma\!\le(z_{i_1;\alpha_2}\ri)}\E{\sigma\!\le(z_{i_2;\alpha_3}\ri)\sigma\!\le(z_{i_2;\alpha_4}\ri)}\, \nonumber\\
=&\frac{1}{8}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}\sum_{j_1,j_2=1}^{n}\, \\
&\times\Big[\bra\!\bra z_{j_1;\beta_1}z_{j_1;\beta_2}\, z_{j_2;\beta_3}z_{j_2;\beta_4} \sigma_{i_1;\alpha_1} \sigma_{i_1;\alpha_2} \sigma_{i_2;\alpha_3}\sigma_{i_2;\alpha_4}   \ket\!\ket_{g}\, \nonumber\\
&\quad -\bra\!\bra z_{j_1;\beta_1}z_{j_1;\beta_2}\, z_{j_2;\beta_3}z_{j_2;\beta_4} \sigma_{i_1;\alpha_1} \sigma_{i_1;\alpha_2} \ket\!\ket_{g}\bra\!\bra\sigma_{i_2;\alpha_3}\sigma_{i_2;\alpha_4}   \ket\!\ket_{g}\, \nonumber\\
&\quad -\bra\!\bra z_{j_1;\beta_1}z_{j_1;\beta_2}\, z_{j_2;\beta_3}z_{j_2;\beta_4} \sigma_{i_2;\alpha_3} \sigma_{i_2;\alpha_4} \ket\!\ket_{g}\bra\!\bra\sigma_{i_1;\alpha_1}\sigma_{i_1;\alpha_2}   \ket\!\ket_{g}\, \nonumber\\
&\quad +\bra\!\bra z_{j_1;\beta_1}z_{j_1;\beta_2}\, z_{j_2;\beta_3}z_{j_2;\beta_4} \ket\!\ket_{g}\bra\!\bra\sigma_{i_1;\alpha_1} \sigma_{i_1;\alpha_2} \ket\!\ket_{g}\bra\!\bra\sigma_{i_2;\alpha_3}\sigma_{i_2;\alpha_4}   \ket\!\ket_{g}\Big]\, \nonumber\\
=&\frac{1}{4}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1} z_{\beta_2}-g_{\beta_1\beta_2}\ri)\ket_{g}\bra \sigma_{\alpha_3}\sigma_{\alpha_4} \le(z_{\beta_3} z_{\beta_4}-g_{\beta_3\beta_4}\ri)\ket_{g}+\o{v^2}\, ,\nonumber 
\end{align}
where we get nonzero contributions only when $j_1=i_1$ and $j_2=i_2$ or when $j_1=i_2$ and $j_2=i_1$.
This shows that pairs of activations can only correlate with the addition of the quartic coupling to the action, hinting at the role of finite width for features learning.
More generally, consider functions $\mathcal{F}\!\le(z_{i_1;\A_1}\ri)$
and $\mathcal{G}\!\le(z_{i_2; \A_2}\ri)$
of preactivations that depend on subsamples $\A_1$ and $\A_2\subset\D$, respectively, where with a slight abuse of notation we put the set dependences into the subscripts. For distinct neurons $i_1\ne i_2$, the calculation identical to the one just above shows that their covariance is given by
\begin{align}\label{eq:general-covariance}
&\text{Cov}\Big[\mathcal{F}\!\le(z_{i_1; \A_1}\ri)\!, \, \mathcal{G}\!\le(z_{i_2; \A_2}\ri)\Big] \\
\equiv&\mathbb{E}\Big[ \mathcal{F}\!\le(z_{i_1;\A_1}\ri)\mathcal{G}\!\le(z_{i_2;\A_2}\ri)\Big]-\mathbb{E}\Big[\mathcal{F}\!\le(z_{i_1;\A_1}\ri)\Big]\, \mathbb{E}\Big[\mathcal{G}\!\le(z_{i_2;\A_2}\ri)\Big]\, \notag \\
=&\frac{1}{4}\sum_{\beta_1,\ldots,\beta_4\in\D}v^{(\beta_1\beta_2)(\beta_3\beta_4)}\Big\langle \le(z_{\beta_1} z_{\beta_2}-g_{\beta_1\beta_2}\ri)  \mathcal{F}\!\le(z_{\A_1}\ri) \Big\rangle_{g}\Big\langle  \le(z_{\beta_3} z_{\beta_4}-g_{\beta_3\beta_4}\ri)\mathcal{G}\!\le(z_{\A_2}\ri)\Big\rangle_{g}+\o{v^2}\, .\nonumber 
\end{align}
This formula will be very useful in the future.

\section{Deeper Layers: Accumulation of Non-Gaussianity}
\label{sec:deeper-layer-accumulation}
The preactivations in the deeper layers are recursively given by
\be
\z{i}{\alpha}{\ell+1} =\bias{i}{\ell+1}+\sum_{j=1}^{n_{\ell}}\W{ij}{\ell+1}\s{j}{\alpha}{\ell}\, , \quad \text{for} \quad i=1,\ldots,n_{\ell+1}\, ,
\ee
with the activations in the previous layer abbreviated as
\be
\s{i}{\alpha}{\ell}\equiv \sigma\!\le(\z{i}{\alpha}{\ell}\ri)\, .
\ee
We can obtain the marginal distributions of the preactivations in these deeper layers -- including the output distribution $p\!\le(z^{(L)}\Big\vert\D\ri)$ -- by following the procedure that we implemented for the second-layer distribution.
The only complication is that the preactivation distribution in the previous layer is no longer Gaussian, like it was for the first layer.

The three key concepts of the derivation are: recursion, action, and $1/n$-expansion. Let's walk through them one by one.

\subsubsection{Recursion}
The idea of recursion is to start with information contained in the marginal distribution $p\!\le( z^{(\ell)}\Big\vert\D\ri)$ in the $\ell$-th layer and obtain the marginal distribution for the $(\ell+1)$-th layer.
The change of the marginal preactivation distribution from layer to layer can be captured by first writing out the joint probability distribution of preactivations in adjacent layers $\ell$ and $\ell+1$,
\be\label{eq:joint-distribution-adjacent-layers}
p\!\le(z^{(\ell+1)},z^{(\ell)}\Big\vert\D\ri)=p\!\le(z^{(\ell+1)}\Big\vert z^{(\ell)}\ri) p\!\le( z^{(\ell)}\Big\vert\D\ri)\, ,
\ee
then calculating the conditional probability distribution $p\!\le(z^{(\ell+1)}\Big\vert z^{(\ell)}\ri)$, and finally marginalizing over the preactivations at the $\ell$-th layer as
\be\label{eq:marginal-integrated-out-ell-layer}
p\!\le(z^{(\ell+1)}\Big\vert\D\ri)=\int \le[\prod_{i,\alpha}  d\z{i}{\alpha}{\ell}\ri] p\!\le(z^{(\ell+1)}\Big\vert z^{(\ell)}\ri)  p\!\le( z^{(\ell)}\Big\vert\D\ri)\, .
\ee
In particular, the conditional probability distribution $p\!\le(z^{(\ell+1)}\Big\vert z^{(\ell)}\ri)$ serves as a \term{transition matrix}, bridging preactivation distributions in adjacent layers.

The calculation of this conditional distribution proceeds identically to the one we performed for the first layer \eqref{eq:first-layer-formal-expression} and then repurposed for computing the second-layer conditional distribution \eqref{eq:second-conditioned-first}. 
If you'd like, you can again follow along with~\S\ref{sec:first-layer-gaussian}, replacing $z^{(1)}$ by $z^{(\ell+1)}$ and $\x{j}{\alpha}$ by $\s{j}{\alpha}{\ell}$, and obtain
\be\label{eq:general-layer-conditional}
p\!\le(z^{(\ell+1)}\Big\vert z^{(\ell)}\ri) = \frac{1}{\sqrt{\dete{2\pi \widehat{G}^{(\ell+1)}}^{n_{\ell+1}}}} \exp\!\le(-\frac{1}{2}\sum_{i=1}^{n_{\ell+1}}\sum_{\alpha_1,\alpha_2\in\D}\SKinv{\alpha_1 \alpha_2}{\ell+1}\z{i}{\alpha_1}{\ell+1}\z{i}{\alpha_2}{\ell+1}\ri)\,,
\ee
with the $(\ell+1)$-th-layer stochastic metric\index{metric!l-th-layer@$\ell$-th-layer} %
\be\label{eq:general-stochastic-metric}
\Ti{\widehat{G}}{\alpha_1 \alpha_2}{\ell+1}\equiv  \Cb{\ell+1}+\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}\, ,
\ee
depending on the random variables $z^{(\ell)}$ in the previous layer $\ell$ through the activations $\sigma^{(\ell)}$.  Note that all the correlators with odd numbers of the $(\ell+1)$-th-layer preactivations vanish while even-point correlators are obtained through Wick's contractions, yielding
\be\label{eq:general-even-moment}
\E{\z{i_1}{\alpha_1}{\ell+1}\cdots\z{i_{2m}}{\alpha_{2m}}{\ell+1}}=\sum_{\text{all pairings}}\delta_{i_{k_1} i_{k_2}}\cdots \delta_{i_{k_{2m-1}} i_{k_{2m}}} \E{\Ti{\widehat{G}}{\alpha_{k_1} \alpha_{k_2}}{\ell+1}\cdots \Ti{\widehat{G}}{\alpha_{k_{2m-1}} \alpha_{k_{2m}}}{\ell+1}},
\ee
where the sum runs over all the $(2m-1)!!$ parings of auxiliary indices $(k_1,\ldots,k_{2m})$.
On the left hand, the expectation value characterizes the $(\ell+1)$-th-layer preactivation distribution;
on the right hand, the expectation value becomes a correlator of $\ell$-th-layer activations upon plugging in the stochastic metric~\eqref{eq:general-stochastic-metric}, which can be evaluated with the $\ell$-th-layer distribution.

The mean of the stochastic metric is given by
\be\label{eq:mean-metric-any-layer}
\Ti{G}{\alpha_1 \alpha_2}{\ell+1}\equiv\E{\Ti{\widehat{G}}{\alpha_1 \alpha_2}{\ell+1}}=\Cb{\ell+1}+\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}\, ,
\ee
and this mean metric governs the two-point correlator in the $(\ell+1)$-th layer through
\be\label{eq:C2_MLPH}
\E{\z{i_1}{\alpha_1}{\ell+1}\z{i_2}{\alpha_2}{\ell+1}}=\delta_{i_1i_2}\E{\Ti{\widehat{G}}{\alpha_1 \alpha_2}{\ell+1}}=\delta_{i_1i_2}\Ti{G}{\alpha_1 \alpha_2}{\ell+1}\, ,
\ee
as we saw for the second layer~\eqref{eq:C2_MLP2} as a special case of the equation~\eqref{eq:general-even-moment}.
Meanwhile, the fluctuation around the mean
\be\label{eq:metric-fluctuation-general-layer}
\dKi{\alpha_1\alpha_2}{\ell+1}\equiv \Ti{\widehat{G}}{\alpha_1 \alpha_2}{\ell+1}-\Ti{G}{\alpha_1 \alpha_2}{\ell+1}=\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\le(\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}-\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}\ri)\, ,
\ee
obviously has zero mean,
\be
\E{\dKi{\alpha_1\alpha_2}{\ell+1}}=0\, ,
\ee
and has a magnitude 
\be\label{eq:vertex-in-terms-of-metric-fluctuation}
\frac{1}{n_{\ell}}V^{(\ell+1)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}\equiv \E{\dKi{\alpha_1\alpha_2}{\ell+1}\dKi{\alpha_3\alpha_4}{\ell+1}}=\E{ \Ti{\widehat{G}}{\alpha_1 \alpha_2}{\ell+1} \Ti{\widehat{G}}{\alpha_3 \alpha_4}{\ell+1}}-\Ti{G}{\alpha_1 \alpha_2}{\ell+1}\Ti{G}{\alpha_3 \alpha_4}{\ell+1}\, .
\ee
Here we have introduced the $(\ell+1)$-th-layer four-point vertex $V^{(\ell+1)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}$, generalizing the second-layer four-point vertex~\eqref{eq:second-layer-metric-fluctuation-two-point-function},
which governs the connected four-point correlator in the $(\ell+1)$-th layer. Specifically, following along with the manipulations for the second layer -- cf.~\eqref{eq:M4_MLP2} and~\eqref{eq:C4_MLP2} -- or simply applying the general expression~\eqref{eq:general-even-moment}, we see
\begin{align}\label{eq:C4_MLPH}
&\E{\z{i_1}{\alpha_1}{\ell+1}\z{i_2}{\alpha_2}{\ell+1}\z{i_3}{\alpha_3}{\ell+1}\z{i_4}{\alpha_4}{\ell+1}}\Big\vert_{\text{connected}}\, \\
=&\frac{1}{n_{\ell}}\le[\delta_{i_1i_2}\delta_{i_3 i_4}V^{(\ell+1)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}+\delta_{i_1i_3}\delta_{i_2 i_4}V^{(\ell+1)}_{(\alpha_1\alpha_3)(\alpha_2\alpha_4)}+\delta_{i_1i_4}\delta_{i_2 i_3}V^{(\ell+1)}_{(\alpha_1\alpha_4)(\alpha_2\alpha_3)} \ri]\, .\nonumber
\end{align}

In summary, what we have so far are the expressions for the two-point correlator~\eqref{eq:C2_MLPH} and the connected four-point correlator~\eqref{eq:C4_MLPH} of the $(\ell+1)$-th-layer preactivations in terms of the correlators of the $\ell$-th-layer activations, and related expressions for higher-point correlators~\eqref{eq:general-even-moment} if the need arises. The strategy of our recursive approach is to first evaluate these $\ell$-th-layer activation correlators given the $\ell$-th-layer distribution $p\!\le(z^{(\ell)}\Big\vert\D\ri)$ and from them obtain the $(\ell+1)$-th-layer preactivation correlators. 
Using these correlators, we can then reconstruct the $(\ell+1)$-th layer marginal distribution $p\!\le(z^{(\ell+1)}\Big\vert\D\ri)$. %
Both the evaluation of the $\ell$-th-layer activation correlators and the reconstruction of the distribution at the $(\ell+1)$-th layer can be efficiently implemented through the use of the action.



\subsubsection{Action}
The preactivation distribution $p\!\le(z^{(\ell)}\Big\vert\D\ri)$ can be written in terms of an action as
\be\label{eq:marginal-distribution-action-ansatz}
p\!\le(z^{(\ell)}\Big\vert\D\ri)=\frac{e^{-\EFT{\ell}}}{Z(\ell)} \, ,
\ee
with the $\ell$-th layer \terminate{partition function} given by
\be\label{eq:chapter-ngp-partition-function}
Z(\ell) \equiv \int \le[\prod_{i,\alpha} d\z{i}{\alpha}{\ell}\ri] \, e^{-\EFT{\ell}} \, ,
\ee
and our ansatz for the action given by the following expansion:
\begin{align}\label{eq:general-ell-action}
\ac\!\le(z^{(\ell)}\ri)
\equiv&\frac{1}{2}\sum_{i=1}^{n_{\ell}}\sum_{\alpha_1,\alpha_2\in\D} g^{\alpha_1\alpha_2}_{(\ell)} \z{i}{\alpha_1}{\ell}\z{i}{\alpha_2}{\ell}\, \\
&-\frac{1}{8}\sum_{i_1,i_2=1}^{n_{\ell}}\sum_{\alpha_1,\ldots,\alpha_4\in\D}v^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}_{(\ell)} \z{i_1}{\alpha_1}{\ell}\z{i_1}{\alpha_2}{\ell}\, \z{i_2}{\alpha_3}{\ell}\z{i_2}{\alpha_4}{\ell}+\ldots\, .\nonumber%
\end{align}
This ansatz encompasses both the actions we had in~\S\ref{sec:first-layer-gaussian} for the first-layer preactivations -- with $\TI{g}{\alpha_1\alpha_2}{1}=\TI{G}{\alpha_1\alpha_2}{1}$ and $v_{(1)}=0$ -- and for the second-layer preactivations in~\S\ref{sec:second-layer-non-gaussian} -- with the couplings $g_{(2)}$ and $v_{(2)}$ given by~\eqref{eq:second-layer-quadratic-coupling} and~\eqref{eq:second-layer-quartic-coupling}, respectively. In fact, this represents the most general expansion around the Gaussian action, given the symmetries of preactivation correlators~\eqref{eq:general-even-moment}. In particular, only even powers of preactivations show up in the action since we know that correlators with odd numbers of preactivations vanish.


Here, the coefficients $g^{\alpha_1\alpha_2}_{(\ell)}$, $v^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}_{(\ell)}$,
and the implied additional terms in the expansion are \textbf{data-dependent couplings}\index{data-dependent coupling|textbf}\index{coupling!data-dependent|see{data-dependent coupling}} that together govern the interactions of the neural preactivations and are simply related to the correlators of preactivations $z^{(\ell)}$. In particular, in~\S\ref{sec:second-layer-non-gaussian} we gave two derivations  for the relations between quadratic and quartic couplings on the one hand and two-point and four-point correlators on the other hand. The same argument applies for an arbitrary layer $\ell$, and so we have
\begin{align}
\TI{g}{\alpha_1\alpha_2}{\ell}&=\TI{G}{\alpha_1\alpha_2}{\ell}+\o{v,\ldots}\, ,\label{eq:two-point-match-general}\\
\TI{v}{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}{\ell}&=\frac{1}{n_{\ell-1}}\TI{V}{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}{\ell}+\o{v^2, \ldots}\, , \label{eq:four-point-match-general} 
\end{align}
with the understanding that the raised indices of the four-point vertex are shorthand for contraction with the $\ell$-th-layer inverse metric
\be\label{eq:vertex-UUUU-dddd}
\TI{V}{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}{\ell}\equiv \sum_{\beta_1,\ldots,\beta_4\in\D}\TI{G}{\alpha_1\beta_1}{\ell}\TI{G}{\alpha_2\beta_2}{\ell}\TI{G}{\alpha_3\beta_3}{\ell}\TI{G}{\alpha_4\beta_4}{\ell}\Ti{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\, .
\ee
Note that the higher-order terms $\o{...}$ in~\eqref{eq:two-point-match-general} and~\eqref{eq:four-point-match-general} can be neglected self-consistently if and only if the quartic coupling $v$ and higher-order couplings are perturbatively small. This is indeed the case when networks are sufficiently wide,
as we will show next.






\subsubsection{Large-width expansion}
\index{$1/n$ expansion}
Now we have our work cut out for us. 
First, note that these mappings,~\eqref{eq:two-point-match-general} and~\eqref{eq:four-point-match-general},
between the correlators and couplings already accomplish one task mentioned in our recursive strategy. Namely, when applied to the $(\ell+1)$-th layer, they reconstruct the $(\ell+1)$-th-layer distribution out of the $(\ell+1)$-th-layer preactivation correlators. The only remaining task then is to use the $\ell$-th-layer action~\eqref{eq:general-ell-action} to compute
the expectations of the $\ell$-th-layer activations $\sigma^{(\ell)}$ that appear in the expressions for the two-point correlator~\eqref{eq:C2_MLPH} and four-point correlator~\eqref{eq:C4_MLPH} of the $(\ell+1)$-th-layer preactivations $z^{(\ell+1)}$. 

\index{$1/n$ expansion}
These calculations simplify in the wide regime with a large number of neurons per layer
\be\label{eq:wide-regime}
n_1,n_2,\ldots,n_{L-1}\sim n \gg1\, .
\ee 
As has been advertised, this large-but-finite-width regime is where networks become both practically usable and theoretically tractable.
Specifically, the relations~\eqref{eq:two-point-match-general} and~\eqref{eq:four-point-match-general} between correlators and couplings simplify in this regime and higher-order non-Gaussian corrections can be self-consistently truncated in a series in $1/n$.\footnote{In the language of~\S\ref{sec:marginalization-group-flow}, such a truncation is preserved under the RG flow.}
To be precise, we inductively assume that the mean metric $G^{(\ell)}=\o{1}$ and the four-point vertex $V^{(\ell)}=\o{1}$ are both of order one at the $\ell$-th layer  -- as was the case for the first and second layers -- and show that the same holds true at the $(\ell+1)$-th layer.
This inductive assumption in particular implies through~\eqref{eq:two-point-match-general} and~\eqref{eq:four-point-match-general} that the quartic coupling $v_{(\ell)}=\o{1/n}$ is perturbatively small at the $\ell$-th layer and that the quadratic coupling is given by $g_{(\ell)}=G_{(\ell)}+\o{1/n}$. In carrying out this inductive proof, we obtain the recursion relations that govern the change in the preactivation distributions from the $\ell$-th layer to the $(\ell+1)$-th layer.%




To begin, we see that the two-point correlator in the $(\ell+1)$-th layer~\eqref{eq:C2_MLPH} is given simply in terms of the metric
\be\label{eq:C2_MLP_exact}
\Ti{G}{\alpha_1 \alpha_2}{\ell+1}=\Cb{\ell+1}+\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}\, .
\ee
With foresight, we already evaluated this particular two-point correlator of activations \eqref{eq:two-activations-deep} in the last section. Inserting this result, along with the quadratic coupling $g_{(\ell)}=G_{(\ell)}+\o{1/n}$ and quartic coupling $v_{(\ell)}=\o{1/n}$, we find  
\be\label{eq:G-recursion-tree}
\Ti{G}{\alpha_1 \alpha_2}{\ell+1}=\Cb{\ell+1}+\CW{\ell+1}\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(\ell)}}+\o{\frac{1}{n}}\, ,
\ee
which is the leading recursion for the two-point correlator of preactivations.\footnote{Note that the difference from the second-layer calculation in \S\ref{sec:second-layer-non-gaussian} is just that the expectation in \eqref{eq:C2_MLP_exact} is not exactly Gaussian, but has a $1/n$ correction. This highlights the main difference with that section, which is that the distribution in the prior layer is nearly-Gaussian.} We see  that this is self-consistent; any metric $\Ti{G}{}{\ell}$ that is of order one will give an order-one metric $\Ti{G}{}{\ell+1}$ in the next layer as well. The correction is suppressed by $\o{1/n}$, which affects only the subleading term  in the  quadratic coupling $g_{(\ell+1)}=G_{(\ell+1)}+\o{1/n}$. Note that, neglecting the subleading $1/n$ correction and replacing $G$ by $g$,  the recursion~\eqref{eq:G-recursion-tree} for the two-point correlator can also be thought of as the leading recursion for the quadratic coupling.

Next, let's evaluate the four-point correlator~\eqref{eq:C4_MLPH},
which involves computing the magnitude of the metric fluctuation~\eqref{eq:vertex-in-terms-of-metric-fluctuation}. Substituting in our general expression for the $(\ell+1)$-th-layer metric fluctuation \eqref{eq:metric-fluctuation-general-layer}, we get
\begin{align}\label{eq:midpoint-vertex-recursion}
&\frac{1}{n_{\ell}}V^{(\ell+1)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}\, \\
=&\le(\frac{\CW{\ell+1}}{n_{\ell}}\ri)^2  \sum_{j,k=1}^{n_{\ell}}  \le\{ \E{ \s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}\s{k}{\alpha_3}{\ell}\s{k}{\alpha_4}{\ell} }  -\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}\E{\s{k}{\alpha_3}{\ell}\s{k}{\alpha_4}{\ell}} \ri\} \, .\nonumber%
\end{align}
Here, there are two types of the contributions: from coincident neurons and from separate pairs of neurons. Again, with foresight, we have already evaluated both types of four-point activation correlators in the last section. When all four are coincident $j=k$, substituting in~\eqref{eq:four-activations-same-deep} we find
\begin{align}\label{eq:activation-four-point-same-neurons}
 &\E{ \s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}\s{j}{\alpha_3}{\ell}\s{j}{\alpha_4}{\ell} }  -\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}\E{\s{j}{\alpha_3}{\ell}\s{j}{\alpha_4}{\ell}} \, \\
 =&\bra\sigma_{\alpha_1} \sigma_{\alpha_2} \sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(\ell)}}  - \bra \sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(\ell)}} \bra \sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(\ell)}} +\o{\frac{1}{n}}\, , \nonumber
\end{align}
where we have truncated to leading order in $1/n$ as a consequence of the inductive assumption at the $\ell$-th layer.\index{$1/n$ expansion} Meanwhile, when $j \neq k$ and the correlation is between two neurons, we substitute in our expression~\eqref{eq:four-activations-different-deep-connected}, finding
\begin{align}\label{eq:activation-four-point-different-neurons}
&\E{ \s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}\s{k}{\alpha_3}{\ell}\s{k}{\alpha_4}{\ell} }  -\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}\E{\s{k}{\alpha_3}{\ell}\s{k}{\alpha_4}{\ell}}\, \\
=&\frac{1}{4n_{\ell-1}}\sum_{\beta_1,\ldots,\beta_4\in\D}\TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1} z_{\beta_2}-g_{\beta_1\beta_2}\ri)\ket_{G^{(\ell)}}\bra \sigma_{\alpha_3}\sigma_{\alpha_4} \le(z_{\beta_3} z_{\beta_4}-g_{\beta_3\beta_4}\ri)\ket_{G^{(\ell)}}\, \nonumber\\
&+\o{\frac{1}{n^2}} \, ,\nonumber
\end{align}
where again we have truncated to leading order in the large-width expansion using the inductive assumption.\footnote{Again, the difference with the second-layer calculation is that in \S\ref{sec:second-layer-non-gaussian} these expectations are over the exactly Gaussian first-layer distribution. In that case, there was a contribution of the form \eqref{eq:activation-four-point-same-neurons} from the case with all neurons coincident, but \emph{not} of the form \eqref{eq:activation-four-point-different-neurons} from the two neurons -- cf.~\eqref{eq:second-layer-metric-fluctuation-two-point-function}.}
Inserting both of these expressions back into \eqref{eq:midpoint-vertex-recursion} and performing the sums, we get a recursion for the four-point vertex
\begin{align}\label{eq:V-recursion-tree}
&\frac{1}{n_{\ell}}V^{(\ell+1)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}\, \\
=&\frac{1}{n_{\ell}} \le(\CW{\ell+1}\ri)^2\le[\bra\sigma_{\alpha_1} \sigma_{\alpha_2} \sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(\ell)}}  - \bra \sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(\ell)}} \bra \sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(\ell)}} \ri]\, \nonumber\\
&+\frac{1}{n_{\ell-1}}\frac{\le(\CW{\ell+1}\ri)^2}{4}\sum_{\beta_1,\ldots,\beta_4\in\D}\TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\bra\sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1} z_{\beta_2}-g_{\beta_1\beta_2}\ri)\ket_{G^{(\ell)}}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \times\bra \sigma_{\alpha_3}\sigma_{\alpha_4} \le(z_{\beta_3} z_{\beta_4}-g_{\beta_3\beta_4}\ri)\ket_{G^{(\ell)}}+\o{\frac{1}{n^2}} \, . \nonumber
\end{align}
Importantly, we see that
\be\label{eq:V-recursion-tree-redux}
\frac{1}{n_{\ell}}V^{(\ell+1)}=\o{\frac{1}{n}}\, ,
\ee
and $V^{(\ell+1)}=\o{1}$, thus completing our inductive proof and concluding our derivations of the recursion relations~\eqref{eq:G-recursion-tree} and~\eqref{eq:V-recursion-tree} for the two-point and four-point correlators.
As was the case for the quadratic coupling,  if we neglect the subleading $1/n^2$ correction and replace $G$ by $g$ and $V$ by $v$, the recursion~\eqref{eq:V-recursion-tree} for the connected four-point correlator can also be thought of as the recursion for the quartic coupling. \index{$1/n$ expansion}



\index{$1/n$ expansion}
Note that in the strict $n\to \infty$ limit, the quartic coupling vanishes, and the marginal distribution of preactivations $p\!\le(z^{(\ell)}\Big\vert\D\ri)$ is Gaussian for all layers $\ell$. The first nontrivial correction to this \terminate{infinite-width limit} is captured by studying the quartic action with couplings $v_{(\ell)}$. 
In what follows, we will mostly focus on the \terminate{effective theory} with this quartic action, as we expect significant qualitative differences in the behavior of networks described by the quadratic action vs.~the quartic action. The additional finite-width corrections given by the higher-order terms in the action can change quantitative results but should not really exhibit qualitative differences.






\section{Marginalization Rules} 
\label{sec:sum-rule}
In the past sections, at each step in the recursions we marginalized over all the preactivations in a given layer.
This section collects two remarks on other sorts of \emph{partial} marginalizations we can perform, rather than integrating out an \emph{entire} layer. In particular, we'll discuss marginalization over a subset of the $\ND$ samples in the dataset\index{input data} $\D$ and marginalization over a subset of neurons in a layer. 

Loosely speaking, these marginalizations let us focus on specific input data and neurons of interest. Tightly speaking, let's consider evaluating the expectation of a function $F\!\le(z_{I;\A}\ri)=F\!\le(\le\{z_{i;\alpha}\ri\}_{i\in I; \alpha\in\A}\ri)$ that depends on a subsample $\A\subset\D$ and a subset of neurons $I\subset\le\{1,\ldots,n_\ell\ri\}\equiv \mathcal{N}$ in a layer $\ell$, where with a slight abuse of notation we put the set dependences into the subscripts. We then have
\begin{align}\label{eq:marginalization-rule}
&\E{F\!\le(z_{I;\A}\ri)}\, \\
=&\int \le[\prod_{i\in \mathcal{N}}\prod_{\alpha\in\D} dz_{i;\alpha}\ri]F\!\le(z_{I;\A}\ri)\, p\!\le(z_{\mathcal{N};\D}\Big\vert\D\ri) \, \nonumber\\
=&\int \le[\prod_{i\in I}\prod_{\alpha\in\A} dz_{i;\alpha}\ri] F\!\le(z_{I;\A}\ri)\le\{\int \le[ \prod_{(j;\beta)\in\le[ \mathcal{N}\times\D-I\times\A\ri]}dz_{j;\beta}\ri] p\!\le(z_{\mathcal{N};\D}\Big\vert\D\ri)\ri\}\, \nonumber\\
=&\int \le[\prod_{i\in I}\prod_{\alpha\in\A} dz_{i;\alpha}\ri] F\!\le(z_{I;\A}\ri)\, p\!\le(z_{I;\A}\Big\vert\A\ri) \notag
\end{align}
where the last equality is just the marginalization over the spectator variables that do not enter into the observable of interest and, in a sense, defines the subsampled and subneuroned distribution as
\be\label{eq:sum-rule-mlp}
p\!\le(z_{I;\A}\Big\vert\A\ri) \equiv \int \le[ \prod_{(j;\beta)\in\le[ \mathcal{N}\times\D-I\times\A\ri]}dz_{j;\beta}\ri]\ p\!\le(z_{\mathcal{N};\D}\Big\vert\D\ri) \, .
\ee
In words, in evaluating the expectation of the function $F\!\le(z_{I;\A}\ri)$, the full distribution $p\!\le(z_{\mathcal{N};\D}\Big\vert\D\ri)$ can simply be restricted to that of the subsample $\A$ and subneurons $I$, i.e., $p\!\le(z_{I;\A}\Big\vert\A\ri)$.
We call this property a \textbf{marginalization rule}\index{marginalization rule|textbf}.
Yes, this is somewhat trivial -- we're just restating the consistency of probability distributions with respect to marginalization -- but it has two rather useful consequences for us.
 




\subsubsection{Marginalization over samples}
The first corollary of the marginalization rule is that we can use it to reduce a gigantic integral over all the samples in the dataset to a compact integral over only a handful of samples. For example,
in recursively obtaining the two-point correlator through
\be
\E{\z{i_1}{\alpha_1}{\ell+1}\z{i_2}{\alpha_2}{\ell+1}}
=\delta_{i_1i_2} \le[\Cb{\ell+1}+\CW{\ell+1}\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(\ell)}}+\o{\frac{1}{n}}\, \ri]\, ,
\ee
we can reduce the $\ND$-dimensional Gaussian integrals
$\bra \sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(\ell)}}$ with the $\ND$-by-$\ND$ variance matrix $G^{(\ell)}$
to a manageable two-dimensional integral with a two-by-two submatrix spanned by $\alpha_1$ and $\alpha_2$ (or a one-dimensional integral if $\alpha_1=\alpha_2$). Similarly, a Gaussian integral for four activations $\bra\sigma_{\alpha_1} \sigma_{\alpha_2} \sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{G^{(\ell)}}$ that appears in the recursion for four-point vertex involves integrals over four variables \emph{at most}.
Generally, in using the action~\eqref{eq:general-ell-action} to evaluate a specific expectation, the summation over the whole dataset\index{input data} $\D$ in the action can be restricted to the subset of input data that actually appears in the expectation.
By the same token, in recursively evaluating the four-point vertex $V^{(\ell+1)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}$ via the recursion \eqref{eq:V-recursion-tree}, the summation on the right-hand side over the dataset $\D$  can be restricted to the set of samples being correlated, $\{\alpha_1,\alpha_2,\alpha_3,\alpha_4\}$.
However, please keep in mind that the inverse metrics used to construct $\TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}$ in \eqref{eq:vertex-UUUU-dddd} must then be taken to be the inverse of the metric submatrix on this restricted subspace.\footnote{A similar restriction of the summation can be applied to any of our other recursions and will prove especially useful when you try to evaluate them numerically or analytically.}



\subsubsection{Marginalization over neurons}
\index{marginalization rule}
The second corollary involves integrating out a subset of neurons in a layer.
Prudent readers might have worried that the quartic term in the $\ell$-th-layer action,
\be\label{eq:marginalization-rules-quartic}
-\frac{1}{8}\sum_{i_1,i_2=1}^{n_{\ell}}\sum_{\alpha_1,\ldots,\alpha_4\in\D}v^{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}_{(\ell)} \z{i_1}{\alpha_1}{\ell}\z{i_1}{\alpha_2}{\ell}\, \z{i_2}{\alpha_3}{\ell}\z{i_2}{\alpha_4}{\ell} \, ,
\ee
seems to naively scale like $\sim n_{\ell}^2 / n_{\ell-1} = \o{n}$, since there are two sums over $n_{\ell}$, and we know from \eqref{eq:four-point-match-general}  that the coupling $v_{(\ell)}$ scales like $\sim 1/n_{\ell-1}$. Similarly, the quadratic term,
\be\label{eq:marginalization-rules-quadratic}
\frac{1}{2}\sum_{i=1}^{n_{\ell}}\sum_{\alpha_1,\alpha_2\in\D} g^{\alpha_1\alpha_2}_{(\ell)} \z{i}{\alpha_1}{\ell}\z{i}{\alpha_2}{\ell} \, ,
\ee
has a single sum over $n_{\ell}$ and so seems naively $\o{n}$ as well. This would imply that the quartic term isn't perturbatively suppressed in comparison to the quadratic term, naively calling our perturbative approach into question.

We first observe that this problem never arises for the final layer $\ell=L$, since the output dimension $n_{L}$ is never parametrically large: the quadratic term scales as $\sim n_{L}=\o{1}$ while the quartic term scales as $\sim n_{L}^2/n_{L-1}=\o{1/n}$, which is perturbatively suppressed.

\index{marginalization rule}
This observation, combined with the marginalization rule, points at a resolution to the naive scale-counting problem above for the hidden layers.
Indeed, all the expectation we evaluated so far
-- both preactivation and activation correlators -- each individually involves only a few neurons $m_{\ell}$ in any given layer $\ell$, with $m_{\ell} \ll n_{\ell}$. This will always be true; we can't actually correlate an infinite number of neurons at once!
 Thus, when using the action representation~\eqref{eq:general-ell-action} of the probability distribution to compute these correlators at the $\ell$-th layer, we can first use the marginalization rule~\eqref{eq:marginalization-rule}\index{marginalization rule} to integrate out the $(n_{\ell} - m_{\ell})$ spectator neurons that do not participate in the computation, letting us focus on those $m_{\ell}$ relevant neurons that actually appear in the expectation. This in turn lets us replace the summations over $n_{\ell}$ neurons by ones over the $m_{\ell}$ neurons.\footnote{In evaluating generic expectation value such as~\eqref{eq:arbitrary-function-quartic-expectation}, one can always check that the contributions from the $(n_{\ell}-m_{\ell})$ spectator neurons consistently cancel out at each order in $1/n_{\ell-1}$ expansion. If you go back to your personal note that fills in the small gaps between lines in our computations, you will surely notice this cancellation due to Gaussian factorization.}

 All the while, the numbers of neurons in the previous layers $n_{1},\ldots,n_{\ell-1}$, having been integrated out to get the action representation at the $\ell$-th layer, \emph{are} parametrically large.
 This means that the quadratic term in the $\ell$-th-layer action, reduced to the $m_{\ell}$ relevant neurons, scales as $\sim m_{\ell}=\o{1}$, while the quartic term scales as $\sim m_{\ell}^2/n_{\ell-1}=\o{1/n}$. Thus, this ensures a perturbative treatment of the non-Gaussianity.





\subsubsection{Running couplings with partial marginalizations}\index{running coupling}\index{coupling!running|see{running coupling}}
In focusing our attention on only a subset of samples or neurons, the data-dependent couplings\index{data-dependent coupling} of the action need to be adjusted. Since this running of the couplings is instructive and will be necessary for later computations, let us illustrate here how the quadratic coupling\index{running coupling!quadratic} $g_{(\ell),m_{\ell}}^{\alpha_1\alpha_2}$ depends on the number of neurons $m_{\ell}$ in the action.

For simplicity in our illustration, let us specialize to a single input $x$ and drop all the \terminate{sample indices}. Then, denote the distribution over $m_{\ell}$ neurons as 
\begin{align}\label{eq:m-neuron-action}
p\!\le(z_1^{(\ell)},\ldots, z_{m_{\ell}}^{(\ell)}\ri)&\propto e^{-\ac\big(z_1^{(\ell)}\!,\,\ldots\,, \, z_{m_{\ell}}^{(\ell)}\big)}\, \\
&=\exp\!\le[-\frac{g_{(\ell),m_{\ell}}}{2}\sum_{j=1}^{m_{\ell}}z_{j}^{(\ell)}z_{j}^{(\ell)}+\frac{v_{(\ell)}}{8}\sum_{j_1,j_2=1}^{m_{\ell}}z_{j_1}^{(\ell)}z_{j_1}^{(\ell)}z_{j_2}^{(\ell)}z_{j_2}^{(\ell)}\ri]\, ,\notag
\end{align}
which is expressed by the same action we've already been using~\eqref{eq:general-ell-action}, though now the dependence of the quadratic coupling\index{coupling!quadratic} on $m_{\ell}$ is made explicit.\footnote{Note that in principle the quartic coupling\index{coupling!quartic} should also depend on $m_{\ell}$: $v_{(\ell)} \to v_{(\ell),m_{\ell}}$. However, since such a dependence only shows up at higher order in $v$, we will suppress it.}
We'll now see in two ways how the quadratic coupling\index{coupling!quadratic} $g_{(\ell),m_{\ell}}$ \emph{runs} with $m_{\ell}$.\index{running coupling}\index{running coupling}

\index{integrating out}
The first way is to begin with the action for $n_{\ell}$ neurons and formally integrate out $(n_{\ell}-m_{\ell})$ neurons. Without loss of generality, let's integrate out the \emph{last} $(n_{\ell}-m_{\ell})$ neurons, leaving the \emph{first} $m_\ell$ neurons labeled as $1, \ldots, m_\ell$. 
Using the marginalization rule\index{marginalization rule}~\eqref{eq:sum-rule-mlp}, we see that
\begin{align}
e^{-\ac\big(z_1^{(\ell)}\!,\,\ldots\,, \, z_{m_{\ell}}^{(\ell)}\big)}\propto&\ p\!\le(z_1^{(\ell)},\ldots, z_{m_{\ell}}^{(\ell)}\ri)=\int d{ z_{m_{\ell}+1}^{(\ell)}}\cdots d{ z_{n_{\ell}}^{(\ell)}}\ p\!\le(z_1^{(\ell)},\ldots, z_{n_{\ell}}^{(\ell)}\ri)\\
\propto&\int d{ z_{m_{\ell}+1}^{(\ell)}}\cdots d{ z_{n_{\ell}}^{(\ell)}}\exp\!\!\le[-\frac{g_{(\ell),n_{\ell}}}{2}\sum_{i=1}^{n_{\ell}}z_{i}^{(\ell)}z_{i}^{(\ell)}+\frac{v_{(\ell)}}{8}\!\!\sum_{i_1,i_2=1}^{n_{\ell}}z_{i_1}^{(\ell)}z_{i_1}^{(\ell)}z_{i_2}^{(\ell)}z_{i_2}^{(\ell)}\ri]\, ,\notag
\end{align}
throughout which we neglected \terminate{normalization factor}s that are irrelevant if we're just interested in the running of the coupling.
Next, we can separate out the dependence on the $m_{\ell}$ neurons, perturbatively expand the integrand in quartic coupling\index{coupling!quartic}, and finally integrate out the last $(n_{\ell}-m_{\ell})$ neurons by computing a few simple Gaussian integrals:
\begin{align}
&p\!\le(z_1^{(\ell)},\ldots, z_{m_{\ell}}^{(\ell)}\ri)\, \\
\propto&\exp\!\!\le[-\frac{g_{(\ell),n_{\ell}}}{2}\sum_{j=1}^{m_{\ell}}z_{j}^{(\ell)}z_{j}^{(\ell)}+\frac{v_{(\ell)}}{8}\!\!\sum_{j_1,j_2=1}^{m_{\ell}}z_{j_1}^{(\ell)}z_{j_1}^{(\ell)}z_{j_2}^{(\ell)}z_{j_2}^{(\ell)}\ri]\, \notag\\
&\times\int d{ z_{m_{\ell}+1}^{(\ell)}}\cdots d{z_{n_{\ell}}^{(\ell)}}\exp\!\!\le[-\frac{g_{(\ell),n_{\ell}}}{2}\sum_{k=m_{\ell}+1}^{n_{\ell}}z_{k}^{(\ell)}z_{k}^{(\ell)}\ri]\, \notag\\
&\quad \quad \quad \times\le[1+\frac{2v_{(\ell)}}{8}\sum_{j=1}^{m_{\ell}}\sum_{k=m_{\ell}+1}^{n_{\ell}}z_{j}^{(\ell)}z_{j}^{(\ell)}z_{k}^{(\ell)}z_{k}^{(\ell)}+\frac{v_{(\ell)}}{8}\!\!\sum_{k_1,k_2=m_{\ell}+1}^{n_{\ell}}z_{k_1}^{(\ell)}z_{k_1}^{(\ell)}z_{k_2}^{(\ell)}z_{k_2}^{(\ell)}+\o{v^2}\ri]\, \notag\\
=&\exp\!\!\le[-\frac{g_{(\ell),n_{\ell}}}{2}\sum_{j=1}^{m_{\ell}}z_{j}^{(\ell)}z_{j}^{(\ell)}+\frac{v_{(\ell)}}{8}\!\!\sum_{j_1,j_2=1}^{m_{\ell}}z_{j_1}^{(\ell)}z_{j_1}^{(\ell)}z_{j_2}^{(\ell)}z_{j_2}^{(\ell)}\ri]\, \notag\\
&\times\le\{1+\frac{(n_{\ell}-m_{\ell})}{4} \frac{v_{(\ell)}}{g_{(\ell),n_{\ell}}}\!\le(\sum_{i=1}^{m_{\ell}}z_{i}^{(\ell)}z_{i}^{(\ell)}\ri)+\frac{v_{(\ell)}}{8g_{(\ell),n_{\ell}}^2}\!\le[(n_{\ell}-m_{\ell})^2+2(n_{\ell}-m_{\ell})\ri]\!+\!\o{v^2}\!\ri\}\, .\notag%
\end{align}
Finally,
resumming the correction arising from the quartic coupling proportional to $\sum_{i=1}^{m_{\ell}}z_{i}^{(\ell)}z_{i}^{(\ell)}$ back into the exponential, ignoring the proportionality factor, and comparing with the action for $m_{\ell}$ neurons~\eqref{eq:m-neuron-action}, we find
\be\label{eq:quadratic-partial-running}
g_{(\ell),m_{\ell}}=g_{(\ell),n_{\ell}}-\frac{(n_{\ell}-m_{\ell})}{2} \frac{v_{(\ell)}}{g_{(\ell),n_{\ell}}}+\o{v^2}\, 
\ee
as the running equation for the quadratic coupling.

\index{running coupling}
The second way to see the coupling run -- and find a solution to the running equation~\eqref{eq:quadratic-partial-running} -- is to compute the single-input metric $\Ti{G}{}{\ell}\equiv\E{z_{i}^{(\ell)}z_{i}^{(\ell)}}$ and compute it directly using the $m_{\ell}$-neuron action~\eqref{eq:m-neuron-action}.
We've already computed this in~\eqref{eq:second-moment-from-action} using the quartic action for multiple inputs. Specializing to a single input, considering an action of $m_\ell$ neurons, and being explicit about the dependence of the quadratic coupling\index{running coupling!quadratic} on the number of neurons, we get
\be\label{eq:second-moment-from-partial-action}
\Ti{G}{}{\ell}=\le[\frac{1}{g_{(\ell),m_{\ell}}}+\frac{(m_{\ell}+2)}{2}\frac{v^{(\ell)}}{g_{(\ell),m_{\ell}}^3}\ri]+\o{v^2}\, .
\ee
Solving this equation for $g_{(\ell),m_{\ell}}$ by perturbatively expanding in $v^{(\ell)}$, we find 
\be\label{eq:quadratic-reprint-m-emphasis}
\frac{1}{g_{(\ell),m_{\ell}}}=G^{(\ell)}-\frac{(m_{\ell}+2)}{2}\frac{ V^{(\ell)}}{n_{\ell-1} G^{(\ell)}}+\o{\frac{1}{n^2}}\, ,
\ee
where we have also plugged in
\be\label{eq:quartic-single-input-coupling-for-vertex}
v_{(\ell)}= \frac{V^{(\ell)}}{n_{\ell-1} \le(G^{(\ell)}\ri)^4 }+\o{\frac{1}{n^2}} \, ,
\ee
using \eqref{eq:four-point-match-general} and \eqref{eq:vertex-UUUU-dddd} to relate the quartic coupling to the \terminate{four-point vertex} and again specializing to a single input.
Now, it's easy to check that this expression~\eqref{eq:quadratic-reprint-m-emphasis} solves the running equation~\eqref{eq:quadratic-partial-running}.\footnote{Note that the coupling $g_{(\ell),m_{\ell}}$ depends on $m_{\ell}$ -- and also on the other hidden-layer widths $n_{1},n_{2},\ldots,n_{\ell-1}$ -- but does \emph{not} depend on the overall width of the current layer $n_{\ell}$. This implies that the quadratic coupling\index{coupling!quadratic} $g_{(\ell),m_{\ell}}$ is the same coupling we would have used if instead there were actually only $m_{\ell}$ neurons in the $\ell$-th layer. 
} 

The key step in this alternative derivation is realizing that observables without any \terminate{neural indices} such as $\Ti{G}{}{\ell}$ should \emph{not} depend on which version of the $m_{\ell}$ action we use in computing them.  Interpreted another way, what this running of the coupling\index{running coupling} means is that for different numbers of neurons in a layer $\ell$ -- e.g.~$m_\ell$ and $n_\ell$ -- we need different quadratic couplings -- in this case $g_{(\ell),m_{\ell}}$ and $g_{(\ell),n_{\ell}}$ -- in order to give the correct value for an $\ell$-th-layer observable such as $\Ti{G}{}{\ell}$.
If you're ever in doubt, it's always safest to express an observable of interest in terms of the metric $\Ti{G}{}{\ell}$ and the \terminate{four-point vertex} $\Ti{V}{}{\ell}$ rather than the couplings. %
\index{marginalizing over}




\section{Subleading Corrections}
\label{sec:loop-correction}
\index{$1/n$ expansion}\index{subleading corrections|textbf}\index{subleading corrections|seealso{$1/n$ expansion}}
At finite width, all of the correlators receive an infinite series of subleading corrections. Concretely, the metric governing two-point correlator and the four-point vertex governing the connected four-point correlator have $1/n$ series expansions of the form
\begin{align}\label{eq:self-energy-decomposition}
\Ti{G}{\alpha_1\alpha_2}{\ell}=&G_{\alpha_1\alpha_2}^{\le\{0\ri\}(\ell)}+\frac{1}{n_{\ell-1}}\se{\alpha_1\alpha_2}{\ell}+\frac{1}{n_{\ell-1}^2}G_{\alpha_1\alpha_2}^{\le\{2\ri\}(\ell)}+\o{\frac{1}{n^3}}\, ,\\
\label{eq:vertex-decomposition}
\Ti{V}{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}{\ell}=&V_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}^{\le\{0\ri\}(\ell)}+\frac{1}{n_{\ell-1}}V_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}^{\le\{1\ri\}(\ell)}+\o{\frac{1}{n^2}}\, .
\end{align}
While so far we have focused on the leading contributions $G_{\alpha_1\alpha_2}^{\le\{0\ri\}(\ell)}$ and $V_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}^{\le\{0\ri\}(\ell)}$, the subleading corrections can be systematically calculated as well. Let us illustrate the procedure by deriving the recursion for the next-to-leading-order (NLO) correction to the metric, $\se{\alpha_1\alpha_2}{\ell}$.

Before proceeding, let us remark that the leading contribution of the mean metric fully describes the infinite-width limit of the preactivation distributions and so is given a symbol
\be\label{eq:definition-of-kernel-first}
 \Ti{\ker}{\alpha_1\alpha_2}{\ell}\equiv G_{\alpha_1\alpha_2}^{\le\{0\ri\}(\ell)}\, ,
\ee 
and name, the \textbf{kernel}\index{kernel|textbf}\index{kernel|seealso{metric}}\index{kernel!infinite-width limit of the metric}\index{metric!infinite-width limit}. Since the kernel captures the leading-order correlation between any pair of samples, it will be a central object of study for us in the following chapters.
In a similar vein, we will call $\se{\alpha_1\alpha_2}{\ell}$ the \textbf{NLO metric}\index{NLO metric|see{metric}}\index{metric!next-to-leading-order correction|textbf}.

\index{$1/n$ expansion}
Our first step will be to express the layer-$\ell$ quadratic coupling $\TI{g}{\beta_1\beta_2}{\ell}$ to order $1/n$ in terms of the $1/n$ correlator data in \eqref{eq:self-energy-decomposition} and \eqref{eq:vertex-decomposition}.
Let's begin by recalling the expression \eqref{eq:second-moment-from-action} for the two-point correlator that we derived from the quartic action, reprinted here for layer $\ell$
\begin{align}\label{eq:second-moment-from-action-reprinted}
&\E{\z{i_1}{\alpha_1}{\ell} \z{i_2}{\alpha_2}{\ell}} = \delta_{i_1 i_2}\Ti{G}{\alpha_1\alpha_2}{\ell} \\%=\frac{1}{Z}\int \le[\prod_{i,\alpha} d z_{i;\alpha}\ri] e^{-S\le(
=&\delta_{i_1 i_2}\le[\Ti{g}{\alpha_1\alpha_2}{\ell}+\frac{1}{2}\sum_{\beta_1,\ldots,\beta_4\in\D}\TI{v}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\le(n_\ell\, \Ti{g}{\alpha_1\beta_1}{\ell}\Ti{g}{\alpha_2\beta_2}{\ell}\Ti{g}{\beta_3\beta_4}{\ell}+2\Ti{g}{\alpha_1\beta_1}{\ell}\Ti{g}{\alpha_2\beta_3}{\ell}\Ti{g}{\beta_2\beta_4}{\ell}\ri)\ri]+\o{v^2}\, .\nonumber
\end{align}
As a reminder $\Ti{g}{\alpha_1\alpha_2}{\ell}$ is the matrix inverse of the quadratic coupling $\TI{g}{\alpha_1\alpha_2}{\ell}$ .
Substituting in the expansion \eqref{eq:self-energy-decomposition} into \eqref{eq:second-moment-from-action-reprinted}, substituting for the quartic coupling $\TI{v}{}{\ell} = \TI{V}{}{\ell}/n_{\ell-1}$~\eqref{eq:four-point-match-general}, and rearranging to solve for $\Ti{g}{\alpha_1\alpha_2}{\ell}$ to the subleading order, we get
\be\label{eq:matching-two-point-subleading}
\Ti{g}{\alpha_1\alpha_2}{\ell}=\Ti{\ker}{\alpha_1\alpha_2}{\ell}+\frac{1}{n_{\ell-1}}\le[\se{\alpha_1\alpha_2}{\ell}-\sum_{\beta_1,\beta_2\in\D}\TI{\ker}{\beta_1\beta_2}{\ell}\le(\frac{n_{\ell}}{2} \Ti{V}{(\alpha_1\alpha_2)(\beta_1\beta_2)}{\ell}+\Ti{V}{(\alpha_1\beta_1)(\alpha_2\beta_2)}{\ell}\ri)\ri]+\o{\frac{1}{n^2}}\, .
\ee
Note that in obtaining the above, we have self-consistently replaced $g^{(\ell)}$ by $K^{(\ell)}$ in the subleading term, which in turn let us lower the indices of the four-point vertices.
Inverting this expression~\eqref{eq:matching-two-point-subleading} yields the subleading correction to the quadratic coupling in terms of the correlators
\begin{align}\label{eq:inverse-difference}
&\TI{g}{\beta_1\beta_2}{\ell}-\TI{\ker}{\beta_1\beta_2}{\ell}\, \\
=&\frac{1}{n_{\ell-1}}\sum_{\beta_3,\beta_4\in\D}\le[-\TI{\ker}{\beta_1\beta_3}{\ell}\TI{\ker}{\beta_2\beta_4}{\ell}\se{\beta_3\beta_4}{\ell}+\Ti{\ker}{\beta_3\beta_4}{\ell}\le(\frac{n_{\ell}}{2} \TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}+\TI{V}{(\beta_1\beta_3)(\beta_2\beta_4)}{\ell}\ri)\ri]+\o{\frac{1}{n^2}}\, .\nonumber
\end{align}

\index{$1/n$ expansion}\index{subleading corrections}
Note that one term in this correction scales as $n_\ell / n_{\ell-1}$. As discussed in the previous section, the marginalization rule for the $\ell$-th-layer action guarantees that we can treat this quantity as small, $n_\ell / n_{\ell-1} \ll 1$, ensuring that $\Ti{g}{\alpha_1\alpha_2}{\ell}-\Ti{\ker}{\alpha_1\alpha_2}{\ell}$ is a subleading-in-$1/n$ correction to the quadratic coupling. In line with this statement, we'll soon see the cancellation for the factor of $n_{\ell}$ when computing the recursion for this subleading correction to the metric $\se{}{\ell}$.



\index{$1/n$ expansion}\index{subleading corrections}
Having finished working out the $1/n$-corrected $\ell$-th-layer action, we turn to computing the $(\ell+1)$-th-layer two-point correlator.\footnote{We already knew the $1/n$ contribution to the quartic coupling, namely the relation $\TI{v}{}{\ell} = \TI{V}{}{\ell}/n_{\ell-1}$.} This will let us express the $(\ell+1)$-th-layer two-point correlator in terms of the $\ell$-th-layer statistics, ultimately yielding a recursion for $\se{\alpha_1\alpha_2}{\ell}$.
Starting with the expansion \eqref{eq:self-energy-decomposition} in the $(\ell+1)$-th layer  and substituting in the expression~\eqref{eq:C2_MLP_exact} for the two-point correlator, we obtain
\be\label{eq:recursion-two-point-subleading}
\Ti{\ker}{\alpha_1\alpha_2}{\ell+1}+\frac{1}{n_{\ell}}\se{\alpha_1\alpha_2}{\ell+1}+\o{\frac{1}{n^2}}=\Ti{G}{\alpha_1\alpha_2}{\ell+1}=\Cb{\ell+1}+\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}\, .
\ee
Thus, we need the expectation of two activations in the $\ell$-th layer up to the order $\o{1/n}$, which we evaluated before in expression~\eqref{eq:two-activations-deep} in terms of the $\ell$-th-layer couplings.

\index{$1/n$ expansion}\index{subleading corrections}
Looking at \eqref{eq:two-activations-deep}, there are two types of contributions at the subleading order, one arising from the $1/n$ correction to the quadratic coupling $g_{(\ell)}$ in~\eqref{eq:matching-two-point-subleading} and the other from the near-Gaussianity of the distribution due to the quartic coupling $v_{(\ell)}$.
The latter contribution is easy to handle: since the quartic coupling is already suppressed by $1/n$, we can just make the replacement $g^{(\ell)} \to \ker^{(\ell)}$ in the second term in~\eqref{eq:two-activations-deep}, yielding
\begin{align}\label{eq:quartic-easy}
&\frac{1}{8n_{\ell-1}}\sum_{\beta_1,\ldots,\beta_4\in\D}\TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\, \\
&\times\Big[\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1} z_{\beta_2}-\Ti{\ker}{\beta_1\beta_2}{\ell}\ri)\le(z_{\beta_3} z_{\beta_4}-\Ti{\ker}{\beta_3\beta_4}{\ell}\ri)\ket_{\ker^{(\ell)}}\, \nonumber\\
&\quad +2n_{\ell} \bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1} z_{\beta_2}-\Ti{\ker}{\beta_1\beta_2}{\ell}\ri)\ket_{\ker^{(\ell)}} \Ti{\ker}{\beta_3\beta_4}{\ell}-2\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{\ker^{(\ell)}}\Ti{\ker}{\beta_1\beta_3}{\ell}\Ti{\ker}{\beta_2\beta_4}{\ell}\Big]+\o{\frac{1}{n^2}}\, .\nonumber
\end{align}
 However, for the former contribution, the Gaussian term $\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{g^{(\ell)}}$ needs be carefully separated into the leading and subleading pieces. To that end, we can trade the Gaussian expectation with $g^{(\ell)}$ for one in terms of the leading kernel $K^{(\ell)}$
\begin{align}\label{eq:subleading-sigma-sigma}
&\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{g^{(\ell)}}\, \\
=&\frac{\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \exp\!\le[-\frac{1}{2}\sum_{\beta_1,\beta_2}\le(\TI{g}{\beta_1\beta_2}{\ell}-\TI{\ker}{\beta_1\beta_2}{\ell}\ri)z_{\beta_1}z_{\beta_2} \ri]\ket_{\ker^{(\ell)}}}{\bra \exp\!\le[-\frac{1}{2}\sum_{\beta_1,\beta_2}\le(\TI{g}{\beta_1\beta_2}{\ell}-\TI{\ker}{\beta_1\beta_2}{\ell}\ri)z_{\beta_1}z_{\beta_2} \ri]\ket_{\ker^{(\ell)}}}\, \nonumber\\
=&\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{\ker^{(\ell)}}-\frac{1}{2}\sum_{\beta_1,\beta_2}\le(\TI{g}{\beta_1\beta_2}{\ell}-\TI{\ker}{\beta_1\beta_2}{\ell}\ri)\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1}z_{\beta_2}-\Ti{\ker}{\beta_1\beta_2}{\ell}\ri)\ket_{\ker^{(\ell)}}+\o{\frac{1}{n^2}}\, .\nonumber
\end{align}
Plugging~\eqref{eq:inverse-difference} into~\eqref{eq:subleading-sigma-sigma}, we obtain the subleading contribution due to the change in the quadratic coupling, giving
\begin{align}\label{eq:subleading-sigma-sigma-more-explicit}
&\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{g^{(\ell)}}\, \\
=&\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{\ker^{(\ell)}}+\frac{1}{2n_{\ell-1}}\TI{\ker}{\beta_1\beta_3}{\ell}\TI{\ker}{\beta_2\beta_4}{\ell}\se{\beta_3\beta_4}{\ell}\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1}z_{\beta_2}-\Ti{\ker}{\beta_1\beta_2}{\ell}\ri)\ket_{\ker^{(\ell)}}\, \nonumber\\
&-\frac{1}{n_{\ell-1}}\sum_{\beta_1,\ldots,\beta_4}\le(\frac{n_{\ell}}{4} \TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}+\frac{1}{2}\TI{V}{(\beta_1\beta_3)(\beta_2\beta_4)}{\ell}\ri)\Ti{\ker}{\beta_3\beta_4}{\ell}\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1}z_{\beta_2}-\Ti{\ker}{\beta_1\beta_2}{\ell}\ri)\ket_{\ker^{(\ell)}}\, \nonumber\\
&+\o{\frac{1}{n^2}}\, .\nonumber
\end{align}\index{subleading corrections}

Now that we've computed everything, we can add the two contributions to $\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}$, \eqref{eq:quartic-easy} and~\eqref{eq:subleading-sigma-sigma-more-explicit}, and plug them into the expression for the preactivation correlator~\eqref{eq:recursion-two-point-subleading}. Collecting terms, we recover the leading contribution, the recursion for the kernel
\be\label{eq:K-recursion}
\Ti{\ker}{\alpha_1\alpha_2}{\ell+1}=\Cb{\ell+1}+\CW{\ell+1}\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{\ker^{(\ell)}}\, ,
\ee
and also find a recursion for the NLO metric\index{metric!next-to-leading-order correction} as promised\index{subleading corrections}\index{metric!next-to-leading-order correction}
\begin{align}\label{eq:NLO-metric-recursion}
&\frac{1}{n_{\ell}}\se{\alpha_1\alpha_2}{\ell+1}\, \\
=&\CW{\ell+1}\frac{1}{n_{\ell-1}}\sum_{\beta_1,\ldots,\beta_4\in\D}\Bigg[\frac{1}{2}\TI{\ker}{\beta_1\beta_3}{\ell}\TI{\ker}{\beta_2\beta_4}{\ell}\se{\beta_3\beta_4}{\ell}\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1}z_{\beta_2}-\Ti{\ker}{\beta_1\beta_2}{\ell}\ri)\ket_{\ker^{(\ell)}}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \quad +\frac{1}{8} \TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1}z_{\beta_2}-\Ti{\ker}{\beta_1\beta_2}{\ell}\ri) \le(z_{\beta_3}z_{\beta_4}-\Ti{\ker}{\beta_3\beta_4}{\ell}\ri)\ket_{\ker^{(\ell)}}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \quad +\frac{1}{4}\TI{V}{(\beta_1\beta_3)(\beta_2\beta_4)}{\ell}\Ti{\ker}{\beta_3\beta_4}{\ell}\bra \sigma_{\alpha_1}\sigma_{\alpha_2} \le(-2z_{\beta_1}z_{\beta_2}+\Ti{\ker}{\beta_1\beta_2}{\ell}\ri) \ket_{\ker^{(\ell)}}\Bigg]\, . \nonumber
\end{align}
In going through this calculation in your personal notes or on the margins of this book, you can explicitly see the cancellation of contributions from $n_{\ell}-1$ spectator neurons that does not participate in the expectation $\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}$ 
as required by the marginalization rule for the $\ell$-th-layer action. Indeed, every term in the square bracket on the right-hand side of the equation~\eqref{eq:NLO-metric-recursion} is manifestly of order one.


\index{$1/n$ expansion}\index{subleading corrections}
This process can be systematically pushed to higher orders. Just as the computation of the NLO metric\index{metric!next-to-leading-order correction} $G_{\alpha_1\alpha_2}^{\le\{1\ri\}(\ell)}$ involved the leading quartic coupling, the computation of the subleading correction to the four-point vertex, $V_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}^{\le\{1\ri\}(\ell)}$, and the computation of the order $1/n^2$ correction to the two-point correlator, $G_{\alpha_1\alpha_2}^{\le\{2\ri\}(\ell)}$, would involve the leading sextic coupling. Such a sextic coupling appears at order $1/n^2$ in the action and contributes to the connected six-point function, which also vanishes as
$\o{1/n^2}$.\footnote{
For those familiar with field theory, the leading part of the couplings in the action are \emph{tree-level} contributions to correlators.
They are to be contrasted with \terminate{subleading corrections} to the two-point correlator discussed in this section, which included both \emph{loop-level} contributions from quartic interaction and tree-level contributions from the NLO correction to the bare quadratic coupling.\index{coupling!quadratic}\index{coupling!quartic}\index{interactions}}


\section{RG Flow and RG Flow}\index{RG flow|see{representation group flow}}\index{RG flow|see{renormalization group flow}}\index{RG flow and RG flow|textbf}
\label{sec:marginalization-group-flow}
Since the past five sections have been a whirlwind of equations, algebra, and integration, let's take a moment to recap and assemble the main results.

The goal of this chapter was to find the marginal distribution of preactivations $p\!\le(z^{(\ell)}\Big\vert\D\ri)$ in a given layer $\ell$  in terms of an \term{effective action}\index{action!effective|see{effective action}} with data-dependent couplings\index{data-dependent coupling}. These couplings change -- or \textbf{run}\index{running coupling|textbf} -- from layer to layer, and the running is determined via recursions, which in turn determine how the distribution of preactivations changes with depth.
Equivalently, these recursions tell us how correlators of preactivations evolve with layer.
In this language, starting with independent neurons in the first layer (\S\ref{sec:first-layer-gaussian}),
we saw how interactions among neurons are induced in the second layer (\S\ref{sec:second-layer-non-gaussian})
and then amplified in deeper layers  (\S\ref{sec:deeper-layer-accumulation}).

Concretely, let's summarize the behavior of finite-width networks to leading order in the wide-network expansion. Expressing the two-point correlator of preactivations in terms of the \textbf{kernel}\index{kernel} $\Ti{\ker}{\alpha_1\alpha_2}{\ell}$ as
\be\label{eq:leading-kernel-reminder}
\E{\z{i_1}{\alpha_1}{\ell}\z{i_2}{\alpha_2}{\ell}}=\delta_{i_1i_2}\Ti{G}{\alpha_1\alpha_2}{\ell}=\delta_{i_1i_2}\le[\Ti{\ker}{\alpha_1\alpha_2}{\ell}+\o{\frac{1}{n}}\ri] \, ,
\ee
and expressing the four-point connected correlator in terms of the \term{four-point vertex} $\Ti{V}{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}{\ell}$  as
\begin{align}\label{eq:C4_MLPH_reprint}
&\E{\z{i_1}{\alpha_1}{\ell}\z{i_2}{\alpha_2}{\ell}\z{i_3}{\alpha_3}{\ell}\z{i_4}{\alpha_4}{\ell}}\Big\vert_{\text{connected}}\, \\
=&\frac{1}{n_{\ell-1}}\le[\delta_{i_1i_2}\delta_{i_3 i_4}\Ti{V}{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}{\ell}+\delta_{i_1i_3}\delta_{i_2 i_4}\Ti{V}{(\alpha_1\alpha_3)(\alpha_2\alpha_4)}{\ell}+\delta_{i_1i_4}\delta_{i_2 i_3}\Ti{V}{(\alpha_1\alpha_4)(\alpha_2\alpha_3)}{\ell} \ri]\, , \nonumber
\end{align}
the running of these correlators is given by the recursions
\begin{align}
\label{eq:K-recursion-reprint}
\Ti{\ker}{\alpha_1\alpha_2}{\ell+1}=&\Cb{\ell+1}+\CW{\ell+1}\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{\ker^{(\ell)}}\, ,\\
\label{eq:V-recursion-reprint}
V^{(\ell+1)}_{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}=&\le(\CW{\ell+1}\ri)^2\Big[\bra\sigma_{\alpha_1} \sigma_{\alpha_2} \sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{\ker^{(\ell)}}  - \bra \sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{\ker^{(\ell)}} \bra \sigma_{\alpha_3} \sigma_{\alpha_4}\ket_{\ker^{(\ell)}} \Big]\, \\
&+\frac{1}{4}\le(\CW{\ell+1}\ri)^2\frac{n_{\ell}}{n_{\ell-1}}\sum_{\beta_1,\ldots,\beta_4\in\D}\TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\bra\sigma_{\alpha_1}\sigma_{\alpha_2} \le(z_{\beta_1} z_{\beta_2}-\Ti{\ker}{\beta_1\beta_2}{\ell}\ri)\ket_{\ker^{(\ell)}}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \times\bra \sigma_{\alpha_3}\sigma_{\alpha_4} \le(z_{\beta_3} z_{\beta_4}-\Ti{\ker}{\beta_3\beta_4}{\ell}\ri)\ket_{\ker^{(\ell)}}+\o{\frac{1}{n}} \, ,\nonumber
\end{align}
where the indices on the four-point vertex are raised by the inverse metric $G_{(\ell)}$
\begin{align}
\TI{V}{(\alpha_1\alpha_2)(\alpha_3\alpha_4)}{\ell}\equiv& \sum_{\beta_1,\ldots,\beta_4\in\D}\TI{G}{\alpha_1\beta_1}{\ell}\TI{G}{\alpha_2\beta_2}{\ell}\TI{G}{\alpha_3\beta_3}{\ell}\TI{G}{\alpha_4\beta_4}{\ell}\Ti{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\, \\
=&\sum_{\beta_1,\ldots,\beta_4\in\D}\TI{\ker}{\alpha_1\beta_1}{\ell}\TI{\ker}{\alpha_2\beta_2}{\ell}\TI{\ker}{\alpha_3\beta_3}{\ell}\TI{\ker}{\alpha_4\beta_4}{\ell}\Ti{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}+\o{\frac{1}{n}}\, .\nonumber
\end{align}
These recursions dictate how the statistics of preactivations  flow with depth.

This flow is very reminiscent of the following heuristic picture, which is offered as an explanation for how neural networks are supposed to work: given an input, such as the image of a cat,
the first few layers identify low-level features from the pixels -- such as the \texttt{edges} between areas of low and high intensity -- and then the middle layers assemble these low-level features into
mid-level features -- such as the texture and pattern of \texttt{fur} -- which are further aggregated in deeper layers into higher-level representations
-- such as \texttt{tails} and \texttt{ears} -- which the last layer combines into an estimate of the probability that original pixels represents a \texttt{cat}.
Indeed, some studies support this hierarchically-ordered arrangement of feature representation
in trained networks~\cite{Rob}.\footnote{It has been suggested that even untrained networks have features that can act as types of filters, effectively allowing for primitive edge detecting in untrained networks. For a related set of ideas, see~\cite{NIPS2007_3182}.} The desirability of such an arrangement emphasizes both the role and importance of depth in deep learning.

Some of the terms we used in discussing this heuristic picture can actually be given more precise definitions. For instance, each neuron in the network -- including not only those in the output layer but also those in the hidden layers -- is a scalar function of the input and called a \term{feature}. The neurons of a given layer can be organized into a vector-valued function of the input, which we'll refer to as a \term{representation}.\footnote{While the main focus of our study of supervised learning (\S\ref{ch:training}) will be understanding how the representation $z^{(L)}$ in the output layer is learned via gradient-based training,
it is also important to understand how representations 
are learned in hidden layers (\S\ref{ch:features}). In addition to being necessary components of determining the coarse-grained representation at the output,
in some applications of deep learning learned representations in the hidden layers can be used as inputs themselves for other learning tasks. This occurs quite often in \terminate{unsupervised learning}, for example with the word embeddings of natural language processing tasks. In these scenarios, the embeddings -- representations for an input word in the larger context of a full sentence -- typically are taken not just from the final layer, but from the concatenation of the final few layers. See e.g.~\cite{BERT2018}.}
In terms of these concepts, our formalism tracks the transformation of representations from one layer to the next. It is this flow of representations that we term \term{representation group flow} or \textbf{RG flow} for short.\footnote{
Two apologies are in order for the name \emph{representation group flow}\index{representation group flow!name}: \emph{(i)} it is confusingly close to the notion of \neo{group representation theory} in mathematics; and \emph{(ii)} the flow is technically a \neo{semigroup}\index{semigroup|seealso{RG flow}}, rather than a group. (Both group theory and semigroup theory are the studies of transformations but a group requires inverses while a semigroup does not; and the flow has no inverse.) 
This is just to repeat a historic mistake in physics as we'll explain further in a footnote below.} 
RG flow is induced via the repeated marginalization of fine-grained features in the shallow layers to give a coarse-grained representation in the output layer. Our notion of RG flow makes the heuristic picture given above concrete. %




 




This pattern of coarse-graining has a parallel in theoretical physics\index{physics}, known as \term{renormalization group flow} or \textbf{RG flow} for short. In this case, the RG flow is generated by the repeated marginalization of the microscopic fine-grained degrees of freedom in the system in order to obtain an \neo{effective theory} of the system in terms of macroscopic coarse-grained variables. Analogously, the physical couplings controlling the interactions of these effective degrees of freedom \emph{run}\index{running coupling} with the length scale at which they are probed -- e.g.~the effective charge of the electron will change when interrogated at different scales. Similar to the recursion equations describing the running couplings of the network representations, one can derive differential equations -- historically called beta functions -- that govern the running of the physical couplings with scale.\footnote{\emph{A brief history of renormalization in physics.}

Renormalization was originally developed in the 1930s and 1940s to deal with divergences -- infinities -- that plagued the calculations of experimental observables in quantum field theory. At first, these infinities were simply subtracted off -- swept under the rug, if you will -- yielding answers that, despite these shenanigans, matched extremely well with experiments. This whole state of affairs was considered embarrassing, leading to near abandonment of the theory. 

These divergences arose essentially due to a failure to properly take into account 
that couplings can be scale-dependent. The idea of running couplings was first put forth by Gell-Mann and Low~\cite{gellmanlow} in 1954,
however a full conceptualization of renormalization
wasn't available until Wilson developed the modern notion of RG flow~\cite{PhysRevB.4.3174,PhysRevB.4.3184} in 1971, offering a theoretical explanation for critical phenomena in statistical physics as well as giving a
sound grounding for the understanding of
divergences in quantum field theory.

At this point, all historical accounts of RG are contractually obligated to mention the following: the renormalization group is not a group; it's a semigroup. (The mistake was made in an early paper by Stueckelberg and Petermann, referring to the flow as a ``group of normalization''~\cite{Petermann:1953wpa}.)  Mathematically, this is because there are no
inverse elements; the marginalization of variables out of a joint distribution deletes information and cannot be undone. In particular, two different joint distributions can sometimes flow to the same distribution after marginalization. Intuitively, this is because these flows go from fine-grained descriptions to coarse-grained descriptions. (Such convergent flows lead to the notion of \neo{universality}, which we will explain in \S\ref{ch:signalprop} in the context of neural networks with different activations that flow to the same marginal distributions under RG.)

Clearly, RG flow in physics is a very rich subject. If you're interested in learning more, we recommend both~\cite{goldenfeld2018lectures,cardy_1996}.%
}








To make the connection between this RG flow and that RG flow abundantly clear, let's peek into how it is implemented in field theory in physics. In this scenario, the \terminate{degrees of freedom} are represented by a field $\phi\!\le(x\ri)$ that may take different values as a function of spacetime coordinate $x$. First, one divides $\phi\!\le(x\ri)$ into fine-grained variables $\phi^+$ consisting of high-frequency modes and coarse-grained variables $ \phi^-$ consisting of low-frequency modes, such that the field decomposes as $\phi\!\le(x\ri)=\phi^{+}\!\le(x\ri) + \phi^{-}\!\le(x\ri)$. The full distribution is governed by the full action
\be
\ac_{\text{full}}(\phi) = \ac(\phi^+) + \ac(\phi^-) + \SI(\phi^+,\phi^-)\, ,
\ee
where in particular the last term describes the interactions between these two sets of modes.

\index{coarse-graining}\index{effective action!as an effective theory}
Now, if all we care about are observables that depend only on the coarse-grained modes $\phi^{-}$ at macroscopic scales -- and such long-range scales are usually the relevant ones for experiments -- then this full description is too cumbersome to usefully describe the outcome of such experiments. In order to obtain an effective description in terms of only these coarse-grained variable $\phi^{-}$, we can integrate out (i.e.~marginalizes over) the fine-grained variables $\phi^{+}$ as
\be\label{eq:field-theory-marginalization}
e^{-\ac_{\text{eff}}(\phi^-)}=\int d\phi^{+}\ e^{-\ac_{\text{full}}(\phi)}\, ,
\ee
and obtain an \textbf{effective action}\index{effective action!in physics}\index{effective action!connection to RG flow} $\ac_{\text{eff}}(\phi^-)$, providing an \terminate{effective theory} for the observables of experimental interest. 
In practice, this marginalization is carried out scale by scale, dividing up the field as $\phi=\phi^{(1)}+\ldots+\phi^{(L)}$ from microscopic modes $\phi^{(1)}$ all the way to macroscopic modes $\phi^{(L)}=\phi^{-}$, and then integrating out the variables $\phi^{(1)}$, \ldots, $\phi^{(L-1)}$ in sequence. Tracking the flow of couplings in the effective action through this marginalization results in the aforementioned beta functions, and
in solving these differential equations up to the scale of interest, we get an effective description of observables at that scale.

This is precisely what we have been doing in this chapter for neural networks. The full field $\phi$ is analogous to a collection of all the preactivations $\le\{z^{(1)}, \ldots, z^{(L)}\ri\}$.
Their distribution is governed by the full joint distribution of preactivations
\be\label{eq:full-distribution-factorization}
p\!\le(z^{(1)}, \dots,  z^{(L)}\Big\vert \D\ri) = p\!\le(z^{(L)} \Big\vert z^{(L-1)} \ri) \cdots p\!\le(z^{(2)} \Big\vert z^{(1)} \ri) p\!\le(z^{(1)}\Big\vert \D \ri) \, ,
\ee
with the full action
\be\label{eq:full-action-decomposition}
 \ac_{\text{full}}\!\le(z^{(1)}, \dots,  z^{(L)}\ri)\equiv \sum_{\ell=1}^{L} \SGP\!\le(z^{(\ell)}\ri) + \sum_{\ell=1}^{L-1} \SI\!\le(z^{(\ell+1)}\Big\vert z^{(\ell)} \ri) \, .
\ee
Here, the full action is decomposed into the mean quadratic action for variables $z^{(\ell)}$
\be
\SGP\!\le(z^{(\ell)}\ri)=\frac{1}{2}\sum_{i=1}^{n_{\ell}}\sum_{\alpha_1,\alpha_2\in\D}\TI{G}{\alpha_1 \alpha_2}{\ell}\z{i}{\alpha_1}{\ell}\z{i}{\alpha_2}{\ell} \, 
\ee
in terms of the mean metric
$\Ti{G}{}{\ell}$, \eqref{eq:mean-metric-any-layer}, and the interaction between neighboring layers
\be
\SI\!\le(z^{(\ell+1)}\Big\vert z^{(\ell)} \ri)=\frac{1}{2}\sum_{i=1}^{n_{\ell+1}}\sum_{\alpha_1,\alpha_2\in\D}\le[\TI{\widehat{G}}{\alpha_1\alpha_2}{\ell+1}\le(z^{(\ell)}\ri)-\TI{G}{\alpha_1 \alpha_2}{\ell+1}\ri]\z{i}{\alpha_1}{\ell+1}\z{i}{\alpha_2}{\ell+1} \, .
\ee
Here we emphasized that the stochastic metric $\Ti{\widehat{G}}{\alpha_1\alpha_2}{\ell+1}$ is a function of $z^{(\ell)}$, and the induced coupling of $z^{(\ell)}$ with $z^{(\ell+1)}$ is what leads to the interlayer interactions.

Now, if all we care about are observables that depend only on the outputs of the network -- which includes a very important observable \ldots the output! -- then this full description is too cumbersome.
In order to obtain an effective (i.e.~useful) description of the distribution of outputs $z^{(L)}$, we can marginalizes over all the features $\le\{z^{(1)},\ldots,z^{(L-1)}\ri\}$ as
\be\label{eq:effective-action-full}
e^{-\ac_{\text{eff}}\le(z^{(L)}\ri)} = \int \le[ \prod_{\ell=1}^{L-1}dz^{(\ell)}\ri] e^{-\ac_{\text{full}}\le(z^{(1)}, \dots,  z^{(L)}\ri)}\, ,
\ee
just as we integrated out the fine-grained modes $\phi^{+}$ in~\eqref{eq:field-theory-marginalization} to get the effective description in terms of coarse-grained modes $\phi^{-}$.
And, just like in the field theory example, rather than carrying out this marginalization all at once, we proceeded sequentially, integrating out the preactivations layer by layer.
This resulted in the recursion relations \eqref{eq:K-recursion-reprint} and \eqref{eq:V-recursion-reprint}, and
in solving these recursion relations up to the depth of interest, we get an effective description of neural network output at that depth.\footnote{Note to physicists: the flow in networks from input to output is a flow from the ultraviolet\index{ultraviolet (RG flow)} to the infrared\index{infrared (RG flow)}.}


Now, this last sentence suggests a subtle but interesting shift of perspective, so let us elaborate.
So far in this chapter, we have implicitly assumed a fixed network depth $L$ and described how the preactivation distribution changes as an input $x$ propagates through the intermediate layers, yielding recursion relations for correlators and couplings for the evolution from layer $\ell$ to layer $\ell+1$, for $\ell=0,\ldots,L-1$.
However, it is also valid to view the resulting recursion equations as governing the change in output distributions as the overall network depth changes from $L$ to $L+1$.\footnote{To be precise, the output dimension $n_{\text{out}}$ is fixed. So, as the depth changes from $L$ to $L+1$, we imagine holding fixed the widths for $\ell<L$, inserting a new layer $L$ with $n_L \sim n\gg1$, and then setting the final layer $L+1$ to have width $n_{\text{out}}$.}
In other words, these recursion relations describe the effect of adding an additional layer to the neural network by comparing distributions $p\!\le(z^{(L)}\Big\vert\D\ri)$ and $p\!\le(z^{(L+1)}\Big\vert\D\ri)$.

\index{representation group flow}
Given this perspective, our RG flow can address head-on the effect of the \emph{deep} in \emph{deep learning}.
For instance, as a network get deeper, do the interactions between neurons -- encoded in the finite-width corrections such as the four-point vertex $V^{(\ell)}$ -- get amplified or attenuated?
In the language of RG flow,  couplings that grow with the flow are called \textbf{relevant}\index{relevant (RG flow)|textbf} and those that shrink
are called \textbf{irrelevant}\index{irrelevant (RG flow)|textbf}.\footnote{Couplings that neither grow nor shrink are called \textbf{marginal}\index{marginal (RG flow)|textbf}.}
These names are evocative of whether the interaction matters or not for the effective theory, and so we'll employ the same terminology.
Thus, to explore the effect of depth on the neuron-neuron interactions, we are simply asking whether the four-point vertex  $V^{(\ell)}$ is relevant or irrelevant.


























This question has important implications for deep learning. If all the finite-width couplings were irrelevant, then finite-width networks would asymptote to infinite-width architectures under RG flow.
This would then mean that these networks behave more like infinite-width models as they get deeper, and so deep learning would really be the study of these much simpler Gaussian models.
Fortunately
we'll soon find that the couplings \emph{are} relevant, making our life richer, albeit more complicated. In the next chapter, we'll show that finite networks deviate more and more from their infinite-width counterparts as they get deeper. This has important practical consequences in controlling the instantiation-to-instantiation fluctuations in supervised training 
and also in allowing networks to learn nontrivial representations of their input (\S\ref{ch:features}).

The next chapter explores these relevant questions by explicitly solving recursion equations such as~\eqref{eq:K-recursion-reprint} and~\eqref{eq:V-recursion-reprint}.



 




















































