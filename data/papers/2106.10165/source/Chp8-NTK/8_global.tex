
\chapter{RG Flow of the Neural Tangent Kernel}\index{representation group flow!of the NTK}
\label{ch:NTKa}




\epigraph{People get things backwards and they shouldn't---it has been said, and wisely said, that every successful physical theory swallows its predecessors alive.}{Sidney Coleman,
more forward and a little bit deeper in that same \\
``Quantum Mechanics\index{quantum mechanics} in Your Face'' Dirac\index{Dirac, Paul Adrien Maurice} Lecture  \cite{Coleman:2020put}.\index{Coleman, Sidney}}











\noindent{}In the last chapter, we introduced gradient-based learning as an alternative to Bayesian learning and specifically focused on the \terminate{gradient descent} algorithm.
In short, the gradient descent algorithm involved instantiating a network from the prior distribution and then repeatedly updating the model parameters by running training data through the network. 
This algorithm is straightforward to implement and very efficient to run for any particular network.
In practice, it makes things very easy.

In theory, it makes things a little more difficult. %
For the Bayesian prior, we were able to integrate out the model parameters layer by layer in deriving the output distribution because the initialization distribution\index{initialization distribution} of the biases and weights was extremely simple; in addition,
the large-width expansion made 
it possible to derive analytic expressions for the Bayesian posterior for finite-width networks.
By contrast, the model parameters and the outputs of any particular
network trained by gradient descent are a complicated correlated mess.


\index{typicality}
To make progress, we first need to shift the perspective back to a statistical one. Rather than focusing on how any particular network learns from the data, we instead ask how a \emph{typical} network behaves when being trained. 
If we understand the typical behavior (i.e.~the mean) under gradient descent and have control of the fluctuations from network instantiation to instantiation (i.e.~the variance), then we can describe gradient-based learning as used in practice.

With that statistical perspective in mind, recall from the last chapter that the gradient-descent 
updates decompose 
into an error factor times a function-approximation factor. The latter factor was dubbed the \neo{neural tangent kernel} (NTK) and conveniently summarizes the effect of the model parameters' changes on the behavior of the network.  
This means that the statistics of changes in network observables
in the initial stage of training are governed by the statistics of the NTKs at initialization.
To proceed forward, the core of the current chapter and the next will involve explicitly computing such NTK statistics for deep MLPs; we will postpone the actual analysis of neural network training -- enabled by these computations of the NTK statistics -- until~\S\ref{ch:NTHb} and~\S\ref{ch:eot}. 







\index{forward equation!MLP preactivations}\index{forward equation!NTK}
In~\S\ref{sec:NTH-recursions}, we will lay the groundwork for the recursive computation of the NTK statistics. Namely, starting from the MLP iteration equation, or the \emph{forward} equation for the preactivations, we'll derive a corresponding forward equation for the NTK.
This equation is a layer-to-layer iteration equation that holds for each distinct instantiation of the model parameters.
(Here we'll also remark on how the learning-rate tensor\index{learning rate!learning-rate tensor} should be scaled with network width, an important point that is often neglected in practice.) %


\index{representation group flow}
By averaging over different instantiations, we can then use the forward equation to recursively compute the joint statistics of the NTK and the preactivations.  %
The approach taken here completely mirrors the \emph{RG-flow} approach taken in~\S\ref{ch:ngp} for the preactivations. In~\S\ref{sec:first-layer-deterministic-NTK},~\S\ref{sec:second-layer-fluctuating-NTK}, and~\S\ref{sec:deeper-layer-accumulation-NTK}, we will progressively determine the sequence of joint NTK-preactivation distributions in the first, second, and deeper layers, respectively.

     
  

  






















\setcounter{section}{-1}
\section{Forward Equation for the NTK}\label{sec:NTH-recursions}
As we saw in the previous chapter, 
the evolution of observables $\mathcal{O}(z)$ under \terminate{gradient descent} is governed by the NTK,
\be\label{eq:NTH-definition-reprint}
H_{i_1i_2;\alpha_1\alpha_2} \equiv \sum_{\mu,\nu} \lambda_{\mu\nu} \frac{\td z_{i_1;\alpha_1}}{\td \theta_\mu} \frac{\td z_{i_2;\alpha_2}}{\td \theta_\nu}\,  ,
\ee
where $\lambda_{\mu\nu}$ is the learning-rate tensor\index{learning rate!learning-rate tensor}. %

\index{feature}\index{representation}
Specializing to MLPs, observables can depend not only  on the network's output $z_{i;\alpha}=\z{i}{\alpha}{L}$, but also on the preactivations $z_{i;\alpha}=\z{i}{\alpha}{\ell}$ in any layer. Such $\ell$-th-layer observables for $\ell <L$ tell us about the \emph{hidden-layer representations} of the network. For instance, the neural component $\O = z_i^{(\ell)}(x)$ tells us about an $\ell$-th-layer \terminate{feature} evaluated on an input $x$, while $\O = z_i^{(\ell)}(x)\, z_j^{(\ell)}(x)$ with \terminate{neural indices} $i\neq j$ tracks correlations among different features given $x$.

With similar manipulations as before, we find that an observable $\O$ that depends only on the $\ell$-th-layer preactivations 
\be\label{eq:observable-ell-layer}
\O\!\le(\theta \ri) \equiv \O\Big( z^{(\ell)}\!\le(x_{\delta_1} ; \theta \ri)\!,\,\ldots\,,z^{(\ell)}\!\le(x_{\delta_M} ; \theta \ri) \Big)\, , 
\ee
evolves after a \terminate{gradient descent} update as
\be\label{eq:obsevable-evolution-layer-ell}
\O\Big(\theta(t+1)\Big) - \O\Big(\theta(t)\Big) = - \eta\sum_{i_1,i_2=1}^{n_{\ell}} \sum_{\alpha \in \A} \sum_{\delta \in \D} \le[\frac{\td \L_\A}{\td z_{i_1;\alpha}^{(\ell)}}\frac{\partial \O}{\partial z_{i_2;\delta}^{(\ell)}}\ri]  \NTKM_{i_1 i_2;\alpha\delta}^{(\ell)}  + O(\eta^2)\, ,
\ee
where $x_{\delta_1}, \ldots, x_{\delta_M} \in \D$  for some dataset\index{input data} $\D$.\footnote{
However, note that this is not quite as simple as the expression for the evolution of the network output that we gave in the last chapter~\eqref{eq:change-in-observable-NTH}. In particular, the derivative of the loss with respect to the $\ell$-th-layer preactivations needs to be computed by the \terminate{chain rule} as
\be
\frac{\td \L_\A}{\td z_{i_1;\alpha}^{(\ell)}}=\sum_{j=1}^{n_{L}}\frac{\partial \L_\A}{\partial z_{j;\alpha}^{(L)}}\frac{\td z_{j;\alpha}^{(L)}}{\td z_{i_1;\alpha}^{(\ell)}} \, ,
\ee
with the \terminate{error factor} $\partial \L_\A / \partial z_{j;\alpha}^{(L)}$ now multiplied by the \terminate{chain-rule factor} $\td \z{j}{\alpha}{L}/\td \z{i_1}{\alpha}{\ell}$.
For observables that depend on preactivations from multiple layers, the generalization of \eqref{eq:obsevable-evolution-layer-ell} further involves additional chain-rule factors as well as a sum over NTKs from different layers.
\index{chain rule}
} 
Here, we have defined the
\textbf{$\ell$-th-layer NTK}\index{neural tangent kernel!l-th-layer@$\ell$-th-layer|textbf} as
\be\label{eq:midNTH-definition}
\Tia{H}{i_1i_2}{\alpha_1\alpha_2}{\ell} \equiv \sum_{\mu,\nu} \lambda_{\mu\nu} \frac{\td \z{i_1}{\alpha_1}{\ell}}{\td \theta_\mu} \frac{\td \z{i_2}{\alpha_2}{\ell}}{\td \theta_\nu}  \, ,
\ee
which governs the evolution of the $\ell$-th-layer observables; in terms of this notation, the output NTK is simply $\Tia{H}{i_1i_2}{\alpha_1\alpha_2}{\ell=L}$. Note that whenever we write the $\ell$-th-layer NTK as above, we will always assume that the learning-rate tensor\index{learning rate!learning-rate tensor!layer-diagonal} $\lambda_{\mu\nu}$ does not mix network parameters from different layers, though in general it can still mix the biases and weights within a layer. We will place further restrictions on this in another paragraph.








At initialization, the model parameters are sampled from their initialization distributions, and the $\ell$-th-layer NTK is a stochastic object. In order to emphasize this stochasticity, in what follows we'll decorate the NTK \emph{at initialization} with a hat: $\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{\ell}$. Our goal is to evaluate its statistics.


\index{initialization distribution}\index{learning rate!global}
Before we go any further, it is convenient to make a specialized choice for the learning-rate tensor\index{learning rate!learning-rate tensor} $\lambda_{\mu\nu}$.
In practice, typically $\lambda_{\mu\nu}=\delta_{\mu\nu}$, and there is only the \emph{global} learning rate $\eta$ for the entire model. %
Even in a more general setup,
a learning rate is often shared among each group of parameters that are sampled from the same distribution. Recalling that the same distribution was shared among the biases in a given layer with the same variance $\Cb{\ell}$~\eqref{eq:bias-variance-def-naive} and similarly for the weights with the \emph{rescaled} weight variance $\CW{\ell}$~\eqref{eq:weight-variance-def-naive}, this suggests 
an ansatz
for our \term{training hyperparameters}: we should decompose the learning-rate tensor\index{learning rate!learning-rate tensor} $\lambda_{\mu\nu}$ into a diagonal matrix
\be\label{eq:diag_LR}
\lambda_{\bias{i_1}{\ell} \bias{i_2}{\ell}}=\delta_{i_1i_2}\Lb{\ell}\, ,\quad \lambda_{\W{i_1j_1}{\ell} \W{i_2 j_2}{\ell}}=\delta_{i_1 i_2}\delta_{j_1 j_2}\lamWtil{\ell}\, ,
\ee
giving each group of biases in a layer the same learning rate and each group of weights in a layer the same learning rate, and allowing such learning rates to vary from layer to layer. 

Importantly, we have normalized the learning rate for a given weight $\W{i_1j_1}{\ell}$ by the width of the previous layer $n_{\ell-1}$, just as we did for the variance of the weight's \terminate{initialization distribution}. This normalization is there for much the same reason: the freedom to tune the weight learning rates separately from the bias learning rates will prove necessary for having a sensible large-width expansion.
Going forward, our \terminate{training hyperparameters} will consist of the global learning rate $\eta$
and the individual $\ell$-th-layer learning rates for the biases and weights, $\Lb{\ell}$ and $\lamW{\ell}$.



\index{neural tangent kernel!l-th-layer@$\ell$-th-layer}
Substituting our choice for $\lambda_{\mu\nu}$ back into the definition of the $\ell$-th-layer NTK~\eqref{eq:midNTH-definition}, this expression decomposes as
\be\label{eq:nth-layer-sum-definition}
\Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{\ell} = \sum_{\ell'=1}^{\ell} \le[ \sum_{j=1}^{n_{\ell'}} \le( \Lb{\ell'}  \frac{\td \z{i_1}{\alpha_1}{\ell}}{\td \bias{j}{\ell'}} \frac{\td \z{i_2}{\alpha_2}{\ell}}{\td \bias{j}{\ell'}} + \lamWtil{\ell'}  \sum_{k=1}^{n_{\ell'-1}} \frac{\td \z{i_1}{\alpha_1}{\ell}}{\td \W{j k}{\ell'}} \frac{\td \z{i_2}{\alpha_2}{\ell}}{\td \W{j k}{\ell'}}\ri)\ri] \, .
\ee
Here, the part in the square brackets is the per-layer contribution of the model parameters to the $\ell$-th-layer NTK, treating the biases and weights separately.
We also see that our intuition above in \eqref{eq:diag_LR} was correct: the $\ell'$-th-layer weight learning rate $\LW{\ell'}$ needs to be accompanied by a factor of $1/n_{\ell'-1}$ in order to compensate for the additional summation over the $(\ell'-1)$-th layer \terminate{neural indices} in the second term as compared to the first.
Even so, the layer sum in \eqref{eq:nth-layer-sum-definition} makes this expression somewhat unwieldy and suggests that we should
search for an alternate representation.




\index{neural tangent kernel!l-th-layer@$\ell$-th-layer}
Following our analysis of the preactivations, let's try to find a recursive expression.
To that end, 
consider the $(\ell+1)$-th-layer NTK, $\Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{\ell+1}$, and decompose the sum over layers in its definition by separating the $(\ell+1)$-th-layer term from all of the rest, giving
\begin{align}\label{eq:NTHchain}
\Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{\ell+1} =& \sum_{j=1}^{n_{\ell+1}} \le(\Lb{\ell+1}\frac{\td \z{i_1}{\alpha_1}{\ell+1}}{\td \bias{j}{\ell+1}} \frac{\td \z{i_2}{\alpha_2}{\ell+1}}{\td \bias{j}{\ell+1}} + \frac{\lamW{\ell+1}}{n_{\ell}} \sum_{k=1}^{n_{\ell}} \frac{\td \z{i_1}{\alpha_1}{\ell+1}}{\td \W{jk}{\ell+1}} \frac{\td \z{i_2}{\alpha_2}{\ell+1}}{\td \W{jk}{\ell+1}}\ri)\, \\
&+  \sum_{j_1,j_2=1}^{n_{\ell}} \frac{\td \z{i_1}{\alpha_1}{\ell+1}}{\td \z{j_1}{\alpha_1}{\ell}} \frac{\td \z{i_2}{\alpha_2}{\ell+1}}{\td  \z{j_2}{\alpha_2}{\ell}}\Tia{\NTH}{j_1j_2}{\alpha_1\alpha_2}{\ell} \, .\nonumber
\end{align}
Here, the first line is the $(\ell+1)$-th-layer term that we left alone, while the second line gives the terms from all the other layers after applying the \terminate{chain rule} and then recalling the definition \eqref{eq:nth-layer-sum-definition}.
In this way, the $\ell$-th-layer NTK appears naturally.
This 
means that we can find a simple iterative expression for the NTK,
similar in spirit to the forward equation for the preactivations that defines the MLP.

\index{forward equation!MLP preactivations}\index{forward equation!NTK}
To finish our derivation, we need to evaluate the derivatives in \eqref{eq:NTHchain}. To do so,
recall the preactivation forward iteration equation
\be\label{eq:forward-pass}
\z{i}{\alpha}{\ell+1} = \bias{i}{\ell+1}+\sum_{j=1}^{n_{\ell}}\W{ij}{\ell+1}\s{j}{\alpha}{\ell} \, ,
\ee
and remember that the activations are explicit functions of the preactivations $\s{i}{\alpha}{\ell}\equiv\sigma\!\le(\z{i}{\alpha}{\ell}\ri)$.
The factors in the second line of \eqref{eq:NTHchain} 
coming from the \terminate{chain rule} evaluate to
\be\label{eq:chain-rule-factor}
 \frac{\td \z{i}{\alpha}{\ell+1}}{\td \z{j}{\alpha}{\ell}}= \W{ij}{\ell+1}\ds{j}{\alpha}{\ell} \, ,
\ee
while the derivatives with respect to the $(\ell+1)$-th-layer parameters evaluate to
\be\label{eq:same-layer-derivatives}
\frac{\td \z{i}{\alpha}{\ell+1}}{\td \bias{j}{\ell+1}} = \delta_{ij}\, , \qquad \frac{\td \z{i}{\alpha}{\ell+1}}{\td \W{jk}{\ell+1}} = \delta_{ij} \, \s{k}{\alpha}{\ell} \, .
\ee
All together, we can rewrite \eqref{eq:NTHchain} as
\begin{align}\label{eq:NTH-recursion-without-expectation}
\Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{\ell+1}=&\delta_{i_1i_2} \le[\Lb{\ell+1} +  \lamW{\ell+1}\!\le(\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}} \s{j}{\alpha_1}{\ell} \s{j}{\alpha_2}{\ell} \ri) \ri]\, \\
&+  \sum_{j_1,j_2=1}^{n_{\ell}} \W{i_1j_1}{\ell+1} \W{i_2j_2}{\ell+1}  \ds{j_1}{\alpha_1}{\ell} \ds{j_2}{\alpha_2}{\ell} \Tia{\NTH}{j_1j_2}{\alpha_1\alpha_2}{\ell}\, .\nonumber
\end{align}
This is the \textbf{forward equation for the NTK}\index{forward equation!NTK}, which is an iteration equation that computes the NTK  layer by layer  for any realization of the biases and weights. This is analogous to the way in which \eqref{eq:forward-pass} computes the network output -- as well as all the hidden-layer preactivations -- via a layer-to-layer iteration for a given realization of model parameters.


\subsubsection{Scaling in the effective theory}
\index{forward equation!NTK}\index{learning rate!global}\index{effective theory}
The forward equation \eqref{eq:NTH-recursion-without-expectation} further clarifies our decomposition \eqref{eq:diag_LR} in which we made a distinction between the learning rates for the biases and those for the weights, giving each a different scaling with respect to the layer widths $n_\ell$ of the network.\footnote{You'll have to wait until \S\ref{ch:eft-ntk} to understand why it is advantageous to give a layer dependence to $\Lb{\ell}$ and $\lamW{\ell}$ and to learn how they should be scaled with depth.}

To see why, first recall from \S\ref{ch:training} that the change in the training loss after a step of gradient descent is proportional to the product of the global learning rate $\eta$ and the final-layer NTK $\Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{L}$:
\be\label{eq:change-in-loss-NTH-reprint}
\Delta\L_\A= - \eta\sum_{i_1,i_2=1}^{n_{L}} \sum_{\alpha_1,\alpha_2 \in \A}   \epsilon_{i_1;\alpha_1}  \epsilon_{i_2;\alpha_2}\, \Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{L} + O(\eta^2)\,  ,
\ee
where here we also recall the definition of the \terminate{error factor}%
\be\label{eq:error-factor-ntk}
\epsilon_{i;\alpha} \equiv \frac{\partial \L_\A}{\partial z^{(L)}_{i;\alpha}}\, .
\ee
Note that this \terminate{error factor} generally stays of order one in the large-width limit, cf.~the explicit expression when using the MSE loss~\eqref{eq:mse-function-approximation-error}. Thus, it's essential that the product of the global learning rate and the NTK, $ \eta \NTK^{(L)}$, also stays of order one for large-width networks: if it diverged as the width increases, then the higher-order terms in~\eqref{eq:change-in-loss-NTH-reprint} would dominate and the loss would no longer be guaranteed to decrease; if instead it vanished in this limit, then no training would take place. Either way, training would fail.\index{loss!MSE} 

\index{learning rate!global}\index{forward equation!NTK}
With that in mind, we chose the width scaling of our learning-rate tensor\index{learning rate!learning-rate tensor} so that the NTK naturally stays of order one in the large-width limit and hence a (sufficiently small but not parametrically small) order-one global learning $\eta$ ensures the success of training. In particular, the $(\ell+1)$-th-layer contribution in the first line of the forward equation~\eqref{eq:NTH-recursion-without-expectation} stays of order one if we take
$\Lb{\ell+1}, \LW{\ell+1}=\o{1}$,
with the $1/n_{\ell}$ normalization of the $(\ell+1)$-th-layer weight learning rate playing an essential role in compensating for the summation over the $n_{\ell}$ terms.\footnote{
With this choice, the recursive term in the second line of the forward equation~\eqref{eq:NTH-recursion-without-expectation} also stays of order one. To see this, let's evaluate its expectation:
\begin{align}
\E{\sum_{j_1,j_2=1}^{n_{\ell}} \W{i_1j_1}{\ell+1} \W{i_2j_2}{\ell+1}  \ds{j_1}{\alpha_1}{\ell} \ds{j_2}{\alpha_2}{\ell} \Tia{\NTH}{j_1j_2}{\alpha_1\alpha_2}{\ell}}&=\sum_{j_1,j_2=1}^{n_{\ell}} \E{\W{i_1j_1}{\ell+1} \W{i_2j_2}{\ell+1}}\E{ \ds{j_1}{\alpha_1}{\ell} \ds{j_2}{\alpha_2}{\ell} \Tia{\NTH}{j_1j_2}{\alpha_1\alpha_2}{\ell}}\, \notag\\
&=\delta_{i_1i_2}\,\CW{\ell+1}\!\le( \frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell} \Tia{\NTH}{jj}{\alpha_1\alpha_2}{\ell}}\ri)\, .
\end{align}
In particular, we see that the $1/n_{\ell}$ scaling of the initialization weight variance $\CW{\ell+1}$ is important for ensuring principled behavior of not only the network output, but also the NTK.\index{neural tangent kernel}
}




If instead we 
had
considered the original version of gradient descent with $\lambda_{\mu\nu} = \delta_{\mu\nu}$ rather than \emph{tensorial gradient descent}, \index{gradient descent!tensorial}
we would have been in trouble. In the language of our \terminate{effective theory}, the original gradient descent corresponds to setting $\Lb{\ell}=1$ and $\LW{\ell}=n_{\ell-1}$, which means that the NTK itself would be $O(n)$. We'd then have to scale the global learning rate as $\eta=O(1/n)$ %
to compensate for this $O(n)$ scaling of the NTK. However, since in this case $\eta\Lb{\ell}=O(1/n)$, the order-one contribution from the weights to the NTK would completely overwhelm the %
$1/n$-suppressed contribution from the biases. This would lead to both a lack of appropriate contribution of the biases to the updates of the %
weights
 as well as an extreme under-training of the biases themselves. 


Finally,
let's make 
a general point: in any \terminate{effective theory},
it's really essential to make all large or small scales explicit -- and rescale hyperparameters\index{hyperparameters!scaling in an effective theory} accordingly -- as we did earlier for the variance of the weight \terminate{initialization distribution}, did here for the weight \terminate{learning rate}, and will do later for the depth scaling of both the bias and weight learning rates.
For the effective theorist this ensures that the asymptotic $1/n$ and $1/\ell$ expansions are sound and nontrivial,
and for the practical practitioner\index{practical practitioners} this enables comparisons of hyperparameter values across architectures with different widths and depths. In particular, we expect very generally that this should help mitigate expensive hyperparameter tuning, remove the need for heuristic fixes, and increase the robustness of optimal hyperparameter settings when scaling a model up. %
\index{$1/n$ expansion} %

\subsubsection{Getting things backwards}\index{chain rule}\index{chain-rule factor}
N.B.~the chain-rule factors \eqref{eq:chain-rule-factor} also appear when evaluating the derivative of the network outputs with respect to model parameters
\be
\frac{\td \z{i}{\alpha}{L}}{\td \bias{j}{\ell}}=\frac{\td \z{i}{\alpha}{L}}{\td \z{j}{\alpha}{\ell}}\, , \qquad  \frac{\td \z{i}{\alpha}{L}}{\td  \W{jk}{\ell}}=\sum_{m}\frac{\td \z{i}{\alpha}{L}}{\td \z{m}{\alpha}{\ell}}\frac{\td \z{m}{\alpha}{\ell}}{\td  \W{jk}{\ell}}=\frac{\td \z{i}{\alpha}{L}}{\td \z{j}{\alpha}{\ell}}\s{k}{\alpha}{\ell-1} \, . 
\ee
Evaluating these derivatives gives another neural-network iteration equation,
\be\label{eq:backward-pass}
\frac{\td \z{i}{\alpha}{L}}{\td \z{j}{\alpha}{\ell}}=\sum_{k=1}^{n_{\ell+1}}\frac{\td \z{i}{\alpha}{L}}{\td \z{k}{\alpha}{\ell+1}}\frac{\td \z{k}{\alpha}{\ell+1}}{\td \z{j}{\alpha}{\ell}}=\sum_{k=1}^{n_{\ell+1}}\frac{\td \z{i}{\alpha}{L}}{\td \z{k}{\alpha}{\ell+1}}\W{kj}{\ell+1}\ds{j}{\alpha}{\ell} \, \ \ \ \text{for}\ \ \ \ell < L\, ,
\ee
but in this case for the derivative of the output. In particular,
\eqref{eq:backward-pass} is a \emph{backward} equation: starting from the \emph{final} condition \index{backward equation!MLP}
\be
\frac{\td \z{i}{\alpha}{L}}{\td \z{j}{\alpha}{L}}=\delta_{ij}\, ,
\ee 
we iterate layer-to-layer 
backwards, $\ell=L-1, L-2, \ldots, 1$, by sequential multiplications of the chain-rule factors~\eqref{eq:chain-rule-factor}.\index{chain rule}\index{chain-rule factor}


\index{forward equation!MLP preactivations}
An algorithm based on this backward equation can be efficiently implemented to compute derivatives with respect to the model parameters and, for that reason, is used by most deep-learning packages to compute the gradient as part of any neural network gradient-based learning algorithm. Such a package typically lets practitioners specify a deep learning model by defining a \term{forward pass} -- for MLPs a practitioner would implement the forward equation \eqref{eq:forward-pass} -- and then the package will automatically work out the \term{backward pass} -- i.e.~for MLPs it would implement \eqref{eq:backward-pass}.
The computational algorithm based on \eqref{eq:backward-pass} is termed \term{backpropagation},
which was discovered and rediscovered numerous times in the history of \terminate{deep learning}.
Among them, a particular rediscovery \cite{rumelhart1985learning} was essential in convincing the \terminate{machine learning} community that multilayer neural networks can be trained efficiently.

All that said, when evaluating the NTK in the \terminate{effective theory} it's essential that we use the forward equation \eqref{eq:NTH-recursion-without-expectation}  rather than getting things backwards.
\index{forward equation!NTK}
In the next three-plus-one sections, we'll indeed use the forward equation to recursively compute the \emph{joint} \terminate{initialization distribution} for the $\ell$-th-layer preactivations \emph{and} the $\ell$-th-layer NTK:  %
\begin{align}\label{eq:joint-preactivation-NTK}
&p\!\le(z^{(\ell)}, \NTK^{(\ell)}\Big\vert \D\ri)  \equiv p
\begin{pmatrix}
z^{(\ell)}(x_1) & z^{(\ell)}(x_2) & \ldots &  z^{(\ell)}(x_{N_\D})\\
\NTK^{(\ell) }(x_1, x_1) & \NTK^{(\ell) }(x_1, x_2)& \ldots & \NTK^{(\ell) }(x_1, x_{N_\D})\\
\NTK^{(\ell) }(x_2, x_1) & \NTK^{(\ell) }(x_2, x_2) & \ldots & \NTK^{(\ell) }(x_2, x_{N_\D})\\
\vdots  & \vdots  & \ddots & \vdots  \\
\NTK^{(\ell) }(x_{N_\D}, x_1) & \NTK^{(\ell) }(x_{N_\D}, x_2) & \ldots & \NTK^{(\ell) }(x_{N_\D}, x_{N_\D}) \\ 
\end{pmatrix} \, .  \notag \\
\end{align}
(On the right-hand side, we've
suppressed \terminate{neural indices} while explicitly writing out the input dependence.
This emphasizes that the preactivations are each functions of a single input and that the NTK components are each functions of a pair of inputs.)















\section{First Layer: Deterministic NTK}
\label{sec:first-layer-deterministic-NTK}\index{neural tangent kernel!first-layer}
Recall from~\S\ref{sec:first-layer-gaussian} that at initialization the first-layer preactivations,
\be\label{eq:first-layer-preactivation-def-reprint}\index{forward equation!MLP preactivations}
\z{i}{\alpha}{1} \equiv \bias{i}{1}+\sum_{j=1}^{n_{0}}\W{ij}{1}\x{j}{\alpha}\, , %
\ee
are distributed according to a zero-mean \terminate{Gaussian distribution},
\be\label{eq:first-layer-distribution-reprint}
p\!\le(z^{(1)}\Big\vert \D\ri)= \frac{1}{\dete{2\pi G^{(1)}}^{\frac{n_1}{2}}}\exp\!\le(-\frac{1}{2}\sum_{i=1}^{n_1}\sum_{\alpha_1,\alpha_2\in\D}\Kinv{\alpha_1 \alpha_2}{1}\z{i}{\alpha_1}{1}\z{i}{\alpha_2}{1}\ri)\, ,
\ee
with the first-layer deterministic metric -- a function of the inputs -- given by \index{action!quadratic}
\be\label{eq:first-layer-metric-reprint}
\Ti{G}{\alpha_1 \alpha_2}{1}\equiv\Cb{1}+\CW{1}\frac{1}{n_0}\sum_{j=1}^{n_0}\x{j}{\alpha_1}\x{j}{\alpha_2}\,  .
\ee
In particular, the quadratic action in the exponent of \eqref{eq:first-layer-distribution-reprint} indicates the absence of \terminate{interactions} between neurons.
This enables us to factor expectation values of first-layer observables into separate Gaussian integrals for each neuron.

\index{neural tangent kernel!first-layer}
The first-layer NTK at initialization is even more trivial and can be read off from the original definition of the NTK~\eqref{eq:nth-layer-sum-definition} by plugging in the derivatives \eqref{eq:same-layer-derivatives} and remembering the identification $\s{i}{\alpha}{0}=\x{i}{\alpha}$:
\be\label{eq:NTHinitial}
\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{1}=\delta_{i_1i_2} \le[\Lb{1} + \lamW{1} \le(\frac{1}{n_0}\sum_{j=1}^{n_{0}}\x{j}{\alpha_1} \x{j}{\alpha_2}\ri)  \ri]\equiv\delta_{i_1i_2}\Ti{\NTKM}{\alpha_1\alpha_2}{1}\, .
\ee
Like the first-layer metric,
the first-layer NTK is completely deterministic -- hence no hat on the right-hand side of the equation -- 
and is diagonal in its \terminate{neural indices}. Remembering our exposition on the off-diagonal components of the NTK in \S\ref{sec:gd}, this in particular means that, for single-layer networks, a feature captured by a particular neuron cannot affect the gradient-descent update for another feature on any other neuron.
\index{neural tangent kernel!first-layer}\index{gradient descent}

\index{neural tangent kernel!first-layer}
Finally, recalling our discussion of deterministic distributions in \S\ref{sec:MLP_distribution}, the joint distribution of the first-layer preactivations and the first-layer NTK can be written as
\be
p\!\le(z^{(1)},\NTK^{(1)}\Big\vert \D\ri)=p\!\le(z^{(1)}\Big\vert \D\ri) \prod_{(i_1i_2),(\alpha_1\alpha_2)}\delta\!\le(\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{1}-\delta_{i_1i_2}\Ti{\NTKM}{\alpha_1\alpha_2}{1}\ri)\, ,
\ee
where the product of the \terminate{Dirac delta function}s runs over all pairs of \terminate{neural indices} and \terminate{sample indices}.  Just as the first-layer preactivation distribution was representative of deeper layers in the \terminate{infinite-width limit}, this first-layer joint distribution is also representative of deeper-layer joint distributions in the infinite-width limit: the preactivation distribution is exactly Gaussian, the NTK distribution is completely deterministic, and there is no correlation between the two, i.e., they are statistically independent from each other once the dataset is fixed.
\index{statistical independence}


\section{Second Layer: Fluctuating NTK}
\label{sec:second-layer-fluctuating-NTK}\index{forward equation!MLP preactivations}\index{neural tangent kernel!second-layer}
Now, let us see how finite-width corrections can modify this picture in the second layer.

Recall from~\S\ref{sec:second-layer-non-gaussian} that the second-layer preactivations are given by
\be\label{eq:second-layer-preactivations-reprint}\index{forward equation!MLP preactivations}
\z{i}{\alpha}{2}=\bias{i}{2}+\sum_{j=1}^{n_{1}}\W{ij}{2}\s{j}{\alpha}{1}\, .%
\ee
After \terminate{marginalizing over} the first-layer preactivations $z^{(1)}$, the correlated fluctuations of the preactivations in the first layer resulted in nontrivial interaction\index{interactions} between different neurons in the second layer.
At the leading nontrivial order in $1/n_1$, this led to a \terminate{nearly-Gaussian distribution} with a quartic action~\eqref{eq:second-layer-quartic-action-in-SD-subsubsection} for the second-layer preactivations, with the leading non-Gaussianity captured by a nonzero connected four-point correlator.\index{connected correlator!four-point} %



\index{forward equation!NTK}\index{neural tangent kernel!second-layer}\index{neural tangent kernel!first-layer}
As for the NTK, looking at its forward equation~\eqref{eq:NTH-recursion-without-expectation} and 
recalling that the first-layer NTK is deterministic~\eqref{eq:NTHinitial}, we see that
the second-layer NTK is given by
\be\label{eq:NTK-second-stochastic}
\Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{2}=\delta_{i_1i_2} \!\le[\Lb{2} + \lamW{2}\le(\frac{1}{n_{1}}\sum_{j=1}^{n_{1}} \s{j}{\alpha_1}{1} \s{j}{\alpha_2}{1}\ri)  \ri] +  \sum_{j=1}^{n_{1}} \W{i_1 j }{2} \W{i_2 j}{2} \, \ds{j}{\alpha_1}{1} \ds{j}{\alpha_2}{1} \,\Ti{\NTKM}{\alpha_1\alpha_2}{1}\, .
\ee
This second-layer NTK depends on two sets of stochastic variables, the weights $W^{(2)}_{ij}$ and the first-layer preactivations $\z{i}{\alpha}{1}$, and hence it fluctuates.

To compute its mean we take an expectation of \eqref{eq:NTK-second-stochastic}, finding
\begin{align}\label{eq:NTK-second-mean}
&\E{\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{2}} \\
=&\delta_{i_1i_2} \!\le[\Lb{2} + \lamW{2}\le(\frac{1}{n_1}\sum_{j=1}^{n_{1}} \E{\s{j}{\alpha_1}{1} \s{j}{\alpha_2}{1}}\ri)\ri]\, \notag +\sum_{j=1}^{n_{1}} \E{\W{i_1j}{2} \W{i_2j}{2}} \E{\ds{j}{\alpha_1}{1} \ds{j}{\alpha_2}{1}}\Ti{\NTKM}{\alpha_1\alpha_2}{1}\, \nonumber\\
=&\delta_{i_1i_2} \le[\Lb{2} + \lamW{2}\bra\sigma_{\alpha_1} \sigma_{\alpha_2}\ket_{G^{(1)}}  +  \CW{2}\bra\sigma^{\prime}_{\alpha_1} \sigma^{\prime}_{\alpha_2}\ket_{G^{(1)}}\Ti{\NTKM}{\alpha_1\alpha_2}{1}\ri] \notag\\
\equiv& \delta_{i_1i_2}\, \Ti{\NTKM}{\alpha_1\alpha_2}{2}\, .\nonumber
\end{align}
Here, in the second line, the expectation of the recursive term
factorized because the second-layer weights $W^{(2)}_{ij}$ are 
statistically independent from the first-layer preactivations. Additionally, in the third line we recalled \eqref{eq:two-activations-Gauss}, in which we showed that the two-point correlators can be expressed as a separate Gaussian expectations\index{Gaussian expectation} 
for each neuron, with the variance given by the first-layer metric $G^{(1)}$.\footnote{Note that the logic around \eqref{eq:two-activations-Gauss} is the same whether or not the Gaussian expectation is of activations or derivatives of the activation. In other words, for the first-layer preactivations we also have $\E{\ds{j}{\alpha_1}{1} \ds{j}{\alpha_2}{1}}= \bra \sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\ket_{G^{(1)}}$.}
Further, inspecting our answer \eqref{eq:NTK-second-mean}, we see that the mean of the second-layer NTK is diagonal in its \terminate{neural indices}. Furthermore, we separated the part that encodes the sample dependence and symbolized it by taking off its hat because it is a mean, not a stochastic variable.
\index{Gaussian expectation}\index{statistical independence}\index{neural tangent kernel!second-layer}


\index{neural tangent kernel!second-layer}\index{fluctuations}\index{correlator!two-point}\index{correlator!four-point}\index{neural tangent kernel!variance}
Now, let's compute the variance.
First, define the second-layer NTK fluctuation through our usual decomposition,\index{tensor decomposition!NTK mean and fluctuation}
\be
\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{2}\equiv \delta_{i_1i_2} \Ti{\NTKM}{\alpha_1\alpha_2}{2}+\DNTK{i_1i_2}{\alpha_1\alpha_2}{2}\, ,
\ee
so that the expectation of the magnitude of this fluctuation determines the covariance:
 \be
\E{\DNTK{i_1i_2}{\alpha_1\alpha_2}{2}\DNTK{i_3i_4}{\alpha_3\alpha_4}{2}}=\E{\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{2}\Tia{\NTK}{i_3i_4}{\alpha_3\alpha_4}{2}}-\E{\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{2}}\E{\Tia{\NTK}{i_3i_4}{\alpha_3\alpha_4}{2}}\, .
\ee
Substituting in our expression \eqref{eq:NTK-second-stochastic} for the second-layer stochastic NTK and using the independence of the second-layer weights from the first-layer preactivations, we find a complicated-looking result
\begin{align}\label{eq:second-layer-ntk-pre-omega}
&\E{\DNTK{i_1i_2}{\alpha_1\alpha_2}{2}\DNTK{i_3i_4}{\alpha_3\alpha_4}{2}}\, \\
=&\frac{1}{n_1}\delta_{i_1i_2}\delta_{i_3i_4}\Bigg\{\le(\lamW{2}\ri)^2\Big[\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{G^{(1)}}-\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(1)}}\bra\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{G^{(1)}}\Big]\, \nonumber\\
&\quad \quad \quad \quad \quad +\CW{2}\Ti{\NTKM}{\alpha_1\alpha_2}{1}\lamW{2}\le[\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{G^{(1)}}-\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\ket_{G^{(1)}}\bra\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{G^{(1)}}\ri]\, \nonumber\\
&\quad \quad \quad \quad \quad +\lamW{2}\CW{2}\Ti{\NTKM}{\alpha_3\alpha_4}{1}\le[\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}-\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(1)}}\bra\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}\ri]\, \nonumber\\
&\quad \quad \quad \quad \quad +\le(\CW{2}\ri)^2\Ti{\NTKM}{\alpha_1\alpha_2}{1}\Ti{\NTKM}{\alpha_3\alpha_4}{1}\le[\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}-\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\ket_{G^{(1)}}\bra\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}\ri]\Bigg\}\, \nonumber\\
&+\frac{1}{n_1}\le(\delta_{i_1i_3}\delta_{i_2i_4}+\delta_{i_1i_4}\delta_{i_2i_3}\ri)\le(\CW{2}\ri)^2\Ti{\NTKM}{\alpha_1\alpha_2}{1}\Ti{\NTKM}{\alpha_3\alpha_4}{1}\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}\, .\nonumber
\end{align}
To get this expression, we recalled not only \eqref{eq:two-activations-Gauss} for the two-point correlators, 
but also both \eqref{eq:four-activations-one-neuron-Gauss} and \eqref{eq:four-activations-two-neurons-Gauss} for the different pairings of the four-point correlators, with the pairings depending on whether all activations are on the same neuron or are on two different neurons, respectively.
As with our computation of the mean above,
the computations of these four-point correlators proceed similarly regardless of whether an activation has a derivative or not.



\index{neural tangent kernel!second-layer}
To help make sense of this rather ugly expression \eqref{eq:second-layer-ntk-pre-omega}, let's first decompose the second-layer NTK variance\index{neural tangent kernel!variance} into a sum of two different types of tensors
\begin{align}\label{eq:NTH-variance-decomposition-second}\index{tensor decomposition!NTK variance $A$/$B$}
&\E{\DNTK{i_1i_2}{\alpha_1\alpha_2}{2}\DNTK{i_3i_4}{\alpha_3\alpha_4}{2}}\, \\
\equiv&\frac{1}{n_1}\le[\delta_{i_1i_2}\delta_{i_3i_4} \NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{2}+\delta_{i_1i_3}\delta_{i_2i_4}\NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{2}+\delta_{i_1i_4}\delta_{i_2i_3}\NTHB{\alpha_1\alpha_4\alpha_2\alpha_3}{2}\ri]\, . \nonumber
\end{align}
This decomposition was motivated by the pattern of \terminate{Kronecker delta}s that appear in \eqref{eq:second-layer-ntk-pre-omega}. Next, by comparing this 
to our original expression \eqref{eq:second-layer-ntk-pre-omega}, we 
see that these tensors are given by 
\begin{align}
\NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{2}=& \bra \Oi{\alpha_1\alpha_2}{2} \Oi{\alpha_3\alpha_4}{2} \ket_{G^{(1)}}-\bra \Oi{\alpha_1\alpha_2}{2}\ket_{G^{(1)}}\bra \Oi{\alpha_3\alpha_4}{2} \ket_{G^{(1)}}\, ,\label{eq:A-recursion-second}\\
\NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{2}=& \le(\CW{2}\ri)^2\Ti{\NTKM}{\alpha_1\alpha_2}{1}\Ti{\NTKM}{\alpha_3\alpha_4}{1}\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}\, ,\label{eq:B-recursion-second}
\end{align}
where on the first line we've introduced an auxiliary stochastic variable,
\be\label{eq:def-omega-without-neural-second}
\Oi{\alpha_1\alpha_2}{2} \equiv \lamW{2} \, \Ti{\sigma}{\alpha_1}{1} \Ti{\sigma}{\alpha_2}{1} + \CW{2}\Ti{\NTKM}{\alpha_1\alpha_2}{1}\, \dTi{\sigma}{\alpha_1}{1} \dTi{\sigma}{\alpha_2}{1}  \, ,
\ee
in order to remedy the ugliness of what would have otherwise been a very long expression.\footnote{Note that $\NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{2}$~\eqref{eq:A-recursion-second} has the same symmetries as the \terminate{four-point vertex} $\V{\alpha_1\alpha_2}{\alpha_3\alpha_4}{\ell}$. In particular, it's symmetric under exchanges of \terminate{sample indices} $\alpha_1\leftrightarrow \alpha_2$, $\alpha_3\leftrightarrow \alpha_4$, and $(\alpha_1\alpha_2) \leftrightarrow (\alpha_3\alpha_4)$, and this symmetry persists to deeper layers, cf.~\eqref{eq:A-recursion}.

Now, you might raise your hand and say that $\NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{2}$~\eqref{eq:B-recursion-second} also respects the same symmetry. That's correct here, but this symmetry will be broken in deeper layers. In general -- cf.~\eqref{eq:B-recursion} -- $\NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell}$ will be symmetric under  $(\alpha_1\alpha_2) \leftrightarrow (\alpha_3\alpha_4)$ and $(\alpha_1\alpha_3) \leftrightarrow (\alpha_2\alpha_4)$ but \emph{not} under $\alpha_1\leftrightarrow \alpha_2$ or $\alpha_3\leftrightarrow \alpha_4$ individually.}
From \eqref{eq:A-recursion-second} and \eqref{eq:B-recursion-second} we see clearly that these tensors are of order one. Given \eqref{eq:NTH-variance-decomposition-second}, this in turn means that the second-layer NTK variance\index{neural tangent kernel!variance} is suppressed by $1/n_1$ in the large-width limit. In other words, the second-layer NTK is deterministic in the strict \terminate{infinite-width limit}, but in backing off that limit it fluctuates according to \eqref{eq:NTH-variance-decomposition-second}, \eqref{eq:A-recursion-second}, and \eqref{eq:B-recursion-second}. 


\index{correlator!two-point}\index{correlator!four-point}
Moreover, at finite width the second-layer NTK not only fluctuates, but also has nontrivial cross correlation\index{cross correlation!NTK-preactivation} with the second-layer preactivations. 
This can ultimately be traced to the fact that the second-layer preactivations~\eqref{eq:second-layer-preactivations-reprint} and the second-layer NTK~\eqref{eq:NTK-second-stochastic} 
are both functions of the same stochastic variables: the second-layer weights $W^{(2)}_{ij}$ and the first-layer preactivations $\z{i}{\alpha}{1}$.


This cross correlation can be computed analogously to the way we computed the NTK mean and variance. 
Substituting in the definition of the second-layer preactivations~\eqref{eq:second-layer-preactivations-reprint} and the second-layer NTK~\eqref{eq:NTK-second-stochastic}, and again using the \terminate{statistical independence} of the second-layer weights $W^{(2)}_{ij}$ from the first-layer preactivations $\z{i}{\alpha}{1}$, we find
\begin{align}\label{eq:unrolling-cross-first}
&\E{\z{i_1}{\alpha_1}{2}\DNTK{i_2i_3}{\alpha_2\alpha_3}{2}} = 0 \, ,\\
\label{eq:unrolling-cross-second}
&\E{\z{i_1}{\alpha_1}{2}\z{i_2}{\alpha_2}{2}\DNTK{i_3i_4}{\alpha_3\alpha_4}{2}}=\E{\z{i_1}{\alpha_1}{2}\z{i_2}{\alpha_2}{2}\Tia{\NTK}{i_3i_4}{\alpha_3\alpha_4}{2}}-\E{\z{i_1}{\alpha_1}{2}\z{i_2}{\alpha_2}{2}}\E{\Tia{\NTK}{i_3i_4}{\alpha_3\alpha_4}{2}}\, \notag\\
=&\frac{1}{n_1}\delta_{i_1i_2}\delta_{i_3i_4}\Bigg\{\lamW{2}\CW{2}\Big[\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{G^{(1)}}-\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(1)}}\bra\sigma_{\alpha_3}\sigma_{\alpha_4}\ket_{G^{(1)}}\Big]\, \nonumber\\
&\quad \quad \quad \quad \quad +\le(\CW{2}\ri)^2\Ti{\NTKM}{\alpha_3\alpha_4}{1}\Big[\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}-\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(1)}}\bra\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}\Big]\Bigg\}\, \nonumber\\
&+\frac{1}{n_1}\le(\delta_{i_1i_3}\delta_{i_2i_4}+\delta_{i_1i_4}\delta_{i_2i_3}\ri)\le(\CW{2}\ri)^2\Ti{\NTKM}{\alpha_3\alpha_4}{1}\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}\, . 
\end{align}
Here, as for the variance \eqref{eq:second-layer-ntk-pre-omega}, we recalled the suitably generalized versions of
\eqref{eq:two-activations-Gauss}, \eqref{eq:four-activations-one-neuron-Gauss}, and \eqref{eq:four-activations-two-neurons-Gauss} for the two- and four-point correlators. %
Thus,
we see that the first measure of cross correlation\index{cross correlation!NTK-preactivation} between the second-layer preactivations and the second-layer NTK~\eqref{eq:unrolling-cross-first} vanishes, but the second one~\eqref{eq:unrolling-cross-second} is nonzero at finite width.

To aid us in our deep-layer analysis, it will be convenient to 
 decompose this cross correlation\index{cross correlation!NTK-preactivation} \eqref{eq:unrolling-cross-second} into two tensors with \terminate{sample indices} only, just as we did for the variance in \eqref{eq:NTH-variance-decomposition-second}:
\begin{align}\label{eq:F-and-D-decomposition}\index{tensor decomposition!NTK-preactivation $D$/$F$}
&\E{\z{i_1}{\alpha_1}{2}\z{i_2}{\alpha_2}{2}\DNTK{i_3i_4}{\alpha_3\alpha_4}{2}} \\
=&\frac{1}{n_1}\le[\delta_{i_1i_2}\delta_{i_3i_4} \NTHD{\alpha_1\alpha_2\alpha_3\alpha_4}{2}+\delta_{i_1i_3}\delta_{i_2i_4}\NTHF{\alpha_1\alpha_3\alpha_2\alpha_4}{2}+\delta_{i_1i_4}\delta_{i_2i_3}\NTHF{\alpha_1\alpha_4\alpha_2\alpha_3}{2}\ri]\, \notag .
\end{align}
Comparing this decomposition with our explicit formula for the correlator \eqref{eq:unrolling-cross-second}, we can identify expressions for these tensors
\begin{align}
\NTHD{\alpha_1\alpha_2\alpha_3\alpha_4}{2}=&\CW{2}\le[ \bra \sigma_{\alpha_1}\sigma_{\alpha_2} \Oi{\alpha_3\alpha_4}{2} \ket_{G^{(1)}}-\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(1)}}\bra \Oi{\alpha_3\alpha_4}{2} \ket_{G^{(1)}}\ri]\, ,\label{eq:D-recursion-second}\\
\NTHF{\alpha_1\alpha_3\alpha_2\alpha_4}{2}=&\le(\CW{2}\ri)^2\Ti{\NTKM}{\alpha_3\alpha_4}{1}\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(1)}}\, ,\label{eq:F-recursion-second}
\end{align}
where we've also recalled the stochastic tensor $\Oi{\alpha_1\alpha_2}{2}$ defined in~\eqref{eq:def-omega-without-neural-second}.\footnote{The cross-correlation tensor $\NTHD{\alpha_1\alpha_2\alpha_3\alpha_4}{2}$~\eqref{eq:D-recursion-second} -- and more generally $\NTHD{\alpha_1\alpha_2\alpha_3\alpha_4}{\ell}$ in deeper layers, cf.~\eqref{eq:D-recursion} -- is symmetric under exchanges of \terminate{sample indices} $\alpha_1\leftrightarrow \alpha_2$ and $\alpha_3\leftrightarrow \alpha_4$, but of course \emph{not} under $(\alpha_1\alpha_2) \leftrightarrow (\alpha_3\alpha_4)$.
The other tensor $\NTHF{\alpha_1\alpha_3\alpha_2\alpha_4}{2}$~\eqref{eq:F-recursion-second} respects this same symmetry in the second layer, but has no symmetry at all in deeper layers, cf.~\eqref{eq:F-recursion}.}
Just as for $\NTHAwo{2}$ and  $\NTHBwo{2}$ above, both $\NTHDwo{2}$ and $\NTHFwo{2}$ are manifestly of order one. Similar to the second-layer NTK variance\index{neural tangent kernel!variance}, this means that the cross correlator~\eqref{eq:F-and-D-decomposition} is suppressed by $1/n_1$ in the large-width limit, vanishing in the strict \terminate{infinite-width limit}.






In summary, the joint distribution of the second-layer preactivations and second-layer NTK,
\be
p\!\le(z^{(2)},\NTK^{(2)}\Big\vert \D\ri) \, ,
\ee
at leading nontrivial order in the \terminate{$1/n$ expansion} is \neo{nearly-Gaussian distribution} with  \emph{(i)} a quartic interaction among preactivations on different neurons, \emph{(ii)} a fluctuating NTK, and \emph{(iii)} cross correlation\index{cross correlation!NTK-preactivation} between the preactivations and NTK. %
All of these finite-width effects become more complicated for deeper layers.












\section{Deeper Layers: Accumulation of NTK Fluctuations}
\label{sec:deeper-layer-accumulation-NTK}
As before with~\S\ref{sec:first-layer-gaussian}$\,\parallel\,$\S\ref{sec:first-layer-deterministic-NTK} and~\S\ref{sec:second-layer-non-gaussian}$\,\parallel\,$\S\ref{sec:second-layer-fluctuating-NTK}, this section parallels~\S\ref{sec:deeper-layer-accumulation}. 
In \S\ref{sec:deeper-layer-accumulation}, we investigated the \terminate{nearly-Gaussian distribution} of preactivations $p\!\le(z^{(\ell+1)}\Big\vert\D\ri)$
at finite width by considering an interlayer joint distribution $p\!\le(z^{(\ell+1)}, z^{(\ell)}\Big\vert\D\ri)$ and then integrating out the $\ell$-th-layer preactivations.
In particular, due to correlated dependence on the preactivations in previous layers, the non-Gaussianity in the preactivation distribution accumulated as depth increased, manifesting itself in the running \terminate{four-point vertex} $V^{(\ell)}$.
\index{nearly-Gaussian distribution}\index{Gaussian distribution}



\index{neural tangent kernel!l-th-layer@$\ell$-th-layer}\index{neural tangent kernel!variance}\index{neural tangent kernel!mean}
This same mechanism makes the NTK fluctuations accumulate, amplifying the NTK variance as well as the cross correlation\index{cross correlation!NTK-preactivation} between the NTK and preactivations.
In this section, we will derive recursions for the NTK mean,  the NTK-preactivation cross correlation\index{cross correlation!NTK-preactivation}, and the NTK variance that together determine the $\ell$-th-layer joint distribution at leading nontrivial order in $1/n$.\index{$1/n$ expansion}
What follows is a \neo{goode olde calculation}, so please sharpen your quills, unfurl your parchment, and inform your majordomo that you require a cleared schedule for the rest of the day. %



\setcounter{subsection}{-1}
\subsection{\emph{Inter}lude: \emph{Inter}layer Correlations}\label{subsec:interlayer-interlude}\index{interlayer correlation}
If you have an eidetic memory, then perhaps you recall that the main complication with our derivation of the general $(\ell+1)$-th-layer preactivation statistics -- as compared to the second layer statistics -- was that the $\ell$-th-layer preactivation distribution $p\!\le(z^{(\ell)}\Big\vert\D\ri)$ was also non-Gaussian, unlike the Gaussian preactivation distribution in the first layer.
For such a \terminate{nearly-Gaussian distribution} $p\!\le(z^{(\ell)}\Big\vert\D\ri)$, \neo{interactions} imply a nontrivial \term{intralayer correlation} between observables of the preactivations across different neurons $i_1\ne i_2$. Specifically, the covariance of two arbitrary single-neuron functions $\mathcal{F}\!\le(\z{i_1}{\A_1}{\ell}\ri)$ and $\mathcal{G}\!\le(\z{i_2}{\A_2}{\ell}\ri)$ depending on data subsamples $\A_1, \A_2\subset\D$, respectively, is given by~\eqref{eq:general-covariance} and reprinted here:
\begin{align}\label{eq:general-covariance-reprint}
&\cov{\mathcal{F}\!\le(\z{i_1}{\A_1}{\ell}\ri)\!}{\mathcal{G}\!\le(\z{i_2}{\A_2}{\ell}\ri)} \equiv \E{\mathcal{F}\!\le(\z{i_1}{\A_1}{\ell}\ri)\mathcal{G}\!\le(\z{i_2}{\A_2}{\ell}\ri)}-\E{\mathcal{F}\!\le(\z{i_1}{\A_1}{\ell}\ri)}\E{\mathcal{G}\!\le(\z{i_2}{\A_2}{\ell}\ri)}\, \notag \\
=&\!\!\!\!\!\sum_{\beta_1,\ldots,\beta_4\in\D}\!\frac{1}{4n_{\ell-1}}\TI{V}{(\beta_1\beta_2)(\beta_3\beta_4)}{\ell}\bra \le(z_{\beta_1} z_{\beta_2}-G^{(\ell)}_{\beta_1\beta_2}\ri) \mathcal{F}\!\le(z_{\A_1}\ri) \ket_{G^{(\ell)}}\!\!\bra  \le(z_{\beta_3} z_{\beta_4}-G^{(\ell)}_{\beta_3\beta_4}\ri)\mathcal{G}\!\le(z_{\A_2}\ri) \ket_{G^{(\ell)}}\, \notag\\
&+\o{\frac{1}{n^2}}\,  .
\end{align}
In this reprinting, we implicitly substituted in our leading large-width expressions~\eqref{eq:two-point-match-general} and~\eqref{eq:four-point-match-general} for the quadratic coupling $g_{(\ell)}$ and quartic coupling $v_{(\ell)}$, respectively.
We have also recalled our long-forgotten shorthand notation for the covariance of random variables~\eqref{eq:C2}, which we will use judiciously throughout this section.
This \emph{intralayer} formula will soon prove itself useful.\index{coupling!quadratic}\index{coupling!quartic}


Enlarging
our view to the preactivation-NTK joint distribution~\eqref{eq:joint-preactivation-NTK}, we'll encounter another complication due to \textbf{\emph{inter}layer correlation}\index{interlayer correlation} of the form
\be\label{eq:abstract-interlayer-goal}
\E{\mathcal{O}\!\le(z^{(\ell+1)}\ri)\mathcal{P}\!\le(W^{(\ell+1)}\ri)\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)} \ri)}\, ,
\ee
where $\O$ is some
function of $(\ell+1)$-th-layer preactivations,
$\mathcal{P}$ is a polynomial of $(\ell+1)$-th-layer weights,
and $\mathcal{Q}$ is a function of $\ell$-th-layer preactivations
and the $\ell$-th-layer NTK.\index{neural tangent kernel!l-th-layer@$\ell$-th-layer}
For instance, taking the NTK-preactivation cross correlation\index{cross correlation!NTK-preactivation}
\be\label{eq:cross-general-ell-first-appearance}
\E{\z{i_1}{\alpha_1}{\ell+1}\z{i_2}{\alpha_2}{\ell+1} \Tia{\NTK}{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}\, 
\ee
and unraveling the NTK through its forward equation~\eqref{eq:NTH-recursion-without-expectation}\index{forward equation!NTK}, we get an interlayer correlation of the form~\eqref{eq:abstract-interlayer-goal} with $\mathcal{P}=1$ from the additive term in the square brackets and an interlayer correlation of the same form with $\mathcal{P}=\W{i_3j_3}{\ell+1}\W{i_4j_4}{\ell+1}$ from the recursive term.
While it was simple enough to evaluate such an expectation for the second layer,
it's somewhat subtle for a general layer.

\index{interlayer correlation}
That said, there's actually a pretty neat trick that lets us reduce such interlayer correlations~\eqref{eq:abstract-interlayer-goal} to expectations of solely $\ell$-th-layer variables. Such expectations can subsequently be evaluated with the \emph{intra}layer formula~\eqref{eq:general-covariance-reprint} above.
Let us now teach you this \terminate{magic trick} before diving deep into learning the deeper-layer analysis.\footnote{
    As similar interlayer correlations appear in~\S\ref{ch:features}, we'll keep our exposition completely general rather than specializing to the NTK-preactivation cross correlation\index{cross correlation!NTK-preactivation} \eqref{eq:cross-general-ell-first-appearance}.
}




First, using the definition of the expectation and the conditional structure of the distribution, the \terminate{interlayer correlation}~\eqref{eq:abstract-interlayer-goal} can be expressed as (suppressing all indices)
\begin{align}\label{eq:decompose-interlayer}
&\E{\mathcal{O}\!\le(z^{(\ell+1)}\ri)\mathcal{P}\!\le(W^{(\ell+1)}\ri)\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)} \ri)}\, \\
=&\int dz^{(\ell)} d\NTK^{(\ell)} p\!\le(z^{(\ell)}, \NTK^{(\ell)}\Big\vert \D\ri)\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)} \ri)\, \notag\\
&\quad \times\Bigg[\int db^{(\ell+1)}dW^{(\ell+1)} p\!\le(b^{(\ell+1)}\ri)p\!\le(W^{(\ell+1)}\ri)\mathcal{P}\!\le(W^{(\ell+1)}\ri)\, \notag\\
&\quad \quad\quad \times\int dz^{(\ell+1)} p\!\le(z^{(\ell+1)}\Big\vert b^{(\ell+1)},W^{(\ell+1)},z^{(\ell)}\ri) \mathcal{O}\!\le(z^{(\ell+1)}\ri)\Bigg]\, .\notag
\end{align}
Our strategy will be to \emph{integrate out}\index{integrating out} or marginalize over\index{marginalizing over} the $(\ell+1)$-th-layer parameters in order to express the object inside the square bracket as a function of the $\ell$-th-layer variables \emph{only}. In this way, 
the entire object will become an $\ell$-th-layer expectation that we already know how to handle.



\index{interlayer correlation}
Second, rather than working with an abstract polynomial $\mathcal{P}$, let's construct a \neo{generating function} for these interlayer correlations through the use of a \neo{source term}: 
\be\label{eq:weighty-source}
\mathcal{P}\!\le(W^{(\ell+1)}\ri)=e^{\sum_{i,j} \JW{ij}W^{(\ell+1)}_{ij}}\, .
\ee
Recall from your pretraining days (\S\ref{sec:Gauss}) that a generating function such as \eqref{eq:weighty-source} could be used to evaluate expectations such as~\eqref{eq:abstract-interlayer-goal} with any polynomial insertions of weights $W^{(\ell+1)}_{ij}$. To do this, we differentiate the evaluated generating function some number of times with respect to the source $\JW{ij}$ and then set the source to zero.


\index{initialization distribution}
Now with our choice~\eqref{eq:weighty-source} 
in mind for $\mathcal{P}$, we can explicitly evaluate the expression in the square brackets in~\eqref{eq:decompose-interlayer} as follows: \emph{(i)} recall the initialization distributions for the biases~\eqref{eq:full-bias-initialization} and weights~\eqref{eq:full-weights-initialization}, \emph{(ii)} recall from  \eqref{eq:deeper-layer-formal-expression-first-encounter} that the conditional distribution $p\!\le(z^{(\ell+1)}\Big\vert b^{(\ell+1)},W^{(\ell+1)},z^{(\ell)}\ri)$
encodes the MLP forward equation~\eqref{eq:forward-pass} as a \terminate{Dirac delta function}, and finally \emph{(iii)} recall the integral representation of the \terminate{Dirac delta function} \eqref{eq:integral-form-delta-function}.
All together, this gives the following set of integrals
\begin{align}
& \int \le[\prod_{i} \frac{d b_{i}}{\sqrt{2\pi C_b^{(\ell+1)}}}\ri]  \le[\prod_{i,j} \frac{d W_{ij}}{\sqrt{2\pi C_W^{(\ell+1)}/n_{\ell}}} \ri]\le[\prod_{i,\alpha} \frac{d \HS_{i}^{\ \alpha} \,d\z{i}{\alpha}{\ell+1}}{2\pi}\ri] \mathcal{O}\!\le(z^{(\ell+1)}\ri)\, \\
&\times\exp\!\le[-\sum_{i} \frac{b_i^2}{2C_b^{(\ell+1)}}-\sum_{i,j} \frac{n_{\ell} W_{ij}^2}{2C_W^{(\ell+1)}}+i\sum_{i,\alpha} \HS_{i}^{\ \alpha}\le(\z{i}{\alpha}{\ell+1}-b_i-\sum_{j}W_{ij}\,\x{j}{\alpha}\ri)+\sum_{i,j}\JW{ij}W_{ij}\ri]\, ,  \nonumber
\end{align}
which we recognize as the good-old \neo{Hubbard-Stratonovich transformation} that we first encountered in~\S\ref{sec:first-layer-gaussian}.

Next, as we did in~\S\ref{sec:first-layer-gaussian} and in high school, we can \emph{complete the squares}\index{complete the square} with respect to the biases and weights and integrate them out.\index{integrating out} The only substantial deviation here from the presentation in~\S\ref{sec:first-layer-gaussian} is that the source term shifts the linear coupling of 
the weights as
\be
 -iW_{ij}\sum_{\alpha} \HS_{i}^{\ \alpha}\s{j}{\alpha}{\ell}\to -i W_{ij}\le(\sum_{\alpha} \HS_{i}^{\ \alpha}\s{j}{\alpha}{\ell}+i\JW{ij}\ri)\, .
\ee
Performing these Gaussian integrals, we find
\begin{align}
&\int \!\! \le[\prod_{i,\alpha} \frac{d \HS_{i}^{\ \alpha}\,d\z{i}{\alpha}{\ell+1}}{2\pi}\ri]\!\mathcal{O}\!\le(z^{(\ell+1)}\ri)\exp\!\Bigg[\!-\!\sum_{i,\alpha_1,\alpha_2}\HS_{i}^{\ \alpha_1}\HS_{i}^{\ \alpha_2} \le(\frac{C_b^{(\ell+1)}}{2}+\frac{C_W^{(\ell+1)}}{2n_{\ell}}\sum_{j} \s{\alpha_1}{j}{\ell} \s{\alpha_2}{j}{\ell} \ri) \, \notag \\
&\quad \quad \quad \quad \quad \quad \quad \quad + i\sum_{i,\alpha} \HS_{i}^{\ \alpha}\le(\z{i}{\alpha}{\ell+1}-\frac{\CW{\ell+1}}{n_{\ell}}\sum_{j}\JW{ij}\s{j}{\alpha}{\ell}\ri)+\frac{\CW{\ell+1}}{2n_\ell}\sum_{i,j}\JW{ij}^2
\Bigg]\, .
\end{align}
Just as in our previous Hubbard-Stratonoviching~\eqref{eq:to-be-referenced-in-interlayer-part-far-ahead-in-the-future}, the \emph{stochastic metric}~\eqref{eq:general-stochastic-metric},
\be\label{eq:general-stochastic-metric-reprint}
\Ti{\widehat{G}}{\alpha_1 \alpha_2}{\ell+1}\equiv  \Cb{\ell+1}+\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}\, ,
\ee
appears in the quadratic term of the Hubbard-Stratonovich variables $\HS_{i}^{\ \alpha}$, while the linear term is slightly modified by a shifting of the preactivations with the subtraction of the quantity
\be\label{eq:stochastic-mean}
\widehat{\mathcal{M}}_{i;\alpha}\equiv\CW{\ell+1}\le(\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\JW{ij}\s{j}{\alpha}{\ell}\ri)\, .
\ee
Completing the squares\index{complete the square} with the Hubbard-Stratonovich variables and integrating them out\index{integrating out}, we get
\begin{align}
&\frac{\exp\!\le(\frac{\CW{\ell+1}}{2n_\ell}\sum_{i,j}\JW{ij}^2\ri)}{\sqrt{\dete{2\pi \widehat{G}^{(\ell+1)}}^{n_{\ell+1}}}} \int  \le[\prod_{i,\alpha}d\z{i}{\alpha}{\ell+1}\ri] \mathcal{O}\!\le(z^{(\ell+1)}\ri) \, \\
&\qquad\qquad\quad\times\exp\!\le[-\frac{1}{2}\sum_{i}\sum_{\alpha_1,\alpha_2}\SKinv{\alpha_1 \alpha_2}{\ell+1}\le(\z{i}{\alpha_1}{\ell+1}-\widehat{\mathcal{M}}_{i;\alpha_1}\ri)\le(\z{i}{\alpha_2}{\ell+1}-\widehat{\mathcal{M}}_{i;\alpha_2}\ri)\ri]\, .  \nonumber
\end{align}
Ignoring the quadratic source factor $\JW{ij}^2$ outside the integral, this is just a \emph{Gaussian expectation}\index{Gaussian expectation} 
of $\mathcal{O}$ against the $(\ell+1)$-th-layer preactivation distribution
with a mean $\widehat{\mathcal{M}}_{i;\alpha}$ and a variance $\Ti{\widehat{G}}{\alpha_1 \alpha_2}{\ell+1}$. (Make sure you remember our \terminate{general relativity} convention: $\SKinv{\alpha_1 \alpha_2}{\ell+1}$ is the inverse of $\Ti{\widehat{G}}{\alpha_1 \alpha_2}{\ell+1}$.)

Now, let's compensate for this mean by shifting the dummy integration variable as $\z{i}{\alpha}{\ell+1} \to \z{i}{\alpha}{\ell+1} + \widehat{\mathcal{M}}_{i;\alpha_1}$, which yields a compact expression in terms of our \emph{zero-mean} Gaussian expectation notation~\eqref{eq:many-neuron-gaussian-notation}:
\begin{align}\label{eq:compact-stochastic-with-source}
\exp\!\le(\frac{\CW{\ell+1}}{2n_\ell}\sum_{i,j}\JW{ij}^2\ri) \brabra\mathcal{O}\!\le(z^{(\ell+1)}+\widehat{\mathcal{M}}\ri)\ketket_{\widehat{G}^{(\ell+1)}} \, .
\end{align}
Plugging this result~\eqref{eq:compact-stochastic-with-source} back into our \terminate{interlayer correlation}~\eqref{eq:abstract-interlayer-goal} 
and substituting back in for the mean shift~\eqref{eq:stochastic-mean}, we arrive at a simple formula for our \terminate{generating function}:
\begin{align}\label{eq:master-weight-insertion}
&\E{\mathcal{O}\!\le(z^{(\ell+1)}\ri)e^{\sum_{i,j} \JW{ij}W^{(\ell+1)}_{ij}}\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)} \ri)}\, \\
=&\exp\!\le(\frac{\CW{\ell+1}}{2n_\ell}\sum_{i,j}\JW{ij}^2\ri) \E{\bra\!\!\!\bra\mathcal{O}\Big(z^{(\ell+1)}_{i;\alpha}+\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\JW{ij}\s{j}{\alpha}{\ell}\Big)\ket\!\!\!\ket_{\widehat{G}^{(\ell+1)}}\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)}\ri) }\, .\notag 
\end{align}
After performing the \terminate{Gaussian expectation} over the $(\ell+1)$-th-layer preactivations $z^{(\ell+1)}_{i;\alpha}$ -- which is typically trivial in all the concrete applications that we'll encounter -- the expectation in \eqref{eq:master-weight-insertion} is only with respect to $\ell$-th-layer variables.\footnote{Recall that the stochastic metric $\widehat{G}^{(\ell+1)}$~\eqref{eq:general-stochastic-metric-reprint} depends only on the $\ell$-th-layer preactivations.} This was our desired result.

To see how to use the \terminate{generating function}~\eqref{eq:master-weight-insertion}, let's work out some explicit examples. First, consider the case with no weight insertions. Setting the source to zero, $\JW{ij}=0$, we find
\begin{align}\label{eq:no-weight-insertion-general}
&\E{\mathcal{O}\!\le(z^{(\ell+1)}\ri)\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)} \ri)}=\E{\brabra\mathcal{O}\Big(z^{(\ell+1)}\Big)\ketket_{\widehat{G}^{(\ell+1)}}\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)}\ri) }\, .%
\end{align}
This formula is not trivial and is not something we knew before: here we see that the correlation between preactivations in neighboring layers\index{interlayer correlation} is given by first computing a Gaussian expectation\index{Gaussian expectation} of the $(\ell+1)$-th-layer function against the stochastic metric and then taking the full expectation of the resulting $\ell$-th-layer quantity.

Next, let's consider two weight insertions. Twice-differentiating the \terminate{generating function}~\eqref{eq:master-weight-insertion} by the source as $\frac{d}{d\JW{i_3j_3}}\frac{d}{d\JW{i_4j_4}}$ and then setting the source to zero, we get
\begin{align}\label{eq:two-weight-insertions-general}
&\E{\mathcal{O}\!\le(z^{(\ell+1)}\ri)\W{i_3j_3}{\ell+1}\W{i_4j_4}{\ell+1}\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)} \ri)}\, \\
=&\delta_{i_3i_4}\delta_{j_3j_4}\frac{\CW{\ell+1}}{n_{\ell}}\E{\bra\!\bra\mathcal{O}\ket\!\ket_{\widehat{G}^{(\ell+1)}}\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)}\ri) }\, \notag\\
&\quad\quad+\le(\frac{\CW{\ell+1}}{n_{\ell}}\ri)^2\sum_{\beta_3,\beta_4,\gamma_3,\gamma_4}\mathbb{E}\Bigg[\brabra\le(\z{i_3}{\beta_3}{\ell+1}\z{i_4}{\beta_4}{\ell+1}-\delta_{i_3i_4}\Ti{\widehat{G}}{\beta_3\beta_4}{\ell+1}\ri)\mathcal{O}\ketket_{\widehat{G}^{(\ell+1)}}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \times\SKinv{\beta_3\gamma_3}{\ell+1}\SKinv{\beta_4\gamma_4}{\ell+1}\s{j_3}{\gamma_3}{\ell}\s{j_4}{\gamma_4}{\ell}\mathcal{Q}\!\le(z^{(\ell)},  \NTK^{(\ell)}\ri) \Bigg]\, .\notag
\end{align}
Here, we used \terminate{integration by parts} to exchange the derivatives for a projection as
\be
\bra\!\!\!\bra\frac{\partial^2\mathcal{O}}{\partial \z{i_3}{\gamma_3}{\ell+1}\partial \z{i_4}{\gamma_4}{\ell+1}}\ket\!\!\!\ket_{\widehat{G}^{(\ell+1)}}=\sum_{\beta_3,\beta_4}\SKinv{\beta_3\gamma_3}{\ell+1}\SKinv{\beta_4\gamma_4}{\ell+1}\brabra\le(\z{i_3}{\beta_3}{\ell+1}\z{i_4}{\beta_4}{\ell+1}-\delta_{i_3i_4}\Ti{\widehat{G}}{\beta_3\beta_4}{\ell+1}\ri)\mathcal{O}\ketket_{\widehat{G}^{(\ell+1)}}\, .
\ee
Intuitively, the first term in~\eqref{eq:two-weight-insertions-general} comes from forming a Wick contraction\index{Wick contraction} with the two weight insertions, while the second term comes from two pairs of Wick contractions\index{Wick contraction}, each between an inserted weight and a weight hidden inside the $z^{(\ell+1)}$'s in $\mathcal{O}$.

Thusly, with the \emph{intra}layer formula~\eqref{eq:general-covariance-reprint} recalled and the \emph{inter}layer formulae~\eqref{eq:no-weight-insertion-general} and~\eqref{eq:two-weight-insertions-general} derived, we are as ready as we'll ever be to recursively analyze the joint statistics of the NTK and preactivations in deeper layers. This concludes our \emph{inter}lude.




\subsection{NTK Mean}\label{subsec:NTKmean}\index{forward equation!NTK}\index{neural tangent kernel!l-th-layer@$\ell$-th-layer}
Taking the expectation of the stochastic NTK forward equation~\eqref{eq:NTH-recursion-without-expectation}, we get
\begin{align}\label{eq:NTK-mean-pre-recursion}
\E{\Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{\ell+1}}=&\delta_{i_1i_2} \le[\Lb{\ell+1} + \lamW{\ell+1}\le(\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\s{j}{\alpha_1}{\ell} \s{j}{\alpha_2}{\ell}}\ri)\ri]\, \\
&+ \delta_{i_1i_2} \CW{\ell+1} \frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}} \E{ \ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell} \Tia{\NTK}{jj}{\alpha_1\alpha_2}{\ell}}\, ,\nonumber
\end{align}
where, as is now familiar, on the second line we used the independence of the $(\ell+1)$-th-layer weights from the $\ell$-th-layer preactivations, and then immediately evaluated the weight expectation. Immediately, we see that NTK mean is diagonal in \terminate{neural indices} at any network depth.

Given that, let's decompose the $\ell$-th-layer NTK into a mean and fluctuation as\index{neural tangent kernel!mean}
\be\label{eq:NTK-fluc-decomposition}\index{tensor decomposition!NTK mean and fluctuation}
\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{\ell}\equiv \delta_{i_1i_2} \Ti{\NTKM}{\alpha_1\alpha_2}{\ell}+\DNTK{i_1i_2}{\alpha_1\alpha_2}{\ell}\, ,
\ee
where we have denoted the $\ell$-th-layer NTK mean as $ \delta_{i_1i_2} \Ti{\NTKM}{\alpha_1\alpha_2}{\ell}$.
As before, we separated the part of the mean that encodes the sample dependence and symbolized it without a hat. 
Substituting this decomposition into our expression \eqref{eq:NTK-mean-pre-recursion} for the NTK mean, we see that the $(\ell+1)$-th-layer mean obeys a recursion
\begin{align}\label{eq:NTK-mean-recursion-with-E}
\Ti{\NTKM}{\alpha_1\alpha_2}{\ell+1}=&\Lb{\ell+1} + \lamW{\ell+1}\!\le(\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\s{j}{\alpha_1}{\ell} \s{j}{\alpha_2}{\ell}}\ri)+\CW{\ell+1}\Ti{\NTKM}{\alpha_1\alpha_2}{\ell} \!\le(\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}} \E{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}}\ri)\, \nonumber\\
&+\CW{\ell+1} \!\le(\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}} \E{ \ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell} \DNTK{jj}{\alpha_1\alpha_2}{\ell}}\ri)\, ,
\end{align}
depending on both the mean and fluctuation in the previous layer $\ell$.

To the leading order in $1/n$, the first two expectation values on the right-hand side of~\eqref{eq:NTK-mean-recursion-with-E} %
are given by Gaussian expectations\index{Gaussian expectation}
\begin{align}\label{eq:ntk-mean-gaussian-1}
\E{\s{j}{\alpha_1}{\ell} \s{j}{\alpha_2}{\ell}}=&\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(\ell)}}+\o{\frac{1}{n}}\, ,\\
\E{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}}=&\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\ket_{G^{(\ell)}}+\o{\frac{1}{n}}\, .\label{eq:ntk-mean-gaussian-2}
\end{align}
To see this, note that the first expectation is just the leading Gaussian contribution~\eqref{eq:two-activations-deep} with the non-Gaussian coupling $v$ suppressed as $\sim1/n$ as per \eqref{eq:four-point-match-general}, and that the evaluation of the second expectation proceeds identically to the first regardless of whether the activation has a derivative or not. Meanwhile, the final expectation on the second line of \eqref{eq:NTK-mean-recursion-with-E} involves an NTK-preactivation cross correlation, which is also suppressed in the large-width limit:
\be\label{eq:ntk-mean-cross-leading}
 \E{ \ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell} \DNTK{jj}{\alpha_1\alpha_2}{\ell}}=\o{\frac{1}{n}}\, .
\ee
We will prove this rather shortly in the next subsection in~\eqref{eq:most-general}.

Assembling these leading contributions, the NTK mean recursion simplifies to 
\begin{align}\label{eq:NTHmean_final}\index{neural tangent kernel!mean|textbf}
\Ti{\NTKM}{\alpha_1\alpha_2}{\ell+1}=&\Lb{\ell+1} + \lamW{\ell+1}\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(\ell)}}+\CW{\ell+1}\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\ket_{G^{(\ell)}}\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}+\o{\frac{1}{n}}\, .
\end{align}
If you're in the habit of marking up your book, feel free to draw a box around this formula.



\subsection{NTK-Preactivation Cross Correlations}\label{subsec:NTK-cross}\index{cross correlation!NTK-preactivation|textbf}\index{neural tangent kernel!NTK-preactivation cross correlation|see{cross correlation}}
Next, let's evaluate a cross-correlation expectation of a very general form
\be\label{eq:cross-general}
\E{\O\!\le(z^{(\ell+1)}\ri)\,\DNTK{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}\, .
\ee
For instance, setting $\O=\z{i_1}{\alpha_1}{\ell+1}\z{i_2}{\alpha_2}{\ell+1}$ gives the elementary cross correlation~\eqref{eq:cross-general-ell-first-appearance}, while setting $\O=\ds{i_1}{\alpha_1}{\ell} \ds{i_2}{\alpha_2}{\ell}$ gives the subleading cross correlation~\eqref{eq:ntk-mean-cross-leading} that just appeared in (and then immediately disappeared from) our recursion for the NTK mean.\index{neural tangent kernel!mean}




\index{forward equation!NTK}\index{cross correlation!NTK-preactivation}
To begin, simply substitute the NTK forward equation~\eqref{eq:NTH-recursion-without-expectation} into the cross correlator~\eqref{eq:cross-general}, which yields
\begin{align}\label{eq:cross-general-reexpress}
&\E{\O\!\le(z^{(\ell+1)}\ri)\DNTK{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}=\cov{\O\!\le(z^{(\ell+1)}\ri)}{\Tia{\NTK}{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}\, \\
=&\delta_{i_3i_4}\,\lamW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\le\{\E{\O\!\le(z^{(\ell+1)}\ri)\s{j}{\alpha_3}{\ell}\s{j}{\alpha_4}{\ell}}-\E{\O\!\le(z^{(\ell+1)}\ri)}\E{\s{j}{\alpha_3}{\ell}\s{j}{\alpha_4}{\ell}}\ri\}\, \nonumber\\
&+\sum_{j_3,j_4=1}^{n_{\ell}}\Big\{\E{\O\!\le(z^{(\ell+1)}\ri)\W{i_3j_3}{\ell+1}\W{i_4j_4}{\ell+1}\ds{j_3}{\alpha_3}{\ell}\ds{j_4}{\alpha_4}{\ell}\Tia{\NTK}{j_3j_4}{\alpha_3\alpha_4}{\ell}}\, \nonumber\\
&\quad \quad \quad \quad -\E{\O\!\le(z^{(\ell+1)}\ri)}\E{\W{i_3j_3}{\ell+1}\W{i_4j_4}{\ell+1}}\E{\ds{j_3}{\alpha_3}{\ell}\ds{j_4}{\alpha_4}{\ell}\Tia{\NTK}{j_3j_4}{\alpha_3\alpha_4}{\ell}}\Big\}\, .\nonumber
\end{align}
Now putting the freshly-derived interlayer formulae~\eqref{eq:no-weight-insertion-general} and~\eqref{eq:two-weight-insertions-general} to use,
this cross correlator becomes
\begin{align}\label{eq:cross-general-reexpress-II}
&\E{\O\!\le(z^{(\ell+1)}\ri)\DNTK{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}\, \\
=&\delta_{i_3i_4}\frac{\lamW{\ell+1}}{\CW{\ell+1}}\,\E{\brabra\O\!\le(z^{(\ell+1)}\ri)\ketket_{\widehat{G}^{(\ell+1)}} \dKi{\alpha_3\alpha_4}{\ell+1}}\, \nonumber\\
&+\delta_{i_3i_4}\frac{\CW{\ell+1}}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\Big\{\E{\brabra\O\!\le(z^{(\ell+1)}\ri)\ketket_{\widehat{G}^{(\ell+1)}}\ds{j}{\alpha_3}{\ell}\ds{j}{\alpha_4}{\ell}\Tia{\NTK}{jj}{\alpha_3\alpha_4}{\ell}}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \quad \quad -\E{\brabra\O\!\le(z^{(\ell+1)}\ri)\ketket_{\widehat{G}^{(\ell+1)}}}\E{\ds{j}{\alpha_3}{\ell}\ds{j}{\alpha_4}{\ell}\Tia{\NTK}{jj}{\alpha_3\alpha_4}{\ell}}\Big\}\, \nonumber\\
&+\le(\frac{\CW{\ell+1}}{n_{\ell}}\ri)^2\sum_{j_3,j_4=1}^{n_{\ell}}\sum_{\beta_3,\beta_4,\gamma_3,\gamma_4}\mathbb{E}\Bigg[\brabra\le(\z{i_3}{\beta_3}{\ell+1}\z{i_4}{\beta_4}{\ell+1}-\delta_{i_3i_4}\Ti{\widehat{G}}{\beta_3\beta_4}{\ell+1}\ri)\O\!\le(z^{(\ell+1)}\ri)\ketket_{\widehat{G}^{(\ell+1)}}\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \times\SKinv{\beta_3\gamma_3}{\ell+1}\SKinv{\beta_4\gamma_4}{\ell+1}\s{j_3}{\gamma_3}{\ell}\s{j_4}{\gamma_4}{\ell}\ds{j_3}{\alpha_3}{\ell}\ds{j_4}{\alpha_4}{\ell}\Tia{\NTK}{j_3j_4}{\alpha_3\alpha_4}{\ell} \Bigg]\, .\nonumber
\end{align}
Here, for the first term, we also recalled the definition of the metric fluctuation~\eqref{eq:metric-fluctuation-general-layer}.
From this general expression, we can already learn two important lessons.

First, setting $\O=\z{i_1}{\alpha_1}{\ell+1}\z{i_2}{\alpha_2}{\ell+1}$, we get an expression for the  elementary $(\ell+1)$-th-layer cross correlation\index{cross correlation!NTK-preactivation} in terms of $\ell$-th-layer variables %
\begin{align}\label{eq:D-F-decomposition-general-layer}\index{tensor decomposition!NTK-preactivation $D$/$F$}
&\E{\Tia{z}{i_1}{\alpha_1}{\ell+1}\Tia{z}{i_2}{\alpha_2}{\ell+1}\DNTK{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}\, \notag\\
=&\delta_{i_1i_2}\delta_{i_3i_4}\le\{\frac{\lamW{\ell+1}}{\CW{\ell+1}}\E{\dKi{\alpha_1\alpha_2}{\ell+1}\dKi{\alpha_3\alpha_4}{\ell+1}}+\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\dKi{\alpha_1\alpha_2}{\ell+1}\ds{j}{\alpha_3}{\ell}\ds{j}{\alpha_4}{\ell}\Tia{\NTK}{jj}{\alpha_3\alpha_4}{\ell}}\ri\}\, \nonumber\\
&+\delta_{i_1i_3}\delta_{i_2i_4}\le(\frac{\CW{\ell+1}}{n_{\ell}}\ri)^2\sum_{j,k=1}^{n_{\ell}}\mathbb{E}\Bigg[\s{j}{\alpha_1}{\ell}\s{k}{\alpha_2}{\ell}\ds{j}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}\Tia{\NTK}{jk}{\alpha_3\alpha_4}{\ell} \Bigg]\, \nonumber\\
&+\delta_{i_1i_4}\delta_{i_2i_3}\le(\frac{\CW{\ell+1}}{n_{\ell}}\ri)^2\sum_{j,k=1}^{n_{\ell}}\mathbb{E}\Bigg[\s{j}{\alpha_2}{\ell}\s{k}{\alpha_1}{\ell}\ds{j}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}\Tia{\NTK}{jk}{\alpha_3\alpha_4}{\ell} \Bigg]\, \nonumber\\
\equiv&\frac{1}{n_{\ell}}\le[\delta_{i_1i_2}\delta_{i_3i_4} \NTHD{\alpha_1\alpha_2\alpha_3\alpha_4}{\ell+1}+\delta_{i_1i_3}\delta_{i_2i_4}\NTHF{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell+1}+\delta_{i_1i_4}\delta_{i_2i_3}\NTHF{\alpha_1\alpha_4\alpha_2\alpha_3}{\ell+1}\ri]\, ,  
\end{align}
where on the final line we decomposed the cross correlation\index{cross correlation!NTK-preactivation} into two tensors with \terminate{sample indices} only,
just as we did for the second layer in~\eqref{eq:F-and-D-decomposition}.
Equating the first expression with the second, we see that these tensors are defined by the following $\ell$-th-layer expectations:
\begin{align}
\frac{1}{n_{\ell}}\NTHD{\alpha_1\alpha_2\alpha_3\alpha_4}{\ell+1}\equiv&\frac{\lamW{\ell+1}}{\CW{\ell+1}}\,\E{\dKi{\alpha_1\alpha_2}{\ell+1}\dKi{\alpha_3\alpha_4}{\ell+1}}\, \label{eq:D-recursion-almost-there}\\
&+\CW{\ell+1}\le(\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\dKi{\alpha_1\alpha_2}{\ell+1}\ds{j}{\alpha_3}{\ell}\ds{j}{\alpha_4}{\ell}\Tia{\NTK}{jj}{\alpha_3\alpha_4}{\ell}}\ri)\, ,\notag\\
\frac{1}{n_{\ell}}\NTHF{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell+1}\equiv&\le(\CW{\ell+1}\ri)^2\le(\frac{1}{n_{\ell}^2}\sum_{j,k=1}^{n_{\ell}}\mathbb{E}\Bigg[\s{j}{\alpha_1}{\ell}\s{k}{\alpha_2}{\ell}\ds{j}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}\Tia{\NTK}{jk}{\alpha_3\alpha_4}{\ell} \Bigg]\ri)\, .\label{eq:F-recursion-almost-there}
\end{align}
We'll come back to evaluate these last two expressions -- and thereby derive recursions for both cross correlation tensors -- after we reveal the second lesson.

Second, we can start again from the general cross correlator~\eqref{eq:cross-general-reexpress-II} and push our calculation a little bit further to leading order in $1/n$.
The key step is perturbatively expanding the Gaussian expectation -- just as we expanded the stochastic Gaussian distribution~\eqref{eq:second-layer-stochastic-exponential} before using the Schwinger-Dyson equation~\eqref{eq:stochastic-metric-inversion} -- to get
\begin{align}\label{eq:SD-again}
&\brabra\O\!\le(z^{(\ell+1)}\ri)\ketket_{\widehat{G}^{(\ell+1)}}\, \\
=&\brabra\O\!\le(z^{(\ell+1)}\ri)\ketket_{G^{(\ell+1)}}\, \notag\\
&+\frac{1}{2}\!\sum_{\beta_1,\beta_2,\gamma_1,\gamma_2}\!\!\!\bra\!\!\!\bra\sum_{m}\le(z_{m;\beta_1}^{(\ell+1)}z_{m;\beta_2}^{(\ell+1)}-\Ti{G}{\beta_1\beta_2}{\ell+1}\ri)\O\!\le(z^{(\ell+1)}\ri)\ket\!\!\!\ket_{G^{(\ell+1)}}\!\!\!\!\!\!\!\TI{G}{\beta_1\gamma_1}{\ell+1}\TI{G}{\beta_2\gamma_2}{\ell+1}\Ti{\widehat{\Delta G}}{\gamma_1 \gamma_2}{\ell+1}\!\!\!+\o{\Delta^2}\, .\notag
\end{align}
Plugging this back into~\eqref{eq:cross-general-reexpress-II}, picking up the (leading-order) pieces, and using the definitions~\eqref{eq:D-recursion-almost-there} and~\eqref{eq:F-recursion-almost-there}, we get
\begin{align}\label{eq:most-general}
&\E{\O\!\le(z^{(\ell)}\ri)\DNTK{i_3i_4}{\alpha_3\alpha_4}{\ell}}\, \\
=&\delta_{i_3i_4}\frac{1}{n_{\ell-1}}\le[\frac{1}{2}\!\sum_{\beta_1,\beta_2,\gamma_1,\gamma_2}\!\!\bra\!\!\!\bra\sum_{m=1}^{n_{\ell}}\le(z_{m;\beta_1}^{(\ell)}z_{m;\beta_2}^{(\ell)}-\Ti{G}{\beta_1\beta_2}{\ell}\ri)\O\!\le(z^{(\ell)}\ri)\ket\!\!\!\ket_{G^{(\ell)}}\!\!\!\!\!\!\!\!\!\TI{G}{\beta_1\gamma_1}{\ell}\TI{G}{\beta_2\gamma_2}{\ell}\ri]\NTHD{\gamma_1\gamma_2\alpha_3\alpha_4}{\ell}\, \notag\\
&\,\,\,+\frac{1}{n_{\ell-1}}\sum_{\beta_1,\beta_2,\gamma_1,\gamma_2}\brabra\le(\z{i_3}{\beta_1}{\ell}\z{i_4}{\beta_2}{\ell}-\delta_{i_3i_4}\Ti{G}{\beta_1\beta_2}{\ell}\ri)\O\!\le(z^{(\ell)}\ri)\ketket_{G^{(\ell)}}\TI{G}{\beta_1\gamma_1}{\ell}\TI{G}{\beta_2\gamma_2}{\ell}\NTHF{\gamma_1\alpha_3\gamma_2\alpha_4}{\ell}\, \notag\\
&\,\,\,+\o{\frac{1}{n^2}}\, \, ,\nonumber
\end{align}
where we have also relabeled \neo{layer indices} as $(\ell+1)\to\ell$ everywhere for the ease of later substitutions.\footnote{You might worry about the summation $\sum_{m=1}^{n_{\ell}}$ inside the first Gaussian expectation in~\eqref{eq:most-general}. However, due to Gaussian factorization, this  expectation stays of order one so long as the observable $\O$ depends on only a finite number of neurons.}
This result illustrates that these more general cross correlations
are governed by the same tensors, $D^{(\ell)}$ and $F^{(\ell)}$, as the elementary cross correlation~\eqref{eq:D-F-decomposition-general-layer}.
We can indeed compute all the cross correlators
if we find and solve recursions for $D^{(\ell)}$ and $F^{(\ell)}$. %
It is this task that we turn to next.





\subsubsection{$D$-recursion}\index{cross correlation!NTK-preactivation!D-recursion@$D$-recursion}
Starting from our expression for $D^{(\ell+1)}$~\eqref{eq:D-recursion-almost-there} and substituting in the definition of the stochastic metric~\eqref{eq:general-stochastic-metric-reprint}, we get
\begin{align}\label{eq:D-all-pieces}
\NTHD{\alpha_1\alpha_2\alpha_3\alpha_4}{\ell+1}=&\CW{\ell+1}\frac{1}{n_{\ell}}\sum_{j,k=1}^{n_{\ell}}\cov{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}{\lamW{\ell+1}\s{k}{\alpha_3}{\ell}\s{k}{\alpha_4}{\ell}+ \CW{\ell+1}\Ti{\NTKM}{\alpha_3\alpha_4}{\ell}\ds{k}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}}\, \nonumber\\
&+\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}}\sum_{j,k=1}^{n_{\ell}}\cov{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}\DNTK{k k}{\alpha_3\alpha_4}{\ell}}+\o{\frac{1}{n}}\, ,
\end{align}
where we have again decomposed the $\ell$-th-layer NTK\index{neural tangent kernel!l-th-layer@$\ell$-th-layer} into a mean and fluctuation piece.
We see that there are two types of terms here: covariances on a single neuron $j=k$ and covariances between pairs of neurons $j\ne k$.

For the single-neuron contribution with $j=k$, at the leading order, we find the same contribution that we found for the second layer~\eqref{eq:D-recursion-second}\footnote{The subleading $\o{1/n}$ piece includes both a contribution from the non-Gaussian part of the distribution as well as a cross correlation contribution from the previous layer.
}
\be\label{eq:D-piece1}
\CW{\ell+1} \le[\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\Oi{\alpha_3\alpha_4}{\ell+1} \ket_{G^{(\ell)}}-\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(\ell)}}\bra\Oi{\alpha_3\alpha_4}{\ell+1} \ket_{G^{(\ell)}}\ri]+\o{\frac{1}{n}}\, .
\ee
Here, the auxiliary stochastic matrix $\Oi{\alpha_1\alpha_2}{\ell+1}$ is defined as
\be\label{eq:def-omega-without-neural}
\Oi{\alpha_1\alpha_2}{\ell+1} \equiv \lamW{\ell+1} \Ti{\sigma}{\alpha_1}{\ell} \Ti{\sigma}{\alpha_2}{\ell} + \CW{\ell+1}\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}  \dTi{\sigma}{\alpha_1}{\ell} \dTi{\sigma}{\alpha_2}{\ell}  \, ,
\ee
which simply generalizes the second-layer definition~\eqref{eq:def-omega-without-neural-second}.
As a reminder the unhatted matrix $\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}$ is the %
NTK mean\index{neural tangent kernel!mean}, which is not a random variable and can safely be taken outside the Gaussian expectation\index{Gaussian expectation} $\bra\cdot\ket_{G^{(\ell)}}$. This means that, as a stochastic variable, $\Oi{\alpha_1\alpha_2}{\ell+1}$ depends only on the $\ell$-th-layer preactivations.


Next, for the pairs-of-neurons contribution to~\eqref{eq:D-all-pieces} with $j\ne k$, the first term can be evaluated by the intralayer formula~\eqref{eq:general-covariance-reprint} and yields
\be
\frac{n_{\ell}}{4n_{\ell-1}} \CW{\ell+1} \!\!\!\!\!\sum_{\gamma_1, \gamma_2, \gamma_3, \gamma_4} \!\!\!\!\!
\VU{\gamma_1 \gamma_2}{\gamma_3 \gamma_4}{\ell}\!
\bra\le(\!z_{\gamma_1} z_{\gamma_2} \!-\!\Ti{G}{\gamma_1 \gamma_2}{\ell} \!\ri)\sigma_{\alpha_1}\sigma_{\alpha_2}\!\ket_{G^{(\ell)}}\!\bra\le(\!z_{\gamma_3} z_{\gamma_4} \!-\!\Ti{G}{\gamma_3 \gamma_4}{\ell} \!\ri)\Oi{\alpha_3\alpha_4}{\ell+1}\!\ket_{G^{(\ell)}}\, .
\ee
Meanwhile, the covariance in the second term can be unrolled as
\begin{align}\label{eq:how-to-unroll}
&\cov{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}\DNTK{k k}{\alpha_3\alpha_4}{\ell}}\, \\
=&\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}\ds{k}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}\DNTK{k k}{\alpha_3\alpha_4}{\ell}}-\E{\s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}}\E{\ds{k}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}\DNTK{k k}{\alpha_3\alpha_4}{\ell}}\, \notag\\
=&\frac{1}{2n_{\ell-1}}\!\sum_{\beta_1,\beta_2,\gamma_1,\gamma_2}\!\!\!\!\!\!\bra\le(\!z_{\beta_1}z_{\beta_2}\!-\!\Ti{G}{\beta_1\beta_2}{\ell}\!\ri) \sigma_{\alpha_1}\sigma_{\alpha_2}\!\ket_{G^{(\ell)}}\!\bra \sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(\ell)}}\!\!\TI{G}{\beta_1\gamma_1}{\ell}\TI{G}{\beta_2\gamma_2}{\ell}\NTHD{\gamma_1\gamma_2\alpha_3\alpha_4}{\ell}\!+\o{\frac{1}{n^2}}\,\! ,\notag
\end{align}
where in the last line we used the cross-correlation formula~\eqref{eq:most-general} with the observables
$\O = \s{j}{\alpha_1}{\ell}\s{j}{\alpha_2}{\ell}\ds{k}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}$ and $\O = \ds{k}{\alpha_3}{\ell}\ds{k}{\alpha_4}{\ell}$, respectively.
Combining these contributions with the first piece~\eqref{eq:D-piece1}, we get our desired recursion:
\begin{align}\label{eq:D-recursion}
&\NTHD{\alpha_1\alpha_2\alpha_3\alpha_4}{\ell+1}\, \\
=&\CW{\ell+1} \le(\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\Oi{\alpha_3\alpha_4}{\ell+1} \ket_{G^{(\ell)}}-\bra \sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{G^{(\ell)}}\bra\Oi{\alpha_3\alpha_4}{\ell+1} \ket_{G^{(\ell)}}\ri)\, \nonumber\\
&+\frac{n_{\ell}}{4n_{\ell-1}} \CW{\ell+1} \!\!\!\!\!\sum_{\gamma_1, \gamma_2, \gamma_3, \gamma_4} \!\!\!\!\!
\VU{\gamma_1 \gamma_2}{\gamma_3 \gamma_4}{\ell}\!
\bra\le(\!z_{\gamma_1} z_{\gamma_2} \!-\!\Ti{G}{\gamma_1 \gamma_2}{\ell} \!\ri)\sigma_{\alpha_1}\sigma_{\alpha_2}\!\ket_{G^{(\ell)}}\!\bra\le(\!z_{\gamma_3} z_{\gamma_4} \!-\!\Ti{G}{\gamma_3 \gamma_4}{\ell} \!\ri)\Oi{\alpha_3\alpha_4}{\ell+1}\!\ket_{G^{(\ell)}}\, \nonumber\\
&+\frac{n_{\ell}}{2n_{\ell-1}}\!\le(\CW{\ell+1}\ri)^2\!\!\!\!\!\sum_{\beta_1,\beta_2,\gamma_1,\gamma_2}\!\!\!\!\!\NTHD{\gamma_1\gamma_2\alpha_3\alpha_4}{\ell}\!\bra\le(\!z_{\beta_1}z_{\beta_2}\!-\!\Ti{G}{\beta_1\beta_2}{\ell}\!\ri) \sigma_{\alpha_1}\sigma_{\alpha_2}\!\ket_{G^{(\ell)}}\!\TI{G}{\beta_1\gamma_1}{\ell}\TI{G}{\beta_2\gamma_2}{\ell}\!\bra \sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(\ell)}}\, \notag \\ &+\o{\frac{1}{n}}\, .\nonumber
\end{align}
Interestingly, we see that at leading order the $D$-type cross correlation\index{cross correlation!NTK-preactivation} in layer $(\ell+1)$ mixes $D$-type correlations from layer $\ell$ with the \terminate{four-point vertex} $V^{(\ell)}$, but does not mix with the $F$-type cross correlations\index{cross correlation!NTK-preactivation} or any part of the NTK variance\index{neural tangent kernel!variance}.


\subsubsection{$F$-recursion}\index{cross correlation!NTK-preactivation!F-recursion@$F$-recursion}
Starting from our expression for $F^{(\ell+1)}$~\eqref{eq:F-recursion-almost-there} and decomposing the NTK into a mean and fluctuation, we get
\begin{align}
\NTHF{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell+1}=&\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\s{j}{\alpha_1}{\ell}\ds{j}{\alpha_3}{\ell}\s{j}{\alpha_2}{\ell}\ds{j}{\alpha_4}{\ell}}\Ti{\NTKM}{\alpha_3\alpha_4}{\ell}\, \\
&+\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}}\sum_{j,k=1}^{n_{\ell}}\E{\s{j}{\alpha_1}{\ell}\ds{j}{\alpha_3}{\ell}\s{k}{\alpha_2}{\ell}\ds{k}{\alpha_4}{\ell}\DNTK{jk}{\alpha_3\alpha_4}{\ell}}\, .\nonumber
\end{align}
At leading order, the first term simply becomes a single-neuron Gaussian expectation\index{Gaussian expectation}; the second term can be evaluated with the cross-correlation formula~\eqref{eq:most-general}, where the diagonal sum with $j=k$ is of order $\o{1/n}$ and can be neglected while the off-diagonal sum with $j\ne k$ yields the term involving $F^{(\ell)}$. All together, this gives
\begin{align}\label{eq:F-recursion}
\NTHF{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell+1}
=&\le(\CW{\ell+1}\ri)^2\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(\ell)}}\Ti{\NTKM}{\alpha_3\alpha_4}{\ell}\, \\
&+\frac{n_{\ell}}{n_{\ell-1}}\le(\CW{\ell+1}\ri)^2\!\!\!\!\!\sum_{\beta_1,\beta_2,\gamma_1,\gamma_2}\!\!\!\!\!\bra \sigma_{\alpha_1}\sigma^{\prime}_{\alpha_3}z_{\beta_1}\ket_{G^{(\ell)}}\bra \sigma_{\alpha_2}\sigma^{\prime}_{\alpha_4}z_{\beta_2}\ket_{G^{(\ell)}}\TI{G}{\beta_1\gamma_1}{\ell}\TI{G}{\beta_2\gamma_2}{\ell}\,\NTHF{\gamma_1\alpha_3\gamma_2\alpha_4}{\ell}\, \nonumber\\
&+\o{\frac{1}{n}}\, . \notag
\end{align}
As with the $D$-recursion before, the first term was present in the second-layer $F^{(2)}$ \eqref{eq:F-recursion-second}, while the second term is a direct consequence of having a fluctuating NTK in the previous layer $\ell$. Additionally, we see that at leading order the $F$-type cross correlation\index{cross correlation!NTK-preactivation} doesn't mix at all with any of our other finite-width tensors.


Finally, before moving on to discuss the NTK variance, let us note that the recursions for both $\NTHDwo{\ell}$ and $\NTHFwo{\ell}$ -- combined with
the initial condition $\NTHDwo{1}=\NTHFwo{1}=0$ from the first layer where the NTK is deterministic -- ensure that they each stay of order one. Given the factor of $1/n_\ell$ in the decomposition of the cross correlation\index{cross correlation!NTK-preactivation} into these tensors \eqref{eq:D-F-decomposition-general-layer} and our ``second lesson'' encapsulated by the cross-correlation formula~\eqref{eq:most-general}, this means that any and all cross correlations\index{cross correlation!NTK-preactivation} are suppressed in the \terminate{$1/n$ expansion} and vanish identically in the strict \terminate{infinite-width limit}.




\subsection{NTK Variance}\label{subsec:NTHvariance}
Now let's finally \emph{slay the beast}\index{slay the beast (NTK variance)} that is the NTK variance\index{neural tangent kernel!variance|textbf}.
Similar to the NTK-preactivation cross correlation\index{cross correlation!NTK-preactivation}, the NTK-variance calculation in deeper layers differs from the second-layer calculation due to nontrivial intralayer correlations~\eqref{eq:general-covariance-reprint} in the previous layer
and due to the fluctuating NTK~\eqref{eq:NTK-fluc-decomposition}.

The NTK variance is given by the expected magnitude of the NTK fluctuation
\begin{align}\label{eq:NTHvR_primitive_before}
&\E{\DNTK{i_1i_2}{\alpha_1\alpha_2}{\ell+1}\DNTK{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}=\cov{\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{\ell+1}}{\Tia{\NTK}{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}\, .
\end{align}
To begin our calculation, let us plug the NTK forward equation~\eqref{eq:NTH-recursion-without-expectation} into this defining expression %
and then integrate out\index{integrating out} the weights $W^{(\ell+1)}$, which is easy since they are independent random variables. Although there are many terms, the algebra is mostly straightforward:
\begin{align}\label{eq:NTHvR_primitive}
&\E{\DNTK{i_1i_2}{\alpha_1\alpha_2}{\ell+1}\DNTK{i_3i_4}{\alpha_3\alpha_4}{\ell+1}}\\
=&\delta_{i_1i_2}\delta_{i_3i_4}\frac{1}{n_{\ell}^2}\sum_{j,k=1}^{n_{\ell}}\Bigg\{\le(\lamW{\ell+1}\ri)^2\cov{\s{j}{\alpha_1}{\ell} \s{j}{\alpha_2}{\ell}}{\s{k}{\alpha_3}{\ell} \s{k}{\alpha_4}{\ell}}  \notag \\
&\quad \quad \quad \quad \quad \quad +\le(\lamW{\ell+1}\ri)\CW{\ell+1}\cov{\s{j}{\alpha}{\ell} \s{j}{\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \Tia{\NTH}{kk}{\alpha_3\alpha_4}{\ell} } \nonumber\\
&\quad \quad \quad \quad \quad \quad +\le(\lamW{\ell+1}\ri)\CW{\ell+1}\cov{\ds{j}{\alpha}{\ell} \ds{j}{\alpha_2}{\ell}\Tia{\NTH}{jj}{\alpha\alpha_2}{\ell}}{\s{k}{\alpha_3}{\ell} \s{k}{\alpha_4}{\ell}  }\nonumber\\
&\quad \quad \quad \quad \quad \quad +\le(\CW{\ell+1}\ri)^2\cov{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}\Tia{\NTH}{jj}{\alpha_1\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \Tia{\NTH}{kk}{\alpha_3\alpha_4}{\ell} }\Bigg\}\nonumber\\
&+\delta_{i_1i_3}\delta_{i_2 i_4}\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}^2}\sum_{j,k=1}^{n_{\ell}}\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell}\Tia{\NTH}{jk}{\alpha_1\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \Tia{\NTH}{jk}{\alpha_3\alpha_4}{\ell} }\nonumber\\
&+\delta_{i_1i_4}\delta_{i_2 i_3}\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}^2}\sum_{j,k=1}^{n_{\ell}}\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell}\Tia{\NTH}{jk}{\alpha_1\alpha_2}{\ell} \ds{k}{\alpha_3}{\ell} \ds{j}{\alpha_4}{\ell} \Tia{\NTH}{kj}{\alpha_3\alpha_4}{\ell} }\, .\nonumber
\end{align}
In obtaining these last three terms,
you should have made three distinct pairings for the two pairs of Wick contractions\index{Wick contraction} of the four $W^{(\ell+1)}$'s, one paring %
within the same NTK and two pairings 
across the NTKs.


An inspection of the pattern of \terminate{neural indices} in the \terminate{Kronecker delta}s from~\eqref{eq:NTHvR_primitive} suggests that we should again decompose the NTK variance\index{neural tangent kernel!variance} into two tensors as
\begin{align}\label{eq:NTH-variance-decomposition}\index{tensor decomposition!NTK variance $A$/$B$}
&\E{\DNTK{i_1i_2}{\alpha_1\alpha_2}{\ell}\DNTK{i_3i_4}{\alpha_3\alpha_4}{\ell}}\, \\
\equiv&\frac{1}{n_{\ell-1}}\le[\delta_{i_1i_2}\delta_{i_3i_4} \NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{\ell} + \delta_{i_1i_3}\delta_{i_2i_4} \NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell} + \delta_{i_1i_4}\delta_{i_2i_3} \NTHB{\alpha_1\alpha_4\alpha_2\alpha_3}{\ell} \ri] \, ,\notag
\end{align}
just as we did for the second layer before in~\eqref{eq:NTH-variance-decomposition-second}.
Here, a factor of $1/n_{\ell-1}$ was pulled out in anticipation that the overall variance will be $O(1/n)$ just as it was for the second layer.
For now you can think of this parameterization as an ansatz; we will soon recursively show that $\NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{\ell}$ and $\NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell}$ stay of order one as the network width increases.%

Now, let's work out the layer recursions for $\NTHAwo{\ell}$ and $\NTHBwo{\ell}$.











\subsubsection{$B$-recursion}\index{neural tangent kernel!variance!B-recursion@$B$-recursion}
We'll start with $B$-recursion because it's simpler.
Considering 
\eqref{eq:NTHvR_primitive} with the decomposition~\eqref{eq:NTH-variance-decomposition} in mind, we see that $\NTHBwo{\ell+1}$ is given by the following $\ell$-th-layer expectation:
\be\label{eq:Bstart}
\NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell+1}=\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}}\sum_{j,k=1}^{n_{\ell}}\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell}\Tia{\NTH}{jk}{\alpha_1\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \Tia{\NTH}{jk}{\alpha_3\alpha_4}{\ell} }\, .
\ee
As should now be familiar, the double summation in~\eqref{eq:Bstart} splits into two types of terms, diagonal ones with $j=k$ and off-diagonal ones with $j\ne k$.

\index{neural tangent kernel!mean}
For the diagonal part, the leading contribution is from the NTK mean 
\begin{align}\label{eq:NTHvariance_piece1}
&\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{j}{\alpha_4}{\ell} }\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}\Ti{\NTKM}{\alpha_3\alpha_4}{\ell} +\o{\frac{1}{n}}\, \\
=&\le(\CW{\ell+1}\ri)^2\bra\sigma^{\prime}_{\alpha_1} \sigma^{\prime}_{\alpha_2}\sigma^{\prime}_{\alpha_3} \sigma^{\prime}_{\alpha_4}  \ket_{G^{(\ell)}}\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}\Ti{\NTKM}{\alpha_3\alpha_4}{\ell} +\o{\frac{1}{n}}\, ,\nonumber
\end{align}
which is analogous to what we found in the second layer~\eqref{eq:B-recursion-second}.\footnote{Again, the subleading $\o{1/n}$ piece includes both a contribution from the non-Gaussian distribution as well as a cross correlation contribution from the previous layer.
}



\index{neural tangent kernel!mean}\index{neural tangent kernel!variance}
For the off-diagonal part of \eqref{eq:Bstart}, the NTK mean vanishes and the leading contribution is from the NTK fluctuation
\begin{align}\label{eq:NTHvariance_piece2_first_line}
&\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}}\sum_{\substack{j,k=1\\j\ne k}}^{n_{\ell}}\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}\DNTK{jk}{\alpha_1\alpha_2}{\ell}\DNTK{jk}{\alpha_3\alpha_4}{\ell} }\, .
\end{align}
The expectation already is $\o{\Delta^2}$ from the two NTK fluctuations inside it and thus, neglecting higher-order correlations of order $\o{\Delta^3}$, we have
\begin{align}\label{eq:the-most-subtle}
&\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}\DNTK{jk}{\alpha_1\alpha_2}{\ell}\DNTK{jk}{\alpha_3\alpha_4}{\ell} }\, \\
=&\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}}\E{\DNTK{jk}{\alpha_1\alpha_2}{\ell}\DNTK{jk}{\alpha_3\alpha_4}{\ell} }+\o{\frac{1}{n^2}}\, ,\notag
\end{align}
where the detailed explanation for such a factorization is given in this footnote.\footnote{In greater detail, you can think of what we are doing here as separating $\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}$ into a mean $\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}}$ and fluctuation 
and -- since the expectation already contains two fluctuations $\DNTK{jk}{\alpha_1\alpha_2}{\ell}\DNTK{jk}{\alpha_3\alpha_4}{\ell}$ -- the latter fluctuating piece contributes $\o{\Delta^3}$ and thus can be neglected.

In alternate detail, we can view this expectation~\eqref{eq:the-most-subtle} as a correlator of three random variables $\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}$, $\DNTK{jk}{\alpha_1\alpha_2}{\ell}$, and $\DNTK{jk}{\alpha_3\alpha_4}{\ell}$, and decompose it into one-point and two-point correlators as
\begin{align}\label{eq:correlator-paradise}
&\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}\DNTK{jk}{\alpha_1\alpha_2}{\ell}\DNTK{jk}{\alpha_3\alpha_4}{\ell} }  \,   \\
=&\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}} \E{\DNTK{jk}{\alpha_1\alpha_2}{\ell}}\E{\DNTK{jk}{\alpha_3\alpha_4}{\ell} } \, \notag\\
&+\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}} \E{\DNTK{jk}{\alpha_1\alpha_2}{\ell}\DNTK{jk}{\alpha_3\alpha_4}{\ell} } \, \notag  \\
&+\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \DNTK{jk}{\alpha_1\alpha_2}{\ell}}\E{\DNTK{jk}{\alpha_3\alpha_4}{\ell} }
 \, \notag \\
&+\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \DNTK{jk}{\alpha_3\alpha_4}{\ell}}\E{\DNTK{jk}{\alpha_1\alpha_2}{\ell} } + \o{\frac{1}{n^2}}\, \notag ,
\end{align}
where the $\o{1/n^2}$ part contains the connected piece of the decomposition.
Since the NTK fluctuation has mean zero, only the second term survives at this order.}
Then, using the decomposition~\eqref{eq:NTH-variance-decomposition} for the NTK variance\index{neural tangent kernel!variance} and similar logic as \eqref{eq:four-activations-different-deep-connected} to evaluate the four-point correlator of off-diagonal activations, we get
\begin{align}\label{eq:NTHvariance_piece2}
&\le(\CW{\ell+1}\ri)^2\frac{1}{n_{\ell}}\sum_{\substack{j,k=1\\j\ne k}}^{n_{\ell}}\E{\ds{j}{\alpha_1}{\ell} \ds{k}{\alpha_2}{\ell} \ds{j}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}}\E{\DNTK{jk}{\alpha_1\alpha_2}{\ell}\DNTK{jk}{\alpha_3\alpha_4}{\ell} }+\o{\frac{1}{n}}\, \\
=&\le(\CW{\ell+1}\ri)^2\bra\sigma^{\prime}_{\alpha_1} \sigma^{\prime}_{\alpha_3} \ket_{G^{(\ell)}}\bra\sigma^{\prime}_{\alpha_2} \sigma^{\prime}_{\alpha_4}  \ket_{G^{(\ell)}}\frac{n_{\ell}}{n_{\ell-1}}\NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell}+\o{\frac{1}{n}}\, .\nonumber
\end{align}





Substituting both the diagonal contribution~\eqref{eq:NTHvariance_piece1} and off-diagonal contribution~\eqref{eq:NTHvariance_piece2} back into~\eqref{eq:Bstart}, we get the $B$-recursion:
\begin{align}\label{eq:B-recursion}
\NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell+1}&= \le(\CW{\ell+1}\ri)^2\Bigg[\bra \dsNL{\alpha_1} \dsNL{\alpha_2}  \dsNL{\alpha_3} \dsNL{\alpha_4}\ket_{G^{(\ell)}}  \Ti{\NTKM}{\alpha_1\alpha_2}{\ell} \Ti{\NTKM}{\alpha_3\alpha_4}{\ell} \\
&\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  +  \le(\frac{n_\ell}{n_{\ell-1}} \ri) \bra\sigma^{\prime}_{\alpha_1} \sigma^{\prime}_{\alpha_3} \ket_{G^{(\ell)}}\bra\sigma^{\prime}_{\alpha_2} \sigma^{\prime}_{\alpha_4}  \ket_{G^{(\ell)}} \NTHB{\alpha_1\alpha_3\alpha_2\alpha_4}{\ell}\Bigg] +\oninv \, .\nonumber
\end{align}
As promised, we recursively see that $B^{(\ell)}$ is an order-one quantity. Additionally, we note that at leading order this $B$-type NTK variance doesn't mix with any other finite-width tensors.



\subsubsection{$A$-recursion}\index{neural tangent kernel!variance!A-recursion@$A$-recursion}
Let us now determine the $A$-recursion.
Again equating our expression for the NTK variance \eqref{eq:NTHvR_primitive} with the $A/B$-decomposition \eqref{eq:NTH-variance-decomposition}, we see that $A^{(\ell+1)}$ is given by the following $\ell$-th-layer covariances:
\begin{align}\label{eq:A-recursion-mid-point}
\NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{\ell+1}=&\frac{1}{n_{\ell}}\sum_{j,k=1}^{n_{\ell}}\Bigg\{\le(\lamW{\ell+1}\ri)^2\cov{\s{j}{\alpha_1}{\ell} \s{j}{\alpha_2}{\ell}}{\s{k}{\alpha_3}{\ell} \s{k}{\alpha_4}{\ell}}   \\
&\quad \quad \quad \quad +\lamW{\ell+1}\CW{\ell+1}\cov{\s{j}{\alpha}{\ell} \s{j}{\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \Tia{\NTH}{kk}{\alpha_3\alpha_4}{\ell} } \nonumber\\
&\quad \quad \quad \quad +\lamW{\ell+1}\CW{\ell+1}\cov{\ds{j}{\alpha}{\ell} \ds{j}{\alpha_2}{\ell}\Tia{\NTH}{jj}{\alpha\alpha_2}{\ell}}{\s{k}{\alpha_3}{\ell} \s{k}{\alpha_4}{\ell}  }\nonumber\\
&\quad \quad \quad \quad +\le(\CW{\ell+1}\ri)^2\cov{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}\Tia{\NTH}{jj}{\alpha_1\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \Tia{\NTH}{kk}{\alpha_3\alpha_4}{\ell} }\Bigg\}\, .\nonumber
\end{align}
As we've now seen many times previously, our approach will be to divide up the double summation in \eqref{eq:A-recursion-mid-point} into two types of terms, diagonal terms on a single neuron with $j=k$ and off-diagonal terms on pairs of neurons with $j\ne k$.

\index{neural tangent kernel!mean}
As was the case for the $B$-recursion,
the leading contribution from the diagonal part with $j=k$ comes from the NTK mean and matches what we found for the second layer~\eqref{eq:A-recursion-second}
\be\label{eq:NTHA-piece1}
\bra \Oi{\alpha_1\alpha_2}{\ell+1} \Oi{\alpha_3\alpha_4}{\ell+1} \ket_{G^{(\ell)}}-\bra \Oi{\alpha_1\alpha_2}{\ell+1}\ket_{G^{(\ell)}}\bra \Oi{\alpha_3\alpha_4}{\ell+1} \ket_{G^{(\ell)}}+\o{\frac{1}{n}}\, ,
\ee
where the definition of the auxiliary stochastic tensor $\Oi{\alpha_1\alpha_2}{\ell+1}$ was given in~\eqref{eq:def-omega-without-neural}.\footnote{Once again, the subleading $\o{1/n}$ piece includes both a contribution from the non-Gaussian distribution as well as a cross correlation contribution from the previous layer
\emph{and} now also a contribution from the previous layer's NTK variance.}

\index{neural tangent kernel!mean}
This leaves us with the off-diagonal part of~\eqref{eq:A-recursion-mid-point} with $j\ne k$. Here, there will be leading contributions both from the NTK mean and from the NTK fluctuations. The contributions from the mean are given by
replacing $\Tia{\NTH}{i_1i_2}{\alpha_1\alpha_2}{\ell} \to \delta_{i_1i_2}\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}$:
\begin{align}\label{eq:A-recursion-mean}
&\frac{1}{n_{\ell}}\sum_{\substack{ j,k=1\\j\ne k}}^{n_{\ell}}\Bigg\{\le(\lamW{\ell+1}\ri)^2\cov{\s{j}{\alpha_1}{\ell} \s{j}{\alpha_2}{\ell}}{\s{k}{\alpha_3}{\ell} \s{k}{\alpha_4}{\ell}}   \\
&\quad \quad \quad \quad +\lamW{\ell+1}\CW{\ell+1}\Ti{\NTKM}{\alpha_3\alpha_4}{\ell}\cov{\s{j}{\alpha}{\ell} \s{j}{\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} } \nonumber\\
&\quad \quad \quad \quad +\lamW{\ell+1}\CW{\ell+1}\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}\cov{\ds{j}{\alpha}{\ell} \ds{j}{\alpha_2}{\ell}}{\s{k}{\alpha_3}{\ell} \s{k}{\alpha_4}{\ell}  }\nonumber\\
&\quad \quad \quad \quad +\le(\CW{\ell+1}\ri)^2\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}\Ti{\NTKM}{\alpha_3\alpha_4}{\ell}\cov{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}}\Bigg\} .\nonumber
\end{align}
All four of these covariances can be evaluated using the intralayer formula~\eqref{eq:general-covariance-reprint}. After a little bit of algebra, this gives 
\begin{align}\label{eq:NTHA-piece2}%
&\frac{n_{\ell}}{4n_{\ell-1}}  \sum_{\gamma_1, \gamma_2, \gamma_3, \gamma_4} \!\!\!\!
\VU{\gamma_1 \gamma_2}{\gamma_3 \gamma_4}{\ell}
\bra\le(z_{\gamma_1} z_{\gamma_2} -\Ti{G}{\gamma_1 \gamma_2}{\ell} \ri)\Oi{\alpha_1\alpha_2}{\ell+1}\ket_{G^{(\ell)}}
\bra\le(z_{\gamma_3} z_{\gamma_4} -\Ti{G}{\gamma_3 \gamma_4}{\ell} \ri)\Oi{\alpha_3\alpha_4}{\ell+1}\ket_{G^{(\ell)}}\, 
\end{align}
at leading order, where we again made use of the definition of $\Oi{\alpha_1\alpha_2}{\ell+1}$~\eqref{eq:def-omega-without-neural}.

Finally, we're left with the off-diagonal contributions from the NTK fluctuations, which -- if we write them out in excruciating detail -- are given by
\begin{align}\label{eq:A-recursion-rest}
&\frac{1}{n_{\ell}}\sum_{\substack{ j,k=1\\j\ne k}}^{n_{\ell}}\Bigg\{\le(\lamW{\ell+1}\ri)\CW{\ell+1}\cov{\s{j}{\alpha_1}{\ell} \s{j}{\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \DNTK{kk}{\alpha_3\alpha_4}{\ell} }\, \\
&\quad \quad \quad \quad +\le(\lamW{\ell+1}\ri)\CW{\ell+1}\cov{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}\DNTK{jj}{\alpha_1\alpha_2}{\ell}}{\s{k}{\alpha_3}{\ell} \s{k}{\alpha_4}{\ell}  }\, \nonumber\\
&\quad \quad \quad \quad +\le(\CW{\ell+1}\ri)^2\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}\cov{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \DNTK{kk}{\alpha_3\alpha_4}{\ell} }\, \nonumber\\
&\quad \quad \quad \quad +\le(\CW{\ell+1}\ri)^2\Ti{\NTKM}{\alpha_3\alpha_4}{\ell}\cov{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}\DNTK{jj}{\alpha_1\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell}}\, \nonumber\\
&\quad \quad \quad \quad +\le(\CW{\ell+1}\ri)^2\cov{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}\DNTK{jj}{\alpha_1\alpha_2}{\ell}}{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \DNTK{kk}{\alpha_3\alpha_4}{\ell} }\Bigg\}\nonumber \, .
\end{align}
The last term involving the two NTK fluctuations can be evaluated similarly to how we evaluated such a term for the $B$-recursion in~\eqref{eq:the-most-subtle},\footnote{The only difference between this and the $B$-version before~\eqref{eq:the-most-subtle} is that here, since we're evaluating a covariance, the term
\be
\E{\ds{j}{\alpha_1}{\ell} \ds{j}{\alpha_2}{\ell}\DNTK{jj}{\alpha_1\alpha_2}{\ell}}\E{\ds{k}{\alpha_3}{\ell} \ds{k}{\alpha_4}{\ell} \DNTK{kk}{\alpha_3\alpha_4}{\ell}}
\ee
is being subtracted. However, this term is of order $\o{1/n^2}$ and can thus be neglected.}
here giving
\be\label{eq:NTHA-piece3}
\le(\CW{\ell+1}\ri)^2\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\ket_{G^{(\ell)}}\bra\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(\ell)}}\frac{n_{\ell}}{n_{\ell-1}}\NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{\ell}+\o{\frac{1}{n}}\, .
\ee
The remaining four covariances in \eqref{eq:A-recursion-rest} with only a single NTK fluctuation 
are identical in structure to~\eqref{eq:how-to-unroll}, letting us leave the details of this to you and your roll of parchment.




At this point, let's review all the components of our expression 
for $A^{(\ell+1)}$: we have the diagonal contribution~\eqref{eq:NTHA-piece1}; and off-diagonal contributions from the NTK mean~\eqref{eq:NTHA-piece2}, from the covariance of two NTK fluctuations~\eqref{eq:NTHA-piece3}, and from the four covariances
on your parchment. 
Assembling these components, we get the $A$-recursion:
\begin{align}\label{eq:A-recursion}
&\NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{\ell+1}\, \\
=&\bra \Oi{\alpha_1\alpha_2}{\ell+1} \Oi{\alpha_3\alpha_4}{\ell+1} \ket_{G^{(\ell)}}-\bra \Oi{\alpha_1\alpha_2}{\ell+1}\ket_{G^{(\ell)}}\bra \Oi{\alpha_3\alpha_4}{\ell+1} \ket_{G^{(\ell)}}\, \nonumber\\
&+\frac{n_{\ell}}{4n_{\ell-1}} \! \sum_{\gamma_1, \gamma_2, \gamma_3, \gamma_4} \!\!\!
\VU{\gamma_1 \gamma_2}{\gamma_3 \gamma_4}{\ell}
\bra\Oi{\alpha_1\alpha_2}{\ell+1}\le(z_{\gamma_1} z_{\gamma_2} -\Ti{G}{\gamma_1 \gamma_2}{\ell} \ri)\ket_{G^{(\ell)}}\bra\Oi{\alpha_3\alpha_4}{\ell+1} \le(z_{\gamma_3} z_{\gamma_4} -\Ti{G}{\gamma_3 \gamma_4}{\ell} \ri)\ket_{G^{(\ell)}}\, \nonumber\\
&+\frac{n_{\ell}}{n_{\ell-1}}\le(\CW{\ell+1}\ri)^2\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\ket_{G^{(\ell)}}\bra\sigma^{\prime}_{\alpha_3}\sigma^{\prime}_{\alpha_4}\ket_{G^{(\ell)}}\NTHA{\alpha_1\alpha_2}{\alpha_3\alpha_4}{\ell}\, \, \nonumber\\
&+\frac{n_{\ell}}{n_{\ell-1}}\frac{\CW{\ell+1}}{2}\!\!\!\sum_{\beta_1,\beta_2,\gamma_1,\gamma_2}\!\!\!\!\!\Big[\bra \Oi{\alpha_1\alpha_2}{\ell+1}\le(z_{\beta_1}z_{\beta_2}-\Ti{G}{\beta_1\beta_2}{\ell}\ri)\ket_{G^{(\ell)}}\TI{G}{\beta_1\gamma_1}{\ell}\TI{G}{\beta_2\gamma_2}{\ell}\NTHD{\gamma_1\gamma_2\alpha_3\alpha_4}{\ell}\bra \dsNL{\alpha_3} \dsNL{\alpha_4}\ket_{G^{(\ell)}}\, \nonumber\\
&\quad \quad \quad \quad  \quad \quad  \quad \quad +\bra \Oi{\alpha_3\alpha_4}{\ell+1}\le(z_{\beta_1}z_{\beta_2}-\Ti{G}{\beta_1\beta_2}{\ell}\ri)\ket_{G^{(\ell)}}\TI{G}{\beta_1\gamma_1}{\ell}\TI{G}{\beta_2\gamma_2}{\ell}\NTHD{\gamma_1\gamma_2\alpha_1\alpha_2}{\ell}\bra \dsNL{\alpha_1} \dsNL{\alpha_2}\ket_{G^{(\ell)}}\Big]\, \nonumber\\
&+\o{\frac{1}{n}}\, .\nonumber
\end{align}
As promised, we recursively see that $A^{(\ell)}$ is an order-one quantity.
Additionally, we note with interest that at leading order this $A$-type contribution to the NTK variance at layer $(\ell+1)$ mixes with the \terminate{four-point vertex} $V^{(\ell)}$ and the cross correlation\index{cross correlation!NTK-preactivation} $D^{(\ell)}$ in layer $\ell$, though not with $B^{(\ell)}$ or $F^{(\ell)}$.\index{$1/n$ expansion}


This completes our analysis of all the finite-width effects for the NTK-preactivation joint distribution; both the leading NTK-preactivation cross correlations\index{cross correlation!NTK-preactivation} and the NTK variance\index{neural tangent kernel!variance} scale as $\sim 1/n$ in the large-width expansion and vanish in the \terminate{infinite-width limit}.\footnote{
    Recalling our discussion from footnote~\ref{footnote:self-average} in \S\ref{sec:MLP_distribution}, this means that the NTK \emph{self-averages}\index{self-averaging} in the strict \terminate{infinite-width limit}; in this limit, the particular value of the NTK in any instantiation of the network parameters is fixed and equal to the ensemble mean.\index{neural tangent kernel!mean}\index{neural tangent kernel!variance}
} 
These quantities are sufficient to fully characterize the leading finite-width effects of gradient-based learning.








