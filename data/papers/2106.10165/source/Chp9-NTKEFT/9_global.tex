
\chapter{Effective Theory of the NTK at Initialization}\label{ch:eft-ntk}%

\epigraph{
In short, we believe that we have answered Minsky and Papert's challenge and \emph{have} found a learning result sufficiently powerful to demonstrate that their pessimism about learning in multilayer machines was misplaced.
}{Rumelhart, Hinton, and Williams \cite{rumelhart1985learning}, acausally
rising to meet the criticism
from \S\ref{ch:eft-mlp}.
\index{Rumelhart, David Everett}\index{Hinton, Geoffrey Everest}\index{Williams, Ronald J.}\index{multilayer perceptron}\index{Minsky, Marvin}\index{Papert, Seymour} }



\noindent{}Since the last chapter was a tempest of equations, algebra, and integration,
let's take some moments\index{moment}
to 
value 
our expectations.\index{expectation value}

\index{connected correlator!}
Our goal in \S\ref{ch:NTKa} was to determine the NTK-preactivation joint distribution  for a given layer $\ell$ at initialization: $p\!\le(z^{(\ell)}, \NTK^{(\ell)}\Big\vert\D\ri)$. The data-dependent couplings\index{data-dependent coupling} and the connected correlators of this distribution \emph{run}\index{running coupling} with depth according to the recursions that we just laboriously derived --~\eqref{eq:NTHmean_final} for the NTK mean\index{neural tangent kernel!mean},~\eqref{eq:D-recursion} and~\eqref{eq:F-recursion} for the NTK-preactivation cross correlations\index{cross correlation!NTK-preactivation}, and~\eqref{eq:B-recursion} and~\eqref{eq:A-recursion} for the NTK variance\index{neural tangent kernel!variance} -- in addition to the recursions derived in~\S\ref{ch:ngp} 
for the \terminate{kernel} \eqref{eq:K-recursion-reprint} and for the \terminate{four-point vertex} \eqref{eq:V-recursion-reprint}.
This \emph{RG-flow}\index{representation group flow} analysis taught us that the NTK is a deterministic object in the first layer (\S\ref{sec:first-layer-deterministic-NTK}),
stochastically fluctuates and cross-correlates in the second layer (\S\ref{sec:second-layer-fluctuating-NTK}),
and then further accumulates fluctuations and cross correlations in deeper layers (\S\ref{sec:deeper-layer-accumulation-NTK}).




Now that we've thoroughly discussed the math,
in this chapter we'll finally be able to consider the physics of this joint distribution.
Building on our discussion of \neo{criticality} and \neo{universality} in~\S\ref{ch:eft-mlp}, we'll first lay the groundwork for a similar analysis of the NTK while highlighting the relevant results from the last chapter (\S\ref{sec:ntk_criticality}).
In particular, our focus will be on understanding how the \terminate{initialization hyperparameters} and the \terminate{training hyperparameters} affect \terminate{gradient descent} at finite width.
We'll once again find that the depth-to-width ratio $L/n$ plays a starring role in controlling finite-width effects, first for the scale-invariant universality class\index{universality class!scale-invariant} (\S\ref{sec:ntk_criticality_scale_invariant}) and then for the $\ker^\star=0$ universality class\index{universality class!K@$K^\star=0$} (\S\ref{sec:ntk_criticality_tanh_univ}).
For both cases, the growing importance of NTK fluctuations and cross correlations\index{cross correlation!NTK-preactivation} with depth makes the finite-width interaction \emph{relevant}\index{relevant (RG flow)} under RG flow of the NTK.\index{representation group flow}\index{neural tangent kernel}

Finally, we'll introduce the infamous \neo{exploding and vanishing gradient problem} of \terminate{deep learning} and see how our notion of \terminate{criticality} completely mitigates this problem (\S\ref{sec:EVGP-WEP}). In this context, we also explain how the bias and weight learning rates should each be scaled with the network depth.


\section{Criticality Analysis of the NTK}\label{sec:ntk_criticality}
Let's set the stage for our \terminate{criticality} analysis.
As we did in our discussion of preactivation criticality in~\S\ref{ch:eft-mlp}, throughout this section we'll set the bias variance $C_b^{(\ell)}$ and the rescaled weight variance $C_W^{(\ell)}$ to be uniform across layers
\be\label{eq:ntk-chapter-initialziation-hyperparameters-dropped-layer-dependence}
C_b^{(\ell)}=C_b\, , \qquad C_W^{(\ell)}=C_W\, .
\ee
Further paralleling 
\S\ref{sec:signal_prop_finite_width}, we will consider MLPs with uniform hidden layer widths
\be\label{eq:ntk-chapter-layer-widths-equalized}
n_1=\ldots=n_{L-1}\equiv n\, ,
\ee
which is a sensible choice in practice
as well as notationally simplifying.

For the \terminate{training hyperparameters}, however, we'll preserve the layer dependence of the bias learning rate $\Lb{\ell}$ and weight learning rate $\LW{\ell}$ for now, as different universality classes will require different treatments. We'll explore the general principle behind these hyperparameter choices in \S\ref{sec:EVGP-WEP}.

Going forward, we'll only focus on the leading contributions from the \terminate{$1/n$ expansion} to the single-input statistics, neglecting the \terminate{subleading corrections} at next-to-leading-order and 
reserving the multi-input analysis for your private amusement.





\subsubsection{Leading-order NTK recursions for a single input}\label{sec:NTHsummary}\index{neural tangent kernel!mean}
Let's start with the NTK mean recursion. Analogously to all other observables, the \terminate{$1/n$ expansion} induces a series expansion on the NTK mean of the form
\begin{align}\label{eq:NTK-mean-expansion}
\Ti{\NTKM}{\alpha_1\alpha_2}{\ell}=&\NTKM_{\alpha_1\alpha_2}^{\le\{0\ri\}(\ell)}+\frac{1}{n_{\ell-1}}\NTKM_{\alpha_1\alpha_2}^{\le\{1\ri\}(\ell)}+\frac{1}{n_{\ell-1}^2}\NTKM_{\alpha_1\alpha_2}^{\le\{2\ri\}(\ell)}+\o{\frac{1}{n^3}}\, .
\end{align}
Just as we defined the \terminate{kernel} $\Ti{K}{\alpha_1\alpha_2}{\ell}$ as the infinite-width limit of the mean metric $\Ti{G}{\alpha_1\alpha_2}{\ell}$ \eqref{eq:definition-of-kernel-first}, let us give the leading $\o{1}$ piece of the NTK mean a special symbol,
\be\label{eq:frozen-NTK}
\Ti{\NTKI}{\alpha_1\alpha_2}{\ell} \equiv \NTKM_{\alpha_1\alpha_2}^{\le\{0\ri\}(\ell)}\, ,
\ee
and a special name: the \textbf{frozen NTK}\index{frozen NTK|textbf}\index{frozen NTK!infinite-width limit of the NTK}\index{neural tangent kernel!frozen|see{frozen NTK}}. The frozen NTK controls the \terminate{training} dynamics in the \terminate{infinite-width limit}, %
which we will investigate in detail next chapter.\footnote{
    Typically in the literature, the \neo{neural tangent kernel} or NTK refers to this deterministic infinite-width NTK mean $\Ti{\NTKI}{\alpha_1\alpha_2}{\ell}$. Since we are principally concerned with understanding finite-width networks, we instead chose to define and refer to the stochastic object $\Tia{\NTK}{i_1i_2}{\alpha_1\alpha_2}{\ell}$ as the NTK. As a concession to the literature, we've used the customary symbol for the NTK, $\NTKI$, to represent the frozen NTK. (As a helpful mnemonic, note that there is an $\NTKM$ frozen inside the $\NTKI$.) Unfortunately, you'll have to wait until~\S\ref{ch:features} to understand the reason why we call the infinite-width NTK \emph{frozen}; there we'll see how finite-width effects \emph{defrost}\index{defrosted NTK|see{neural tangent kernel}}\index{neural tangent kernel!defrosted} the \terminate{training} process and make the NTK move. Here, in this chapter, you can at least see how it gets \emph{agitated} by finite-width fluctuations.
\index{neural tangent kernel!name}\index{neural tangent kernel!agitated}
}

Now, taking the leading piece of the NTK mean recursion~\eqref{eq:NTHmean_final}, we get a recursion solely for the frozen NTK\index{frozen NTK}
\be\label{eq:frozen-NTK-recursion}
\Ti{\NTKI}{\alpha_1\alpha_2}{\ell+1}=\Lb{\ell+1} + \lamW{\ell+1}\bra\sigma_{\alpha_1}\sigma_{\alpha_2}\ket_{K^{(\ell)}}+C_W\bra\sigma^{\prime}_{\alpha_1}\sigma^{\prime}_{\alpha_2}\ket_{K^{(\ell)}}\Ti{\NTKI}{\alpha_1\alpha_2}{\ell}\, .
\ee
Concurrently, as we are neglecting subleading contributions, we have exchanged the Gaussian expectations\index{Gaussian expectation} over the mean metric $G^{(\ell)}$ for ones over the %
\terminate{kernel} $K^{(\ell)}$. 
Finally specializing to a single input, we simply drop the \terminate{sample indices} to get this recursion's final form,
\be\label{eq:frozen-NTK-recursion-single}
\Ti{\NTKI}{}{\ell+1}=\Lb{\ell+1} + \lamW{\ell+1}g\!\le(K^{(\ell)}\ri)+\chi_{\perp}\!\le(K^{(\ell)}\ri)\Ti{\NTKI}{}{\ell}\, ,
\ee
with the initial condition coming directly from our first-layer NTK analysis \eqref{eq:NTHinitial}
\be\label{eq:frozen-ntk-intial}
\Ti{\NTKI}{}{1}=\Lb{1} + \lamW{1} \le(\frac{1}{n_0}\sum_{j=1}^{n_{0}}x_j^2\ri)\, .
\ee
Note that here we have also made use of a helper function and susceptibility\index{perpendicular susceptibility} from \S\ref{ch:eft-mlp}.   

For your convenience, let us also recall and reprint the full set of helper functions --~\eqref{eq:helper_first} and~\eqref{eq:h-function} --  and susceptibilities --~\eqref{eq:chi-parallel}\index{parallel susceptibility} and~\eqref{eq:chi-perp} -- that we first made popular 
in~\S\ref{ch:eft-mlp}: 
\begin{align}
g(K)&=\le\langle \sigma(z)\, \sigma(z)\ri\rangle_{K}\, ,\label{eq:g-function-reprint}\\
h(K)&\equiv \frac{C_W}{4K^2}\le\langle \sigma'(z)\, \sigma'(z)\le(z^2-K\ri)\ri\rangle_{K}=\frac{1}{2}\frac{\td }{\td K}\chi_{\perp}(\ker)\, ,\label{eq:h-function-reprint}\\
\chi_{\parallel}(\ker)&=C_W g'(K)=\frac{C_W}{2\ker^2} \bra \sigma(z)\, \sigma(z)\le(z^2-K\ri)\ket_{\ker}=\frac{C_W}{\ker} \bra z\, \sigma'(z)\, \sigma(z)\ket_{\ker}\, ,\label{eq:chi-parallel-reprint}\\
\chi_{\perp}(K)&=C_W \bra\sigma^\prime(z)\, \sigma^\prime(z) \ket_{\ker}\, .\label{eq:chi-perp-reprint}
\end{align}
As a reminder, to go between the middle and right-hand expression in \eqref{eq:chi-parallel-reprint} you should integrate by parts.\index{integration by parts}









\index{parallel susceptibility}\index{perpendicular susceptibility}
For the remaining recursions, we're going to fast-forward the process as the procedure for converting the multi-input recursions to leading-order single-input recursions surely requires your attention but is somewhat mindless:
\emph{(i)} drop the layer dependence of the \terminate{initialization hyperparameters} as \eqref{eq:ntk-chapter-initialziation-hyperparameters-dropped-layer-dependence}
and uniformize the layer widths as \eqref{eq:ntk-chapter-layer-widths-equalized};
\emph{(ii)} drop \terminate{sample indices} everywhere;
\emph{(iii)} replace the mean metric $G^{(\ell)}$ and the NTK mean $\NTKM^{(\ell)}$\index{neural tangent kernel!mean} with the kernel $K^{(\ell)}$\index{kernel} and the frozen NTK $\NTKI^{(\ell)}$\index{frozen NTK}, respectively;\footnote{Picking nits, we should really make $1/n$ expansions -- similar to~\eqref{eq:definition-of-kernel-first} for the mean metric $G^{(\ell)}$ and~\eqref{eq:NTK-mean-expansion} for the NTK mean $H^{(\ell)}$ -- for the finite-width tensors $A^{(\ell)}$, $B^{(\ell)}$, $D^{(\ell)}$, $F^{(\ell)}$, and also properly make use of the one that we made for $V^{(\ell)}$~\eqref{eq:vertex-decomposition}, denoting the leading-order pieces as $A^{\le\{0\ri\}(\ell)}$ and such, and dropping the subleading pieces. For the interest of notational sanity we won't impose this on you, though our recursions for these tensors should all be understood as referring to these leading-order pieces. (The kernel and the frozen NTK are special in that these infinite-width objects have already been well-studied by the community, and so in this case it's important to differentiate between the finite-width object and the infinite-width piece.)\index{$1/n$ expansion}}
and \emph{(iv)} substitute in for helper functions and susceptibilities~\eqref{eq:g-function-reprint}--\eqref{eq:chi-perp-reprint}.
In particular, this last step has the benefit of letting us recycle our results from \S\ref{ch:eft-mlp} on the deep asymptotic behavior of these functions.

It will also be necessary to recall the single-input leading-order expression for the auxiliary stochastic variable~\eqref{eq:def-omega-without-neural},
\be\label{eq:def-omega-without-neural-single}
\Oi{}{\ell+1} \equiv \lamW{\ell+1} \sigma(z)\sigma(z) +C_W\,\Ti{\NTKI}{}{\ell} \, \sigma'(z)\sigma'(z)  \, ,
\ee
which appears in the recursions for $D^{(\ell)}$~\eqref{eq:D-recursion} and $A^{(\ell)}$~\eqref{eq:A-recursion}; we'll make this substitution the penultimate step \emph{(iii-b)}, if you will. In making these substitutions, please keep in mind that 
the frozen NTK $\Ti{\NTKI}{}{\ell}$ multiplying the second term is not a random variable and hence can be escorted out of any Gaussian expectations\index{Gaussian expectation}.

At this point, you should grab another roll of parchment,
jot down expressions~\eqref{eq:g-function-reprint}--\eqref{eq:def-omega-without-neural-single},
flip back a few pages to locate recursions~\eqref{eq:D-recursion},~\eqref{eq:F-recursion},~\eqref{eq:B-recursion}, and~\eqref{eq:A-recursion},  for $D^{(\ell)}$, $F^{(\ell)}$, $B^{(\ell)}$, and $A^{(\ell)}$, respectively  (or perhaps you kiddos can simply click the equation references in your \terminate{eBook} and copy over the equations to your \terminate{tablet}), and simplify them %
according to the four-(though-sometimes-secretly-five-)step process \emph{(i)}--\emph{(iv)} above. When you're finished, make sure you agree with us:
\begin{align}
\label{eq:D-recursion-single}
\NTHD{}{\ell+1}=&\ \chi_{\perp}^{(\ell)}\chi_{\parallel}^{(\ell)} D^{(\ell)}+\le( \frac{\lamW{\ell+1}}{C_W}\ri)\!  \le[C_W^2\bra \sigma(z)\sigma(z)\sigma(z)\sigma(z) \ket_{\Ti{\ker}{}{\ell}}- \le(C_W g^{(\ell)}\ri)^2+\le(\chi_{\parallel}^{(\ell)}\ri)^2\Ti{\FPV}{}{\ell}\ri]\,  \notag \\
&+\Ti{\NTKI}{}{\ell}\le[C_W^2\bra \sigma(z)\sigma(z) \sigma'(z)\sigma'(z)\ket_{K^{(\ell)}}-C_W g^{(\ell)}\chi_{\perp}^{(\ell)}+2h^{(\ell)}\chi_{\parallel}^{(\ell)} \, V^{(\ell)} \ri]    \, , \\
\NTHF{}{\ell+1}=&\le(\chi_{\parallel}^{(\ell)}\ri)^2\NTHF{}{\ell}+C_W^2\bra\sigma(z)\sigma(z) \sigma'(z)\sigma'(z)\ket_{K^{(\ell)}}\Ti{\NTKI}{}{\ell}\, ,\label{eq:F-recursion-single} \\
\NTHB{}{\ell+1}=&\le(\chi_{\perp}^{(\ell)}\ri)^2 \NTHB{}{\ell}  + C_W^2 \bra  \sigma'(z)\sigma'(z)\sigma'(z)\sigma'(z)\ket_{K^{(\ell)}}\le(\Ti{\NTKI}{}{\ell}\ri)^2  \, ,\label{eq:B-recursion-single} \\
A^{(\ell+1)}=& \le(\chi_{\perp}^{(\ell)}\ri)^2 A^{(\ell)}  + \le(\frac{\LW{\ell+1}}{C_W}\ri)^2 \!\le[C_W^2\bra \sigma(z)\sigma(z)\sigma(z)\sigma(z) \ket_{\Ti{\ker}{}{\ell}}- \le(C_W g^{(\ell)}\ri)^2+\le(\chi_{\parallel}^{(\ell)}\ri)^2\Ti{\FPV}{}{\ell}\ri]
\, \nonumber\\
&+2\le(\frac{\LW{\ell+1}}{C_W}\ri)\Ti{\NTKI}{}{\ell}\le[C_W^2\bra \sigma(z)\sigma(z) \sigma'(z)\sigma'(z)\ket_{K^{(\ell)}}-C_W g^{(\ell)}\chi_{\perp}^{(\ell)}+2h^{(\ell)}\chi_{\parallel}^{(\ell)} \, V^{(\ell)} \ri]\, \nonumber\\
&+2\le(\frac{\LW{\ell+1}}{C_W}\ri)\chi_{\perp}^{(\ell)}\chi_{\parallel}^{(\ell)}D^{(\ell)}+4h^{(\ell)}\chi_{\perp}^{(\ell)}\Ti{\NTKI}{}{\ell}D^{(\ell)}\, \nonumber\\
&+\le(\Ti{\NTKI}{}{\ell}\ri)^2 \le[C_W^2\bra \sigma'(z)\sigma'(z)\sigma'(z)\sigma'(z) \ket_{K^{(\ell)}}-\le(\chi_{\perp}^{(\ell)}\ri)^2+\le(2h^{(\ell)}\ri)^2V^{(\ell)}\ri]\, .\label{eq:A-recursion-single}
\end{align}
For these recursions, the initial conditions (recalling that the first-layer NTK is fully deterministic) all vanish identically as
\be\label{eq:ntk-fluctuations-initial-conditions}
A^{(1)}=B^{(1)}=D^{(1)}=F^{(1)}=0\, .
\ee
Here also, for helper functions and susceptibilities, we used the following simplifying notation 
\be\index{parallel susceptibility}\index{perpendicular susceptibility}\label{eq:notational-squashing}
g^{(\ell)} \equiv g\!\le(K^{(\ell)}\ri) \, , \qquad h^{(\ell)} \equiv h\!\le(K^{(\ell)}\ri) \, , \qquad \Ti{\chi}{\parallel}{\ell} \equiv \chi_\parallel\!\le(K^{(\ell)}\ri) \, , \qquad \Ti{\chi}{\perp}{\ell} \equiv \chi_\perp\!\le(K^{(\ell)}\ri)  \, ,
\ee
making the kernel dependence implicit.

We can further simplify \eqref{eq:D-recursion-single} and~\eqref{eq:A-recursion-single} by recalling the single-input recursion for the \terminate{four-point vertex} \eqref{eq:finite-width-reprinted-vertex}
\begin{align}
\Ti{\FPV}{}{\ell+1}&=\le(\chi_{\parallel}^{(\ell)}\ri)^2\Ti{\FPV}{}{\ell}+C_W^2\le[\bra \sigma(z)\sigma(z)\sigma(z)\sigma(z) \ket_{\Ti{\ker}{}{\ell}}-\le(g^{(\ell)}\ri)^2\ri]\, .
\end{align}
Keep staring at these equations, and you'll see slightly more compact expressions emerge
\begin{align}
\label{eq:D-recursion-single-compacter}
\NTHD{}{\ell+1}=&\ \chi_{\perp}^{(\ell)}\chi_{\parallel}^{(\ell)} D^{(\ell)}+\le( \frac{\lamW{\ell+1}}{C_W}\ri)\!  V^{(\ell+1)}\,  \\
&+\Ti{\NTKI}{}{\ell}\le[C_W^2\bra \sigma(z)\sigma(z) \sigma'(z)\sigma'(z)\ket_{K^{(\ell)}}-C_W g^{(\ell)}\chi_{\perp}^{(\ell)}+2h^{(\ell)}\chi_{\parallel}^{(\ell)} \, V^{(\ell)} \ri] \notag   \, , \\
A^{(\ell+1)}=&\ \le(\chi_{\perp}^{(\ell)}\ri)^2 A^{(\ell)}  - \le(\frac{\LW{\ell+1}}{C_W}\ri)^2 \!V^{(\ell+1)}+2\le(\frac{\LW{\ell+1}}{C_W}\ri)\NTHD{}{\ell+1}+4h^{(\ell)}\chi_{\perp}^{(\ell)}\Ti{\NTKI}{}{\ell}D^{(\ell)} \, \nonumber\\
&+\le(\Ti{\NTKI}{}{\ell}\ri)^2 \le[C_W^2\bra \sigma'(z)\sigma'(z)\sigma'(z)\sigma'(z) \ket_{K^{(\ell)}}-\le(\chi_{\perp}^{(\ell)}\ri)^2+\le(2h^{(\ell)}\ri)^2V^{(\ell)}\ri]\, ,\label{eq:A-recursion-single-compacter}
\end{align}
which you may find makes things simpler when solving these recursions. However, please use these formulae with caution as both $\ell$-th-layer and $(\ell+1)$-th-layer objects appear on their right-hand sides. %













\subsubsection{The relevance of scaling laws}\index{relevant (RG flow)}\index{scaling law}
For the rest of this chapter, we will work through solving the five leading-order single-input NTK recursions~\eqref{eq:frozen-NTK-recursion-single} and~\eqref{eq:D-recursion-single}--\eqref{eq:A-recursion-single}. (Remember that we already solved single-input recursions for the \terminate{kernel} and \terminate{four-point vertex} way back in~\S\ref{ch:eft-mlp}.) In solving these recursions, we will find that each observable 
obeys our \neo{scaling ansatz} \eqref{eq:master-scaling-ansatz}:
\begin{align}\label{eq:master-scaling-ansatz-reprint}
\Ti{\O}{}{\ell} &= \le( \frac{1}{\ell} \ri)^{p_\O} \le[c_{0,0}+  c_{1,1} \le( \frac{\log \ell}{\ell} \ri) + c_{1,0}\le( \frac{ 1}{\ell}\ri) +  c_{2,2} \le(  \frac{\log^2 \ell}{\ell^2} \ri)+  \dots \ri]%
\notag \\
&= \le( \frac{1}{\ell} \ri)^{p_\O} \le[\sum_{s = 0}^\infty \sum_{q=0}^{s} c_{s,q} \le( \frac{\log^q \ell}{\ell^s}  \ri)\ri].
\end{align}
Recall that $p_\O$ is a \neo{critical exponent}, which is universal for a given universality class of activation functions, while the constants $c_{s,q}$ depend on some of the details of the particular activation function under consideration.

\index{neural tangent kernel!variance}\index{frozen NTK}
To properly understand the physics of these observables, recall from \S\ref{sec:signal_prop_finite_width} that we need to consider dimensionless quantities. For the two tensors controlling the NTK variance, we should normalize by the square of the frozen NTK\index{frozen NTK}
\be\label{eq:ntk-variance-scaling-ansatz}
\frac{\Ti{A}{}{\ell}}{n\le( \Ti{\NTKI}{}{\ell} \ri)^2} \sim \frac{1}{n} \le( \frac{1}{\ell} \ri)^{p_A - 2 p_\Theta}+ \dots \, , \qquad \frac{\Ti{B}{}{\ell}}{n\le( \Ti{\NTKI}{}{\ell} \ri)^2} \sim \frac{1}{n} \le( \frac{1}{\ell} \ri)^{p_B - 2 p_\Theta}+ \dots \, ,
\ee
while for the NTK-preactivation cross correlation\index{cross correlation!NTK-preactivation}, we should instead normalize by one factor of the frozen NTK\index{frozen NTK} and one factor of the \terminate{kernel}
\be\label{eq:ntk-cross-corr-scaling-ansatz}
\frac{\Ti{D}{}{\ell}}{ n\Ti{K}{}{\ell}  \Ti{\NTKI}{}{\ell}  } \sim \frac{1}{n} \le( \frac{1}{\ell} \ri)^{p_D - p_\Theta - p_0}+ \dots \, , \qquad \frac{\Ti{F}{}{\ell}}{ n\Ti{K}{}{\ell}  \Ti{\NTKI}{}{\ell}  } \sim \frac{1}{n} \le( \frac{1}{\ell} \ri)^{p_F - p_\Theta - p_0}+ \dots \,,
\ee
where $p_0$ was the \terminate{critical exponent} for the single-input kernel $K^{(\ell)}$.  

By looking at these dimensionless quantities, we'll find \emph{scaling laws}\index{scaling law} that transcend even beyond universality classes. As a particular example, recall that the normalized \terminate{four-point vertex} \eqref{eq:k-star-equals-zero-normalized-four-point-scaling-law},
\be\label{eq:k-star-equals-zero-normalized-four-point-scaling-law-reprint}
\frac{\Ti{\FPV}{}{\ell}}{n \le(\Ti{ \ker}{}{\ell} \ri)^2} \sim  \frac{1}{n}\le(\frac{1}{\ell} \ri)^{p_V-2p_0} + \dots \, ,%
\ee
gave rise to a scaling law~\eqref{eq:vertex-scaling-law}
\be\label{eq:vertex-scaling-law-reprint}
p_V-2p_0 = -1\, ,
\ee
for both scale-invariant and $K^\star=0$ activation functions.\index{universality class!scale-invariant}\index{universality class!K@$K^\star=0$}
This \terminate{scaling law} let us interpret the ratio $\ell/n$ as an \neo{emergent scale} controlling the leading finite-width behavior of the preactivation distribution. \emph{Spoiler alert:}\index{spoiler alert} in much the same way, we'll find scaling laws
\be\label{eq:NTK-scaling-laws}
p_A - 2 p_\Theta=-1\, , \quad p_B- 2 p_\Theta=-1\, , \quad p_D - p_\Theta - p_0=-1\,, \quad p_F - p_\Theta - p_0=-1\, ,
\ee
that also hold for both the scale-invariant and $K^\star=0$ universality classes.\index{universality class!scale-invariant}\index{universality class!K@$K^\star=0$}
Thus, we'll be able to conclude that all the leading finite-width effects of the NTK-preactivation joint distribution are \emph{relevant}\index{relevant (RG flow)} and controlled by the same $\ell/n$ perturbative cutoff\index{cutoff, effective theory}. This means that we can effectively describe the training of realistic deep networks of finite width and nonzero $L/n$.




\subsubsection{Formalities: perpendicular perturbations and the frozen NTK}
\index{frozen NTK}
Before explicitly analyzing universality classes, let us note that the frozen-NTK recursion~\eqref{eq:frozen-NTK-recursion-single} admits a formal solution
\be\label{eq:ntk-mean-k-star-formal}
\NTKI^{(\ell)}=\sum_{\ell'=1}^{\ell}\le\{\le[\Lb{\ell'}+\LW{\ell'}g^{(\ell'-1)}\ri]\le[\prod_{\ell''=\ell'}^{\ell-1}\Ti{\chi}{\perp}{\ell''}\ri] \ri\} \, .
\ee
In words, we see that the solution involves a sum over all the previous layers $1, \dots, \ell$, and that each term in the sum involves an additive contribution $\Lb{\ell'}+\LW{\ell'}g^{(\ell'-1)}$. Such a contribution then gets recursively multiplied by perpendicular susceptibilities up to the $(\ell-1)$-th layer, resulting in an overall multiplicative factor $\prod_{\ell''=\ell'}^{\ell-1}\Ti{\chi}{\perp}{\ell''}$.
To avoid the exponential behavior that's generic with such a factor, we must set $\chi_{\perp}=1$.


It is enlightening to tie this insight to the discussion we had in~\S\ref{sec:bootstrapping} where we performed our general \terminate{criticality} analysis of the kernel recursion. There, we first looked at the single-input kernel and set $\chi_{\parallel}=1$ to avoid exponential behavior in the network outputs. Then, we looked at the two-input kernel and analyzed how the off-diagonal perpendicular perturbations $\Ti{\delta\delta\ker}{[2]}{\ell}$ flow. Turning off the odd perturbations $\Ti{\delta\ker}{[1]}{\ell}$, a brief inspection of the perpendicular recursion~\eqref{K2}
\be\label{eq:ddK-reprint-ntk-eft}
\Ti{\delta\delta \ker}{[2]}{\ell+1}=\chi_{\perp}^{(\ell)}\Ti{\delta\delta \ker}{[2]}{\ell}\, ,
\ee
necessitated the \terminate{criticality} condition $\chi_{\perp}=1$ so as to preserve the difference between nearby inputs as they propagate through the network. 
At the time, we presumed that such a condition would be useful for comparing nearby inputs when learning from data.
Indeed, the same multiplicative factor that appeared in the formal solution for the frozen NTK \eqref{eq:ntk-mean-k-star-formal},
\be\label{eq:chi-perp-factor-solution-ntk}
\prod_{\ell''=\ell'}^{\ell-1}\Ti{\chi}{\perp}{\ell''}=\frac{\Ti{\delta\delta \ker}{[2]}{\ell}}{\Ti{\delta\delta \ker}{[2]}{\ell'}}\, ,
\ee
also appears in a formal solution for $\Ti{\delta\delta \ker}{[2]}{\ell}$. Thus, with both formal solutions \eqref{eq:ntk-mean-k-star-formal} and \eqref{eq:chi-perp-factor-solution-ntk}, we have formalized the connection between preserving $\Ti{\delta\delta \ker}{[2]}{\ell}$ data and
learning from data. %

With the formalities out of the way, let's now analyze our two eminent universality classes, the scale-invariant universality class\index{universality class!scale-invariant} and the $\ker^\star=0$ universality class\index{universality class!K@$K^\star=0$}.


\section{Scale-Invariant Universality Class}\label{sec:ntk_criticality_scale_invariant}
As a reminder, the canonical members of the scale-invariant universality class\index{universality class!scale-invariant} are the $\relu$ and $\linear$ activation functions.
For a general activation function in this universality class,
\be\label{eq:scale-invariant-one-kink-tired}
\sigma(z) = 
    \begin{cases}
   a_+ z \, , & z \ge 0  \, , \\
    a_- z \, , & z < 0 \, ,
    \end{cases}
\ee
recall from \S\ref{sec:scale-invariant-eft} that the helper functions and susceptibilities\index{parallel susceptibility}\index{perpendicular susceptibility} evaluate to
\begin{align}\label{eq:helper-ntk-scale-invariant-g}
g^{(\ell)}&=A_2 K^{(\ell)}\, ,\\
\label{eq:helper-ntk-scale-invariant-h}
h^{(\ell)}&=0\, ,\\
\chi_{\parallel}^{(\ell)}&= \chi\, , \\
\chi_{\perp}^{(\ell)}&= \chi\, ,
\end{align}
with 
$\chi \equiv C_W A_2$. %
By substituting in \eqref{eq:scale-invariant-one-kink-tired} and performing the integrals,
we can just as easily evaluate the three other Gaussian expectations\index{Gaussian expectation} that we'll need
\begin{align}
\label{eq:gaussian-expectation-ntk-scale-invariant-1}
\bra \sigma(z)\sigma(z)\sigma(z)\sigma(z) \ket_{K^{(\ell)}}&=3 A_4 \le(K^{(\ell)}\ri)^2\, ,\\
\label{eq:gaussian-expectation-ntk-scale-invariant-2}
\bra \sigma(z)\sigma(z)\sigma'(z)\sigma'(z) \ket_{K^{(\ell)}}&=A_4 K^{(\ell)}\, ,\\
\label{eq:gaussian-expectation-ntk-scale-invariant-3}
\bra \sigma'(z)\sigma'(z)\sigma'(z)\sigma'(z) \ket_{K^{(\ell)}}&=A_4\, .
\end{align}
Here and right
before, we've also made use of our previous definitions for the constants that naturally arise from these integrations:
\be
A_2\equiv \frac{a_+^2+a_-^2}{2}\, , \qquad A_4\equiv \frac{a_+^4+a_-^4}{2}\, .
\ee


With these recollections, we are reminded of one of this class's principal characteristics: both susceptibilities are independent of the kernel and constant for all layers.
With that in mind, we were able to easily satisfy
\terminate{criticality} for the scale-invariant universality class\index{universality class!scale-invariant} 
by setting the \terminate{initialization hyperparameters} to 
\be\label{eq:scale-invariant-criticality-ntk-reprint}
C_b=0\, ,\qquad  C_W =\frac{1}{A_2} \, .
\ee
With these tunings, both susceptibilities are set to unity $\chi=1$ and the fixed-point value of the kernel is given in terms of the input by the expression \eqref{eq:chapter-5-scale-invariant-kernel-fixed}
\be
\Tif{\ker}{} \equiv \frac{1}{A_2} \le(\frac{1}{n_0}\sum_{j=1}^{n_0}x_j^2\ri)\, ,
\ee
and the \terminate{four-point vertex} at \terminate{criticality} is given by \eqref{eq:scale-invariant-vertex-solution}
\be\label{eq:scale-invariant-vertex-solution-reprint}
\Ti{\FPV}{}{\ell}=\le(\ell-1\ri)\le(\frac{3A_4}{A_2^2}-1\ri) \le(\Tif{\ker}{}\ri)^2 \, .
\ee



\subsubsection{NTK mean (frozen NTK)}\index{neural tangent kernel!mean}\index{frozen NTK}\index{learning rate}
With the above expressions in mind, at \terminate{criticality} the recursion~\eqref{eq:frozen-NTK-recursion-single} for the single-input frozen NTK simplifies to
\be\label{eq:frozen-NTK-recursion-single-scale-invariant}
\Ti{\NTKI}{}{\ell+1}=\Ti{\NTKI}{}{\ell}+\Lb{\ell+1} + \lamW{\ell+1} A_2  \Tif{\ker}{}\, ,
\ee
This recursion, together with the initial condition~\eqref{eq:frozen-ntk-intial}, is easy to solve for a given set of bias and weight learning rates. 

For instance, assuming layer-independent learning rates $\Lb{\ell}=\lambda_b$ and $\lamW{\ell}=\lambda_W$,
we find
\be\label{eq:frozen-ntk-critical-solution-relu}
\Ti{\NTKI}{}{\ell}=\le(\lambda_b+\lambda_W A_2 \Tif{\ker}{}\ri)\ell\, .
\ee
With these uniform learning rates, we see that the frozen NTK\index{frozen NTK} for the scale-invariant universality class\index{universality class!scale-invariant} grows linearly with depth. Since the NTK involves a sum over all the previous layers \eqref{eq:ntk-mean-k-star-formal}, linear growth implies that
these contributions are uniform across the layers;
this is in contrast to the non-critical cases, for which we would have had exponentially different contributions from the different layers of a deep network, as is clear from the formal solution~\eqref{eq:ntk-mean-k-star-formal}. 
Finally, a comparison with the ansatz \eqref{eq:master-scaling-ansatz-reprint} implies that the \terminate{critical exponent} for the frozen NTK\index{frozen NTK} is given by $p_\Theta  = -1$. We will interpret all these points further in \S\ref{sec:EVGP-WEP}.







\subsubsection{NTK variance and NTK-preactivation cross correlation (agitated NTK)}\index{neural tangent kernel!agitated}

Now, let's evaluate our finite-width recursions \eqref{eq:D-recursion-single}--\eqref{eq:A-recursion-single} to find the NTK variance and the NTK-preactivation cross correlations.\footnote{
    You could also choose to evaluate \eqref{eq:D-recursion-single-compacter} and then \eqref{eq:A-recursion-single-compacter} for $D^{(\ell)}$ and $A^{(\ell)}$, respectively; it's about the same level of difficulty and obviously yields the same solution either way.
} First, we can simplify them by substituting in for the helper functions $g^{(\ell)}=A_2 K^{(\ell)}$ \eqref{eq:helper-ntk-scale-invariant-g} and $h^{(\ell)}=0$ \eqref{eq:helper-ntk-scale-invariant-h} as well as making use of our formulae for the three other Gaussian expectations\index{Gaussian expectation}~\eqref{eq:gaussian-expectation-ntk-scale-invariant-1}--\eqref{eq:gaussian-expectation-ntk-scale-invariant-3} involving $A_4$. Then, let us tune the \terminate{initialization hyperparameters} to \terminate{criticality}~\eqref{eq:scale-invariant-criticality-ntk-reprint} by picking $C_W=1/A_2$, which sets both susceptibilities to unity $\Ti{\chi}{\parallel}{\ell}=\Ti{\chi}{\perp}{\ell}=1$ and makes the kernel fixed $K^{(\ell)}=\Tif{\ker}{}$. With these manipulations, we get
\begin{align}
\NTHD{}{\ell+1}=&D^{(\ell)}+\lambda_W A_2  \le[\le(\frac{3A_4}{A_2^2}-1\ri)\le(\Tif{\ker}{}\ri)^2+V^{(\ell)}\ri]+\le(\frac{A_4}{A_2^2}-1\ri)\Tif{\ker}{}\Ti{\NTKI}{}{\ell}\, ,\\
\NTHF{}{\ell+1}=&\NTHF{}{\ell}+\frac{A_4}{A_2^2}\Tif{\ker}{}\Ti{\NTKI}{}{\ell}\, ,\\
\NTHB{}{\ell+1}=& \NTHB{}{\ell}+\frac{A_4}{A_2^2}\le(\Ti{\NTKI}{}{\ell}\ri)^2 \, ,\\
A^{(\ell+1)}=& A^{(\ell)}+\le(\lambda_W A_2\ri)^2 \le[\le(\frac{3A_4}{A_2^2}-1\ri)\le(\Tif{\ker}{}\ri)^2+V^{(\ell)}\ri]\, \\
&+2\lambda_W A_2\le(\frac{A_4}{A_2^2}-1\ri)\Tif{\ker}{}\Ti{\NTKI}{}{\ell}+2\lambda_W A_2 D^{(\ell)}+\le(\frac{A_4}{A_2^2}-1\ri)\le(\Ti{\NTKI}{}{\ell}\ri)^2  \, .\nonumber
\end{align}
Note that we have also assumed layer-independent learning rates as we did just before when working out the NTK mean.

Next, substituting in our solutions for $\Ti{\FPV}{}{\ell}$ \eqref{eq:scale-invariant-vertex-solution-reprint} and $\Ti{\NTKI}{}{\ell}$ \eqref{eq:frozen-ntk-critical-solution-relu},
we can easily solve the recursions for $\NTHD{}{\ell}$, $\NTHF{}{\ell}$, and $\NTHB{}{\ell}$.
Then, with our solution for $\NTHD{}{\ell}$ in hand, we can also solve the recursion for $A^{(\ell)}$. All together, this gives the following solutions
\begin{align}
\Ti{D}{}{\ell}&=\frac{\ell(\ell-1)}{2}\le[\lambda_b \le(\frac{A_4}{A_2^2}-1\ri)\Tif{\ker}{}+\lambda_W A_2 \le(\frac{4A_4}{A_2^2}-2\ri)\le(\Tif{\ker}{}\ri)^2 \ri] \, ,\\
\label{eq:F-solution}
\Ti{F}{}{\ell}&=\frac{\ell(\ell-1)}{2}\le[\frac{A_4}{A_2^2} \le(\lambda_b+\lambda_W A_2 \Tif{\ker}{}\ri)\Tif{\ker}{}\ri] \, ,\\
\Ti{B}{}{\ell}&=\frac{\ell(\ell-1)(2\ell-1)}{6}\le(\frac{A_4}{A_2^2}\ri)\le(\lambda_b+\lambda_W A_2 \Tif{\ker}{}\ri)^2\, ,\\
\Ti{A}{}{\ell}&=\frac{\ell^3}{3}\le[
    \le(\frac{A_4}{A_2^2}-1\ri) \lambda_b^2 + 3\le(\frac{A_4}{A_2^2}-1\ri) \lambda_b \lambda_W A_2 \Tif{\ker}{} + \le(5 A_4 - 3 A_2^2 \ri) \lambda_W^2 \le(\Tif{\ker}{}\ri)^2
\ri] + \ldots \, ,
\end{align}
where for $A^{(\ell)}$ we kept only the leading large-$\ell$ contribution.

From these four solutions, we can read off another four critical exponents\index{critical exponent} for the scale-invariant universality class,\index{universality class!scale-invariant}
\be\label{eq:NTK-agited-exponents-scale-invariant}
p_D = -2\, , \qquad p_F = -2\, , \qquad p_B = -3\, , \qquad p_A = -3\, ,
\ee
which corresponds to the quadratic growth of $\Ti{D}{}{\ell}$ and $\Ti{F}{}{\ell}$ and the cubic growth of $\Ti{B}{}{\ell}$ and $\Ti{A}{}{\ell}$. 
Combined with $p_0=0$ for the kernel and $p_{\Theta}=-1$ for the frozen NTK, we obtain the advertised $\ell/n$-scaling~\eqref{eq:NTK-scaling-laws} of the appropriately           normalized quantities 
\eqref{eq:ntk-variance-scaling-ansatz} and \eqref{eq:ntk-cross-corr-scaling-ansatz}.






\section{\texorpdfstring{$\Tif{\ker}{}=0$}{K*=0} Universality Class}\label{sec:ntk_criticality_tanh_univ}\index{universality class!K@$K^\star=0$}
As a reminder, two notable members of this class are $\tanhA$ and $\sinA$. More generally, the $\ker^\star=0$ universality class contains any activation function with a corresponding kernel that has a nontrivial fixed point at $\Tif{\ker}{}=0$. 

Specifically, recall from~\S\ref{subsec:tanh_univ} that we used the following notation for the Taylor coefficients of an activation function:
\be\label{eq:taylor-expansion-k-star-reprint}
\sigma(z)=\sum_{p=0}^{\infty}\frac{\sigma_{p}}{p!}z^p\,  .
\ee
Then, from an analysis of the single-input kernel recursion we learned that there's nontrivial fixed point at $\Tif{\ker}{}=0$ if and only if the activation function vanishes at the origin with nonzero slope~\eqref{eq:tanh_cri_condition}
\be\label{eq:tanh_cri_condition-reprint-ntk}
\sigma_0=0\,  , \qquad \sigma_1\ne0\, ,
\ee
for which we can satisfy the \terminate{criticality} conditions by tuning the \terminate{initialization hyperparameters} as \eqref{eq:k-star-equals-zero-critical-initialization} 
\be\label{eq:k-star-equals-zero-critical-initialization-reprint-ntk}
C_b=0\,  , \qquad C_W=\frac{1}{\sigma_1^2}\, .
\ee
Going forward, we will assume that the bias variance and rescaled weight variance have been tuned to \terminate{criticality} as \eqref{eq:k-star-equals-zero-critical-initialization-reprint-ntk}. 

\index{parallel susceptibility}\index{perpendicular susceptibility}
Unlike the scale-invariant universality class\index{universality class!scale-invariant}, the \terminate{criticality} analysis for the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$} was perturbative around $K=0$.
For this analysis, we expanded the helper function $g(K)$ and both susceptibilities as \eqref{eq:g0}--\eqref{eq:chi-perp-expansion-K-star-equals-zero}, which 
-- now with \eqref{eq:tanh_cri_condition-reprint-ntk} and \eqref{eq:k-star-equals-zero-critical-initialization-reprint-ntk} in mind --
evaluate to
\begin{align}\label{eq:g0-reprint-ntk}
g(\ker)&=\sigma_1^2\le[ K+a_1 K^2+\o{K^3}\ri]\, , \\
\label{eq:chi-para-reprint-ntk-kstar}
\chi_{\parallel}(\ker)&=1+2 a_1K+\o{K^2}\, ,\\
\label{eq:chi-perp-reprint-ntk-kstar}
\chi_{\perp}(\ker)&=1+b_1 K+\o{K^2}\, ,
\end{align}
where we've also recalled the following combinations of Taylor coefficients
\begin{align}
a_1&\equiv \le(\frac{\sigma_3}{\sigma_1}\ri)+\frac{3}{4}\le(\frac{\sigma_2}{\sigma_1}\ri)^2\ ,\\
b_1&\equiv \le(\frac{\sigma_3}{\sigma_1}\ri)+\le(\frac{\sigma_2}{\sigma_1}\ri)^2\, .%
\end{align}
As a reminder, to get these expressions we first Taylor expanded their definitions \eqref{eq:g-function-reprint},  \eqref{eq:chi-parallel-reprint}, and \eqref{eq:chi-perp-reprint} in $z$,
and then evaluated each series of Gaussian expectations\index{Gaussian expectation} to the desired order. 
Following the same method, we can evaluate the helper function $h(K)$ \eqref{eq:h-function-reprint} as well as the two other Gaussian expectations\index{Gaussian expectation} needed to solve our NTK recursions:
\begin{align}
\label{eq:k-star-h-ntk}
h\!\le(K\ri)&=\frac{b_1}{2}+\o{K^1}\, ,\\
\label{eq:gaussian-expectation-ntk-k-star-2}
\bra \sigma(z)\sigma(z)\sigma'(z)\sigma'(z) \ket_{K}&=\sigma_1^4 \le[K+\o{K^2} \ri]  \, ,\\
\label{eq:gaussian-expectation-k-star-3}
\bra \sigma'(z)\sigma'(z)\sigma'(z)\sigma'(z) \ket_{K}&=  \sigma_1^4 \le[1+\o{K^1} \ri]\, .
\end{align}

Remembering that the \terminate{parallel susceptibility} characterizes the linear response of the kernel perturbations around the fixed point \eqref{eq:chi_parallel_first}, we note from above that the parallel susceptibility at criticality~\eqref{eq:chi-para-reprint-ntk-kstar} is close to one near the nontrivial fixed point at $\Tif{\ker}{}=0$. Consequently, 
we found a power-law large-$\ell$ asymptotic solution for the single-input kernel \eqref{eq:tanh_asymptotic}
\be\label{eq:tanh_asymptotic-reprint-ntk}
\ker^{(\ell)}=\Tif{\ker}{}+\Ti{\Delta\ker}{}{\ell}=\Ti{\Delta\ker}{}{\ell}=\le[\frac{1}{(-a_1)}\ri]\frac{1}{\ell}+  \o{\frac{\log\ell}{\ell^2}}   \, ,
\ee
which slowly but surely approaches the $\Tif{\ker}{}=0$ nontrivial fixed point, justifying our perturbative approach to the deep asymptotics.
As for the single-input \terminate{four-point vertex}, we previously found \eqref{eq:Kstar-equals-zero-vertex-solution}
\begin{align}\label{eq:Kstar-equals-zero-vertex-solution-reprint-ntk}
\Ti{\FPV}{}{\ell}&=\le[\frac{2}{3a_1^2}\ri]\frac{1}{\ell}+  \o{\frac{\log\ell}{\ell^2}}\, , 
\end{align}
and the appropriately normalized quantity \eqref{eq:k-star-equals-zero-normalized-four-point-scaling-law-reprint} has an $\ell/n$ scaling:
\be\label{eq:k-star-equals-zero-normalized-four-point-tanh-univ-reprint}
\frac{\Ti{\FPV}{}{\ell}}{n \le(\Ti{ \ker}{}{\ell} \ri)^2} = \le(\frac{2}{3}\ri)\frac{\ell}{n} + \o{\frac{\log \le(\ell\ri)}{n}} \, .
\ee





\subsubsection{NTK mean (frozen NTK)}\index{neural tangent kernel!mean}\index{frozen NTK}\index{learning rate}\index{universality class!scale-invariant}\index{universality class!K@$K^\star=0$}
Let's start with our generic formal solution~\eqref{eq:ntk-mean-k-star-formal} to the frozen NTK recursion~\eqref{eq:frozen-NTK-recursion-single}. Note that for $K^\star=0$ activation functions the  multiplicative factor~\eqref{eq:chi-perp-factor-solution-ntk} takes the form
\be\label{eq:chi-perp-factor-solution-ntk-specific}
\prod_{\ell''=\ell'}^{\ell-1}\Ti{\chi}{\perp}{\ell''}=\frac{\Ti{\delta\delta \ker}{[2]}{\ell}}{\Ti{\delta\delta \ker}{[2]}{\ell'}}=\le(\frac{\ell'}{\ell}\ri)^{p_{\perp}}\!+\,\ldots\, ,
\ee
when we plug in our large-$\ell$ asymptotic solution for 
$\Ti{\delta\delta\ker}{[2]}{\ell}$~\eqref{eq:perp-asymptotic-solution}. As a reminder, the \terminate{critical exponent} controlling the falloff was given in terms of the Taylor coefficient combinations,  $p_{\perp}=b_1/a_1$, which evaluates to $1$ for the $\tanhA$ and $\sinA$ activation functions.
Plugging this multiplicative factor back into the formal solution 
\eqref{eq:ntk-mean-k-star-formal} along with our expansion for $g(K)$ \eqref{eq:g0-reprint-ntk} evaluated on the asymptotic kernel~\eqref{eq:tanh_asymptotic-reprint-ntk}, we find
\be\label{eq:ntk-mean-k-star-formal-2}
\NTKI^{(\ell)}=\sum_{\ell'=1}^{\ell}\le\{\le[\Lb{\ell'}+\LW{\ell'}\frac{\sigma_1^2}{(-a_1)} \le(\frac{1}{\ell'}\ri) + \dots  \ri] \le[\le(\frac{\ell'}{\ell}\ri)^{p_{\perp}}\!+\,\ldots\,\ri]\ri\} \, .
\ee
Here, 
the factor in the first square bracket is an additive contribution picked up from the $\ell'$-th layer, while the factor in the second square bracket is a multiplicative contribution from recursively passing from the $\ell'$-th layer to the $\ell$-th layer. 





\index{universality class!K@$K^\star=0$}\index{effective theory}\index{practical practitioners}
Effective theorists may take issue with two aspects of this solution 
\eqref{eq:ntk-mean-k-star-formal-2} 
if we naively continue to choose layer-independent learning rates $\Lb{\ell}=\lambda_b$ and $\lamW{\ell}=\lambda_W$.
\bi
\item Firstly, notice in the first square bracket that the $\ell'$-dependence of the bias term differs from the $\ell'$-dependence of the weight term by a factor of $\sim (1/\ell')$. This means that the contribution of the weights to the NTK decreases with depth relative to the contribution of the biases.
\item Secondly, notice in the second square bracket that the $\sim\le(\ell'\ri)^{p_\perp}$ behavior means that the NTK is dominated by contributions from deeper layers for $p_\perp > 0$ in comparison to the shallower layers. Remembering that the NTK controls the dynamics of observables \eqref{eq:obsevable-evolution-layer-ell}, this in turn means that the training dynamics %
will be heavily influenced by the model parameters near the output layer.
\ei
Additionally, practical practitioners may now wonder whether these unnatural depth scalings also contribute to the empirical preference for $\relu$ over $\tanhA$ in the \terminate{deep learning} community. (More on this in \S\ref{sec:EVGP-WEP}.)





\index{learning rate}
Having said all that, we can rectify this
imbalance by scaling out the layer dependence as
\be\label{eq:layer-independent-rates}
\Lb{\ell} \equiv \widetilde{\lambda}_b \le( \frac{1}{\ell}\ri)^{p_\perp} \, , \qquad \LW{\ell} \equiv \widetilde{\lambda}_W \le( \frac{1}{\ell}\ri)^{p_\perp-1} \, ,
\ee
where $\widetilde{\lambda}_b$ and $\widetilde{\lambda}_W$ are layer-independent constants.
Substituting this ansatz into our solution \eqref{eq:ntk-mean-k-star-formal-2}, we find
\be\label{eq:frozen-ntk-k-star-solution}
\Ti{\NTKI}{}{\ell}=\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]\le( \frac{1}{\ell}\ri)^{p_{\perp}-1}+\ldots\, ,
\ee
which manifestly balances the weight and bias contributions.
Thus, we see for the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$} that the \terminate{critical exponent} for the frozen NTK\index{neural tangent kernel!mean}\index{frozen NTK} is given by
\be\label{eq:frozen-ntk-k-star-critical-exponent}
p_{\NTKI}=p_{\perp}-1\, .
\ee
In particular, for both the $\tanhA$ and $\sinA$ activation functions,
$p_{\NTKI}=0$. 



\subsubsection{NTK variance and NTK-preactivation cross correlation (agitated NTK)}
\index{neural tangent kernel!agitated}
Now, let's finally deal with the agitated NTK statistics for the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$}.
To aid our computation at \terminate{criticality}, let us make use of the asymptotic behavior of the kernel~\eqref{eq:tanh_asymptotic-reprint-ntk} and record the leading large-$\ell$ asymptotics of the helper functions~\eqref{eq:g0-reprint-ntk} and~\eqref{eq:k-star-h-ntk}, the susceptibilities~\eqref{eq:chi-para-reprint-ntk-kstar} and~\eqref{eq:chi-perp-reprint-ntk-kstar}, and the two other needed Gaussian expectations~\eqref{eq:gaussian-expectation-ntk-k-star-2} and~\eqref{eq:gaussian-expectation-k-star-3}:
\begin{align}
&C_W g^{(\ell)}=\le[\frac{1}{(-a_1)}\ri]\frac{1}{\ell}+\ldots\, , \label{eq:ntk-k-star-asymptotic-begin}\\
&h^{(\ell)}=\frac{b_1}{2}+\ldots\, ,\\
&\chi_{\parallel}^{(\ell)}=1- \frac{2}{\ell}+\ldots\, ,\\
&\chi_{\perp}^{(\ell)}=1-\frac{p_{\perp}}{\ell}+\ldots\, ,\label{eq:ntk-k-star-perp-susceptibility-asymptotic}\\
&C_W^2\bra \sigma(z)\sigma(z)\sigma'(z)\sigma'(z) \ket_{K^{(\ell)}}=\le[\frac{1}{(-a_1)}\ri]\frac{1}{\ell}+\ldots\, ,\\
&C_W^2\bra \sigma'(z)\sigma'(z)\sigma'(z)\sigma'(z) \ket_{K^{(\ell)}}=1+\ldots\, .\label{eq:ntk-k-star-asymptotic-end}
\end{align}
Additionally, going forward we will assume that the bias and weight learning rates have the layer-dependence \eqref{eq:layer-independent-rates} motivated by equal per-layer NTK contribution.

With all this out of the way, it's straightforward to evaluate the large-$\ell$ asymptotics of $\Ti{F}{}{\ell}$ and $\Ti{B}{}{\ell}$. Plugging in the above expressions and the frozen NTK asymptotic solution~\eqref{eq:frozen-ntk-k-star-solution} into their recursions~\eqref{eq:F-recursion-single} and~\eqref{eq:B-recursion-single}, we get
\begin{align}
\NTHF{}{\ell+1}=&\le[1-\frac{4}{\ell}+\ldots\ri]\NTHF{}{\ell}+\le\{\le[\frac{1}{(-a_1)}\ri]\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]\le(\frac{1}{\ell}\ri)^{p_{\perp}}+\ldots\ri\}\, , \\
\NTHB{}{\ell+1}=&\le[1-\frac{2p_{\perp}}{\ell}+\ldots\ri]\NTHB{}{\ell}+\le\{\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]^2 \le(\frac{1}{\ell}\ri)^{2p_{\perp}-2}+\ldots\ri\}\, .
\end{align}
Substituting in our \terminate{scaling ansatz} \eqref{eq:master-scaling-ansatz-reprint}, they have the following asymptotic solutions at large $\ell$:
\begin{align}
\Ti{F}{}{\ell}&=\frac{1}{(5-p_{\perp})}\le[\frac{1}{(-a_1)}\ri] \le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]\le(\frac{1}{\ell}\ri)^{p_{\perp}-1}+\ldots\, ,\label{eq:F-k-star-solution}\\
\Ti{B}{}{\ell}&=\frac{1}{3} \le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]^2\le(\frac{1}{\ell}\ri)^{2p_{\perp}-3}+\ldots\, .\label{eq:B-k-star-solution}
\end{align}


Next, for the $\Ti{D}{}{\ell}$ recursion, let's start with the slightly more compact expression~\eqref{eq:D-recursion-single-compacter}.
Plugging in the expressions~\eqref{eq:ntk-k-star-asymptotic-begin}--\eqref{eq:ntk-k-star-asymptotic-end} along with the learning rates~\eqref{eq:layer-independent-rates} and the asymptotic solutions for the four-point vertex~\eqref{eq:Kstar-equals-zero-vertex-solution-reprint-ntk} and the frozen NTK~\eqref{eq:frozen-ntk-k-star-solution}, we get a recursion
\be
\NTHD{}{\ell+1}=\!\!\le[1-\frac{(p_{\perp}+2)}{\ell}+\ldots\ri]\!\NTHD{}{\ell}+\le\{\le[\frac{2}{3(-a_1)}\ri]\!\!\le[-p_{\perp}\widetilde{\lambda}_b-(p_{\perp}-1)\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]\!\!\le(\frac{1}{\ell}\ri)^{p_{\perp}}\!\!\!\!\!+\ldots\ri\}\, .
\ee
This recursion can also be easily solved by using our scaling ansatz \eqref{eq:master-scaling-ansatz-reprint}, giving
\be\label{eq:D-k-star-solution}
\NTHD{}{\ell}=\frac{-2}{9(-a_1)}\le[p_{\perp}\widetilde{\lambda}_b+(p_{\perp}-1)\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]\le(\frac{1}{\ell}\ri)^{p_{\perp}-1}+\ldots\, .
\ee

Finally, for $\Ti{A}{}{\ell}$ recursion~\eqref{eq:A-recursion-single-compacter}, the by-now-familiar routine of flipping 
back and forth in your book and plugging in the large-$\ell$ asymptotic expressions~\eqref{eq:ntk-k-star-asymptotic-begin}--\eqref{eq:ntk-k-star-asymptotic-end}, the learning rates~\eqref{eq:layer-independent-rates}, and  the asymptotic solutions for the four-point vertex~\eqref{eq:Kstar-equals-zero-vertex-solution-reprint-ntk}, for the frozen NTK~\eqref{eq:frozen-ntk-k-star-solution}, and for $\Ti{D}{}{\ell}$~\eqref{eq:D-k-star-solution}, gives
\be
\Ti{A}{}{\ell+1}=\le[1-\frac{2p_{\perp}}{\ell}+\ldots\ri]\Ti{A}{}{\ell}+\le\{\frac{4}{9}\le[p_{\perp}\widetilde{\lambda}_b+(p_{\perp}-1)\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]^2 \le(\frac{1}{\ell}\ri)^{2p_{\perp}-2}+\ldots\ri\}\, ,
\ee
which can be solved using the same large-$\ell$ \terminate{scaling ansatz}~\eqref{eq:master-scaling-ansatz-reprint}, giving
\be\label{eq:A-k-star-solution}
\Ti{A}{}{\ell}=\frac{4}{27}\le[p_{\perp}\widetilde{\lambda}_b+(p_{\perp}-1)\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]^2\le(\frac{1}{\ell}\ri)^{2p_{\perp}-3}+\ldots\, .
\ee
With this, we complete our evaluation of the agitated NTK\index{neural tangent kernel!agitated} statistics for the $K^\star=0$ universality class.\index{universality class!K@$K^\star=0$}\footnote{Curiously, for activation functions with $p_\perp=1$ such as $\tanhA$ and $\sinA$, the single-input tensors $D^{(\ell)}$ and $A^{(\ell)}$ are independent of the weight \terminate{learning rate} at leading order.
}

\index{scaling law}
Having solved all the recursions we have, let's collect and recollect the critical exponents.\index{critical exponent} From \eqref{eq:F-k-star-solution} and \eqref{eq:D-k-star-solution} we collect $p_F=p_D=p_{\perp}-1$, while from~\eqref{eq:B-k-star-solution} and~\eqref{eq:A-k-star-solution} we collect $p_B=p_A=2p_{\perp}-3$. Recollecting $p_0=1$ for the kernel and $p_{\Theta}=p_{\perp}-1$ for the frozen NTK, these critical exponents for the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$} again obey $\ell/n$-scaling~\eqref{eq:NTK-scaling-laws} for the normalized quantities
defined in \eqref{eq:ntk-variance-scaling-ansatz} and \eqref{eq:ntk-cross-corr-scaling-ansatz}.
Together with our scale-invariant results~\eqref{eq:NTK-agited-exponents-scale-invariant}, this means the posited relations \eqref{eq:NTK-scaling-laws} do indeed persists across universality classes as scaling laws.\index{scaling law}

\index{neural tangent kernel!variance}
\index{cross correlation!NTK-preactivation}
In summary, we have found that the leading finite-width behavior of the NTK-preactivation joint distribution -- as measured by the NTK variance and NTK-preacitvation cross correlation -- has a \emph{relevant}\index{relevant (RG flow)} $\ell/n$ scaling regardless of activation function, as is natural according to the principles of our \terminate{effective theory}.

\section{Criticality, Exploding and Vanishing Problems, and None of That}\label{sec:EVGP-WEP}\index{exploding and vanishing gradient problem}



\index{exploding and vanishing gradient problem}\index{exploding and vanishing kernel problem}
Having now analyzed the NTK statistics of deep networks, let us culminate our discussion by revisiting our original motivation for \terminate{criticality}: \emph{exploding and vanishing problems}. In particular, let us finally introduce -- and then immediately abolish -- the exploding and vanishing gradient problem.\footnote{This problem was first noticed \cite{hochreiter1991untersuchungen,bengio-evgp-recurrent} in the context of \terminate{training} (the now somewhat deprecated) \emph{recurrent neural networks}\index{recurrent neural network} (RNNs), during the era when neural networks were still \emph{neural networks}\index{neural network} and not yet \neo{deep learning}, that is, at a time when MLPs\index{multilayer perceptron} weren't yet deep enough for this to have been an obvious issue.}






\subsubsection{Traditional view on the exploding and vanishing gradient problem}\index{exploding and vanishing gradient problem}\index{traditionality|seealso{exploding and vanishing gradient problem}}
Traditionally, the \terminate{exploding and vanishing gradient problem} is manifested by considering the behavior of the gradient of the loss for a deep network. Using the \terminate{chain rule} twice, the derivative 
of the loss with respect to a model parameter $\theta^{(\ell)}_{\mu}$ in the $\ell$-th layer -- either a bias $\theta_\mu^{(\ell)} \equiv \bias{j}{\ell}$ or a weight $\theta_\mu^{(\ell)} \equiv \W{jk}{\ell}$ -- takes the form
\be\label{eq:gradient-for-evgp}
\frac{\td \L_\A}{\td \theta_\mu^{(\ell)} }=\sum_{\alpha \in \D} \sum_{i_{L}=1}^{n_{L}}\sum_{i_{\ell}=1}^{n_{\ell}}\epsilon_{i_{L};\alpha} \frac{\td z_{i_{L};\alpha}^{(L)}}{\td z_{i_{\ell};\alpha}^{(\ell)}} \frac{\td  z_{i_{\ell};\alpha}^{(\ell)} }{\td \theta_\mu^{(\ell)}} \, .
\ee
In this gradient, the first factor is the \neo{error factor} \eqref{eq:error-factor-ntk}
\be\label{eq:error-factor-ntk-reprint}
\epsilon_{i;\alpha} \equiv \frac{\partial \L_\A}{\partial z^{(L)}_{i;\alpha}}\, ,
\ee 
the final factor is a \neo{trivial factor}~\eqref{eq:same-layer-derivatives}\index{trivial factor|seealso{exploding and vanishing gradient problem}}
\be\label{eq:same-layer-derivatives-reprint}
\frac{\td \z{i}{\alpha}{\ell}}{\td \bias{j}{\ell}} = \delta_{i j}\, , \qquad \frac{\td \z{i}{\alpha}{\ell}}{\td \W{jk}{\ell}} = \delta_{i j} \, \s{k}{\alpha}{\ell-1} \, ,
\ee
and the middle factor is the \neo{chain-rule factor}
\be\label{eq:backward-pass-iterated}
\frac{\td \z{i_L}{\alpha}{L}}{\td \z{i_\ell}{\alpha}{\ell}}=\sum_{i_{\ell+1}, \ldots, i_{L-1}}\frac{\td \z{i_L}{\alpha}{L}}{\td \z{i_{L-1} }{\alpha}{L-1}} \cdots\frac{\td \z{i_{\ell+1} }{\alpha}{\ell+1}}{\td \z{i_\ell}{\alpha}{\ell}}= \sum_{i_{\ell+1}, \ldots, i_{L-1}} \prod_{\ell'=\ell}^{L-1} \le[ \W{i_{\ell'+1} i_{\ell'} }{\ell'+1}\ds{i_{\ell'} }{\alpha}{\ell'} \ri]\, ,
\ee
which can be derived by iterating
the backward equation \eqref{eq:backward-pass} or equivalently by repeatedly using the \terminate{chain rule} in conjunction with the MLP forward equation~\eqref{eq:forward-pass}.
If this text causes you to experience a large \terminate{error factor} yourself, please flip backward to~\S\ref{sec:NTH-recursions} and review our discussion of the \neo{backpropagation} algorithm.

The point is that without any fine-tuning, the product of matrices from layer $\ell$ to layer $L$ in the chain-rule factor~\eqref{eq:backward-pass-iterated} will generically lead to exponential behavior. Even for networks of moderate depth, this makes it extremely difficult for the shallower-layer parameters to receive a well-behaved gradient and consequentially be properly trained: a vanishing gradient means that such parameters receive no training signal from the data and loss, while an exploding gradient is indicative of an instability in which the loss may increase or even blow up.
This is the \term{exploding and vanishing gradient problem}.
In a sense, this is a backward iteration dual of the already familiar \neo{exploding and vanishing kernel problem} that arises from the forward iteration equation.\footnote{
If you'd like, you can see this \terminate{duality} concretely by considering a deep linear network, for which the statistics of such a product of weights can be worked out exactly exactly as in \S\ref{ch:deep-linear-eft}.\index{deep linear network}}

Of course, not only does the \terminate{chain-rule factor}~\eqref{eq:backward-pass-iterated} need to be well behaved for stable \terminate{training}, but the \terminate{error factor}~\eqref{eq:error-factor-ntk-reprint} and \terminate{trivial factor}~\eqref{eq:same-layer-derivatives-reprint} must be as well. As we'll explain next, these latter factors are directly tied to the \terminate{exploding and vanishing kernel problem}. However, we'll also see that our well-understood notion of \terminate{criticality} is already sufficient to mitigate both exploding and vanishing problems together.





\subsubsection{Critical view on the exploding and vanishing gradient problem}\index{exploding and vanishing gradient problem!relation to criticality}
Critically, let us recall from~\S\ref{sec:criticality_DLN} and then~\S\ref{sec:bootstrapping} our discussion of the \terminate{exploding and vanishing kernel problem}. In those sections, we first motivated \terminate{criticality} as remedying exponential behavior in the \terminate{kernel} $\Ti{\ker}{\alpha_1\alpha_2}{\ell}$. As the $L$-th-layer kernel controls the \emph{typical} values of the network output -- and as the dataset's \terminate{label}s are generically order-one numbers -- we suggested that such an exploding or vanishing kernel would be problematic for \terminate{training}. 
Now that we know a little about \terminate{gradient descent}, we can actually see a more direct manifestation of this instability by considering all the factors that make up the network's gradient \eqref{eq:gradient-for-evgp}.


First, let's see how the \terminate{error factor}~\eqref{eq:error-factor-ntk-reprint} is tied to the \emph{exploding} kernel problem.
For example, the \terminate{error factor} for the MSE loss\index{loss!MSE}~\eqref{eq:MSE-loss} is given by~\eqref{eq:mse-function-approximation-error}
\be\label{eq:mse-function-approximation-error-mlp}
\epsilon_{i;\alpha}=\z{i}{\alpha}{L}-\y{i}{\alpha} \, .
\ee
As you can clearly see, if the kernel explodes, then the typical output -- and hence typical values of the \terminate{error factor} -- will explode as well.\footnote{Getting ahead of ourselves, a precocious reader might wonder whether this matters for the \emph{cross-entropy loss}\index{loss!cross-entropy}, since for that \terminate{loss} the \terminate{error factor} will stay of order one even if the network output explodes. However, in this case the model would then be (exponentially) overconfident on its predictions and such an \terminate{inductive bias} would be difficult to correct via \terminate{training}.
}
To ensure this does not happen, we must set~$\chi_{\parallel}\!\le(\Tif{\ker}{}{}\ri)\leq1$.

Second, notice that the \terminate{trivial factor}~\eqref{eq:same-layer-derivatives-reprint} for the weights is proportional to the activation. For activation functions contained in either of the scale-invariant and $K^\star=0$ universality classes\index{universality class!scale-invariant}\index{universality class!K@$K^\star=0$}, if the \terminate{kernel} -- and consequently the typical preactivation -- is exponentially small, then the activation -- and consequently the \terminate{trivial factor} -- will be exponentially suppressed. Subsequently, the weights in the deeper layers of the network would struggle to train as they only receive an exponentially small update. Thus, in order to avoid this \emph{vanishing} kernel problem, we demand $\chi_{\parallel}\!\le(\Tif{\ker}{}{}\ri)\geq1$.\footnote{Incidentally, for the scale-invariant universality class\index{universality class!scale-invariant}, this same logic provides an additional justification for avoiding the \emph{exploding} kernel problem. That is, since scale-invariant activation functions don't \emph{saturate}\index{saturation (of an activation)}, if $\chi_{\parallel}\!\le(\Tif{\ker}{}{}\ri)>1$, then the activation -- and consequentially the \terminate{trivial factor} -- would explode.}





Combining these two observations,  we see that the \terminate{exploding and vanishing kernel problem} is directly manifested as a subproblem of the \terminate{exploding and vanishing gradient problem} and further see how our \terminate{criticality} condition imposed on the \terminate{parallel susceptibility}, $\chi_{\parallel}\!\le(\Tif{\ker}{}{}\ri)=1$, serves to mitigate it.





\index{frozen NTK}
Moreover, we can shed further light on the vanishing of the \terminate{trivial factor} by considering its embodiment in the NTK. Considering our formal solution for the frozen NTK~\eqref{eq:ntk-mean-k-star-formal} and recalling the original definition \eqref{eq:nth-layer-sum-definition}, we can track the contribution of the weight derivatives as leading to the additive term $\LW{\ell'}g^{(\ell'-1)}$. For an exponentially vanishing kernel, this factor is exponentially suppressed as
\be
\LW{\ell'}g^{(\ell'-1)}\propto K^{(\ell'-1)}\lll 1\, ,
\ee
since $g\!\le(\ker\ri)=A_2 \ker$~\eqref{eq:helper-ntk-scale-invariant-g} for the scale-invariant universality class\index{universality class!scale-invariant} and $g\!\le(\ker\ri)=\sigma_1^2 \ker+\o{K^2}$~\eqref{eq:g0-reprint-ntk} for the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$}. This is another way of seeing that such deeper-layer weights are not contributing to the training dynamics and, in particular, also implies that such weights will have a minimal effect on the updates to \emph{other} parameters.



\index{frozen NTK}\index{neural tangent kernel!mean}\index{gradient descent}\index{forward equation!NTK}
Similarly, we see that the \terminate{chain-rule factor}~\eqref{eq:backward-pass-iterated} is also encoded in the NTK in the multiplicative factor $\prod_{\ell''=\ell'}^{\ell-1}\Ti{\chi}{\perp}{\ell''}$~\eqref{eq:chi-perp-factor-solution-ntk}. In fact, such a factor was secretly always lurking in the NTK forward equation \eqref{eq:NTHchain} or \eqref{eq:NTH-recursion-without-expectation} as the coefficient of the recursive term.
To disclose that secret, note that in the \terminate{infinite-width limit} the expectation of the \terminate{chain-rule factor} factorizes, and we see from \eqref{eq:NTHchain} or \eqref{eq:NTH-recursion-without-expectation} that
\be
\E{\sum_{j_1,j_2}\frac{\td z_{i}^{(\ell+1)}}{\td z_{j_1}^{(\ell)}}\frac{\td z_{i}^{(\ell+1)}}{\td z_{j_2}^{(\ell)}}}\!=\E{\sum_{j_1,j_2}\W{ij_1}{\ell+1}\W{ij_2}{\ell+1}\sigma_{j_1}^{\prime\,(\ell)}\sigma_{j_2}^{\prime\,(\ell)}}\!=C_W\!\bra \sigma'(z)\sigma'(z)\ket_{\ker^{(\ell)}}=\chi_{\perp}^{(\ell)}\, ,
\ee
thus making this connection explicit.

\index{chain-rule factor}
As we discussed in \S\ref{sec:ntk_criticality} under the heading \emph{Formalities: perpendicular perturbations and the frozen NTK}, we need to ensure that these multiplicative chain-rule factors are under control for training to be well behaved, on average. In particular, if $\chi_{\perp}\!\le(\Tif{\ker}{}{}\ri)>1$, then the deeper-layer NTKs will exponentially explode, and the \terminate{training} dynamics will be unstable, potentially leading to growing losses. If instead $\chi_{\perp}\!\le(\Tif{\ker}{}{}\ri)<1$,
then the contribution of the biases and weights in the shallower layers to the deeper-layer NTKs will be exponentially diminished. This means both that such parameters will struggle to move via gradient-descent\index{gradient descent} updates and \emph{also} that they will struggle to influence the evolution of the parameters in the deeper layers. 

All in all, we see that contribution of the chain-rule factor to the \terminate{exploding and vanishing gradient problem} is directly connected to an exponentially growing or decaying multiplicative factor in the NTK and further see how our \terminate{criticality} condition imposed on the \terminate{perpendicular susceptibility}, $\chi_{\perp}\!\le(\Tif{\ker}{}{}\ri)=1$, serves to mitigate it.\footnote{
Having now explained why \neo{criticality} is a complete solution to the \terminate{exploding and vanishing gradient problem},
let us discuss remedies of \neo{traditionality}.

One of the first heuristic solutions -- first discussed in the context of recurrent neural networks\index{recurrent neural network} -- is \terminate{gradient clipping}\index{gradient clipping|seealso{exploding and vanishing gradient problem}} \cite{pascanu2013difficulty}, in which the norm of the gradient is reduced whenever it exceeds a certain threshold. As should be apparent given our discussion, such an ad hoc, distortionary, and hard-to-tune heuristic is completely unnecessary -- and potentially even destructive -- in networks that are at \terminate{criticality}. 

\index{activation function} 
A second heuristic solution was the adoption of the $\relu$. Recall that activation functions such as the $\tanhA$ and the $\sigmoid$ \emph{saturate}\index{saturation (of an activation)} when $|z| \to \infty$. This implies that the derivative of the activation vanishes upon saturation as $\sigma'(z)=0$. 
Such saturation naturally leads to vanishing gradients, as we can easily see from the right-hand side of \eqref{eq:backward-pass-iterated}. It is partially within this context that practitioners adopted the $\relu$ over saturating activation functions such as the $\tanhA$ (see, e.g.~\cite{glorot2011deep}). However, with our deeper and more critical understanding, we now appreciate that \terminate{criticality} is sufficient to mitigate this vanishing gradient problem for any activation function that admits critical \terminate{initialization hyperparameters}, even for saturating ones like $\tanhA$. 
}


























\subsubsection{An equivalence principle for learning rates}
\index{frozen NTK}\index{exploding and vanishing gradient problem}\index{forward equation!NTK}
Although originally derived by considering the behavior of the \terminate{kernel} recursion, we just recovered both of our criticality conditions $\chi_{\parallel}\!\le(\Tif{\ker}{}{}\ri)=1$ and $\chi_{\perp}\!\le(\Tif{\ker}{}{}\ri)=1$ by a direct analysis of gradient-descent\index{gradient descent} updates and of the NTK forward equation.
Importantly, the guiding principle we found was that each layer should not make an exponentially different contribution to the NTK.

\index{learning rate equivalence principle|see{equivalence principle}}
However, there's really no reason for us to stop at the exponential level. In fact, we can further demand at the polynomial level that no type of model parameter and no layer dominate over another for \terminate{training}. In other words, rather than requiring \emph{more or less equal} contributions from the parameters in different layers, we demand parametrically \emph{equal} contributions to the NTK for each parameter group from every layer. This gives an \term{equivalence principle} for setting the \terminate{training hyperparameters}, i.e., the bias and weight learning rates. 
In fact, en route to this section, we already found a way to satisfy this equivalence principle for each of our universality classes.














As we retroactively saw in~\S\ref{sec:ntk_criticality_scale_invariant}, the equivalence principle was easily met for activation functions contained in the scale-invariant universality class\index{universality class!scale-invariant} 
by setting the bias and weight learning rates to be layer-independent:
\be\label{eq:super-scale-invariant}
\eta\Lb{\ell}=\frac{\eta\widetilde{\lambda}_b}{L}\, , \qquad \frac{\eta\lamW{\ell}}{n_{\ell-1}}=\frac{\eta\widetilde{\lambda}_W}{Ln_{\ell-1}}\, .
\ee
Here, we have also re-emphasized the rescaling of the weight learning rates by width of the previous layer as we discussed multiple times in~\S\ref{sec:NTH-recursions}. In particular, you should understand the \emph{parametrically equal contributions} provision of equivalence principle as requiring appropriate depth \emph{and} width scalings.
Also here -- with our discussion in~\S\ref{sec:NTH-recursions} under the heading \emph{Scaling in the effective theory} in mind -- note that we rescaled the learning rates by the overall depth $L$ of the network so as to have an order-one NTK in the output layer.
This ensures that we can naturally compare these rescaled learning rates $\eta\widetilde{\lambda}_b$ and $\eta\widetilde{\lambda}_W$ across models of different depths as well as that we have properly scaled order-one changes in observables\index{observable}, e.g.~the loss, after any gradient-descent\index{gradient descent} update.




Meanwhile for the $\ker^\star=0$ universality class\index{universality class!K@$K^\star=0$}, we retroactively saw in~\S\ref{sec:ntk_criticality_tanh_univ} that the \terminate{equivalence principle} requires
\be\label{eq:super-tanh-general}
\eta\Lb{\ell}=\eta\widetilde{\lambda}_b\le(\frac{1}{\ell}\ri)^{p_{\perp}}L^{p_{\perp}-1}\, , \qquad \frac{\eta\lamW{\ell}}{n_{\ell-1}}=\frac{\eta\widetilde{\lambda}_W}{n_{\ell-1}}\le(\frac{L}{\ell}\ri)^{p_{\perp}-1}\, ,
\ee
where here we recall our discussion of having separate depth scalings for the bias and weight learning rates~\eqref{eq:layer-independent-rates} as a way to ensure both uniform contributions in parameter groups and in layers to the asymptotic frozen NTK solution~\eqref{eq:frozen-ntk-k-star-solution}.\footnote{
    Please do not confuse these $\ell$-rescalings with $L$-rescalings. The former modifies the learning rates of a given layer $\ell$ by a factor of $\ell$, and the later modifies the learning rates of any layer in the network by the overall network depth $L$.


Also, with the recent critical discussion of the \terminate{exploding and vanishing kernel problem} in mind, you should see that this relative $\ell$-scaling between the bias and weight learning rates is just a polynomial version of the vanishing kernel problem: the \terminate{equivalence principle} ensures that deeper-layer weights receive polynomially non-vanishing gradients as well as contribute polynomially-equally to the training dynamics of other parameters.
}
In particular, for odd smooth activation functions such as $\tanhA$ and $\sinA$ the \terminate{critical exponent} for perpendicular perturbations is given by $p_{\perp}=1$, and we have a simpler prescription:
\be\label{eq:super-tanh}
\eta\Lb{\ell}=\frac{\eta\widetilde{\lambda}_b}{\ell}\, , \qquad \frac{\eta\lamW{\ell}}{n_{\ell-1}}=\frac{\eta\widetilde{\lambda}_W}{n_{\ell-1}}\, .
\ee
With this, we further wonder whether the empirical preference for $\relu$ over $\tanhA$ is at least partially due to the fact that the $\relu$ learning rates are naturally $\ell$-independent \eqref{eq:super-scale-invariant} while the $\tanhA$ learning rates require nontrivial $\ell$-rescalings \eqref{eq:super-tanh}.

In conclusion, while the optimal values of the order-one training hyperparameters $\eta\widetilde{\lambda}_{b}$ and $\eta\widetilde{\lambda}_{W}$ surely depend on the specifics of a task, we expect that the layer $\ell$, depth $L$, and width  $n_{\ell-1}$ scalings dictated by the \terminate{equivalence principle} will lead to the least variation across network architectures with differing widths $n_\ell$ and depths $L$.















































