
\chapter{Kernel Learning}\label{ch:NTHb}
\epigraph{I protest against the use of an infinite quantity as an actual entity; this is never allowed in mathematics. The infinite is only a manner of speaking \dots .}{Carl Friedrich Gauss~\cite{gaussquote}.\index{Gauss, Carl Friedrich}\index{infinity}}

\noindent{}Now that we know essentially everything we can possibly know about the initialization distribution of the preactivations \emph{and the NTK}, it's finally time to learn \emph{with gradients}!

In this chapter, we'll analyze the training of infinite-width neural networks by gradient-descent optimization. Of course, the infinite-width network is really only a manner of speaking, and we cannot actually instantiate one in practice. However, as we saw from our previous finite-width analyses, they can still provide a useful \emph{model} of an actual entity when the depth-to-width ratio 
is sufficiently small.

Thus, the analysis of such networks is important for two reasons. First, this limit can tell us a lot about the correct scaling and tuning of our various hyperparameters; we've already seen this previously as our criticality analysis always begins at infinite width. Second, since our finite-width analysis is perturbative in $1/n$, understanding the infinite-width limit is a prerequisite for us to understand learning with more realistic finite-width networks in \S\ref{ch:features} and \S\ref{ch:eot}. 
With those remarks in mind, let's preview our analysis of gradient-based learning at infinite width.









Just as a new \terminate{biological neural network} begins its journey by taking a small step, so does a freshly-initialized artificial neural network. In \S\ref{sec:small-step} we'll take such a step, observing that the gradient-descent training of infinite-width networks is simply described by the frozen NTK and that the change in the network outputs can be consitently truncated to linear order in the global learning rate.
This simplicity leads us first to observe that the network's output components move independently of each other  (\S\ref{subsec:GD_no_wiring_at_infinity}) and then second to find an absence of representation learning in the hidden layers (\S\ref{subsec:GD_no_RL_at_infinity}).
At this point you might have an uncanny sense of d\'ej\`a vu, as we found the exact same limitations in \S\ref{sec:infinite-posterior} for infinite-width networks that learn via exact Bayesian inference.







After a small step comes a giant leap. In \S\ref{sec:giant-leap} we'll make a large parameter update and find a closed-form solution for a \emph{fully-trained} infinite-width network. For these networks, such a solution memorizes the entire training set, and we'll show that this solution is the same regardless of whether we reach it in one Newton step (\S\ref{subsec:memorization-at-infinity}) or many steps of (stochastic) gradient descent (\S\ref{subsec:algorithmic-independence-at-infinity}), and doesn't depend on the form of the loss that we use (\S\ref{subsec:cross-entropy}). 

In 
fact, in \S\ref{subsec:NTKprediction} we'll see that the prediction of a \emph{particular} fully-trained infinite-width network on unseen test inputs is fixed entirely by the initial network output, the frozen NTK, and the contents of the training set. To analyze this, we evaluate the statistics of the associated \emph{ensemble}, identifying the mean (neural tangent) kernel prediction as well as the covariance of that prediction across different realizations. Recalling our discussion of approximate methods for Bayesian model fitting in \S\ref{subsec:ForIO}, we are now able to make more precise the connection between gradient-based learning and maximum likelihood estimation by discussing the sense in which our distribution over fully-trained infinite-width networks is a generalized posterior distribution.\index{posterior!generalized posterior distribution}\index{generalized posterior distribution|see{posterior distribution}}\index{posterior!generalized posterior distribution|seealso{gradient-based learning}}










In \S\ref{sec:generalization-at-infinity}, we'll put the predictions of these fully-trained infinite-width networks to the test. Here we'll introduce the quantitative measure of training success, the \emph{generalization error}, and decompose it into a \emph{bias} term and a \emph{variance} term. The former term compares the mean predictions of the ensemble on the test inputs to the true function values from the test set, while the latter term measures the instantiation-to-instantiation fluctuations of that prediction across different fully-trained networks in our ensemble. 





Naturally, there is a tradeoff between these bias and variance terms, corresponding to our preference for the ensemble to contain networks that are both flexible \emph{and} confident. By explicitly working out the generalization error when a test input is near one of the training samples in \S\ref{subsec:robustness-from-infinite-GD}, we'll see how balancing such a tradeoff gives a prescription for tuning the initialization hyperparmeters, via the principle of criticality, and for tuning the training hyperparameters, according to the learning rate equivalence principle. 

In \S\ref{subsec:star-polation} we'll extend our analysis to situations where a test input is near two training samples. This will let us understand how our fully-trained networks interpolate and extrapolate, letting us comment  on the activation-function-induced inductive bias of the network output in general. In particular, we'll be able to see how nonlinear activation functions are able to nonlinearly interpolate or extrapolate around two training examples.


Finally, in \S\ref{sec:lazy-kernel} we'll take a small step back to give our discussion of infinite-width networks a broader context. In particular, we'll introduce the linear model, one of the simplest models in traditional machine learning, and explain its relationship to another traditional set of algorithms known as kernel methods. This will let us see that infinite-width MLPs are essentially just linear models based on random features and dually let us identify both the infinite-width Bayesian \emph{kernel} and the frozen neural tangent \emph{kernel} with this more traditional notion of a \emph{kernel}.

After discussing the limitations of such kernel methods, you will thoroughly understand the need to go beyond the infinite-width limit so that our effective theory can fully incorporate some of the more exciting properties of practical deep learning models.
























\section{A Small Step}\label{sec:small-step}\index{small step}\index{small step|seealso{giant leap}}\index{training dynamics!infinite width}
Now let's take our first step in a journey towards the minimum of the loss. We'll begin by considering how the preactivations change in the first step after initialization at $t=0$. 

Recalling that the  evolution of any neural-network observable $\O\!\le(z^{(\ell)}\ri)$  is governed by the NTK
through the update equation \eqref{eq:obsevable-evolution-layer-ell}, we have for an $\ell$-th-layer preactivation: %
\begin{align}\label{eq:GD-preactivation-reprint}
\dz{i}{\delta}{\ell}&\equiv \z{i}{\delta}{\ell}(t=1)- \z{i}{\delta}{\ell}(t=0) \, \\
&=-\eta\sum_{j=1}^{n_{\ell}}\sum_{\tra\in\A}\frac{d\L_{\A}}{d\z{j}{\tra}{\ell}} %
\Tia{\NTK}{ij}{\tra\delta}{\ell}+\o{\eta^2}\, . \notag
\end{align}
Here, the $\ell$-th-layer NTK $\Tia{\NTK}{ij}{\tra\delta}{\ell}$ and the factor $d\L_{\A}/d\z{j}{\tra}{\ell}$ are both evaluated at initialization; from now on we'll drop the explicit dependence on the number of steps $t$ when a quantity is  being evaluated at initialization $t=0$, unless we want to emphasize it for clarity. Henceforth, 
we'll  use the prefix $\dbar$  to indicate the \emph{update} to a quantity after the first step of gradient descent.

Further, in writing \eqref{eq:GD-preactivation-reprint} we have resurrected the sample-index notation of alpha-with-tilde for the inputs in the \terminate{training set} $\tra \in\A$, and we will soon resurrect beta-with-dot for inputs in the \terminate{test set} $\tea\in\B$; as before, we'll also use delta-with-no-decoration for generic inputs in either set: $\delta\in\D=\A\cup\B$. Thus, to be explicitly clear, the update \eqref{eq:GD-preactivation-reprint} gives the change after the first gradient descent training step in an $\ell$-th-layer preactivation evaluated on a sample from either the test set or the training set.




\index{frozen NTK}
Now, let's specialize to the \terminate{infinite-width limit}. Recall in this limit that the NTK self-averages\index{self-averaging}, such that the NTK for \emph{any} particular realization of the network parameters will be equal to the infinite-width NTK mean, which we have been calling the \emph{frozen NTK}: $\Tia{\NTK}{ij}{\tra\delta}{\ell}=\delta_{ij}\Ti{\NTKI}{\tra\delta}{\ell} + \o{1/n}$. With this in mind, the update equation  at infinite width simplifies to 
\be\label{eq:GD-preactivation-at-infty}
\dz{i}{\delta}{\ell}=-\eta\sum_{\tra\in\A}\Ti{\NTKI}{\delta\tra}{\ell}\frac{d\L_{\A}}{d\z{i}{\tra}{\ell}} + \oninv\, .
\ee
Here, the update does not mix \terminate{neural indices} as the mean of the NTK is diagonal in those indices, while the presence of off-diagonal terms in the frozen NTK would indicate that information from one training sample informs the update of another.
Note importantly that we have purposefully truncated the  $+\, \o{\eta^2}$ part of~\eqref{eq:GD-preactivation-reprint} that contains higher-order corrections to the update from the series expansion in the global learning rate\index{learning rate!global} $\eta$; in \S\ref{ch:features} we'll explicitly analyze these $\o{\eta^2}$ terms and show that they are suppressed by $1/n$.
Thus, in the strict infinite-width limit they identically vanish, making the linear truncation exact.


In this section, we'll take a look at what such an infinite-width small-step update entails for the network outputs with $\ell=L$ (\S\ref{subsec:GD_no_wiring_at_infinity}) and for preactivations in the final hidden layer  with $\ell=L-1$ (\S\ref{subsec:GD_no_RL_at_infinity}).\footnote{After reading the next section, it should be clear that the results here are true even at the minimum of the loss at the end of training. We say this now to head off any potential objections of the form, ``What if there are some number of steps $t$ for which the quantity $\eta t/n$ is of order one?''
}
 These analyses will more or less parallel \S\ref{subsec:absence-FF-Bayes} and \S\ref{subsec:absence-RL-Bayes}, where we considered the posterior distribution for infinite-width networks updated via exact \terminate{Bayesian inference}.


\subsection{No Wiring}\label{subsec:GD_no_wiring_at_infinity}\index{training dynamics!infinite width}
Specializing to  the network output $\z{i}{\delta}{L}$, the update~\eqref{eq:GD-preactivation-at-infty} 
simply becomes
\be\label{eq:GD-output-at-infty}
\dz{i}{\delta}{L}=-\eta\sum_{\tra\in\A}\Ti{\NTKI}{\delta\tra}{L}\epsilon_{i;\tra}\, ,
\ee
where we recall the now-familiar \terminate{error factor} defined in \eqref{eq:grad-loss-def} as
\be\label{eq:error-factor-reprint}
\epsilon_{i;\tra}\equiv \frac{\partial \L_\A}{\partial \z{i}{\tra}{L}}\, .
\ee
We're going to extensively analyze this update to network outputs in \S\ref{sec:giant-leap} and onwards. 
Here, let us just point out that the update to the $i$-th feature $\z{i}{\delta}{L}(t=1)$ depends only on the $i$-th component of the error factor $\epsilon_{i;\tra}$. This mirrors the phenomenon of \emph{no wiring} for the network outputs that we observed in \S\ref{subsec:absence-FF-Bayes} for the exact Bayesian inference at infinite width. 

To be more concrete, for the MSE loss\index{loss!MSE}~\eqref{eq:MSE-loss},
\be\label{eq:MSE-loss-reprint}
\L_{\A}\equiv \frac{1}{2}\sum_{i=1}^{n_L}\sum_{\tra\in\A}\Big( \z{i}{\tra}{L}-\y{i}{\tra}\Big)^2 \, ,
\ee
the error factor is simply given by the difference between the true output and the initial output, $\epsilon_{i;\tra}= \z{i}{\tra}{L}-\y{i}{\tra}$. We thus see that all the output components move independently from each other, and there's no way for correlations between these components to be 
learned.\footnote{As another example, we can take a cross-entropy loss\index{loss!cross-entropy} of the form $\L_{\A}=-\sum_{j,\tra} p_{j;\tra} \log(q_{j;\tra})$; feel free to flip forward and look at~\eqref{eq:loss-cross-entropy}. In this case we have 
a target distribution $p_{i;\tra}$ for which we want to fit the \emph{softmax}\index{softmax distribution} distribution -- cf.~\eqref{eq:softmax} -- of the network outputs $q_{i;\tra}\equiv \exp(\z{i}{\tra}{L})/\big[\sum_{k}\exp(\z{k}{\tra}{L})\big]$. 
After noting that $\partial q_{j;\tra}/\partial \z{i}{\tra}{L}=(\delta_{ij}-q_{i;\tra})q_{j;\tra}$, we find for the error factor
\be\label{eq:error-factor-cross-entropy}
\epsilon_{i;\tra}=\sum_{j}\frac{\partial \L_{\A}}{\partial q_{j;\tra}}\frac{\partial q_{j;\tra}}{\partial \z{i}{\tra}{L}}=-\sum_{j}\frac{p_{j;\tra}}{q_{j;\tra}}q_{j;\tra}(\delta_{ij}-q_{i;\tra})=-p_{i;\tra}+\big(\sum_{j}p_{j;\tra}\big)q_{i;\tra}=q_{i;\tra}-p_{i;\tra}\, .
\ee
Therefore, in this case too we see that the error factor $\epsilon_{i;\tra}$ depends only on the $i$-th component of the softmax\index{softmax distribution} distribution, and no correlation between output components will be generated.\index{error factor!cross-entropy loss}}
In addition, since \eqref{eq:GD-output-at-infty} is a stochastic equation describing the update to any particular network in the ensemble, there is no wiring for any particular realization of a one-step-into-training infinite-width network.
\index{gradient descent!wiring!infinite width}


\subsection{No Representation Learning}\label{subsec:GD_no_RL_at_infinity}\index{training dynamics!infinite width}
We'll have to work a little harder to analyze the update to the preactivations in the penultimate layer $\z{i}{\delta}{L-1}$. To start, we can evaluate the derivative of the loss in the update equation~\eqref{eq:GD-preactivation-at-infty} using the \emph{backward} equation\index{backward equation!MLP} \eqref{eq:backward-pass}:
\be
\frac{d\L_{\A}}{d\z{j}{\tra}{L-1}}=\sum_{i=1}^{n_{L}}\frac{\partial\L_{\A}}{\partial\z{i}{\tra}{L}}\frac{d\z{i}{\tra}{L}}{d\z{j}{\tra}{L-1}}=\sum_{i=1}^{n_{L}}\frac{\partial\L_{\A}}{\partial\z{i}{\tra}{L}}\,\W{ij}{L}\ds{j}{\tra}{L-1}\, .
\ee
Substituting this into the update \eqref{eq:GD-preactivation-at-infty} at $\ell = L-1$, we get a stochastic equation describing the change in the final hidden-layer representation for any particular network:
\be\label{eq:stochastic-penultimate-update}
\dz{j}{\delta}{L-1}=-\eta\sum_{i=1}^{n_{L}}\sum_{\tra\in\A}\Ti{\NTKI}{\delta\tra}{L-1} \frac{\partial\L_{\A}}{\partial\z{i}{\tra}{L}}\, \W{ij}{L}\ds{j}{\tra}{L-1}\, .
\ee 
To make progress here, we're going to have to analyze the distribution over such updates.

First, the mean update is given by
\be\label{eq:expected-move-penultimate-infty-intermediate}
\E{\dz{j}{\delta}{L-1}}=-\eta\sum_{i=1}^{n_{L}}\sum_{\tra\in\A}\Ti{\NTKI}{\delta\tra}{L-1}\E{\frac{\partial\L_{\A}}{\partial\z{i}{\tra}{L}}\,\W{ij}{L}\ds{j}{\tra}{L-1}}\, .
\ee
This expectation involves an \terminate{interlayer correlation} between the error factor $\partial \L_\A/\partial \z{i}{\tra}{L}$ 
from the $L$-th layer and the derivative of the activation $\ds{j}{\tra}{L-1}$ from the $(L-1)$-th layer, in addition to a weight insertion $\W{ij}{L}$. From our previous experience we know that such interlayer expectations are suppressed by a factor of $1/n$, vanishing in the strict infinite-width limit. To be extra careful,
 let's compute the mean explicitly.




\index{training dynamics!infinite width}
To do so, recall our \terminate{generating function} for interlayer correlations\index{interlayer correlation}~\eqref{eq:master-weight-insertion} and specialize to the penultimate layer $\ell=L-1$. (You'll probably want to flip back and remind yourself.) Since we have not previously evaluated the case with a single weight insertion, let's calculate and record it here. Differentiating the generating function once with respect to the source as $\frac{d}{d\JW{ij}}$ and then setting the source to zero, we get
\be\label{eq:one-weight-insertion}
\E{\O\!\le(z^{(L)}\ri) \W{ij}{L} \mathcal{Q}\!\le(z^{(L-1)}\ri)}=\frac{\CW{L}}{n_{L-1}}\sum_{\delta\in\D}\E{\bra\!\!\!\bra\frac{\partial \O}{\partial \z{i}{\delta}{L}} \ket\!\!\!\ket_{\!\!\!\widehat{G}^{(L)}}\!\!  \s{j}{\delta}{L-1} \mathcal{Q}\!\le(z^{(L-1)}\ri)}\, .
\ee
Applying this formula to the above expression for our update \eqref{eq:expected-move-penultimate-infty-intermediate},
we get
\begin{align}\label{eq:infinite-width-no-rl}
\E{\dz{j}{\delta}{L-1}}&=-\eta\,\frac{\CW{L}}{n_{L-1}}\sum_{i=1}^{n_{L}}\sum_{\tra_1\tra_2\in\A}\!\!\Ti{\NTKI}{\delta\tra_1}{L-1}\,\E{\bra\!\!\!\bra \frac{\partial^2 \L_\A}{\partial \z{i}{\tra_1}{L}\partial \z{i}{\tra_2}{L}} \ket\!\!\!\ket_{\!\!\!\widehat{G}^{(L)}}\!\!\s{j}{\tra_2}{L-1}\ds{j}{\tra_1}{L-1}}\, \\
&=-\eta\,\frac{\CW{L}}{n_{L-1}}\sum_{\tra_1,\tra_2\in\A}\!\Ti{\NTKI}{\delta\tra_1}{L-1} \sum_{i=1}^{n_{L}}\bra\!\!\!\bra \frac{\partial^2 \L_\A}{\partial \z{i}{\tra_1}{L}\partial \z{i}{\tra_2}{L}} \ket\!\!\!\ket_{\!\!\!K^{(L)}}\!\! \bra \sigma^{\prime}_{\tra_1}\sigma_{\tra_2} \ket_{K^{(L-1)}}\!+\!\o{\frac{1}{n^2}}\, ,\notag
\end{align}
and see that this expression is manifestly suppressed by $1/n$. Thus, on average across our ensemble, we have found that there's no representation learning in the penultimate layer in the infinite-width limit.

Next, let's consider the variance of the update~\eqref{eq:stochastic-penultimate-update}:
\begin{align}\label{eq:variance-move-penultimate-infty}
 &\E{\dz{j_1}{\delta_1}{L-1} \dz{j_2}{\delta_2}{L-1}} \\
 =&\eta^2 \sum_{i_1,i_2=1}^{n_{L}}\sum_{\tra_1 \tra_2 \in\A}\Ti{\NTKI}{\delta_1\tra_1}{L-1}\Ti{\NTKI}{\delta_2\tra_2}{L-1}\,\E{\frac{\partial\L_{\A}}{\partial\z{i_1}{\tra_1}{L}}\frac{\partial\L_{\A}}{\partial\z{i_2}{\tra_2}{L}}\,\W{i_1j_1}{L} \W{i_2j_2}{L}\ds{j_1}{\tra_1}{L-1}\ds{j_2}{\tra_2}{L-1} }\, \notag \\
 =& \delta_{j_1 j_2} \, \eta^2 \frac{\CW{L} }{n_{L-1}} \sum_{\tra_1 \tra_2 \in\A}\Ti{\NTKI}{\delta_1\tra_1}{L-1}\Ti{\NTKI}{\delta_2\tra_2}{L-1} \sum_{i=1}^{n_L}\bra \!\!\!\bra\frac{\partial\L_{\A}}{\partial\z{i_1}{\tra_1}{L}}\frac{\partial\L_{\A}}{\partial\z{i_2}{\tra_2}{L}}\ket\!\!\!\ket_{\ker^{(L)}} \braket{\dsNL{\tra_1}\dsNL{\tra_2} }{L-1}  + \o{\frac{1}{n^2}}\, \notag .
\end{align}
In the last step, we applied our interlayer correlation\index{interlayer correlation} formula with two weight insertions \eqref{eq:two-weight-insertions-general} and then picked up the leading contribution.\footnote{Note that the second term in \eqref{eq:two-weight-insertions-general} is of order $\o{1/n^2}$ and hence subleading.
} With this, we see that the covariance of the update,
\begin{align}
\cov{\dz{j_1}{\delta_1}{L-1}}{\dz{j_2}{\delta_2}{L-1} } &\equiv\E{\dz{j_1}{\delta_1}{L-1} \dz{j_2}{\delta_2}{L-1}} - \E{\dz{j_1}{\delta_1}{L-1}}\E{\dz{j_2}{\delta_2}{L-1}}=\oninv  \, ,
\end{align}
is manifestly suppressed by $1/n$, 
vanishing in the strict infinite-width limit. Since the distribution of updates to the penultimate-layer preactivations has a vanishing mean and covariance, we conclude that the distributions before and after the learning update are equal: mirroring what we found for exact Bayesian inference in \S\ref{subsec:absence-RL-Bayes}, there's no representation learning for gradient-based learning in the infinite-width limit.\footnote{
    You can check the higher-order connected correlators of the update distribution, if you'd like. However, as we already said before we started these computations, these sorts of interlayer correlations\index{interlayer correlation} are naturally suppressed by factors of $1/n$, and so will be the higher-order connected correlators.
}

\section{A Giant Leap}\label{sec:giant-leap}  \index{giant leap}\index{giant leap|seealso{small step}}\index{training dynamics!infinite width}
\epigraph{That's one \terminate{small step} for [a] machine, one giant leap for AI.}{
Neil AI-Strong\index{Armstrong, Neil}
}




\index{gradient descent}\index{training}
\noindent{}In the last section, we started to understand training for infinite-width networks by taking a small step of \terminate{gradient descent}. 
Of course, what we'd actually like is to understand the behavior of fully-trained networks at the minimum of their losses.
Naturally, we could continue by taking many many small steps until our networks are fully trained, and
indeed this is how networks are typically trained in practice. %




That said, in 
\S\ref{subsec:memorization-at-infinity} we'll first show that we can actually fully train infinite-width networks in \emph{one} theoretical gradient-descent step. That is, we can take a \emph{giant leap} right to the minimum of the loss. We'll then explain in \S\ref{subsec:algorithmic-independence-at-infinity} that the theoretical minimum
we've found by our giant leap is the same minimum we would have found in practice by taking many steps of gradient descent, or even by using \neo{stochastic gradient descent} with decreasing learning rates. This equivalence makes our giant leap a powerful theoretical tool.
After a brief aside about the cross-entropy loss in \S\ref{subsec:cross-entropy},
finally in \S\ref{subsec:NTKprediction} we'll see how our fully-trained infinite-width networks make predictions on previously unseen examples, though a detailed analyses of these test-set predictions will be postponed until the following  section.%







\subsection{Newton's Method}\label{subsec:memorization-at-infinity}\index{training dynamics!infinite width}
Our first goal is to find a single step such that the network outputs equal the true outputs, 
\be\label{eq:fully-trained-condition}
\z{i}{\tra}{L}(t=1) = \y{i}{\tra} \, , 
\ee
for all samples $x_{\tra}$ in the training set $\tra\in\A$. This condition will be our definition of \emph{fully trained}, and it's easy to see that such a condition will minimize the training loss for any of the loss functions that we've described.\index{fully-trained condition!infinite width}
Recalling the gradient-descent update for neural-network outputs~\eqref{eq:GD-output-at-infty} and rearranging terms, we see that our giant-leap update must satisfy
\be\label{eq:what-we-want-rearranged}
\z{i}{\tra_1}{L}-\y{i}{\tra_1}=\eta\sum_{\tra_2 \in\A} \Ti{\NTKIsub}{\tra_1 \tra_2}{L} \frac{\partial \L_{\A}}{\partial \z{i}{\tra_2}{L}}\, 
\ee
for the network to be fully trained. 

As a reminder, our convention is that quantities without an explicit step argument are evaluated at the point of initialization $t=0$; in particular, the constraint \eqref{eq:what-we-want-rearranged} is written solely in terms of the quantities at initialization. Additionally, note that the tilde on $\Ti{\NTKIsub}{\tra_1 \tra_2}{L}$ emphasizes that it's a $\NR\times\NR$-dimensional submatrix of the full frozen NTK matrix $\Ti{\NTKI}{\delta_1 \delta_2}{L}$ evaluated on  pairs of inputs $(x_{\tra_1}, x_{\tra_2})$ in the \terminate{training set} $\A$ \emph{only}. This emphasis will soon prove itself useful, as it did before in \S\ref{ch:bayesian-inference}.







How can we satisfy our giant-leap condition \eqref{eq:what-we-want-rearranged}? Since the left-hand side is exactly the error factor of the \emph{MSE loss}\index{loss!MSE}~\eqref{eq:MSE-loss-reprint}, let's first specialize to the MSE loss. Plugging in~\eqref{eq:MSE-loss-reprint} for $\L_{\A}$, we get a concrete equation to solve: 
\be\label{eq:newton-minimum-implicit}
\z{i}{\tra_1}{L}-\y{i}{\tra_1}=\sum_{\tra_2 \in\A}\eta \Ti{\NTKIsub}{\tra_1 \tra_2}{L}\le(\z{i}{\tra_2}{L}-\y{i}{\tra_2}\ri) \,.
\ee
However, for generic neural networks, the frozen NTK\index{frozen NTK} $\Ti{\NTKI}{\tra_1 \tra_2}{L}$ will have nonzero off-diagonal components mixing different sample indices.
This unfortunately means that the condition \eqref{eq:newton-minimum-implicit} is impossible to satisfy by the tuning of the single global learning rate $\eta$\index{learning rate!global}.

Said another way, the issue is that our global learning rate $\eta$ is a \emph{scalar}, but here we need it to be \neo{tensor} in order to undo the mixing of the sample indices by the frozen NTK\index{frozen NTK}.
To enable this, we need to further generalize gradient descent. In our first extension of the gradient descent algorithm \eqref{eq:gd-update-lambda} -- discussed under the heading \emph{Tensorial Gradient Descent}\index{gradient descent!tensorial} -- we introduced a \emph{learning-rate tensor}\index{learning rate!learning-rate tensor} on \terminate{parameter space},
\be\label{eq:generalization-original}
\eta\to\eta\lambda_{\mu\nu} \, ,
\ee
which let us mediate how each model parameter individually contributes to the gradient-descent update of the others and let us take steps with unequal magnitudes in various directions in parameter space.  The consequence of having such a learning-rate tensor was integrated into the definition of the NTK and then informed our analyses in \S\ref{ch:training}--\S\ref{ch:eft-ntk}.\footnote{Most importantly, this let us scale the effective learning rate differently for the biases and weights; we saw in \S\ref{sec:NTH-recursions} and then in \S\ref{sec:EVGP-WEP} that this was essential for ensuring that both parameter groups get properly trained. Also, please remember that, even when written generally as $\lambda_{\mu\nu}$, our learning-rate tensor is restricted such that it does not mix parameters from different layers.}



\index{training dynamics!infinite width}
Now, we need to further extend this generalization to sample indices as
\be\label{eq:generalization-of-generalization}
\eta\lambda_{\mu\nu}\to\eta \lambda_{\mu\nu}\kappa^{\tra_1\tra_2}\, ,
\ee
where we have introduced a new symmetric matrix $\kappa^{\tra_1\tra_2}$ that we will call the \term{Newton tensor}\index{Newton tensor|seealso{second-order update}}\index{tensor!Newton tensor|see{Newton tensor}}.
This enables us to take an anisotropic step in \terminate{sample space} as well.
Specifically, we extend the parameter update equation \eqref{eq:gd-update-lambda} to
\begin{align}\label{eq:second-order-update}
\dtheta_\mu \equiv\theta_\mu(t=1)-\theta_\mu(t=0) &=  
- \sum_{\nu,\tra_1,\tra_2,i}\eta \lambda_{\mu\nu} \kappa^{\tra_1\tra_2}\frac{\td \z{i}{\tra_1}{L}}{\td \theta_\nu}\frac{\partial \L_{\A}}{\partial \z{i}{\tra_2}{L}} \, ,
\end{align}
which we will call a \term{second-order update}\index{second-order update|seealso{Newton tensor}}.\footnote{The name of this update descends from the fact that similar updates are used to define optimization algorithms that
incorporate information from the second derivative of the loss. Such algorithms are generally called \emph{second-order methods}\index{second-order method (optimization)}\index{second-order method (optimization)|seealso{Newton's method}}. We will show shortly that this new algorithm minimizes the loss just as well (better, actually).}
Plugging this second-order update 
into our expansion for the network outputs, we get
\begin{align}\label{eq:expansion-outputs}
\z{i}{\delta_1}{L}(t=1) &= \z{i}{\delta_1}{L} + \sum_\mu\frac{\td \z{i}{\delta_1}{L}}{d \theta_\mu}\dtheta_\mu + \o{\frac{1}{n}}  \, \\
&=\z{i}{\delta_1}{L} - \eta\sum_{\tra_2, \tra_3 \in\A}  \Ti{\NTKI}{\delta_1 \tra_2}{L} \kappa^{\tra_2 \tra_3}\frac{\partial \L_{\A}}{\partial \z{i}{\tra_3}{L}} + \oninv \, .\notag
\end{align}
Substituting this update into our fully-trained condition~\eqref{eq:fully-trained-condition} and still using the MSE loss\index{loss!MSE}, we get a new constraint
\be\label{eq:newton-minimum-implicit-satisfiable}
\z{i}{\tra_1}{L}-\y{i}{\tra_1}=\sum_{\tra_2,\tra_3 \in\A}\le(\eta \Ti{\NTKIsub}{\tra_1 \tra_2}{L}\kappa^{\tra_2\tra_3}\ri)\le(\z{i}{\tra_3}{L}-\y{i}{\tra_3}\ri) \, .
\ee
We'll satisfy this constraint shortly.







\index{training dynamics!infinite width}
For a different perspective on this new second-order update, rather than modifying the optimization algorithm, we can instead find the same constraint \eqref{eq:newton-minimum-implicit-satisfiable} by adopting a different loss. Consider a \emph{generalized} MSE loss\index{loss!MSE!generalized}
\be\label{eq:loss-MSE-gen}
\Laux{\A}(\theta) =\frac{1}{2}\sum_{i=1}^{n_L} \sum_{\tra_1,\tra_2 \in \A}\kappa^{\tra_1 \tra_2}\le(\z{i}{\tra_1}{L}-\y{i}{\tra_1}\ri)\le(\z{i}{\tra_2}{L}-\y{i}{\tra_2}\ri) \, ,
\ee
where here the \terminate{Newton tensor} $\kappa^{\tra_1 \tra_2}$ acts as a \emph{metric}\index{Newton tensor!as a metric on sample space} on \terminate{sample space}.\footnote{Similarly, we could have taken the perspective that the learning-rate tensor\index{learning rate!learning-rate tensor} $\lambda_{\mu\nu}$ acts as a metric on \neo{parameter space}.

Note also that with this interpretation, the standard MSE loss is just the generalized MSE loss with the Euclidean metric $\kappa^{\tra_1 \tra_2} \to \delta^{\tra_1 \tra_2}$. In some sense, it's more pleasing to write it this way if you're familiar with general relativity\index{general relativity}; writing the \terminate{Newton tensor} with \terminate{sample indices} \emph{raised} allows us to adopt a rule of only summing over sample indices when they come in a raised-lowered pair. Similarly, note that the insertion of the Newton tensor in our \terminate{second-order update} \eqref{eq:second-order-update} follows this pattern as well.
} For this loss the derivative with respect to the network output -- i.e.~the error factor --
is now given by
\be
\frac{\partial \L_{\A}}{\partial \z{i}{\tra_2}{L}}=\sum_{\tra_3\in\A}\kappa^{\tra_2 \tra_3}\le(\z{i}{\tra_3}{L}-\y{i}{\tra_3}\ri)\, .
\ee
Substituting this error factor into our condition for being fully trained~\eqref{eq:what-we-want-rearranged}, we find the same constraint~\eqref{eq:newton-minimum-implicit-satisfiable} using a standard gradient-descent update with our generalized MSE loss \eqref{eq:loss-MSE-gen} as we did just before using our \terminate{second-order update}~\eqref{eq:second-order-update} with the standard MSE loss. Either perspective is a valid way to think about our theoretical optimization and, as we will explain more generally in \S\ref{subsec:algorithmic-independence-at-infinity},  any of these choices of algorithms and losses will lead to the same fully-trained network.








\index{training dynamics!infinite width}
Now, let's find the solution to our giant-leap constraint \eqref{eq:newton-minimum-implicit-satisfiable}. By inspection, this is satisfiable if we can set the term in the first parenthesis to the identity matrix
\be
\sum_{\tra_2 \in\A}\eta \Ti{\NTKIsub}{\tra_1 \tra_2}{L}\kappa^{\tra_2\tra_3} = \delta_{\tra_1}^{\ \tra_3} \, ,
\ee
which we can ensure by setting the product of the global learning rate\index{learning rate!global} and the \terminate{Newton tensor} as
\be\label{eq:newtons-method-update}
\eta \kappa^{\tra_1\tra_2}=\TI{\NTKIsub}{\tra_1 \tra_2}{L} \, .
\ee
Here, the object on the right-hand side is the \emph{inverse} of the $\NR\times\NR$-dimensional submatrix $\Ti{\NTKIsub}{\tra_1 \tra_2}{L}$, which as a reminder is evaluated on pairs of inputs $(x_{\tra_1}, x_{\tra_2})$ in the \terminate{training set} $\A$ \emph{only}, and is defined via the equation
\be\label{eq:training-set-ntk-inverse}
 \sum_{\tra_2\in\A} \TI{\NTKIsub}{\tra_1 \tra_2}{L} \, \Ti{\NTKIsub}{\tra_2 \tra_3}{L} =\delta^{\tra_1}_{\ \tra_3}\, .
\ee
Similarly to our work in \S\ref{sec:infinite-posterior} on infinite-width \terminate{Bayesian inference}, the decoration of these submatrices with tildes is useful in order to clearly distinguish these submatrices from submatrices that also involve the \neo{test set} $\B$. 
Also, as before for the kernel and its inverse, we will always denote the NTK inverse by an object with sample indices \emph{raised}.

The algorithm with the particular choice \eqref{eq:newtons-method-update} is known as \term{Newton's method} (which acausally explains why we called $\kappa^{\tra_1 \tra_2}$ the \neo{Newton tensor}).\footnote{\emph{Newton's method}\label{footnote:newtons-method} is a numerical method for finding a zero of a function. For simplicity of presentation, let's take a single-variable function $g(x)$ and suppose that we want to find a solution to the equation $g\!\le(x_\star\ri)=0$; note that this is equivalent to extremizing a function $L(x)$ whose derivative is $g(x)$, i.e.~$L'(x)=g(x)$.
\terminate{Newton's method} instructs us to start with some guess $x_{0}$ and then iterate as
    \be\label{eq:Newtons-method}
       x_{t+1} = x_{t} - \frac{g\!\le(x_{t}\ri)}{g'\!\le(x_{t}\ri)}= x_{t} - \frac{L'\!\le(x_{t}\ri)}{L''\!\le(x_{t}\ri)}\, .
    \ee 
This algorithm is based on making a linear approximation $g'(x_{t})\approx [g(x_{t+1})-g(x_{t})]/(x_{t+1}-x_{t})$ and then solving for $g(x_{t+1})=0$.
In general, one needs to iterate \eqref{eq:Newtons-method} for several steps in order to get a good approximate solution $x_{\star}$.
When the function is linear as $g(x)=a (x-x_{\star})$, however, we get
 \be
 x_{1}=x_0-\frac{a(x_0-x_{\star})}{a}= x_{\star}\, ,
 \ee
for any starting point $x_0$.
Hence Newton's method can land right on the solution in one step, just like our giant leap \eqref{eq:parameters-update-reprint-generalized} did.

\index{training dynamics!infinite width}
The right-hand side of \eqref{eq:Newtons-method} offers another perspective: Newton's method is gradient descent with a ``loss'' $L(x)$ and a learning rate set as $\eta_t=1/L''(x_{t})$. To see why this is a good choice for the learning rate, let's choose a generic learning rate $x_{t+1}=x_t-\eta_t L'(x_t)$ and Taylor-expand the updated loss $L(x_{t+1})$ to the second order in $\eta_t$:
\be
L\!\le(x_{t+1}\ri) = L\!\le(x_{t}\ri) - \eta_t L'\!\le(x_{t}\ri)^2 + \frac{\eta_t^2}{2} L'\!\le(x_{t}\ri)^2L''\!\le(x_{t}\ri)  + \o{\eta_t^3}\, .
\ee
Optimizing the learning rate, we see that the truncated expression on the right-hand side is minimized when $\eta_t=1/L''(x_{t})$. In particular, for a quadratic function $L(x)=a(x-x_{\star})^2/2$ this truncation is exact, and Newton's method again reaches the minimum in one step.
This also makes it clear why optimization algorithms based on Newton's method\index{Newton's method! as a second-order method} fall in the class of \emph{second-order methods}\index{second-order method (optimization)}: each iteration uses second-order information from the function -- the second derivative $L''(x)$ -- to set the locally optimal learning rate. 
Our giant leap expressed in \eqref{eq:parameters-update-reprint-generalized} and \eqref{eq:network-step-general} is doing exactly that -- successfully -- for the parameter optimization and for the function approximation, respectively.
}
With it, we can simply write down a solution that fully trains the network in one step:
\begin{align}
\theta_\mu^\star &= \theta_\mu(t=0)  - \sum_{\nu,\tra_1,\tra_2,i}\lambda_{\mu\nu} \frac{\td \z{i}{\tra_1}{L}}{\td \theta_\nu} \TI{\NTKIsub}{\tra_1 \tra_2}{L}\le(\z{i}{\tra_2}{L}-\y{i}{\tra_2}\ri) \, . \label{eq:parameters-update-reprint-generalized}
\end{align}
In particular, this is exactly what we'd find by setting the gradient of the loss to zero and solving for the optimal parameters as in~\eqref{eq:gradient-vanishing-mind}.
As we explained back there, such a direct and explicit solution to an optimization problem is only available in special cases, and
it turns out that this is precisely the case at infinite width.\footnote{We'll later show in \S\ref{sec:another-leap} that perturbative solutions are possible at finite width.}






\index{training dynamics!infinite width}
Plugging the Newton's method update in our expansion for the network outputs \eqref{eq:expansion-outputs}, we then find the fully-trained network output for a general input $\delta\in\D$: 
\be
\z{i}{\delta}{L}(t=1)=\z{i}{\delta}{L}- \sum_{\tra_1, \tra_2 \in\A}  \Ti{\NTKI}{\delta \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L}\le(\z{i}{\tra_2}{L}-\y{i}{\tra_2}\ri) \, .\label{eq:network-step-general}
\ee
In particular,
for samples in the training set $\A$, the network output equals the true output $\z{i}{\tra}{L}(t=1)=\y{i}{\tra}$, satisfying our condition for the network to be fully trained~\eqref{eq:fully-trained-condition}.
In other words, our network has perfectly memorized the entire training set.\footnote{Since infinite-width networks have infinite parameters, it shouldn't be surprising that in this limit the network can memorize the finite training set.} 
As such, this solution \eqref{eq:parameters-update-reprint-generalized} also minimizes \emph{any} loss $\L_\A(\theta)$
that is minimized by setting the network outputs to the true outputs $z^{(L)}(x_{\tra};\theta)=\y{i}{\tra}$:
\be\label{eq:loss-minimization-by-Newton}
\theta^\star_{\text{Newton}} = \argmin_\theta\, \L_\A(\theta)\, .
\ee
This means that regardless of whether we used the standard MSE loss \eqref{eq:MSE-loss-reprint} or the generalized MSE loss~\eqref{eq:loss-MSE-gen} (or an entirely different loss as long as it has a minimum at $z^{(L)}(x_{\tra};\theta)=\y{i}{\tra}$), our solution \eqref{eq:network-step-general} will faithfully describe the minimum.\footnote{
    Note that the solution~\eqref{eq:parameters-update-reprint-generalized} depends on the network output at initialization $\z{i}{\delta}{L}$, which ultimately depend on the initialization of the parameters $\theta_{\text{init}}=\theta(t=0)$. For different initializations, we will reach different solutions~\eqref{eq:parameters-update-reprint-generalized}, each of which will minimize the loss given that particular initialization $\theta_{\text{init}}$. We'll have more to say about this in \S\ref{subsec:NTKprediction}.
}













    






\subsection{Algorithm Independence}\label{subsec:algorithmic-independence-at-infinity}
Now let's discuss a related -- and by now well anticipated -- property of the infinite-width limit: given a particular initialization $\z{i}{\delta}{L}(t=0)$ and the frozen NTK $\Ti{\NTKI}{\delta \tra_1}{L}$, we'll always get to exactly the same minimum \eqref{eq:network-step-general}, whether we get there by one giant leap or we get there by a sequence of many small steps. That is, at infinite width we have \term{algorithm independence}.

\index{learning rate!global!step-dependent}
Let's suppose that we have taken $T-1$ steps towards the minimum with a global learning rate $\eta(t)$, a \terminate{Newton tensor} $\kappa^{\tra_1\tra_2}(t)$, and loss $\L_{\A}(t)$, where these quantities
will in general depend on the step $t$. Different choices of $\eta(t)$, $\kappa^{\tra_1\tra_2}(t)$, and $\L_{\A}(t)$ will lead to different optimization algorithms; included in this class are \terminate{Newton's method}, \terminate{gradient descent}, and \terminate{stochastic gradient descent} (SGD).\footnote{To see how this includes SGD~\eqref{eq:sgd-update}, note that we can either restrict the loss to be a summation over a different \neo{batch} $\mathcal{S}_t\subset \A$ at each step $t$ as $\L_{\A}(t)= \L_{\mathcal{S}_t}$, or equivalently we can choose the Newton tensor $\kappa^{\tra_1\tra_2}(t)$ to project onto the subset $\mathcal{S}_t$.}
Iterating the update~\eqref{eq:expansion-outputs}, the network outputs accumulate the changes as
\be\label{eq:general-gd-steps}
\z{i}{\delta}{L}(T-1)=\z{i}{\delta}{L}(t=0)-\sum_{t=0}^{T-2}\sum_{\tra_1,\tra_2}\Ti{\NTKI}{\delta\tra_1}{L}\,\eta(t)\,\kappa^{\tra_1\tra_2}(t) \, \epsilon_{i;\tra_2}(t) \, ,
\ee
where $\epsilon_{i;\tra_2}(t)\equiv\partial \L_{\A}(t)/\partial \z{i}{\tra_2}{L}$ is the error factor for the training loss $\L_{\A}(t)$ evaluated with respect to the network output $\z{i}{\tra}{L}(t)$ at step $t$. 


Let's then suppose that in the next step $t=T$ that we reach the true minimum. We can ensure this by taking a Newton step from $\z{i}{\delta}{L}(T-1)$ such that
\be\label{eq:newton-at-last-intermediate}
\z{i}{\delta}{L}(T)=\z{i}{\delta}{L}(T-1)-\sum_{\tra_1,\tra_2}\Ti{\NTKI}{\delta\tra_1}{L}\TI{\NTKIsub}{\tra_1\tra_2}{L}\le[\z{i}{\tra_2}{L}(T-1)- \y{i}{\tra_2}\ri]\, ,
\ee
where here we set $\eta(T-1)\, \kappa^{\tra_1\tra_2}(T-1)=\TI{\NTKIsub}{\tra_1\tra_2}{L}$ and chose the standard MSE loss at $t=T-1$ with $\epsilon_{i;\tra_2}(T-1)=\z{i}{\tra_2}{L}(T-1)-\y{i}{\tra_2}$.\footnote{Note that if we had already reached a minimum at step $T-1$, then this last Newton's step in \eqref{eq:newton-at-last-intermediate} would give no change to the network outputs: $\z{i}{\delta}{L}(T)=\z{i}{\delta}{L}(T-1)$. Thus, our argument also applies to any algorithm that already reached a minimum with $T-1$ other steps, and we do not have to actually apply the Newton step in practice. We'll address this point again in the next-to-next footnote.
}
Plugging in our expression for the network outputs after the first $T-1$ steps~\eqref{eq:general-gd-steps}, we see
\begin{align}
\z{i}{\delta}{L}(T)=&\z{i}{\delta}{L}(t=0)-\sum_{t=0}^{T-2}\sum_{\tra_1,\tra_2}\Ti{\NTKI}{\delta\tra_1}{L}\, \eta(t)\, \kappa^{\tra_1\tra_2}(t)\, \epsilon_{i;\tra_2}(t)\, \notag\\
&-\sum_{\tra_1,\tra_2}\!\Ti{\NTKI}{\delta\tra_1}{L}\TI{\NTKIsub}{\tra_1\tra_2}{L}\le\{\le[\z{i}{\tra_2}{L}(t=0)\!-\!\!\sum_{t=0}^{T-2}\sum_{\tra_3,\tra_4}\!\Ti{\NTKIsub}{\tra_2\tra_3}{L}\, \eta(t)\, \kappa^{\tra_3\tra_4}(t)\, \epsilon_{i;\tra_4}(t)\ri]\!-\!\y{i}{\tra_2}\!\ri\}\, \notag\\
=&\z{i}{\delta}{L}(t=0)- \sum_{\tra_1, \tra_2 \in\A}  \Ti{\NTKI}{\delta \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L}\le[\z{i}{\tra_2}{L}(t=0)-\y{i}{\tra_2}\ri]\, ,\label{eq:newton-at-last}
\end{align}
where to go from the second to the third line we made use of the defining equation for the NTK submatrix inverse~\eqref{eq:training-set-ntk-inverse}, thus enabling the cancellation. What this result shows is that all the details of the training algorithm in the previous steps $\big\{\eta(t), \kappa^{\tra_1\tra_2}(t), \L_{\A}(t)\big\}_{t=0,\ldots, T-2}$  were erased: the network output $\z{i}{\delta}{L}(T)$ after our final step $t=T$ here in~\eqref{eq:newton-at-last} is exactly the same as the network output reached after one giant leap~\eqref{eq:network-step-general}.\footnote{In \S\ref{subsec:real-GD-at-finite-width}, we'll explicitly analyze the dynamics of another optimization algorithm -- many many steps of vanilla gradient descent \eqref{eq:gd-update-lambda} -- and evaluate its corresponding fully-trained solution. As expected by \terminate{algorithm independence} \eqref{eq:newton-at-last}, in the infinite-width limit this solution agrees completely with other solutions obtained by different training algorithms.}



Thus, in the infinite-width limit the fully-trained solution is determined by \emph{(i)} the frozen NTK\index{frozen NTK} $\Ti{\NTKI}{\delta\tra}{L}$, with its details depending on the \neo{training hyperparameters} in the learning-rate tensor\index{learning rate!learning-rate tensor} $\lambda_{\mu\nu}$,
\emph{(ii)} the initial newtork outputs $\z{i}{\delta}{L}(t=0)$, with its distribution depending on the \neo{initialization hyperparameters}, and \emph{(iii)} the true outputs $\y{i}{\tra}$ for the training set $\A$. It doesn't matter which loss function we used, e.g.~MSE\index{loss!MSE} or cross-entropy\index{loss!cross-entropy}, how many steps we took to get to the minimum, or whether we used gradient descent or SGD\index{stochastic gradient descent}.\footnote{
    Some additional comments that didn't make the cut for the main body:
    \bi
    \item \terminate{Newton's method} is often impractical to implement directly, since we have to compute the inverse of the frozen NTK submatrix evaluated on the entire training set, $\TI{\NTKIsub}{\tra_1 \tra_2}{L}$, similar to how we had to invert the kernel for \terminate{Bayesian inference} in \S\ref{subsec:absence-FF-Bayes}. Unlike the case of exact Bayesian inference where we were considering the feasibility of the learning algorithm, here the point is that Newton's method is a theoretical tool that lets us describe a fully-trained  extremely-wide network, even if the network was trained very practically by a many-step version of (stochastic) gradient descent.
    \item Often theorists will take the limit of a very small step size and approximate the optimization dynamics with an ordinary differential equation (ODE)\index{gradient descent!continuum or ODE limit}. Such an approximation is sometimes misleading, and here we see that it's entirely unnecessary.
    \item For SGD to actually converge to a minimum, you need to decrease the learning rate over the course of training, otherwise the network will fluctuate around, but never actually reach, the minimum. Intuitively, this is because at each step the optimization problem does not include the entire training set.
    \item A curious reader might wonder what happens if one cannot take a final step according to \terminate{Newton's method}, for instance because it's impractical to invert the frozen NTK submatrix. In fact, if you're already close to the minimum at $t=T-1$, i.e.~if you're essentially at the end of training, then the final step to land exactly on the minimum will be extremely small, and the solution \eqref{eq:newton-at-last} is a very good approximation of the network before taking this last theoretical jump.
    \ei
}
Said another way, \neo{algorithm independence} means that these hyperparameters and training set uniquely specify the statistics of fully-trained networks in the infinite-width limit; \terminate{Newton's method} is just a nice theoretical trick to leap right to the solution.
In this way, we can use the giant-leap solution \eqref{eq:network-step-general} to study the outcome of all these different optimization algorithms, which is what we'll do after a brief aside about the cross-entropy loss.\index{loss!cross-entropy}





\subsection{\emph{Aside}: Cross-Entropy Loss}\label{subsec:cross-entropy}
Let's take a brief aside to bring the \emph{cross-entropy loss}\index{loss!cross-entropy} out of the footnotes and into the main body. In general, the cross-entropy loss for some dataset\index{input data} $\D$ is defined as
\be\label{eq:loss-cross-entropy}
\L_{\D}=-\sum_{\delta\in\D}\sum_{i=1}^{n_\text{out}} p\!\le(i|x_\delta\ri) \log\!\le[q\!\le(i|x_\delta\ri)\ri] \, ,
\ee
where $p\!\le(i|x_\delta\ri)$ is a discrete distribution over the components $i$ of the true output
\be\label{eq:softmax-target}
p\!\le(i|x_\delta\ri)\equiv \frac{\exp\!\le[\y{i}{\delta}\ri]}{\sum_{j=1}^{n_\text{out}}\exp\!\le[\y{j}{\delta}\ri]}\, ,
\ee
and $q\!\le(i|x_\delta\ri)$ is similarly a discrete distribution over the components $i$ of the network's output
\be\label{eq:softmax-output}
q\!\le(i|x_\delta\ri)\equiv \frac{\exp\!\le[z_{i;\delta}^{(L)}(t) \ri]}{\sum_{j=1}^{n_\text{out}}\exp\!\le[z_{i;\delta}^{(L)}(t)\ri]}\, .
\ee
As we mentioned when discussing the categorical hypothesis\index{hypothesis (Bayesian inference)!categorical} in the context of Bayesian model fitting\index{Bayesian inference!model fitting} in \S\ref{subsec:ForIO}, the discrete distribution used for \eqref{eq:softmax-target} and \eqref{eq:softmax-output} is sometimes referred to as the \emph{softmax}\index{softmax distribution}~\eqref{eq:softmax}.
The cross-entropy loss~\eqref{eq:loss-cross-entropy}\index{loss!cross-entropy} is a natural measure of the closeness of discrete distributions such as \eqref{eq:softmax-target} and \eqref{eq:softmax-output}.\footnote{\label{foot:KL-in-KL-chapter}The proper measure of closeness of distributions is really the \emph{Kullback–Leibler (KL) divergence}\index{Kullback–Leibler divergence} \eqref{eq:KL-divergence-def}, which we will describe in detail in Appendix~\ref{app:mi-stuff}. However, the KL divergence $KL\le[ p \, ||\, q \ri]$ and the cross-entropy loss\index{loss!cross-entropy}~\eqref{eq:loss-cross-entropy} only differ by a $z^{(L)}$-independent constant, the entropy $\entropy\!\le[p\!\le(i|x_\delta\ri) \ri] = -\sum_{\delta \in \D} \sum_{i=1}^{n_\text{out}} p\!\le(i|x_\delta\ri) \log\!\le[p\!\le(i|x_\delta\ri)\ri]$ to be exact, and thus the use of one versus the other is identical under any gradient-based learning algorithm. Note also the lack of exchange symmetry in either loss between $p$ and $q$. The choice in \eqref{eq:loss-cross-entropy} is purposeful and reflects the fact that an untrained model is on a different footing than the true distribution from which observations arise, analogous to the asymmetry between the prior and posterior in Bayesian inference.\label{footnote:KL}}


\index{regression!linear|see{linear regression}}\index{regression!nearly-linear|see{quadratic regression}}\index{regression!polynomial|see{polynomial regression}}
In particular, cross-entropy loss is the appropriate choice for \neo{classification}, when we want to sort the input $x$ into one of $n_\text{out}$ different classes or categories. Accordingly, the softmax distribution \eqref{eq:softmax-output} transforms the model's output vector with $n_\text{out}$ real components into a discrete probability distribution.
In contrast, the MSE loss\index{loss!comparison of MSE and cross-entropy} is the appropriate choice for \neo{regression}, when the function we want to learn is a vector of real numbers.\footnote{We can think of each loss as descending from a different Bayesian hypothesis, cf.~our discussion of the uncertain hypothesis and the MSE loss \eqref{eq:noisy-conditional-Bayes} \index{hypothesis (Bayesian inference)!uncertain} and the categorical hypothesis and the softmax distribution \eqref{eq:softmax} \index{hypothesis (Bayesian inference)!categorical} in the context of Bayesian model fitting in \S\ref{subsec:ForIO}.} Importantly, when the initialization hyperparameters are tuned to criticality and the training hyperparameters are selected according to the learning rate equivalence principle\index{equivalence principle}, \emph{both losses will be completely well behaved during training}.





When using the cross-entropy loss, typically the true outputs for the training set are given in terms of the softmax values $p\!\le(i|x_{\tra}\ri)$ rather than in terms of continuous vectors $\y{i}{\tra}$. Even more typically, the values $p\!\le(i|x_{\tra}\ri)$ specify a particular \neo{label}, $i=i^\star_{\tra}$, with \neo{absolute certainty}, $p\!\le(i^\star_{\tra}|x_{\tra}\ri) = 1$, while the rest of the components vanish, $p\!\le(i|x_{\tra}\ri) = 0$ for $i\neq i^\star_{\tra}$; this is known  as \emph{hard labeling}\index{label!hard}\index{label!hard|seealso{one-hot encoding}} or \term{one-hot encoding}, and puts the true value of the network output $\y{i^\star_{\tra}}{\tra}$ at infinity. In such case, no finite amount of training will actually reach the minimum, 
and in practice as you approach such a minimum the generalization of the network becomes worse and worse. To remedy this, \textbf{early stopping}\index{regularization!early stopping}\index{early stopping|see{regularization}} of the training algorithm is used as a regularization technique to effectively get finite targets $\y{i^\star_{\tra}}{\tra}$.\footnote{Alternatively we can also explicitly pick a target distribution over the output classes with multiple nonzero components $p\!\le(i|x_{\tra}\ri)$, which is known as \emph{soft labeling}\index{label!soft}. This can be implemented as a \terminate{regularization} technique called \emph{label smoothing}\index{regularization!label smoothing}\index{label smoothing|see{regularization}}, where $p\!\le(i^\star_{\tra}|x_{\tra}\ri) = 1-\epsilon$ and $p\!\le(i\ne i^\star_{\tra}|x_{\tra}\ri) = \epsilon/(n_{\text{out}}-1)$, or as  \neo{knowledge distillation}, mentioned in footnote~\ref{foot:distillation} of \S\ref{ch:bayesian-inference}, when you actually want to learn such a distribution over output classes.}



Now let's specialize to the current context of training neural networks in the infinite-width limit (and assume some kind of regularization is used as described above). In the last section, we noted that any loss that's minimized by setting the network outputs to the true outputs for the training set, $z^{(L)}(x_{\tra};\theta)=\y{i}{\tra}$, is described at the minimum by the Newton's method giant-leap solution~\eqref{eq:loss-minimization-by-Newton}. It's easy to check that the cross-entropy loss \eqref{eq:loss-cross-entropy} is minimized when $ q\!\le(i|x_\delta\ri)  = p\!\le(i|x_\delta\ri)$, and a quick inspection of \eqref{eq:softmax-target} and \eqref{eq:softmax-output} shows that this is obtained by the condition $z^{(L)}(x_{\tra};\theta)=\y{i}{\tra}$.

We do need to make one additional important remark for the cross-entropy loss\index{loss!cross-entropy}. Since in this setting we specify the true output in terms of a softmax\index{softmax distribution}  $p\!\le(i|x_{\tra}\ri)$ rather than an $n_L$-component vector of real numbers $\y{i}{\tra}$, there is an ambiguity in how to set the network output $\z{i}{\tra}{L}$: any component-independent shift $\y{i}{\tra}\to\y{i}{\tra}+c_{\tra}$ keeps the target distribution $p\!\le(i|x_\delta\ri)$ invariant. However, since in this case we care not about the network outputs $\z{i}{\delta}{L}$, but rather their softmax $q\!\le(i|x_\delta\ri)$ \eqref{eq:softmax-output}, this ambiguity doesn't matter in the end. In particular, a shift $\y{i}{\tra}\to\y{i}{\tra}+c_{\tra}$ in the giant-leap solution~\eqref{eq:newton-at-last} shifts all the output components by the same amount for each input $x_\delta$, $\sum_{\tra_1, \tra_2 \in\A} \Ti{\NTKI}{\delta \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L}c_{\tra_2}$, leading to the same softmax $q\!\le(i|x_\delta\ri)$. Thus, we see explicitly that our solution~\eqref{eq:newton-at-last} unambiguously describes networks fully-trained according to the cross-entropy loss\index{loss!cross-entropy}.








































 




\subsection{Kernel Prediction}\label{subsec:NTKprediction}\index{kernel methods!prediction}
After an intelligence -- artificial or otherwise -- undergoes an intense memorization session, often that intelligence is then subjected to \emph{test} with unseen problems in order to probe its actual understanding. In the context of \terminate{machine learning}, we typically evaluate our model's understanding by asking it to make predictions on novel inputs $x_{\tea}$ from the \neo{test set} $\tea \in \B$. 

In the infinite-width limit, the predictions of a fully-trained MLP are governed by the stochastic equation
\be\label{eq:kernel-prediction}
\z{i}{\tea}{L}(T)=\z{i}{\tea}{L}- \sum_{\tra_1, \tra_2 \in\A}  \Ti{\NTKI}{\tea \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L}\le(\z{i}{\tra_2}{L}-\y{i}{\tra_2}\ri)\, ,
\ee
whether we train the model in one step \eqref{eq:network-step-general} or in many steps \eqref{eq:newton-at-last}, and regardless of the choice of loss function or any other details of the \terminate{learning algorithm}.\footnote{
    Note that our analyses of \S\ref{sec:small-step} apply just as much to a small step as they do to a giant leap: the fully-trained infinite-width network has neither wiring in the vectorial components of the network output~(\S\ref{subsec:GD_no_wiring_at_infinity}) nor \terminate{representation learning}~(\S\ref{subsec:GD_no_RL_at_infinity}).
}
For clarity, note that the inverse frozen NTK $\TI{\NTKIsub}{\tra_1 \tra_2}{L}$ is taken with respect to the $\NR$-by-$\NR$ training-set\index{training set} submatrix only, while the frozen NTK $\Ti{\NTKI}{\tea \tra_1}{L}$ is an \emph{off-diagonal} block of the full frozen NTK, connecting an element of the training set to an element of the test set. Note also that the network outputs $\z{i}{\delta}{L}$ on the right-hand side of the equation are evaluated at initialization: once again, 
observables without any step argument should be assumed to be evaluated at initialization, while observables with a step argument $T$ should be assumed to be evaluated at the end of training.







The stochastic equation \eqref{eq:kernel-prediction} describes the predictions of a particular instantiation of a fully-trained neural network. The stochasticity arises from the fact that the prediction \eqref{eq:kernel-prediction} depends on the network outputs at initialization $\z{i}{\delta}{L}$, which themselves depend on the particular realization of the initialized parameters $\theta_{\text{init}} \equiv \theta(t=0)$.
Since we already know that such a network has completely memorized the training set so that $\z{i}{\tra}{L}(T) = \y{i}{\tra}$, the stochasticity here means that any given network in the ensemble can potentially make different predictions on elements of the test set.

With that in mind, let us now compute the full distribution over such test-set predictions for our entire ensemble of fully-trained networks. Inspecting \eqref{eq:kernel-prediction}, we see that the prediction $\z{i}{\tea}{L}(T)$ is a simple linear transformations of the Gaussian-distributed initial outputs $\z{i}{\delta}{L}$ and thus will itself be Gaussian. The mean prediction is simply given by
\be\label{eq:GD-frozen-mean}
\GDGPmean_{i;\tea} \equiv \E{\z{i}{\tea}{L}(T) } = \sum_{\tra_1, \tra_2 \in\A}  \Ti{\NTKI}{\tea \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L}  \y{i}{\tra_2}  \, .
\ee
This expression is entirely analogous to the infinite-width posterior mean\index{posterior!posterior mean} prediction for exact Bayesian inference~\eqref{eq:GP-mean}, with a simple replacement of all types of frozen neural tangent kernels with kernels: $\NTKI^{(L)}\to \ker^{(L)}$. (More on this soon.) Meanwhile, the covariance of the prediction~\eqref{eq:kernel-prediction} is given by
\begin{align}
&\cov{\z{i_1}{\tea_1}{L}(T) }{ \z{i_2}{\tea_2}{L}(T) } \equiv \E{\z{i_1}{\tea_1}{L}(T) \, \z{i_2}{\tea_2}{L}(T)} - \GDGPmean_{i_1;\tea_1}\GDGPmean_{i_2;\tea_2} \notag \\
=& \delta_{i_1 i_2} \Bigg[\Ti{\ker}{\tea_1\tea_2}{L} - \sum_{\tra_1, \tra_2 \in\A}  \Ti{\NTKI}{\tea_2 \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L}  \Ti{\ker}{\tea_1\tra_2}{L} - \sum_{\tra_1, \tra_2 \in\A}  \Ti{\NTKI}{\tea_1 \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L}  \Ti{\ker}{\tea_2\tra_2}{L}  \notag \\
&\qquad+  \sum_{\tra_1, \tra_2, \tra_3, \tra_4 \in\A} \Ti{\NTKI}{\tea_1 \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L} \Ti{\NTKI}{\tea_2 \tra_3}{L} \TI{\NTKIsub}{\tra_3 \tra_4}{L} \Ti{\ker}{\tra_4\tra_2}{L}
\Bigg] \, .
\label{eq:generalized-posterior-variance}
\end{align}
While this expression is somewhat complicated looking, involving both kernels and frozen NTKs, it similarly reduces to the Bayesian infinite-width posterior covariance\index{posterior!posterior covariance}~\eqref{eq:GP-posterior-variance} with the substitution  $\NTKI^{(L)}\to\ker^{(L)}$.\footnote{
    The fact that exact Bayesian inference and gradient descent in general make different predictions is indicative of the fact that -- for general hyperparameter settings -- they are actually different learning algorithms\index{learning algorithm}.
}

This ensemble of network predictions~\eqref{eq:kernel-prediction}, completely specified by its mean~\eqref{eq:GD-frozen-mean} and covariance~\eqref{eq:generalized-posterior-variance}, defines a kind of \textbf{generalized posterior distribution}\index{posterior!generalized posterior distribution|textbf}. This distribution comprises a complete closed-form solution for our ensemble of infinite-width networks at the \terminate{end of training}, regardless of the path that we take to get there.
The mean of the distribution is the prediction of the network \emph{averaged} over instantiations, while the covariance quantifies the instantiation-to-instantiation fluctuations of these predictions.

\index{Bayesian inference!model fitting}\index{maximum likelihood estimation!gradient descent approximation}
Indeed, it is sensible to identify the ensemble of trained networks as a kind of \terminate{posterior} distribution, if you recall  %
our discussion of approximation methods for Bayesian inference in
\S\ref{subsec:ForIO}: minimizing a training loss $\L_\A(\theta)$ gives the maximum likelihood estimation (MLE) of the model parameters~\eqref{eq:mle-estimate-loss}, which we now identify with our fully-trained solution~\eqref{eq:loss-minimization-by-Newton} $\theta_\text{MLE}^\star = \theta_\text{Newton}^\star$. Further recalling the content of footnote~\ref{foot:foretelling-GD} in \S\ref{subsec:ForIO}, for wide networks the minimum of the loss is not unique, 
and the MLE approach will give a family of minima parameterized by the initialization: $\theta^\star_{\text{MLE}}(\theta_{\text{init}})$.\footnote{
    As per that same footnote, we could also try to analyze the MAP\index{maximum a posteriori!gradient descent approximation} estimate~\eqref{eq:map-estimate-loss} in the context of infinite-width gradient-based learning with the addition of a \terminate{regularization} term of the form $\sum_{\mu=1}^P a_{\mu}\theta_\mu^2$ to the loss. If you start this analysis, you'll immediately find that the gradient-descent update $\dtheta_\mu$ includes an additional term $-2 \eta \sum_\nu \lambda_{\mu\nu}\, a_\nu \theta_\nu$, and after some reflection you'll likely also realize the need to define a new stochastic tensor,
    \be
    \Tia{\reg}{i}{\delta}{\ell} \equiv \sum_{\mu, \nu} \lambda_{\mu \nu}\, a_\mu \theta_\mu \frac{\td \z{i}{\delta}{\ell} }{\td \theta_\nu}  \, ,
    \ee
    which has a stochastic iteration given by
    \be
    \Tia{\reg}{i}{\delta}{\ell+1} = a_b^{(\ell+1)} \Lb{\ell+1} \bias{i}{\ell+1} + a_W^{(\ell+1)} \frac{\LW{\ell+1}}{n_\ell} \sum_{j=1}^{n_\ell} \W{ij}{\ell+1} \s{j}{\delta}{\ell} + \sum_{j=1}^{n_\ell} \W{ij}{\ell+1} \ds{j}{\delta}{\ell} \Tia{\reg}{j}{\delta}{\ell} \, ,
    \ee 
    and whose cross-correlation with the preactivations you'll want to compute. Here, you will have defined separate layer-dependent bias and weight regularizations for the coefficients $a_{\mu}$ analogous to what we did for the learning-rate tensor in \eqref{eq:diag_LR}, and you may want to work out the interplay between these \terminate{regularization hyperpameters}\index{hyperparameters!regularization|see{regularization hyperpameters}} and \terminate{initialization hyperparameters} for extra credit.
    \label{footnote:regularization-recursion}
}
This lack of uniqueness ultimately stems from the lingering dependence of the trained network prediction $\z{i}{\tea}{L}(T)$ on the initial function output $\z{i}{\delta}{L}$, which stochastically varies from instantiation to instantiation, cf.~our prediction~\eqref{eq:kernel-prediction}. Considering the ensemble over instantiations of $\theta_{\text{init}}$, we now see exactly how this generalized distribution with the mean \eqref{eq:GD-frozen-mean} and covariance \eqref{eq:generalized-posterior-variance} depends on the \terminate{training hyperparameters} $\Lb{\ell}$ and $\LW{\ell}$, \terminate{initialization hyperparameters} $\Cb{\ell}$ and $\CW{\ell}$, and the training data $\le(x_{\tra}, y_{\tra}\ri)_{\tra\in\A}$. 


To be a little pedantic for a paragraph, the covariance in the generalized posterior distribution~\eqref{eq:generalized-posterior-variance} really has a different interpretation than the posterior covariance\index{posterior!posterior covariance}~\eqref{eq:GP-posterior-variance} we computed for exact \terminate{Bayesian inference} at infinite width. In the setting of gradient-based learning, the covariance of the output encodes the variation in the predictions among networks in the ensemble, each corresponding to different parameter settings that still minimize the training loss $\L_\A(\theta)$. In the setting of exact \terminate{Bayesian inference}, 
the covariance encodes our intrinsic uncertainty about unseen data and a small uncertainty can serve as a measure of confidence in our prediction. Thus, these covariances arise for different reasons and are epistemologically quite different in nature. 


However, if we can be somewhat pragmatic for a sentence, when you have multiple trained models it's not entirely unreasonable to try to think of this generalized posterior covariance~\eqref{eq:generalized-posterior-variance} as a measure of confidence
as well.






























\subsubsection{That One Place Where Gradient Descent = Exact Bayesian Inference}
\index{gradient descent!as Bayesian inference!at infinite width}\index{Bayesian inference!via gradient descent!at infinite width}


Just before, we casually noticed that if we replaced all frozen \emph{neural tangent kernels}\index{frozen NTK} with \emph{kernels}, $\NTKI^{(L)}\to \ker^{(L)}$, then the generalized posterior distribution\index{posterior!generalized posterior distribution} based on~\eqref{eq:kernel-prediction} reduces to the exact Bayesian posterior distribution\index{posterior}~\eqref{eq:posterior-at-infinite-width}. Let us now show how we can actually implement such a substitution in the context of gradient-based learning with a particular choice of \terminate{training hyperparameters}.





To see how to do this, let's put side-by-side the recursion  that defines the output-layer kernel~\eqref{eq:K-recursion-reprint} and the recursion that defines the output-layer frozen NTK\index{frozen NTK}~\eqref{eq:frozen-NTK-recursion}:
\begin{align}\label{eq:kernel-recursion-reminder-reprint}
\Ti{\ker}{\delta_1\delta_2}{L} &= \Cb{L} + \CW{L} \braket{\sigma_{\delta_1}\sigma_{\delta_2}}{L-1} \, , \\
\Ti{\NTKI}{\delta_1\delta_2}{L}&=\Lb{L} + \lamW{L}\bra\sigma_{\delta_1}\sigma_{\delta_2}\ket_{\ker^{(L-1)}}+\CW{L}\bra\sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_2}\ket_{\ker^{(L-1)}}\Ti{\NTKI}{\delta_1\delta_2}{L-1}\, .
\label{eq:NTK-recursion-reminder-reprint}
\end{align}
By inspection, it's immediately clear that setting the final-layer learning rates as
\be\label{eq:last-layer-bayes-learning-rate}
\Lb{L}=\Cb{L}, \qquad \LW{L}=\CW{L}\, ,
\ee
gives us what we want, almost:
\be
\Ti{\NTKI}{\delta_1\delta_2}{L}=\Ti{\ker}{\delta_1\delta_2}{L}+\CW{L}\bra\sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_2}\ket_{\ker^{(L-1)}}\Ti{\NTKI}{\delta_1\delta_2}{L-1}\, .
\ee
To get rid of that pesky last term, we need a way to make the penultimate-layer frozen NTK $\Ti{\NTKI}{\delta_1\delta_2}{L-1}$vanish. To ensure this,  we can simply set all the other \terminate{training hyperparameters} to zero:
\be\label{eq:hidden-layer-bayes-learning-rate}
\Lb{\ell}=0\,, \qquad \LW{\ell}=0\,,\qquad \text{for}\quad \ell <L\, .
\ee
Combined with the initial condition for the NTK recursion \eqref{eq:NTHinitial}, this ensures that $\Ti{\NTKI}{\delta_1\delta_2}{\ell} = 0$ for $\ell <L$, including $\ell=L-1$. Hence, this particular configuration of training hyperparameters sets 
\be
\Ti{\NTKI}{\delta_1\delta_2}{L}=\Ti{\ker}{\delta_1\delta_2}{L} \, ,
\ee
giving us what we wanted, exactly. 




In words, this choice of the \terminate{training hyperparameters} \eqref{eq:last-layer-bayes-learning-rate} and \eqref{eq:hidden-layer-bayes-learning-rate} means that we are training only the biases and weights in the last layer.
This establishes that, in the infinite-width limit, exact Bayesian inference is actually a very special case of an ensemble of networks trained with gradient-based learning.

In practice, this means that one could train an ensemble of networks with gradient descent using the \terminate{training hyperparameters} choices \eqref{eq:last-layer-bayes-learning-rate} and \eqref{eq:hidden-layer-bayes-learning-rate} in order to implement a very good approximation of exact \terminate{Bayesian inference}. (The approximation would become exact if you had an infinite number of networks in your ensemble.) On the one hand,
unlike the exact version of Bayesian inference presented in \S\ref{sec:infinite-posterior},
in this case we no longer need to explicitly store or invert the kernel. On the other hand, we may need to fully train a large number of very wide networks for our ensemble to give a good approximation, which may again be expensive in terms of computation and memory.




Interestingly, by explicitly turning off the learning in the hidden layers, 
we are significantly changing the features used to compute our predictions.
In particular, in this case the NTK only has contributions from the biases and weights in the final layer.
Intuitively, what's happening here is that we're taking random features\index{feature} in the penultimate layer $\sigma\!\le(z^{(L-1)}_i\ri)$ and then explicitly training the biases $b^{(L)}$ and weights $W^{(L)}$ to fit the best possible \neo{linear model} of these random features.\footnote{We'll explain in \S\ref{sec:lazy-kernel} that the general version of gradient-based learning in the infinite-width limit is also a \neo{linear model} of random features, but is constructed from a larger set of such features encompassing all the hidden layers.}


























\section{Generalization}\label{sec:generalization-at-infinity}\index{generalization}
As remarked before, ultimately we care about how well a model performs on a previously unseen \terminate{test set} $\B$ as compared to the \terminate{training set} $\A$. Some fully-trained networks will \emph{generalize} to these new examples better than others, depending strongly on their initialization\index{initialization hyperparameters} and \terminate{training hyperparameters}. 

To assess this, we can compute the \term{generalization error}: 
\be\label{eq:generalization-error}
\gen \equiv \L_\B - \L_\A\, .
\ee
The generalization error is a quantitative measure of how well a network is really approximating the desired function.\footnote{Without loss of generality\index{loss!of generality}, in the following discussion  we'll implicitly assume that the minimal value of the loss is zero.}
Specifically, if the training loss\index{loss!training loss!relation to overfitting} is small but the test loss\index{loss!test loss!relation to generalization} is large such that there's significant generalization error, then the network isn't really a good model of $f(x)$; instead it's just a lookup table of the values of $f(x)$ when $x$ is taken from the training set $\A$. This is known as \term{overfitting}. In contrast,
if the training and test losses are both small such that there's  little generalization error, then we expect that our model is going beyond simple memorization.\footnote{
    If the generalization error $\gen$ is small but the training loss $\L_\A$ is large, then the model is said to be \neo{underfitting}.
    This situation is not really relevant for very wide networks since, as we've already explained, we can fully train them to achieve zero training loss.
    \index{loss!training loss!relation to underfitting}
}
As such, the generalization error is often considered to be the main quantitative measure of success of a machine learning model.


In the infinite-width limit, we saw in the last section that we can easily set the training loss to zero $\L_\A = 0$ for any particular network. Thus, in the current context the generalization error is completely assessed by the test loss, 
\be
\gen = \L_\B \, ,
\ee
and our current goal is to understand the statistics of the test error in order to characterize how infinite-width networks generalize.

For wide networks we know that there are many configurations of the model parameters that will minimize the training loss and memorize the training data. Some of these configurations might generalize well leading to a small test loss, while some might overfit leading to a large test loss. Thus, the statistics of the generalization error are determined by the statistics of these configurations, which are in turn determined by our initialization and training hyperparameters. 

The mean generalization error captures the \terminate{generalization} properties of the ensemble of networks, and its variance tells us how the instantiation-to-instantiation fluctuations lead some particular networks to generalize better than others.  Understanding these statistics will inform how we should pick our hyperparameters to achieve the best generalization performance on average as well as to ensure that the typical behavior of any fully-trained network is likely to be close to the average.\footnote{
In some applications of \terminate{machine learning}, unseen examples are further divided into two types of datasets, \emph{(i)} a \neo{validation set}, used generally for model selection and often specifically for tuning hyperparameters,
and \emph{(ii)} a \neo{test set}, used to assess the generalization properties of a particular trained model. Despite our liberal usage of the term \neo{test set}, here, as we analytically compute the statistics of the loss on unseen examples and then use them to tune the hyperparameters, what we have is really closer in meaning to a \neo{validation set}, as we are tuning an ensemble of models rather than assessing any particular one.
}







With that in mind, let's first evaluate the MSE test loss\index{loss!MSE}, averaged over an ensemble of fully-trained networks:
\begin{align}\label{eq:bias-variance-decomposition-generalized-mse}
\E{\L_{\B}(T)}=&\E{\frac{1}{2}\sum_{i=1}^{n_L}\sum_{\tea\in\B}\le(\z{i}{\tea}{L}(T)-\y{i}{\tea}\ri)^2}\, \notag\\
=&\E{\frac{1}{2}\sum_{i=1}^{n_L}\sum_{\tea\in\B}\le(\z{i}{\tea}{L}(T)-\GDGPmean_{i;\tea}+\GDGPmean_{i;\tea}-\y{i}{\tea}\ri)^2}\, \notag\\
=&\frac{1}{2}\sum_{\tea\in\B}\le\{\sum_{i=1}^{n_L}\le(\GDGPmean_{i;\tea}-\y{i}{\tea}\ri)^2+\sum_{i=1}^{n_L}\cov{\z{i}{\tea}{L}(T) }{ \z{i}{\tea}{L}(T) }\ri\}\, . 
\end{align}
In the second line, we added and subtracted the infinite-width mean prediction~\eqref{eq:GD-frozen-mean}, and to get to the third line we noted that the cross terms cancel under the expectation.


This decomposition~\eqref{eq:bias-variance-decomposition-generalized-mse} illustrates a type of generalized \term{bias-variance tradeoff}: 
the first term, the \emph{bias}\index{generalization error!bias}, measures the deviation of the mean prediction of the ensemble $\GDGPmean_{i;\tea}$ from the true output $\y{i}{\tea}$; the second term, the \emph{variance}\index{generalization error!variance} -- or specifically the covariance of the generalized posterior distribution\index{posterior!generalized posterior distribution} \eqref{eq:generalized-posterior-variance} -- measures the instantiation-to-instantiation fluctuations of that prediction across different models in our ensemble. The reason why this is called a \emph{tradeoff} is that typically different settings of the hyperparameters
will decrease one term at the cost of increasing the other, making the modeler have to choose between improving one at the cost of the other.\footnote{The reason why we call it \emph{generalized} bias-variance tradeoff\index{bias-variance tradeoff!generalized} is that, in the \emph{standard} bias-variance tradeoff\index{bias-variance tradeoff!generalized!vs.~standard}, the expectation is over different realizations of the training set $\A$ rather than over different initializations of the model parameters $\theta_\mu$. In that typical setting we have only a single model, and the \emph{bias}\index{generalization error!bias} characterizes how well that model can be trained on each different training set -- with a large bias indicative of \neo{underfitting} -- while the \emph{variance}\index{generalization error!variance} characterizes the fluctuations of that model's performance over the different training sets -- with a large variance indicative of \neo{overfitting}. %
}




For more general losses, we can Taylor expand the test loss\index{loss!test loss} around the mean prediction as
\begin{align}
\L_{\B}=&\L_{\B}\!\le(\GDGPmean\ri)+\sum_{i,\tea}\frac{\partial\L_{\B}}{\partial \z{i}{\tea}{L}}\Bigg\vert_{z^{(L)}=\GDGPmean}\!\!\!\!\!\!\!\!\le(\z{i}{\tea}{L}(T)-\GDGPmean_{i;\tea}\ri)\, \\
&+\frac{1}{2}\sum_{i_1,i_2,\tea_1,\tea_2}\frac{\partial^2\L_{\B}}{\partial \z{i_1}{\tea_1}{L}\partial \z{i_2}{\tea_2}{L}}\Bigg\vert_{z^{(L)}=\GDGPmean}\!\!\!\!\!\!\!\!\le(\z{i_1}{\tea_1}{L}(T)-\GDGPmean_{i_1;\tea_1}\ri)\le(\z{i_2}{\tea_2}{L}(T)-\GDGPmean_{i_2;\tea_2}\ri)+\ldots\, ,\notag
\end{align}
where we've denoted the test loss evaluated at the mean prediction as
\be\label{eq:general-loss-bias}
\L_{\B}\!\le(\GDGPmean\ri) \equiv \L_{\B}\!\le(z^{(L)} =\GDGPmean\ri)\, .
\ee 
Performing the expectation over the ensemble and noting that $\E{\z{i}{\tea}{L}(T)-\GDGPmean_{i;\tea}}=0$ by definition~\eqref{eq:GD-frozen-mean}, we get
\be\label{eq:bias-variance-decomposition-generalized-general-loss}
\E{\L_{\B}}=\L_{\B}\!\le(\GDGPmean\ri)+\frac{1}{2}\sum_{i_1,i_2,\tea_1,\tea_2}\frac{\partial^2\L_{\B}}{\partial \z{i_1}{\tea_1}{L}\partial \z{i_2}{\tea_2}{L}}\Bigg\vert_{z^{(L)}=\GDGPmean}\!\!\!\!\!\!\!\!\cov{\z{i_1}{\tea_1}{L}(T) }{ \z{i_2}{\tea_2}{L}(T) }+\ldots\, .
\ee
Here again we find a generalized \terminate{bias-variance decomposition}\index{bias-variance decomposition|seealso{generalization error}}: the first term $\L_{\B}\!\le(\GDGPmean\ri)$ is the bias\index{generalization error!bias}, measuring the deviation of the mean prediction from the true output on the \terminate{test set}, and the second term is the variance\index{generalization error!variance}, measuring the instantiation-to-instantiation uncertainty as the trace of the covariance multiplied by the \terminate{Hessian} of the loss with respect to the network outputs. Thus, for \emph{any} choice of loss function, these bias and variance terms will give a good proxy for the generalization error $\gen$ so long as our models are making predictions that are close to the mean prediction.



Now, let's see how to compute these bias and variance terms in a few different setups.
In most common cases in practice
the loss is \emph{extensive}\index{extensivity!of loss} or additive in samples, i.e.~$\L_{\B}=\sum_{\tea\in\B}\L_{\tea}$, and we can consider the test loss evaluated on one test sample at a time.
Thus, for the purpose of our analysis here, the question is: for a given test input, how many training examples are relevant for making a prediction?


In \S\ref{subsec:robustness-from-infinite-GD}, we'll compute the bias and variance terms of the generalization error \eqref{eq:bias-variance-decomposition-generalized-general-loss} around one training sample using our $\delta$ expansion introduced in \S\ref{ch:signalprop}, giving another lens into hyperparameter tuning and \terminate{criticality} for our two universality classes. However, this view will be somewhat limited by the restriction of our training set to one sample.

In \S\ref{subsec:star-polation}, we'll enlarge our training set to include two samples. Rather than computing the \terminate{generalization error} itself, here we'll be able to explore directly 
a different aspect of generalization: how a fully-trained network either interpolates or extrapolates to make predictions. %


\subsection{Bias-Variance Tradeoff and Criticality}\label{subsec:robustness-from-infinite-GD}
Let us index one training sample by $\tra=+$ and a nearby test sample by $\tea=-$; let us also focus on the output layer $\ell=L$ and temporarily drop the layer index from the frozen NTK\index{frozen NTK}, $\NTKI_{\delta_1\delta_2}\equiv\Ti{\NTKI}{\delta_1\delta_2}{L}$, until later when we need to discuss the depth dependence of various NTK components.


The bias term in the generalization error\index{generalization error!bias} is determined by the deviation of the mean prediction \eqref{eq:GD-frozen-mean} from the true output:
\begin{align}\label{eq:bias-factor-gen-error-not-squared}
\GDGPmean_{i;-}-\y{i}{-}&=\frac{\NTKI_{-+}}{\NTKI_{++}}\y{i}{+}-\y{i}{-} \, \notag \\
&=(\y{i}{+}-\y{i}{-}) + \le(\frac{\NTKI_{-+}}{\NTKI_{++}}-1\ri)\y{i}{+}\, .
\end{align}
In this expression, the first term is the true difference in the function outputs on the two different inputs, $f(x_{+})-f(x_-)$, while the second term is a similar (expected) difference between our predicted output on $x_-$ and the learned true output on $x_+$, $z_-(T) - z_+(T)$.\footnote{
    In general, the bias term $\L_{\B}\!\le(\GDGPmean\ri)$ in the generalization error\index{generalization error!bias}~\eqref{eq:bias-variance-decomposition-generalized-general-loss} depends on the details of the loss. Of course, we can expand $\L_{\B}\!\le(\GDGPmean\ri)$ around the true output $\y{i}{-}$, and the expansion will depend on the difference $\GDGPmean_{i;-}-\y{i}{-}$~\eqref{eq:bias-factor-gen-error-not-squared}. For the MSE loss\index{loss!MSE}, the bias is precisely the square of this difference. For the cross-entropy loss\index{loss!cross-entropy}, it is more natural to expand in terms of the difference $\overline{q}(i\vert x_{-})-p(i\vert x_{-})$, with $\overline{q}\!\le(i|x_\delta\ri)\equiv \exp\!\le[\GDGPmean_{i;\delta}\ri]/\sum_{j=1}^{n_\text{out}}\exp\!\le[\GDGPmean_{j;\delta}\ri]$.
} Note the opposite ordering of $+$ and $-$ in these two terms: if our prediction is exactly correct, these two terms are equal in magnitude and opposite in sign, and the bias term in the generalization error will vanish.\index{generalization error!bias}

With that in mind, the quantity in parenthesis in the second factor of the bias term~\eqref{eq:bias-factor-gen-error-not-squared},
\be\label{eq:robustness-measure}
\frac{\NTKI_{-+}}{\NTKI_{++}}-1\, ,
\ee 
serves as a natural measure of \emph{robustness}\index{generalization error!robustness measure} since it characterizes how sensitively our prediction changes -- i.e.~how $\vert z_-(T) - z_+(T)\vert$ grows -- with corresponding small changes in the input. A model that isn't robust will often be incorrect, making predictions that vary greatly from the network output on nearby training points,
while too robust a model will not have much flexibility in its output. Since a priori we don't know what type of function we are going to approximate, we naturally would want to pick hyperparameters that include a class of networks that are robust, but not overly inflexible.\footnote{A dedicated reader might notice the parallel with \S\ref{subsec:Occam-criticality} where we argued for \terminate{criticality} by considering the evidence for two inputs with differing true outputs, $f(x_{+})-f(x_-)\ne 0$, which called for similar flexibility in choice of function approximators.\index{Bayesian inference!evidence!relation to generalization error}}



Now, since we're considering a test input that's nearby our training input, that should remind you of our $\delta$ expansion from our \terminate{criticality} analysis in \S\ref{sec:bootstrapping}.\footnote{
    The following applies to smooth activation functions and is intended to give the general picture. We will give an analysis particular to nonlinear scale-invariant activation functions later when discussing them in particular.
}
Specifically, we can expand the frozen NTK\index{frozen NTK} in our $\gamma^{[a]}$ basis as we did for the kernel in~\eqref{eq:kernel-expand-gamma},
\be\label{eq:NTK-in-gamma-basis}
\NTKI_{\pm\pm}=\NTKI_{[0]}\pm\NTKI_{[1]}+\NTKI_{[2]}\, ,\qquad \NTKI_{\pm\mp}=\NTKI_{[0]}-\NTKI_{[2]}\, ,
\ee
and make $\delta$ expansions similar to the ones we did for the kernel in \eqref{eq:kernel-expand-1}--\eqref{eq:kernel-expand-3},
\begin{align}\label{eq:frozen-NTK-delta-0}
\NTKI_{[0]}=&\NTKI_{\M\M}+\delta\delta\NTKI_{[0]}+\o{\delta^4}\, ,\\
\NTKI_{[1]}=&\delta\NTKI_{[1]}+\o{\delta^3}\, ,\label{eq:frozen-NTK-delta-1}\\
\NTKI_{[2]}=&\delta\delta\NTKI_{[2]}+\o{\delta^4}\, ,
\label{eq:frozen-NTK-delta-2}
\end{align}
where the expansion is taken around the midpoint frozen NTK\index{frozen NTK!midpoint} $\NTKI_{\M\M}$ evaluated on the midpoint input\index{midpoint input} $\x{i}{0}\equiv (\x{i}{+}+\x{i}{-})/2$.

For simplicity of our presentation, let's now assume that the two inputs have the same norm $\sum_{i=1}^{n_0} \x{i}{+}^2=\sum_{i=1}^{n_0} \x{i}{-}^2$, so that $\ker_{[1]}=0$ and $\NTKI_{[1]}=(\NTKI_{++}-\NTKI_{--})/2=0$.
With this simplification, plugging the decomposition~\eqref{eq:NTK-in-gamma-basis} and then the expansions \eqref{eq:frozen-NTK-delta-0} and \eqref{eq:frozen-NTK-delta-2} into the expression for our robustness measure~\eqref{eq:robustness-measure}\index{generalization error!robustness measure}, we get
\be\label{eq:robustness-measure-plugged-in}
\frac{\NTKI_{-+}}{\NTKI_{++}}-1=\le(\frac{\NTKI_{[0]}-\NTKI_{[2]}}{\NTKI_{[0]}+\NTKI_{[2]}}-1\ri)=-2\frac{\delta\delta\NTKI_{[2]}}{\NTKI_{\M\M}}+\o{\delta^4}\, .
\ee
Thus, we see that the ratio $\delta\delta\NTKI_{[2]}/\NTKI_{\M\M}$ captures the robustness of predictions for nearby test inputs. We'll analyze its depth dependence for two universality classes shortly.\index{generalization error!universality class analysis}




Having covered the bias term in the generalization error\index{generalization error!bias}, let's next consider the variance term\index{generalization error!variance}. The loss-independent piece of the variance
is given by the covariance of the generalized posterior distribution\index{posterior!generalized posterior distribution}~\eqref{eq:generalized-posterior-variance}.
Evaluating \eqref{eq:generalized-posterior-variance} for a single training sample $\tra=+$, using our decompositions for the kernel~\eqref{eq:kernel-expand-gamma} and frozen NTK~\eqref{eq:NTK-in-gamma-basis}, and then using expansions~\eqref{eq:kernel-expand-1},~\eqref{eq:kernel-expand-3},~\eqref{eq:frozen-NTK-delta-0}, and~\eqref{eq:frozen-NTK-delta-2}, we find\index{$\gamma^{[a]}$ basis!kernel}\index{$\delta$ expansion}
\begin{align}\label{eq:generalization-variance-expanded}
&\cov{\z{i}{-}{L}(T) }{ \z{i}{-}{L}(T)}\, \\
=&\ker_{--}-2\frac{\NTKI_{-+}}{\NTKI_{++}}\ker_{-+}+\le(\frac{\NTKI_{-+}}{\NTKI_{++}}\ri)^2\ker_{++}\, \notag\\
=&\ker_{[0]}+\ker_{[2]}-2\le(\frac{\NTKI_{[0]}-\NTKI_{[2]}}{\NTKI_{[0]}+\NTKI_{[2]}}\ri)\le(\ker_{[0]}-\ker_{[2]}\ri)+\le(\frac{\NTKI_{[0]}-\NTKI_{[2]}}{\NTKI_{[0]}+\NTKI_{[2]}}\ri)^2\le(\ker_{[0]}+\ker_{[2]}\ri)\, \notag\\
=&4\delta\delta\ker_{[2]}+\o{\delta^4}\, .\notag
\end{align}
Thus, to leading order the variance term depends only on the perpendicular perturbation of the kernel $\delta\delta\ker_{[2]}$.



At this point, we know everything there is to know about how $\delta\delta\ker_{[2]}$ behaves as a function of depth for our universality classes (cf.~\S\ref{sec:non-scale-invariant-eft} and \S\ref{sec:finite_angle}). On the one hand, we could pick initialization hyperparameters such that $\delta\delta\ker_{[2]}$ grows exponentially with depth. However, with this choice the variance term will grow very quickly, leading to large fluctuations in model predictions between different realizations. On the other hand, we could pick \terminate{initialization hyperparameters} that decay exponentially with depth, leading to a quickly vanishing variance term and very overconfident predictions. However, we will soon see that this overconfidence comes at a cost: an exponentially vanishing perpendicular perturbation $\delta\delta\ker_{[2]}$ implies an exponentially vanishing frozen NTK component $\delta\delta\NTKI_{[2]}$ and thus a vanishing robustness measure~\eqref{eq:robustness-measure-plugged-in} signaling extreme inflexibility. In particular, we will have learned a constant function that's always equal to $y_+$, regardless of the input.

This is precisely the generalized \terminate{bias-variance tradeoff} that we described above: if we try to set the variance to zero by having $\delta\delta\ker_{[2]}$ vanish exponentially, then the vanishing of $\delta\delta\NTKI_{[2]}$ will cause our bias to be larger for generic inputs and consequently the network will not be able to generalize in a nontrivial manner. Vice versa, making the function too flexible with large $\delta\delta\NTKI_{[2]}$ will make the model predictions not only too sensitive to small changes in the input through $\delta\delta\NTKI_{[2]}$, but also will cause large fluctuations in that prediction from realization to realization through $\delta\delta\ker_{[2]}$.

Of course, we know that there's a third option: we could pick our \terminate{criticality} condition $\chi_\perp\!\le(\ker^\star\ri) =1$. This setting of the initialization hyperparameters has the potential to balance the bias-variance tradeoff\index{bias-variance tradeoff!relation to criticality}, leading to the best outcome without a priori knowing anything more about the underling dataset we're trying to model.



What about our other criticality condition $\chi_\parallel\!\le(\ker^\star\ri) =1$? Recall from our discussion of the \terminate{exploding and vanishing gradient problem} in \S\ref{sec:EVGP-WEP} that the \terminate{parallel susceptibility} $\chi_\parallel$ affects the way in which the midpoint frozen NTK $\NTKI_{00}$
receives contributions from different layers. %
As the midpoint frozen NTK $\NTKI_{00}$ enters in the robustness measure as in~\eqref{eq:robustness-measure-plugged-in}, this suggests that it also plays an important role in generalization. In fact, we will see soon in \S\ref{sec:lazy-kernel} that ensuring equal contributions from all layers is another way of saying that we use the greatest set of features available to us in making a prediction. Thus, it stands to reason that also picking the criticality condition $\chi_\parallel\!\le(\ker^\star\ri) =1$ in conjunction with the condition $\chi_\perp\!\le(\ker^\star\ri) =1$ is a natural choice for generalization, in addition to all our other evidence for such a choice.\footnote{Just like in footnote \ref{foot:parallel-criticality} of \S\ref{subsec:Occam-criticality}, additional justification comes from the consideration of  two inputs with unequal norms: $\sum_{i=1}^{n_0} \x{i}{+}^2\ne\sum_{i=1}^{n_0} \x{i}{-}^2$.
In such a case,  the robustness measure~\eqref{eq:robustness-measure-plugged-in} is given by
\be\label{eq:robustness-with-parallel}
\frac{\NTKI_{-+}}{\NTKI_{++}}-1=-\frac{\delta\NTKI_{[1]}}{\NTKI_{\M\M}}-2\frac{\delta\delta\NTKI_{[2]}}{\NTKI_{\M\M}}+\le(\frac{\delta \NTKI_{[1]}}{\NTKI_{\M\M}}\ri)^2+\o{\delta^3}\, ,
\ee
and the covariance is given by
\be
\cov{\z{i}{-}{L}(T) }{ \z{i}{-}{L}(T)}=4\delta\delta\ker_{[2]}-2\delta\ker_{[1]}\frac{\delta \NTKI_{[1]}}{\NTKI_{\M\M}}+\ker_{\M\M}\le(\frac{\delta \NTKI_{[1]}}{\NTKI_{\M\M}}\ri)^2+\o{\delta^3}\, .
\ee
First, we see that the kernel components $\delta\ker_{[1]}$ and $\ker_{\M\M}$ both contribute, necessitating that we set $\chi_{\parallel}=1$ as per our previous discussions. In addition, we will also need to tame the exploding and vanishing problem of $\delta \NTKI_{[1]}$.

For the scale-invariant universality class\index{universality class!scale-invariant}, $\NTKI_{[1]}=(\NTKI_{++}-\NTKI_{--})/2$ has exactly the same depth dependence as the single-input frozen NTK\index{frozen NTK} \eqref{eq:frozen-ntk-critical-solution-relu}. In this case, $\chi_{\parallel}=\chi_{\perp}\equiv \chi$, and all the exponential explosions and vanishments are mitigated by setting $\chi=1$.

For the $\Tif{\ker}{}=0$ universality class, %
we can write a recursion for $\delta\NTKI_{[1]}$ by projecting out the $\gamma^{[1]}$ component of the full frozen NTK recursion \eqref{eq:tired-repetition-of-frozen-yogurt} using \eqref{eq:trace-projection}:
\be\label{eq:deltaNTK-recursion}
\Ti{\delta\NTKI}{[1]}{\ell+1}=\chi_{\perp}^{(\ell)}\Ti{\delta\NTKI}{[1]}{\ell}+\le(\frac{\LW{\ell+1}}{C_W}\chi_{\parallel}^{(\ell)}+\frac{C_W}{\KML}\bra z\sigma'\sigma''\ket_{\KML} \Ti{\NTKI}{\M\M}{\ell}\ri) \Ti{\delta\ker}{[1]}{\ell}\, ,
\ee
i.e.~with a derivation almost isomorphic to the one below for $\delta\delta\NTKI_{[2]}$ \eqref{eq:deltadeltaNTK-recursion}. We in particular see that $\Ti{\delta\ker}{[1]}{\ell}$ contributes to $\Ti{\delta\NTKI}{[1]}{\ell}$ --
which can be thought of as the Bayesian contribution per our last discussion in \S\ref{subsec:NTKprediction} --
and its exploding and vanishing problem is mitigated by setting  $\chi_\parallel\!\le(\ker^\star\ri)=1$: cf.~\eqref{K1}. (At this point you may find it useful to re-read and re-flect on the last paragraph of footnote \ref{foot:parallel-criticality} in \S\ref{subsec:Occam-criticality}.) You can further study the depth dependence of $\Ti{\delta\NTKI}{[1]}{\ell}$ at criticality and find that $\delta\NTKI_{[1]}^{(\ell)}$ decays faster than $\NTKI_{\M\M}^{(\ell)}$ and $\delta\delta\NTKI_{[2]}^{(\ell)}$, thus reducing the problem back to the one studied in the main text.}





Now, returning to the bias part of the generalization error\index{generalization error!bias}, to  complete our analysis we'll need to solve a recursion for the $\delta\delta\NTKI_{[2]}$ component of the frozen NTK recursion~\eqref{eq:frozen-NTK-recursion}, reprinted here in full:
\begin{align}\label{eq:tired-repetition-of-frozen-yogurt} %
\Ti{\NTKI}{\delta_1\delta_2}{\ell+1}&=\Lb{\ell+1} + \lamW{\ell+1}\bra\sigma_{\delta_1}\sigma_{\delta_2}\ket_{\ker^{(\ell)}}+C_W\bra\sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_2}\ket_{\ker^{(\ell)}}\Ti{\NTKI}{\delta_1\delta_2}{\ell}\, .
\end{align}
Let's first work this out for the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$}, and then we'll consider the scale-invariant universality class\index{universality class!scale-invariant} for which we'll need to make use of our finite-angle results from \S\ref{sec:finite_angle}. 
Either way, this should be child's play for us at this point.\footnote{An even more childish play would be studying the Bayesian version of generalization error\index{generalization error!exact Bayesian inference} by setting the \terminate{training hyperparameters} according to \eqref{eq:last-layer-bayes-learning-rate} and \eqref{eq:hidden-layer-bayes-learning-rate}, such that $\Ti{\NTKI}{\delta_1\delta_2}{L}=\Ti{\ker}{\delta_1\delta_2}{L}$. In this case, we know exactly how the bias and variance terms of the generalization error behave. This is a very particular setting of the \terminate{training hyperparameters} and unlikely to be optimal in general (cf.~our discussion of the differences between the frozen NTK and Bayesian kernel in terms of feature functions in \S\ref{sec:lazy-kernel}). Indeed for the scale-invariant universality class, we'll explicitly see around \eqref{eq:weird-asymptotic-behavior} that exact Bayesian inference has inferior asymptotic behavior than the more general gradient-based learning.}


\subsubsection{\texorpdfstring{$\Tif{\ker}{}=0$}{K*=0} Universality Class}\index{universality class!K@$K^\star=0$}
Recall~\eqref{eq:useful-much-later-too} from much much earlier describing the decomposition of the Gaussian expectation of two activations in the $\gamma^{[a]}$ basis.\index{$\gamma^{[a]}$ basis!$\sigma \sigma$} With the parallel perturbation turned off, $\ker_{[1]}^{(\ell)}=0$, this expansion reads 
\begin{align}
\braket{\sigma_{\delta_1} \sigma_{\delta_2}}{\ell}=\!\le[\le\langle \sigma\sigma\ri\rangle_{\KML}+\o{\delta^2}\ri] \gamma^{[0]}_{\delta_1\delta_2}\!+\!\le[\Ti{\delta\delta\ker}{[2]}{\ell}\bra \sigma'\sigma'\ket_{\KML}+\o{\delta^4}\ri] \gamma^{[2]}_{\delta_1\delta_2}\, .
\end{align}
With a replacement $\sigma\to\sigma'$, we have a similar decomposition for the Gaussian expectation of the derivatives of activations:\index{$\gamma^{[a]}$ basis!$\sigma^\prime \sigma^\prime$}
\begin{align}\label{eq:primeprimeprimeprime}
\braket{\sigma'_{\delta_1} \sigma'_{\delta_2}}{\ell}=\!\le[\le\langle \sigma'\sigma'\ri\rangle_{\KML}+\o{\delta^2}\ri] \gamma^{[0]}_{\delta_1\delta_2}\!+\!\le[\Ti{\delta\delta\ker}{[2]}{\ell}\bra \sigma''\sigma''\ket_{\KML}+\o{\delta^4}\ri] \gamma^{[2]}_{\delta_1\delta_2}\, .
\end{align}

\index{$\gamma^{[a]}$ basis!frozen NTK}
Plugging these expansions into the full frozen NTK recursion~\eqref{eq:tired-repetition-of-frozen-yogurt} and 
using the component-wise identities $\gamma^{[0]}_{\delta_1\delta_2}\gamma^{[0]}_{\delta_1\delta_2}=\gamma^{[2]}_{\delta_1\delta_2}\gamma^{[2]}_{\delta_1\delta_2}=\gamma^{[0]}_{\delta_1\delta_2}$ and $\gamma^{[0]}_{\delta_1\delta_2}\gamma^{[2]}_{\delta_1\delta_2}=\gamma^{[2]}_{\delta_1\delta_2}$, we get
\be\label{eq:deltadeltaNTK-recursion}
\Ti{\delta\delta\NTKI}{[2]}{\ell+1}=\chi_{\perp}^{(\ell)}\Ti{\delta\delta\NTKI}{[2]}{\ell}+\le(\frac{\LW{\ell+1}}{C_W}\chi_{\perp}^{(\ell)}+C_W\bra \sigma''\sigma''\ket_{\KML} \Ti{\NTKI}{\M\M}{\ell}\ri) \Ti{\delta\delta\ker}{[2]}{\ell}\, ,
\ee
where we've recalled the definition of the \terminate{perpendicular susceptibility}~\eqref{eq:chi-perp}, $\chi_{\perp}^{(\ell)}=C_W\bra \sigma'\sigma'\ket_{\KML}$.\footnote{More generally there are terms proportional to $\le(\delta\ker_{[1]}\ri)^2$ and $\delta\ker_{[1]} \delta\NTKI_{[1]}$ in this recursion for $\delta\delta\NTKI_{[2]}$; however, when training and test inputs have equal norm, $\delta\ker_{[1]}=0$, these terms vanish. 
}

\index{bias-variance tradeoff!for a universality class!K@$K^\star=0$ activations}
We learned long ago that the perpendicular susceptibility governs the behavior of the perpendicular perturbation~$\Ti{\delta\delta\ker}{[2]}{\ell}$, and we see from~\eqref{eq:deltadeltaNTK-recursion} that it also controls the behavior of the frozen NTK component $\Ti{\delta\delta\NTKI}{[2]}{\ell}$. As we alluded to before, the exponential decay/growth of $\Ti{\delta\delta\ker}{[2]}{\ell}$ and $\Ti{\delta\delta\NTKI}{[2]}{\ell}$ are thusly linked. In particular, trying to eliminate the variance term of the generalization error\index{generalization error!variance} by letting $\Ti{\delta\delta\ker}{[2]}{\ell}$ exponentially decay will also cause $\Ti{\delta\delta\NTKI}{[2]}{\ell}$ to exponentially decay, making the model prediction constant, inflexible, and highly biased.

Given this and our previous discussion on the role of the parallel susceptibility $\chi_{\parallel}$, 
let's now tune to criticality $\chi_{\parallel}\!\le(\ker^\star\ri)=\chi_\perp\!\le(\ker^\star\ri) =1$ and evaluate the depth dependence of $\Ti{\delta\delta\NTKI}{[2]}{\ell}$. For the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$}, \terminate{criticality}~\eqref{eq:k-star-equals-zero-critical-initialization} is found by tuning $C_b=0$ and $C_W=\frac{1}{\sigma_1^2}$. %
With these settings, we recall the large-$\ell$ asymptotic solutions from~\eqref{eq:initial-tanh-wthout-subleading} and~\eqref{eq:perp-asymptotic-solution}
\be\label{eq:perp-asymptotic-solution-reprint}
\KML=\le[\frac{1}{(-a_1)}\ri] \frac{1}{\ell}+\ldots\, , \qquad \delta\delta \ker_{[2]}^{(\ell)}= \frac{\delta^2}{\ell^{p_{\perp}}}+\ldots\,  ,
\ee
where $p_{\perp}\equiv b_1/a_1$, and the activation-function dependent constants $a_1$ and $b_1$ were defined in~\eqref{eq:a1} and~\eqref{eq:b1}, and $\delta^2$ is a constant related to the initial separation of the inputs but isn't fixed by the asymptotic analysis. 
Also recall from the more recent past~\eqref{eq:ntk-k-star-perp-susceptibility-asymptotic} that we can asymptotically expand the perpendicular susceptibility as
\be\label{eq:perp-perp-perp-perp}
\chi_{\perp}^{(\ell)}=1-\frac{p_{\perp}}{\ell}+\ldots\, .
\ee
Similarly, by a simple Gaussian integral we can evaluate the following Gaussian expectation,
\be\label{eq:deri-deri-deri-deri}
\bra \sigma''\sigma''\ket_{\KML}=\sigma_2^2+\o{\KML}=\sigma_2^2+\o{\frac{1}{\ell}}\, ,
\ee
remembering our notation $\sigma_2\equiv \sigma''(0)$.


\index{universality class!K@$K^\star=0$}
Next, we also have to make a choice about the \terminate{training hyperparameters} $\Lb{\ell}$ and $\LW{\ell}$. Indeed, the depth scaling of the \terminate{generalization error} will depend on these hyperparameters, a fact that should not be surprising: we expect the selection of our relative learning rates to affect the performance of our model. Let's first follow the guidance of~\S\ref{sec:EVGP-WEP} where we discussed an \neo{equivalence principle} for learning rates, and set these \terminate{training hyperparameters} according to \eqref{eq:super-tanh-general}, i.e.~\eqref{eq:layer-independent-rates} multiplied by $L^{p_{\perp}-1}$:
\be\label{eq:super-tanh-general-reprint}
\Lb{\ell}=\widetilde{\lambda}_b\le(\frac{1}{\ell}\ri)^{p_{\perp}}L^{p_{\perp}-1}\, , \qquad \lamW{\ell}=\widetilde{\lambda}_W\le(\frac{L}{\ell}\ri)^{p_{\perp}-1}\, .
\ee
With such a choice, we have an asymptotic solution for the midpoint frozen NTK\index{frozen NTK!midpoint}, which is the same solution as in \eqref{eq:frozen-ntk-k-star-solution} up to a multiplication by $L^{p_{\perp}-1}$: 
\be\label{eq:frozen-ntk-k-star-solution-reprint}
\Ti{\NTKI}{\M\M}{\ell}=\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]\le( \frac{L}{\ell}\ri)^{p_{\perp}-1}+\ldots\, .
\ee
Further plugging these results
\eqref{eq:perp-asymptotic-solution-reprint}--\eqref{eq:frozen-ntk-k-star-solution-reprint}
into \eqref{eq:deltadeltaNTK-recursion}, we get
\begin{align}\label{eq:frozen-ntk-dd-recursion-k-star}
\delta\delta \NTKI_{[2]}^{(\ell+1)}=&\le[1-\frac{p_{\perp}}{\ell}+\ldots\ri]\delta\delta \NTKI_{[2]}^{(\ell)}\, \\
&+ \delta^2\le\{\widetilde{\lambda}_W\sigma_1^2+\frac{\sigma_2^2}{\sigma_1^2}\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri] \ri\}L^{p_{\perp}-1}\le(\frac{1}{\ell}\ri)^{2p_{\perp}-1}+\ldots\, .\notag
\end{align}
With our usual methods, we can solve this recursion in the asymptotically large-$\ell$ limit with
\be\label{eq:k-star-odd-ddTheta-sol}
\delta\delta \NTKI_{[2]}^{(\ell)}=\delta^2\frac{L^{p_{\perp}-1}}{(2-p_{\perp})}\le\{\widetilde{\lambda}_W\sigma_1^2+\frac{\sigma_2^2}{\sigma_1^2}\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri] \ri\} \le(\frac{1}{\ell}\ri)^{2p_{\perp}-2}+\ldots\, .
\ee
Finally, taking the ratio of the midpoint frozen NTK\index{frozen NTK!midpoint} \eqref{eq:frozen-ntk-k-star-solution-reprint} and the perpendicular perturbation \eqref{eq:k-star-odd-ddTheta-sol} and evaluating at the output layer $\ell=L$,
we find the overall network depth dependence for our robustness measure:
\begin{align}\label{eq:robustness-k-star-sol}
\frac{-2\delta\delta \NTKI_{[2]}^{(L)}}{\NTKI_{\M\M}^{(L)}}&=\frac{2\delta^2}{(p_\perp-2)}\le\{
\frac{\widetilde{\lambda}_W\sigma_1^2}{\le[\widetilde{\lambda}_b+(\widetilde{\lambda}_W\sigma_1^2)/(-a_1)\ri]}
+ \frac{\sigma_2^2}{\sigma_1^2} \ri\}L^{1-p_{\perp}}\propto L^{1-p_{\perp}} \, .
\end{align}
This is astonishing: the desire to keep the robustness measure of order one for very deep networks exactly picks out activation functions in this universality class with $p_{\perp}=1$!\footnote{\label{foot:p-perp-less-than-or-equal-to-zero}Since $p_\perp = b_1 / a_1$, cf.~\eqref{eq:perp-asymptotic-solution}, $b_1 \geq a_1$, cf.~\eqref{eq:a1} and \eqref{eq:b1}, and $a_1 <0$,
    cf.~\eqref{eq:initial-tanh-wthout-subleading}, these  overall imply that $p_\perp \leq 1$ for any $K^\star=0$ activation function. In particular, for a non-odd activation function with $\sigma_2\ne 0$, the exponent for perpendicular perturbations is strictly less than one, $p_{\perp}<1$, and thus the bias term in the generalization error \eqref{eq:bias-factor-gen-error-not-squared} will grow with network depth $L$. %
} Such a condition is satisfied by any odd $K^\star=0$ activation function, such as $\tanhA$ and $\sinA$, both of which we've been discussing prominently throughout the book. %







Now, let's zoom out for a moment and reflect on these calculations more broadly. Somewhat miraculously, the theoretically-motivated tuning of all our hyperparameters -- \neo{criticality} for the \terminate{initialization hyperparameters} and the learning rate \neo{equivalence principle} for the \terminate{training hyperparameters} -- has led to the most practically-optimal solution for the generalization error in this one-training-one-test setting\index{generalization error!optimal hyperparameter tuning}, keeping the bias-variance tradeoff in check. Even more importantly, these choices and solutions are robust across many different network widths and depths, making them quite useful for experimentation and the scaling up of models.\footnote{These tunings are more or less still valid even as we relax the infinite-width requirement to allow nonzero aspect ratio, $L/n \ll 1$.
}
Of course, there really was no miracle: our theoretical principles were practically motivated from the start. 

Now that we understand what we \emph{should} do, let's discuss a different choice of training hyperparameters that we \emph{should not} make. Had we not followed the learning rate \terminate{equivalence principle}, perhaps we would have just made both weight and bias learning rates layer independent as $\lambda_b^{(\ell)} = \lambda_b$ and  $\lamW{\ell}=\lambda_W$. Let's see what happens then, specializing to odd activation functions with $\sigma_2=0$ and $p_{\perp}=1$ for simplicity. In this case, our general formal solution for the single-input frozen NTK~\eqref{eq:ntk-mean-k-star-formal-2} reduces to
\be\label{eq:frozen-ntk-k-star-solution-odd-mis-scaled}
\Ti{\NTKI}{\M\M}{\ell}=\le(\frac{\lambda_b}{2} \ri) \ell +\ldots\, ,
\ee
with a linear dependence on the layer $\ell$, while the same calculation as above with $\sigma_2=0$ and $p_{\perp}=1$ in mind gives a layer-independent constant asymptotic solution for $\delta\delta \NTKI_{[2]}^{(\ell)}$:
\be
\delta\delta \NTKI_{[2]}^{(\ell)}=\lambda_W\sigma_1^2\delta^2+\ldots\, .
\ee
Combined, our robustness measure becomes
\be\label{eq:robustness-k-star-sol-bad}
\frac{-2\delta\delta \NTKI_{[2]}^{(L)}}{\NTKI_{\M\M}^{(L)}}=\le[\frac{-4 \lambda_W\sigma_1^2 \delta^2}{ \lambda_b }\ri] \frac{1}{L} +\ldots\, ,
\ee
which is slowly but surely decaying with the overall depth $L$ of the network. (The consideration of more general activation functions with $p_{\perp} \neq 1$ doesn't change this conclusion.) Therefore, this choice of the training hyperparameters is polynomially suboptimal compared to the choice based on our \terminate{equivalence principle}.\footnote{We leave it to the reader to see how disastrous things would be -- in terms of our one-training-one-test generalization error -- if we had decided not to rescale the weight learning rate by the widths of the previous layer as in~\eqref{eq:diag_LR}, leading to an even more extreme violation of the learning rate \terminate{equivalence principle}.\index{criticality!principle of}\index{equivalence principle!connection to generalization}
}

Reflecting back, when we first discussed the learning rate \terminate{equivalence principle} by staring at our formal solution \eqref{eq:ntk-mean-k-star-formal-2}, we were motivated by the desire to ensure equal contributions to the NTK from each layer. Then in \S\ref{sec:EVGP-WEP} we realized that such choices solve a polynomial version of the exploding and vanishing gradient problem\index{exploding and vanishing gradient problem!connection to generalization}. Here we see the downstream consequences of those choices through the lens of generalization error, giving a solid support for the \terminate{equivalence principle} according to our quantitative measure of training success.










\subsubsection{Scale-Invariant Universality Class}\index{universality class!scale-invariant}\index{$\delta$ expansion}
To analyze scale-invariant activation functions, we need to use results from our 
finite-angle analysis in \S\ref{sec:finite_angle}. In particular, the \terminate{Gaussian expectation} $\bra \sigma''\sigma''\ket$ that appeared in the $\delta$ expansion of $\bra \sigma' \sigma' \ket$ in~\eqref{eq:primeprimeprimeprime} is singular for nonlinear scale-invariant functions due to the kink at the origin, and we promised we'd have to recall our finite-angle results when such a singularity occurs. %



Keeping our promise to you, let's recall a bunch of things from that 
section. First, we decomposed the two-input kernel matrix as \eqref{eq:angle-parametrization}\index{kernel!kernel matrix!polar angle parameterization}
\be\label{eq:angle-parametrization-reprint}
\Ti{\ker}{\delta_1\delta_2}{\ell}=
\begin{pmatrix}
\Ti{\ker}{++}{\ell} & \Ti{\ker}{+-}{\ell} \\
\Ti{\ker}{-+}{\ell}  & \Ti{\ker}{--}{\ell} 
\end{pmatrix}=\Kdi{\ell}\begin{pmatrix}
1 & \cos\!\le(\psi^{(\ell)}\ri)\\
\cos\!\le(\psi^{(\ell)}\ri)  & 1 
\end{pmatrix} \, , \qquad \psi^{(\ell)}\in\le[0,\pi\ri]\, ,
\ee
with two dynamical variables being the  diagonal kernel\index{kernel!kernel matrix!diagonal} $\Kdi{\ell}$ and the polar angle $\psi^{(\ell)}$. With this parametrization in mind, let us reprint a bunch of the previous results that we'll need, \eqref{eq:needed-to-recall-in-kernel-learning-chapter-I}, \eqref{eq:needed-to-recall-in-kernel-learning-chapter-II}, \eqref{eq:evaluated-gaussian-expectation}, and \eqref{eq:johnny-b-goode}:
\begin{align}\label{eq:scale-invariant-finite-identity-1}
\bra \sigma_+\sigma_+\ket_{K^{(\ell)}}=&\bra \sigma_-\sigma_-\ket_{K^{(\ell)}}=A_2 \Kdi{\ell}\, ,\\ %
C_W\bra \sigma'_+\sigma'_+\ket_{K^{(\ell)}}=&C_W\bra \sigma'_-\sigma'_-\ket_{K^{(\ell)}}=C_W A_2\equiv\chi\, , \\ %
\bra \sigma_+\sigma_-\ket_{K^{(\ell)}}=&A_2\Kdi{\ell}\le\{\cos\!\le(\psi^{(\ell)}\ri)+\rho\le[\sin\!\le(\psi^{(\ell)}\ri)-\psi^{(\ell)}\cos\!\le(\psi^{(\ell)}\ri)\ri]\ri\}\, ,\\
C_W\bra \sigma'_+\sigma'_-\ket_{K^{(\ell)}}=&\chi (1-\rho \psi^{(\ell)})\, , %
\label{eq:scale-invariant-finite-identity-4}
\end{align}
where $A_2 \equiv (a_+^2+a_{-}^2)/2$, 
$\rho \equiv \frac{1}{\pi} \frac{\le(a_{+}-a_{-}\ri)^2}{\le(a_{+}^2+a_{-}^2\ri)}$, and $a_{+}$ and $a_{-}$ are the two constants that define the particular activation function (though by now you know that by heart).

Let us now make a similar decomposition for the frozen NTK\index{frozen NTK!polar angle parameterization} as 
\be\label{eq:angle-parametrization-for-ntk}
\Ti{\NTKI}{\delta_1\delta_2}{\ell}=
\begin{pmatrix}
\Ti{\NTKI}{++}{\ell} & \Ti{\NTKI}{+-}{\ell} \\
\Ti{\NTKI}{-+}{\ell}  & \Ti{\NTKI}{--}{\ell} 
\end{pmatrix}=\NTKIdi{\ell}\begin{pmatrix}
1 & \cos\!\le(\zeta^{(\ell)}\ri)\\
\cos\!\le(\zeta^{(\ell)}\ri)  & 1 
\end{pmatrix} \, , \qquad \zeta^{(\ell)}\in\le[0,\pi\ri]\, ,
\ee
with a diagonal frozen NTK $\NTKIdi{\ell}$ and another polar angle $\zeta^{(\ell)}$.

Then, plugging this decomposition \eqref{eq:angle-parametrization-for-ntk} and recollected results \eqref{eq:scale-invariant-finite-identity-1}--\eqref{eq:scale-invariant-finite-identity-4} into the frozen NTK recursion \eqref{eq:tired-repetition-of-frozen-yogurt}, we get coupled recursions for the frozen NTK, casted in our finite-angle parameterization: 
\begin{align}
\NTKIdi{\ell+1}=&\chi\NTKIdi{\ell}+\Lb{\ell+1} + \LW{\ell+1} A_2 \Kdi{\ell}\, ,\label{eq:NTKI-diagonal-recursion}\\
\NTKIdi{\ell+1}\cos\!\le(\zeta^{(\ell+1)}\ri)=&\chi (1-\rho \psi^{(\ell)})\NTKIdi{\ell}\cos\!\le(\zeta^{(\ell)}\ri)\, \label{eq:NTKI-offdiagonal-recursion}\\
&+\Lb{\ell+1}\!\!+ \!\LW{\ell+1}\! A_2\Kdi{\ell}\!\!\le\{\cos\!\le(\psi^{(\ell)}\ri)\!+\rho\le[\sin\!\le(\psi^{(\ell)}\ri)\!-\psi^{(\ell)}\cos\!\le(\psi^{(\ell)}\ri)\ri]\ri\}\, .\notag
\end{align}
We see here in the off-diagonal recursion \eqref{eq:NTKI-offdiagonal-recursion} a finite-angle analog of what we saw perturbatively for the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$} in the infinitesimal-angle recursion~\eqref{eq:deltadeltaNTK-recursion}: the polar angle for the kernel $\psi^{(\ell)}$ sources the finite angle for the frozen NTK $\zeta^{(\ell)}$. Said another way, the exponential growth and decay of the kernel angle $\psi^{(\ell)}$ -- at least for small enough angle -- are linked to  the exponential growth and decay of the frozen-NTK\index{frozen NTK} angle $\zeta^{(\ell)}$, which are in turn linked to the generalized bias-variance tradeoff\index{bias-variance tradeoff!for a universality class!scale-invariant activations}.\index{bias-variance tradeoff!generalized}

\index{universality class!scale-invariant}
With that chain of links in mind (as well as parallel discussions of similar issues in almost every other chapter of this book), it's natural that we should set our \terminate{initialization hyperparameters} by tuning to \terminate{criticality}: $\chi=1$. With this choice, we recall the critical solutions from our finite-angle analysis of the kernel in \S\ref{sec:finite_angle}:
\be\label{eq:scale-invariant-polar-angle-reprint}
\Kdi{\ell}=\Kdif\, , \qquad \psi^{(\ell)} = \le(\frac{3}{\rho}\ri) \frac{1}{\ell}+\ldots\, ,
\ee
where $\Kdif$ is exactly constant, set by the first layer.
Additionally, having already made the case for the learning rate equivalence principle when discussing the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$}, 
let's just simplify our discussion here by setting \terminate{training hyperparameters} according to that \terminate{equivalence principle} for scale-invariant activations~\eqref{eq:super-scale-invariant}: $\Lb{\ell} = \widetilde{\lambda}_b /L$ and $\LW{\ell} = \widetilde{\lambda}_W /L$. With this choice, we see that 
\be\label{eq:frozen-ntk-scale-invariant-equivalence-rates}
\NTKIdi{\ell}=\le(\widetilde{\lambda}_b+\widetilde{\lambda}_W A_2 \Kdif\ri)\frac{\ell}{L}\, 
\ee
solves the recursion for the diagonal frozen NTK~\eqref{eq:NTKI-diagonal-recursion} with the initial condition~\eqref{eq:frozen-ntk-intial}. Importantly, here $\ell$ refers to a particular layer of the network, while $L$ is the overall network depth.\footnote{
    The frozen NTK solution~\eqref{eq:frozen-ntk-scale-invariant-equivalence-rates} is identical to our previous single-input solution~\eqref{eq:frozen-ntk-critical-solution-relu}, here we have just rescaled the bias and weight learning rates by the overall depth, $\lambda_b = \widetilde{\lambda}_b /L$ and $\lambda_W = \widetilde{\lambda}_W /L$, as required by the \terminate{equivalence principle}~\eqref{eq:super-scale-invariant}.
}

Plugging our choice of learning rates, our kernel solution \eqref{eq:scale-invariant-polar-angle-reprint}, and NTK solution \eqref{eq:frozen-ntk-scale-invariant-equivalence-rates} into the finite-angle recursion~\eqref{eq:NTKI-offdiagonal-recursion}, we get
after a bit of rearranging
\begin{align}
\cos\!\le(\zeta^{(\ell+1)}\ri)=\le(1-\frac{4}{\ell}+\ldots\ri) \cos\!\le(\zeta^{(\ell)}\ri)+\le(\frac{1}{\ell}+\ldots\ri)\, ,
\end{align}
which we can see easily is solved by an everything-independent constant
\be
\cos\!\le(\zeta^{(\ell)}\ri)=\frac{1}{4}+\ldots\, .
\ee
Thus, our robustness measure~\eqref{eq:robustness-measure} in the bias term of generalization error for nonlinear scale-invariant activation functions is given by a simple order-one number:
\be\label{eq:weird-asymptotic-behavior}
\frac{\Ti{\NTKI}{-+}{L}}{\Ti{\NTKI}{++}{L}}-1=\cos\!\le(\zeta^{(L)}\ri)-1=-\frac{3}{4}+\ldots\, .
\ee

Similarly, given that the nearby-input analysis can break down for nonlinear scale-invariant activations, let's use our finite-angle analysis here to also work out the variance term of the generalization error \eqref{eq:generalization-variance-expanded}; plugging in the asymptotic falloff for the kernel \eqref{eq:finite-angle-spin-0-solution} and \eqref{eq:finite-angle-spin-2-solution} as well as using \eqref{eq:weird-asymptotic-behavior} for the frozen NTK, we get 
\begin{align}\label{eq:scale-invariant-variance-gen-error}
\cov{\z{i}{-}{L}(T) }{ \z{i}{-}{L}(T)} &=\ker_{--}-2\frac{\NTKI_{-+}}{\NTKI_{++}}\ker_{-+}+\le(\frac{\NTKI_{-+}}{\NTKI_{++}}\ri)^2\ker_{++}\, \\
&= \Kdif \le[1- \cos\!\le(\zeta^{(L)}\ri) \ri]^2=\frac{9}{16}\Kdif+\ldots\, . \notag
\end{align}

Unlike the previous case for $K^\star=0$ activations, these asymptotic results for the generalization error, \eqref{eq:weird-asymptotic-behavior} and \eqref{eq:scale-invariant-variance-gen-error}, don't depend on the \terminate{training hyperparameters} $\widetilde{\lambda}_b$ and $\widetilde{\lambda}_W$, nor do they depend on a constant like $\delta^2$ that knows about the separation of the test and training points. (However, just as we discussed for $\psi^{(\ell)}$ in \S\ref{sec:finite_angle}, the depth at which these asymptotic results become valid does depend on $\delta^2$, the input norm, the activation function, and the training hyperparameters.)
Nonetheless, again with the correct tuning of our hyperparameters based on the principles of criticality and equivalence\index{criticality!principle of}\index{equivalence principle!connection to generalization}, we found a constant bias and variance, giving us the best possible tradeoff when training deep networks with nonlinear scale-invariant activations.\footnote{It is worth noting what happens with the special case of exact Bayesian inference where the only nonzero learning rates are in the last layer in order to set $\NTKI^{(L)}=\ker^{(L)}$. In that case, the robustness measure is given by $\cos\!\le(\psi^{(L)}\ri)-1=\o{1/\ell^2}$. Given this decay with depth, we see that the restricted Bayesian case is clearly inferior to an ensemble of networks that are fully-trained via gradient descent with uniform learning rates across layers. %
}

Let us end with the special remark on \terminate{deep linear network}s. For these networks, we use the $\linear$ activation function with $a_{+}=a_{-}$ and hence have $\rho=0$. In particular, we saw in \S\ref{sec:finite_angle} that not only was the diagonal kernel preserved at criticality, but the polar angle was preserved as well: $\Kdi{\ell}=\Kdif$ and $\psi^{(\ell)} = \psi^{\star}$. Noting this, the off-diagonal recursion for the frozen NTK\index{frozen NTK} \eqref{eq:NTKI-offdiagonal-recursion} then becomes
\be
\NTKIdi{\ell+1}\cos\!\le(\zeta^{(\ell+1)}\ri)=\NTKIdi{\ell}\cos\!\le(\zeta^{(\ell)}\ri)+\frac{\widetilde{\lambda}_b}{L}+\frac{\widetilde{\lambda}_W}{L} A_2\Kdif\cos\!\le(\psi^{\star}\ri)\, .
\ee
This recursion is exactly solved by
\be
\NTKIdi{\ell}\cos\!\le(\zeta^{(\ell)}\ri)=\le[\widetilde{\lambda}_b+\widetilde{\lambda}_W A_2\Kdif\cos\!\le(\psi^{\star}\ri)\ri]\frac{(\ell-1)}{L}+\NTKIdi{1}\cos\!\le(\zeta^{(1)}\ri)\, .
\ee
Dividing this result by our solution for the diagonal frozen NTK\index{frozen NTK} \eqref{eq:frozen-ntk-scale-invariant-equivalence-rates} then gives
\be
\cos\!\le(\zeta^{(\ell)}\ri)=\frac{\widetilde{\lambda}_b+\widetilde{\lambda}_W A_2\Kdif\cos\!\le(\psi^{\star}\ri)}{\widetilde{\lambda}_b+\widetilde{\lambda}_W A_2\Kdif}+\ldots\, ,
\ee
which polynomially asymptotes to a constant.
Unlike the case for the nonlinear scale-invariant activation functions, this constant depends on the observables in the first layer in a rather detailed manner, naturally connecting the generalization properties of the network to the input.
The real limitation of the \terminate{deep linear network}s becomes immediately apparent upon considering their (in)ability to interpolate/extrapolate, which we'll analyze next for linearly and nonlinearly activated MLPs.






 
 





\subsection{Interpolation and Extrapolation}\label{subsec:star-polation}
Rather than focusing primarily on our evaluation criteria for successful training, the \terminate{generalization error}, in this subsection we will focus more on
the kinds of functions that our trained neural networks actually compute. This analysis will enable us to consider the \emph{inductive bias}\index{inductive bias!of activation functions} of different activation functions and tell us how to relate the properties of those activation functions to the properties of the dataset and function that we're trying to approximate. 


In the previous subsection we asked: given the true output $\y{i}{+}$ for an input $\x{i}{+}$, what does a fully-trained MLP in the infinite-width limit predict for the output of a nearby input $\x{i}{-}$? Here, we up the ante and ask: given the true outputs $\y{i}{\pm}$ for \emph{two} inputs $\x{i}{\pm}=\x{i}{\M}\pm \frac{\delta x_i}{2}$, what is the prediction for a one-parameter family\index{one-parameter families} of test inputs,
\be\label{eq:polation-input}
s \x{i}{+}+(1-s)\x{i}{-}=\x{i}{\M}+ \frac{(2s-1)}{2}\delta x_i\equiv \x{i}{(2s-1)}\, ,
\ee
that sit on a line passing through $\x{i}{+}$ and $\x{i}{-}$?
When our parameter $s$ is inside the unit interval $s\in[0,1]$, this is a question about neural-network \term{interpolation}\index{interpolation|seealso{$\ast$-polation}}; for $s$ outside the unit interval, it's a question about \term{extrapolation}\index{extrapolation|seealso{$\ast$-polation}}. For general $s$, let's refer to this collectively as \textbf{$\ast$-polation}.\index{$\ast$-polation|textbf} 

\index{deep linear network!limitations} 
First we'll perform a little exercise in $\ast$-polation with deep linear networks to see what networks with $\linear$ activation functions do. Accordingly, we'll see concretely how deep linear networks approximate a very limited set of functions. Then we'll follow up by assessing smooth nonlinear networks.

\subsubsection{Linear $\ast$-Polation by Deep Linear Networks}\index{$\ast$-polation!deep linear networks}

\index{deep linear network}\index{linear transformations}
There's a very simple way to see how $\ast$-polation works for deep linear networks.
If we recall for a moment (and for one last time) the forward equation for deep linear networks~\eqref{eq:deep-linear-foward-pass},
\be\label{eq:deep-linear-foward-pass-reprint}
\z{i}{\alpha}{\ell+1} = \bias{i}{\ell+1} + \sum_{j=1}^{n_\ell}\W{ij}{\ell+1} \z{j}{\alpha}{\ell} \, ,
\ee
with $\z{j}{\alpha}{0}\equiv\x{j}{\alpha}$,  it's clear that the linear structure in the input \eqref{eq:polation-input} will be preserved from layer to layer. That is, given an $\ell$-th-layer preactivations of the form
\be
\z{i}{(2s-1)}{\ell} = s\z{i}{+}{\ell} + (1-s)\z{i}{-}{\ell}
\ee
that has such a linear structure, we then have for the next layer
\begin{align}
\z{i}{(2s-1)}{\ell+1} &= \bias{i}{\ell+1} + \sum_{j=1}^{n_\ell}\W{ij}{\ell+1}\le[s\z{j}{+}{\ell} + (1-s)\z{j}{-}{\ell}\ri] \, \\
&=s\le( \bias{i}{\ell+1} +    \sum_{j=1}^{n_\ell}\W{ij}{\ell+1}\z{j}{+}{\ell} \ri) + (1-s)\le( \bias{i}{\ell+1} + \sum_{j=1}^{n_\ell}\W{ij}{\ell+1}\z{j}{-}{\ell}\ri) \notag \\
&= s\z{i}{+}{\ell+1} + (1-s)\z{i}{-}{\ell+1} \, \notag ,
\end{align}
which still respects the linear structure. This is just a direct consequence of the fact that deep linear networks compute linear functions of their input.


Therefore, for a test input that's a
linear sum of our two training points \eqref{eq:polation-input}, the network will output the linear sum of the network outputs on the two individual training points:
\be\label{eq:linear-polation-at-init}
\z{i}{(2s-1)}{L}= s\z{i}{+}{L} + (1-s)\z{i}{-}{L} \, .
\ee
This equation holds at initialization as well as at the end of training, which means that any particular fully-trained deep linear network will $\ast$-polate as
\be\label{eq:linear-polation}
\z{i}{(2s-1)}{L}(T) =s \y{i}{+}+(1-s)\y{i}{-}\, ,
\ee
since the fully-trained network output will equal the true output for any element in the training set: $\z{i}{\pm}{L}(T) = \y{i}{\pm}$. With this, we see that fully-trained deep linear network \emph{linearly} $\ast$-polate, no matter what. This is both intuitive and pretty obvious; as deep linear networks perform linear transformations they can only compute linear functions. 

Of course, this is exactly what we said when we studied deep linear networks way back
in \S\ref{ch:deep-linear-eft}.
Here, we explicitly see \emph{why} these networks are limited after training, by showing the (limited) way in which they can use training examples to make predictions. Accordingly, if the function you're trying to approximate is a linear function of the input data, then deep linear networks are a great modeling choice. If the function is nonlinear, we'll have to consider nonlinear activation functions. 
It's not that deep.





















\subsubsection{Nonlinear $\ast$-Polation by Smooth Nonlinear Deep Networks}\index{$\ast$-polation!nonlinear networks}
We'll have to work a little harder to see what nonlinear networks do.\footnote{We would have to work even harder to see what nonlinear scale-invariant activation functions do, so here we'll focus on smooth nonlinear activation functions. For such nonlinear scale-invariant activation functions with kinks, since the $\ast$-polated input $\x{i}{(2s-1)}$ does not have the same norm as $\x{i}{\pm}$ for $s\ne0,1$, we would need to extend the finite-angle analysis from \S\ref{sec:finite_angle} to the case of unequal input norms.
This is left as a challenge in pedagogy to future deep-learning book authors.
} In the last section, we saw that the output of a fully-trained network is given by the stochastic kernel prediction equation~\eqref{eq:kernel-prediction}, which we reprint here for convenience:
\be\label{eq:kernel-prediction-reprint}
\z{i}{\tea}{L}(T)=\z{i}{\tea}{L}- \sum_{\tra_1, \tra_2 \in\A}  \Ti{\NTKI}{\tea \tra_1}{L} \TI{\NTKIsub}{\tra_1 \tra_2}{L}\le(\z{i}{\tra_2}{L}-\y{i}{\tra_2}\ri)\, .
\ee
Thus, we see that to study $\ast$-polation\index{$\ast$-polation} more generally we 
will
need to evaluate elements of the frozen NTK between our test and training set, $\Ti{\NTKI}{(2s-1) \pm}{L}$,
and also need to invert the  two-by-two submatrix of the frozen NTK on the training set only, $\TI{\NTKIsub}{\tra_1 \tra_2}{L}$.


This latter inversion can be easily completed with
the standard textbook formula for the inverse of a two-by-two matrix,
\be\label{eq:frozen-NTK-submatrix-inverse}
\NTKIsub^{\tra_1 \tra_2} =
\frac{1}{\NTKI_{++}\NTKI_{--} - \NTKI_{+-}^2}
\begin{pmatrix}
\NTKI_{--} & -\NTKI_{+-} \\
-\NTKI_{+-}  & \NTKI_{++}  \\
\end{pmatrix}\, ,
\ee
where here we've also used the symmetry $\NTKI_{+-}=\NTKI_{-+}$ and further dropped the \terminate{layer indices}. For the rest of this section, we will always assume that these frozen NTKs are evaluated at the output layer.

Next, to compute the off-diagonal elements between the training set and the test set $\NTKI_{(2s-1) \pm}$, we'll need to generalize our $\delta$ expansion a bit.\index{$\delta$ expansion!generalized to $\epsilon_{1,2}$ expansion}
Let's first recall our expressions for the components of the frozen NTK in $\gamma^{[a]}$ basis, \eqref{eq:NTK-in-gamma-basis}, and then plug in the $\delta$ expansion we performed on the two-by-two submatrix $\NTKIsub_{\tra_1\tra_2}$, \eqref{eq:frozen-NTK-delta-0}--\eqref{eq:frozen-NTK-delta-2}, which gives
\begin{align}
\NTKI_{\pm\pm}=&\NTKI_{\M\M}\pm \delta\NTKI_{[1]}+ \le(\delta\delta\NTKI_{[2]}+\delta\delta\NTKI_{[0]}\ri)+\o{\delta^3}\ ,\label{eq:kernel-diagonal-Taylor-in-delta}\\
\NTKI_{\pm\mp}=&\NTKI_{\M\M}+ \le(-\delta\delta\NTKI_{[2]}+\delta\delta\NTKI_{[0]}\ri)+\o{\delta^3}\ ,\label{eq:kernel-offdiagonal-Taylor-in-delta}
\end{align}
for the pair of inputs $\x{i}{\pm}=\x{i}{\M}\pm \frac{\delta x_i}{2}$. Let's now consider a pair of perturbed inputs of a more general form
\be\label{eq:generic-two-perturabations-for-polation}
\x{i}{\epsilon_1}\equiv\x{i}{\M}+\frac{\epsilon_1}{2}\delta x_{i}\, , \qquad \x{i}{\epsilon_2}\equiv\x{i}{\M}+\frac{\epsilon_2}{2}\delta x_{i} \, .
\ee 
Note that picking $\epsilon_{1,2}$ from $\pm1$ reduces them to $\x{i}{\pm}$,
while the new case of the interest, $\NTKI_{(2s-1) \pm}$, corresponds to setting $\epsilon_1=(2s-1)$ and $\epsilon_2=\pm1$.
For a generic pair of inputs \eqref{eq:generic-two-perturabations-for-polation}, the $\delta$ expansion gets modified as
\begin{align}\label{eq:key-extension}
\NTKI_{\epsilon_1\epsilon_2}=&\NTKI_{\M\M}+\le(\frac{\epsilon_1+\epsilon_2}{2}\ri) \delta\NTKI_{[1]}+\le(\frac{\epsilon_1+\epsilon_2}{2}\ri)^2 \le(\delta\delta\NTKI_{[2]}+\delta\delta\NTKI_{[0]}\ri)\, \\
&+\le(\frac{\epsilon_1-\epsilon_2}{2}\ri)^2 \le(-\delta\delta\NTKI_{[2]}+\delta\delta\NTKI_{[0]}\ri)+\o{\epsilon^3\delta^3}\ .\notag
\end{align}
To see why this is the correct expression, note that \emph{(i)} each term has the right scaling with $\epsilon_{1,2}$, \emph{(ii)} for $\epsilon_1=\epsilon_2=\pm1$ we correctly recover the expression for $\NTKI_{\epsilon_1\epsilon_2}=\NTKI_{\pm\pm}$ \eqref{eq:kernel-diagonal-Taylor-in-delta}, \emph{(iii)} for $\epsilon_1=-\epsilon_2=\pm1$, we correctly recover the expression for $\NTKI_{\epsilon_1\epsilon_2}=\NTKI_{\pm\mp}$ \eqref{eq:kernel-offdiagonal-Taylor-in-delta}, and \emph{(iv)} the expression is symmetric under $\epsilon_1\leftrightarrow\epsilon_2$. The frozen NTK\index{frozen NTK!$\epsilon_{1,2}$ expansion|see{$\delta$ expansion}}\index{frozen NTK!$\delta$ expansion|see{$\delta$ expansion}} component $\NTKI_{\epsilon_1\epsilon_2}$ must satisfy these four constraints, and the expression \eqref{eq:key-extension} is the unique formula that satisfies them all.

Applying this formula to evaluate $\NTKI_{(2s-1)\pm}$ and simplifying a bit, we find
\begin{align}\label{eq:test-train-frozen-ntk-polation}
\NTKI_{(2s-1)\pm}=s \NTKI_{\pm+}+(1-s)\NTKI_{\pm-}-2s(1-s)\delta\delta\NTKI_{[0]}+\o{\delta^3}\, .
\end{align}
As we'll see, the key to nonlinear $\ast$-polation\index{$\ast$-polation}, at least for nearby inputs, is in the $\delta\delta\NTKI_{[0]}$ term.\footnote{N.B.~$\delta\delta\NTKI_{[0]}$ is very different from $\delta\delta\NTKI_{[2]}$: the former is the second term in the expansion of the $\gamma^{[0]}$ component  $\NTKI_{[0]}$, cf.~\eqref{eq:frozen-NTK-delta-0}, while the latter is the first term in the expansion of the $\gamma^{[2]}$ component $\NTKI_{[2]}$, cf.~\eqref{eq:frozen-NTK-delta-2}.}
Firstly, it's clear that this term nonlinearly depends on the test input, as evidenced by $s(1-s)$ prefactor.
Indeed, you can go back and check that this term identically vanishes for deep linear networks, i.e., for those networks we simply have $\NTKI_{(2s-1)\pm}=s \NTKI_{\pm+}+(1-s)\NTKI_{\pm-}$.\footnote{To see this quickly, note that both the first-layer metric \eqref{eq:first-layer-metric} and the first-layer NTK \eqref{eq:NTHinitial} are bilinear in the two inputs, and that such bilinear structure is preserved under the recursions for deep linear networks: $\Ti{\ker}{\delta_1\delta_2}{\ell+1}=\Cb{\ell+1} + \CW{\ell+1}\Ti{\ker}{\delta_1\delta_2}{\ell}$,
cf.~\eqref{eq:K-recursion-reprint},
and $ \Ti{\NTKI}{\delta_1\delta_2}{\ell+1}=\Lb{\ell+1} + \lamW{\ell+1}\Ti{\ker}{\delta_1\delta_2}{\ell}+\CW{\ell+1}\Ti{\NTKI}{\delta_1\delta_2}{\ell}$,
cf.~\eqref{eq:frozen-NTK-recursion}.} With that in mind, it also helps to decompose the initial preactivation into linear and nonlinear pieces as
\be\label{eq:init-preactivation-decomposition-for-polation}
\z{i}{(2s-1)}{L}=s\z{i}{+}{L}+(1-s)\z{i}{-}{L}+\le[\z{i}{(2s-1)}{L}-s\z{i}{+}{L}-(1-s)\z{i}{-}{L}\ri]\, .
\ee
Here, the second term vanishes for deep linear networks, as per \eqref{eq:linear-polation-at-init}, and so in general it captures nonlinearity of the network output at initialization.


Plugging \eqref{eq:frozen-NTK-submatrix-inverse}, \eqref{eq:test-train-frozen-ntk-polation}, and \eqref{eq:init-preactivation-decomposition-for-polation} into our kernel prediction formula \eqref{eq:kernel-prediction-reprint}, we see that our fully-trained prediction on the test input $\x{i}{(2s-1)}=s\x{i}{+}+(1-s)\x{i}{-}$ is given by
\begin{align}
&\z{i}{(2s-1)}{L}(T)\, \label{eq:polation-general}\\
=&\le[\z{i}{(2s-1)}{L}-s\z{i}{+}{L}-(1-s)\z{i}{-}{L}\ri]+\le[s \y{i}{+}+(1-s)\y{i}{-}\ri]\, \notag\\
&-s(1-s)\le[\frac{2\delta\delta\NTKI_{[0]}}{\NTKI_{\M\M}\delta\delta\NTKI_{[2]}-\delta\NTKI_{[1]}^2}\ri]\Big[2\delta\delta\NTKI_{[2]}\le(\z{i}{+}{L}+\z{i}{-}{L}+ \y{i}{+}+\y{i}{-}\ri)\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad-\delta\NTKI_{[1]}\le(\z{i}{+}{L}-\z{i}{-}{L}+\y{i}{+}-\y{i}{-}\ri)\Big]+\o{\delta^3}\, .\notag
\end{align}
Comparing with our linear $\ast$-polation formula \eqref{eq:linear-polation}, we see that both the first and last terms are new: nonlinear networks can \emph{nonlinearly} $\ast$-polate!\index{$\ast$-polation!nonlinear networks}
Interestingly, the fully-trained $\ast$-polation for nonlinear activation functions depends on the network output at initialization through the nonlinearity $\z{i}{(2s-1)}{L}-s\z{i}{+}{L}-(1-s)\z{i}{-}{L}$; in contrast, for deep linear networks the $\ast$-polation only depended on the true output of the training examples. %

As a particular illustration of this formula, consider the case when the two training inputs have the same norm. In such a case $\NTKI_{[1]}=0$, and we find a much simpler formula:
\begin{align}\label{eq:polation-equal-norm-inputs}
\z{i}{(2s-1)}{L}(T)=&\le[\z{i}{(2s-1)}{L}-s\z{i}{+}{L}-(1-s)\z{i}{-}{L}\ri]+\le[s \y{i}{+}+(1-s)\y{i}{-}\ri]\, \\
&-4s(1-s)\le(\frac{\delta\delta\NTKI_{[0]}}{\NTKI_{\M\M}}\ri) \le(\z{i}{+}{L}+\z{i}{-}{L}+ \y{i}{+}+\y{i}{-}\ri)+\o{\delta^3}\, .\notag
\end{align}
Averaging over our ensemble, this prediction has a mean
\be\label{eq:equal-norm-polation-mean}
\GDGPmean_{i;(2s-1)}=s \y{i}{+}+(1-s)\y{i}{-}-4s(1-s)\le(\frac{\delta\delta\NTKI_{[0]}}{\NTKI_{\M\M}}\ri)\le(\y{i}{+}+\y{i}{-}\ri)+\o{\delta^3}\, .
\ee
Here the first term in \eqref{eq:polation-equal-norm-inputs} that captured the nonlinearity of the network output at initialization vanished under the expectation, and so the nonlinearity of the $\ast$-polation mean is entirely captured by the  dimensionless ratio $\delta\delta\NTKI_{[0]}/\NTKI_{\M\M}$. 



\index{$\ast$-polation!curvature}
So, what kind of a function is our fully-trained infinite-width nonlinear neural network computing? 
To assess this, note that the ratio $\delta\delta\NTKI_{[0]}/\NTKI_{\M\M}$ captures the \emph{curvature} of the $\ast$-polation in the neighborhood of the training points.\footnote{Note that as the two training samples  begin to coincide $x_\pm \to x_\M$, the curvature vanishes quadratically $\delta\delta\NTKI_{[0]}/\NTKI_{\M\M} =\o{\delta^2}$, and the closer the $\ast$-polation will be to a linear $\ast$-polation. Further applying our generalized $\delta$ expansion~\eqref{eq:key-extension} to the kernel, we can show that that the variance of the $\ast$-polation vanishes even more quickly in this coincident limit as
\begin{align}
 \E{\z{i}{2s-1}{L}(T)\z{i}{2s-1}{L}(T)}-\le(\E{\z{i}{2s-1}{L}(T)}\ri)^2=\o{\delta^3}\, .
\end{align}
}\index{$\ast$-polation!curvature}
This curvature encodes a non-universal \emph{inductive bias}\index{inductive bias!of activation functions} of the activation function and architecture indicating how this class of function approximators will generalize to novel data.

For a given task and dataset, some activation functions might produce a more desired type of $\ast$-polation. This can be measured directly via the bias term in the generalization error.\index{generalization error!bias!related to $\ast$-polation} Substituting in our equal norm expression for the mean~\eqref{eq:equal-norm-polation-mean},
\begin{align}\label{eq:polation-bias}
\GDGPmean_{i;(2s-1)}-\y{i}{(2s-1)} =& \le[\y{i}{+}+(1-s)\y{i}{-}-\y{i}{(2s-1)}\ri]  \notag \\
      &-4s(1-s)\le(\frac{\delta\delta\NTKI_{[0]}}{\NTKI_{\M\M}}\ri)\le(\y{i}{+}+\y{i}{-}\ri)+\o{\delta^3} \, ,
\end{align}
we see that this generalization error bias decomposes into a comparison between the nonlinearity in the true output -- given by the first square brackets -- and the network curvature around the midpoint of the true output $\le(\y{i}{+}+\y{i}{-}\ri)/2$.
With this framing, \terminate{deep linear network}s promote a very particular type of inductive bias: only linear functions are computed. %
More generally, we could (but won't here)
compute and solve a recursion for $\delta\delta\NTKI_{[0]}$ for any particular activation function in order to learn more about the kinds of functions computed by deep networks with that activation function.

Finally, note that this analysis doesn't make any particular distinction between \emph{interpolation} and \emph{extrapolation}\index{$\ast$-polation!same for inter- and extra-}, and also that as $s\to 0,1$, the $\ast$-polation \eqref{eq:polation-general} reduces to $\y{i}{\pm}$ with absolute certainty\index{absolute certainty!$\ast$-polation}. In fact, in the neighborhood of $s=0,1$, the $\ast$-polation bias \eqref{eq:polation-bias} has much in common with the prediction bias \eqref{eq:bias-factor-gen-error-not-squared} and \eqref{eq:robustness-measure-plugged-in} that we saw in \S\ref{subsec:robustness-from-infinite-GD} when considering a training set consisting of only one training sample. Importantly, it is the most nearby training point that contributes the most to a test point's prediction.

Taken as a guide to thinking about larger training sets, the \emph{local} nature of these predictions is highly suggestive of some ways to make further progress. On the one hand, we might be able to make theoretical progress on more complicated prediction formulae by weighting the predictions given by nearby training points to a given test point, perhaps using an approximation from \S\ref{subsec:robustness-from-infinite-GD} when there's only one nearby training point and using $\ast$-polation \eqref{eq:polation-general} when there's a nearby pair. On the other hand, we might be able to make practical progress on training-set design -- given the network's inductive biases -- by using this kind of analysis to inform how best to sample training inputs over the data manifold.












































\section{Linear Models and Kernel Methods}\label{sec:lazy-kernel} 
Before we back off the infinite-width limit, let's take a section to place what we've done in this chapter into the broader context of \terminate{machine learning}. In the next chapter, such a context will help us understand  the ways in which deep learning at finite width is qualitatively quite different from its infinite-width counterpart.

In particular, in this section we'll explain a \emph{dual}\index{duality!linear model -- kernel methods} way of thinking about the class of models that can be described by a kernel prediction\index{kernel methods!prediction!as a linear model} formula such as \eqref{eq:kernel-prediction}.  On the one hand, kernel predictions can be thought of as being made by $\ast$-polating the training data using the kernel. On the other hand, we can think of them as the output of a trained model that's linear in its parameters. The former perspective has been more natural to us, given that we always consider an ensemble over the model parameters and then integrate them out. So let's begin by explaining the latter \neo{linear model} perspective.\footnote{
The connection between infinite-width networks trained by gradient descent and kernel methods was pointed out in \cite{jacot2018neural} in the context of introducing the NTK. Following that, an extended discussion of such networks as linear models was given in  \cite{brainNTK2019}. %
}






\subsection{Linear Models}\label{subsec:linear-models}

The simplest linear model -- and perhaps the simplest machine learning model -- is just a one-layer (i.e.~zero-hidden-layer) network
\be\label{eq:dumb-linear-model-def}
z_i(x_{\delta}; \theta) =b_i+\sum_{j=1}^{n_0} W_{ij} \x{j}{\delta} \, .
\ee
While this model is linear in both the parameters $\theta = \{b_i,  W_{ij}\}$ and the input $\x{j}{\delta}$, the \emph{linear} in \emph{linear model} takes its name from the dependence on the parameters $\theta$ and not the input $x$. In particular, while the components of the input samples $\x{j}{\delta}$ sometime can serve as a reasonable set of features for function approximation\index{function approximation!for linear models}, in general they do not.
Indeed, considering how much ink we've already spilled on \terminate{representation group flow} and \terminate{representation learning} in the context of deep learning, it's natural to expect that we would need to (pre-)process the input data before it's useful for any machine learning task. 


One traditional way to fix this, inherited from statistics, is to engineer better features. Such an approach was necessary when computers were less powerful and models had to be much simpler to optimize.
For instance, in addition to the features $x_j$ perhaps it would also be useful for the model to take into account features $x_j x_k$ that let us consider the dependence of one component upon another. More generally, we might design a fixed set of \index{feature function|textbf}\textbf{feature functions} $\fea_j(x)$ that's meant to work well for the dataset\index{input data} $\D$ and the underlying task at hand.\footnote{
    These type of feature functions are also useful if the input $x$ is something abstract -- such as a document of text -- and thus needs to be transformed into a numerical vector before it can be processed by a parameterized model.\index{feature function!feature engineering for abstract inputs}
} 


In this traditional approach, the hope is that all the complicated modeling work goes into the construction of these feature functions $\fea_j(x)$ and, if we do a good enough job, then its associated \term{linear model},
\be\label{eq:linear-model-def}
z_i(x_{\delta}; \theta) =b_i+ \sum_{j=1}^{n_f} W_{ij} \fea_j(x_{\delta})=\sum_{j=0}^{n_f}W_{ij}\fea_j(x_{\delta}) \, ,
\ee
is simple to train, easy to interpret, and performs well on the desired task.
Here, we've
followed a customary notational reductionism, subsuming the bias vector into the weight matrix by setting $\fea_0(x)\equiv1$ and $W_{i0}\equiv b_i$. Thus, the output $z_i(x; \theta)$ of a linear model
depends linearly on the model parameters $\theta$, 
consisting of a combined weight matrix $W_{ij}$ of dimension $n_{\text{out}}\times (n_{f}+1)$. We can still think of this model as a one-layer neural network, but in this case we pre-process each input with the function $\fea_j(x)$ before passing it through the network.



Now let's explain how to learn the optimal values for weight matrix $W_{ij}^\star$ given a training set $\A$. The most common approach is to minimize the MSE loss\index{loss!MSE!for linear models}
\be\label{eq:linear-regression}
\L_{A}(\theta)=\frac{1}{2}\sum_{\tra\in\A}\sum_{i=1}^{n_{\text{out}}}\le[\y{i}{\tra}-z_i(x_{\tra}; \theta)\ri]^2=\frac{1}{2}\sum_{\tra\in\A}\sum_{i=1}^{n_{\text{out}}}\le[\y{i}{\tra}-\sum_{j=0}^{n_f}W_{ij} \fea_j(x_{\tra})\ri]^2\, .
\ee
Supervised learning\index{supervised learning!with linear models|see{linear regression}} with a linear model is known as \term{linear regression}\index{linear regression|seealso{linear model}}, and -- as the MSE loss of a linear model is necessarily quadratic in the model parameters -- this is another case of an analytically-solvable learning problem \eqref{eq:gradient-vanishing-mind}. Taking the derivative of $\L_{\A}$ with respect to the parameters and setting it to zero, we get an implicit equation that determines the optimal weight matrix  $W_{ij}^\star$:
\be\label{eq:linear-model-implicit-expression}
\sum_{k=0}^{n_f}W^{\star}_{ik}\le[\sum_{\tra\in\A}\fea_k(x_{\tra})\fea_j(x_{\tra})\ri]=\sum_{\tra\in\A} \y{i}{\tra}\fea_{j}(x_{\tra})\, .
\ee
To solve this equation, let's define a symmetric $(n_f+1)$-by-$(n_f+1)$ matrix of features,
\be\label{eq:no-good-matrix-name}
M_{ij}\equiv\sum_{\tra\in\A}\fea_i(x_{\tra})\fea_j(x_{\tra})\, ,
\ee
with elements that give a pairwise aggregation of feature functions summed over all the training samples $\tra \in \A$.
Then, applying its inverse to both sides of the implicit expression~\eqref{eq:linear-model-implicit-expression}, we find a solution:
\be\label{eq:linear-regression-optimal}
W^{\star}_{ij}=\sum_{k=0}^{n_f}\sum_{\tra\in\A} \y{i}{\tra}\fea_{k}(x_{\tra}) \le(M^{-1}\ri)_{kj}\, .
\ee
Notice that the solution depends on the training set, linearly for the true function values $\y{i}{\tra}$ and in a more complicated way on the input features $\fea_{k}(x_{\tra})$.\footnote{
    If the number of features $(n_f +1)$ is larger than the size of the training set $\NR$, then the model is \emph{overparameterized}\index{overparameterization}, and $M_{ij}$ is not uniquely invertible. One scheme to specify the solution is to add a regularization\index{regularization!for linear models} term of the form $a \sum_{ij} W_{ij}^2$ to the loss \eqref{eq:linear-regression}, cf.~footnote~\ref{footnote:regularization-recursion} in \S\ref{subsec:NTKprediction} for a related discussion of regularization for infinite-width networks. In this modified regression problem, we can then invert the regularized matrix
    \be\label{eq:inverse-of-M-regularized}
   M_{ij} = 2a\, \delta_{ij} + \sum_{\tra\in\A}\fea_i(x_{\tra})\fea_j(x_{\tra})\, ,
    \ee
and send the regulator to zero, $a \to 0^+$, at the end of our calculations. 
Note that either when the regulator $a$ is kept finite or when we're in the \emph{underparameterized}\index{underparameterization} regime with $(n_f+1) < \NR$, the linear model will no longer reach zero training loss even when fully optimized.\label{footnote:inverse-existing}
}
Finally, we can use this fully-trained linear model with its associated optimal parameters $W^{\star}_{ij}$ to make predictions on novel test-set inputs $x_{\tea}$ as
\be\label{eq:kernel-prediction-kernel-methods-before-duality}
z_i\big(x_{\tea}; \theta^\star\big)=   \sum_{j=0}^{n_f} W_{ij}^\star \fea_j(x_{\tea}) \, , %
\ee
giving us a closed-form solution for our \terminate{linear regression} problem. %
Importantly, after learning is complete we can simply store the optimal parameters $W^{\star}_{ij}$ and forget about the training data.





\subsection{Kernel Methods}\label{subsec:kernel-methods}


While this is all very easy, it's less familiar in our book since we typically do not work explicitly with the parameters. To cast our linear model into a more familiar form, let's consider a \emph{dual}\index{duality} expression for the solution. First, let's substitute our expression for the optimal parameters $W^{\star}_{ij}$, \eqref{eq:linear-regression-optimal}, into our linear regression solution, \eqref{eq:kernel-prediction-kernel-methods-before-duality}, giving
\be\label{eq:kernel-prediction-kernel-methods-before-duality-square}
z_i\big(x_{\tea}; \theta^\star\big) = \sum_{\tra\in \A} \le[\sum_{j,k=0}^{n_f}\fea_{j}(x_{\tea}) \le(M^{-1}\ri)_{jk}\fea_{k}(x_{\tra})\ri]\y{i}{\tra} \, .
\ee
Note that the expression in the square brackets involves the inversion of an $(n_f+1)\times(n_f+1)$-dimensional matrix $M_{ij}$, which was required to obtain the optimal parameters $W^{\star}_{ij}$. 
This works well if the number of features is small, but if the number of feature functions we defined is very large $n_f \gg 1$, then representing and inverting such a matrix might be computationally difficult.

However, it turns out that we actually don't need to do any of that. To see why, let us introduce a new $\ND\times\ND$-dimensional symmetric matrix:
\be\label{eq:kernel-with-features}
\kerm_{\delta_1\delta_2}\equiv\kerm\!\le(x_{\delta_1},x_{\delta_2} \ri) \equiv \sum_{i=0}^{n_{f}} \fea_i\!\le(x_{\delta_1}\ri) \fea_i\!\le(x_{\delta_2}\ri) \, .
\ee
As an inner product of feature functions, $\kerm_{\delta_1\delta_2}$ is a measure of similarity between two inputs $x_{i;\delta_1}$ and $x_{i;\delta_2}$ in \terminate{feature space}. Such a measure of similarity is called a \textbf{kernel}\index{kernel methods!kernel}.\footnote{
For instance, in the case of the simplest linear model \eqref{eq:dumb-linear-model-def}, the kernel is just given by  the inner product between the two inputs
\be\label{eq:linear-kernel-methods}
\kerm_{\delta_1\delta_2} \equiv \sum_{i=1}^{n_0} x_{i;\delta_1} x_{i;\delta_2} \, ,
\ee
which is often called the \emph{linear kernel}\index{kernel methods!kernel!linear}.
}
In a way that should feel very familiar, we'll also denote an $\NR$-by-$\NR$-dimensional submatrix of the kernel evaluated on the training set as $\kermsub_{\tra_1 \tra_2}$ with a tilde. This lets us write its inverse as $\widetilde{\kerm}^{\tra_1 \tra_2}$, which satisfies
\be
\sum_{\tra_2 \in \A} \kermsub^{\tra_1 \tra_2} \kermsub_{\tra_2 \tra_3} = \delta^{\tra_1}_{\ \tra_3} \, .
\ee
Note that given the definition of the kernel \eqref{eq:kernel-with-features}, for this inverse to exist and for this equation to hold we must be in the \emph{overparameterized}\index{overparameterization} regime with $(n_f+1) \ge \NR$. 


Now with this, let's see how we might rearrange the factor in the square brackets of our solution \eqref{eq:kernel-prediction-kernel-methods-before-duality-square}. Multiplying it by the submatrix $\kermsub_{\tra \tra_1}$, we can simplify this factor as
\begin{align}\label{eq:kernel-trick}
&\sum_{\tra\in\A}\le[\sum_{j,k=0}^{n_f}\fea_{j}(x_{\tea}) \le(M^{-1}\ri)_{jk}\fea_{k}(x_{\tra})\ri] \kermsub_{\tra\tra_1}\, \\
=&\sum_{\tra\in\A}\sum_{j,k=0}^{n_f}\fea_{j}(x_{\tea}) \le(M^{-1}\ri)_{jk}\fea_{k}(x_{\tra}) \sum_{i=0}^{n_{f}} \fea_i\!\le(x_{\tra}\ri) \fea_i\!\le(x_{\tra_1}\ri)\, \notag\\
=&\sum_{i, j,k=0}^{n_f}\fea_{j}(x_{\tea}) \le(M^{-1}\ri)_{jk}M_{ki}\, \fea_i\!\le(x_{\tra_1}\ri)\, \notag\\
=&\sum_{i=0}^{n_f}\fea_{i}(x_{\tea})\fea_i\!\le(x_{\tra_1}\ri)=\kerm_{\tea\tra_1}\, .\notag
\end{align}
To get this result, in the second line we plugged in the definition of the kernel \eqref{eq:kernel-with-features}, in the third line we performed the sum over $\tra$ using the definition of the feature matrix $M_{ij}$ \eqref{eq:no-good-matrix-name}, and in the last equality of the fourth line we again used the definition of the kernel. Finally, multiplying the first and last expressions by the inverse submatrix $\widetilde{\kerm}^{\tra_1 \tra_2}$, we get a new representation for the factor in the square brackets
\be\label{eq:kernel-trick-result}
\le[\sum_{j,k=0}^{n_f}\fea_{j}(x_{\tea}) \le(M^{-1}\ri)_{jk}\fea_{k}(x_{\tra_2})\ri]=\sum_{\tra_1\in\A}\kerm_{\tea\tra_1}\kermsub^{\tra_1\tra_2}\, ,
\ee
which lets us rewrite the prediction of our linear model \eqref{eq:kernel-prediction-kernel-methods-before-duality} as
\be\label{eq:kernel-prediction-kernel-methods}
z_i\big(x_{\tea}; \theta^\star\big)= \sum_{\tra_1,\tra_2 \in \A} \kerm_{\tea \tra_1} \widetilde{\kerm}^{\tra_1 \tra_2} \y{i}{\tra_2} \, .
\ee
When the prediction of a linear model is computed in this way, it's known as a \emph{kernel machine}\index{kernel machine|see{kernel methods}} or \term{kernel methods}.

Note that in this dual\index{duality} expression of the solution, the optimal parameters $W^{\star}_{ij}$ and the feature functions $\phi_i(x)$ don't appear. Thus, we've successfully exchanged our feature-space quantities, an $(n_f+1)$-dimensional feature vector and the inverse of an $(n_f+1)\times(n_f+1)$-dimensional matrix, for sample-space quantities, an $\NR$-dimensional vector $ \kerm_{\tea \tra_1}$ and the inverse of an $\NR \times \NR$-dimensional matrix $\widetilde{\kerm}_{\tra_1 \tra_2}$.\footnote{In some situations,\label{footnote:kernel-vs-feature-functions} specifying and evaluating the kernel is much simpler than specifying and evaluating the feature functions. For instance, the \emph{Gaussian kernel}\index{kernel methods!kernel!Gaussian}, given by
\be\label{eq:gaussian-kernel-methods}
\kerm_{\delta_1\delta_2} \equiv \exp\!\le[- \frac{1}{2 \sigma^2} \sum_{i=1}^{n_0} \le(x_{i;\delta_1} - x_{i;\delta_2}\ri)^2  \ri]\, ,
\ee
implies an infinite-dimensional feature space, but can be evaluated by simply computing the squared distance between the $n_\text{in}$-dimensional input vectors and then exponentiating the result. (To see why the Gaussian kernel implies an infinite number of feature functions, we can express the squared distance as a sum of three inner products and then Taylor expand the exponential in those inner products; the terms in the Taylor expansion give the feature functions.) 
In this way, we see how computing the kernel can be far easier than representing the features explicitly.

In fact, any algorithm based on a linear kernel \eqref{eq:linear-kernel-methods} can be generalized by swapping the simple kernel for a more complicated kernel like the Gaussian kernel \eqref{eq:gaussian-kernel-methods}.
This is known as the \textbf{kernel trick}\index{kernel trick|see{kernel methods}}\index{kernel methods!kernel trick} and is a way to describe in the language of kernel methods how we generalized our simplest linear model \eqref{eq:dumb-linear-model-def} -- that was linear in the input -- to the more general linear model \eqref{eq:linear-model-def} -- that was nonlinear in the input.
}
This works because in our solution \eqref{eq:kernel-prediction-kernel-methods}, we actually only care about the inner product of the feature functions -- i.e.~the kernel -- and not the values of the features themselves. 


By writing the linear model's prediction in terms of the kernel in \eqref{eq:kernel-prediction-kernel-methods}, we can interpret the prediction in terms of direct comparison with previously-seen examples. In particular, this solution computes the similarity of a new test input $x_{\tea}$ with all the training examples with $ \kerm_{\tea \tra_1}$ and then uses that similarity to linearly weight the true function values  from the training set $\y{i}{\tra_2}$ with the sample-space metric $\kermsub^{\tra_1 \tra_2}$.
For this reason, kernel methods are sometimes referred to as \emph{memory-based} methods\index{memory-based method|see{kernel methods}}\index{kernel methods!as a memory-based method} since they involve memorizing the entire training set.\footnote{Often, for a particular kernel method to be tractable, the model's predictions are made \emph{locally}, incorporating information mostly from the training samples nearest to the test sample of interest. Given the $\ast$-polation results of last section, it's not hard to imagine how such methods could be made to work well.

A canonical example of such a local method is \emph{$k$-nearest neighbors}\index{k-nearest neighbors@$k$-nearest neighbors|see{kernel methods}}\index{kernel methods!$k$-nearest neighbors} \cite{fix1952discriminatory,Cover1967NearestNP}, which is a special type of kernel method.  By only considering nearby training points, these kinds of local algorithms can skirt some of the impracticality that we've been pointing out for our exact Bayesian inference predictions as well as for our frozen NTK predictions.
It would be interesting to extend such local algorithms to the finite-width exact Bayesian inference that we discussed in \S\ref{sec:finite-posterior} or the finite-width gradient-based learning prediction that we'll discuss in \S\ref{ch:eot}.
}
This should be contrasted with the parameterized linear model solution \eqref{eq:kernel-prediction-kernel-methods-before-duality}, where we forget the training set samples and instead just explicitly store the optimal parameter values $W^{\star}_{ij}$. 


Finally, note that 
there's ``no wiring'' in the prediction: the $z_i$ component of the prediction is entirely determined by the $y_i$ component of the training examples.
This is only implicit in the optimal weight matrix $W^{\star}_{ij}$ \eqref{eq:linear-regression-optimal} in the linear model solution \eqref{eq:kernel-prediction-kernel-methods-before-duality} but is explicit in the kernel method solution \eqref{eq:kernel-prediction-kernel-methods}.
This is one of many ways that such linear models and kernel methods are limited machine learning models.










\subsection{Infinite-Width Networks as Linear Models}\label{subsec:linear-at-infinity}

Surely, the kernel methods' prediction formula \eqref{eq:kernel-prediction-kernel-methods}  should seem awfully familiar to you:  it is precisely the same as the exact Bayesian mean prediction \eqref{eq:GP-mean} if we identify the kernel methods' kernel $k_{\delta_1\delta_2}$ with the Bayesian kernel $\Ti{\ker}{\delta_1\delta_2}{L}$, and it is exactly the same as the (neural tangent) kernel mean prediction~\eqref{eq:GD-frozen-mean} if we identify the kernel methods' kernel $k_{\delta_1\delta_2}$ with the frozen neural tangent kernel\index{frozen NTK} $\Ti{\NTKI}{\delta_1\delta_2}{L}$.\footnote{You may have noticed that our stochastic (neural tangent) kernel prediction\index{kernel methods!prediction} formula \eqref{eq:kernel-prediction} also depended on the network output at initialization and had a nonzero covariance. This is related to our earlier discussion in footnote~\ref{footnote:inverse-existing} that, when we're in the \emph{overparameterized}\index{overparameterization} regime with $(n_f+1)>\NR$, as is especially the case when we have an infinite number of features, the $(n_f+1)$-by-$(n_f+1)$ matrix matrix $M_{ij} \equiv\sum_{\tra\in\A}\fea_i(x_{\tra})\fea_j(x_{\tra})$  \eqref{eq:no-good-matrix-name} does not have a unique inverse. Thus, in this regime, the optimal weight matrix $W^{\star}$ is not unique: if we don't use the regulation trick \eqref{eq:inverse-of-M-regularized} to uniquely pick out one of the solutions, 
the prediction in the dual\index{duality} kernel description 
will have a dependence on the model's initialization.
} 
This finally provides a justification for the names of these objects as well as for the name of the current chapter.



Indeed, there is a very direct connection between these traditional linear models and kernel methods on the one hand and our (neural tangent) kernel learning of infinite-width models on the other hand.
 First, let's discuss the simpler case of the Bayesian kernel.
 As pointed out in \S\ref{subsec:NTKprediction}, this choice corresponds to treating only output-layer biases $b_{i}^{(L)}$ and weights $W_{ij}^{(L)}$ as trainable model parameters, and so the network output at initialization is given by
\be\label{eq:linear-model-BI}
z_i^{(L)}\le(x_{\delta};\theta\ri)=b_{i}^{(L)}+\sum_{j=1}^{n_{L-1}}W_{ij}^{(L)}\sigma^{(L-1)}_{j;\delta}=\sum_{j=0}^{n_{L-1}}W_{ij}^{(L)}\sigma^{(L-1)}_{j;\delta}\, ,
\ee
where on the right-hand side we defined $\sigma^{(L-1)}_{0;\delta}\equiv1$ and $W_{i0}^{(L)}\equiv b_i^{(L)}$. This is almost the same as our linear model~\eqref{eq:linear-model-def} except that the feature functions are \emph{random}: $\widehat{\fea}_{j;\delta} \equiv \sigma^{(L-1)}_{j;\delta}$.\index{feature function!random}
In particular, here we've \emph{hatted} these feature functions to emphasize that  they depend on the parameters in the hidden layers that are sampled from the initialization distribution at the beginning and then \emph{fixed}; this is sometimes called a \term{random feature model}.

In this case, the kernel methods' notion of the kernel is also stochastic\index{kernel methods!stochastic kernel}\index{kernel methods!stochastic kernel|seealso{random feature model}}
\begin{align}\label{eq:stochastic-kernel}
\widehat{k}_{\delta_1\delta_2}=&\Cb{L}\widehat{\fea}_{0;\delta_1}\widehat{\fea}_{0;\delta_2}+\frac{\CW{L}}{n_{L-1}}\sum_{j=1}^{n_{L-1}}\widehat{\fea}_{j;\delta_1}\widehat{\fea}_{j;\delta_2}\, \\
=&\Cb{L}+\CW{L}\le(\frac{1}{n_{L-1}}\sum_{j=1}^{n_{L-1}}\sigma^{(L-1)}_{j;\delta_1}\sigma^{(L-1)}_{j;\delta_2}\ri)\, ,\notag
\end{align}
where in the first line we have re-weighted the terms in the feature sum in the definition of the kernel \eqref{eq:kernel-with-features} by $\Cb{L}$ and $\CW{L}/ n_L$.\footnote{
    A more general definition of the kernel methods' kernel \eqref{eq:kernel-with-features} allows us to weight the contribution of each pair of feature functions as
\be\label{eq:kernel-with-features-weighted}
\kerm_{\delta_1\delta_2} \equiv \sum_{i,j=0}^{n_{f}} c_{ij}\, \fea_i\!\le(x_{\delta_1}\ri) \fea_j\!\le(x_{\delta_2}\ri) \, .
\ee
}
Note that we called this object \eqref{eq:stochastic-kernel} the \emph{stochastic metric}\index{metric!stochastic} \eqref{eq:general-stochastic-metric} when studying the RG flow of preactivations.
Now, taking an expectation over the initialization ensemble, in the infinite-width limit we have
\be
\E{\widehat{k}_{\delta_1\delta_2}}= \Cb{L}+\CW{L} \braket{\sigma_{\delta_1}\sigma_{\delta_2}}{L-1}=K^{(L)}_{\delta_1\delta_2}\, ,
\ee
where in the last equality we used the recursion for the kernel \eqref{eq:kernel-recursion-reminder-reprint}.\footnote{
    Note alternatively, by the central limit theorem, that the stochastic kernel $\widehat{k}_{\delta_1\delta_2}$ will be equal to the kernel $K^{(L)}_{\delta_1\delta_2}$ in the infinite-width limit without explicitly averaging over initializations. This \emph{self-averaging}\index{self-averaging} of the kernel is equivalent to the fact that the connected four-point correlator vanishes at infinite width. Here we see that such self-averaging of the kernel can also be thought of as arising from a sum over an infinite number of random features.
} In this way, we see how we can interpret exact Bayesian inference at infinite width as a simple linear model~\eqref{eq:linear-model-BI} of fixed random features.\index{feature function!random}\index{Bayesian inference!connection to linear models}







Now, let's give a linear model interpretation to gradient-based learning at infinite width\index{frozen NTK!features}. 
Since a linear model is linear in its parameters, we can more generally define the \textbf{random features}\index{feature function!random} by
\be\label{eq:feature-function-stochastic}
 \widehat{\fea}_{i,\mu}(x_{\delta}) \equiv \frac{\td \z{i}{\delta}{L}}{d \theta_\mu}  \, .
\ee
To be clear, this derivative is evaluated at initialization and these features are thus fixed in the infinite-width limit.
Explicitly, for an MLP the random features are given by
\begin{align}
 \widehat{\phi}_{i, \W{k_1 k_2}{\ell}}\!\!\!(x_{\delta}) &=  \le(\sum_{j_{L-1}, \ldots, j_{\ell+1}}\W{i j_{L-1}}{L} \ds{j_{L-1}}{\delta}{L-1} \cdots \W{j_{\ell+1} k_1}{\ell+1} \ds{k_1}{\delta}{\ell}\ri) \s{k_2}{\delta}{\ell-1} \, ,
 \label{eq:NTK-weight-features}
\end{align}
with the bias component given by setting $\s{0}{\delta}{\ell}\equiv1$ and $\W{k 0}{\ell}\equiv\bias{k}{\ell}$. As is apparent from this expression, these features are stochastic, depending on the specific values of the biases and weights at initialization. Note also that the Bayesian linear model \eqref{eq:linear-model-BI} only uses a subset of these features, $ \widehat{\phi}_{i, \W{i j}{L}}=\s{j}{\delta}{L-1}$, and thus is a much more limited and less expressive model.


Note further that these feature functions\index{feature function} $\widehat{\phi}_{i,\mu}(x_{\delta})$ are related to, but not exactly equivalent to,  the previous notion of feature we gave when we discussed \terminate{representation group flow} in \S\ref{sec:marginalization-group-flow}. In that case, our $\ell$-th-layer features corresponds to $\ell$-th-layer preactivations $\z{i}{\delta}{\ell}$ or activations $\s{i}{\delta}{\ell}$. However, here we see that the random feature functions~\eqref{eq:NTK-weight-features}
are proportional to $(\ell-1)$-th-layer activations $\s{i}{\delta}{\ell-1}$ but are also multiplied by objects from deeper layers.\footnote{
Going forward, when referring to the \emph{features} of a network, we will mean the kernel methods' notion of a feature function $\widehat{\fea}_{i,\mu}(x_{\delta})$,
rather than an activation $\sigma_i^{(\ell)}(x_\delta)$.\index{feature!vs.~feature function}\index{kernel methods!feature|see{feature function}}\index{representation learning!as the evolution of feature functions} Accordingly, we will now understand \neo{representation learning} to describe how such feature functions develop a data dependence during training.
}





As should be clear, the stochastic kernel\index{kernel methods!kernel!stochastic} associated with these features,
\begin{align}\label{eq:stochastic-kernel-ntk}
\widehat{\kerm}_{ij;\delta_1\delta_2}\equiv  \sum_{\mu,\nu}\lambda_{\mu\nu} \, \widehat{\phi}_{i,\mu}(x_{\delta_1}) \widehat{\phi}_{j,\nu}(x_{\delta_2})=\sum_{\mu,\nu}\lambda_{\mu\nu}\frac{\td \z{i}{\delta_1}{L}}{d \theta_\mu}\frac{\td \z{j}{\delta_2}{L}}{d \theta_\nu} \equiv \Tia{\NTK}{ij}{\delta_1\delta_2}{L}\, ,
\end{align}
is just the $L$-th-layer stochastic NTK~\eqref{eq:midNTH-definition}.\footnote{
    However please note importantly that at finite width the stochastic neural tangent \emph{kernel} $\widehat{k}_{ij;\delta_1\delta_2}=\Tia{\NTK}{i_1i_2}{\delta_1\delta_2}{L}$ is not fixed during training -- learning useful features from the data -- and randomly varies across initializations; hence  -- despite its name -- it is not actually a kernel.} Here, we have taken advantage of our more general definition of the kernel methods' kernel \eqref{eq:kernel-with-features-weighted} to incorporate the learning-rate tensor $\lambda_{\mu\nu}$ into the expression. Accordingly, at infinite width the NTK is frozen and diagonal in final layer neural indices, giving
\begin{align}
\kerm_{\delta_1\delta_2}\equiv  \sum_{\mu,\nu}\lambda_{\mu\nu} \, \widehat{\phi}_{i,\mu}(x_{\delta_1}) \, \widehat{\phi}_{i,\nu}(x_{\delta_2})=\sum_{\mu,\nu}\lambda_{\mu\nu}\frac{\td \z{i}{\delta_1}{L}}{d \theta_\mu}\frac{\td \z{i}{\delta_2}{L}}{d \theta_\nu}\equiv \Ti{\NTKI}{\delta_1\delta_2}{L} \, .
\end{align}
In this way, we see that at infinite width the fully-trained mean network output is just a linear model based on random features \eqref{eq:feature-function-stochastic}.\index{feature function!random}\index{infinite-width limit!connection to linear models}
In this sense, infinite-width neural networks are rather shallow in terms of model complexity, however deep they may appear.

Looking back, when we discussed the linear model at the beginning of this section, we had to introduce feature functions $\phi_i(x)$, designed using our knowledge of the task and data at hand, as a way to (pre-)process the input. This way, the parametric model that we learn is really simple, i.e.~linear.\footnote{Please don't confuse our discussion of \emph{linear models}\index{linear model} here and our discussion of linear vs.~nonlinear functions as in~\S\ref{subsec:star-polation}.

A linear model is a model that's linear in the model parameters \eqref{eq:linear-model-def} and has a dual\index{duality} kernel description that's linear in the true outputs $y_{i;\tra}$ in the training set $\A$ \eqref{eq:kernel-prediction-kernel-methods}; as we have seen, linear models are very simple and easy to solve analytically. As is clear from the definition~\eqref{eq:linear-model-def}, a linear model in general will be \emph{nonlinear} in the inputs $x$ for general nonlinear feature functions $\phi_i(x)$. Accordingly, linear models can compute nonlinear functions of their input. We saw this explicitly when we worked out how nonlinear $\ast$-polation works for smooth nonlinear networks in~\eqref{eq:polation-general}.  

In contrast, a \terminate{deep linear network} is a neural network that uses a $\linear$ activation function. Such networks compute linear functions of their input and thus may only linearly $\ast$-polate between training points, cf.~\eqref{eq:linear-polation}. However, since they are \emph{not} linear models (for $L>1$), the function they compute depends nonlinearly on the model parameters. Accordingly, their training dynamics can be somewhat complicated, and at finite width they even exhibit representation learning\index{representation learning!for deep linear networks}.\index{linear model!is not a deep linear network} %
}
We then reinterpreted this fit linear model in terms of its associated kernel, which itself has a natural interpretation as measuring similarity between our designed features.


However, for infinite-width networks we didn't \emph{design} the frozen NTK, and its associated features are \emph{random}. Instead, the network is defined by the architecture, hyperparameters, and biases and weights, and the path from those variables to the NTK and kernel prediction is filled with calculations. So the abstraction of the actual neural network seems like a very odd way to design a kernel.



Indeed, we've just learned that infinite-width neural networks can only make predictions that are linear in the true outputs from the training set; they are linear models that can only compute linear combinations of random features.
Of course, deep learning is exciting because it works on problems where classic machine learning methods have failed; it works in cases where we don't know how to design feature functions or kernels, or doing so would be too complicated. 
For neural networks to go beyond the kernel methods, they need to be able to \emph{learn} useful features \emph{from the data}, not just make use of a complicated linear combination of random features.\index{representation learning!vs.~kernel learning} 
Thus, in order for the feature functions \eqref{eq:feature-function-stochastic} to evolve during training 
-- that is, in order to have \emph{representation learning} -- we will need to go beyond kernel methods.



Luckily, finite-width networks are not kernel methods. Please now turn to the next chapter to find out exactly what they are instead.
























 











