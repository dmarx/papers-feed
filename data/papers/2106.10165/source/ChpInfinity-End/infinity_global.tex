
\strangechapter{$\infty$}{The End of Training}
\label{ch:eot}\index{end of training}

\epigraph{The job of a scientist is to listen carefully to nature, not to tell nature how to behave.}{Freeman Dyson, explaining Richard Feynman's approach %
\cite{feynman2006classic}.}



\noindent{}In this chapter, we'll finally finish our leading-order effective-theory analysis 
of finite-width networks and solve their training dynamics under gradient descent.
In contrast to the infinite-width limit, for which the solution is independent of the training algorithm,  the dynamics of such deep networks have a rich phenomenology that captures the different ways in which useful features may develop over the course of training.
The solution to these training dynamics  gives first-principles description of the ensemble of fully-trained finite-width networks, realizing a main goal of the book.




Unfortunately, our job will be disrupted by two facts of nature: \emph{(i)} in order to have a consistent description of training dynamics at order $1/n$, we'll need to incorporate two additional objects that arise in the Taylor expansion
of the update to the network output to third order, the update to the NTK to second order, and the update to the dNTK to first order; and 
\emph{(ii)} due to a lack of smoothness, we won't be able to describe the dynamics of $\relu$ networks nor networks consisting of any of the other nonlinear activation functions from the scale-invariant universality class.\index{universality class!scale-invariant}




As for the first
point,
while the analysis of representation learning in the context of the quadratic model was illuminating,
we've already telegraphed that it was insufficient to capture the particular details of finite-width networks. In particular, 
to leading order in $1/n$, there are two more NTK differentials, which we'll refer to as \neo{ddNTKs}.
Although it's straightforward, working out the stochastic forward equations, recursions, and effective theory for these ddNTKs is somewhat tedious, and no longer has any pedagogical value. As such, we won't provide the details of our derivations -- you've already seen these sets of manipulations three times before in \S\ref{ch:ngp}--\S\ref{ch:eft-mlp}, \S\ref{ch:NTKa}--\S\ref{ch:eft-ntk}, and \S\ref{sec:dNTK-RG}--\S\ref{sec:dNTK-criticality}, for the preactivations, NTK, and dNTK, respectively -- instead we'll simply state the results, leaving the details for you as a kind of post-training test evaluation; after all, this is the end of your training as well.



As for the second
point,
throughout the book we've had to use special methods in order to work out exceptional explanations for any non-smooth activation function such as the $\relu$. In our minds, this extra work was justified by the $\relu$'s current privileged status as one of the most popular activation functions in practice. 
However, we have finally run out of tricks and will have to give up: for a reason that is simple to explain, our Taylor expansion in the global learning rate $\eta$ will break down when applied to the dynamics of networks built with non-smooth activation functions.
Instead, we'll have to follow the direction of the community and begin thinking again about smoothed versions of the $\relu$ -- though only the ones that permit a type of \terminate{criticality} -- such as the $\gelu$ and the $\swish$.

With both those disruptions to our work heard, in \S\ref{sec:ddNTKs} we'll present all the relevant results for the ddNTKs -- we'll define them, we'll give their tensor decomposition, and we'll explain their scaling with width and depth -- while hiding all the irrelevant details at the back of the chapter in \S\ref{sec:gross-ddNTK-things}. 
If you've been paying attention, you'll not be shocked to hear that -- when properly normalized -- the \terminate{ddNTKs} scale as the effective theory cutoff:  $\ell/n$. 
This scaling indicates that we need to consider the joint statistics of the preactivation-NTK-dNTK-ddNTKs in order to understand the leading-order finite-width dynamics of deep MLPs. 
Importantly, these ddNTKs endow the dNTK with its own dynamics; from the parameter-space perspective\index{parameter space} of \S\ref{subsec:nonlinear-models}, this means that the \emph{meta feature functions}\index{meta feature function!dynamical} of the model will now evolve.






With those results stated, in \S\ref{sec:another-leap} we'll return to our regularly scheduled pedagogy and, at long last,
solve the training dynamics at finite width. 
After an initial false start following our infinite-width giant leap, first in  \S\ref{subsec:giant-plus-small} we'll learn how to take a small step following an adjusted  giant leap,
giving us our first finite-width solution.
Then in \S\ref{subsec:real-GD-at-finite-width}, we'll analyze many many steps of vanilla gradient descent, giving us our second finite-width solution. The nonlinear dynamics at finite width ultimately lead to a dependence of the fully-trained solution on the training algorithm, and so the solutions derived in these two subsections actually exhibit meaningful differences. 

In particular, the function approximation of a fully-trained finite-width network can be decomposed into a universal part, independent of the optimization details, and a set of \emph{algorithm projectors}\index{algorithm projector}, whose functional form encodes the entire dependence of the solution on the training algorithm. 
These projectors provide a dual sample-space perspective on the learning algorithm, analogous to the relationship between the model parameters and the different kernels.

Accordingly, in  \S\ref{subsec:prediction-at-finite-width} we'll discuss how these projectors impact the solution, letting us understand the inductive bias of the \emph{training dynamics}\index{training dynamics!inductive bias}\index{inductive bias!of learning algorithms}\index{inductive bias!of learning algorithms|seealso{algorithm projector}} separately from the inductive bias of the \emph{network architecture}. We'll also further analyze the predictions made by such fully-trained networks, considering the growing tradeoff between increased representation learning and increased instantiation-to-instantiation fluctuations with network depth.


While this is the final chapter of the main text, in a small epilogue following this chapter, Epilogue~\ref{epi:overparameterization}, we'll explore 
how to define model complexity for overparameterized networks from our effective theory's macroscopic perspective. Then in two appendices, we'll further touch on some topics that are outside the scope of our main line of inquiry. In Appendix~\ref{app:mi-stuff}, we'll introduce the framework of information theory, which will give us the tools we need in order 
to estimate the \terminate{optimal aspect ratio} that separates \emph{effectively-deep}\index{effectively deep} networks from \emph{overly-deep}\index{overly deep} networks. In Appendix~\ref{app:residual}, we'll apply our effective theory approach to learn about residual networks and see how they can be used to extend the range of effectively-deep networks to greater and greater depths.








\section{Two More Differentials}\label{sec:ddNTKs}
\epigraph{Who ordered that?}{I. I. Rabi, quipping about the $\o{1/n}$ \terminate{ddNTKs}.
\index{Rabi, Isidor Isaac}
}


\noindent{}One last time, let's expand the $\ell$-th-layer preactivations after a parameter update, this time recording terms up to \emph{third order}:
\begin{align}\label{eq:preactivation-change-third-order-in-model-parameter}
\dz{i}{\delta}{\ell}\equiv&\z{i}{\delta}{\ell}(t=1)-\z{i}{\delta}{\ell}(t=0)\, \\
=&\sum_{\ell_1=1}^{\ell} \sum_{\mu}\frac{d\z{i}{\delta}{\ell}}{d\theta_{\mu}^{(\ell_1)}}\dtheta_{\mu}^{(\ell_1)}+\frac{1}{2}\sum_{\ell_1, \ell_2 = 1}^\ell \sum_{\mu_1,\mu_2}\frac{d^2\!\z{i}{\delta}{\ell}}{d\theta^{(\ell_1)}_{\mu_1}d\theta_{\mu_2}^{(\ell_2)}}\dtheta_{\mu_1}^{(\ell_1)}\dtheta_{\mu_2}^{(\ell_2)} \notag\\
&+\frac{1}{6}\sum_{\ell_1, \ell_2, \ell_3 = 1}^\ell \sum_{\mu_1,\mu_2,\mu_3}\frac{d^3\!\z{i}{\delta}{\ell}}{d\theta^{(\ell_1)}_{\mu_1}d\theta_{\mu_2}^{(\ell_2)}d\theta_{\mu_3}^{(\ell_3)}}\dtheta_{\mu_1}^{(\ell_1)}\dtheta_{\mu_2}^{(\ell_2)}\dtheta_{\mu_3}^{(\ell_3)}+\ldots\, .\notag
\end{align}
For gradient descent, also recall that after use of the chain rule \eqref{eq:chain-rule-rules}, the change in the $\ell_a$-th-layer parameters of any particular network is given by 
\eqref{eq:parameter-update-rewritten-error-factor},
\be\label{eq:parameter-update-rewritten-error-factor-reprint}
\dtheta_{\mu}^{(\ell_a)}=-\eta\sum_{\nu}\lambda_{\mu\nu}^{(\ell_a)}\le(\sum_{j,k,\tra}\frac{\partial\L_{\A}}{\partial \z{k}{\tra}{L}}\frac{\td\z{k}{\tra}{L}}{\td \z{j}{\tra}{\ell}}\frac{d\z{j}{\tra}{\ell}}{d\theta_{\nu}^{(\ell_a)}}\ri)=-\eta\sum_{\nu,j,\tra}\lambda_{\mu\nu}^{(\ell_a)}\,\Tia{\epsilon}{j}{\tra}{\ell}\frac{d\z{j}{\tra}{\ell}}{d\theta_{\nu}^{(\ell_a)}}\, ,
\ee
where
we've used our convention from \S\ref{sec:dNTK}
of explicitly specifying which layer each parameter comes from.
Please also recall from there
that the learning-rate tensor\index{learning rate!learning-rate tensor} $\lambda_{\mu\nu}^{(\ell)}$ only connects the parameters within a given layer $\ell$. In the above expression, $\ell$ is an intermediate layer such that $\ell_a \leq \ell$, and we also used our
\emph{$\ell$-th-layer error factor}\index{error factor!$\ell$-th-layer} \eqref{eq:ellth-error-chain}:
\be\label{eq:ellth-error-chain-reprint}
\Tia{\epsilon}{j}{\tra}{\ell}\equiv \sum_{k=1}^{n_{L}}\frac{\partial\L_{\A}}{\partial \z{k}{\tra}{L}}\frac{d\z{k}{\tra}{L}}{d\z{j}{\tra}{\ell}}= \frac{\td\L_{\A}}{\td \z{j}{\tra}{\ell}}\, .
\ee
After substituting the parameter update \eqref{eq:parameter-update-rewritten-error-factor-reprint} back into the preactivation update \eqref{eq:preactivation-change-third-order-in-model-parameter}, you should be able to write it in the form
\begin{align}\label{eq:preactivation-updated-finite-width-refined}
\dz{i}{\delta}{\ell}=&-\eta\sum_{j,\tra}\Tia{\NTK}{ij}{\delta\tra}{\ell}\Tia{\epsilon}{j}{\tra}{\ell}+\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2}\Tia{\dNTK}{i j_1j_2}{\delta\tra_1\tra_2}{\ell}  \Tia{\epsilon}{j_1}{\tra_1}{\ell}\Tia{\epsilon}{j_2}{\tra_2}{\ell}\, \\
&-\frac{\eta^3}{6}\sum_{j_1,j_2,j_3,\tra_1,\tra_2,\tra_3}\Tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}{\ell}  \Tia{\epsilon}{j_1}{\tra_1}{\ell}\Tia{\epsilon}{j_2}{\tra_2}{\ell}\Tia{\epsilon}{j_3}{\tra_3}{\ell}\, , \notag\\
&+\o{\eta^4}\, \notag
\end{align}
where the first two terms we found in the last chapter \eqref{eq:preactivation-updated-finite-width}, and the cubic term is new, with the \emph{first} of the \term{ddNTKs} defined as
\begin{align}
\Tia{\ddNTK}{i_0i_1i_2i_3}{\delta_0 \delta_1\delta_2\delta_3}{\ell}\equiv&\sum_{\ell_1, \ell_2,\ell_3=1}^\ell\,\, \sum_{\substack{\mu_1,\nu_1, \\ \mu_2,\nu_2, \\ \mu_3,\nu_3} }\lambda_{\mu_1\nu_1}^{(\ell_1)}\lambda_{\mu_2\nu_2}^{(\ell_2)}\lambda_{\mu_3\nu_3}^{(\ell_3)}\frac{d^3\!\z{i_0}{\delta_0}{\ell}}{d\theta^{(\ell_1)}_{\mu_1}d\theta^{(\ell_2)}_{\mu_2}d\theta^{(\ell_3)}_{\mu_3}}\frac{d\z{i_1}{\delta_1}{\ell}}{d\theta^{(\ell_1)}_{\nu_1}}\frac{d\z{i_2}{\delta_2}{\ell}}{d\theta^{(\ell_2)}_{\nu_2}}\frac{d\z{i_3}{\delta_3}{\ell}}{d\theta^{(\ell_3)}_{\nu_3}} \, .\label{eq:ddNTK-definition}
\end{align}
As always, the hat on the ddNTK indicates that it's stochastic, depending on the particular realization of the model parameters at initialization. Also, similar to the dNTK, this ddNTK is totally symmetric in its second, third, and fourth paired set of indices $(i_1, \delta_1) \leftrightarrow (i_2, \delta_2) \leftrightarrow (i_3, \delta_3)$, while the first neural-sample index $(i_0, \delta_0)$ is distinguished from the other three.

By expanding to order $\eta^3$, the update, \eqref{eq:preactivation-updated-finite-width-refined}, is now \emph{cubic} in error factors. Expanding to this order is necessary because the first ddNTK has statistics at initialization that are $\o{1/n}$, and so needs to be included in our analysis. However, any higher-order terms in the update are subleading, so we may replace $\o{\eta^4} = \o{1/n^2}$ in this expression.

Just as we had to expand the update to the NTK to order $\eta$ when we expanded the update to the preactivations to order $\eta^2$, we will now have to expand the update to the NTK to order $\eta^2$ for the dynamics with our cubic update \eqref{eq:preactivation-updated-finite-width-refined} to be consistent:
\begin{align}
\dbar\Tia{\NTKM}{i_1i_2}{\delta_1\delta_2}{\ell}\equiv&\Tia{\NTKM}{i_1i_2}{\delta_1\delta_2}{\ell}(t=1)-\Tia{\NTKM}{i_1i_2}{\delta_1\delta_2}{\ell}(t=0)\, \label{eq:NTK-updated-finite-width-refined}\\
=&\sum_{\ell_1=1}^\ell \sum_{\mu_1}\frac{d\Tia{\NTKM}{i_1i_2}{\delta_1\delta_2}{\ell}}{d\theta_{\mu_1}^{(\ell_1)}}\dtheta_{\mu_1}^{(\ell_1)}+\sum_{\ell_1,\ell_2=1}^\ell \sum_{\mu_1,\mu_2}\frac{d^2\!\Tia{\NTKM}{i_1i_2}{\delta_1\delta_2}{\ell}}{d\theta_{\mu_1}^{(\ell_1)}d\theta_{\mu_2}^{(\ell_2)}}\dtheta_{\mu_1}^{(\ell_1)}\dtheta_{\mu_2}^{(\ell_2)}+\ldots\, \nonumber\\
=&-\eta\sum_{j,\tra}\le(\Tia{\dNTK}{i_1 i_2j}{\delta_1\delta_2\tra}{\ell}+\Tia{\dNTK}{i_2i_1j}{\delta_2\delta_1\tra}{\ell}\ri)\Tia{\epsilon}{j}{\tra}{\ell}\, \notag\\
&+\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2}\le[\Tia{\ddNTK}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2}{\ell}+\Tia{\ddNTK}{i_2i_1j_1j_2}{\delta_2\delta_1\tra_1\tra_2}{\ell}\ri]\Tia{\epsilon}{j_1}{\tra_1}{\ell}\Tia{\epsilon}{j_2}{\tra_2}{\ell}\, \notag\\
&+ \eta^2 \sum_{j_1,j_2,\tra_1,\tra_2}\Tia{\ddNTKII}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2}{\ell}
\Tia{\epsilon}{j_1}{\tra_1}{\ell}\Tia{\epsilon}{j_2}{\tra_2}{\ell}
+\o{\eta^3}\, .\notag
\end{align}
To go to the final equality, we substituted in our NTK definition \eqref{eq:ell-layer-ntk-def-with-layer-indices} and our parameter update \eqref{eq:parameter-update-rewritten-error-factor-reprint}, computed the derivatives, and then collected the terms. To do so, we identified the \emph{second} of the \term{ddNTKs}, defined as
\begin{align}
\Tia{\ddNTKII}{i_1i_2i_3i_4}{\delta_1 \delta_2\delta_3\delta_4}{\ell}\equiv&\sum_{\ell_1, \ell_2,\ell_3=1}^\ell\,\, \sum_{\substack{\mu_1,\nu_1, \\ \mu_2,\nu_2, \\ \mu_3,\nu_3} }\lambda_{\mu_1\nu_1}^{(\ell_1)}\lambda_{\mu_2\nu_2}^{(\ell_2)}\lambda_{\mu_3\nu_3}^{(\ell_3)}\frac{d^2\!\z{i_1}{\delta_1}{\ell}}{d\theta^{(\ell_1)}_{\mu_1}d\theta^{(\ell_3)}_{\mu_3}}\frac{d^2\z{i_2}{\delta_2}{\ell}}{d\theta^{(\ell_2)}_{\mu_2}d\theta^{(\ell_3)}_{\nu_3}}\frac{d\z{i_3}{\delta_3}{\ell}}{d\theta^{(\ell_1)}_{\nu_1}}\frac{d\z{i_4}{\delta_4}{\ell}}{d\theta^{(\ell_2)}_{\nu_2}} \, .\label{eq:ddNTKII-definition}
\end{align}
The hat on this ddNTK indicates that it's also stochastic at initialization, and we will soon detail that it also has $\o{1/n}$ statistics at leading order. Finally, $\tia{\ddNTKII}{i_1i_2i_3i_4}{\delta_1 \delta_2\delta_3\delta_4}$ has a more constrained symmetry, only symmetric under a joint swap of the paired set of indices as $(i_1, \delta_1)\leftrightarrow (i_2, \delta_2)$ \emph{and} $(i_3, \delta_4) \leftrightarrow  (i_4, \delta_4)$. However, this means that we can also swap indices as
\be\label{eq:symmetry-of-ddNTKII}
\sum_{j_1,j_2,\tra_1,\tra_2}\Tia{\ddNTKII}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2}{\ell}\Tia{\epsilon}{j_1}{\tra_1}{\ell}\Tia{\epsilon}{j_2}{\tra_2}{\ell}=\sum_{j_1,j_2,\tra_1,\tra_2}\Tia{\ddNTKII}{i_2i_1j_1j_2}{\delta_2\delta_1\tra_1\tra_2}{\ell}\Tia{\epsilon}{j_1}{\tra_1}{\ell}\Tia{\epsilon}{j_2}{\tra_2}{\ell}\, ,
\ee
which we used to simplify the final line of the NTK update  \eqref{eq:NTK-updated-finite-width-refined}.


Overall, this NTK update is now \emph{quadratic} in error factors, making its dynamics coupled and nonlinear.
Again, expanding to this order is necessary because both ddNTKs have statistics at initialization that are $\o{1/n}$, and so both need to be included in our analysis.  However, any higher-order terms in the NTK update are subleading, so in \eqref{eq:NTK-updated-finite-width-refined} we may replace $\o{\eta^3} = \o{1/n^2}$.


Finally, consider the leading-order update to the dNTK\index{differential of the neural tangent kernel}:
\begin{align}%
\label{eq:dNTK-updated-finite-width}
\dbar\Tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}{\ell}\equiv&\Tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}{\ell}(t=1) - \Tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}{\ell}(t=0) \, \\
=&\sum_{\ell_1=1}^\ell \sum_{\mu_1}\frac{d\Tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}{\ell}}{d\theta_{\mu_1}^{(\ell_1)}}\dtheta_{\mu_1}^{(\ell_1)}+\ldots\, \notag\\
=&-\eta\sum_{j,\tra}\le(\Tia{\ddNTK}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra}{\ell}+\Tia{\ddNTKII}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra}{\ell}+\Tia{\ddNTKII}{i_0i_2 i_1j}{\delta_0\delta_2\delta_1\tra}{\ell}\ri)\Tia{\epsilon}{j}{\tra}{\ell}\,\notag \\
&+\o{\eta^2}\, \notag .
\end{align}
To go to the final equality, we substituted our dNTK definition \eqref{eq:dNTK-definition} and parameter update \eqref{eq:parameter-update-rewritten-error-factor-reprint}, took the derivative, and then collected all the terms using our ddNTK definitions, \eqref{eq:ddNTK-definition} and \eqref{eq:ddNTKII-definition}. The dNTK update is \emph{linear} in error factors and its dynamics will be the simplest. Finally, the higher-order terms in the dNTK update are subleading, so in \eqref{eq:dNTK-updated-finite-width} we may replace $\o{\eta^2} = \o{1/n^2}$. We also would like to apologize for the $\dbar \dNTKM$ notation, and we promise that we won't have to use it again.

The updates \eqref{eq:preactivation-updated-finite-width-refined}, \eqref{eq:NTK-updated-finite-width-refined}, and \eqref{eq:dNTK-updated-finite-width} comprise the complete set of finite-width updates at $\o{1/n}$. Thus, to proceed further in our analysis, we'll need to work out the leading-order statistics of the ddNTKs.












\subsubsection{ddNTK Statistics}\index{representation group flow!of the ddNTKs}
To find the distribution of fully-trained finite-width networks, we need the joint distribution of the network output, the NTK, the dNTK, and the ddNTKs:\index{ddNTKs!statistics}
\be\label{eq:finite-width-joint-earlier}
p\!\le(z^{(L)}, \, \NTK^{(L)},\, \dNTK^{(L)},\, \ddNTK^{(L)} , \,\ddNTKII^{(L)}   \Big\vert \D\ri) \, .
\ee
Rather than working through the details here, we'll do it in our own private notebooks. You already have all the tools you need: you can follow \S\ref{ch:ngp}, \S\ref{ch:NTKa}, and \S\ref{sec:dNTK-RG} for examples of how to use RG flow\index{representation group flow} to work out the recursions; and you can follow \S\ref{ch:eft-mlp}, \S\ref{ch:eft-ntk}, and \S\ref{sec:dNTK-criticality} for examples of how to work out the details of the effective theory at initialization after tuning to \terminate{criticality}. The full details of this distribution \eqref{eq:finite-width-joint-earlier} can be found at the end of the chapter in \S\ref{sec:gross-ddNTK-things}. Here, we'll highlight the results that you need in order to understand our dynamical computations in the following section. 

After working out the stochastic forward equations for both ddNTKs -- feel free to flip forward to \eqref{eq:ddNTK-forward-equation} and \eqref{eq:ddNTK-II-forward-equation} if you're curious about them -- we'll need to find recursions for its statistics. For these tensors, the leading-order statistics come from their means; their cross correlations and their variances are all subleading. When evaluating the mean of each of the ddNTKs,
it will become convenient to decompose them  into the following set of tensors with sample indices only:\index{tensor decomposition!ddNTKs $R$/$S$/$T$/$U$}
\begin{align}\label{eq:decomposition-ddNTK}
\E{\Tia\ddNTK{i_0i_1i_2i_3}{\delta_0\delta_1\delta_2\delta_3}{\ell}} \equiv&\frac{1}{n_{\ell-1}}\le[\delta_{i_0i_1}\delta_{i_2i_3}\ddNTKR{\delta_0\delta_1\delta_2\delta_3}{\ell}+\delta_{i_0i_2}\delta_{i_3i_1}\ddNTKR{\delta_0\delta_2\delta_3\delta_1}{\ell}+\delta_{i_0i_3}\delta_{i_1i_2}\ddNTKR{\delta_0\delta_3\delta_1\delta_2}{\ell}\ri]\, , \\
\E{\Tia\ddNTKII{i_1i_2i_3i_4}{\delta_1\delta_2\delta_3\delta_4}{\ell}}\equiv&\frac{1}{n_{\ell-1}}\le[\delta_{i_1i_2}\delta_{i_3i_4}\ddNTKS{\delta_1\delta_2\delta_3\delta_4}{\ell}+\delta_{i_1i_3}\delta_{i_4i_2}\ddNTKT{\delta_1\delta_3\delta_4\delta_2}{\ell}+\delta_{i_1i_4}\delta_{i_2i_3}\ddNTKU{\delta_1\delta_4\delta_2\delta_3}{\ell}\ri]\, .
\label{eq:decomposition-ddNTK-II}
\end{align}
Thus, there are four new objects whose recursions we need to determine and whose  scaling with depth we'll need to compute.

The ddNTKs both vanish in the first layer, just like the dNTK. Proceeding then from the second layer to deeper layers,
after a bunch of tedious algebra and plenty of flipping around in the book to make various substitutions, you can find recursions for $\ddNTKRS^{(\ell)}$, $\ddNTKSS^{(\ell)}$, $\ddNTKTS^{(\ell)}$, and $\ddNTKUS^{(\ell)}$: \eqref{eq:R-recursion}, \eqref{eq:S-recursion}, \eqref{eq:T-recursion}, and \eqref{eq:U-recursion}, respectively. You can flip forward to take a look at them, but you probably won't want to\dots. Importantly, these recursions in conjunction with the decompositions \eqref{eq:decomposition-ddNTK} and \eqref{eq:decomposition-ddNTK-II} altogether demonstrate the both ddNTKs have nontrivial order-$1/n$ statistics:
\be
\E{\Tia\ddNTK{i_0i_1i_2i_3}{\delta_0\delta_1\delta_2\delta_3}{\ell}} = \oninv\, ,\qquad \E{\Tia\ddNTKII{i_1i_2i_3i_4}{\delta_1\delta_2\delta_3\delta_4}{\ell}} = \oninv \, .
\ee
Thus, as we've been forecasting, we must include them in our finite-width analysis of training dynamics.


Focusing on the single-input statistics, we again make the layer-independent choices $\Cb{\ell}=C_b$, $\CW{\ell}=C_W$, and $n_1=\cdots=n_{L-1}\equiv n$. Ignoring contributions that are subleading in $1/n$, in particular replacing the mean metric by the kernel $G^{(\ell)} \to \ker^{(\ell)}$ and the NTK mean by the frozen NTK $\NTKM^{(\ell)} \to \NTKI^{(\ell)}$,  the four recursions  \eqref{eq:R-recursion}, \eqref{eq:S-recursion}, \eqref{eq:T-recursion}, and \eqref{eq:U-recursion} together reduce to a form that at least fits on a page:
\begin{align}\label{eq:R-recursions-single-input}\index{ddNTKs!statistics!R-recursion@$R$-recursion}
\ddNTKR{}{\ell+1}=&\le(\chi_{\perp}^{(\ell)}\ri)^2\ddNTKR{}{\ell}\, \\
&+\LW{\ell+1}C_W\bra\sigma^{\prime\prime}\sigma^{\prime}\sigma^{\prime}\sigma\ket_{\ker^{(\ell)}}\le(\Ti{\NTKI}{}{\ell}\ri)^2+C_W^2\bra\sigma^{\prime\prime\prime}\sigma^{\prime}\sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\le(\Ti{\NTKI}{}{\ell}\ri)^3\, \notag\\
&+\chi_{\perp}^{(\ell)}\le(\LW{\ell+1}\bra\sigma^{\prime\prime}\sigma\ket_{\ker^{(\ell)}}+C_W\Ti{\NTKI}{}{\ell}\bra\sigma^{\prime\prime\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\ri)\le(\NTHB{}{\ell}+\dNTKP{}{\ell}\ri)\, \notag\\
&+\chi_{\perp}^{(\ell)}\le(\LW{\ell+1}\bra\sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}+C_W\Ti{\NTKI}{}{\ell}\bra\sigma^{\prime\prime}\sigma^{\prime\prime}\ket_{\ker^{(\ell)}}\ri)\dNTKP{}{\ell}\, ,\notag\\
\label{eq:S-recursions-single-input}\index{ddNTKs!statistics!S-recursion@$S$-recursion}
\ddNTKS{}{\ell+1}=&\le(\chi_{\perp}^{(\ell)}\ri)^2\ddNTKS{}{\ell}\, \\
&+C_W\LW{\ell+1}\bra \sigma^{\prime} \sigma^{\prime} \sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\le(\Ti{\NTKI}{}{\ell}\ri)^2+C_W^2\bra \sigma^{\prime\prime}\sigma^{\prime\prime}\sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\le(\Ti{\NTKI}{}{\ell}\ri)^3\, \notag\\
&+\chi_{\perp}^{(\ell)}\le[\LW{\ell+1}\bra \sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}+C_W\Ti{\NTKI}{}{\ell}\bra \sigma^{\prime\prime}\sigma^{\prime\prime}\ket_{\ker^{(\ell)}}\ri]\NTHB{}{\ell}\, ,\notag\\
\label{eq:T-recursions-single-input}\index{ddNTKs!statistics!T-recursion@$T$-recursion}
\ddNTKT{}{\ell+1}=&\le(\chi_{\perp}^{(\ell)}\ri)^2\ddNTKT{}{\ell}\, \\
&+2C_W\LW{\ell+1}\bra \sigma^{\prime\prime}\sigma^{\prime}\sigma^{\prime}\sigma\ket_{\ker^{(\ell)}}\le(\Ti{\NTKI}{}{\ell}\ri)^2+C_W^2\bra \sigma^{\prime\prime} \sigma^{\prime\prime} \sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\le(\Ti{\NTKI}{}{\ell}\ri)^3\, \notag\\
&+\le(\LW{\ell+1}\ri)^2\bra \sigma^{\prime}\sigma^{\prime} \sigma\sigma\ket_{\ker^{(\ell)}}\Ti{\NTKI}{}{\ell}\, \notag\\
&+\le(\LW{\ell+1}\bra z \sigma^{\prime}\sigma\ket_{\ker^{(\ell)}}+C_W\Ti{\NTKI}{}{\ell}\bra z \sigma^{\prime\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\ri)^2\frac{\NTHF{}{\ell}}{\le(\ker^{(\ell)}\ri)^2}\, \notag\\
&+2\chi_{\perp}^{(\ell)}\le[\LW{\ell+1}\le(\bra  \sigma^{\prime\prime}\sigma\ket_{\ker^{(\ell)}}+\bra  \sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\ri)+C_W\Ti{\NTKI}{}{\ell}\le(\bra  \sigma^{\prime\prime\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}+\bra  \sigma^{\prime\prime}\sigma^{\prime\prime}\ket_{\ker^{(\ell)}}\ri)\ri]\dNTKQ{}{\ell}\, ,\notag\\
\label{eq:U-recursions-single-input}\index{ddNTKs!statistics!U-recursion@$U$-recursion}
\ddNTKU{}{\ell+1}=&\le(\chi_{\perp}^{(\ell)}\ri)^2\ddNTKU{}{\ell}+C_W^2\bra \sigma^{\prime\prime} \sigma^{\prime\prime}\sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\le(\Ti{\NTKI}{}{\ell}\ri)^3\, .
\end{align}
Here, we also simplified these expressions by recalling the two susceptibilities, \eqref{eq:chi-parallel} and~\eqref{eq:chi-perp}: 
\begin{align}
\Ti{\chi}{\parallel}{\ell}  \equiv \frac{C_W}{\ker^{(\ell)}} \bra \sigma^\prime \sigma  z \ket_{\ker^{(\ell)}} \, , \qquad \Ti{\chi}{\perp}{\ell} 
\equiv  C_W\bra\sigma^{\prime}\sigma^{\prime}\ket_{\ker^{(\ell)}}\, .
\end{align}


\subsubsection{ddNTK Scalings}\index{ddNTKs!scaling laws}



Now, let's tune to criticality, and use our \terminate{scaling ansatz}  \eqref{eq:master-scaling-ansatz} to find the critical exponents $p_\ddNTKRS$, $p_\ddNTKSS$, $p_\ddNTKTS$, and $p_\ddNTKUS$ that describe the asymptotic depth scaling of $\ddNTKR{}{\ell}$, $\ddNTKS{}{\ell}$, $\ddNTKT{}{\ell}$, and $\ddNTKU{}{\ell}$, respectively. 

To understand the relative size of these tensors controlling the means of the ddNTKs, we will again need to identify  appropriate dimensionless ratios. Following the \terminate{dimensional analysis} logic from our dNTK discussion, cf.~\eqref{eq:dNTK-dimensional-analysis}, let's look at our third-order update for preactivations \eqref{eq:preactivation-updated-finite-width-refined}. Remembering again that we can only add terms that have the same dimensions, we see that
\be\label{eq:ddNTK-dimensional-analysis}
[z]=[\eta]\, [\epsilon] \,[\NTK]=[\eta]^2\,[\epsilon]^2\,[\dNTK] =[\eta]^3\,[\epsilon]^3\,[\ddNTK] =[\eta]^3\,[\epsilon]^3\,[\ddNTKII] \,.
\ee 
From the first equality, we see as before that $[\eta]\,[\epsilon]=[z]\,[\NTK]^{-1}$, and so $\ddNTKRS$, $\ddNTKSS$, $\ddNTKTS$, and $\ddNTKUS$ each have dimensions of NTK cubed:
\be\label{eq:dimensions-of-ddNTK-stuff}
[\ddNTKRS]\equiv [\ddNTK]=[\NTK]^{3} \, [z]^{-2} \, , \qquad [\ddNTKSS] = [\ddNTKTS]=[\ddNTKUS] \equiv [\ddNTKII]= [\NTK]^{3} \, [z]^{-2} \, .
\ee
This means that the proper dimensionless combinations for the ddNTKs are
\begin{align}\label{eq:scaling-relations-ddNTKs}
\frac{\ddNTKR{}{\ell} \Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3 } \sim \frac{1}{n}\le( \frac{1}{\ell} \ri)^{p_\ddNTKRS +p_0 - 3p_\Theta} \,, \qquad \frac{\ddNTKS{}{\ell}\Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3} \sim \frac{1}{n}\le( \frac{1}{\ell} \ri)^{p_\ddNTKSS+p_0  - 3p_\Theta}\, , \\
\frac{\ddNTKT{}{\ell} \Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3 } \sim \frac{1}{n}\le( \frac{1}{\ell} \ri)^{p_\ddNTKTS +p_0- 3p_\Theta} \,, \qquad \frac{\ddNTKU{}{\ell}\Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3} \sim \frac{1}{n}\le( \frac{1}{\ell} \ri)^{p_\ddNTKUS +p_0 - 3p_\Theta}\, ,\label{eq:scaling-relations-ddNTKs-2}
\end{align}
where to get a normalized ratio, we multiplied by the kernel $\Ti{\ker}{}{\ell}$ to account for the $[z]^{-2}$ and then divided by the three factors of the frozen NTK $\Ti{\NTKI}{}{\ell}$. Here, $p_0$ is the \terminate{critical exponent} for the kernel, and $p_\Theta$ is the \terminate{critical exponent} for the NTK.

Finally, as a brief aside, let us comment on one aspect of dimensionality that we've been ignoring but will soon become important. In particular, since the network output $z$ is set to the true output $y$, they should really have the same dimensions:
\be\label{eq:dimension-of-labels}
[z] = [y] \, .
\ee
However, while the leading depth scaling of the preactivations is given by the kernel,
\be
\E{z^{(\ell)} z^{(\ell)}} = \ker^{(\ell)} \sim \le( \frac{1}{\ell}\ri)^{p_0} \, ,
\ee
the true output $y$ is fixed and doesn't scale with depth. When comparing the performance of networks of different depths, this suggests that it might be helpful to rescale the final network outputs as
\be
z_{i;\delta} \to z_{i;\delta} \le(\frac{1}{L}\ri)^{-p_0/2} \, ,
\ee
effectively fixing the scaling of the overall network output as $p_0=0$,
or equivalently rescale the training outputs as
\be\label{eq:rescale-true-outputs}
y_{i;\tra} \to y_{i;\tra} \le(\frac{1}{L}\ri)^{p_0/2} \,.
\ee
We will further see how this can affect the predictions of a fully-trained network on its test set in \S\ref{subsec:prediction-at-finite-width}.\footnote{
    For \terminate{regression} tasks, where we want to learn a vector of real numbers, this rescaling is appropriate. For \terminate{classification} tasks, where we want to learn a discrete probability distribution, we should rescale either the network outputs (before any softmax layer) or the raw output targets $y_{i;\delta}$ (again before any softmax layer) as in \eqref{eq:softmax-target}.
} 





\subsubsection{$K^\star=0$ Universality Class}


For the $K^\star=0$ universality class\index{universality class!K@$K^\star=0$}, remember that we Taylor expanded the activation function as %
\be\label{eq:taylor-expansion-k-star-reprint-last-actually-through}
\sigma(z)=\sum_{p=0}^{\infty}\frac{\sigma_{p}}{p!}z^p\,  ,
\ee
and defined the following Taylor coefficient for convenience
\begin{align}
a_1&\equiv \le(\frac{\sigma_3}{\sigma_1}\ri)+\frac{3}{4}\le(\frac{\sigma_2}{\sigma_1}\ri)^2\ ,\label{eq:a1-recall-last-actually}
\end{align}
and required that all activation functions in this class satisfy $\sigma_0 = 0$ and $\sigma_1 \neq 0$.

To solve our single input ddNTK recursions, \eqref{eq:R-recursions-single-input}--\eqref{eq:U-recursions-single-input}, you'll have to evaluate a few new Gaussian expectations, taking particular note of that some of them now depend on the third derivative of the activation function. Finally, to tune to $K^\star=0$ \terminate{criticality}  \eqref{eq:k-star-equals-zero-critical-initialization}, we need to set the initialization hyperparameters as $C_b=0$ and $C_W =1/\sigma_1^2$; to implement the learning rate \terminate{equivalence principle}, we need to set our training hyperparameters as
\eqref{eq:super-tanh-general},
\be
\Lb{\ell}=\widetilde{\lambda}_b\le(\frac{1}{\ell}\ri)^{p_{\perp}}L^{p_{\perp}-1}\, , \qquad \lamW{\ell}=\widetilde{\lambda}_W\le(\frac{L}{\ell}\ri)^{p_{\perp}-1} \, .
\ee
For simplicity, let us also focus on odd activation functions, such as $\tanhA$, for which importantly $\sigma_2=0$ and $p_\perp = 1$.

Inspecting the single-input ddNTK recursions \eqref{eq:R-recursions-single-input}--\eqref{eq:U-recursions-single-input}, we see that they depend on Gaussian expectations of preactivations as well as our previous solutions for all the other objects that we've considered: the NTK variance, the NTK-preactivation cross correlation, and the dNTK-preactivation cross correlation. 
Substituting in the solutions for all these quantities as needed -- you'll have to flip around to find them, though most were reprinted in \S\ref{subsec:dntk_criticality_tanh_univ}\index{universality class!K@$K^\star=0$} for the dNTK analysis -- we can find solutions to all the single-input ddNTK recursions:
\begin{align}
\ddNTKR{}{\ell}=&-\frac{\ell^2}{48}\le[3\widetilde{\lambda}_b+4\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]^2 (-a_1)+\ldots\, ,\\
\ddNTKS{}{\ell}=&\frac{\ell^2}{12} \le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]^2\widetilde{\lambda}_W\sigma_1^2+\ldots\, ,\\
\ddNTKT{}{\ell}=&\frac{\ell^2}{32}\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri](-a_1) \widetilde{\lambda}_b^2+\ldots\, ,\\
\ddNTKU{}{\ell}=&\frac{1}{2}\le[\widetilde{\lambda}_b+\frac{\widetilde{\lambda}_W\sigma_1^2}{(-a_1)}\ri]^3(-a_1)+\ldots\, .
\end{align}
From these, we can read off the critical exponents as $p_\ddNTKRS = p_\ddNTKSS = p_\ddNTKTS = -2$, and $p_\ddNTKUS=0$, and we see that the dimensionless ratios are given by 
\begin{align}\label{eq:scaling-relations-ddNTKs-K-star}
\frac{\ddNTKR{}{\ell} \Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3 } &=-\frac{1}{48}\le[\frac{3\widetilde{\lambda}_b +4 \widetilde{\lambda}_W\sigma_1^2/(-a_1) }{\widetilde{\lambda}_b +\widetilde{\lambda}_W\sigma_1^2/(-a_1) }\ri] \frac{\ell}{n}+ \ldots\, , \\
\frac{\ddNTKS{}{\ell}\Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3} &=\frac{1}{12}\le[\frac{ \widetilde{\lambda}_W\sigma_1^2/(-a_1) }{\widetilde{\lambda}_b +\widetilde{\lambda}_W\sigma_1^2/(-a_1) }\ri] \frac{\ell}{n}+ \ldots\, , \\
\frac{\ddNTKT{}{\ell} \Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3 } &=\frac{1}{32}\le[\frac{ \widetilde{\lambda}_b  }{\widetilde{\lambda}_b +\widetilde{\lambda}_W\sigma_1^2/(-a_1) }\ri]^2 \frac{\ell}{n}+ \ldots \, , \\
\frac{\ddNTKU{}{\ell}\Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3} &=\frac{1}{2} \frac{1}{\ell n } + \ldots \, .
\end{align}
This means that $\ddNTKR{}{\ell}$, $\ddNTKS{}{\ell}$, and $\ddNTKT{}{\ell}$ all scale according to our leading effective theory cutoff as
\be
p_\ddNTKRS + p_0 - 3p_\Theta = -1 \, \qquad p_\ddNTKSS + p_0 - 3p_\Theta = -1 \, , \qquad p_\ddNTKTS + p_0 - 3p_\Theta = -1 \, .
\ee
Thus, we see that the leading-order finite-width dynamics of $\ker^\star=0$ activation functions have contributions from the first ddNTK, $\ddNTK$, via $\ddNTKR{}{\ell}$, and from the second ddNTK, $\ddNTKII$,  via $\ddNTKS{}{\ell}$ and $\ddNTKT{}{\ell}$.\footnote{If we relaxed the restriction for the activation function to be odd, we'd find the same scalings for  $\ddNTKR{}{\ell}$, $\ddNTKS{}{\ell}$, and $\ddNTKT{}{\ell}$ -- though with different coefficients -- and we'd find that $\ddNTKU{}{\ell}$ was up by a factor of $\ell$, but still subleading overall.}





\subsubsection{Scale-Invariant Universality Class}\index{universality class!scale-invariant}
Perhaps the most important fact to remember about nonlinear scale-invariant activation functions \eqref{eq:scale-invariant-def-first},
\be\label{eq:scale-invariant-re-reprinted}
\sigma(z) = 
    \begin{cases}
    a_+ z \, , & z \ge 0\, , \\
     a_- z \, , & z < 0  \, ,
    \end{cases}
\ee 
with $a_+ \neq -a_-$, 
is that they are not smooth: their first derivative is a step function centered at the origin, and their second derivative is a \terminate{Dirac delta function}, \eqref{eq:integral-form-delta-function},
\be
\sigma'(z) =
    \begin{cases}
    a_+  \, , & z \ge 0\, , \\
     a_-  \, , & z < 0  \, ,
    \end{cases}
\, , \qquad  
\sigma''(z) = (a_+ - a_-)\delta(z) \, ,
\ee
and higher derivatives will involve  derivatives of the \terminate{Dirac delta function}.
Inspecting again the single-input ddNTK recursions \eqref{eq:R-recursions-single-input}--\eqref{eq:U-recursions-single-input}, the kink in these activation functions
 and the presence of  Gaussian expectations with up to three derivatives of $\sigma(z)$ should scare you, especially if you heeded our warning at the end of \S\ref{sec:finite_angle}. In fact, if you formally try to evaluate some of these expectations, particularly $\bra \sigma'' \sigma'' \ket_{\ker}$ and others related to it via integration by parts, you'll find that they want to blow up, even if you use all the \emph{magic tricks}\index{magic trick} from \terminate{physics} that you might have at your disposal for trying to make sense of divergent integrals.\footnote{
    We were somewhat lucky in \S\ref{subsec:dntk_criticality_scale_invariant} when analyzing the dNTK: all the higher-derivative Gaussian integrals that we needed simplified via integration by parts and gave finite -- in fact, vanishing -- answers, cf.~\eqref{eq:scale-invariant-higher-derivative-first} and \eqref{eq:scale-invariant-higher-derivative-second}.
} 

The divergence of these Gaussian correlators is actually telling us that there is something very wrong with our expansions for the updates to the preactivations \eqref{eq:preactivation-updated-finite-width-refined}, the NTK \eqref{eq:NTK-updated-finite-width-refined}, and the dNTK \eqref{eq:dNTK-updated-finite-width}. In particular, the Taylor expansion in the global learning rate $\eta$ breaks down for these non-smooth activation functions and doesn't accurately describe how a network is updated. As a result, our approach for solving the finite-width dynamics will not work for nonlinear scale-invariant activation functions.

To understand why, let's consider an extremely simple model of a network, a single neuron with a bias:
\be
z(x) = \sigma(x+b) \, .
\ee
Here, the input $x$ and the output $z$ are both scalars, and for the activation function $\sigma(z)$ we'll pick the $\relu$, with $a_+=1$ and $a_-=0$. 
Accordingly, for a particular input $x$ such that $x+b > 0$, the activation fires, and the output is $z(x)=x+b$; for a particular input $x'$ such that $x' + b < 0$, the activation doesn't fire, and the output is $z(x')=0$. 

Now, let's consider a gradient-descent update to the parameters with a training example $x>-b$ such that the activation fires. Then, the bias updates as
\be
\dbar b = - \eta \frac{dz}{db} \epsilon = -\eta \epsilon = \o{\eta}\, ,
\ee
where $\epsilon$ is the error factor of the loss, depending on the true output $y$. Now, if $x + b + \dbar b > 0$, then the change in the output is
\be
\dbar z = -\eta \epsilon = \o{\eta}\, ,
\ee
and would be perfectly described by our Taylor expansion \eqref{eq:preactivation-updated-finite-width-refined}. However, if the training error is large enough
such that $x + b + \dbar b=x + b -\eta \epsilon < 0$, then the activation turns off and
\be
\dbar z = -x -b = \o{1}\, .
\ee
Importantly, this update is independent of our expansion parameter $\eta$, and a Taylor expansion in $\eta$ cannot detect this discontinuity at $\eta=(x+b)/\epsilon$.

Thus, for the $\relu$, any time an activation crosses its firing threshold it can contribute  to the gradient-descent update in an $\eta$-independent way. 
Empirically, if you try to measure the NTK update for a deep MLP consisting of $\relu$ activation functions, you don't find anything consistent with the expansion \eqref{eq:NTK-updated-finite-width-refined}. Instead, for the appropriately normalized quantity, you'll find an $\sim 1/\sqrt{n}$ scaling with width and a linear scaling with depth $\ell$, in contrast to the $\ell/n$ scaling expected from our perturbative formalism.\footnote{
    It's temping to think that this $\sim 1/\sqrt{n}$ scaling arises from
    accumulating the probabilities that one of the $L n$ total hidden-layer $n \gg 1$ activations
 experiences an $\eta$-independent $\o{1}$ change after the gradient-descent step.
} From this we reach the unfortunate conclusion that we'll have to give up on describing the finite-width dynamics of nonlinear scale-invariant activation functions using these methods.

Note that everything we have discussed for nonlinear scale-invariant activation functions with respect to criticality and the infinite-width training dynamics is perfectly fine and experimentally validated, and the presence of a nonzero dNTK at finite width is still indicative of a dynamical NTK\index{dynamical NTK} and representation learning at finite width. 
The problem we just described only affects the analysis we're going to perform in the following section for the  training dynamics of finite-width networks, 
and the breakdown of the Taylor expansion just means that we will be unable to give a quantitative picture of representation learning for these non-smooth activation functions.\footnote{
    However, our analysis applies to any of the smoothed versions of the $\relu$ that permit a type of criticality, cf.~our criticality discussion of the $\swish$ and $\gelu$ in \S\ref{subsec:half_stability}. In particular, the training dynamics we'll work out in the next section describe these networks.
    These dynamical solutions, in conjunction with the output-layer solutions to all their associated recursions, will accurately characterize such fully-trained $\relu$-like networks in practice.
}
So, if you do want to understand this, you'll probably need an entirely new approach.

This leaves us one activation function in the entire scale-invariant universality class\index{universality class!scale-invariant}: the $\linear$ activation function used for deep linear networks. Tuning to criticality, $C_b =0$ and $C_W = 1$, which fixes $\chi=1$, and choosing layer-independent learning rates \eqref{eq:super-scale-invariant} as 
\be\label{eq:learning-rate-EP-scale-invariant-reprint-for-ddNTKs}
\Lb{\ell} = \frac{\widetilde{\lambda}_b}{L} \, , \qquad \LW{\ell} = \frac{\widetilde{\lambda}_W }{L}  \,,
\ee
we can solve the single-input ddNTK recursions \eqref{eq:R-recursions-single-input}--\eqref{eq:U-recursions-single-input}. Note that  for every term in the $\ddNTKUS$-recursion, \eqref{eq:U-recursions-single-input}, and for every term but one in the $\ddNTKRS$-recursion, \eqref{eq:R-recursions-single-input}, the Gaussian expectations of activations involve second derivatives or third derivatives, which vanish for a $\linear$ function. For the one term in the $\ddNTKRS$-recursion that does not vanish, it is also multiplied by $\dNTKP{}{\ell}$, which vanishes for all scale-invariant activations, cf.~\eqref{eq:P-vanish-scale-invariant}. This means that 
\begin{align}
\ddNTKR{}{\ell}=0\, ,\qquad \ddNTKU{}{\ell}=0\, ,
\end{align}
and in particular that the first ddNTK, $\ddNTK$, does not contribute to the deep linear network's dynamics at $\o{1/n}$ since it's entirely determined by $\ddNTKR{}{\ell}$.
However, for the $\ddNTKSS$-recursion, \eqref{eq:S-recursions-single-input}, and the $\ddNTKTS$-recursion, \eqref{eq:T-recursions-single-input}, we find something nontrivial: these recursions can be exactly solved as
\begin{align}
\ddNTKS{}{\ell}=&\frac{\ell^2(\ell^2-1)}{12L^3} (\widetilde{\lambda}_b+\widetilde{\lambda}_W \ker^{\star})^2\widetilde{\lambda}_W \, ,\\
\ddNTKT{}{\ell}=&\frac{\ell^2(\ell^2-1)}{12L^3} (\widetilde{\lambda}_b+\widetilde{\lambda}_W \ker^{\star}) \widetilde{\lambda}_W^2 \ker^{\star}\, ,
\end{align}
from which we see that the critical exponents are $p_\ddNTKSS = p_\ddNTKTS = -4$. 
Finally, the dimensionless ratios are 
\begin{align}\label{eq:scaling-relations-ddNTK-DLN}
\frac{\ddNTKS{}{\ell} \Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3 } &= \frac{1}{12}\le[\frac{\widetilde{\lambda}_W K^\star}{\widetilde{\lambda}_b + \widetilde{\lambda}_W K^\star } \ri] \frac{\ell}{n} + \dots\,, \\
\frac{\ddNTKT{}{\ell}\Ti{\ker}{}{\ell}}{n \le( \Ti{\NTKI}{}{\ell} \ri)^3} &= \frac{1}{12}\le[ \frac{\widetilde{\lambda}_W K^\star }{ \widetilde{\lambda}_b + \widetilde{\lambda}_W K^\star } \ri]^2 \frac{\ell}{n} + \dots \, ,
\end{align}
and we see that these scale according to our leading effective theory cutoff as
\be
p_\ddNTKSS + p_0 - 3p_\Theta = -1 \, , \qquad p_\ddNTKTS + p_0 - 3p_\Theta = -1 \, .
\ee
In conclusion, we see that the second ddNTK, $\ddNTKII$, contributes via $\ddNTKS{}{\ell}$ and $\ddNTKT{}{\ell}$ to the dynamics of deep linear networks at leading order.
















\section{Training at Finite Width}\label{sec:another-leap}\index{training dynamics!finite width}


Now that we understand the joint statistics of the preactivations, the NTK, the dNTK, and the ddNTKs, we have nearly all the tools we need in order to evaluate the distribution of \emph{fully-trained} networks at finite width and nonzero depth. To see why, recall our finite-width expansion of the network output evolution \eqref{eq:preactivation-updated-finite-width-refined}
\begin{align}\label{eq:preactivation-updated-finite-width-decomposition}
\z{i}{\delta}{L}(t=1)=\z{i}{\delta}{L}&-\eta\sum_{j,\tra}\Tia{\NTK}{ij}{\delta\tra}{L}\Tia{\epsilon}{j}{\tra}{L}+\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2}\Tia{\dNTK}{i j_1j_2}{\delta\tra_1\tra_2}{L}  \Tia{\epsilon}{j_1}{\tra_1}{L}\Tia{\epsilon}{j_2}{\tra_2}{L}\\
&-\frac{\eta^3}{6}\sum_{j_1,j_2,j_3,\tra_1,\tra_2,\tra_3}\Tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}{L}  \Tia{\epsilon}{j_1}{\tra_1}{\ell}\Tia{\epsilon}{j_2}{\tra_2}{\ell}\Tia{\epsilon}{j_3}{\tra_3}{\ell} +\o{\frac{1}{n^2}}\, . \notag
\end{align}
Importantly, all the quantities on the right-hand side of the network update \eqref{eq:preactivation-updated-finite-width-decomposition} -- $\z{i}{\delta}{L}$, $\Tia{\NTK}{ij}{\delta\tra}{L}$, $\Tia{\epsilon}{j}{\tra}{L}$, $\Tia{\dNTK}{i j_1j_2}{\delta\tra_1\tra_2}{L}$, and $\Tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}{\ell}$
-- are evaluated at initialization and thus are determined completely by the statistics of the joint preactivation-NTK-dNTK-ddNTKs distribution,
\be\label{eq:finite-width-joint}
p\!\le(z^{(L)}, \, \NTK^{(L)},\, \dNTK^{(L)},\, \ddNTK^{(L)} , \,\ddNTKII^{(L)}   \Big\vert \D\ri) \, ,
\ee
that we spent the majority of this book evaluating in detail. 
Accordingly -- just as we did before at infinite width (\S\ref{ch:NTHb}) -- we could use the joint distribution \eqref{eq:finite-width-joint} to compute the statistics of the fully-trained network outputs after the update \eqref{eq:preactivation-updated-finite-width-decomposition}, \emph{if} we tuned a single step of gradient descent for each realization of a network so that we landed on the minimum of the training loss $\z{i}{\tra}{L}(t=1)=\y{i}{\tra}$.

More generally, \emph{if} we trained the network with $T$ steps of gradient descent such that
\be\label{eq:multi-step-satisfaction}
\z{i}{\tra}{L}(t=T)=\y{i}{\tra}\, ,\qquad \text{for all}\ \tra\in\A\, ,
\ee
\emph{and if} we determined how to express the output of such a fully-trained network for general inputs $\delta\in\D$ as a functional of our statistical variables at initialization,
\be\label{eq:multi-step-satisfied-outputs}
\z{i}{\delta}{L}(T)\equiv \le[\z{i}{\delta}{L}(t=T)\ri]\!\le(z^{(L)}, \, \NTK^{(L)},\, \dNTK^{(L)},\, \ddNTK^{(L)} , \,\ddNTKII^{(L)}  \ri)\, ,
\ee
then we could give an analytical expression for the distribution of \emph{fully-trained network outputs}:
\be\label{eq:finite-width-trained}
p\!\le(z^{(L)}(T) \ri) \, .
\ee
The equation \eqref{eq:multi-step-satisfaction} is our \emph{fully-trained condition}\index{fully-trained condition!finite width}, and the distribution \eqref{eq:finite-width-trained} completely describes the ensemble of finite-width networks at the end of training. The theoretical understanding of this distribution is exactly the goal we set for ourselves at the beginning of this book in \S\ref{ch:introduction}.
The only thing left for us to work out is the functional \eqref{eq:multi-step-satisfied-outputs}; to do so, we first need to figure out what kind of steps to take in order to fully train these finite-width networks.\index{training dynamics!finite width}

Now, recall from \S\ref{subsec:algorithmic-independence-at-infinity} that in the infinite-width limit the fully-trained network solution \eqref{eq:multi-step-satisfied-outputs} had an \neo{algorithm independence}: the distribution at the end of training didn't depend on the details of the optimization algorithm, and thus we could perform our theoretical analysis with any algorithm we wanted. In contrast, at finite width the fully-trained solution~\eqref{eq:multi-step-satisfied-outputs} will have an \term{algorithm dependence}: different fully-trained solutions will make different test-set predictions depending on the details of the particular optimization algorithm used to train the network, even when holding fixed the statistical variables at initialization and their associated initialization and training hyperparameters. Encouragingly, the form of the solutions will nonetheless take a universal form, with the non-universal details of the particular training algorithm captured by six projective tensors: cf.~\eqref{eq:very-general-finite-width-solution-DONT-CHANGE}. Thus, we will be able to very generally study the distribution of fully-trained networks at finite width by working out such solutions.\index{universality!of the fully-trained network solution}


With that in mind, in this section we'll present fully-trained solutions for two different optimization algorithms. First, in \S\ref{subsec:giant-plus-small}
we'll take \emph{two} Newton-like steps in order to satisfy our fully-trained condition \eqref{eq:multi-step-satisfaction}. While practically infeasible for large training sets, this training algorithm is rich in pedagogical value, emphasizing the way in which a finite-width network needs to adapt its representation to the training data in order to minimize its training error. Then, in \S\ref{subsec:real-GD-at-finite-width} we'll analytically solve the dynamics of the vanilla gradient descent at order $1/n$ and obtain a slightly different ensemble of fully-trained finite-width networks. This algorithm is not only practically implementable, but also quite often used to optimize real neural networks, and our corresponding solution is an actual theoretical description of such fully-trained networks. Together, these solutions will help us understand the ways in which the details of the optimization algorithm can affect the corresponding fully-trained solution. Finally, in 
\S\ref{subsec:prediction-at-finite-width}
we'll be able to generally analyze the predictions of these different fully-trained networks on novel examples from the test set.\index{training dynamics!finite width}







Throughout this section, we will declutter the notation a bit by dropping the layer indices, since to understand training we only need to focus on the network output at layer $\ell=L$.







\subsubsection{An Infinite-Width Giant Leap at Finite Width}\index{training dynamics!finite width}
Before we begin, let's first review the \terminate{giant leap} that we took in \S\ref{sec:giant-leap} at infinite width. From the finer-grained perspective of finite width, we'll see that our leap actually missed the minimum, exhibiting training errors of order $1/n$. However, our new eyes on this leap will be instructive, as they will help us see how we can correct for these errors and reduce the finite-width training error even further. 


Recall from \S\ref{sec:giant-leap} that, in order to fully train an infinite-width network in a single step, we needed to make a \emph{second-order update} of the form
\begin{align}\label{eq:second-order-update-reprint}
\dtheta_\mu &=  - \sum_{\nu,\tra_1,\tra_2,i}\eta \lambda_{\mu\nu} \kappa^{\tra_1\tra_2}\frac{\td z_{i;\tra_1}}{\td \theta_\nu} (z_{i;\tra_2}-\y{i}{\tra_2})\, ,  %
\end{align}
which we interpreted either 
\emph{(i)} as a \emph{generalized training algorithm}
\eqref{eq:second-order-update} optimizing the standard MSE loss~\eqref{eq:MSE-loss-reprint}, or \emph{(ii)} as a standard (tensorial) gradient-descent step \eqref{eq:gd-update-lambda} optimizing a \emph{generalized MSE loss} \eqref{eq:loss-MSE-gen}. Here, $\kappa^{\tra_1\tra_2}$ was called the \neo{Newton tensor}, and -- in the first understanding of \eqref{eq:second-order-update-reprint} -- could be interpreted as allowing us to take anisotropic steps in training \terminate{sample space}. 


\index{training dynamics!finite width}
With this type of parameter update, a finite-width network output will evolves as
\begin{align}\label{eq:preactivation-updated-finite-width-decomposition-kappa}
&z_{i;\delta}(t=1)\, \\
=&z_{i;\delta}-\eta\sum_{\tra_1, \tra_2}\NTKM_{\delta\tra_1}\kappa^{\tra_1\tra_2}\le(z_{i;\tra_2}-\y{i}{\tra_2}\ri) -\eta\sum_{j,\tra_1, \tra_2}\DNTKS_{ij;\delta\tra_1}\kappa^{\tra_1\tra_2}\le(z_{j;\tra_2}-\y{j}{\tra_2}\ri) \, \notag\\
&+\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2,\tra_3,\tra_4}\!\!\!\!\!\!\dNTK_{i j_1j_2;\delta\tra_1\tra_2} \kappa^{\tra_1\tra_3}\kappa^{\tra_2\tra_4}\! \le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\!\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri) \, \notag \\
&-\frac{\eta^3}{6}\sum_{j_1,j_2,j_3,\tra_1,\ldots, \tra_6}\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}  
\kappa^{\tra_1\tra_4}\kappa^{\tra_2\tra_5}\kappa^{\tra_3\tra_6} \, \notag \\
&\qquad\qquad\qquad\qquad\quad\times\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)
+\o{\frac{1}{n^2}}\, ,\notag
\end{align}
where here we've made our usual decomposition of the NTK into a mean and fluctuation\index{tensor decomposition!NTK mean and fluctuation} %
\be
\NTK_{ij;\delta\tra}\equiv\delta_{ij}\NTKM_{\delta\tra}+\DNTKS_{ij;\delta\tra} \, .
\ee
In terms of such an update, our fully-trained condition \eqref{eq:multi-step-satisfaction} after a single step $T=1$,
\be
z_{i;\tra}(t=1)=\y{i}{\tra} \,,\qquad \text{for all}\ \tra\in\A\, ,
\ee
can be written as
\begin{align}\label{eq:what-we-want-rearranged-finite-width}
&z_{i;\tra}-\y{i}{\tra}\, \\
=&\eta\sum_{\tra_1, \tra_2}\NTKM_{\tra\tra_1}\kappa^{\tra_1\tra_2}\le(z_{i;\tra_2}-\y{i}{\tra_2}\ri) +\eta\sum_{j,\tra_1, \tra_2}\DNTKS_{ij;\tra\tra_1}\kappa^{\tra_1\tra_2}\le(z_{j;\tra_2}-\y{j}{\tra_2}\ri)  \, \notag\\
&-\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2,\tra_3,\tra_4}\dNTK_{i j_1j_2;\tra\tra_1\tra_2}\kappa^{\tra_1\tra_3}\kappa^{\tra_2\tra_4}\!\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri) \, \notag \\
&+\frac{\eta^3}{6}\sum_{j_1,j_2,j_3,\tra_1,\ldots, \tra_6}\tia{\ddNTK}{i j_1j_2j_3}{\tra\tra_1\tra_2\tra_3}  
\kappa^{\tra_1\tra_4}\kappa^{\tra_2\tra_5}\kappa^{\tra_3\tra_6}  \, \notag \\
&\qquad\qquad\qquad\qquad\quad\times \le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)
+\o{\frac{1}{n^2}}
\, . \notag
\end{align}
This new giant-leap condition \eqref{eq:what-we-want-rearranged-finite-width} perhaps seems a little daunting, and further it's not obvious that there's any particular choice of Newton tensor $\kappa^{\tra_1\tra_2}$ that can land us on the minimum.
Nonetheless, we do expect that our infinite-width solution should be near the true finite-width solution, up to errors of order $\o{1/n}$.\footnote{
    For deep networks, we know that technically the corrections will be of order $\o{L/n}$, corresponding to the cutoff scale of our effective theory description. 
} 

\index{training dynamics!finite width}
With that in mind, as a first step let's try our infinite-width giant leap \eqref{eq:parameters-update-reprint-generalized}
and see where we land. This infinite-width giant leap had an interpretation as \neo{Newton's method} and was given by the second-order update \eqref{eq:second-order-update-reprint}, with a particular choice of the product of the global learning rate and the Newton tensor,
\be\label{eq:newtons-method-update-reprint-finite}
\eta \kappa^{\tra_1\tra_2}=\NTKMsub^{\tra_1 \tra_2} \, ,
\ee
where the \emph{inverse} NTK mean submatrix
$\NTKMsub^{\tra_1 \tra_2}$
was defined implicitly via
\be\label{eq:training-set-full-mean-ntk-inverse}
\sum_{\tra_2\in\A} \NTKMsub^{\tra_1 \tra_2} \, \NTKMsub_{\tra_2 \tra_3} =\delta^{\tra_1}_{\ \tra_3}\, .
\ee
As always, the tilde on
$\NTKMsub_{\tra_1 \tra_2}$
emphasizes that it's an $\NR\times\NR$-dimensional submatrix of the NTK mean evaluated on pairs of training inputs \emph{only}.
By now this distinction should be familiar enough that we will stop belaboring it.


\index{training dynamics!finite width}
More importantly, here we've used the inverse of the full NTK mean
$\NTKMsub_{\tra_1\tra_2}$
rather than the infinite-width frozen NTK\index{frozen NTK}
$\NTKIsub_{\tra_1\tra_2}$.
To explain why, let us recall from \eqref{eq:NTK-mean-expansion} that the NTK mean receives a series of corrections at each order in the $1/n$ expansion\index{$1/n$ expansion}, of which the leading-order piece is the frozen NTK \eqref{eq:frozen-NTK}. Since we're now working at order $1/n$, we should in particular take into account the next-to-leading-order (NLO) $1/n$ correction to the NTK mean
$\NTKM_{\tra_1\tra_2}^{\le\{1\ri\}}$ by working with $\NTKMsub_{\tra_1\tra_2}$ instead of $\NTKIsub_{\tra_1\tra_2}$.\footnote{
    While we won't show it explicitly, we expect that this \emph{NLO NTK mean}\index{neural tangent kernel!mean!next-to-leading-order correction}, $\NTKM_{\tra_1\tra_2}^{\le\{1\ri\}(\ell)}$, will have a solution that scales like $\o{1/n}$ as compared to the frozen NTK; this would be analogous to what we found for the NLO metric\index{metric!next-to-leading-order correction} $\se{\tra_1\tra_2}{\ell}$, cf.~the discussion in \S\ref{sec:signal_prop_finite_width} after \eqref{eq:nlo-metric-subleading}. In particular, if we make a \terminate{$1/n$ expansion} for our training hyperparameters $\lambda_b^{(\ell)}$ and $\lambda_W^{(\ell)}$ as we did for our initialization hyperparameters in \eqref{eq:Cb-expansion} and \eqref{eq:CW-expansion}, then we will have extra freedom in the subleading hyperparameters $\lambda_b^{(\ell)\{1\}}$ and $\lambda_W^{(\ell)\{1\}}$ to eliminate the growing-in-$\ell$ contribution to $\NTKM_{\tra_1\tra_2}^{\le\{1\ri\}(\ell)}$. Overall, this will make the NLO correction to the NTK mean scale as $\o{1/n}$, subleading to the leading finite-width effects which scale as $\o{L/n}$: in the language of RG flow\index{renormalization group flow}\index{representation group flow}, the NLO NTK mean is \emph{marginal}\index{marginal (RG flow)}. In practice, such a contribution is negligible and can thus be neglected for networks of any real depth.

}





\index{training dynamics!finite width}
Substituting our
infinite-width giant-leap Newton tensor \eqref{eq:newtons-method-update-reprint-finite}
into our giant-leap condition \eqref{eq:what-we-want-rearranged-finite-width} at finite width and rearranging, we get
\begin{align}\label{eq:preactivation-updated-finite-width-decomposition-Newton-1-naive}
0=&\sum_{j,\tra_1,\tra_2}\DNTKS_{ij;\tra\tra_1} \NTKMsub^{\tra_1\tra_2}\!\le(z_{j;\tra_1}-\y{j}{\tra_1}\ri)\, \\
&-\frac{1}{2}\sum_{\substack{j_1,j_2,\\ \tra_1,\ldots, \tra_4}}\dNTK_{i j_1j_2;\tra\tra_1\tra_2} \NTKMsub^{\tra_1\tra_3}\NTKMsub^{\tra_2\tra_4}\!\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\!\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri) \, \notag \\
&+\frac{1}{6}\sum_{\substack{j_1,j_2,j_3, \\ \tra_1,\ldots, \tra_6} }\tia{\ddNTK}{i j_1j_2j_3}{\tra\tra_1\tra_2\tra_3}  
\NTKMsub^{\tra_1\tra_4}\NTKMsub^{\tra_2\tra_5}\NTKMsub^{\tra_3\tra_6}  \notag \\
&\qquad\qquad\qquad\times \le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)
+\o{\frac{1}{n^2}}\, . \notag
\end{align}
Thus, we actually missed the minimum: for the network to be fully trained, the right-hand side of \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1-naive} should have vanished, while here it is clearly nonzero in general.
Taking a step back, it's clear now that what we actually found at infinite width in \S\ref{sec:giant-leap} was
\be
z_{i;\tra}(t=1)-\y{i}{\tra} = \oninv \, .
\ee
In other words, our networks were fully trained only at leading order, and \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1-naive} gives an explicit expression for the $1/n$ correction.\index{training dynamics!finite width}

Disentangling a little further, there are two such corrections at order $1/n$: the first correction -- the first term on the right-hand side of \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1-naive} -- arises from the instantiation-to-instantiation fluctuations of the NTK across different realizations of the biases and weights at initialization; the second correction -- comprised of the second and third terms on the right-hand side of \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1-naive} -- arises from nonlinear changes in the output as we take our step. In particular, this second correction is a \neo{bona fide} manifestation of representation learning\index{representation learning!manifested at finite width} at finite width, accounting for the fact that the network's \emph{effective features}\index{feature function!effective} are evolving. 
If we properly account for these two types of the $1/n$ corrections, we should be able to attain a training error of order $1/n^2$:
\be\label{eq:fully-trained-condition-really}
z_{i;\tra}(T)-\y{i}{\tra} = \o{\frac{1}{n^2}} \, .
\ee
That is, we should be able to improve our effective theory\index{effective theory!representation learning} of fully-trained networks, quantitatively by another multiplicative factor of $1/n$, and qualitatively by properly including representation learning into such an effective description.

Our first approach (\S\ref{subsec:giant-plus-small}) to attain such effectively-zero training error, \eqref{eq:fully-trained-condition-really}, is to continue to engineer theoretical giant leaps so as to account for both the instantiation-to-instantiation fluctuations of the NTK and the effect of the dNTK and the first ddNTK.

Another approach (\S\ref{subsec:real-GD-at-finite-width}) is to simply use the vanilla tensorial gradient descent algorithm as we do in practice; in that case, we will have to not only account for the dynamics of the network output, but also account for the dynamics of the NTK and dNTK. After doing so, we
will see that we can iteratively decrease the training loss to zero after many many such steps.


\index{training dynamics!finite width}
\subsection{A Small Step Following a Giant Leap}\label{subsec:giant-plus-small}
Here we'll train our networks with \emph{two} second-order updates. For the first update, a giant leap, we'll need to further generalize our theoretical optimization algorithm in order to properly account for the instantiation-to-instantiation fluctuations of the NTK. In the second update, a small step, we'll be able to account for the $1/n$ change in representation due to the nonzero dNTK and ddNTK, ultimately landing on the minimum of the loss as \eqref{eq:fully-trained-condition-really}. In particular, 
we can think of these updates as loosely corresponding to distinct phases of training that arise
when implementing gradient-based training of neural networks in practice. 



\index{training dynamics!finite width}
\subsubsection{First Update: One Final Generalization of Gradient Decent}
Please flip back to take a closer look at our unsatisfied condition for fully training our networks \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1-naive}.
Right away, you should notice a serious problem in satisfying this constraint: the NTK 
fluctuation, $\DNTKS_{ij;\tra\tra_1}$, the dNTK, $\dNTK_{i j_1j_2;\tra\tra_1\tra_2}$, and the ddNTK, $\tia{\ddNTK}{i j_1j_2j_3}{\tra\tra_1\tra_2\tra_3}$, all
mix different output components together in an update; i.e.~the $j$-th component of the prediction error
$z_{j;\tra}-\y{j}{\tra}$
at initialization affects the $i$-th component of the output
$z_{i;\delta}$
after the update, even for $i\ne j$. This stands in contrast to what we found for an infinite-width update in \S\ref{subsec:GD_no_wiring_at_infinity}, where there was no such mixing or \emph{wiring}\index{gradient descent!wiring!finite width} of output components. Meanwhile, we did see a similar wiring effect for Bayesian inference at finite width as we discussed in depth in \S\ref{subsec:presence-FF-Bayes}.


While such wiring at finite width is fantastic from a practitioner's standpoint, it makes our theoretical work slightly more complicated. 
In particular, in order to satisfy the training constraint \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1-naive}, we will need to further generalize our \terminate{second-order update} \eqref{eq:second-order-update-reprint}. Following in the footsteps of our previous two generalizations,
\eqref{eq:gd-update-lambda} and \eqref{eq:generalization-of-generalization}, let's make one final generalization
\be
\eta\to\eta\lambda_{\mu\nu} \to \eta \lambda_{\mu\nu}\kappa^{\tra_1\tra_2} \to\eta \lambda_{\mu\nu}\kappa^{\tra_1\tra_2}_{ij} \,,
\ee
with the final form of our theoretical update given by\index{second-order update!generalized}
\begin{align}\label{eq:second-order-update-per-component}
\dtheta_\mu  &=  - \sum_{\nu,\tra_1,\tra_2,i,j}\eta \lambda_{\mu\nu} \kappa^{\tra_1\tra_2}_{ij}\frac{\td z_{i;\tra_1}}{\td \theta_\nu}\epsilon_{j;\tra_2} \, .
\end{align}
Here, we also introduced a further
\emph{generalized} Newton tensor\index{Newton tensor!generalized} $\kappa^{\tra_1\tra_2}_{ij}$ with output-component indices. This flexibility will allow us to resolve the mixing of the output components in the residual training error from our infinite-width leap \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1-naive}.\footnote{Similarly to the generalized MSE loss  \eqref{eq:loss-MSE-gen} discussed in \S\ref{subsec:memorization-at-infinity}, we can alternatively think of this further-generalized second-order update \eqref{eq:second-order-update-per-component} optimizing the standard MSE loss as instead arising from a standard first-order (tensorial) gradient descent update \eqref{eq:gd-update-lambda} on a further-generalized MSE loss\index{loss!MSE!generalized},
\be\label{eq:loss-MSE-gen-further}
\Laux{\A}(\theta) \equiv\frac{1}{2}\sum_{i_1,i_2=1}^{n_L} \sum_{\tra_1,\tra_2 \in \A}\kappa^{\tra_1 \tra_2}_{i_1i_2}\le(z_{i_1;\tra_1}-\y{i_1}{\tra_1}\ri)\le(z_{i_2;\tra_2}-\y{i_2}{\tra_2}\ri) \, ,
\ee
as long as the Newton tensor $\kappa^{\tra_1 \tra_2}_{i_1i_2}$ is \emph{also} assumed to be symmetric under the exchange of paired indices $(i_1,\tra_1)\leftrightarrow(i_2,\tra_2)$. This symmetry will in fact be present for both our first update, the giant leap, and our second update, the small step.
With this interpretation \eqref{eq:loss-MSE-gen-further}, our generalized Newton tensor $\kappa^{\tra_1\tra_2}_{ij}$ acts as a metric\index{Newton tensor!as a metric on sample space} on \terminate{sample space}, through its sample indices $\tra_1,\tra_2$, and on output-component space, through its $L$-th layer neural indices $i,j$.}

\index{training dynamics!finite width}
Note importantly that the $\mu,\nu$ indices of $\lambda_{\mu\nu}$ are very different from the $i,j$ indices of $\kappa^{\tra_1\tra_2}_{ij}$: the former each runs over all $P$ parameters, while the latter each only runs over the $n_L$ components of the network output.
In particular, while learning-rate tensor\index{learning rate!learning-rate tensor} $\lambda_{\mu\nu}$ lets us control how the gradient of the $\nu$-th parameter affects the update to the $\mu$-th parameter, the $i,j$ indices of the generalized Newton tensor $\kappa^{\tra_1\tra_2}_{ij}$ instead control how the network's \emph{features}\index{feature} \eqref{eq:feature-function-stochastic}
$\td z_{i;\tra_1}/ \td \theta_\nu$ are combined with the error factor $\epsilon_{j;\tra_2}$
in order to make the update.\footnote{Note that when we developed our interpretation of an infinite-width network as a linear model in \S\ref{sec:lazy-kernel}, we  made a similar distinction between parameter indices $\mu$ and output-component indices $i$ in \eqref{eq:feature-function-stochastic} when defining the random feature functions $\widehat{\fea}_{i,\mu}(x)$.
We also discussed how \emph{wiring} can be incorporated into a non-minimal model of representation learning in \S\ref{subsec:nonlinear-at-finite}.
} Allowing for a $\kappa^{\tra_1\tra_2}_{ij}$ with nonzero off-diagonal components in the $i,j$ indices, we can precisely tune the \emph{wiring}\index{gradient descent!wiring!finite width}\index{wiring!in gradient-based learning|see{gradient descent}} or mixing of output components that occurs in a particular update.

Finally, plugging our final-form second-order update\index{second-order update!generalized}  \eqref{eq:second-order-update-per-component} back into the $1/n$ expansion of the network update 
\eqref{eq:preactivation-updated-finite-width-refined}
and using the NTK, dNTK, and first ddNTK definitions, we can see how the network output changes after making an update with this new optimization algorithm: 
\begin{align}\label{eq:preactivation-updated-finite-width-decomposition-kappa-genearlized}
&z_{i;\delta}(t=1)\, \\
=&z_{i;\delta}-\eta\sum_{j,\tra_1, \tra_2}\NTKM_{\delta\tra_1}\kappa^{\tra_1\tra_2}_{ij}\le(z_{j;\tra_2}-\y{j}{\tra_2}\ri) -\eta\sum_{j,k,\tra_1, \tra_2}\DNTKS_{ij;\delta\tra_1}\kappa^{\tra_1\tra_2}_{jk}\le(z_{k;\tra_2}-\y{k}{\tra_2}\ri) \, \notag\\
&+\frac{\eta^2}{2}\sum_{ \substack{j_1,j_2,k_1,k_2, \\ \tra_1,\tra_2,\tra_3,\tra_4} }\dNTK_{i j_1j_2;\delta\tra_1\tra_2} \kappa^{\tra_1\tra_3}_{j_1k_1}\kappa^{\tra_2\tra_4}_{j_2k_2} \le(z_{k_1;\tra_3}-\y{k_1}{\tra_3}\ri)\le(z_{k_2;\tra_4}-\y{k_2}{\tra_4}\ri) \, \notag \\
&-\frac{\eta^3}{6}\sum_{\substack{j_1,j_2,j_3, k_1, k_2, k_3 \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5, \tra_6} }\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}  
\kappa^{\tra_1\tra_4}_{j_1k_1}\kappa^{\tra_2\tra_5}_{j_2k_2}\kappa^{\tra_3\tra_6}_{j_3k_3}  \, \notag \\
&\qquad\qquad\qquad\qquad\qquad\times \le(z_{k_1;\tra_4}-\y{k_1}{\tra_4}\ri)\le(z_{k_2;\tra_5}-\y{k_2}{\tra_5}\ri)\le(z_{k_3;\tra_6}-\y{k_3}{\tra_6}\ri)
+\o{\frac{1}{n^2}}\, .\notag
\end{align}
Here, we've again used the standard MSE loss\index{loss!MSE}, for which the error factor is given by the residual training error $\epsilon_{j;\tra}=z_{j;\tra}-\y{j}{\tra}$. Now, we'll need to pick our generalized Newton tensor $\kappa^{\tra_1\tra_2}_{ij}$ judiciously in order to make a first update that fully accounts for instantiation-to-instantiation fluctuations of particular networks in our ensemble. %











\index{training dynamics!finite width}
Taking inspiration from our $1/n$-failed infinite-width Newton's step \eqref{eq:newtons-method-update-reprint-finite}, let's take a similar-looking first step according to
\begin{align}\label{eq:newton-step-generalized-first}
\eta\kappa^{\tra_1\tra_2}_{ij}=&\le(\NTK^{-1}\ri)^{\tra_1\tra_2}_{ij}\, \\
=& \delta_{ij} \NTKMsub^{\tra_1 \tra_2} -\sum_{\tra_3,\tra_4\in\A}\NTKMsub^{\tra_1\tra_3}\DNTKS_{ij;\tra_3\tra_4}\NTKMsub^{\tra_4\tra_2} \, \notag\\
&+\sum_{k=1}^{n_L}\sum_{\tra_3,\ldots,\tra_6\in\A}\NTKMsub^{\tra_1\tra_3}\DNTKS_{ik;\tra_3\tra_4}\NTKMsub^{\tra_4\tra_5} \DNTKS_{kj;\tra_5\tra_6}\NTKMsub^{\tra_6\tra_2} +\o{\Delta^3}\, .\notag \,
\end{align}
Here, we've introduced the complete inverse of the stochastic NTK sub-tensor evaluated on the training set, satisfying 
\be\label{eq:stochastic-ntk-mean}
\sum_{j, \tra_2}\le(\NTK^{-1}\ri)^{\tra_1\tra_2}_{ij}\NTK_{jk;\tra_2\tra_3}=\delta_{ik}\delta^{\tra_1}_{\ \tra_3}\, ,
\ee
and in the last equality of \eqref{eq:newton-step-generalized-first}
we've used the Schwinger-Dyson equations\index{Schwinger-Dyson equations} \eqref{eq:stochastic-metric-inversion}, which is a physicist's way of saying that we expanded the inverse of the stochastic NTK around the NTK mean.\footnote{Despite saying that we wouldn't belabor this any further, this is one of those unfortunate situations where we had to decide between decorating the inverse $\le(\NTK^{-1}\ri)^{\tra_1\tra_2}_{ij}$ with either a hat or a tilde, and we went with the hat. Hopefully the tildes on the sample indices, e.g.~$\tra_1,\tra_2$, will remind you that the inverse of this stochastic object is taken only with respect to the training set. Note that at no point will we ever need the inverse of the stochastic NTK evaluated on a general dataset\index{input data} $\D$.}
The main difference between this new giant leap \eqref{eq:newton-step-generalized-first} and our previous infinite-width giant leap \eqref{eq:newtons-method-update-reprint-finite} is that we're now taking into account the instantiation-to-instantiation fluctuations of the NTK across different realizations of the model parameters; in other words, we're implementing a \emph{different} Newton step for each \emph{particular}  network with its associated NTK $\NTK_{i_1i_2;\tra_1\tra_2}$. Accordingly, this step can be thought of as loosely corresponding to the first phase of training for such a particular network. 


\index{training dynamics!finite width}
Taking this step, i.e.~plugging this generalized Newton tensor \eqref{eq:newton-step-generalized-first} into our update to the network output \eqref{eq:preactivation-updated-finite-width-decomposition-kappa-genearlized}, we find the training error decreases to
\begin{align}\label{eq:preactivation-updated-finite-width-decomposition-Newton-1}
&z_{i;\tra}(t=1)-\y{i}{\tra}\, \\
=&\frac{1}{2}\!\!\sum_{\substack{j_1,j_2,\\ \tra_1,\ldots, \tra_4}}\!\!\dNTK_{i j_1j_2;\tra\tra_1\tra_2}\NTKMsub^{\tra_1\tra_3} \NTKMsub^{\tra_2\tra_4}\!\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\!\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri) \, \notag \\
&-\frac{1}{6}\sum_{\substack{j_1,j_2,j_3, \\ \tra_1,\ldots \tra_6} }\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}  
\NTKMsub^{\tra_1\tra_4}\NTKMsub^{\tra_2\tra_5}\NTKMsub^{\tra_3\tra_6} \, \notag \\
&\qquad\qquad\qquad\times \le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)
+\o{\frac{1}{n^2}}\, .\notag
\end{align}
Thus, we've correctly taken care of the first type of the $1/n$ corrections, and
with this step we've reduced the residual prediction error on the training set
from the order-one error of our initial prediction 
\be
z_{i;\tra}(t=0)-\y{i}{\tra} = \o{1} \, ,
\ee
to a much smaller error of
\be\label{eq:one-step-error-factor-suppression}
z_{i;\tra}(t=1)-\y{i}{\tra} = \oninv \, ,
\ee
loosely corresponding to the empirically-large initial decrease of error when training networks in practice.
Moreover, the rest of the error in \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1} is now entirely due to
the additional finite-width corrections
encoded by the dNTK and first ddNTK, a consequence of representation learning.\footnote{
In particular, this first update \eqref{eq:newton-step-generalized-first} would satisfy the training condition \eqref{eq:fully-trained-condition-really} only if the NTK were constant under gradient descent.
There's actually a name given to this type of phenomenon, \neo{lazy training}, referring to situations when the network function behaves as if it is equal to a linearzation around the initial value of its parameters \cite{chizat2018note}. As we know from our discussion in \S\ref{sec:lazy-kernel}, if the NTK is constant, then the network is a \terminate{linear model}.
}

\index{training dynamics!finite width}
\subsubsection{Second Update: Representation Learning Strikes Back}
To reduce the training error further, we'll need to update our network again to further account for the fact that its representation evolved with the first update.  
In other words, we'll need to make a second gradient-descent update. 
If you'd like, you can imagine that this update 
corresponds to a second phase of training -- following a first phase where the network significantly decreased its training error with the NTK evaluated at initialization --
and so now the model must refine its features\index{feature} in order to further improve its performance.

Accordingly, since our training error is already down to $\sim 1/n$, our second update is going to be a lot smaller than our first. In particular, the update itself will necessarily only be of order $1/n$ so as to precisely cancel the remaining $1/n$ training error; that is, it's actually more of a \emph{small step} than another \emph{giant leap}.\footnote{
As we will see, the overall learning rate is essentially the same for both updates; the second update is only smaller because the gradient of the loss after the first update is itself much smaller when near a minimum.
}


To determine which step we need to take, let's write the network output after a second update as
\begin{align}\label{eq:small-step-to-the-bottom}
&z_{i;\delta}(t=2) \\
=&z_{i;\delta}(t=1)-\sum_{j,k,\tra_1, \tra_2}\NTKM_{i j;\delta\tra_1}\!(t=1) \, \eta\kappa^{\tra_1\tra_2}_{jk}(t=1)   \le[z_{k;\tra_2}(t=1)  -\y{k}{\tra_2}\ri]  + \o{\frac{1}{n^2}}\, \notag \\
=&z_{i;\delta}(t=1)-\sum_{j,\tra_1, \tra_2}\NTKM_{\delta\tra_1} \, \eta\kappa^{\tra_1\tra_2}_{ij}(t=1)   \le[z_{j;\tra_2}(t=1)  -\y{j}{\tra_2}\ri]  + \o{\frac{1}{n^2}}\, , \notag
\end{align}
where we left the second update's product of global learning rate and generalized Newton tensor $\eta \kappa^{\tra_1\tra_2}_{ij}(t=1)$ unspecified for now. Note here that in the first equality we dropped the d(d)NTK terms from the update: since after our first update \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1} the training error has already decreased to $\o{1/n}$, the would-be dNTK term is very subleading $\sim \dNTK \times [\epsilon(t=1)]^2=\o{1/n^3}$, and
the would-be ddNTK term is ridiculously subleading $\sim \ddNTK \times [\epsilon(t=1)]^3=\o{1/n^4}$. Similarly, on the third line we replaced the NTK after the first step
$\NTKM_{ij;\delta\tra_1}\!(t=1)$ by the NTK mean 
at initialization $\delta_{ij}\NTKM_{\delta\tra_1}$. Given the $1/n$-suppressed training error after the first step, this substitution can be justified for these two reasons in conjunction: \emph{(i)} the update to the NTK  $\NTK_{ij;\delta\tra}(t=1)-\NTK_{ij;\delta\tra}(t=0)$ is itself suppressed by $1/n$, 
cf.~\eqref{eq:NTK-updated-finite-width-refined}, 
so we may use the version from before the update, and \emph{(ii)} the NTK fluctuation is also suppressed compared to its mean, so we may then swap the stochastic NTK at initialization for its mean.
This means that if we make the following choice for our \emph{small step} second update
\begin{align}\label{eq:newton-step-generalized-second}
\eta \kappa^{\tra_1\tra_2}_{ij}(t=1) =& \delta_{ij} \NTKMsub^{\tra_1 \tra_2} + \oninv \, ,
\end{align}
then it's easy to see from  \eqref{eq:small-step-to-the-bottom} that our network will now be fully trained  as \eqref{eq:fully-trained-condition-really}:
\be\label{eq:two-step-twice-reduced-error}
z_{i;\tra}(t=2) - \y{i}{\tra} = \o{\frac{1}{n^2}}\, .
\ee
Thus, with our second update we were able to reduce the residual training error by an additional factor of $1/n$ as compared to the error after the first update \eqref{eq:preactivation-updated-finite-width-decomposition-Newton-1}.\footnote{
This suggests that additional updates
could continue to reduce the training error by additional factors of $1/n$. However, these further refinements -- determined in terms of the higher-order corrections to our effective theory description -- will be qualitatively the same as our leading finite-width description at $\o{L/n}$. In other words, improving an infinite-width description to finite-width description incorporates representation learning, while more precise finite-width descriptions just allow the model to make further refinements to its features. Practically speaking, we expect our leading finite-width description to be very accurate for networks with reasonable values of the aspect ratio $L/n$.
}









\index{training dynamics!finite width}
For general inputs $\delta\in\D$, plugging our choice of learning rate and Newton tensor \eqref{eq:newton-step-generalized-second} back into our second update \eqref{eq:small-step-to-the-bottom} and further re-expressing the network output after the first step $z_{i;\delta}(t=1)$ by \eqref{eq:preactivation-updated-finite-width-decomposition-kappa-genearlized} with \eqref{eq:newton-step-generalized-first}, we get
\begin{align}\label{eq:finite-width-network-output-general-data}
&z_{i;\delta}(t=2)\, \\
=&z_{i;\delta}-\sum_{\tra_1,\tra_2\in\A}\NTKM_{\delta\tra_1}\NTKMsub^{\tra_1\tra_2}\!\le(z_{i;\tra_2}-\y{i}{\tra_2}\ri)\, \notag \\
&+\sum_{j=1}^{n_{L}}\sum_{\tra_1,\tra_2\in\A}\le[\DNTKS_{ij;\delta\tra_1}-\sum_{\tra_3,\tra_4\in\A}\NTKM_{\delta\tra_3}\NTKMsub^{\tra_3\tra_4}\DNTKS_{ij;\tra_4\tra_1}\ri]\NTKMsub^{\tra_1\tra_2}\!\le(z_{j;\tra_2}-\y{j}{\tra_2}\ri)\, \notag\\
&-\sum_{j,k=1}^{n_{L}}\sum_{\tra_1,\ldots,\tra_4\in\A}\le[\DNTKS_{ij;\delta\tra_1}-\sum_{\tra_5,\tra_6\in\A}\NTKM_{\delta\tra_5}\NTKMsub^{\tra_5\tra_6}\DNTKS_{ij;\tra_6\tra_1}\ri]\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\times\NTKMsub^{\tra_1\tra_2}\DNTKS_{jk;\tra_2\tra_3}\NTKMsub^{\tra_3\tra_4}\!\le(z_{k;\tra_4}-\y{k}{\tra_4}\ri)\, \notag\\
&+\frac{1}{2}\sum_{j_1,j_2=1}^{n_{L}}\sum_{\tra_1,\ldots,\tra_4\in\A}\le[\dNTK_{i j_1j_2;\delta\tra_1\tra_2}-\sum_{\tra_5,\tra_6\in\A}\NTKM_{\delta\tra_5}\NTKMsub^{\tra_5\tra_6}\dNTK_{i j_1j_2;\tra_6\tra_1\tra_2}\ri]\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\times\NTKMsub^{\tra_1\tra_3}\NTKMsub^{\tra_2\tra_4} \!\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\!\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri) \, \notag \\ 
&-\frac{1}{6}\sum_{j_1,j_2,j_3 =1 }\sum_{\tra_1,\ldots,\tra_6\in\A}\le[\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}
- \sum_{\tra_7, \tra_8 \in \A}\NTKM_{\delta\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTK}{i j_1j_2j_3}{\tra_8\tra_1\tra_2\tra_3} \ri]  
 \, \notag \\
&\quad\quad\quad\quad\quad\quad\quad\quad\times \NTKMsub^{\tra_1\tra_4}\NTKMsub^{\tra_2\tra_5}\NTKMsub^{\tra_3\tra_6} \le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri) \, \notag \\
&+\o{\frac{1}{n^2}}\, .\notag
\end{align}
Here, we see that the expressions in the square brackets vanish identically for all the training inputs $\delta=\tra\in\A$: our network is thus fully trained.
In particular, since all of the variables on the right-hand side of \eqref{eq:finite-width-network-output-general-data} are at initialization, this solution realizes our goal \eqref{eq:multi-step-satisfied-outputs} of expressing the output of a fully-trained network as a functional of such variables at initialization. Accordingly, the statistics of the fully-trained distribution \eqref{eq:finite-width-trained} can now be worked out from the joint preactivation-NTK-dNTK-ddNTKs distribution \eqref{eq:finite-width-joint} at initialization.\footnote{Alternatively, we could have reached this same solution in a \emph{single update} if we had instead made a  \emph{finely tuned} giant leap, picking the generalized Newton tensor as
\begin{align}\label{eq:newton-step-generalized}
\eta \kappa^{\tra_1\tra_2}_{ij} =& \delta_{ij} \NTKMsub^{\tra_1 \tra_2} -\sum_{\tra_3,\tra_4\in\A}\NTKMsub^{\tra_1\tra_3}\DNTKS_{ij;\tra_3\tra_4}\NTKMsub^{\tra_4\tra_2} \, \\
&+\sum_{k=1}^{n_L}\sum_{\tra_3,\ldots,\tra_6\in\A}\NTKMsub^{\tra_1\tra_3}\DNTKS_{ik;\tra_3\tra_4}\NTKMsub^{\tra_4\tra_5} \DNTKS_{kj;\tra_5\tra_6}\NTKMsub^{\tra_6\tra_2} \, \notag \\
&+\frac{1}{2}\sum_{k=1}^{n_L}\sum_{\tra_3,\ldots,\tra_6 \in \A} \NTKMsub^{\tra_1\tra_3}\NTKMsub^{\tra_2\tra_4}\NTKMsub^{\tra_5\tra_6}\dNTK_{i j k;\tra_3\tra_4\tra_5}\le(z_{k;\tra_6}-\y{k}{\tra_6} \ri)   \, \notag \\
&-\frac{1}{6}\sum_{k_1,k_2=1}^{n_L}\sum_{\tra_3,\ldots,\tra_8 \in \A} \NTKMsub^{\tra_1\tra_3}\NTKMsub^{\tra_2\tra_4}\NTKMsub^{\tra_5\tra_7}\NTKMsub^{\tra_6\tra_8}\tia{\ddNTK}{i j k_1k_2}{\tra_3\tra_4\tra_5\tra_6}  
  \, \notag \\
&\qquad\qquad\qquad\qquad\qquad\times\le(z_{k_1;\tra_7}-\y{k_1}{\tra_7}\ri)\le(z_{k_2;\tra_8}-\y{k_2}{\tra_8}\ri)
\notag \, .
\end{align}
In essence, the first three terms of \eqref{eq:newton-step-generalized}  come from inverting the stochastic NTK,
corresponding to our first-update giant leap \eqref{eq:newton-step-generalized-first} and 
accounting for the instantiation-to-instantiation fluctuations in the NTK. In contrast, the final two terms correspond to our second-update small step \eqref{eq:two-step-twice-reduced-error} and accounts for the dNTK-ddNTK-induced representation learning.
Note that this generalized Newton tensor \eqref{eq:newton-step-generalized} is \emph{asymmetric} under the exchange of paired indices $(i,\tra_1)\leftrightarrow(j,\tra_2)$ due to the last term, and thus this finely-tuned update doesn't admit an alternative interpretation of optimization with a further generalized loss \eqref{eq:loss-MSE-gen-further}.

This single update is analogous to the \neo{direct optimization} solution \eqref{eq:nearly-linear-regression-optimal-cleanest} for \terminate{quadratic regression} in \S\ref{sec:nonlinear-model}.
The very finely-tuned nature of this single-update algorithm \eqref{eq:newton-step-generalized} suggests that it's easier to make the fine adjustments required to reach a solution by taking many simpler steps rather than fewer complicated steps.
We will see the other side of this next in \S\ref{subsec:real-GD-at-finite-width} when we study training by many many steps of vanilla gradient descent.
}
\index{training dynamics!finite width}







\index{training dynamics!finite width}
While this is all very exciting, let us also caution you that these generalized second-order updates are probably best thought of as giving a simple theoretical model of a training algorithm -- designed to let us understand the training process analytically -- and by no means are we suggesting that they provide a good or useful option for practical optimization.
Instead, the most practical algorithm for optimization is vanilla first-order gradient descent. As we've already pointed out that the output of a fully-trained network  \emph{does} depend on the details of the algorithm used for optimization,
 now we really have no choice left other than to explicitly analyze many many steps of gradient descent.

 \subsection{Many Many Steps of Gradient Descent}\label{subsec:real-GD-at-finite-width}\index{training dynamics!finite width}
In this extended subsection, we're going to study tensorial gradient descent~\eqref{eq:gd-update-lambda} and optimize finite-width neural networks according to the MSE loss with a constant global learning rate $\eta$.\footnote{
    Don't let the word \emph{tensorial} scare you here; this just means that we will allow for our training hyperparameters $\lambda_b^{(\ell)}$ and $\lambda_W^{(\ell)}$ as part of the definition of the NTK. We hope it's already clear why including these hyperparameters is a good idea -- if not, please flip back to \S\ref{sec:EVGP-WEP} and reread the paragraphs on the learning rate \terminate{equivalence principle} -- and they are actually simple to include practically as part of any optimization algorithm. 

    Moreover, as they are just part of the definition of the NTK, they have absolutely no consequence on the dynamics presented here; i.e. our solution also covers \emph{non-tensorial} gradient descent, though in such a case you'd have different asymptotic solutions for the statistics of the NTK, dNTK, and ddNTKs. This is also why we think of the training hyperparameters $\lambda_b^{(\ell)}$ and $\lambda_W^{(\ell)}$ as being independent from the details of the optimization algorithm itself.\index{training hyperparameters!independent from the optimization algorithm}
}
With\index{training dynamics!finite width} this progenitor-of-all-other-gradient-based-learning-algorithm algorithm, the network output will evolve as
 \eqref{eq:preactivation-updated-finite-width-refined}
\begin{align}\label{eq:preactivation-update-redux}
z_{i;\delta}(t+1)=&z_{i;\delta}(t)-\eta\sum_{j,\tra}\NTKM_{ij;\delta\tra}(t)\le[z_{j;\tra}(t)-\y{j}{\tra}\ri]\, \\
&+\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2}\dNTKM_{i j_1j_2;\delta\tra_1\tra_2}(t)\le[z_{j_1;\tra_1}(t)-\y{j_1}{\tra_1}\ri]\le[z_{j_2;\tra_2}(t)-\y{j_2}{\tra_2}\ri]\, \notag \\
&-\frac{\eta^3}{6}\sum_{j_1,j_2,j_3,\tra_1,\tra_2,\tra_3}\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3} \notag \,\\
&\qquad\qquad\qquad\qquad\times\le[z_{j_1;\tra_1}(t)-\y{j_1}{\tra_1}\ri]\le[z_{j_2;\tra_2}(t)-\y{j_2}{\tra_2}\ri]\le[z_{j_3;\tra_3}(t)-\y{j_3}{\tra_3}\ri]
\,  \notag,
\end{align}
in conjunction the NTK will evolve as \eqref{eq:NTK-updated-finite-width-refined}
\begin{align}\label{eq:NTK-update-redux}
&\NTKM_{i_1i_2;\delta_1\delta_2}(t+1) \, \\
=&\NTKM_{i_1i_2;\delta_1\delta_2}(t)-\eta\sum_{j,\tra}\Big(\dNTKM_{i_1 i_2j;\delta_1\delta_2\tra}(t)+\dNTKM_{i_2 i_1j;\delta_2\delta_1\tra}(t)\Big)\le[z_{j;\tra}(t)-\y{j}{\tra}\ri]\, \notag\\
&+\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2}\le(\tia{\ddNTK}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2}+\tia{\ddNTK}{i_2i_1j_1j_2}{\delta_2\delta_1\tra_1\tra_2} + 2\tia{\ddNTKII}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2} \ri) \, \notag\\ 
&\qquad\qquad\qquad\qquad\times\le[z_{j_1;\tra_1}(t)-\y{j_1}{\tra_1}\ri]\le[z_{j_2;\tra_2}(t)-\y{j_2}{\tra_2}\ri] \notag
\, , 
\end{align}
and in further conjunction the dNTK will evolve as \eqref{eq:dNTK-updated-finite-width}
\begin{align}\label{eq:dNTK-update-redux}
&\tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}(t+1) \, \\
=&\tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}(t) \, \notag \\
&-\eta\sum_{j,\tra}\le(\tia{\ddNTK}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra}+\tia{\ddNTKII}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra}+\tia{\ddNTKII}{i_0i_2 i_1j}{\delta_0\delta_2\delta_1\tra}\ri)\le[z_{j;\tra}(t)-\y{j}{\tra}\ri]\,\notag .
\end{align}
In writing these dynamical equations, we've stopped explicitly denoting that our equations have $\o{1/n^2}$ errors, and we've also expressed the fact that the ddNTKs\index{ddNTKs!step-independence} are $t$-independent at order $1/n$, using their values at initialization, $\ddNTK_{i_1 i_2i_3i_4;\delta_1\delta_2\delta_3\delta_4}$ and $\ddNTKII_{i_1 i_2i_3i_4;\delta_1\delta_2\delta_3\delta_4}$, with hats to remind us of their stochasticity.
These joint updates \eqref{eq:preactivation-update-redux}, \eqref{eq:NTK-update-redux}, and \eqref{eq:dNTK-update-redux} are coupled \emph{difference equations}\index{difference equation!nonlinear}\index{difference equation|seealso{training dynamics}}, and the equations for the network output and the dynamical NTK\index{dynamical NTK} are both \emph{nonlinear} in the output $z_{i;\delta}$. 

\index{training dynamics!finite width}
Although this seems daunting, we are now going to solve these equations in a closed form. First, we'll analyze the dynamics while neglecting the finite-width effects of the dNTK and ddNTKs, in which the problem will reduce to a single \emph{linear} difference equation.\index{difference equation!linear} Then, we'll use perturbation theory to incorporate the effect of all the NTK differentials and allow for a dynamically evolving NTK and dNTK.








\subsubsection{Free Theory: Step-Independent NTK}\index{training dynamics!finite width}
Let's begin by setting the dNTK and ddNTKs to zero. Since their leading statistics are $1/n$-suppressed, we'll be able to use perturbation theory to reincorporate all their effects later.
In this limit the NTK update equation \eqref{eq:NTK-update-redux} is trivial, solved by the \emph{free} or \emph{step-independent} NTK:\index{neural tangent kernel!step-independent}
\be\label{eq:step-independent-ntk-solution}
\NTKM_{i_1i_1;\delta_1\delta_2}(t)=\NTKM_{i_1i_2;\delta_1\delta_2}(t=0) \equiv \NTK_{i_1i_2;\delta_1\delta_2} \, .
\ee
Unsurprisingly, this just means that when the dNTK and ddNTKs vanish, the NTK doesn't update from its initialization.

\index{training dynamics!finite width}
Plugging this solution into the preactivation update equation  \eqref{eq:preactivation-update-redux} and turning off the dNTK and  ddNTKs, the remaining dynamical equation simplifies to
\be\label{eq:preactivation-update-redux-free}
z_{i;\delta}(t+1)=z_{i;\delta}(t)-\eta\sum_{j,\tra}\NTK_{ij;\delta\tra}[z_{j;\tra}(t)-\y{j}{\tra}]\, .
\ee
Thus, the residual training error, $z_{\tra}(t)-y_{\tra}$, sources the updates to the network output $z_{\delta}(t)$ for general inputs $\delta \in \D$.
Moreover, when restricted to the inputs from the training set $\tra\in\A$, we can rewrite this \terminate{difference equation} \eqref{eq:preactivation-update-redux-free} as
\be\label{eq:gd-as-matrix-multiplication}
z_{i;\tra}(t+1)-\y{i}{\tra}=\sum_{j,\tra_1}\le(\Iden_{ij;\tra\tra_1}-\eta\NTK_{ij;\tra\tra_1}\ri)[z_{j;\tra_1}\!(t)-\y{j}{\tra_1}]\, ,
\ee
where we've defined the \neo{identity operator}
\be
\Iden_{i_1i_2;\tra_1\tra_2}\equiv \delta_{i_1 i_2}\delta_{\tra_1\tra_2}\, .
\ee
In this form, \eqref{eq:gd-as-matrix-multiplication} is a first-order homogeneous linear difference equation\index{difference equation!linear!homogeneous} for the residual training error, $z_{\tra}(t)-y_{\tra}$, which is just a fancy way of saying that this is going to be a \terminate{piece of cake}\index{piece of cake|seealso{free dynamics}}. 

In particular, the update to the prediction error is just a simple multiplication by a constant matrix, and the solution is given by an exponential:
\be\label{eq:free-solution-training}
\zF{i}{\tra}(t)-\y{i}{\tra}=\sum_{j,\tra_1}\Unit_{ij;\tra\tra_1}\!(t)\le(z_{j;\tra_1}-\y{j}{\tra_1}\ri) \, .
\ee
Here, on the left-hand side, we've labeled the solution with an ``F'' to indicate it's the \emph{free solution}, with the nonlinear effects from the dNTK and ddNTKs turned off; on the right-hand side, we have the residual training error at initialization, and the \term{step-evolution operator} is defined as an iterative product of $t$ steps:
\begin{align}\label{eq:unitary-operator-at-least-in-continuum-limit-with-imaginary-time}
\Unit_{i_t i_0;\tra_t\tra_0}(t)\equiv&\le[\le(\Iden-\eta\NTK\ri)^t\ri]_{i_ti_0;\tra_t\tra_0}\, \\
=&\sum_{\substack{i_1,\ldots i_{t-1}\\ \tra_1,\ldots,\tra_{t-1}}}\le(\Iden_{i_t i_{t-1};\tra_t\tra_{t-1}}-\eta\NTK_{i_t i_{t-1};\tra_{t}\tra_{t-1}}\ri)\cdots\le(\Iden_{i_1 i_0;\tra_1,\tra_0}-\eta\NTK_{i_1 i_0;\tra_1\tra_0}\ri)\, .\notag
\end{align}
For any positive-definite NTK and sufficiently small global learning rate $\eta$, this operator will exponentially decay to zero, $\Unit(t)\to 0$, as the number of steps becomes large, $t\to\infty$.\footnote{
    In particular, for this limit to converge we just need $ ||\Iden-\eta \NTK||_\infty < 1$, i.e.~the largest eigenvalue of the operator $\Iden-\eta \NTK$  must be less than one. With our attention to the principles of criticality and equivalence,\index{equivalence principle}\index{criticality!principle of} our choices of initialization and training hyperparameters were made so that the NTK is always of order one, and thus it's very easy for our networks to satisfy this constraint. \label{footnote:convergence-dynamics}
} 
Thus, the residual training error \eqref{eq:free-solution-training} will vanish exponentially quickly, 
\be\label{eq:free-training-converges}
\lim_{t\to\infty} \zF{i}{\tra}(t) = \y{i}{\tra} \, ,
\ee
with the step scale for the decay of the individual components set by the step-independent NTK.\footnote{If we wanted to study the ODE or \neo{continuum limit}\index{continuum limit|see{gradient descent}}\index{gradient descent!continuum or ODE limit} of the dynamics, we could take the global learning rate to zero, $\eta\to 0$, while holding the product, $\tau\equiv \eta t$, fixed. In such a limit, the step-evolution operator becomes simply $\Unit(t)\to \exp(-\NTK\tau)$. While such a limit is mostly unnecessary for any theoretical purpose -- it's just as easy to study the discrete dynamics that actually describe the practical optimization algorithm -- it does provide more substance to objection in footnote~\ref{footnote:ntk-name} of \S\ref{sec:gd} of the name \neo{neural tangent kernel} for the stochastic operator $\NTK$.
\index{neural tangent kernel!name}
In particular, this continuum limit makes clear that the NTK is really best thought of as a \neo{Hamiltonian}, as it generates the evolution of observables, and the step-evolution operator $\Unit(t)$ is like a unitary time-evolution operator, albeit in $i$maginary time.
\index{imaginary time}
More precisely, in the limit where the dNTK and ddNTKs are set to zero, the NTK is akin to a \emph{free} Hamiltonian, with exactly solvable dynamics; with a nonzero dNTK or ddNTKs, the Hamiltonian includes nontrivial \emph{interactions}\index{interactions!dynamics} and can be analyzed via time-dependent \terminate{perturbation theory}.\label{eq:footnote-continuum-limit}
}










\index{dynamics|see{training dynamics}}\index{training dynamics!finite width}
Having now solved the free dynamics on the training set, we can plug this solution~\eqref{eq:free-solution-training} back into the \terminate{difference equation} \eqref{eq:preactivation-update-redux-free} for general inputs $\delta \in \D$. With the source known explicitly, we can easily write down a solution that satisfies the initial condition $\zF{i}{\delta}(t=0)=z_{i;\delta}$:
\begin{align}\label{eq:free-solution-general}
\zF{i}{\delta}(t)=&z_{i;\delta}-\sum_{j,\tra}\NTK_{ij;\delta\tra}\le\{\eta\sum_{s=0}^{t-1}\le[\zF{j}{\tra}(s)-\y{j}{\tra}\ri]\ri\}\, \\
=&z_{i;\delta}-\sum_{j,\tra_1,\tra_2}\NTK_{ij;\delta\tra_1}a_{j;\tra}(t)\, .\notag
\end{align}
Here we've defined a dynamical helper function, with an explicit representation given by
\begin{align}\label{eq:dynamical-helper-a}
a_{j;\tra}(t)\equiv&\eta\sum_{s=0}^{t-1}\le[\zF{j}{\tra}(s)-\y{j}{\tra}\ri]=\eta\sum_{s=0}^{t-1}\le[\sum_{k,\tra_1}\Unit_{jk;\tra\tra_1}\!(t)\le(z_{k;\tra_1}-\y{k}{\tra_1}\ri)\ri] \, \\
=&\eta\sum_{k,\tra_1} \le\{ \sum_{s=0}^{t-1}\!\le[ \le(\Iden-\eta\NTK\ri)^{s}\ri]_{j k;\tra\tra_1}\ri\}\le(z_{k;\tra_1}-\y{k}{\tra_1}\ri)\, \notag\\
=&\eta\sum_{k,m,\tra_1,\tra_2}\le\{\le[\Iden-\le(\Iden-\eta\NTK\ri)\ri]^{-1}\ri\}_{jm}^{\tra\tra_2}\le[\Iden-\le(\Iden-\eta\NTK\ri)^t\ri]_{mk;\tra_2\tra_1}\le(z_{k;\tra_1}-\y{k}{\tra_1}\ri)\, \notag\\
=&\sum_{m,\tra_2}\le(\NTK^{-1}\ri)_{jm}^{\tra\tra_2}\le\{z_{m;\tra_2}-\y{m}{\tra_2}-\le[\zF{m}{\tra_2}(t)-\y{m}{\tra_2}\ri]\ri\}\,\notag \, .
\end{align}
In this expression, to get to the third line we used the standard formula for evaluating geometric sums, $1+x+ x^2 + \dots + x^{t-1} = (1-x^t)/(1-x)$, and to get to the final line we evaluated the inverse and substituted in for the \terminate{step-evolution operator} \eqref{eq:unitary-operator-at-least-in-continuum-limit-with-imaginary-time}. Recall also that $\NTK^{-1}$,  defined by 
\eqref{eq:stochastic-ntk-mean}, 
is the inverse of the stochastic NTK submatrix evaluated on the training set for a particular realization of the parameters.\footnote{Unfortunately, in the absence of a Newton tensor, the raised/lowered sample indices in \eqref{eq:dynamical-helper-a} do not align well. (In fact, the problem really began in the update equation \eqref{eq:preactivation-update-redux-free} with the doubled-lowered $\tra$ index.) If you'd like, you can fix this by judicious use of identity matrices such as $\delta_{\tra_1 \tra_2}$ and $\delta^{\tra_3 \tra_4}$.
However, such usage over-clutters the presentation and so is probably not worth it, despite the additional clarity and \neo{type safety} that such index alignment provides.
}




\index{training dynamics!finite width}
In particular, for sufficiently small $\eta$, as the number of steps becomes large, $t\to\infty$, the residual error $\zF{m}{\tra_2}(t)-\y{m}{\tra_2}$ exponentially vanishes \eqref{eq:free-training-converges}.
Thus, the prediction for a general input $\delta \in \D$ \eqref{eq:free-solution-general} will exponentially converge to
\be\label{eq:free-solution-general-end-of-time}
\zF{i}{\delta}(t=\infty)=z_{i;\delta}-\sum_{j,k,\tra_1,\tra_2}\NTK_{ij;\delta\tra_1}\le(\NTK^{-1}\ri)_{jk}^{\tra_1\tra_2}\le(z_{k;\tra_2}-\y{k}{\tra_2}\ri)\, .
\ee
Although we took the limit of a large number of steps here, the exponential convergence means that for any number of steps $T$ such that $T \gtrsim (\eta \NTK)^{-1}$, the prediction error will be exponentially close to its final $T\to\infty$ value.



In fact, this solution \eqref{eq:free-solution-general-end-of-time} precisely matches the solution we  would have gotten in \S\ref{subsec:giant-plus-small} after the first update \eqref{eq:second-order-update-per-component} with the Newton tensor \eqref{eq:newton-step-generalized-first}, so long as the dNTK and ddNTKs are turned off.
This means that the \neo{algorithm dependence} that emerges at finite width is solely due to the presence of the NTK differentials and the resulting nonlinear dynamics. %


\index{training dynamics!finite width}
\subsubsection{Interacting Theory: Dynamical NTK and dNTK}\index{interactions!dynamics}
Now, let's incorporate the nonzero dNTK and ddNTKs into our analysis. To do so, we need to decompose both the dynamical network output and the dynamical NTK\index{dynamical NTK} as
\begin{align}\label{eq:free-interaction-decomposition-it-is-physics}
z_{i;\delta}(t)\equiv& \zF{i}{\delta}(t)+\zI{i}{\delta}(t)\, , \\
\NTKM_{ij ;\delta\tra}(t) \equiv& \NTK_{ij;\delta\tra} + \NTKM^{\text{I}}_{ij ;\delta\tra}(t)  \, .
\label{eq:free-interaction-decomposition-for-NTK} 
\end{align}
Here, for the network output, $\zF{i}{\delta}(t)$ is the \emph{free} part, satisfying the linear difference equation\index{difference equation!linear} \eqref{eq:preactivation-update-redux-free} with a solution given by \eqref{eq:free-solution-general}, and $\zI{i}{\delta}(t)$ is the \emph{interacting}\index{interactions!dynamics} part, encapsulating the corrections due to all the nonzero NTK differentials. Similarly, for the NTK, $\NTK_{ij;\delta\tra}$ is the step-independent or \emph{free} part, fixed at initialization, and $\NTKM^{\text{I}}_{ij ;\delta\tra}(t)$  is the dynamical step-dependent or \emph{interaction} NTK\index{interaction NTK}\index{neural tangent kernel!interaction|see{interaction NTK}}.
Since both $\zI{i}{\delta}(t)$ and $\NTKM^{\text{I}}_{ij ;\delta\tra}(t)$ are absent in the free limit,  $\dNTK,\, \ddNTK,\, \ddNTKII \to 0$, at leading order we expect them to be a linear combination of these objects, schematically
\begin{align}\label{eq:linear-combo}
z^{\text{I}}(t)  &= [\text{thing}\ 0](t)~ \dNTK + [\text{thing}\ 1](t)~ \ddNTK  + [\text{thing}\ 2](t)~ \ddNTKII  \, , \\
\NTKM^{\text{I}}(t)  &=  [\widetilde{\text{thing}}\ 0](t)~ \dNTK + [\widetilde{\text{thing}}\ 1](t)~ \ddNTK  + [\widetilde{\text{thing}}\ 2](t)~ \ddNTKII\, ,
\end{align}
where various tensorial things will have various time dependencies.
This  means in turn that any product of  $\zI{i}{\delta}(t)$ or $\NTKM^{\text{I}}_{ij ;\delta\tra}(t)$ with one of $ \dNTK, \ddNTK, \ddNTKII $ can be neglected to leading order, which is the main reason why we'll be able to systematically solve these nonlinear dynamics by perturbation theory.
Finally, the initial condition for the interacting parts of the network output and the NTK must satisfy
\begin{align}\label{eq:interacting-piece-time-initial-condition}
\zI{i}{\delta}(t=0)=0\, ,\\
\NTKM^{\text{I}}_{ij ;\delta\tra}(t=0) =0 \, ,
\label{eq:interacting-piece-NTK-time-initial-condition}
\end{align}
since the free part of the network output already satisfies $\zF{i}{\delta}(t=0)=z_{i;\delta}$ at initialization, and the free part of the NTK is step-independent and thus \emph{is} the NTK at initialization.
With all those in mind, our goal now is to find a solution for $\zI{i}{\delta}(t)$ such that 
the full network output, $z_{i;\delta}(t)$, 
the \terminate{interaction NTK}, $\NTKM^{\text{I}}_{ij ;\delta\tra}(t)$, and the dynamical dNTK\index{dynamical dNTK}\index{differential of the neural tangent kernel!dynamical|see{dynamical dNTK}}, $\tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}(t)$, all together satisfy the coupled nonlinear dynamics \eqref{eq:preactivation-update-redux}, \eqref{eq:NTK-update-redux}, and \eqref{eq:dNTK-update-redux} to leading order in $1/n$.




\index{training dynamics!finite width}
First, let's work out the dynamics of the dNTK. From \eqref{eq:dNTK-update-redux} we see that the updates to the dNTK are sourced by the residual training error $z_{i;\tra}(t)-y_{i;\tra}$. Using our decomposition for the network output, \eqref{eq:free-interaction-decomposition-it-is-physics} and the initial condition,
\be
\tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}(t=0) =\tia{\dNTK}{i_0i_1i_2}{\delta_0\delta_1\delta_2} \, ,
\ee
after iterating the dynamics, we have
\begin{align}\label{eq:dNTK-time-dependent-solution-semiformal}
&\tia{\dNTKM}{i_0i_1i_2}{\delta_0\delta_1\delta_2}(t) \,  \\
=&\tia{\dNTK}{i_0i_1i_2}{\delta_0\delta_1\delta_2}-\sum_{j,\tra}\le(\tia{\ddNTK}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra}+\tia{\ddNTKII}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra}+\tia{\ddNTKII}{i_0i_2 i_1j}{\delta_0\delta_2\delta_1\tra}\ri) \, \notag \\
&\qquad\qquad\times \le\{\eta\sum_{s=0}^{t-1}\le[\zF{j}{\tra}(s)+\zI{j}{\tra}(s)-\y{j}{\tra}\ri]\ri\} \, \notag \\
=&\tia{\dNTK}{i_0i_1i_2}{\delta_0\delta_1\delta_2}-\!\!\sum_{j,\tra}\!\le(\tia{\ddNTK}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra}\!+\!\tia{\ddNTKII}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra}\!+\!\tia{\ddNTKII}{i_0i_2 i_1j}{\delta_0\delta_2\delta_1\tra}\!\ri)\! a_{j;\tra}(t)\, . \notag
\end{align}
Here in the last step, we first dropped the product of $\zI{i}{\delta}(t)$ with $\ddNTK$ as explained earlier, and then substituted in our dynamical helper function \eqref{eq:dynamical-helper-a}.
Now, plugging in our final expression
from \eqref{eq:dynamical-helper-a} and neglecting the fluctuation part of the NTK inverse as subleading, we find an expression for the %
dynamical dNTK:
\begin{align}\label{eq:dNTK-time-dependent-solution}
&\tia{\dNTK}{i_0i_1i_2}{\delta_0\delta_1\delta_2}(t) \, \notag \\
=&\tia{\dNTK}{i_0i_1i_2}{\delta_0\delta_1\delta_2}-\sum_{k,\tra_1,\tra_2}\le(\tia{\ddNTK}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra_1}+\tia{\ddNTKII}{i_0i_1 i_2j}{\delta_0\delta_1\delta_2\tra_1}+\tia{\ddNTKII}{i_0i_2 i_1j}{\delta_0\delta_2\delta_1\tra_1}\ri) \, \notag \\
&\qquad\qquad\times\NTKMsub^{\tra_1\tra_2}\le\{z_{j;\tra_2}-\y{j}{\tra_2}-\le[\zF{j}{\tra_2}(t)-\y{j}{\tra_2}\ri]\ri\}\,  . %
\end{align}
In particular, the quantity in the curly brackets represents the difference in training errors between initialization and step $t$: the larger this difference -- i.e.~the more the residual training error decreases -- the greater the evolution of the dNTK, and the more the meta feature functions evolve, undergoing their own form of \emph{meta representation learning}\index{representation learning!meta representation learning}\index{meta representation learning|see{representation learning}} %
over the course of training. %












\index{neural tangent kernel!dynamics}\index{training dynamics!finite width}
Next, using our decompositions for the network output and NTK, \eqref{eq:free-interaction-decomposition-it-is-physics} and \eqref{eq:free-interaction-decomposition-for-NTK}, we can rewrite the NTK dynamics \eqref{eq:NTK-update-redux} as a \terminate{difference equation} for the \terminate{interaction NTK}:
\begin{align}\label{eq:interacting-NTK-dynamics-unsimplified}
&\NTKM^{\text{I}}_{i_1i_2;\delta_1\delta_2}(t+1) \, \\
=&\NTKM^{\text{I}}_{i_1i_2;\delta_1\delta_2}(t)-\eta\sum_{j,\tra}\Big(\dNTKM_{i_1 i_2j;\delta_1\delta_2\tra}(t)+\dNTKM_{i_2 i_1j;\delta_2\delta_1\tra}(t)\Big)\le[\zF{j}{\tra}(t)-\y{j}{\tra}\ri]\, \notag\\
&+\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2}\le(\tia{\ddNTK}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2}+\tia{\ddNTK}{i_2i_1j_1j_2}{\delta_2\delta_1\tra_1\tra_2} + 2\tia{\ddNTKII}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2} \ri) \, \notag\\ 
&\qquad\qquad\qquad\qquad\times\le[\zF{j_1}{\tra_1}(t)-\y{j_1}{\tra_1}\ri]\le[\zF{j_2}{\tra_2}(t)-\y{j_2}{\tra_2}\ri] \, .\notag %
\end{align}
Here, once again, we've dropped the interacting part of the network output on the right-hand side, as it is always multiplied by either the dNTK or the ddNTKs and thus will be subleading in $1/n$. Denoting the free part of the residual training error \eqref{eq:free-solution-training} as
\be\label{eq:free-residual-training-error}
\eF{j}{\tra}(t)\equiv\zF{j}{\tra}(t)-\y{j}{\tra}\, ,
\ee
for notational convenience, and substituting in our solution for the dynamical  dNTK  \eqref{eq:dNTK-time-dependent-solution},  we get
\begin{align}\label{eq:interacting-NTK-dynamics-unsimplified-substituted}
&\NTKM^{\text{I}}_{i_1i_2;\delta_1\delta_2}(t+1)-\NTKM^{\text{I}}_{i_1i_2;\delta_1\delta_2}(t) \, \\
=&-\eta\sum_{j,\tra}\Big(\dNTK_{i_1 i_2j;\delta_1\delta_2\tra}+\dNTK_{i_2 i_1j;\delta_2\delta_1\tra}\Big)\eF{j}{\tra}(t)\, \notag\\
&+ \eta\sum_{j,k,\tra,\tra_1,\tra_2}\Big(
\tia{\ddNTK}{i_1i_2 j k}{\delta_1\delta_2\tra\tra_1}+
\tia{\ddNTKII}{i_1i_2 j k}{\delta_1\delta_2\tra\tra_1}+
\tia{\ddNTKII}{i_1ji_2  k}{\delta_1\tra\delta_2\tra_1}
\, \notag \\
&\qquad\qquad\quad\ 
+\tia{\ddNTK}{i_2i_1 j k}{\delta_2\delta_1\tra\tra_1}
+\tia{\ddNTKII}{i_2i_1 j k}{\delta_2\delta_1\tra\tra_1}
+\tia{\ddNTKII}{i_2ji_1  k}{\delta_2\tra\delta_1\tra_1}
\Big) \, \notag \\
&\qquad\qquad\qquad\qquad\times\NTKMsub^{\tra_1\tra_2}\eF{j}{\tra}(t)\le[ z_{k;\tra_2}-\y{k}{\tra_2}- \eF{k}{\tra_2}(t)\ri] \notag \, \\
&+\frac{\eta^2}{2}\sum_{j_1,j_2,\tra_1,\tra_2}\le(\tia{\ddNTK}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2}+\tia{\ddNTK}{i_2i_1j_1j_2}{\delta_2\delta_1\tra_1\tra_2} + 2\tia{\ddNTKII}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2} \ri) \, \notag\\ 
&\qquad\qquad\qquad\qquad\times\eF{j_1}{\tra_1}(t)\, \eF{j_2}{\tra_2}(t) \notag
\, .
\end{align}
In particular, the step dependence on the right-hand side is expressed entirely in terms of the free residual training error $\eF{j}{\tra}(t)$, and each term is either 
linear or quadratic in $\eF{j}{\tra}(t)$.
Thus, in order to solve this \terminate{difference equation} and get $\NTKM^{\text{I}}_{i_1i_2;\delta_1\delta_2}(t)$, we'll just have to compute sums over these terms.







\index{training dynamics!finite width}
One of those sums -- the one that's linear in the free residual training error -- is the dynamical helper function $a_{j;\tra}(t)\equiv\eta\sum_{s=0}^{t-1}\eF{j}{\tra}(t)$ that we evaluated in \eqref{eq:dynamical-helper-a}.
The other type of sum is quadratic in the free residual training error, which will define a second dynamical helper function:
\begin{align}\label{eq:dynamical-helper-b}
 &b_{j_1j_2;\tra_1\tra_2}(t)\, \\
\equiv&\eta\sum_{s=0}^{t-1}\eF{j_1}{\tra_1}(t)\eF{j_2}{\tra_2}(t)\, \notag\\
=&\eta\!\!\sum_{k_1,k_2,\tra_3,\tra_4}\sum_{s=0}^{t-1}\!\le[ \le(\Iden-\eta\NTK\ri)^{s}\ri]_{j_1k_1;\tra_1\tra_3}\!\!\le[ \le(\Iden-\eta\NTK\ri)^{s}\ri]_{j_2k_2;\tra_2\tra_4}\!\!\le(z_{k_1;\tra_3}-\y{k_1}{\tra_3}\ri)\le(z_{k_2;\tra_4}-\y{k_2}{\tra_4}\ri)\, \notag\\
=&\sum_{\tra_3,\tra_4}\!\!\geosumtwo^{\tra_1\tra_2\tra_3\tra_4}\le[\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri) -\eF{j_1}{\tra_3}(t)\eF{j_2}{\tra_4}(t)\ri]+\o{\frac{1}{n}}\, .\notag
\end{align}
To evaluate this sum, we again used our expression for the free residual training error, \eqref{eq:free-solution-training}, in conjunction with the definition of the \terminate{step-evolution operator}, \eqref{eq:unitary-operator-at-least-in-continuum-limit-with-imaginary-time}. Then, we replaced the stochastic NTK $\tia{\NTK}{ij}{\tra_1\tra_2}$ of the training set by its mean $\delta_{ij}\NTKM_{\tra_1\tra_2}$ at the cost of subleading corrections, and formally performed the geometric sum as in \eqref{eq:dynamical-helper-a}. This last operation yielded an \neo{inverting tensor}
$\geosumtwo^{\tra_1\tra_2\tra_3\tra_4}$ implicitly defined by
\begin{align}
\delta^{\tra_1}_{\ \tra_5}\delta^{\tra_2}_{\ \tra_6}=&\sum_{\tra_3,\tra_4\in\A}\geosumtwo^{\tra_1\tra_2\tra_3\tra_4}\frac{1}{\eta}\le[\delta_{\tra_3\tra_5}\delta_{\tra_4\tra_6}-(\delta_{\tra_3\tra_5}-\eta\NTKMsub_{\tra_3\tra_5})(\delta_{\tra_4\tra_6}-\eta\NTKMsub_{\tra_4\tra_6})\ri]\, \notag\\
=&\sum_{\tra_3,\tra_4\in\A}\geosumtwo^{\tra_1\tra_2\tra_3\tra_4}\le(\NTKMsub_{\tra_3\tra_5}\delta_{\tra_4\tra_6}+\delta_{\tra_3\tra_5}\NTKMsub_{\tra_4\tra_6}-\eta\NTKMsub_{\tra_3\tra_5}\NTKMsub_{\tra_4\tra_6}\ri)\, .\label{eq:implicit-X-tensor}
\end{align}
This tensor is a generalization of the familiar inverting matrix $\geosumone^{\tra_1\tra_2}\equiv \NTKMsub^{\tra_1\tra_2}$ for geometric sums over matrices that satisfies
\be
\delta^{\tra_1}_{\ \tra_3}=\sum_{\tra_2\in\A}\geosumone^{\tra_1\tra_2}\frac{1}{\eta}\le[\delta_{\tra_2\tra_3}-(\delta_{\tra_2\tra_3}-\eta\NTKMsub_{\tra_2\tra_3})\ri]=\sum_{\tra_2\in\A}\geosumone^{\tra_1\tra_2}\NTKMsub_{\tra_2\tra_3}\, ,
\ee
and appeared in the last expression of the dynamical helper function $a_{j;\tra}(t)$ \eqref{eq:dynamical-helper-a}.
While we are on fire like an activated neuron, let's also define a final dynamical helper function for sums that are cubic in the free residual training error:
\begin{align}\label{eq:dynamical-helper-c}
 &c_{j_1j_2j_3;\tra_1\tra_2\tra_3}(t)\, \\
\equiv&\eta\sum_{s=0}^{t}\eF{j_1}{\tra_1}(t)\eF{j_2}{\tra_2}(t)\eF{j_3}{\tra_3}(t)\, \notag\\
=&\sum_{\tra_4,\tra_5,\tra_6}\!\!\geosumthree^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\Big[ \le(z_{k_1;\tra_4}-\y{k_1}{\tra_4}\ri)\le(z_{k_2;\tra_5}-\y{k_2}{\tra_5}\ri)\le(z_{k_3;\tra_6}-\y{k_3}{\tra_6}\ri)\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad-\eF{j_1}{\tra_4}(t)\,\eF{j_2}{\tra_5}(t)\,\eF{j_3}{\tra_6}(t)\Big]+\!\o{\frac{1}{n}}\, .\notag
\end{align}
In this case, the \terminate{inverting tensor} $\geosumthree^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}$ is implicitly defined by
\begin{align}\label{eq:implicit-X3-tensor}
&\delta^{\tra_1}_{\ \tra_7}\delta^{\tra_2}_{\ \tra_8}\delta^{\tra_3}_{\ \tra_9}\, \\
 =&\sum_{\tra_4,\tra_5,\tra_6}\geosumthree^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}  \bigg[ \Big(\NTKMsub_{\tra_4\tra_7}\delta_{\tra_5\tra_8}\delta_{\tra_6\tra_9} +\delta_{\tra_4\tra_7}\NTKMsub_{\tra_5\tra_8}\delta_{\tra_6\tra_9} +  \delta_{\tra_4\tra_7}\delta_{\tra_5\tra_8}\NTKMsub_{\tra_6\tra_9} \Big)\notag \, \\
&\qquad\qquad\qquad\qquad\quad-\eta\Big(\NTKMsub_{\tra_4\tra_7}\NTKMsub_{\tra_5\tra_8}\delta_{\tra_6\tra_9} + \NTKMsub_{\tra_4\tra_7}\delta_{\tra_5\tra_8}\NTKMsub_{\tra_6\tra_9} +  \delta_{\tra_4\tra_7}\NTKMsub_{\tra_5\tra_8}\NTKMsub_{\tra_6\tra_9} \Big) \notag \\
&\qquad\qquad\qquad\qquad\quad+ \eta^2 \Big( \NTKMsub_{\tra_4\tra_7}\NTKMsub_{\tra_5\tra_8}\NTKMsub_{\tra_6\tra_9} \Big) \bigg] \, .\notag
\end{align}
Essentially, these three dynamical helper functions encode the step dependence of various geometric sums of the free residual training error $\eF{j}{\tra}(t)$.
Note that we have\index{training dynamics!finite width}
\begin{align}
a_{j_1;\tra_1}(t=0) &= 0 \, ,\label{eq:a-init-vanish}\\
b_{j_1j_2;\tra_1\tra_2}(t=0) &=0 \, ,\label{eq:b-init-vanish}\\
c_{j_1j_2j_3;\tra_1\tra_2\tra_3}(t=0) &=0 \, ,\label{eq:c-init-vanish}
\end{align}
and so they all vanish at initialization, while at the end of training we have
\begin{align}
a_{j_1;\tra_1}(\infty) =&\!\sum_{\tra_2}\geosumone^{\tra_1\tra_2}\le(z_{j_1;\tra_2}-\y{j_1}{\tra_2}\ri)+\o{\frac{1}{n}} \, ,\label{eq:a-end}\\
b_{j_1j_2;\tra_1\tra_2}(\infty) =&\!\!\sum_{\tra_3,\tra_4}\geosumtwo^{\tra_1\tra_2\tra_3\tra_4}\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri)+\o{\frac{1}{n}} \, ,\label{eq:b-end}\\
c_{j_1j_2j_3;\tra_1\tra_2\tra_3}(\infty) =&\!\!\!\sum_{\tra_4,\tra_5,\tra_6}\geosumthree^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\,\notag\\
&+\o{\frac{1}{n}}\, ,\label{eq:c-end}
\end{align}
since the free residual training error  $\eF{j}{\tra}(t)$ vanishes exponentially quickly, cf.~\eqref{eq:free-training-converges}.


\index{training dynamics!finite width}
With the help of the first two of these dynamical helper functions, we can semi-compactly write the solution to the \terminate{difference equation} for the \terminate{interaction NTK} \eqref{eq:interacting-NTK-dynamics-unsimplified-substituted} as
\begin{align}\label{eq:interacting-NTK-solution}
&\NTKM^{\text{I}}_{i_1i_2;\delta_1\delta_2}(t) \, \\
=&-\sum_{j,\tra}\Big(\dNTK_{i_1 i_2j;\delta_1\delta_2\tra}+\dNTK_{i_2 i_1j;\delta_2\delta_1\tra}\Big)
a_{j;\tra}(t)
\, \notag\\
&+ \sum_{j,k,\tra_1,\tra_2,\tra_3}\!\!\Big(
\tia{\ddNTK}{i_1i_2 j k}{\delta_1\delta_2\tra_1\tra_2}+
\tia{\ddNTKII}{i_1i_2 j k}{\delta_1\delta_2\tra_1\tra_2}+
\tia{\ddNTKII}{i_1ji_2  k}{\delta_1\tra_1\delta_2\tra_2}
\, \notag \\
&\qquad\qquad\ \ 
+\tia{\ddNTK}{i_2i_1 j k}{\delta_2\delta_1\tra_1\tra_2}
+\tia{\ddNTKII}{i_2i_1 j k}{\delta_2\delta_1\tra_1\tra_2}
+\tia{\ddNTKII}{i_2ji_1  k}{\delta_2\tra_1\delta_1\tra_2}
\Big) \, \notag \\
&\qquad\qquad\qquad\qquad\times\NTKMsub^{\tra_2\tra_3}\Big[ 
\le( z_{k;\tra_3}-\y{k}{\tra_3} \ri) a_{j;\tra_1}(t) 
-
 b_{jk;\tra_1\tra_3}(t)
\Big]
\, \notag \\
&+\frac{\eta}{2}\!\!\sum_{j_1,j_2,\tra_1,\tra_2}\!\!\!\le(\tia{\ddNTK}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2}+\tia{\ddNTK}{i_2i_1j_1j_2}{\delta_2\delta_1\tra_1\tra_2} + 2\tia{\ddNTKII}{i_1i_2j_1j_2}{\delta_1\delta_2\tra_1\tra_2} \ri)\! b_{j_1j_2;\tra_1\tra_2}(t) \, \notag .
\end{align}
As a quick sanity check, the vanishing initial condition for the \terminate{interaction NTK},  \eqref{eq:interacting-piece-NTK-time-initial-condition}, is satisfied due to the vanishing of the helper functions at initialization, \eqref{eq:a-init-vanish} and \eqref{eq:b-init-vanish}.
 We can also plug in \eqref{eq:a-end} and \eqref{eq:b-end} to evaluate the change in NTK at the end of training. In particular, we see that the larger the change in the NTK, the more the feature functions evolve. Thus, more initial error entails more \terminate{representation learning} %
over the course of training. 

\index{training dynamics!finite width}
Lastly, we need to determine the step dependence of the interacting part of the network output, $\zI{i}{\delta}(t)$.
Inserting our free-interacting decomposition, \eqref{eq:free-interaction-decomposition-it-is-physics}, into our dynamics for the network output, \eqref{eq:preactivation-update-redux}, and using the fact that the free part
satisfies the step-independent evolution equation \eqref{eq:gd-as-matrix-multiplication}, we can 
reorganize terms to find a dynamical equation for the interacting part only:
\be\label{eq:gd-interacting-piece}
\zI{i}{\delta}(t+1)=\zI{i}{\delta}(t)-\sum_{j,\tra}\eta\,\NTK_{ij;\delta\tra}\,\zI{j}{\tra}(t)+\eta \,\force_{i;\delta}(t)\, .
\ee
Here, we've defined
a \neo{damping force}:
\begin{align}\label{eq:damping-force-definition}
\force_{i;\delta}(t)\equiv&-\sum_{j,\tra}\NTKM^{\text{I}}_{ij ;\delta\tra}(t)\, \eF{j}{\tra}(t)+\frac{\eta}{2}\sum_{j_1,j_2,\tra_1,\tra_2}\dNTKM_{i j_1j_2;\delta\tra_1\tra_2}(t)\,\eF{j_1}{\tra_1}(t)\,\eF{j_2}{\tra_2}(t)\, \\
&-\frac{\eta^2}{6}\sum_{j_1,j_2,j_3,\tra_1,\tra_2,\tra_3}\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}\,\eF{j_1}{\tra_1}(t)\,\eF{j_2}{\tra_2}(t)\,\eF{j_3}{\tra_3}(t)\, . \notag
\end{align}
Since we have solutions for the \terminate{interaction NTK}\index{training dynamics!finite width} 
and  the dNTK 
dynamics
in terms of the free residual training error solution $\eF{j}{\tra}(t)$, \eqref{eq:interacting-NTK-solution} and \eqref{eq:dNTK-time-dependent-solution}, this damping force is an explicitly known function of the step $t$.







\index{training dynamics!finite width}
Let us now try to implicitly express the solution to this difference equation \eqref{eq:gd-interacting-piece} as a sum over steps.
First for inputs in the training set $\tra \in \A$, the dynamics of the interacting part of the output, \eqref{eq:gd-interacting-piece}, reduce to a first-order inhomogeneous linear difference equation\index{difference equation!linear!inhomogeneous}, with the \terminate{damping force} ruining the homogeneity:
\be
\zI{i}{\tra}(t+1)=\sum_{j,\tra_1}\le(\Iden_{ij;\tra\tra_1}-\eta\NTK_{ij;\tra\tra_1}\ri)\zI{j}{\tra_1}\!(t)+\eta\, \force_{i;\tra}(t)\, .
\ee
We can formally solve this equation as a convolution of the damping force with the free step-evolution operator \eqref{eq:unitary-operator-at-least-in-continuum-limit-with-imaginary-time},
\be\label{eq:interacting-solution-training-formal}
\zI{i}{\tra}(t)=\eta\sum_{s=0}^{t-1}\sum_{j,\tra_1} \Unit_{ij;\tra\tra_1}\!(t-1-s)\,\force_{j;\tra_1}\!(s)\, ,
\ee
which satisfies the initial condition $\zI{i}{\tra}(t=0)=0$. 
Plugging this result back into the dynamical equation for general inputs $\delta\in\D$, \eqref{eq:gd-interacting-piece}, we can then find a solution for the interacting part of the network output for such general inputs:
\be\label{eq:interacting-solution-general-formal}
\zI{i}{\delta}(t)=\eta\sum_{s=0}^{t-1}\le[\force_{i;\delta}(s)-\sum_{j,\tra}\NTK_{ij;\delta\tra}\, \zI{j}{\tra}(s)\ri]\, .
\ee



\index{training dynamics!finite width}
In order to further simplify these expressions, we need to work out one convoluted sum:
\begin{align}
\eta\sum_{s=0}^{t-1}\zI{j}{\tra}(s)=&\eta^2\sum_{s=0}^{t-1}\sum_{\tilde{s}=0}^{s-1}\sum_{k,\tra_1} \Unit_{jk;\tra\tra_1}\!(s-1-\tilde{s})\,\force_{k;\tra_1}\!(\tilde{s})\, \\
=&\eta \sum_{u=0}^{t-2}\sum_{k,\tra_1} \le[\eta \sum_{\tilde{u}=0}^{t-u-2}\Unit_{jk;\tra\tra_1}\!(\tilde{u})\ri]\force_{k;\tra_1}\!(u)\, \notag\\
=&\eta\sum_{u=0}^{t-2}\sum_{k,m,\tra_1,\tra_2}\le(\NTK^{-1}\ri)_{jm}^{\tra\tra_2}\le[\Iden-\Unit(t-u-1)\ri]_{mk;\tra_2\tra_1}\force_{k;\tra_1}(u)\, \notag\\
=&\sum_{m,\tra_2}\le(\NTK^{-1}\ri)_{jm}^{\tra\tra_2}\le\{\le[\eta\sum_{u=0}^{t-2}\force_{m;\tra_2}(u)\ri]+ \le[ \eta\force_{m;\tra_2}(t-1) -\zI{m}{\tra_2}(t)\ri]   \ri\}\, \notag\\
=&\sum_{m,\tra_2}\le(\NTK^{-1}\ri)_{jm}^{\tra\tra_2}\le\{\le[\eta\sum_{u=0}^{t-1}\force_{m;\tra_2}(u)\ri]-\zI{m}{\tra_2}(t)\ri\}\, ,\notag
\end{align}
Step by step, on the first line we used the expression for the formal solution \eqref{eq:interacting-solution-training-formal}; on the second line we first reversed the order of the sums as $\sum_{s=0}^{t-1} \sum_{\tilde{s}=0}^{s-1} = \sum_{\tilde{s}=0}^{t-2} \sum_{s=\tilde{s}+1}^{t-1}$, and then we rewrote these sums in terms of new variables, $u\equiv \tilde{s}$ and $\tilde{u}\equiv s-1-\tilde{s}$; on the third line, we performed the geometric sum exactly as in \eqref{eq:dynamical-helper-a};
on the fourth line, we used our formal solution \eqref{eq:interacting-solution-training-formal} at step $t$; and on the final line, we combined terms to extend the limits of the sum over the \terminate{damping force}. Plugging this evaluation back into our formal solution for $\zI{i}{\delta}(t)$,
\eqref{eq:interacting-solution-general-formal}, we get a slightly less formal solution:
\begin{align}\label{eq:interacting-solution-general}
\zI{i}{\delta}(t)=&\eta\sum_{s=0}^{t-1}\le[\force_{i;\delta}(s)-\sum_{\tra_1,\tra_2}\NTKM_{\delta\tra_1}\NTKMsub^{\tra_1\tra_2}\force_{i;\tra_2}(s)\ri]+\sum_{\tra_1,\tra_2}\NTKM_{\delta\tra_1}\NTKMsub^{\tra_1\tra_2}\zI{i}{\tra_2}(t) + \o{\frac{1}{n^2}}\, .
\end{align}
In this result, we've replaced the stochastic NTK by its mean, $\NTK_{ij;\delta_1\delta_2}\to\delta_{ij}\NTKM_{\delta_1,\delta_2}$, as every term is otherwise already proportional to the dNTK or ddNTKs. As a quick sanity check, note that for inputs in the training set, $\delta \to \tra \in \A$, this general expression reduces to an identity on $\zI{i}{\tra}(t)$.

\index{training dynamics!finite width}
Ultimately, what we care about is the
interacting
solution at the end of training, $t\to \infty$. As before, let's assume that the product of $\eta$ with the stochastic NTK is sufficiently small such that the free step-evolution operator,
\be\label{eq:step-evolution-operator-coverge}
\lim_{t\to\infty} \Unit(t) \propto \exp\!\le(- \eta \NTK t \ri) \, ,
\ee
exponentially decays to zero.\footnote{
    Note that since the \terminate{step-evolution operator} $U(t)$ is constructed in \eqref{eq:unitary-operator-at-least-in-continuum-limit-with-imaginary-time} from the \emph{step-independent} NTK\index{neural tangent kernel!step-independent}, $\NTK_{ij;\delta\tra}$, the condition for this convergence is the same as the free analysis discussed in footnote~\ref{footnote:convergence-dynamics}. %
}
Then, for training inputs, the interacting part of the network outputs, $\zI{i}{\tra}(t)$, will exponentially converge to zero
\be\label{eq:interacting-training-converge}
\lim_{t\to\infty}\zI{i}{\tra}(t) =0\, .
\ee
To see why 
this
holds, note that the dampening force \eqref{eq:damping-force-definition} decays exponentially as
\be
\lim_{s\to\infty} \force(s) \propto \exp\!\le(- \eta\NTK s \ri) \, ,
\ee
since its leading behavior is linearly proportional to the free residual training error $\eF{j}{\tra}(t)$.
Combined with \eqref{eq:step-evolution-operator-coverge}, this means that the interacting solution \eqref{eq:interacting-solution-training-formal} converges as
\begin{align}\label{eq}
\lim_{t\to\infty}\zI{i}{\tra}(t)&=\eta\lim_{t\to\infty}\sum_{s=0}^{t-1}\sum_{j,\tra_1} \Unit_{ij;\tra\tra_1}\!(t-1-s)\,\force_{j;\tra_1}\!(s) \notag \\
&\propto \lim_{ t\to\infty}\le\{  \eta t \,  \exp\!\le[- (t-1) \eta \NTK  \ri]\ri\}=0 \, ,
\end{align}
which is slightly slower than the convergence of the free solution $\zF{i}{\tra}(t) \propto \exp(-\NTK t)$.
Thus, overall the training algorithm converges:
\be\label{eq:gd-training-converge}
\lim_{t\to\infty} z_{i;\tra}(t) - \y{i}{\tra} =  \lim_{t\to\infty} \le[ \zF{i}{\tra}(t)+\zI{i}{\tra}(t)\ri]- \y{i}{\tra}  = 0\, ,
\ee
where here we also used the free solution \eqref{eq:free-training-converges}.\footnote{
    Remember that in this section we have stopped explicitly denoting that there are $\o{1/n^2}$ corrections. Recalling our fully-trained condition \eqref{eq:fully-trained-condition-really}, this result \eqref{eq:gd-training-converge} should be understood to be true up to such corrections, cf.~\eqref{eq:two-step-twice-reduced-error} for our two-step solution where the situation was analogous.
} 
Incidentally, and of possible broader interest, this altogether shows that gradient descent converges exponentially quickly to a zero-error minimum for realistic deep neural networks of finite width and nonzero depth, up to errors that are at most quadratic in our effective theory cutoff $L/n$.

\index{training dynamics!finite width}



For general inputs, the interacting part of the output, $\zI{i}{\delta}(t)$, is given by the expression \eqref{eq:interacting-solution-general}. With the convergence on the training set in mind \eqref{eq:interacting-training-converge}, the expression in the end-of-training limit reduces to
\be\label{eq:interacting-solution-in-limit}
\lim_{t\to\infty}\zI{i}{\delta}(t)=\le[\eta\sum_{s=0}^{\infty}\force_{i;\delta}(s)\ri]-\sum_{\tra_1,\tra_2}\NTKM_{\delta\tra_1}\NTKMsub^{\tra_1\tra_2}\le[\eta\sum_{s=0}^{\infty}\force_{i;\tra_2}(s)\ri]\, .
\ee
Thus, all that remains is for us to perform an infinite sum over the \terminate{damping force}:
\begin{align}\label{eq:sums-to-evaluate-to-win}
\eta\sum_{s=0}^{\infty}\force_{i;\delta}(s)\equiv&- \sum_{j,\tra_1} \Bigg[ \eta\sum_{s=0}^{\infty} \NTKM^{\text{I}}_{ij ;\delta\tra_1}(s)\,\eF{j}{\tra_1}(s) \Bigg]\, \\
&+\frac{\eta}{2} \sum_{j_1,j_2,\tra_1,\tra_2} \Bigg[\eta\sum_{s=0}^{\infty}\dNTKM_{i j_1j_2;\delta\tra_1\tra_2}(s)\,\eF{j_1}{\tra_1}(s)\,\eF{j_2}{\tra_2}(s)\Bigg]\, \notag \\
&-\frac{\eta^2}{6}\sum_{j_1,j_2,j_3,\tra_1,\tra_2,\tra_3}  \Bigg[\eta\sum_{s=0}^{\infty}\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}\,\eF{j_1}{\tra_1}(s)\,\eF{j_2}{\tra_2}(s)\, \eF{j_3}{\tra_3}(s) \Bigg]
\, . \notag
\end{align}
The third sum is exactly the end-of-training limit of the third dynamical helper function $c_{j_1j_2j_3;\tra_1\tra_2\tra_3}(t)$ that we already evaluated in \eqref{eq:c-end}, giving
\begin{align}\label{eq:ddNTK-time-dependent-solution-sum}
&\eta\sum_{s=0}^{\infty}\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}\,\eF{j_1}{\tra_1}(s)\,\eF{j_2}{\tra_2}(s) \, \eF{j_3}{\tra_3}(s)\, \\
=&\!\!\sum_{\tra_4,\tra_5,\tra_6}\!\!\!\!\!\tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}\geosumthree^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, .\notag
\end{align}
Thus, we are left with two more sums to evaluate. Let's proceed slowly: everything is simple, but there are a lot of terms to get right.

\index{training dynamics!finite width}
To start,  we can evaluate the second sum in \eqref{eq:sums-to-evaluate-to-win} as
\begin{align}\label{eq:dNTK-time-dependent-solution-sum-pre}
&\eta\sum_{s=0}^{\infty}\,\tia{\dNTK}{i j_1j_2}{\delta\tra_1\tra_2}(s)\, \eF{j_1}{\tra_1}(s)\, \eF{j_2}{\tra_2}(s)\, \\
=&\tia{\dNTK}{i j_1j_2}{\delta\tra_1\tra_2}\sum_{s=0}^{\infty} \bigg[ \eta \, \eF{j_1}{\tra_1}(s)\, \eF{j_2}{\tra_2}(s) \bigg] \, \notag \\
&-\!\!\!\sum_{k,\tra_3,\tra_4}\!\!\!\le(\tia{\ddNTK}{i j_1j_2k}{\delta\tra_1\tra_2\tra_3}+2\tia{\ddNTKII}{i j_1j_2k}{\delta\tra_1\tra_2\tra_3}\ri) \notag \, \\ 
&\qquad\qquad\times\NTKMsub^{\tra_3\tra_4}\sum_{s=0}^{\infty} \bigg\{ \eta\,\eF{j_1}{\tra_1}(s)\,\eF{j_2}{\tra_2}(s)\le[z_{k;\tra_4}-\y{k}{\tra_4}-\eF{k}{\tra_4}(s)\ri] \bigg\}\, ,\notag 
\end{align}
where we substituted in our dynamical dNTK\index{dynamical dNTK} solution, \eqref{eq:dNTK-time-dependent-solution} and used the fact that the overall expression is symmetric under $(\tra_1, j_1) \leftrightarrow (\tra_2, j_2)$ to combine the two $\ddNTKMII$ terms. Then, using our already evaluated sums,  \eqref{eq:b-end} and \eqref{eq:c-end}, we get
\begin{align}\label{eq:dNTK-time-dependent-solution-sum}
&\eta\sum_{s=0}^{\infty}\,\tia{\dNTK}{i j_1j_2}{\delta\tra_1\tra_2}(s)\, \eF{j_1}{\tra_1}(s)\, \eF{j_2}{\tra_2}(s)\, \\
=& \sum_{\tra_3,\tra_4 } \tia{\dNTK}{i j_1j_2}{\delta\tra_1\tra_2} \geosumtwo^{\tra_1\tra_2\tra_3\tra_4} \le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri) \notag\, \\
&-\!\!\!\sum_{k,\tra_3,\ldots,\tra_6}\!\!\!\le(\tia{\ddNTK}{i j_1j_2k}{\delta\tra_1\tra_2\tra_3}+2\tia{\ddNTKII}{i j_1j_2k}{\delta\tra_1\tra_2\tra_3}
\ri) \notag \, \\ 
&\qquad\qquad\times\dampsumDNTK^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{k;\tra_6}-\y{k}{\tra_6}\ri) \, . \notag
\end{align}
where we introduced a shorthand
\be\label{eq:damping-force-sum-dNTK}
\dampsumDNTK^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} \equiv \geosumtwo^{\tra_1\tra_2\tra_4\tra_5}\NTKMsub^{\tra_3\tra_6} -\sum_{\tra_7}\NTKMsub^{\tra_3\tra_7} \geosumthree^{\tra_1\tra_2\tra_7\tra_4\tra_5\tra_6}  \, ,
\ee
to ease the collection of terms later.


\index{training dynamics!finite width}
To finish,  let us write the first sum in \eqref{eq:sums-to-evaluate-to-win} as
\begin{align}\label{eq:interacting-NTK-solution-sum-pre}
&\eta\sum_{s=0}^{\infty}\, \NTKM^{\text{I}}_{ij;\delta\tra_1}(s)\,\eF{j}{\tra_1}(s)\, \\
=&-\sum_{k,\tra_2}\Big(\dNTK_{ijk;\delta\tra_1\tra_2}+\dNTK_{ji k;\tra_1\delta\tra_2}\Big)\sum_{s=0}^{\infty}\bigg[\eta\, \eF{j}{\tra_1}(s)\, a_{k;\tra_2}(s)\bigg] \notag \, \\
&+ \sum_{k_1,k_2,\tra_2,\tra_3,\tra_4}\Big(
\tia{\ddNTK}{ij k_1 k_2}{\delta\tra_1\tra_2\tra_3}+
\tia{\ddNTKII}{ij  k_1 k_2}{\delta\tra_1\tra_2\tra_3}+
\tia{\ddNTKII}{i k_1 j  k_2}{\delta\tra_2\tra_1\tra_3}
\, \notag \\
&\qquad\qquad\quad\ \ \ +
\tia{\ddNTK}{ji  k_1 k_2}{\tra_1\delta\tra_2\tra_3}+ 
\tia{\ddNTKII}{ji  k_1 k_2}{\tra_1\delta\tra_2\tra_3}+
\tia{\ddNTKII}{j k_1 i  k_2}{\tra_1\tra_2\delta\tra_3}
\Big)\, \notag \\
&\qquad\qquad\qquad\qquad\times\NTKMsub^{\tra_3\tra_4} \sum_{s=0}^{\infty}\bigg\{ \eta\, \eF{j}{\tra_1}(s)\Big[ 
\le( z_{k_2;\tra_4}-\y{k_2}{\tra_4} \ri) a_{k_1;\tra_2}(s) 
-
 b_{k_1k_2;\tra_2\tra_4}(s)
\Big]\bigg\}
\, \notag \\
&+\frac{\eta}{2}\sum_{k_1,k_2,\tra_2,\tra_3}\!\!\!\le(\tia{\ddNTK}{ij k_1k_2}{\delta\tra_1\tra_2\tra_3}+\tia{\ddNTK}{ji k_1k_2}{\tra_1\delta\tra_2\tra_3} + 2\tia{\ddNTKII}{ij k_1k_2}{\delta\tra_1\tra_2\tra_3} \ri) \, \notag\\
&\qquad\qquad\qquad\qquad\times \sum_{s=0}^{\infty} \bigg[ \eta\, \eF{j}{\tra_1}(s)\, b_{k_1k_2;\tra_2\tra_3}(s) \bigg] \, ,\notag 
\end{align}
where we substituted in our solution for the \terminate{interaction NTK}, \eqref{eq:interacting-NTK-solution}. Then, substituting for the helper functions with \eqref{eq:dynamical-helper-a} and \eqref{eq:dynamical-helper-b}, performing the \emph{additional} sums over these terms, and then using the end-of-training limits
 \eqref{eq:a-end}--\eqref{eq:c-end}, we get
\begin{align}\label{eq:interacting-NTK-solution-sum}
&\eta\sum_{s=0}^{\infty}\, \NTKM^{\text{I}}_{ij;\delta\tra_1}(s)\eF{j}{\tra_1}(s)\, \\
=&-\sum_{k,\tra_2,\tra_3,\tra_4}\Big(\dNTK_{ijk;\delta\tra_1\tra_2}+\dNTK_{ji k;\tra_1\delta\tra_2}\Big)\dampsumNTKminus^{\tra_1\tra_2\tra_3\tra_4}
\le(z_{j;\tra_3}-\y{j}{\tra_3}\ri)\le(z_{k;\tra_4}-\y{k}{\tra_4}\ri)\, \notag\\
&+ \sum_{k_1,k_2,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6}\Big(
\tia{\ddNTK}{ij k_1 k_2}{\delta\tra_1\tra_2\tra_3}+
\tia{\ddNTKII}{ij  k_1 k_2}{\delta\tra_1\tra_2\tra_3}+
\tia{\ddNTKII}{i k_1 j  k_2}{\delta\tra_2\tra_1\tra_3}
\, \notag \\
&\qquad\qquad\qquad\quad\ \ \ +
\tia{\ddNTK}{ji  k_1 k_2}{\tra_1\delta\tra_2\tra_3}+ 
\tia{\ddNTKII}{ji  k_1 k_2}{\tra_1\delta\tra_2\tra_3}+
\tia{\ddNTKII}{j k_1 i  k_2}{\tra_1\tra_2\delta\tra_3}
\Big) \, \notag \\
&\qquad\qquad\qquad\qquad\times\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j;\tra_4}-\y{j}{\tra_4}\ri) \le(z_{k_1;\tra_5}-\y{k_1}{\tra_5}\ri)\le(z_{k_2;\tra_6}-\y{k_2}{\tra_6}\ri) \,  \notag\\
&+\frac{\eta}{2}\sum_{k_1,k_2,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6}\!\!\!\le(\tia{\ddNTK}{ij k_1k_2}{\delta\tra_1\tra_2\tra_3}+\tia{\ddNTK}{ji k_1k_2}{\tra_1\delta\tra_2\tra_3} + 2\tia{\ddNTKII}{ij k_1k_2}{\delta\tra_1\tra_2\tra_3} \ri) \, \notag\\
&\qquad\qquad\qquad\qquad\times\dampsumNTKtwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j;\tra_4}-\y{j}{\tra_4}\ri)\le(z_{k_1;\tra_5}-\y{k_1}{\tra_5}\ri)\le(z_{k_2;\tra_6}-\y{k_2}{\tra_6}\ri) \, , \notag
\end{align}
where we introduced additional shorthands
\begin{align}
\dampsumNTKminus^{\tra_1\tra_2\tra_3\tra_4} \equiv &\NTKMsub^{\tra_1\tra_3}\NTKMsub^{\tra_2\tra_4} -\sum_{\tra_5}\NTKMsub^{\tra_2\tra_5} \geosumtwo^{\tra_1\tra_5\tra_3\tra_4}  \, ,\label{eq:damping-force-sum-NTK-minus}\, \\
\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} \equiv& \NTKMsub^{\tra_1\tra_4} \NTKMsub^{\tra_2\tra_5}\NTKMsub^{\tra_3\tra_6} - \sum_{\tra_7} \NTKMsub^{\tra_2\tra_7} \geosumtwo^{\tra_1\tra_7\tra_4\tra_5} \NTKMsub^{\tra_3\tra_6} \,  \label{eq:damping-force-sum-NTK-one}\\ 
&-\sum_{\tra_7} \NTKMsub^{\tra_1\tra_4}\NTKMsub^{\tra_3\tra_7}\geosumtwo^{\tra_2\tra_7\tra_5\tra_6}  + \sum_{\tra_7,\tra_8,\tra_9}\NTKMsub^{\tra_3\tra_9}\geosumtwo^{\tra_2\tra_9\tra_7\tra_8}\geosumthree^{\tra_1\tra_7\tra_8\tra_4\tra_5\tra_6} \,, \notag \\
\dampsumNTKtwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} \equiv&  \NTKMsub^{\tra_1\tra_4} \geosumtwo^{\tra_2\tra_3\tra_5\tra_6} - \sum_{\tra_7,\tra_8}\geosumtwo^{\tra_2\tra_3\tra_7\tra_8}\geosumthree^{\tra_1\tra_7\tra_8\tra_4\tra_5\tra_6}\, . \label{eq:damping-force-sum-NTK-two}
\end{align}








\index{training dynamics!finite width}
Now, it's time to frantically flip through pages and collect everything we computed.
Plugging the sums \eqref{eq:interacting-NTK-solution-sum}, \eqref{eq:dNTK-time-dependent-solution-sum}, and \eqref{eq:ddNTK-time-dependent-solution-sum} back into our expression for the sum over the \terminate{damping force}, \eqref{eq:sums-to-evaluate-to-win}, plugging that back into our expression for the interaction part of the network output \eqref{eq:interacting-solution-in-limit}, and then combining with the free part of the network output, \eqref{eq:free-solution-general-end-of-time},
we obtain our fully-trained solution for finite-width networks trained by gradient descent:\index{training dynamics!finite width}
\begin{align}\label{eq:very-general-finite-width-solution}
&z_{i;\delta}(t=\infty)\, \\
\equiv&\zF{i}{\delta}(t=\infty)+\zI{i}{\delta}(t=\infty)\, \notag \\
=&z_{i;\delta}-\sum_{j,k,\tra_1,\tra_2}\NTK_{ij;\delta\tra_1}\le(\NTK^{-1}\ri)_{jk}^{\tra_1\tra_2}\le(z_{k;\tra_2}-\y{k}{\tra_2}\ri)\, \notag\\
&+\sum_{j_1,j_2,\tra_1,\tra_2,\tra_3,\tra_4}\le[\dNTK_{j_1 i j_2;\tra_1\delta\tra_2}-\sum_{\tra_5,\tra_6}\NTKM_{\delta\tra_5}\NTKMsub^{\tra_5\tra_6}\dNTK_{j_1 i j_2;\tra_1\tra_6\tra_2}\ri]\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algodNTKone^{\tra_1\tra_2\tra_3\tra_4}\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri)\, \notag \\
&+\sum_{j_1,j_2,\tra_1,\tra_2,\tra_3,\tra_4}\le[\dNTK_{i j_1j_2;\delta\tra_1\tra_2}-\sum_{\tra_5,\tra_6}\NTKM_{\delta\tra_5}\NTKMsub^{\tra_5\tra_6}\dNTK_{i j_1j_2;\tra_6\tra_1\tra_2}\ri]\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri)\, \notag\\
&+\sum_{\substack{ j_1,j_2,j_3, \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6} }\!\! \le[ \tia{\ddNTK}{j_1i j_2j_3}{\tra_1\delta\tra_2\tra_3}-\sum_{\tra_7,\tra_8}\NTKM_{\delta\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTK}{j_1ij_2j_3}{\tra_1\tra_8\tra_2\tra_3}\ri] 
\notag \, \\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algoddNTKIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, \notag \\
&+\sum_{\substack{ j_1,j_2,j_3, \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6} }\!\! \le[ \tia{\ddNTK}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}-\sum_{\tra_7,\tra_8}\NTKM_{\delta\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTK}{i j_1j_2j_3}{\tra_8\tra_1\tra_2\tra_3}\ri] 
\notag \, \\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algoddNTKItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, \notag \\
&+\sum_{\substack{ j_1,j_2,j_3, \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6} }\!\! \le[ \tia{\ddNTKII}{j_1 j_2ij_3}{\tra_1\tra_2\delta\tra_3}-\sum_{\tra_7,\tra_8}\NTKM_{\delta\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTKII}{j_1j_2ij_3}{\tra_1\tra_2\tra_8\tra_3}\ri] 
\notag \, \\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algoddNTKIIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, \notag \\
&+\sum_{\substack{ j_1,j_2,j_3, \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6} }\!\! \le[ \tia{\ddNTKII}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}-\sum_{\tra_7,\tra_8}\NTKM_{\delta\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTKII}{i j_1j_2j_3}{\tra_8\tra_1\tra_2\tra_3}\ri] 
\notag \, \\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algoddNTKIItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, \notag \\
&+\o{\frac{1}{n^2}} \, \notag .
\end{align}
Here we defined our final tensors with our final alphabet letter (and various
subscripts),
\begin{align}
\algodNTKone^{\tra_1\tra_2\tra_3\tra_4}\equiv&\dampsumNTKminus^{\tra_1\tra_2\tra_3\tra_4}\, ,\label{eq:implicit-ZA-tensor} \\
\algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\equiv&
\dampsumNTKminus^{\tra_1\tra_2\tra_3\tra_4}
+\frac{\eta}{2}\geosumtwo^{\tra_1\tra_2\tra_3\tra_4}\, ,\label{eq:implicit-ZB-tensor}\\
\algoddNTKIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\equiv&-\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} -\frac{\eta}{2} \dampsumNTKtwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}
\, ,\label{eq:implicit-ZA-tensor-I}\\
\algoddNTKItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\equiv&-\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}- \frac{\eta}{2}\dampsumNTKtwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\, \notag\\
&- \frac{\eta}{2}\dampsumDNTK^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} - \frac{\eta^2}{6}\geosumthree^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}
\, ,\label{eq:implicit-ZB-tensor-I}\\
\algoddNTKIIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\equiv&-\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}
\, ,\label{eq:implicit-ZA-tensor-II}\\
\algoddNTKIItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\equiv&-\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}-\dampsumNTKone^{\tra_2\tra_1\tra_3\tra_5\tra_4\tra_6}-\dampsumNTKone^{\tra_1\tra_3\tra_2\tra_4\tra_6\tra_5}\, \notag \\
&-\eta\dampsumNTKtwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} - \eta \dampsumDNTK^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}
\,, \label{eq:implicit-ZB-tensor-II}
\end{align}
making use of our previous\index{training dynamics!finite width} shorthand tensors \eqref{eq:damping-force-sum-dNTK} and \eqref{eq:damping-force-sum-NTK-minus}--\eqref{eq:damping-force-sum-NTK-two}.\footnote{Note that for the last of these tensors, \eqref{eq:implicit-ZB-tensor-II}, in order to coax the various contributions in our solution \eqref{eq:very-general-finite-width-solution} into the proper form, we used the symmetry of $\tia{\ddNTKII}{i j_1j_2j_3}{\delta\tra_1\tra_2\tra_3}$ and relabeled various dummy sample indices.}
These \textbf{algorithm projectors}\index{algorithm projector|textbf}\index{training dynamics!finite width}, \eqref{eq:implicit-ZA-tensor}--\eqref{eq:implicit-ZB-tensor-II}, serve to project the initial training error onto two different combinations of the dNTK, two different combinations of the first ddNTK, and two other different combinations of the second ddNTK, all according to the details of the gradient descent algorithm. As a final sanity check, note that as for inputs in the training set, $\delta \to \tra \in \A$, the quantities in the square brackets in the finite-width solution  \eqref{eq:very-general-finite-width-solution} each vanish, and we recover our fully-trained condition \eqref{eq:fully-trained-condition-really}.




Before we retire this subsection, let us elaborate on the algorithm dependence. First, note that our two-update solution \eqref{eq:finite-width-network-output-general-data} has the same form as the gradient-descent solution \eqref{eq:very-general-finite-width-solution}, but with different algorithm projectors:
\begin{align}\label{eq:ZB-two-step}
\algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\equiv \frac{1}{2}\NTKMsub^{\tra_1\tra_3}\NTKMsub^{\tra_2\tra_4}\, ,\qquad \algoddNTKItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\equiv-\frac{1}{6}\NTKMsub^{\tra_1\tra_4}\NTKMsub^{\tra_2\tra_5}\NTKMsub^{\tra_3\tra_6} \, , 
\end{align}
and all the others vanishing.\index{training dynamics!finite width}
Clearly these algorithms have very different inductive biases\index{inductive bias!of learning algorithms}!
Second, we can study the ODE limit\index{gradient descent!continuum or ODE limit} of the dynamics by taking $\eta\to0$, cf.~footnote~\ref{eq:footnote-continuum-limit}: in this case,  we see that the ODE dynamics have a solution given by
\begin{align}\label{eq:ODE-YZ}
&\algodNTKone^{\tra_1\tra_2\tra_3\tra_4}=\algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\equiv\dampsumNTKminus^{\tra_1\tra_2\tra_3\tra_4}\, , \\
\label{eq:ODE-YZ-ddNTK-1}
&\algoddNTKIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} = \algoddNTKItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}= \algoddNTKIIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} \equiv -\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6} \, \\
&\algoddNTKIItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\equiv-\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}-\dampsumNTKone^{\tra_2\tra_1\tra_3\tra_5\tra_4\tra_6}-\dampsumNTKone^{\tra_1\tra_3\tra_2\tra_4\tra_6\tra_5}
 \, ,\label{eq:ODE-YZ-ddNTK-II-2}
\end{align}
where $\dampsumNTKminus^{\tra_1\tra_2\tra_3\tra_4} $ and $\dampsumNTKone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}$ are given by \eqref{eq:damping-force-sum-NTK-minus} and \eqref{eq:damping-force-sum-NTK-one}, respectively, and the inverting tensor $\geosumtwo^{\tra_1\tra_2\tra_3\tra_4}$, \eqref{eq:implicit-X-tensor}, now satisfies
\be\label{eq:ODE-X}
\sum_{\tra_3,\tra_4\in\A}\geosumtwo^{\tra_1\tra_2\tra_3\tra_4}\le(\NTKMsub_{\tra_3\tra_5}\delta_{\tra_4\tra_6}+\delta_{\tra_3\tra_5}\NTKMsub_{\tra_4\tra_6}\ri)=\delta^{\tra_1}_{\ \tra_5}\delta^{\tra_2}_{\ \tra_6}\, ,
\ee
and the other inverting tensor $\geosumtwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}$, \eqref{eq:implicit-X3-tensor}, now satisfies\index{training dynamics!finite width}
\begin{align}\label{eq:ODE-X-III}
&\delta^{\tra_1}_{\ \tra_7}\delta^{\tra_2}_{\ \tra_8}\delta^{\tra_3}_{\ \tra_9}\,  \\
 =&\sum_{\tra_4,\tra_5,\tra_6}\geosumthree^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}  \Big(\NTKMsub_{\tra_4\tra_7}\delta_{\tra_5\tra_8}\delta_{\tra_6\tra_9} +\delta_{\tra_4\tra_7}\NTKMsub_{\tra_5\tra_8}\delta_{\tra_6\tra_9} +  \delta_{\tra_4\tra_7}\delta_{\tra_5\tra_8}\NTKMsub_{\tra_6\tra_9} \Big)\notag \,  \, . 
\end{align}
This entirely captures the difference between gradient flow and gradient descent for these fully-trained networks.
In general, we conjecture that for finite-width networks, at leading order the fully-trained solution takes the universal\index{universality!of the fully-trained network solution} form of \eqref{eq:very-general-finite-width-solution}, with all of the \neo{algorithm dependence} encoded by the  six \emph{algorithm projectors}\index{algorithm projector}: $\algodNTKone^{\tra_1\tra_2\tra_3\tra_4}$, $\algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}$, $\algoddNTKIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}$, $\algoddNTKItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}$, $\algoddNTKIIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}$, and $\algoddNTKIItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}$.\footnote{
More precisely, we conjecture that our conjecture, as stated, holds for the MSE loss\index{loss!algorithm dependence at finite width}. For the cross-entropy loss\index{loss!algorithm dependence at finite width} the solution will take a slightly different form, but with a similar partition into an algorithm-independent part and an algorithm-dependent part described by similar algorithm projectors.
}
\index{training dynamics!finite width}




Finally, let's understand what is meant by the remaining error in our solution \eqref{eq:very-general-finite-width-solution}. It is not really the error of an actual network that is instantiated and then fully trained through many many gradient-descent steps, but instead it is the error in our effective description of such a particular fully-trained network. Of course, our \terminate{effective theory} formalism can compute higher-order corrections, if they're of interest. However, the leading-order finite-width corrections should really be sufficient for most cases: for instance, for a network of depth $L=10$ layers and of hidden-layer width $n=100$ neurons each, our effective description will only be off by $\sim(L/n)^2= 1\%$.

\index{training dynamics!finite width}
Furthermore, for any theoretical analysis, the main qualitative difference in the solution appears when going from infinite width to finite width, as we go from a free theory to an interacting theory and from linear dynamics to nonlinear dynamics. Thus, the effective theory that gave us the solution \eqref{eq:very-general-finite-width-solution} really is ``as simple \ldots as possible'' while still providing an extremely accurate description of real deep learning models.

































\subsection{Prediction at Finite Width}\label{subsec:prediction-at-finite-width}


Having solved the training dynamics in two different ways, we can now rather generally study the predictions of our networks on novel inputs $x_{\tea}$ from the test set $\tea \in \B$. 

\index{finite-width prediction!gradient descent}\index{finite-width prediction!gradient descent|seealso{T-shirt equation}}\index{universality!of the fully-trained network solution}
At finite width, the predictions of a fully-trained network are universally governed by the stochastic equation
\begin{align}\label{eq:very-general-finite-width-solution-DONT-CHANGE}
&z_{i;\tea}(t=T)\, \\
=&z_{i;\tea}-\sum_{\tra_1,\tra_2}\NTKM_{\tea\tra_1}\NTKMsub^{\tra_1\tra_2}\!\le(z_{i;\tra_2}-\y{i}{\tra_2}\ri)\,  \notag\\
&+\sum_{j, \tra_1,\tra_2}\le[\DNTKS_{ij;\tea\tra_1}-\sum_{\tra_3,\tra_4\in\A}\NTKM_{\tea\tra_3}\NTKMsub^{\tra_3\tra_4}\DNTKS_{ij;\tra_4\tra_1}\ri]\NTKMsub^{\tra_1\tra_2}\!\le(z_{j;\tra_2}-\y{j}{\tra_2}\ri)\, \notag\\
&-\sum_{\substack{j,k\\ \tra_1,\ldots,\tra_4}}\le[\DNTKS_{ij;\tea\tra_1}-\sum_{\tra_5,\tra_6}\NTKM_{\tea\tra_5}\NTKMsub^{\tra_5\tra_6}\DNTKS_{ij;\tra_6\tra_1}\ri]\NTKMsub^{\tra_1\tra_2}\DNTKS_{jk;\tra_2\tra_3}\NTKMsub^{\tra_3\tra_4}\!\le(z_{k;\tra_4}-\y{k}{\tra_4}\ri)\, \notag\\
&+\sum_{j_1,j_2,\tra_1,\tra_2,\tra_3,\tra_4}\le[\dNTK_{j_1 i j_2;\tra_1\tea\tra_2}-\sum_{\tra_5,\tra_6}\NTKM_{\tea\tra_5}\NTKMsub^{\tra_5\tra_6}\dNTK_{j_1 i j_2;\tra_1\tra_6\tra_2}\ri]\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algodNTKone^{\tra_1\tra_2\tra_3\tra_4}\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri)\, \notag \\
&+\sum_{j_1,j_2,\tra_1,\tra_2,\tra_3,\tra_4}\le[\dNTK_{i j_1j_2;\tea\tra_1\tra_2}-\sum_{\tra_5,\tra_6}\NTKM_{\tea\tra_5}\NTKMsub^{\tra_5\tra_6}\dNTK_{i j_1j_2;\tra_6\tra_1\tra_2}\ri]\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\le(z_{j_1;\tra_3}-\y{j_1}{\tra_3}\ri)\le(z_{j_2;\tra_4}-\y{j_2}{\tra_4}\ri)\, \notag\\
&+\sum_{\substack{ j_1,j_2,j_3, \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6} }\!\! \le[ \tia{\ddNTK}{j_1i j_2j_3}{\tra_1\tea\tra_2\tra_3}-\sum_{\tra_7,\tra_8}\NTKM_{\tea\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTK}{j_1ij_2j_3}{\tra_1\tra_8\tra_2\tra_3}\ri] 
\notag \, \\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algoddNTKIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, \notag \\
&+\sum_{\substack{ j_1,j_2,j_3, \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6} }\!\! \le[ \tia{\ddNTK}{i j_1j_2j_3}{\tea\tra_1\tra_2\tra_3}-\sum_{\tra_7,\tra_8}\NTKM_{\tea\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTK}{i j_1j_2j_3}{\tra_8\tra_1\tra_2\tra_3}\ri] 
\notag \, \\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algoddNTKItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, \notag \\
&+\sum_{\substack{ j_1,j_2,j_3, \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6} }\!\! \le[ \tia{\ddNTKII}{j_1 j_2ij_3}{\tra_1\tra_2\tea\tra_3}-\sum_{\tra_7,\tra_8}\NTKM_{\tea\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTKII}{j_1j_2ij_3}{\tra_1\tra_2\tra_8\tra_3}\ri] 
\notag \, \\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algoddNTKIIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, \notag \\
&+\sum_{\substack{ j_1,j_2,j_3, \\ \tra_1,\tra_2,\tra_3,\tra_4,\tra_5,\tra_6} }\!\! \le[ \tia{\ddNTKII}{i j_1j_2j_3}{\tea\tra_1\tra_2\tra_3}-\sum_{\tra_7,\tra_8}\NTKM_{\tea\tra_7}\NTKMsub^{\tra_7\tra_8}\tia{\ddNTKII}{i j_1j_2j_3}{\tra_8\tra_1\tra_2\tra_3}\ri] 
\notag \, \\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\times \algoddNTKIItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}\le(z_{j_1;\tra_4}-\y{j_1}{\tra_4}\ri)\le(z_{j_2;\tra_5}-\y{j_2}{\tra_5}\ri)\le(z_{j_3;\tra_6}-\y{j_3}{\tra_6}\ri)\, \notag \\
&+\o{\frac{1}{n^2}} \, \notag .\end{align}
This formula could boarder-line be fit on a T-shirt\index{T-shirt equation}.\footnote{
To better facilitate such \terminate{brand awareness}, first recall that for \terminate{nearly-kernel methods} 
we were able to compress the model predictions in terms of a \emph{trained kernel} \eqref{eqtrained-kernel-prediction}\index{nearly-kernel methods!trained kernel}; similarly, we can compress the predictions of finite-width networks \eqref{eq:very-general-finite-width-solution-DONT-CHANGE} in terms of a \textbf{trained NTK}\index{trained NTK|see{neural tangent kernel}}\index{neural tangent kernel!trained|textbf}\index{neural tangent kernel!trained|seealso{trained kernel}}, $\NTKMA_{ij;\delta \tra}$, as
\begin{align}\label{eq:trained-NTK-prediction}
z_{i;\tea}(t=T) =z_{i;\tea} - \sum_{j,k,\tra_1,\tra_2}\NTKMA_{ij;\tea\tra_1}\NTKMAsub_{jk}^{\tra_1\tra_2}\le(z_{k;\tra_2}-\y{k}{\tra_2}\ri)+\o{\frac{1}{n^2}}\, ,
\end{align}
taking the form of a \emph{(neural tangent) kernel prediction}\index{kernel methods!prediction} \eqref{eq:kernel-prediction}.
To see how this works,
let us decompose the trained NTK into free and training-dependent terms:
\be\label{eq:trained-NTK}
\NTKMA_{ij;\delta \tra} \equiv \NTK_{ij;\delta \tra} + \NTKdd_{ij;\delta \tra}\, .
\ee
Please don't confuse this decomposition with our earlier decomposition \eqref{eq:free-interaction-decomposition-for-NTK}: that former one was convenient for solving the training dynamics, while this new one is useful for determining the trained NTK in \eqref{eq:trained-NTK-prediction}.
Considering the inverse of the trained NTK restricted to the training set only,
\be\label{eq:trained-NTK-inverse}
\NTKMAsub_{jk}^{\tra_1\tra_2} \equiv \le(\NTK^{-1}\ri)_{jk}^{\tra_1\tra_2} - \sum_{\tra_3, \tra_4} \NTKdd_{jk;\tra_3 \tra_4}\NTKMsub^{\tra_1\tra_3}\NTKMsub^{\tra_2\tra_4} + \o{\frac{1}{n^2}} \,,
\ee
and plugging it along with the decomposition \eqref{eq:trained-NTK}
into the formula \eqref{eq:trained-NTK-prediction}, we get
\begin{align}\label{eq:trained-NTK-prediction-expanded}
z_{i;\tea}(t=T) =&z_{i;\tea}-\sum_{j,k,\tra_1,\tra_2}\NTK_{ij;\tea\tra_1}\le(\NTK^{-1}\ri)_{jk}^{\tra_1\tra_2}\le(z_{k;\tra_2}-\y{k}{\tra_2}\ri)\, \\
&-\sum_{j,\tra_1,\tra_2}\le[\NTKdd_{ij;\tea \tra_1}-\sum_{\tra_3,\tra_4}\NTKM_{\tea\tra_3}\NTKMsub^{\tra_3\tra_4}\NTKdd_{ij;\tra_4 \tra_1}\ri]\NTKMsub^{\tra_1\tra_2}(z_{j;\tra_2}-\y{j}{\tra_2}) + \o{\frac{1}{n^2}}\, .\notag
\end{align}
The terms on the first line of the right-hand side of this expression give the free
contribution to the solution,
\eqref{eq:free-solution-general-end-of-time}, while the terms on the second line give the interacting contribution, \eqref{eq:interacting-solution-in-limit}, encapsulating the effect of nontrivial representation learning at finite width. 
The specific form of $\NTKMA_{ij;\delta \tra}$ can be found by matching the terms on the right-hand sides of \eqref{eq:trained-NTK-prediction-expanded} and \eqref{eq:very-general-finite-width-solution-DONT-CHANGE}. You can also express the training-dependent part, $\NTKdd_{ij;\delta \tra}$, implicitly in terms of the \terminate{damping force} \eqref{eq:damping-force-definition} as
\be\label{eq:ntk-data-dependent-in-terms-of-force}
\eta\sum_{s=0}^{\infty}\force_{i;\delta}(s)= -\sum_{j, \tra_1, \tra_2} \NTKdd_{ij;\delta \tra_1} \, \NTKMsub^{\tra_1\tra_2}\le(z_{j;\tra_2}-\y{j}{\tra_2}\ri) \,,
\ee
as is clear by comparing \eqref{eq:trained-NTK-prediction-expanded} with \eqref{eq:interacting-solution-in-limit}. This implicit expression also makes it clear that the form of $\NTKdd_{ij;\delta \tra}$ isn't unique and can always be adjusted by the addition of a term orthogonal to $\sum_{\tra_2}\NTKMsub^{\tra_1\tra_2}\le(z_{j;\tra_2}-\y{j}{\tra_2}\ri)$. In any event, with the trained NTK you can now fit the finite-width prediction formula \eqref{eq:trained-NTK-prediction} on any newborn AI's onesie.
}
For this expression, we've expanded the complete inverse of the stochastic NTK sub-tensor as per \eqref{eq:newton-step-generalized-first}; in particular, the second line is more or less the infinite-width kernel prediction \eqref{eq:kernel-prediction}, and the terms on the third and fourth lines are finite-width corrections due to NTK fluctuations. 
Further, the algorithm projectors\index{algorithm projector} \eqref{eq:implicit-ZA-tensor}--\eqref{eq:implicit-ZB-tensor-II}
contain all the finite-width \terminate{algorithm dependence} of the solution, and thus this general solution \eqref{eq:very-general-finite-width-solution-DONT-CHANGE} can describe both our explicit solutions,
whether we train in two steps \eqref{eq:finite-width-network-output-general-data}, in many many steps \eqref{eq:very-general-finite-width-solution}, or with any other choice of optimization algorithm that uses the MSE loss.

Furthermore, this solution \eqref{eq:very-general-finite-width-solution-DONT-CHANGE} describes the predictions of a \emph{particular} network from our ensemble:
 the instantiation-to-instantiation difference is encoded in the particular initial network output,
 $z$, the NTK fluctuation, $\DNTKS$, the dNTK, $\dNTK$, the first ddNTK, $\ddNTK$, and the second ddNTK, $\ddNTKII$.
Since we know explicitly the statistics of these variables at initialization, we can also analyze the statistics of the fully-trained distribution in full.
With an eye towards a discussion of the depth dependence of these statistics, we'll now revive the layer indices.

First and foremost, the mean prediction is given by
\begin{align}\label{eq:very-general-finite-width-solution-mean-2}
m_{i;\tea}\equiv&\E{z^{(L)}_{i;\tea}(T)}\, \\
=&m_{i;\tea}^{\text{NTK}}+\frac{1}{n_{L-1}}\le(m_{i;\tea}^{\Delta\text{NTK}}+m_{i;\tea}^{\text{dNTK}}+m_{i;\tea}^{\text{ddNTK-I}}+m_{i;\tea}^{\text{ddNTK-II}}\ri)\, \notag\\
&-\frac{1}{n_{L-1}}\sum_{\tra_1,\tra_2}\NTKM^{(L)}_{\tea\tra_1}\NTKMsub_{(L)}^{\tra_1\tra_2}\le(m_{i;\tra_2}^{\Delta\text{NTK}}+m_{i;\tra_2}^{\text{dNTK}}+m_{i;\tra_2}^{\text{ddNTK-I}}+m_{i;\tra_2}^{\text{ddNTK-II}}\ri)+\o{\frac{1}{n^2}}\, ,\notag
\end{align}
where the first term is the (neural tangent) kernel prediction
\be\label{eq:infinite-width-ish-contribution-to-the-bias}
m_{i;\tea}^{\text{NTK}}\equiv\sum_{\tra_1,\tra_2}\NTKM^{(L)}_{\tea\tra_1}\NTKMsub_{(L)}^{\tra_1\tra_2}\y{i}{\tra_2}\, ,
\ee
and the four other kinds of terms come from the leading-order finite-width correction. Specifically, \emph{(i)} the fluctuation of the NTK gives
\be\label{eq:mean-prediction-DNTK-contribution}
m_{i;\delta}^{\Delta\text{NTK}}\equiv\sum_{\tra_1,\ldots,\tra_4}\le(\NTHA{\delta\tra_1}{\tra_2\tra_3}{L}+\NTHB{\delta\tra_2\tra_1\tra_3}{L}+n_{L}\NTHB{\delta\tra_3\tra_1\tra_2}{L}\ri)\NTKMsub_{(L)}^{\tra_1\tra_2}\NTKMsub_{(L)}^{\tra_3\tra_4} \y{i}{\tra_4}\, \,
\ee
where we used \eqref{eq:NTH-variance-decomposition} to evaluate the NTK variance in terms of our decomposition\index{tensor decomposition!NTK variance $A$/$B$} into tensors $A^{(L)}$ and $B^{(L)}$; \emph{(ii)} the dNTK gives
\begin{align}\label{eq:mean-prediction-dNTK-contribution}
m_{i;\delta}^{\text{dNTK}}\equiv&-\sum_{\tra_1,\ldots,\tra_4}\Bigg[2\le(\dNTKP{\delta\tra_1\tra_2\tra_3}{L}+\dNTKQ{\delta\tra_1\tra_2\tra_3}{L}+n_{L}\dNTKQ{\delta\tra_2\tra_1\tra_3}{L}\ri)\algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\, \\
&\quad\quad\quad\quad\ \ +\Big(n_{L}\dNTKP{\tra_1\delta\tra_2\tra_3}{L}+\dNTKQ{\tra_1\delta\tra_2\tra_3}{L}+\dNTKQ{\tra_1\tra_2\delta\tra_3}{L}\Big)\algodNTKone^{\tra_1\tra_2\tra_3\tra_4}\, \notag\\
&\quad\quad\quad\quad\ \ +\Big(\dNTKP{\tra_1\delta\tra_2\tra_3}{L}+n_{L}\dNTKQ{\tra_1\delta\tra_2\tra_3}{L}+\dNTKQ{\tra_1\tra_2\delta\tra_3}{L}\Big)\algodNTKone^{\tra_1\tra_2\tra_4\tra_3}\Bigg] \y{i}{\tra_4}\, ,\notag
\end{align}
where we used \eqref{eq:cross-dNTK-general-leading} to evaluate the dNTK-preactivation cross correlators in terms of our decomposition into tensors $P^{(L)}$ and $Q^{(L)}$; \emph{(iii)} the first ddNTK gives\index{tensor decomposition!dNTK-preactivation $P$/$Q$}
\begin{align}\label{eq:mean-prediction-ddNTK-I-contribution}
&m_{i;\delta}^{\text{ddNTK-I}}\, \\
\equiv&-\sum_{\tra_1,\ldots,\tra_6}\Big[\ddNTKR{\delta\tra_1\tra_2\tra_3}{L}\Big(\algoddNTKItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}+\algoddNTKItwo^{\tra_2\tra_3\tra_1\tra_5\tra_6\tra_4}+\algoddNTKItwo^{\tra_3\tra_1\tra_2\tra_6\tra_4\tra_5}\Big)\, \notag\\
&\qquad\quad\ \ +\ddNTKR{\tra_1\delta\tra_2\tra_3}{L}\algoddNTKIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}+\ddNTKR{\tra_1\tra_2\tra_3\delta}{L}\algoddNTKIone^{\tra_1\tra_2\tra_3\tra_5\tra_6\tra_4}+\ddNTKR{\tra_1\tra_3\delta\tra_2}{L}\algoddNTKIone^{\tra_1\tra_2\tra_3\tra_6\tra_4\tra_5}\Big] \notag\\
&\qquad\qquad\qquad\qquad\qquad\ \times\Big[\y{i}{\tra_4}\Big(\sum_j\y{j}{\tra_5}\y{j}{\tra_6}+n_L \Ti{\ker}{\tra_5\tra_6}{L}\Big)+\y{i}{\tra_5}\Ti{\ker}{\tra_6\tra_4}{L}+\y{i}{\tra_6}\Ti{\ker}{\tra_4\tra_5}{L}\Big]\, \notag
\end{align}
where\index{tensor decomposition!ddNTKs $R$/$S$/$T$/$U$} we used \eqref{eq:decomposition-ddNTK} to evaluate the first ddNTK mean in terms of decomposition into the tensor $\ddNTKRS^{(L)}$; and \emph{(iv)} the second ddNTK gives
\begin{align}\label{eq:mean-prediction-ddNTK-II-contribution}
&m_{i;\delta}^{\text{ddNTK-II}}\, \\
\equiv&-\sum_{\tra_1,\ldots,\tra_6}\Big[\ddNTKS{\delta\tra_1\tra_2\tra_3}{L}\algoddNTKIItwo^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}+\ddNTKT{\delta\tra_1\tra_2\tra_3}{L}\algoddNTKIItwo^{\tra_2\tra_3\tra_1\tra_5\tra_6\tra_4}+\ddNTKU{\delta\tra_1\tra_2\tra_3}{L}\algoddNTKIItwo^{\tra_3\tra_1\tra_2\tra_6\tra_4\tra_5}\, \notag\\
&\qquad\quad\ \ +\ddNTKS{\tra_1\tra_2\delta\tra_3}{L}\algoddNTKIIone^{\tra_1\tra_2\tra_3\tra_5\tra_6\tra_4}+\ddNTKT{\tra_1\delta\tra_3\tra_2}{L}\algoddNTKIIone^{\tra_1\tra_2\tra_3\tra_4\tra_5\tra_6}+\ddNTKU{\tra_1\tra_3\tra_2\delta}{L}\algoddNTKIIone^{\tra_1\tra_2\tra_3\tra_6\tra_4\tra_5}\Big] \notag\\
&\qquad\qquad\qquad\qquad\qquad\ \times\Big[\y{i}{\tra_4}\Big(\sum_j\y{j}{\tra_5}\y{j}{\tra_6}+n_L \Ti{\ker}{\tra_5\tra_6}{L}\Big)+\y{i}{\tra_5}\Ti{\ker}{\tra_6\tra_4}{L}+\y{i}{\tra_6}\Ti{\ker}{\tra_4\tra_5}{L}\Big]\, \notag
\end{align}
where we used \eqref{eq:decomposition-ddNTK-II} to evaluate the second ddNTK mean in terms of decomposition into the tensors $\ddNTKSS^{(L)}$, $\ddNTKTS^{(L)}$, and $\ddNTKUS^{(L)}$.




Interestingly, we see that not only does the mean prediction depend on the
NTK differentials -- as we expect, given the nontrivial representation learning at finite width -- but also it depends on the NTK variance as well, cf.~\eqref{eq:mean-prediction-DNTK-contribution}. 
This is natural as each network fits the training data with its own \emph{particular} NTK, and so the resulting fully-trained particular output \eqref{eq:very-general-finite-width-solution-DONT-CHANGE} depends on the NTK fluctuation, as we've already emphasized. In general, it is really important to understand the tradeoffs between both contributions, and in the next subsubsection we will return to comment on this interplay between random fluctuations and directed representation learning in the overall ensemble.




\index{gradient descent!wiring!finite width}\index{ddNTKs!contribution to finite-width prediction}
In addition, looking at the terms that are cubic in $\y{i}{\tra}$ in the contributions from the ddNTKs, \eqref{eq:mean-prediction-ddNTK-I-contribution} and \eqref{eq:mean-prediction-ddNTK-II-contribution}, we see that the $j$-th component of the observed outputs influences the $i$-th component of the mean prediction for $i\ne j$. Just as we saw for the mean prediction of Bayesian inference, \eqref{eq:mean-posterior-prediction-by-exact-Bayesian-at-finite-width}, this is one consequence of the wiring property of finite-width neural networks.
To see another manifestation of this wiring property, let's consider the covariance:
\be\label{eq:definition-of-covariance-fully-trained-reprint-prolly}
\cov{z_{i_1;\tea_1}^{(L)}(T)}{z_{i_2;\tea_2}^{(L)}(T)}\equiv \E{z_{i_1;\tea_1}^{(L)}(T)~z_{i_2;\tea_2}^{(L)}(T)}-\E{z_{i_1;\tea_1}^{(L)}(T)}\E{z_{i_2;\tea_2}^{(L)}(T)}\, .
\ee
While we won't print this quantity in full -- the full expression doesn't really play nicely with the constraints of the page -- you can easily extract insight by considering some specific contributions.
 For instance, we can see the imprint of output-component wiring by looking at the following contribution to the covariance,
\begin{align}\label{eq:finite-width-fully-trained-cov-wiring-term}
&\sum_{\substack{j_1,j_2\\ \tra_1,\ldots,\tra_4}}\E{\z{i_2}{\tea_2}{L}\Tia{\dNTK}{i_1 j_1j_2}{\tea_1\tra_1\tra_2}{L}} \algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\E{\le(\z{j_1}{\tra_3}{L}-\y{j_1}{\tra_3}\ri)\le(\z{j_2}{\tra_4}{L}-\y{j_2}{\tra_4}\ri)}\, \notag\\
=&\frac{1}{n_{L-1}}\delta_{i_1i_2}\sum_{\tra_1,\ldots,\tra_4}\Bigg[\le(n_L \dNTKP{\tea_1\tra_1\tra_2\tea_2}{L}+ \dNTKQ{\tea_1\tra_1\tra_2\tea_2}{L} + \dNTKQ{\tea_1\tra_2\tra_1\tea_2}{L}\ri)\Ti{G}{\tra_3\tra_4}{L}\, \notag\\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad+\dNTKP{\tea_1\tra_1\tra_2\tea_2}{L}\Big(\sum_{j}\y{j}{\tra_3}\y{j}{\tra_4}\Big)\Bigg]\algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\, \notag\\
&+\frac{1}{n_{L-1}}\sum_{\tra_1,\ldots,\tra_4}\dNTKQ{\tea_1\tra_1\tra_2\tea_2}{L} \algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}\le(\y{i_1}{\tra_3}\y{i_2}{\tra_4}+\y{i_1}{\tra_4}\y{i_2}{\tra_3}\ri)\, ,
\end{align}
which comes from the cross correlation between a factor of $\z{i_2}{\tea_2}{L}$ from $z_{i_2;\tea_2}^{(L)}(T)$ and one of the dNTK terms from $z_{i_1;\tea_1}^{(L)}(T)$ that involves the algorithm projector $ \algodNTKtwo^{\tra_1\tra_2\tra_3\tra_4}$.
In particular, we see that wiring is exhibited in the last term on the final line when the true outputs have nonzero components for both $i_1$ and $i_2$.
In this case, this correlation \eqref{eq:finite-width-fully-trained-cov-wiring-term} implies that the test-set predictions for $z_{i_1;\tea_1}^{(L)}(T)$ can be shifted given $z_{i_2;\tea_2}^{(L)}(T)$, for $i_1 \neq i_2$: cf.~our related discussion of \terminate{Hebbian learning} in terms of the \emph{fire-together} inductive bias in the finite-width prior in \S\ref{subsec:Hebbian} and then our following discussion of how that leads to the posterior distribution's \emph{wiring together} in \S\ref{subsec:presence-FF-Bayes}. Here, the presence of this wiring in the covariance of the solution means that this contribution to wiring occurs differently for  each network in the ensemble: the fluctuations from instantiation to instantiation of the  parameters at initialization break the permutation symmetry among the output neurons. This type of wiring suggests that each network is able to use the fluctuations between the different initial output components to its advantage in order to correlate such components together over the course of learning.





More broadly, unlike the kernel prediction\index{kernel methods!prediction} at infinite width \eqref{eq:kernel-prediction}, the prediction at finite width \eqref{eq:very-general-finite-width-solution-DONT-CHANGE} now has non-Gaussian statistics. In particular, since finite-width prediction 
is a nontrivial functional of the network output, the NTK, the dNTK, and the ddNTKs -- all at initialization --  
and since we know that the joint distribution of those quantities $p\Big(z^{(L)}, \, \NTK^{(L)},\, \dNTK^{(L)}, \, \ddNTK^{(L)},\, \ddNTKII^{(L)}\Big\vert \D\Big)$ 
is a \neo{nearly-Gaussian distribution}, then so is the fully-trained distribution $p\Big(z^{(L)}(T) \, \Big\vert \D\Big)$. In particular, there are nontrivial higher-point connected correlators; the explicit expressions of such correlators are challenging to display in any media format, though all the information needed to do so is contained in \eqref{eq:very-general-finite-width-solution-DONT-CHANGE}, and it's not that hard to zero in on any particular term of interest. The information carried in such correlators probably contains useful insight into some of the behavior of fully-trained networks and is likely worth further consideration.





\subsubsection{Generalization at Finite Width}\index{generalization error!finite-width}
Having discussed many of the qualitative differences between the finite-width prediction \eqref{eq:very-general-finite-width-solution-DONT-CHANGE} and the infinite-width kernel prediction \eqref{eq:kernel-prediction}, now let's  make some quantitative statements. In order to get a high-level understanding of how generalization is modified at finite width as compared to our extensive infinite-width analysis in \S\ref{sec:generalization-at-infinity},  we need to determine the size of the relative importance of the finite-width corrections to our predictions, namely how these corrections depend on the widths of the hidden layers $n_\ell$ and the depth of the network $L$. In particular, we want to understand corrections to our generalized \terminate{bias-variance tradeoff} \eqref{eq:bias-variance-decomposition-generalized-mse} at finite width.




\index{generalization error!bias}
Let's start by considering the bias term, $m_{i;\tea} - \y{i}{\tea}$, for which we just need to look at the mean prediction $m_{i;\tea}$ in  \eqref{eq:very-general-finite-width-solution-mean-2}. In particular, we need to compare the first term, \eqref{eq:infinite-width-ish-contribution-to-the-bias},
which more or less corresponds to the infinite-width kernel prediction $\GDGPmean_{i;\tea}$ -- up to the subleading and unimportant correction to the NTK mean --
against the other set of the finite-width contributions to the mean, \eqref{eq:mean-prediction-DNTK-contribution}--\eqref{eq:mean-prediction-ddNTK-II-contribution}. 

Now, among the finite-width contributions in \eqref{eq:very-general-finite-width-solution-mean-2}, 
the terms inside the first set of large parentheses have a sample index corresponding to a test input, $\tea$, in each of the tensors $A^{(L)}$, $B^{(L)}$, $P^{(L)}$, $Q^{(L)}$, $\ddNTKRS^{(L)}$, $\ddNTKSS^{(L)}$, $\ddNTKTS^{(L)}$, and $\ddNTKUS^{(L)}$. 
Thus, even for a training set with one training sample as we studied in \S\ref{subsec:robustness-from-infinite-GD}, the particular details of these terms require an understanding of asymptotic multiple-input solutions of the recursions for the NTK variance, the preactivation-dNTK cross correlation, and the ddNTK mean; such an analysis 
is kind of annoying and was previously left as an \terminate{adventure for thrill seekers}, with a brief \terminate{instruction manual} for those interested buried in footnote \ref{foot:thrill-seekers-guide} of \S\ref{sec:dNTK-criticality}.
Unfortunately, we have no plans here to update that manual any further, and we don't expect to miss much by not doing so.\footnote{In particular, we expect that this first set of terms will behave qualitatively similar to the second set of terms that we will discuss in the next paragraph, though the particular order-one-level details may differ.}




In contrast, the terms inside the second set of large parentheses in \eqref{eq:very-general-finite-width-solution-mean-2}
can be analyzed with the results we already have in hand.
Thus, to nonlinearly gain a large amount of intuition with a small amount of effort, let's compare the infinite-width term \eqref{eq:infinite-width-ish-contribution-to-the-bias} against only this last set of terms. In particular, since both of these terms are preceded by a common prefactor $\sum_{\tra_1,\tra_2}\NTKM^{(L)}_{\tea\tra_1}\NTKMsub_{(L)}^{\tra_1\tra_2}$
-- whose effect was analyzed in \S\ref{sec:generalization-at-infinity} --
we simply need to understand the depth and width dependence of the tensors inside the second set of brackets in \eqref{eq:very-general-finite-width-solution-mean-2}. Here we'll evaluate these tensors for a single training set input $x_{\tra}=x$, dropping the training sample indices for the rest of this analysis for notational simplicity. 

To see the physics, it's simplest to look at the  ODE limit\index{gradient descent!continuum or ODE limit} of many many steps of gradient descent, $\eta\searrow 0$,
 for which the algorithm projectors\index{algorithm projector} take particularly simple form, 
\begin{align}\label{eq:ode-algo-projector-single-input}
\algodNTKone=\algodNTKtwo=\frac{1}{2\le(\NTKMsub^{(L)}\ri)^2}\, , \quad \algoddNTKIone = \algoddNTKItwo= \algoddNTKIIone= \frac{-1}{6\le(\NTKMsub^{(L)}\ri)^3}\, , \quad \algoddNTKIItwo=\frac{-1}{2\le(\NTKMsub^{(L)}\ri)^3}
 \, ,
\end{align}
cf.~\eqref{eq:damping-force-sum-NTK-minus},  \eqref{eq:damping-force-sum-NTK-one}, and \eqref{eq:ODE-YZ}--\eqref{eq:ODE-X-III} to derive these expressions.\footnote{Backing off the ODE limit\index{gradient descent!continuum or ODE limit}, for many many steps of gradient descent with a finite learning rate $\eta$,
the single-input dNTK algorithm projectors, \eqref{eq:implicit-ZA-tensor} and \eqref{eq:implicit-ZB-tensor}, are instead given by 
\begin{align}
\algodNTKone=\frac{1}{\big(\NTKMsub^{(L)}\big)^2}\le(\frac{1-\eta\NTKMsub^{(L)}}{2-\eta\NTKMsub^{(L)}}\ri)\, , \qquad \algodNTKtwo=\frac{1}{2\big(\NTKMsub^{(L)}\big)^2}\, .
\end{align}
In conjunction with similar single-input limits of the ddNTK algorithm projectors, this will mean that the same set of ratios, \eqref{eq:all-the-ratios},  determine the generalization error bias term, though now with an additional $\eta$ dependence.
In particular, the projector $\algodNTKone$ diverges as the global learning rate approaches from below $\eta\nearrow 2/\NTKMsub^{(L)}$. 
This divergence is expected: as we noted in footnote~\ref{footnote:convergence-dynamics}, we need $ ||\Iden-\eta \NTK||_\infty < 1$ for the training dynamics to converge after many many steps $t\to\infty$.  If you check, you'll also see that some of the ddNTK algorithm projectors have the same divergence.} 
Substituting these projectors \eqref{eq:ode-algo-projector-single-input} into the dNTK and ddNTK contributions to the mean prediction, \eqref{eq:mean-prediction-dNTK-contribution}--\eqref{eq:mean-prediction-ddNTK-II-contribution}, and then considering  the $1/n$ part of the mean prediction $m_{i;\tea}$ \eqref{eq:very-general-finite-width-solution-mean-2}, we see that all the individual terms are proportional to one of the following dimensionless ratios:
\begin{align}\label{eq:all-the-ratios}
\frac{A^{(L)}}{n_{L-1}\le(\NTKMsub^{(L)}\ri)^2}\, ,\qquad\frac{B^{(L)}}{n_{L-1}\le(\NTKMsub^{(L)}\ri)^2}\, ,\qquad\frac{P^{(L)}}{n_{L-1}\le(\NTKMsub^{(L)}\ri)^2}\, ,\qquad\frac{Q^{(L)}}{n_{L-1}\le(\NTKMsub^{(L)}\ri)^2}\,  , \\[1em]
\frac{\ddNTKRS^{(L)}\ker^{(L)}}{n_{L-1}\le(\NTKMsub^{(L)}\ri)^3}\, ,\qquad\frac{\ddNTKSS^{(L)}\ker^{(L)}}{n_{L-1}\le(\NTKMsub^{(L)}\ri)^3}\, ,\qquad\frac{\ddNTKTS^{(L)}\ker^{(L)}}{n_{L-1}\le(\NTKMsub^{(L)}\ri)^3}\, ,\qquad\frac{\ddNTKUS^{(L)}\ker^{(L)}}{n_{L-1}\le(\NTKMsub^{(L)}\ri)^3}\,  . \notag
\end{align}
Thus, these eight ratios
determine the size of the finite-width effects as compared to the leading infinite-width term.\footnote{Incidentally, the form of the final two ratios on the first line of \eqref{eq:all-the-ratios} is the ultimate justification for why in \eqref{eq:scaling-relations-dNTK} we divided the dNTK-preactivation cross-correlation tensors by two factors of the NTK mean, cf.~our discussion of dimensional analysis around~\eqref{eq:dimensions-of-P-Q}. Similarly the form of the four ratios on the second line of \eqref{eq:all-the-ratios}  is the ultimate justification for the ratios \eqref{eq:scaling-relations-ddNTKs} and \eqref{eq:scaling-relations-ddNTKs-2}. For this latter set of ratios, we should have really written $\ker^{(L)}+n_L^{-1}\sum_j y_j^2$ instead of just $\ker^{(L)}$, especially when $\ker^{(L)}$ behaves as a nontrivial power law in $L$; in such cases, we should really rescale the target output $\y{i}{\tra}$ by the power-law factor $L^{-p_0/2}$ as discussed around \eqref{eq:rescale-true-outputs}.
} 



Importantly, recalling our scaling laws\index{scaling law} for the NTK variance, \eqref{eq:NTK-scaling-laws}, for the dNTK-preactivation cross correlation, \eqref{eq:dNTK-scaling-laws}, and for the dNTK-preactivation cross correlation, \eqref{eq:scaling-relations-ddNTKs} and \eqref{eq:scaling-relations-ddNTKs-2}, we see that each of these dimensionless ratios will scale like the depth-to-width ratio $L/n$ (except for the subdominant $\ddNTKUS^{(L)}$ contribution).
Thus, overall we should find for the finite-width corrections
\be
m_{i;\tea} - \GDGPmean_{i;\tea} = \o{\frac{L}{n}} \, ,
\ee
where the exact order-one constant is not important. What is important is that we confirmed our expectation for the overall scaling of this correction, going as the \emph{cutoff}\index{cutoff, effective theory} of our effective theory: $r\equiv L/n$.
Similarly, we could do an analysis for the variance term of the generalization error by looking at the covariance \eqref{eq:definition-of-covariance-fully-trained-reprint-prolly}, which will be a whole lot of work with very little payoff; such an analysis would merely confirm that the leading finite-width corrections will again be of order $\o{L/n}$.\index{generalization error!variance}

\index{fluctuations!vs.~representation learning}\index{representation learning!vs.~fluctuations}
In general, given that the aspect ratio $L/n$ controls both the \terminate{fluctuations} in the ensemble \emph{and} \terminate{representation learning}, the optimal value of the ratio is likely nonzero but also small. In particular, representation learning is enhanced by the depth, but networks with too large a value of $L/n$ will both have an effect on the mean prediction of the ensemble, but perhaps even more importantly lead to exponentially-large problems when working with only a \emph{particular} network: for large enough $L/n$ our principle of typicality\index{typicality!principle of} can break down, and so the generalization error can begin to exhibit exponential behavior.\footnote{In our discussion of \neo{fluctuations} in  \S\ref{sec:signal_prop_finite_width}, we explained that too large a value of $L/n$ may lead to a difficult fine-tuning problem in terms of getting a particular networks to behave critically.

On the one hand, the contribution of such fluctuations to the bias part of the generalization error is one way to see how the downstream effect of such fluctuations may lead to problems after training. On the other hand, the fact that fluctuations can ruin criticality for a particular network is another problem. The former problem affects the ensemble as a whole, while the latter problem affects individual networks.}

This further explains the success of our effective theory description at $\o{L/n}$: a description with vanishing $L/n$, i.e.~the \terminate{infinite-width limit}, is too simple to model the properties of deep neural networks in practice; a description for larger values of $L/n$, i.e.~a \emph{small}-width or \emph{overly}-deep regime that includes many higher-order corrections, describes networks that are unlikely to be trainable; a description with small but nonzero $L/n$, i.e.~the leading finite-width effective theory accurate to $\o{L/n}$, is as simple as it can be and still accurately describe the networks that work well in practice.\footnote{
    To better understand the scale that separates the trainable regime from the overly-deep regime, see Appendix~\ref{app:mi-stuff}.
    For a discussion of how to extend this trainable regime to greater depths for fixed width, see our discussion of residual networks\index{residual network} in Appendix~\ref{app:residual}.
}

In summary, we've seen that the leading finite-width corrections all scale
 exactly according to our long-standing expectations, and we've discussed at a high level the potential tradeoff of depth versus width. In principle, we could go further in our analysis, evaluating the multi-input recursions for nearby inputs, evaluating all the terms in the covariance, and finding all the $\o{L/n}$ contributions to the generalization error with specific coefficients.\footnote{You can also more easily evaluate the multi-input version of the relevant recursions numerically with this scaling in mind in order to determine the overall coefficient for a particular activation function of interest. This would be one way to analyze these solutions for the $\relu$-like $\gelu$ and $\swish$ networks.} If we did this, what could it tell us? In particular, can we theoretically optimize the aspect ratio $L/n$ for a particular activation function without any experimentation?



\index{optimal aspect ratio}
Unfortunately at the order we're working, we won't be able to optimize the aspect ratio $L/n$ using the prediction formula \eqref{eq:very-general-finite-width-solution-DONT-CHANGE} from the end of training: the linear dependence of the generalization on the ratio means that its derivative is independent of the ratio; instead, we'd need to compute the higher-order corrections of order $\o{L^2/n^2}$ to optimize the ratio -- which is hard, though at least straightforward to do in the effective theory framework we've developed.\footnote{
    In \S\ref{sec:information-beyond-infinity}, we'll compute such higher-order effects  from an \emph{information-theoretic}\index{information theory} perspective. This analysis will give a heuristic prescription for optimizing the network's depth-to-width ratio $r$: we consider an auxiliary \neo{unsupervised learning} objective thought to be beneficial for building representations and optimize the aspect ratio in terms of \emph{that} criterion rather than the generalization error. In this setting, we're able to determine optimal aspect ratios for different activation functions by trading off leading-order effects against higher-order effects. This is somewhat similar in spirit to the way in which \terminate{Newton's method} trades off the leading decrease of the loss against the subleading loss increase in order to optimize the overall learning rate, cf.~our discussion of Newton's method in footnote~\ref{footnote:newtons-method} of~\S\ref{sec:giant-leap}.
}
At order $\o{L/n}$, the best we could hope to see is whether nonzero $L/n$ improves generalization or not by looking at the sign of its overall coefficient.
Rather than going through that hassle, we'll instead get a little more mileage out of the mean prediction $m_{i;\tea}$ by trying to understand how the \neo{algorithm dependence} at finite width leads to additional tradeoffs within the bias term of the generalization error.











\subsubsection{Inductive Bias of the Training Algorithm}\index{universality!of the fully-trained network solution}\index{ODE limit|see{gradient descent}}
As multiply mentioned, one of the key differences between the infinite-width and finite-width networks optimized via gradient-based learning is the \neo{algorithm dependence} of the fully-trained solution for finite-width networks. In particular, the solution -- at least for MSE losses -- takes a universal form, \eqref{eq:very-general-finite-width-solution-DONT-CHANGE}, with all of the dependence of the solution on the training algorithm encoded in  the \emph{algorithm projectors}\index{algorithm projector},
$\algodNTKone$, $\algodNTKtwo$, $\algoddNTKIone$, $\algoddNTKItwo$, $\algoddNTKIIone$, and $\algoddNTKIItwo$, 
whose functional forms we've established explicitly for the two second-order updates in \eqref{eq:ZB-two-step}, for many many steps of gradient descent in \eqref{eq:implicit-ZA-tensor}--\eqref{eq:implicit-ZB-tensor-II}, and for the ODE limit\index{gradient descent!continuum or ODE limit} of gradient descent in \eqref{eq:ODE-YZ}--\eqref{eq:ODE-YZ-ddNTK-II-2}. Through this universal projection, we now have a theoretical means of isolating the \emph{inductive bias of the training algorithm} from all the other inductive biases that are present in a fully-trained neural network. This provides a natural way of theoretically evaluating the relative merits of different training algorithms.\index{inductive bias!of learning algorithms|textbf}

One aspect that is apparent just from staring at the stochastic prediction \eqref{eq:very-general-finite-width-solution-DONT-CHANGE} is that the algorithm projectors only induce projections on the NTK-differential part of the finite-width prediction and cannot affect the NTK-fluctuation contribution.
More concretely, let's continue our discussion of the bias term in the generalization error\index{generalization error!bias}. In particular, the bias is given by the following difference:
\be
m_{i;\tea} - \y{i}{\tea} \, .
\ee
On the one hand, it's clear that the NTK-variance contribution to the mean prediction, \eqref{eq:mean-prediction-DNTK-contribution}, 
encoded in $A^{(L)}$ and $B^{(L)}$ is irreducible, entirely independent of the training algorithm; 
this irreducible NTK-variance contribution depends on the training data in a fixed way and arises due to instantiation-to-instantiation fluctuations in the NTK across different realizations of the parameters.
On the other hand, the projectors always act on the NTK-differential tensors $P^{(L)}$, $Q^{(L)}$, $\ddNTKRS^{(L)}$, $\ddNTKSS^{(L)}$, $\ddNTKTS^{(L)}$, and $\ddNTKUS^{(L)}$; this  means that the dNTK's and ddNTKs' effect on the network's predictions is adjustable by the training algorithm and can be tuned differently for different datasets and tasks.

To reiterate our previous discussion of the tradeoff, on the one hand the NTK-fluctuation contribution is likely harmful as it leads to a breakdown of criticality for any \emph{particular} network.
On the other hand, the latter NTK-differential contribution is the ultimate source of the nontrivial representation learning at finite width and so it would be nice to make this contribution large.
With these algorithm projectors, we now have direct means to enhance the latter benefit while keeping fixed the former cost.



One way of understanding these algorithm projectors\index{algorithm projector} is as the sample-space \emph{dual description} of a \terminate{learning algorithm},\index{duality!learning algorithm -- algorithm projectors}\index{learning algorithm!dual to algorithm projector}
analogous to the relationship between the feature functions $\fea_j(x)$ and the kernel $\kerm(x_{\delta_1},x_{\delta_2})$ that we explored in \S\ref{sec:lazy-kernel} and \S\ref{sec:nonlinear-model}.\index{feature function}\index{kernel methods!kernel} 
From the parameter-space \terminate{microscopic perspective}, a learning algorithm, such as \terminate{gradient descent}, explicitly operates on the parameters.  At the end of training, all the details of the algorithm are left implicit in the trained parameters $\theta^\star$, making it difficult to understand its effect on the model's predictions. From this perspective, the algorithm is simple to define  (\S\ref{sec:gd}), but decidedly difficult to analyze for general interacting machine-learning models (\S\ref{subsec:real-GD-at-finite-width}). However, if you can analyze or \emph{solve} a model to find its sample-space dual description,
then the algorithm projectors make the influence of the learning algorithm explicit.\footnote{This is the sense in which we meant that the conditioning on the learning algorithm would be \emph{simple} for the trained ensemble in \eqref{eq:lofty-goal-refined}.}


Analogously to the (nearly-)kernel perspective on (nearly-)linear models, 
the appearance of the algorithm projectors\index{algorithm projector} in the generalization error\index{generalization error!finite-width} means that we can begin to think about \neo{engineering} them directly, either by picking them to be directly used with a prediction formula such as \eqref{eq:very-general-finite-width-solution-DONT-CHANGE}, or alternatively by attempting to find the parameter-space dual of an algorithm projector.
While we expect this latter task to be difficult -- just as it's often hard to find the feature functions that correspond to a particular kernel -- it could be very worthwhile to explore as a means of engineering the inductive bias of the training algorithm\index{inductive bias!of learning algorithms} directly.\footnote{
    This can also be seen as another advantage of nonlinear or interacting machine learning models. For linear models, the solution is always independent of the details of the learning algorithm (\S\ref{subsec:algorithmic-independence-at-infinity}).\index{algorithm independence}
}

In particular, we expect that this could significantly improve generalization by designing algorithms that are better tailored to the details of an architecture or on the properties of the underlying dataset or task.\footnote{
Since the algorithm projectors have sample indices, the \emph{optimal} choice of the algorithm will in general depend on the details and structure of the training set in addition to the definition of the model.
}
 This design process could in principle proceed as follows: %
\emph{(i)} engineer a desired functional form for the algorithm projectors and then \emph{(ii)} determine the parameter-space training algorithm that leads to the projectors having those desired properties or specific functional form. 
While further details of such \term{inverse algorithm design}\index{inverse algorithm design|seealso{algorithm projector}} is outside the scope of this book, we think that this line of \emph{dual} analysis has the potential to unlock a much deeper understanding of the relationship among the inductive bias of the training algorithm, the inductive bias of the network architecture, and the ultimate success of the model.

















\subsubsection{There's No Place Where Gradient Descent = Exact Bayesian Inference}\index{Bayesian inference!via gradient descent!but not at finite width}\index{gradient descent!as Bayesian inference!but not at finite width}
Finally, a curious reader might wonder whether the connection that we detailed in \S\ref{subsec:NTKprediction}  between gradient descent optimization and exact Bayesian inference for the infinite-width limit  persists for more realistic networks at finite width. In short, the answer is no.

In long, 
recall the training hyperparameter settings \eqref{eq:last-layer-bayes-learning-rate} and \eqref{eq:hidden-layer-bayes-learning-rate} that matched gradient-based learning with Bayesian inference at infinite width. In particular, the latter condition \eqref{eq:hidden-layer-bayes-learning-rate} required turning off any learning in the hidden layers:
\be\label{eq:hidden-layer-bayes-learning-rate-reprint}
\Lb{\ell}=0\,, \qquad \LW{\ell}=0\,,\qquad \text{for}\quad \ell <L\, .
\ee
Importantly, this makes the hidden-layer NTK vanish exactly: $\Tia{\NTK}{i_1i_2}{\delta_1\delta_2}{\ell} = 0$, for $\ell <L$ at any finite width, cf.~\eqref{eq:NTH-recursion-without-expectation}.
Next, flip back and look at the $P$- and $Q$-recursions for the dNTK-preactivation cross-correlation tensors, \eqref{eq:dNTKP-recursion-explicit} and \eqref{eq:dNTKQ-recursion-explicit}, and then you'll probably also want to flip further back and visit the $F$- and $B$-recursions, \eqref{eq:F-recursion} and \eqref{eq:B-recursion}, respectively.
(We'll be  waiting here for you when you return.) Immediately, you should notice a problem: if the hidden-layer NTK mean vanishes, then at this order $B^{(\ell)}=0$, $F^{(\ell)}=0$, and all this together implies that $P^{(L)}=Q^{(L)}=0$. Thus, all the effects of the dNTK are turned off. Similarly, if you flip forth and look at the $R$-, $S$-, $T$-, and $U$-recursions, \eqref{eq:R-recursion} and \eqref{eq:S-recursion}--\eqref{eq:U-recursion}, then you'll notice that all the effects of the ddNTKs are turned off as well.
Thus, the mean prediction of our finite-width gradient-based-learning ensemble \eqref{eq:very-general-finite-width-solution-mean-2} \emph{cannot} match the exact Bayesian posterior mean\index{posterior!posterior mean} at finite width \eqref{eq:mean-posterior-prediction-by-exact-Bayesian-at-finite-width}.

Note that this mismatch is obvious in hindsight; by only training the last layer \eqref{eq:hidden-layer-bayes-learning-rate-reprint}, we get a \terminate{linear model} \eqref{eq:linear-model-BI}.
In other words, since the hyperparameter choices of \eqref{eq:hidden-layer-bayes-learning-rate-reprint} lead to a model with random features that are fixed over the course of training, there's no representation learning possible for these settings.
In contrast, we found nontrivial representation learning when studying exact Bayesian inference at finite width in \S\ref{subsec:presence-RL-Bayes}.



Ultimately, 
for Bayesian inference we only care about the preactivation distribution $p\!\le( z^{(\ell)} \Big\vert \D \ri)$, while for gradient-based learning we need to consider the joint preactivation-NTK-dNTK-ddNTKs distribution $p\!\le( z^{(L)}, \NTK^{(L)}, \dNTK^{(L)}, \ddNTK^{(L)}, \ddNTKII^{(L)} \Big\vert \D \ri)$, 
which
incorporates the statistics of the derivatives of the preactivations at initialization: such derivatives of the model output are invisible to the exact Bayesian inferencer.











































\section{RG Flow of the ddNTKs: The Full Expressions}\label{sec:gross-ddNTK-things}\index{ddNTKs!full expressions}
These expressions were kind of horrible, so we decided to hide them here at the end of the chapter. As such, they are only really needed for three reasons: \emph{(i)} to explicitly check the absence of any NTK differentials for the hyperparameter setup \eqref{eq:hidden-layer-bayes-learning-rate-reprint} and thus confirm that the connection between gradient descent and Bayesian inference doesn't persist at finite width, \emph{(ii)} to check the details of the depth-to-width scaling of the ddNTKs that we discussed in \S\ref{sec:ddNTKs}, and \emph{(iii)} to more generally evaluate the ddNTKs' contributions to
the ensemble's mean prediction, \eqref{eq:mean-prediction-ddNTK-I-contribution} and \eqref{eq:mean-prediction-ddNTK-II-contribution}, for multiple inputs  -- analytically or numerically --  or to compute other higher-order statistics of our stochastic prediction \eqref{eq:very-general-finite-width-solution-DONT-CHANGE}.



\subsubsection{$\ddNTK$ Stochastic Forward Equation}\index{forward equation!ddNTKs}
\begin{align}\label{eq:ddNTK-forward-equation}
&\Tia{\ddNTK}{i_0i_1i_2i_3}{\delta_0\delta_1\delta_2\delta_3}{\ell+1}\, \\
=&\ \ \ \delta_{i_0i_1}\frac{\LW{\ell+1}}{n_{\ell}}\!\!\sum_{j_0,j_1,j_2,j_3=1}^{n_{\ell}}\!\!\delta_{j_0j_1}\W{i_2j_2}{\ell+1}\W{i_3j_3}{\ell+1}\s{j_1}{\delta_1}{\ell}\ds{j_2}{\delta_2}{\ell}\ds{j_3}{\delta_3}{\ell} \, \notag \\
&\qquad\qquad\quad\times\Big[\dds{j_0}{\delta_0}{\ell}\Tia{\NTK}{j_0j_2}{\delta_0\delta_2}{\ell}\Tia{\NTK}{j_0j_3}{\delta_0\delta_3}{\ell}+\ds{j_0}{\delta_0}{\ell}\Tia{\dNTK}{j_0j_2j_3}{\delta_0\delta_2\delta_3}{\ell}\Big]\, \notag\\
&+\delta_{i_0i_2}\frac{\LW{\ell+1}}{n_{\ell}}\!\!\sum_{j_0,j_1,j_2,j_3=1}^{n_{\ell}}\!\!\delta_{j_0j_2}\W{i_3j_3}{\ell+1}\W{i_1j_1}{\ell+1}\s{j_2}{\delta_2}{\ell}\ds{j_3}{\delta_3}{\ell}\ds{j_1}{\delta_1}{\ell} \, \notag \\
&\qquad\qquad\quad\times\Big[\dds{j_0}{\delta_0}{\ell}\Tia{\NTK}{j_0j_3}{\delta_0\delta_3}{\ell}\Tia{\NTK}{j_0j_1}{\delta_0\delta_1}{\ell}+\ds{j_0}{\delta_0}{\ell}\Tia{\dNTK}{j_0j_3j_1}{\delta_0\delta_3\delta_1}{\ell}\Big]\, \notag\\
&+\delta_{i_0i_3}\frac{\LW{\ell+1}}{n_{\ell}}\!\!\sum_{j_0,j_1,j_2,j_3=1}^{n_{\ell}}\!\!\delta_{j_0j_3}\W{i_1j_1}{\ell+1}\W{i_2j_2}{\ell+1}\s{j_3}{\delta_3}{\ell}\ds{j_1}{\delta_1}{\ell}\ds{j_2}{\delta_2}{\ell}\, \notag \\
&\qquad\qquad\quad\times\Big[\dds{j_0}{\delta_0}{\ell}\Tia{\NTK}{j_0j_1}{\delta_0\delta_1}{\ell}\Tia{\NTK}{j_0j_2}{\delta_0\delta_2}{\ell}+\ds{j_0}{\delta_0}{\ell}\Tia{\dNTK}{j_0j_1j_2}{\delta_0\delta_1\delta_2}{\ell}\Big]\, \notag\\
&+\!\!\sum_{j_0,j_1,j_2,j_3=1}^{n_{\ell}}\!\!\W{i_0j_0}{\ell+1}\W{i_1j_1}{\ell+1}\W{i_2j_2}{\ell+1}\W{i_3j_3}{\ell+1}\ds{j_1}{\delta_1}{\ell}\ds{j_2}{\delta_2}{\ell}\ds{j_3}{\delta_3}{\ell}\, \notag\\
&\qquad\qquad\quad\times\Big[\ds{j_0}{\delta_0}{\ell}\Tia{\ddNTK}{j_0j_1j_2j_3}{\delta_0\delta_1\delta_2\delta_3}{\ell}
+\dds{j_0}{\delta_0}{\ell}\Tia{\dNTK}{j_0j_1j_2}{\delta_0\delta_1\delta_2}{\ell}\Tia{\NTK}{j_0j_3}{\delta_0\delta_3}{\ell}
\, \notag\\
&\qquad\qquad\quad\qquad
+\dds{j_0}{\delta_0}{\ell}\Tia{\dNTK}{j_0j_2j_3}{\delta_0\delta_2\delta_3}{\ell}\Tia{\NTK}{j_0j_1}{\delta_0\delta_1}{\ell}
+\dds{j_0}{\delta_0}{\ell}\Tia{\dNTK}{j_0j_3j_1}{\delta_0\delta_3\delta_1}{\ell}\Tia{\NTK}{j_0j_2}{\delta_0\delta_2}{\ell}
\, \notag\\
&\qquad\qquad\qquad\qquad
+\sigma^{\prime\prime\prime(\ell)}_{j_0\delta_0}\Tia{\NTK}{j_0j_1}{\delta_0\delta_1}{\ell}\Tia{\NTK}{j_0j_2}{\delta_0\delta_2}{\ell}\Tia{\NTK}{j_0j_3}{\delta_0\delta_3}{\ell}
\Big]\, ,\notag
\end{align}

\subsubsection{$\ddNTKII$ Stochastic Forward Equation}\index{forward equation!ddNTKs}

\begin{align}\label{eq:ddNTK-II-forward-equation}
&\Tia{\ddNTKII}{i_1i_2i_3i_4}{\delta_1\delta_2\delta_3\delta_4}{\ell+1}\, \\
=&\delta_{i_1i_3}\delta_{i_2i_4}\le(\frac{\LW{\ell+1}}{n_{\ell}}\ri)^2\!\!\sum_{j,k=1}^{n_{\ell}}\!\!\ds{j}{\delta_1}{\ell}\ds{k}{\delta_2}{\ell}\s{j}{\delta_3}{\ell}\s{k}{\delta_4}{\ell}\Tia{\NTK}{jk}{\delta_1\delta_2}{\ell}\, \notag\\
&+\delta_{i_1i_2}\frac{\LW{\ell+1}}{n_{\ell}}\!\!\sum_{j_1,\ldots,j_4=1}^{n_{\ell}}\!\!\delta_{j_1j_2}\W{i_3j_3}{\ell+1}\W{i_4j_4}{\ell+1}\ds{j_1}{\delta_1}{\ell}\ds{j_2}{\delta_2}{\ell}\ds{j_3}{\delta_3}{\ell}\ds{j_4}{\delta_4}{\ell}\Tia{\NTK}{j_1j_3}{\delta_1\delta_3}{\ell}\Tia{\NTK}{j_2j_4}{\delta_2\delta_4}{\ell}\, \notag\\
&+\delta_{i_1i_3}\frac{\LW{\ell+1}}{n_{\ell}}\!\!\sum_{j_1,\ldots,j_4=1}^{n_{\ell}}\!\!\delta_{j_1j_3}\W{i_2j_2}{\ell+1}\W{i_4j_4}{\ell+1}\s{j_3}{\delta_3}{\ell}\ds{j_4}{\delta_4}{\ell}\ds{j_1}{\delta_1}{\ell} \, \notag \\
&\qquad\qquad\quad\times\Big[\dds{j_2}{\delta_2}{\ell}\Tia{\NTK}{j_2j_1}{\delta_2\delta_1}{\ell}\Tia{\NTK}{j_2j_4}{\delta_2\delta_4}{\ell} +\ds{j_2}{\delta_2}{\ell}\Tia{\dNTK}{j_2j_1j_4}{\delta_2\delta_1\delta_4}{\ell}\Big]\, \notag\\
&+\delta_{i_2i_4}\frac{\LW{\ell+1}}{n_{\ell}}\!\!\sum_{j_1,\ldots,j_4=1}^{n_{\ell}}\!\!\delta_{j_2j_4}\W{i_1j_1}{\ell+1}\W{i_3j_3}{\ell+1}\s{j_4}{\delta_4}{\ell}\ds{j_3}{\delta_3}{\ell}\ds{j_2}{\delta_2}{\ell}\, \notag \\
&\qquad\qquad\quad\times\Big[\dds{j_1}{\delta_1}{\ell}\Tia{\NTK}{j_1j_2}{\delta_1\delta_2}{\ell}\Tia{\NTK}{j_1j_3}{\delta_1\delta_3}{\ell}+\ds{j_1}{\delta_1}{\ell}\Tia{\dNTK}{j_1j_2j_3}{\delta_1\delta_2\delta_3}{\ell}\Big]\, \notag\\
&+\!\!\sum_{j_1,j_2,j_3,j_4=1}^{n_{\ell}}\!\!\W{i_1j_1}{\ell+1}\W{i_2j_2}{\ell+1}\W{i_3j_3}{\ell+1}\W{i_4j_4}{\ell+1}\ds{j_3}{\delta_3}{\ell}\ds{j_4}{\delta_4}{\ell}\, \notag\\
&\qquad\qquad\quad\times\Big[\ds{j_1}{\delta_1}{\ell}\ds{j_2}{\delta_2}{\ell}\Tia{\ddNTKII}{j_1j_2j_3j_4}{\delta_1\delta_2\delta_3\delta_4}{\ell}+\dds{j_1}{\delta_1}{\ell}\dds{j_2}{\delta_2}{\ell}\Tia{\NTK}{j_1j_2}{\delta_1\delta_2}{\ell}\Tia{\NTK}{j_1j_3}{\delta_1\delta_3}{\ell}\Tia{\NTK}{j_2j_4}{\delta_2\delta_4}{\ell}\, \notag\\
&\qquad\qquad\qquad\quad+\ds{j_1}{\delta_1}{\ell}\dds{j_2}{\delta_2}{\ell}\Tia{\NTK}{j_2j_4}{\delta_2\delta_4}{\ell}\Tia{\dNTK}{j_1j_2j_3}{\delta_1\delta_2\delta_3}{\ell}+\ds{j_2}{\delta_2}{\ell}\dds{j_1}{\delta_1}{\ell}\Tia{\NTK}{j_1j_3}{\delta_1\delta_3}{\ell}\Tia{\dNTK}{j_2j_1j_4}{\delta_2\delta_1\delta_4}{\ell}\Big]\, ,\notag
\end{align}

\subsubsection{$\ddNTK$ Recursion}
The mean of the first ddNTK decomposes as
\begin{align}
&\E{\Tia\ddNTK{i_0i_1i_2i_3}{\delta_0\delta_1\delta_2\delta_3}{\ell}}\, \\
=&\frac{1}{n_{\ell-1}}\le[\delta_{i_0i_1}\delta_{i_2i_3}\ddNTKR{\delta_0\delta_1\delta_2\delta_3}{\ell}+\delta_{i_0i_2}\delta_{i_3i_1}\ddNTKR{\delta_0\delta_2\delta_3\delta_1}{\ell}+\delta_{i_0i_3}\delta_{i_1i_2}\ddNTKR{\delta_0\delta_3\delta_1\delta_2}{\ell}\ri]\, .\notag
\end{align}
The tensor $\ddNTKR{}{\ell}$ satisfies the following layer-to-layer recursion:
\begin{align}\label{eq:R-recursion}\index{ddNTKs!statistics!R-recursion@$R$-recursion}
&\ddNTKR{\delta_0\delta_1\delta_2\delta_3}{\ell+1}\, \\
=&\LW{\ell+1}\CW{\ell+1}\bra\sigma^{\prime\prime}_{\delta_0}\sigma_{\delta_1}\sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_0\delta_2}{\ell}\Ti{\NTKM}{\delta_0\delta_3}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\bra\sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\ri)\le(\LW{\ell+1}\bra\sigma^{\prime\prime}_{\delta_0}\sigma_{\delta_1}\ket_{G^{(\ell)}}\ri)\NTHB{\delta_0\delta_0\delta_2\delta_3}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\bra\sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\ri)\le[\LW{\ell+1}\le(\bra\sigma^{\prime\prime}_{\delta_0}\sigma_{\delta_1}\ket_{G^{(\ell)}}\dNTKP{\delta_0\delta_2\delta_3\delta_0}{\ell}+\bra\sigma^{\prime}_{\delta_0}\sigma^{\prime}_{\delta_1}\ket_{G^{(\ell)}}\dNTKP{\delta_0\delta_2\delta_3\delta_1}{\ell}\ri)\ri]\, \notag\\
&+\le(\CW{\ell+1}\ri)^2\bra\sigma^{\prime\prime\prime}_{\delta_0}\sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_0\delta_1}{\ell}\Ti{\NTKM}{\delta_0\delta_2}{\ell}\Ti{\NTKM}{\delta_0\delta_3}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\bra\sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\ri)\le(\CW{\ell+1}\bra\sigma^{\prime\prime\prime}_{\delta_0}\sigma^{\prime}_{\delta_1}\ket_{G^{(\ell)}}\ri)\NTHB{\delta_0\delta_0\delta_2\delta_3}{\ell}\Ti{\NTKM}{\delta_0\delta_1}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\bra\sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\ri)\le[\CW{\ell+1}\le(\bra\sigma^{\prime\prime\prime}_{\delta_0}\sigma^{\prime}_{\delta_1}\ket_{G^{(\ell)}}\dNTKP{\delta_0\delta_2\delta_3\delta_0}{\ell}+\bra\sigma^{\prime\prime}_{\delta_0}\sigma^{\prime\prime}_{\delta_1}\ket_{G^{(\ell)}}\dNTKP{\delta_0\delta_2\delta_3\delta_1}{\ell}\ri)\ri]\Ti{\NTKM}{\delta_0\delta_1}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\bra\sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\ri)\le(\CW{\ell+1}\bra\sigma^{\prime}_{\delta_0}\sigma^{\prime}_{\delta_1}\ket_{G^{(\ell)}}\ri)\ddNTKR{\delta_0\delta_1\delta_2\delta_3}{\ell}\, +\o{\frac{1}{n}}\, .\notag
\end{align}

\subsubsection{$\ddNTKII$ Recursions}
The mean of the second ddNTK decomposes as
\begin{align}
&\E{\Tia\ddNTKII{i_1i_2i_3i_4}{\delta_1\delta_2\delta_3\delta_4}{\ell}}\, \\
=&\frac{1}{n_{\ell-1}}\le[\delta_{i_1i_2}\delta_{i_3i_4}\ddNTKS{\delta_1\delta_2\delta_3\delta_4}{\ell}+\delta_{i_1i_3}\delta_{i_4i_2}\ddNTKT{\delta_1\delta_3\delta_4\delta_2}{\ell}+\delta_{i_1i_4}\delta_{i_2i_3}\ddNTKU{\delta_1\delta_4\delta_2\delta_3}{\ell}\ri]\, .\notag
\end{align}
The tensor $\ddNTKS{}{\ell}$ satisfies the following layer-to-layer recursion:
\begin{align}\label{eq:S-recursion}\index{ddNTKs!statistics!S-recursion@$S$-recursion}
&\ddNTKS{\delta_1\delta_2\delta_3\delta_4}{\ell+1}\, \\
=&\CW{\ell+1}\LW{\ell+1}\bra \sigma^{\prime}_{\delta_1} \sigma^{\prime}_{\delta_2} \sigma^{\prime}_{\delta_3}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_1\delta_3}{\ell}\Ti{\NTKM}{\delta_2\delta_4}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\bra \sigma^{\prime}_{\delta_3}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\ri)\le[\LW{\ell+1}\bra \sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_2}\ket_{G^{(\ell)}}+\CW{\ell+1}\Ti{\NTKM}{\delta_1\delta_2}{\ell}\bra \sigma^{\prime\prime}_{\delta_1}\sigma^{\prime\prime}_{\delta_2}\ket_{G^{(\ell)}}\ri]\NTHB{\delta_1\delta_2\delta_3\delta_4}{\ell}\, \notag\\
&+\le(\CW{\ell+1}\ri)^2\bra \sigma^{\prime\prime}_{\delta_1} \sigma^{\prime\prime}_{\delta_2} \sigma^{\prime}_{\delta_3}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_1\delta_2}{\ell}\Ti{\NTKM}{\delta_1\delta_3}{\ell}\Ti{\NTKM}{\delta_2\delta_4}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\bra \sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_2}\ket_{G^{(\ell)}}\ri)\le(\CW{\ell+1}\bra \sigma^{\prime}_{\delta_3}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\ri)\ddNTKS{\delta_1\delta_2\delta_3\delta_4}{\ell}+\o{\frac{1}{n}}\, .\notag
\end{align}
The tensor $\ddNTKT{}{\ell}$ satisfies the following layer-to-layer recursion:
\begin{align}\label{eq:T-recursion}\index{ddNTKs!statistics!T-recursion@$T$-recursion}
&\ddNTKT{\delta_1\delta_3\delta_4\delta_2}{\ell+1}\, \\
=&\le(\LW{\ell+1}\ri)^2\bra \sigma^{\prime}_{\delta_1} \sigma^{\prime}_{\delta_2} \sigma_{\delta_3}\sigma_{\delta_4}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_1\delta_2}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\LW{\ell+1}\ri)^2\sum_{\delta_5,\ldots,\delta_8\in\D}\bra z_{\delta_5}\sigma^{\prime}_{\delta_1} \sigma_{\delta_3}\ket_{G^{(\ell)}}\bra z_{\delta_6} \sigma^{\prime}_{\delta_2}\sigma_{\delta_4}\ket_{G^{(\ell)}}\TI{G}{\delta_5\delta_7}{\ell}\TI{G}{\delta_6\delta_8}{\ell}\NTHF{\delta_7\delta_1\delta_8\delta_2}{\ell}\, \notag\\
&+\CW{\ell+1}\LW{\ell+1}\bra \sigma^{\prime}_{\delta_1} \sigma^{\prime\prime}_{\delta_2} \sigma_{\delta_3}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_2\delta_1}{\ell}\Ti{\NTKM}{\delta_2\delta_4}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\CW{\ell+1}\LW{\ell+1}\Ti{\NTKM}{\delta_2\delta_4}{\ell}\sum_{\delta_5,\ldots,\delta_8\in\D}\bra z_{\delta_5}\sigma^{\prime}_{\delta_1} \sigma_{\delta_3}\ket_{G^{(\ell)}}\bra z_{\delta_6} \sigma^{\prime\prime}_{\delta_2}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\TI{G}{\delta_5\delta_7}{\ell}\TI{G}{\delta_6\delta_8}{\ell}\NTHF{\delta_7\delta_1\delta_8\delta_2}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\CW{\ell+1}\LW{\ell+1}\bra  \sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\le(\bra  \sigma^{\prime\prime}_{\delta_1}\sigma_{\delta_3}\ket_{G^{(\ell)}}\dNTKQ{\delta_2\delta_4\delta_1\delta_1}{\ell}+\bra  \sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\dNTKQ{\delta_2\delta_4\delta_1\delta_3}{\ell}\ri)\, \notag\\
&+\CW{\ell+1}\LW{\ell+1}\bra \sigma^{\prime}_{\delta_2} \sigma^{\prime\prime}_{\delta_1} \sigma_{\delta_4}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_1\delta_2}{\ell}\Ti{\NTKM}{\delta_1\delta_3}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\CW{\ell+1}\LW{\ell+1}\Ti{\NTKM}{\delta_1\delta_3}{\ell}\sum_{\delta_5,\ldots,\delta_8\in\D}\bra z_{\delta_5}\sigma^{\prime}_{\delta_2} \sigma_{\delta_4}\ket_{G^{(\ell)}}\bra z_{\delta_6} \sigma^{\prime\prime}_{\delta_1}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\TI{G}{\delta_5\delta_7}{\ell}\TI{G}{\delta_6\delta_8}{\ell}\NTHF{\delta_7\delta_2\delta_8\delta_1}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\CW{\ell+1}\LW{\ell+1}\bra  \sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\le(\bra  \sigma^{\prime\prime}_{\delta_2}\sigma_{\delta_4}\ket_{G^{(\ell)}}\dNTKQ{\delta_1\delta_3\delta_2\delta_2}{\ell}+\bra  \sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\dNTKQ{\delta_1\delta_3\delta_2\delta_4}{\ell}\ri)\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\ri)^2\bra \sigma^{\prime}_{\delta_1} \sigma^{\prime}_{\delta_3} \ket_{G^{(\ell)}}\bra \sigma^{\prime}_{\delta_2} \sigma^{\prime}_{\delta_4} \ket_{G^{(\ell)}}\ddNTKT{\delta_1\delta_3\delta_4\delta_2}{\ell}\, \notag\\
&+\le(\CW{\ell+1}\ri)^2\bra \sigma^{\prime\prime}_{\delta_1} \sigma^{\prime\prime}_{\delta_2} \sigma^{\prime}_{\delta_3}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_1\delta_2}{\ell}\Ti{\NTKM}{\delta_1\delta_3}{\ell}\Ti{\NTKM}{\delta_2\delta_4}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\ri)^2\Ti{\NTKM}{\delta_1\delta_3}{\ell}\Ti{\NTKM}{\delta_2\delta_4}{\ell}\sum_{\delta_5,\ldots,\delta_8\in\D}\bra z_{\delta_5}\sigma^{\prime\prime}_{\delta_1} \sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\bra z_{\delta_6} \sigma^{\prime\prime}_{\delta_2}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\TI{G}{\delta_5\delta_7}{\ell}\TI{G}{\delta_6\delta_8}{\ell}\NTHF{\delta_7\delta_1\delta_8\delta_2}{\ell}\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\ri)^2\Ti{\NTKM}{\delta_2\delta_4}{\ell}\bra  \sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\le(\bra  \sigma^{\prime\prime\prime}_{\delta_2}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\dNTKQ{\delta_1\delta_3\delta_2\delta_2}{\ell}+\bra  \sigma^{\prime\prime}_{\delta_2}\sigma^{\prime\prime}_{\delta_4}\ket_{G^{(\ell)}}\dNTKQ{\delta_1\delta_3\delta_2\delta_4}{\ell}\ri)\, \notag\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\ri)^2\Ti{\NTKM}{\delta_1\delta_3}{\ell}\bra  \sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\le(\bra  \sigma^{\prime\prime\prime}_{\delta_1}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\dNTKQ{\delta_2\delta_4\delta_1\delta_1}{\ell}+\bra  \sigma^{\prime\prime}_{\delta_1}\sigma^{\prime\prime}_{\delta_3}\ket_{G^{(\ell)}}\dNTKQ{\delta_2\delta_4\delta_1\delta_3}{\ell}\ri)\, \notag\\
&+\o{\frac{1}{n}}\, .\notag
\end{align}
The tensor $\ddNTKU{}{\ell}$ satisfies the following layer-to-layer recursion:
\begin{align}\label{eq:U-recursion}\index{ddNTKs!statistics!U-recursion@$U$-recursion}
\ddNTKU{\delta_1\delta_4\delta_2\delta_3}{\ell+1}=&\le(\CW{\ell+1}\ri)^2\bra \sigma^{\prime\prime}_{\delta_1} \sigma^{\prime\prime}_{\delta_2} \sigma^{\prime}_{\delta_3}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\Ti{\NTKM}{\delta_1\delta_2}{\ell}\Ti{\NTKM}{\delta_1\delta_3}{\ell}\Ti{\NTKM}{\delta_2\delta_4}{\ell}\,\\
&+\le(\frac{n_{\ell}}{n_{\ell-1}}\ri)\le(\CW{\ell+1}\bra \sigma^{\prime}_{\delta_1}\sigma^{\prime}_{\delta_4}\ket_{G^{(\ell)}}\ri)\le(\CW{\ell+1}\bra \sigma^{\prime}_{\delta_2}\sigma^{\prime}_{\delta_3}\ket_{G^{(\ell)}}\ri)\ddNTKU{\delta_1\delta_4\delta_2\delta_3}{\ell}+\o{\frac{1}{n}}\, .\notag
\end{align}
