

\chapter{Information in Deep Learning}\label{app:mi-stuff}
\epigraph{What can we demand from any physical theory? \dots
nature
[is]
a difficult problem, but not a mystery for the human
mind. 
}{Ludwig Boltzmann\index{Boltzmann, Ludwig} 
\cite{boltzman-quote}
}


\index{macroscopic perspective}\index{microscopic perspective}
\noindent{}In our \emph{Initialization}\index{initialization (of you)}, \S\ref{ch:introduction}, we introduced our effective theory approach to understanding neural networks via the lens of theoretical physics. In particular, we discussed how thermodynamics was used to clarify the behavior of artificial machines such as the steam engine, and then we described how statistical mechanics was developed to explain how these macroscopic laws  arise from the statistical behavior of many microscopic elements. With this perspective,
we suggested that a similar framework might be applied to the difficult problem of deep learning theory, which we have now  demystified  from \emph{Pretraining}\index{pretraining} all the way to the \emph{End of Training}\index{end of training}, \S\ref{ch:tools} to \S\ref{ch:eot}.



In this first appendix, we'll make the connection between deep learning and these fields of physics even more detailed. To do so, we'll reformulate a few of our previous results in 
terms of 
\term{information theory}. Initially formalized by Shannon\index{Shannon, Claude} as a way of quantitatively understanding digital communication, information theory  was developed about half a century after statistical mechanics 
and is the statistical microscopic theory 
most fitting for the digital \terminate{Information Age}. 





Although they a priori consider very different subject matter, both statistical mechanics and information theory share a joint language and ultimate focus on 
the same main fundamental concept: \neo{entropy}.
A particularly nice organization of entropies defines the \neo{mutual information}, a positive quantity that can be used to characterize how much the measurement of one observable can inform us about another.
This gives a nice way to quantify the overall statistical dependence due to the \terminate{interactions} of random variables. As such, the first section of this appendix (\S\ref{sec:information-theory}) will give a self-contained introduction to entropy and mutual information, 
making sure to point out
the connections between the very physical and analog setting of statistical mechanics and the very abstract and 
digital world of information theory.

With these new tools, we will be able to further 
understand information in deep learning. In particular, for infinite-width neural networks (\S\ref{sec:information-infinite}) we'll find new perspectives on the non-interaction of neurons within a layer and on the necessity of \terminate{criticality} for 
preserving the information essential for distinguishing between input samples in deep networks.
Then, at finite width (\S\ref{sec:information-beyond-infinity}) we'll use a \neo{variational principle}\index{variational principle|seealso{maximum entropy, principle}} to demonstrate how the \emph{principle of maximum entropy}\index{maximum entropy, principle} for nearly-Gaussian distributions\index{nearly-Gaussian distribution} enables us to 
compute entropies and informations up to order $1/n^3$ while only needing to know the effective $\ell$-th-layer preactivation distribution 
truncated at order $1/n$. 

At order $1/n^2$, this will let us see a nonzero \terminate{mutual information} between groups of neurons in a layer that grows quadratically with depth, further quantifying the interaction of neurons at finite width and providing an information-theoretic interpretation and generalization of our \terminate{Hebbian learning} result from \S\ref{subsec:Hebbian}. 

At order $1/n^3$, we will see how to pick a depth-to-width ratio, $r\equiv L/n$, that maximizes the mutual information as a functional of
the activation function. This \terminate{optimal aspect ratio}, $r^\star$, arises from an unsupervised learning objective\index{unsupervised learning}, and defines an activation-function-dependent scale that separates \emph{effectively-deep}\index{effectively deep} networks -- that perform well -- from \emph{overly-deep}\index{overly deep} networks -- that are no longer trainable. 

Also at order $1/n^3$, a generalization of the \terminate{mutual information} for three groups of neurons will show that the information in a finite-width layer is stored redundantly\index{redundancy (information theory)} between neurons. 
This analysis 
provides a new perspective on coarse-graining mechanism of $\emph{RG flow}$\index{representation group flow}
by enabling us to understand how the information from inputs gets represented by, and shared among, the deeper-layer neurons.

Finally, note that while most of the computations presented here focus on the prior distribution\index{prior} of preactivations as a means of investigating the \terminate{inductive bias} of the network architecture and activation function, these information-theoretic tools naturally extend to the various joint distributions we've considered throughout the book as well as the Bayesian posterior distribution and the complicated distributions of fully-trained networks. This leaves many more things to be computed, and so we hope that this chapter provides a useful introduction to a new toolkit that can be used for furthering your understanding of deep learning. 
In our second and final appendix, \S\ref{app:residual}, we'll give a \emph{residual} example 
to demonstrate this
in the setting of \emph{residual networks}\index{residual network}.


























\section{Entropy and Mutual Information}\label{sec:information-theory}
In this section we give a very brief overview of the concepts of entropy and \terminate{mutual information} that play essential roles both in %
\emph{statistical mechanics}\index{statistical physics} \cite{boltzmann}
and 
\neo{information theory} \cite{Shannon-1,Shannon-2}.



Let's start with a discrete random variable $x\in\outcomes$ 
governed by the \terminate{probability distribution} $p(x)$.
For this discussion, it is nice to think of $x$ as a particular observable\index{observable} outcome, and $\outcomes$ as the set of possible outcomes.
The \term{entropy} of a \terminate{probability distribution} is given by 
\be\label{eq:entropy-shannon-formula-sum}
\entropy\!\le[p(x)\ri]
\equiv - \sum_{x \in \outcomes} p(x) \log p(x)  
\, ,
\ee
which is a \neo{functional} of the distribution, taking a
distribution as an argument and outputting a number. Thus, we should think of the entropy as a property of an entire probability distribution.


To gain some intuition for why this quantity could be useful, let's consider its two extremes. First, when the distribution is perfectly \emph{ordered} -- that is, $p(x)=\delta_{xx'}$ such that $p(x')=1$ with \neo{absolute certainty} for a particular outcome, $x'\in \outcomes$, and zero for all others 
-- then the entropy is minimal, given by 
\be\label{eq:entropy-minimal}
\entropy\!\le[p(x)\ri]
= - \sum_{x \in \outcomes} \delta_{x x'} \log \!\le(\delta_{x x'}\ri)
=0 \, ,
\ee 
since $x'$ contributes $1 \log(1)=0$ and the other values of $x$ contribute $0\log(0)=0$.
Second, when the distribution is completely \emph{disordered} -- that is,  $p(x)=1/\vert\outcomes\vert$, such that the possible outcomes $x$ are distributed uniformly and no outcome is more likely than any other -- then entropy is
maximal, given by
\be\label{eq:entropy-maximal}
\entropy\!\le[p(x)\ri]
= - \sum_{x \in \outcomes} \frac{1}{\vert\outcomes\vert} \log \!\le(\frac{1}{\vert\outcomes\vert} \ri)
=\log(\vert\outcomes\vert) \, .
\ee
For this reason, in 
\terminate{physics}
the \terminate{entropy} is often regarded as a measure of the \textbf{disorder}\index{entropy!as a measure of disorder}\index{disorder|see{entropy}} of a system characterized by a random variable $x$.\footnote{Since we're currently wearing our physicist hats, one remark is in (dis)order for the units of the entropy. As per our discussion of  \neo{dimensional analysis} in footnote~\ref{foot:dimensional-analysis} of \S\ref{sec:perturbation}, the logarithm of a probability has the same units as the \neo{action}, and for the same reason must be dimensionless. Nevertheless, by changing the base of the logarithm, we can change the multiplicative constant in front of \eqref{eq:entropy-shannon-formula-sum}, and thus change the meaning of the entropy:
\bi
\item With our physicist\index{physics} hats\index{hat (occupational)} still on, we would use the \terminate{natural logarithm} with base $e$, which measures entropy in units with a very silly name called \emph{nats}\index{nat@\texttt{nat} (unit of entropy)}. (Some physicists also multiply the expression~\eqref{eq:entropy-shannon-formula-sum} by the \terminate{Boltzmann constant} $k_{\text{B}}$, as is natural in macroscopic applications of the entropy in \neo{thermodynamics}, which gives the entropy the unit of \emph{joules per kelvin}\index{joules per kelvin@\texttt{joules per kelvin} (unit of entropy)}.)
\item With our computer scientist hats on, which we put on in anticipation of the next paragraph in the main text, we would use the logarithm with base $2$, which measures units in \emph{binary digits} or the hopefully familiar \textbf{bits}\index{bit@\texttt{bit} (unit of entropy)}. This is most natural in an information theory context, for which the entropy~\eqref{eq:entropy-shannon-formula-sum} is sometimes called the \emph{Shannon entropy}.\index{Shannon entropy|see{entropy}}\index{entropy!Shannon entropy} The reason bits are also used as the units for computer memory is due to the counting-of-states intuition for the entropy: a \terminate{hard drive} that can store $10^9$ $\texttt{bits}$ has $2^{10^9}$ unique states, with a priori equal plausibility for any particular arrangement of those bits, i.e.~for any particular state of the system.
\ei
In this chapter, we'll use the natural base $e$, which is also very natural when studying the entropy of Gaussian distributions\index{Gaussian distribution} and nearly-Gaussian distributions\index{nearly-Gaussian distribution}.\label{footnote-entropy-dimensions-nats-vs-bits}
}
In this maximal case when each outcome is equally likely, the entropy can also be interpreted as a way of counting the number of states of a system -- i.e.~the number of distinct outcomes in $\outcomes$ -- since it's equal to the logarithm of such a count.\footnote{This connection between the uniform distribution over a finite number of states and the maximum of the entropy is an example of the \emph{principle of maximum entropy}\index{maximum entropy, principle} and descends from what's called Laplace's \emph{principle of indifference}\index{Laplace's principle of indifference}: without any other information, all the a priori probabilities 
for a system to be in any particular state should be equal.
(As we will discuss in \S\ref{sec:information-beyond-infinity}, when we do have some information about the state, then the entropy is no longer maximized by a uniform distribution.)

\index{macroscopic perspective}\index{microscopic perspective}
Note that this indifference principle is applied to macroscopic thermodynamic states of (nearly-)equal energy as a principal assumption of microscopic statistical mechanics, and the associated entropy, \eqref{eq:entropy-maximal}, is sometimes called the \emph{Boltzmann entropy}.\index{Boltzmann entropy|see{entropy}}\index{entropy!Boltzmann entropy}
In contrast, the fully-general formula \eqref{eq:entropy-shannon-formula-sum}   is sometimes called the \emph{Gibbs entropy} in the context of statistical mechanics\index{statistical physics}.\index{Gibbs entropy|see{entropy}}\index{entropy!Gibbs entropy} %


Finally, for a Bayesian, this principle of indifference offers a natural way to pick a set of prior beliefs, and a prior distribution that respects this indifference principle is sometimes called a \emph{non-informative prior}\index{non-informative prior|see{prior}}\index{prior!non-information prior}\index{prior!non-information prior|seealso{Laplace's principle of indifference}}. While motivated in part by the \terminate{Occam's razor} heuristic, a Bayesian's \emph{subjective} adoption of such a prior  is very different from the automatic and objective embodiment of \terminate{Occam's razor} by the Bayes' factor\index{Bayesian inference!model comparison!Bayes' factor}, cf.~our discussion of Bayesian model comparison\index{Bayesian inference!model comparison} in~\S\ref{subsec:bayesian-model-comparison}.
}




To gain even more intuition, let's consider a perspective from \terminate{information theory}.
In the \terminate{entropy} formula \eqref{eq:entropy-shannon-formula-sum}, the quantity inside the expectation
is called the \textbf{surprisal}\index{surprisal (information theory)|textbf}: 
\be\label{eq:surprisal-def}
\surprise(x) \equiv - \log p(x)=\log\!\le[\frac{1}{p(x)}\ri] \,;
\ee
since for a particular outcome, $x$, the distribution $p(x)$ quantifies the frequency or plausibility of observing $x$, and since the logarithm is monotonic in its argument, the surprisal
grows with the a priori rareness of the outcome $x$, and hence quantifies how surprising or informative actually observing a particular $x$ is.
As such, the  surprisal is also a quantitative measure of how much new \term{information}\index{information|seealso{surprisal}} is gained after making such an observation. 
Averaging the surprisal over all possible outcomes, $\outcomes$, gives the \terminate{entropy}~\eqref{eq:entropy-shannon-formula-sum}, which can thus be understood as the expected surprise or amount of information to be gained from making an observation: 
\be\label{eq:entropy-as-expectation-of-surprisal}
\entropy\!\le[p(x)\ri] \equiv \E{\surprise(x)}\, .
\ee




In addition to admitting various nice interpretations, the \terminate{entropy} also obeys a few nice mathematical properties.
To start, it is manifestly nonnegative:
\be\label{eq:entropy-positivity}
\entropy\!\le[p(x)\ri]
\geq 0\, .
\ee
You can see this by noting that the allowed range of a probability, $0 \leq p(x) \leq 1$, implies that the nonnegativity of the quantity, $-p(x)\log p(x) \geq 0$, which in turn implies that the entropy \eqref{eq:entropy-shannon-formula-sum} is a sum of nonnegative terms. Moreover, the \terminate{entropy} takes its minimum value and vanishes, \eqref{eq:entropy-minimal}, if and only if the distribution is perfectly ordered, given by a \terminate{Kronecker delta} for one particular outcome.

\index{statistical independence}\index{statistical dependence}
Another important property of the \terminate{entropy} is its  \emph{additivity}\index{entropy!additivity} for two random variables $x \in \mathcal{X}$ and $y \in \mathcal{Y}$ that are described by a factorized joint distribution, $p(x,y)=p(x)\,p(y)$, and thus are statistically independent~\eqref{eq:independence-random-variables}:
\begin{align}
\entropy\!\le[p(x,y)\ri] &= - \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x, y) \notag \\
&=  - \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x) p(y) \le[ \log p(x) + \log p(y)\ri] \notag \\
&= - \sum_{x \in \mathcal{X}} p(x) \log p(x) - \sum_{y \in \mathcal{Y}} p(y) \log p(y) \notag \\
&= \entropy\!\le[p(x)\ri] +\entropy\!\le[p(y)\ri] \, .
\label{eq:entropy-additivity}
\end{align}
Intuitively, if two observables are independent, then the expected amount of \terminate{information} learned from making an observation of each is just the sum of the \terminate{information} learned from making each individual observation.
For macroscopic systems with small probabilities for individual outcomes, this makes the entropy a practically useful quantity to work with: while the probability of independent outcomes multiply and create smaller and smaller numbers, the surprisals, and thus the entropies, simply add.

The additivity property \eqref{eq:entropy-additivity} has a very physical interpretation: the entropy is typically an \textbf{extensive}\index{extensivity!of entropy|textbf} quantity, meaning that it scales with the number of \term{degrees of freedom} or microscopic size of the system.
This should make sense given the counting-of-states interpretation of the entropy: if a variable $x$ describes the potential contents of a $1~\texttt{TB}$ hard drive, and the variable $y$ independently describes the potential contents of another $1~\texttt{TB}$ hard drive, then the total storage capacity of the hard drives together is additive and equal to $2~\texttt{TB}$. As the number of states available to the combined ``hard drive'' system is the product of the states of the individual ``hard drive'' systems, their entropies are additive.

However, if the hard drives are not independent and \emph{constrained} so that one hard drive mirrors the other, then their total storage capacity is \emph{subadditive} and instead would be equal to $1~\texttt{TB}$.\footnote{
    This configuration has a practical realization called \emph{RAID 1}, where ``\terminate{RAID}\index{RAID|seealso{Redundant Array of Inexpensive Disks}}'' stands for \neo{Redundant Array of Inexpensive Disks}\index{Redundant Array of Inexpensive Disks|seealso{RAID}}, and allows for fault tolerance by creating data redundancy between the two hard drives.
}
In this case, the system had only half as many degrees of freedom as we naively thought it had due to the strict constraint creating strong correlations between the contents of the hard drives.









\index{statistical dependence}
Thus, more generally for two\index{observable} statistically \emph{dependent} observables 
constrained by
a nonzero interaction between them, the \terminate{entropy} 
obeys a property called \emph{subadditivity}\index{entropy!additivity!subadditivity}:
\be\label{eq:entropy-subadditivity}
\entropy\!\le[p(x,y)\ri] <  \entropy\!\le[p(x)\ri] +\entropy\!\le[p(y)\ri] \, .
\ee
In words, this means that the entropy of a joint distribution $p(x,y)$ will be always less than or equal to the sum of the entropies of the marginal distributions $p(x)$ and $p(y)$.
This is also physically intuitive: if two random variables are statistically dependent, then an observation of $x$ conveys information about the likely outcome of making an observation of $y$, and so an observation of $y$ doesn't convey as much information as it would have if we didn't already know $x$.\footnote{For the mathematically curious, we can turn this intuition into a quick proof. The usual route is to first prove the \neo{Jensen inequality} and then apply it to an auxiliary object called the  \textbf{Kullback–Leibler (KL) divergence}\index{Kullback–Leibler divergence}\index{KL divergence|see{Kullback–Leibler divergence}}.

First let's state and then prove the \terminate{Jensen inequality}. Consider a discrete probability distribution over $N$ elements $a_{\mu=1,\ldots,N}$ 
with corresponding probabilities $p_\mu$
such that $\sum_{\mu=1}^{N} p_\mu=1$. Next, consider a convex functions\index{convex function} $f(a)$, i.e.~a function that satisfies
\be\label{eq:convexity-def}
f\big(\lambda a_1 + (1-\lambda)a_2\big)\geq \lambda f(a_1) +(1-\lambda)f(a_2)\, ,
\ee 
for any $\lambda\in[0,1]$ and for any numbers $a_{1}$ and $a_2$ in the domain of the function.
The \terminate{Jensen inequality} states that the expectation of such a function is greater than or equal to the function applied to the mean:
\be\label{eq:jensen-def}
\E{f(a)} \geq f\!\le(\E{a}\ri) \,.
\ee
This can be proved by induction on $N$ as follows. First, note that \eqref{eq:jensen-def} holds trivially when $N=1$. Then, see that
\begin{align}
\E{f(a)} \equiv\sum_{\mu=1}^{N+1} p_{\mu} f(a_{\mu})=&p_{N+1} f(a_{N+1})+\le(1-p_{N+1} \ri)\sum_{\mu=1}^{N} \frac{p_{\mu}}{1-p_{N+1} } f(a_{\mu})\, \\
\geq&p_{N+1} f(a_{N+1}) +\le(1-p_{N+1} \ri) f\!\le(\sum_{\mu=1}^{N} \frac{p_{\mu}}{1-p_{N+1}}a_{\mu}\ri)\, \notag\\
\geq&f\!\le(p_{N+1}a_{N+1}+\le(1-p_{N+1}\ri)\sum_{\mu=1}^{N} \frac{p_{\mu}}{1-p_{N+1}}a_{\mu}\ri)=f\!\le(\E{a}\ri) \, ,\notag
\end{align}
where in going from the first line to the second line we used the \terminate{Jensen inequality} \eqref{eq:jensen-def} for $N$ elements, and in going from the second line to the third line we used the convexity of the function \eqref{eq:convexity-def}.




As the next step, let us introduce the KL divergence\index{Kullback–Leibler divergence} (sometimes known as the \emph{relative entropy}\index{relative entropy|see{Kullback–Leibler divergence}}) between two \emph{different} probability distributions $p(x)$ and $q(x)$ for a discrete variable, $x \in \outcomes$,
\be\label{eq:KL-divergence-def}
KL\le[ p(x) \, ||\, q(x) \ri] \equiv\sum_{x \in \outcomes} p(x) \log \!\le[\frac{p(x)}{q(x)} \ri] 
= -\entropy\!\le[p(x)\ri]+\entropy\!\le[p(x), q(x)\ri]
\, ,
\ee
which is a multi-function \terminate{functional} of both distributions, $p(x)$ and $q(x)$, and importantly is \emph{not} symmetric in its function arguments. Here $\entropy\!\le[p(x), q(x)\ri]\equiv - \sum_{x \in \outcomes} p(x) \log q(x) $ is an asymmetric quantity known as the \neo{cross entropy}. As first mentioned in footnote~\ref{footnote:KL} of \S\ref{subsec:cross-entropy}, the KL divergence is a asymmetric measure of the closeness of two different distributions and is closely related to the \emph{cross-entropy loss}\index{loss!cross-entropy} \eqref{eq:loss-cross-entropy}.


Finally, let's show that the KL divergence is nonnegative by applying the \terminate{Jensen inequality} to the convex function $f(a)=-\log(a)$ with discrete elements $a_\mu=q(x)/p(x)$:
\begin{align}\label{eq:KL-proof}
KL\le[ p(x) \, ||\, q(x) \ri]\equiv\sum_{x\in \outcomes} p(x) \log\!\le[\frac{p(x)}{q(x)} \ri]\geq - \log\!\le[\sum_{x\in \outcomes} p(x)\frac{q(x)}{p(x)} \ri]
=-\log(1)=0\, .
\end{align}
To complete our proof, note that the positivity of the KL divergence \eqref{eq:KL-proof} implies the subadditivty\index{entropy!additivity!subadditivity}  of the entropy \eqref{eq:entropy-subadditivity} for a choice of distributions as $KL\le[ p(x,y) \, ||\, p(x)\,p(y) \ri]$.
} %




\index{entropy!additivity!subadditivity}\index{entropy!additivity}\index{statistical independence}\index{interactions}
Turning this argument upside down and shuffling~\eqref{eq:entropy-subadditivity} leftside right, let us define the \term{mutual information} between two random variables as:
\begin{align}\label{eq:mutual-information}
\MI\le[p(x,y)\ri] &\equiv \entropy\!\le[p(x)\ri] +\entropy\!\le[p(y)\ri]-\entropy\!\le[p(x,y)\ri]\, \\
&= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x,y) \log \!\le[ \frac{p(x,y)}{p(x)\, p(y)} \ri] \, . \notag
\end{align}
This is a \terminate{functional} of a joint probability distribution and gives an average measure of how much information an observation of $x\in\mathcal{X}$ conveys about an observation of $y\in\mathcal{Y}$, and vice versa.
Rearranged in this way, we see that the subadditivity\index{entropy!additivity!subadditivity} of the \terminate{entropy}~\eqref{eq:entropy-subadditivity} implies the nonnegativity of the mutual information,
\be\label{eq:positivity-of-mutual-information}
\MI\le[p(x,y)\ri]   \ge 0\, ,
\ee
with equality holding when and only when the sets of observable outcomes, $\mathcal{X}$ and $\mathcal{Y}$, are statistically independent.\footnote{Note that the mutual information \eqref{eq:mutual-information} can also be thought of as the KL divergence\index{Kullback–Leibler divergence} \eqref{eq:KL-divergence-def} between the joint distribution $p(x,y)$ and the product of the marginal distributions $p(x)\,p(y)$.
That is, the mutual information $\MI\le[p(x,y)\ri]$ tells us how close a joint distribution is to being a product of independent distributions, with a nonzero mutual information signaling statistical dependence.
}
Thus, the mutual information of a joint distribution is telling us something about the \neo{interactions} that create the nontrivial correlations that are a signature of statistical dependence.
We'll compute explicitly the \terminate{mutual information} of preactivations for infinite-width neural networks in \S\ref{sec:information-infinite} and for finite-width neural networks in \S\ref{sec:information-beyond-infinity}: as you might imagine, 
the 
interaction\index{interactions} of neurons at finite width will lead to a nonzero mutual information. 

As the last preparation before such computations,  we will need to extend the definition of the \terminate{entropy}~\eqref{eq:entropy-shannon-formula-sum} from discrete outcomes to continuous random variables as
\be\label{eq:entropy-shannon-formula-integral}
\entropy\!\le[ p(x)\ri] \equiv - \int \!dx \ p(x)\, \log p(x)  = \E{s(x)} \, .
\ee
While the entropy is still the expectation of the surprisal\index{surprisal (information theory)} \eqref{eq:surprisal-def}, to take that expectation we now have to evaluate an integral rather than compute a sum.
As passing to the continuum actually involves a physically interesting subtlety, let's take a few paragraphs to unravel this definition \eqref{eq:entropy-shannon-formula-integral}.


First, let's understand this subtlety mathematically. Since our random variable is continuous, we can make a smooth and monotonic change of coordinates in our continuous \neo{outcome space} $\outcomes$ from $x$ to $\widetilde{x}\equiv\widetilde{x}(x)$. Since such coordinates are arbitrary, consistency requires that the probability of observing $x$ in the interval between $[x_1,x_2]$ be the same as the probability of observing $\widetilde{x}$ in $[\widetilde{x}(x_1),\widetilde{x}(x_2)]$.
In equations, this means that
\be
p(x_1 < x < x_2) \equiv \int_{x_1}^{x_2} dx\ p(x) \, 
\ee
must equal
\begin{align}
p(\widetilde{x}_1 < \widetilde x < \widetilde{x}_2)\equiv\int_{\widetilde{x}(x_1)}^{\widetilde{x}(x_2)} d\widetilde{x}\ p(\widetilde{x})=\int_{x_1}^{x_2} dx\  \frac{d\widetilde{x}}{dx}\ p\Big(\widetilde{x}(x)\Big)\, ,
\end{align}
where in the last step we used the standard transformation property of the integral measure under a change of coordinates.
Thus, the two distributions $p(\widetilde{x})$ and $p(x)$ must be related as
\be\label{eq:probability-coordinate-change}
p\Big(\widetilde{x}(x)\Big)\equiv\frac{dx}{d\widetilde{x}}\ p(x)\, ,
\ee
which is the well-known coordinate transformation formula for a probability density of a single variable. What about the expected surprisal? 
Under this same coordinate transformation, the entropy \eqref{eq:entropy-shannon-formula-integral} is given by
\begin{align}\label{eq:entropy-shift-coordinate}
\entropy\!\le[ p(\widetilde{x})\ri]=&-\int\! d\widetilde{x}\ p(\widetilde{x})\, \log p(\widetilde{x}) \, \\
=& -\int dx \, \frac{d\widetilde{x}}{dx}\ \frac{dx}{d\widetilde{x}}\ p(x)\, \le[\log p(x) +\log\!\le(\frac{dx}{d\widetilde{x}}\ri) \ri]\, \notag\\
=& \entropy\!\le[ p(x)\ri]+\int dx \ p(x)\, \log\!\le(\frac{d\widetilde{x}}{dx}\ri)\, ,\notag
\end{align}
where in the second line we again used the standard transformation property of the integral measure under a change of coordinates and we also used \eqref{eq:probability-coordinate-change} to express the probability distribution in the old coordinates. 
So with a general (monotonic) change of coordinates, we can make the entropy take any value we'd like by a judicious choice of the \terminate{Jacobian} of the transformation, $\td \widetilde{x} / \td x$, even a negative one!



Now, let's understand the physical meaning of this subtlety. Let's consider a coordinate change that's just a multiplicative factor, $\widetilde{x}=c x$, which is like changing the base unit that we use to measure the observable $x$ from $\texttt{inches}$ to $\texttt{meters}$. In this case, the surprisal of each particular outcome shifts by the same constant, $\Delta s(x) = \log c$, and thus so does the entropy. Thus, for continuous quantities the entropy is additively sensitive to the choice of units with which we measure our observable quantities, and we really should specify these units along with the definition of the entropy \eqref{eq:entropy-shannon-formula-integral}.\footnote{For any \emph{dimensionful}\index{dimensional analysis} observable, the probability \emph{density}\index{probability distribution!as a density} $p(x)$ must also be dimensionful so that the probability of observing $x$ in the interval $[x_1,x_2]$,  $p(x_1 < x < x_2) \equiv \int_{x_1}^{x_2} dx\ p(x)$, is properly dimensionless. Yet, putting such a dimensionful object $p(x)$ into an argument of the logarithm as $\log p(x)$ is illegitimate, as we discussed in footnote~\ref{foot:dimensional-analysis} of \S\ref{ch:tools}. This is another way of seeing that for continuous probability distributions, i.e.~probability densities, we need to specify the measuring units to properly define the entropy. 

For the curious and potentially worried reader, it should be noted here that the \emph{Bayes' factor}\index{Bayesian inference!model comparison!Bayes' factor}~\eqref{eq:bayes-factor} that contained the observation dependence of our Bayesian model comparison\index{Bayesian inference!model comparison} is invariant under a coordinate transformation of our observations $y_{\A}$. What ultimately mattered there was the relative magnitude of the \emph{evidence}\index{Bayesian inference!evidence} of different hypotheses, and not the absolute value of the individual evidences.

Also, please do not confuse this issue of the units for a probability density, which change the entropy by an additive constant, with the units of entropy that we discussed in footnote~\ref{footnote-entropy-dimensions-nats-vs-bits}, which change the entropy by a multiplicative constant. Note that for discrete probability distributions, the probability distribution gives a simple probability and is already dimensionless, cf.~our brief discussion in footnote~\ref{footnote-entropy-dimensions-nats-vs-bits}.
}


Perhaps the most sensible choice is to set the measuring units according to the smallest possible measurements of $x$ that we can physically distinguish -- in other words, according to the precision limit set by the constraints of the physical world. 
This precision limit $\epsilon$ is sometimes called a \emph{cutoff}, and in practice means that we only care about the discrete probability of finding $x$ between $[x_0, x_0+\epsilon]$.\footnote{
    Please don't confuse our \emph{measurement precision cutoff}\index{measurement precision cutoff|see{cutoff}}\index{cutoff, effective theory!vs.~measurement precision} here with the \emph{perturbative cutoff}\index{perturbative cutoff|see{cutoff}} of the \terminate{effective theory}, the depth-to-width ratio $r \equiv L/n$. While in both cases we can think of them as important scales, they have very different physical meanings: in the former case, the precision cutoff gives the minimum distinguishable difference between measurements of two observables, $\epsilon \equiv \min(|z_2-z_1|)$; in the latter case, the cutoff of the effective theory, $L/n$, sets the scale at which finite-width corrections need to be taken into account in the preactivation distribution $p(z)$.     
}
Such a discretization of the \terminate{outcome space} will then ensure that the \terminate{entropy} is always positive, \eqref{eq:entropy-positivity}, since we're now dealing with discrete probabilities again.\footnote{
    In the context of deep learning, a natural choice is the precision limit of the floating-point representation of the network's variables. Since type \texttt{float}\index{float@\texttt{float}|see{type (data)}}\index{type (data)!floating-point precision} eponymously has a precision that's relative to the value being stored, one could perhaps choose the minimum precision in the relevant range.
}











If this physical sensitivity of the \terminate{entropy} to the somewhat arbitrary cutoff\index{cutoff, effective theory} bothers you, then perhaps you should consider the continuous analog of \terminate{mutual information},  
\begin{align}\label{eq:mutual-information-continuous}
\MI\le[p(x,y)\ri] &\equiv \entropy\!\le[p(x)\ri] +\entropy\!\le[p(y)\ri]-\entropy\!\le[p(x,y)\ri]\, \\
&= \int \! dx dy \ p(x,y) \, \log \!\le[ \frac{p(x,y)}{p(x)\, p(y)} \ri] \, , \notag
\end{align}
where the definition in terms of the entropy is the same as in the discrete case, \eqref{eq:mutual-information}.
In particular,  mutual information is insensitive to the choice of the measuring coordinates. To see why, let's consider  two continuous random variables $x$ and $y$ and independent monotonic coordinate transformations,
\be\label{eq:MI-invariant-transformations}
p\Big(\widetilde{x}(x)\Big)=\frac{dx}{d\widetilde{x}}\ p(x)\, ,\qquad p\Big(\widetilde{y}(y)\Big)=\frac{dy}{d\widetilde{y}}\ p(y)\, , \qquad p\Big(\widetilde{x}(x), \widetilde{y}(y)\Big)=\frac{dx}{d\widetilde{x}}\frac{dy}{d\widetilde{y}}\ p(x, y)\, ,
\ee
where to get this we applied a similar consistency-of-probabilities argument to the one that we gave above.
With this in mind, we can now show that the \terminate{mutual information} \eqref{eq:mutual-information-continuous}  stays invariant under these coordinate transformations:
\begin{align}\label{eq:mutual-information-invariant}
\MI\le[p(\widetilde{x}, \widetilde{y})\ri] &\equiv \entropy\!\le[p(\widetilde{x})\ri] +\entropy\!\le[p(\widetilde{y})\ri]-\entropy\!\le[p(\widetilde{x},\widetilde{y})\ri]\, \\
&= \int \! d\widetilde{x} d\widetilde{y} \ p(\widetilde{x},\widetilde{y}) \,\log\!\le[ \frac{p(\widetilde{x},\widetilde{y})}{p(\widetilde{x})\, p(\widetilde{y})} \ri] \, \notag\\
&=\int\!  dx dy\ p(x,y) \,\log\!\le[ \frac{p(x,y)}{p(x)\, p(y)} \ri]\equiv\MI\le[p(x,y)\ri]\, .\notag
\end{align}
In going from the second line to the third line, the coordinate transformation factors $ dx / d\widetilde{x}$ and $dy/d\widetilde{y}$ completely cancelled inside the logarithm, and the transformation of the measure cancelled the coordinate transformation  factors outside the logarithm.
Thus, the \terminate{mutual information} is completely well defined for continuous random variables, independent of the cutoff\index{cutoff, effective theory} $\epsilon$. For this reason, with a consistent and fixed choice of units, it's completely valid to compute the entropy as an intermediate step in the computation of the mutual information, and we will make use of this fact in the following sections.



Finally, note that the notion of the \terminate{mutual information} can be extended to more than two random variables. 
For instance, in \S\ref{sec:information-beyond-infinity} we will consider the \term{tripartite information} among three random variables $x\in\mathcal{X}$, $y\in\mathcal{Y}$, and $z\in\mathcal{Z}$:
\begin{align}\label{eq:TI-definition}
&\MI_{3}\le[p(x,y,z)\ri]\, \\
\equiv&\MI\le[p(x, y)\ri]-\MI\le[p(x, y| z)\ri] \, \notag\\
= & \entropy\!\le[p(x)\ri]+  \entropy\!\le[p(y)\ri] + \entropy\!\le[p(z)\ri] -  \entropy\!\le[p(x,y)\ri]-  \entropy\!\le[p(y,z)\ri]  -   \entropy\!\le[p(z,x)\ri] +   \entropy\!\le[p(x,y,z)\ri]\, \notag\\
=&\sum_{x \in \mathcal{X}, y \in \mathcal{Y}, z\in\mathcal{Z}} p(x,y,z)\, \log \!\le[ \frac{p(x,y) \, p(y,z)\, p(z,x)}{p(x)\, p(y)\, p(z)\, p(x,y,z)} \ri] \, .\notag
\end{align}
Here, $\MI\le[p(x, y| z)\ri]$ is the mutual information of the joint distribution between $x$ and $y$ conditioned on $z$, $p(x, y| z)$, and the final expression for $\MI_{3}\le[p(x,y,z)\ri]$ makes it clear that \emph{(a)} it is fully symmetric in its three arguments, and that \emph{(b)} its continuous analog that's in your imagination is cutoff independent and invariant under similar coordinate transformations as those in \eqref{eq:MI-invariant-transformations}.

What is not immediately obvious is that tripartite information can be either positive or negative. 
From the first expression in \eqref{eq:TI-definition}, we can gain some intuition for the meaning of the tripartite information: it is a measure of whether knowledge of a third random variable, $z$ in this way of writing the expression,
 increases or decreases the mutual information between the other two variables. When positive, it indicates that $z$ contains information about $x$ and $y$, and so knowing $z$ decreases the amount of information you'd learn about $x$ by measuring $y$; the information is stored \emph{redundantly}\index{redundancy (information theory)} between these three variables. When negative, it indicates that the information is distributed across $x$, $y$, and $z$ in such a way that you'd learn less about $x$ by measuring $y$ than you would with first knowing $z$; in this case, the information is stored \emph{synergistically}\index{synergy (information theory)} between these three variables.\footnote{
An extreme example of positive tripartite information occurs when $x$, $y$, and $z$ are completely dependent and exact copies of each other. In this redundant case, $\MI\le[p(x, y)\ri]>0$, since knowledge of  $y$ tells you everything about $x$, but $\MI\le[p(x, y| z)\ri]=0$, since conditioning on $z$ means that there's nothing left for you to learn about $x$ by also observing $y$. An example of such a situation would be three copies of the same book.

An extreme example of negative tripartite information occurs when the joint distribution between $x$ and $y$ factorizes, $p(x,y)=p(x) \, p(y)$, but joint distribution conditioned on $z$ does not, $p(x, y|z) \neq p(x|z)\,p(y|z)$.  In this synergistic case,
$\MI\le[p(x, y)\ri]=0$, since without $z$ the variables are statistically independent, but $\MI\le[p(x, y| z)\ri]>0$, since there are correlations that are mediated by $z$. An example of such a situation could be a code that distributes a key among three different parties: knowledge of any two parts would give absolutely no information about the key, but with all three parts together the key can be reconstructed.}








\section{Information at Infinite Width: Criticality}\label{sec:information-infinite} 
With that informative introduction out of the way, let's us now make these abstract definitions concrete by using them to better understand the neural-network prior distribution\index{prior}.


To begin, let's focus on $m$ preactivations $\z{i}{\alpha}{\ell}$ from the $\ell$-th layer of an infinite-width neural network. Depending on when you're coming to this appendix from the main text, it's probably ingrained in your mind already that such preactivations are distributed according to a zero-mean Gaussian distribution\index{Gaussian distribution!entropy}  
\be\label{eq:GP-for-once-more}
p\!\le(z_{1},\ldots,z_{m}\Big\vert \D\ri) = \frac{1}{\sqrt{\dete{2\pi \ker}^{m}}} \exp\!\le(-\frac{1}{2}\sum_{i=1}^{m}\sum_{\alpha_1,\alpha_2\in\D}\ker^{\alpha_1 \alpha_2}z_{i;\alpha_1}z_{i;\alpha_2}\ri)\, ,
\ee
where in this expression, and in this section, we will drop \terminate{layer indices} everywhere to declutter expressions. 
The \terminate{entropy}~\eqref{eq:entropy-shannon-formula-integral} 
of this distribution is then given by
\begin{align}\label{eq:entropy-preactivation-infinite-width-multi}
\entropy\!\le[p\!\le(z_{1},\ldots,z_{m}\Big\vert \D\ri)\ri]=&\bra\!\!\!\bra\frac{1}{2}\sum_{i=1}^{m}\sum_{\alpha_1,\alpha_2\in\D}\ker^{\alpha_1 \alpha_2}z_{i;\alpha_1}z_{i;\alpha_2}\ket\!\!\!\ket_{\ker}+\log\!\le(\sqrt{\dete{2\pi \ker}^{m}}\ri)\, \\
=&\frac{m}{2} \le(\ND+\log \dete{2\pi \ker} \ri)=\frac{m}{2} \log\dete{2e\pi \ker}\, ,\notag
\end{align}
where as a reminder $\dete{ 2\pi e \ker }$ is the determinant of the $\ND$-by-$\ND$ matrix $2\pi e\ker_{\alpha_1\alpha_2}$.\footnote{In this and the next section, for convenience we will ignore the ambiguity in the definition of the continuous \terminate{entropy} and use the formula \eqref{eq:entropy-shannon-formula-integral} naively. This is permissible since we ultimately are interested in cutoff-independent quantities, such as the \terminate{mutual information}, and when interpreting an entropy such as \eqref{eq:entropy-preactivation-infinite-width-multi} we will never care about its absolute value.
}
From this we conclude that entropy is \emph{extensive}\index{extensivity!of entropy}, proportional to the number of neurons $m$ in the joint distribution \eqref{eq:GP-for-once-more}. This shows how the entropy can count the \neo{degrees of freedom} in a deep learning context -- in this case, by counting the neurons -- and the exact additivity in the number of neurons signals to us that the individual neurons in an infinite-width layer are non-interacting and statistically independent.\index{statistical independence}



To confirm this directly, let's work out the \terminate{mutual information} between two sets of neurons, $\mathcal{M}_1=\le\{1,\ldots,m_1\ri\}$ and $\mathcal{M}_2=\le\{m_1+1,\ldots,m_1+m_2\ri\}$, in the same layer $\ell$. In excruciating detail for its triviality, 
we have
\begin{align}\label{eq:MIT-zeroth-order}
&\MI\le[p\!\le(\mathcal{M}_1, \mathcal{M}_2\Big\vert \D\ri)\ri]\, \\
=&\entropy\!\le[p\!\le(z_{1},\ldots,z_{m_1}\Big\vert \D\ri)\ri]+\entropy\!\le[p\!\le(z_{m_1+1},\ldots,z_{m_1+m_2}\Big\vert \D\ri)\ri]-\entropy\!\le[p\!\le(z_{1},\ldots,z_{m_1+m_2}\Big\vert \D\ri)\ri]\, \notag\\
=&\le[m_1+m_2-(m_1+m_2)\ri]\frac{1}{2}\log\dete{2e\pi \ker}=0\, .\notag
\end{align}
where to go to the last line we used \eqref{eq:entropy-preactivation-infinite-width-multi} three different ways.
This zero mutual information confirms that at infinite width 
learning the activities of some neurons in a layer conveys no information about the activities of any of the other neurons.
To find a finite result, we'll have to back off the \terminate{infinite-width limit} (\S\ref{sec:information-beyond-infinity}).



That said, for a fixed neuron we do expect nontrivial correlations between different inputs and therefore also a finite mutual information.
To investigate this, let's take two inputs $\x{i}{+}$ and $\x{i}{-}$ and compute the \terminate{mutual information} between the preactivations  for a particular neuron, $z_{1;+}$ and $z_{1;-}$.
Plugging the entropy~\eqref{eq:entropy-preactivation-infinite-width-multi} into the definition of the mutual information~\eqref{eq:mutual-information}, we find
\begin{align}\label{eq:MI-one-neuron-many-samples}
\MI\le[p\!\le(z_{1;+},z_{1;-}| x_{\pm}\ri)\ri]=&\frac{1}{2}\le[\log(\ker_{++})+\log(\ker_{--})-\log\!\le(\ker_{++}\ker_{--}-\ker_{+-}^2\ri)\ri]\, \\
=&\frac{1}{2}\log\!\le(\frac{\ker_{++}\ker_{--}}{\ker_{++}\ker_{--}-\ker_{+-}^2}\ri)\, .\notag
\end{align}
Focusing on inputs $\x{i}{\pm}$ with equal norms such that $\ker_{++}=\ker_{--}=\ker_{[0]}+\ker_{[2]}$ and $\ker_{+-}=\ker_{[0]}-\ker_{[2]}$, cf.~\eqref{eq:K0-decomposition} and \eqref{eq:K2-decomposition}, we can rewrite this mutual information as\index{$\gamma^{[a]}$ basis!kernel}
\be
\MI\le[p\!\le(z_{1;+},z_{1;-}\ri)\ri]=\frac{1}{2}\log\!\le[\frac{\le(1+\frac{\ker_{[2]}}{\ker_{[0]}}\ri)^2}{4\frac{\ker_{[2]}}{\ker_{[0]}}}\ri]\, ,
\ee
which is parameterized  entirely by the dimensionless ratio $\ker_{[2]}/\ker_{[0]}$ that for nearby inputs $\x{i}{\pm} = \x{i}{\M} \pm \delta x_i /2$ characterizes their relative angle after passing through the network to the $\ell$-th layer.

There are two interesting limits here. First, when $\ker_{[2]}/\ker_{[0]}\rightarrow 0$, the mutual information becomes large.
This follows because as the relative angle between the preactivations vanishes, they become close to each other: knowing the preactivation for one input tells us about the value of the preactivation for  the other input.  In a \terminate{classification} problem, this is a great prior if the inputs are from the same class, but would make learning really difficult if they're from different classes. Second, when $\ker_{[0]}=\ker_{[2]}$, the mutual information vanishes. This follows because in this limit the off-diagonal components of the kernel, $\ker_{+-}=\ker_{[0]}-\ker_{[2]}$, vanishes: the preactivations become statistically independent. In a \terminate{classification} problem, this may be a good prior if the inputs are from different classes -- so long as the details of the classes don't correlate in some way -- but would make learning really difficult if the inputs are from the same class. Altogether, this suggests that as a prior we don't want $\ker_{[2]}/\ker_{[0]}$ to be too big or too small, which can be best ensured by setting both $\chi_\parallel =1$ and $\chi_\perp =1$, cf.~\eqref{eq:criticality-conditions}. This gives an information-theoretic\index{information theory!perspective on criticality} perspective on \neo{criticality}.

Finally, while we have focused here on the prior distribution for  networks at initialization, we could also study these same quantities after learning using the Bayesian posterior \eqref{eq:posterior-at-infinite-width} or the fully-trained distribution of gradient-based learning \eqref{eq:kernel-prediction}.
Since the entropy of a Gaussian distribution\index{Gaussian distribution!entropy} is independent of its mean
 -- it can be eliminated by a change of dummy integration variables -- 
the mutual information  for either trained infinite-width network is given by this same expression, \eqref{eq:MI-one-neuron-many-samples}, but with the kernel replaced by the covariance of the Bayesian posterior distribution~\eqref{eq:GP-posterior-variance} or the covariance of the generalized posterior distribution\index{posterior!generalized posterior distribution}~\eqref{eq:generalized-posterior-variance}. 
In the latter case, the mutual information will involve both the kernel and the frozen NTK, and its analysis would yield  similar results to those that we found in \S\ref{subsec:robustness-from-infinite-GD} when we investigated the \terminate{bias-variance tradeoff}. This would give an information-theoretic perspective on \terminate{generalization}\index{$\ast$-polation}.\footnote{
    Analogously, studying the \terminate{tripartite information} generalization of \eqref{eq:MI-one-neuron-many-samples} for  either type of posterior distribution would give an alternative perspective on the $\ast$-polation results of \S\ref{subsec:star-polation}.
}\index{information theory!perspective on generalization}\index{information theory!perspective on $\ast$-polation}  












\section{Information at Finite Width: Optimal Aspect Ratio}\label{sec:information-beyond-infinity}
In this section, we'll see how finite-width networks have a prior for nonzero mutual information between different neurons.
Assuming that nonzero mutual information is desirable by intuition -- and by analogy to an \emph{unsupervised} learning\index{unsupervised learning} objective -- we can use this computation to optimize the depth-to-width ratio for finite-width MLPs at criticality. This \neo{optimal aspect ratio}, $r^\star$, defines the scale that separates effectively-deep networks -- describable by our  effective theory approach -- from overly-deep networks -- not describable due to strong interactions and not trainable due to large fluctuations.\footnote{Note that in this section, we'll need the expressions for the running couplings that we derived in~\S\ref{sec:sum-rule} when finding the effective distribution of $m$ neurons in a wide-but-finite layer. As such, it may be helpful to reread that section before proceeding.}

In general, the \terminate{entropy} and \terminate{mutual information} of any interacting theory\index{interacting theory!entropy and mutual information} are really difficult to compute. However, when the interactions are \emph{nearly-Gaussian}\index{nearly-Gaussian distribution}, we can use \terminate{perturbation theory}. To do so, there is actually a neat \emph{variational principle} that we can use to organize our calculations, so let us explain that first.\index{nearly-Gaussian distribution!entropy}

As in the last section, we'll focus on the distribution of $m$ preactivations in layer $\ell$, drop (almost) all the \terminate{layer indices}, and focus exclusively on a single input $x$.
As we have been doing since the beginning of time (\S\ref{sec:perturbation}), let's express the probability distribution in terms of an \terminate{action} as
\be\label{eq:probability-action-appendix}
p(z_1,\ldots, z_{m}\vert x)=\frac{e^{-\ac\le(z_1,\ldots, z_{m}\ri)}}{Z} \, ,
\ee
which must be normalized by the \neo{partition function} 
\be
Z=\int\le[\prod_{i=1}^{m}dz_i\ri] e^{-\ac\le(z_1,\ldots, z_{m}\ri)}\, .
\ee
In terms of these 
quantities, the entropy~\eqref{eq:entropy-shannon-formula-integral} can be expressed as
\be\label{eq:entropy-action-representation}
\entropy\!\le[p\!\le(z_1,\ldots, z_{m}\vert x\ri)\ri]=\log(Z)+\frac{1}{Z} \int\le[\prod_{i=1}^{m}dz_i\ri] e^{-\ac\le(z_1,\ldots, z_{m}\ri)} \ac(z_1,\ldots, z_{m})\, .
\ee
Just to make sure, please don't get confused between the action $\ac(z)$, which is a function of the preactivations, and the entropy $\entropy\!\le[p(z)\ri]$, which is a functional of the probability distribution.\footnote{
    For choices of units of the preactivations $z$ for which the partition function is unity, $Z=1$, the action is simply the surprisal\index{surprisal (information theory)} \eqref{eq:surprisal-def}, and thus the entropy is the expectation of the action: $\entropy\!\le[p(z)\ri] = \E{\ac(z)}$ . Note that in \terminate{thermodynamics}, the constant $-\log Z$ is sometimes called the \neo{free energy}; this discussion should make clear that only its relative value for two different distributions is physical.
}



To proceed, let's adopt a \term{variational ansatz}: we'll divide the action into two parts as
\be\label{eq:variational-action}
\ac(z_1,\ldots, z_{m})=\acfree(z_1,\ldots, z_{m})+\acvar(z_1,\ldots, z_{m})\, ,
\ee
with the idea being that the second term, $\acvar$, encodes the part of the distribution that is perturbatively small.
Specifically, the \term{variational principle} instructs us to choose the first term in \eqref{eq:variational-action}, $\acfree(z)$,  that gives no variations of the entropy with respect to $\acvar(z)$:
\be\label{eq:variational-principle-equation}
0=\frac{\delta \entropy\!\le[p(z) \ri]}{\delta \acvar(z)}\bigg\vert_{\acvar(z)=0}  \, .
\ee
We'll satisfy this shortly.
Additionally, $\acvar(z)$ should not be completely arbitrary, but instead be constructed to properly reflect the statistics of the preactivation distribution $p(z_1,\ldots, z_{m}\vert x)$.
The first such constraint comes from demanding that the two-point correlator, when computed with the variational action, is determined by the \emph{exact} single-input metric $G$:
\be\label{eq:variational-constraint}
\E{z_{i_1}z_{i_2}}\equiv \delta_{i_1i_2} G \,.%
\ee
The second constraint comes from demanding that the connected four-point correlator, when computed with the variational action, is determined by the \emph{exact} single-input \terminate{four-point vertex}:
\begin{align}\label{eq:variational-constraint-2}
\E{z_{i_1}z_{i_2}z_{i_3}z_{i_4}}\vert_{\text{connected}} = \frac{1}{n} \le(\delta_{i_1i_2}\delta_{i_3 i_4} +\delta_{i_1i_3}\delta_{i_2 i_4}+\delta_{i_1i_4}\delta_{i_2 i_3} \ri) V\, .
\end{align}
Together, the constraints \eqref{eq:variational-constraint} and \eqref{eq:variational-constraint-2} will fix the couplings of the variation action $\acvar$ so that the full action \eqref{eq:variational-action} correctly specifies the $m$-neuron preactivation distribution \eqref{eq:probability-action-appendix}.\footnote{In principle there are additional constraints coming from the statistics of higher-point correlators, but their contribution is  subleading to both the leading and next-to-leading orders that we will work.}





To understand why we're doing this, note that our \terminate{variational principle} is ultimately just a realization of the \textbf{principle of maximum entropy}\index{maximum entropy, principle} for nearly-Gaussian distributions\index{nearly-Gaussian distribution}.\footnote{For those readers that enjoy our historical asides, the maximum entropy principle was discovered by Jaynes\index{Jaynes, Edwin T.} 
    and provides a link between \neo{information theory} on the one hand and \emph{statistical mechanics}\index{statistical physics} on the other hand \cite{PhysRev.106.620,PhysRev.108.171}.
As an example of this, consider a central problem in statistical mechanics: find the probability
distribution $p_i$ for the fundamental \emph{microstates}\index{microstate (statistical mechanics)} $i$ of a system that has a macroscopic average energy $\overline{E}\equiv \E{E_i}=\sum_{i} p_i E_i$. An application of the principle of maximum entropy then correctly picks out the  \neo{Boltzmann distribution} (or sometimes, the \emph{Gibbs distribution}\index{Gibbs distribution|see{Boltzmann distribution}})  of statistical mechanics\index{statistical physics}
\be\label{eq:boltzmann-distribution}
p_i \propto e^{- \beta E_i} \, ,
\ee
if we optimize the entropy \eqref{eq:entropy-shannon-formula-sum} subject to the observable constraint for the energy, $\sum_{i} p_i E_i=\overline{E}$, and the normalization condition for the distribution, $\sum_{i} p_i=1$.
Here, $\beta$ is a Lagrange multiplier that depends on the energy $\overline{E}$ and has a physical interpretation as the inverse temperature, $\beta=1/(k_{\text{B}}T)$, with the aforementioned \terminate{Boltzmann constant} $k_{\text{B}}$ and the familiar-to-everyone \terminate{temperature} $T$. This example also shows how statistical mechanics links the details of the fundamental microstates $i$ to the macroscopic thermodynamic variables such as $\overline{E}$ and $T$.
\label{foot:gibbs}
} In particular, first note that the Gaussian distribution itself can be derived as a distribution that maximizes the entropy \eqref{eq:entropy-shannon-formula-integral}, subject to the constraints of fixed first and second moment.\footnote{
    For those of you keeping track: the maximum entropy distribution with no information fixed is the \terminate{uniform distribution}, cf.~\eqref{eq:entropy-maximal}; the maximum entropy distribution with a fixed first moment is the \terminate{Boltzmann distribution}, cf.~\eqref{eq:boltzmann-distribution}; and the maximum entropy distribution with a fixed first and second moment is the \terminate{Gaussian distribution}, cf.~(nearly-)everywhere.
} As we will see in a moment, in \eqref{eq:variational-principle-equation} we are maximizing the entropy of the distribution with respect to the deformation\index{deformation!Gaussian distribution} of the action away from Gaussianity, $\acvar$, subject to the constraint of fixing the higher-order cumulants\index{cumulant} order by order in perturbation theory. In general, the maximum entropy principle is an appropriate procedure when we have fixed observable information for which we want to find an underlying distribution. Here, we actually know the distribution that produces $G$ and $V$, \eqref{eq:probability-action-appendix}, but we can still use this principle as convenient tool for computing the entropy.\footnote{
    In particular, this procedure will organize our perturbative computation of the entropy and ensure that we only need to compute Gaussian expectations of the form \eqref{eq:multi-neuron-expectation-reprint}.
}









Now, to satisfy the variational principle
 \eqref{eq:variational-principle-equation}, let's choose
\be\label{eq:variational-free-choice}
\acfree(z_1,\ldots, z_{m})=\frac{1}{2G}\sum_{i=1}^m z_i^2\, .
\ee
Importantly, $G$ is 
not just the inverse of the quadratic coefficient in the action $\ac(z)$, but instead is the \emph{exact} two-point correlator that we would actually measure, \eqref{eq:variational-constraint},
incorporating the full series of corrections due to the interactions, cf.~\eqref{eq:self-energy-decomposition}.\footnote{
    This will let us express the entropy in terms of these measurable quantities and in no way will we need to actually compute any of the corrections in this series.
} 
To see why such a choice satisfies the variational principle, let us start by rewriting expectations with respect to the full distribution \eqref{eq:probability-action-appendix} in terms of simpler \index{Gaussian expectation}\emph{Gaussian} expectations taken with respect to a zero-mean Gaussian distribution with the same variance \eqref{eq:variational-constraint}: 
\be
\bra\!\bra z_{i_1}z_{i_2}\ket\!\ket_{G} = \delta_{i_1i_2} G \, .
\ee
Here, please recall 
our notation $\bra\!\bra \cdot \ket\!\ket_G$ for a \terminate{Gaussian expectation} of a multi-neuron function with variance $\delta_{i_1i_2} G$, 
\be\label{eq:multi-neuron-expectation-reprint}
\bra\!\bra f(z_1, \ldots, z_m) \ket\!\ket_G \equiv \frac{1}{Z_{G}} \int\le[\prod_{i=1}^{m}dz_i\ri] e^{-\frac{1}{2G}\sum_{i=1}^{m}z_{i}^2} f(z_1, \ldots, z_m) \,,
\ee
and note also that such a Gaussian distribution will require a different partition function
\be
Z_{G}\equiv \int\le[\prod_{i=1}^{m}dz_i\ri] e^{-\frac{1}{2G}\sum_{i=1}^{m}z_{i}^2}=\le(2\pi G\ri)^{\frac{m}{2}}\, ;
\ee
importantly, $Z \neq Z_G$. Next, let us
rewrite the entropy~\eqref{eq:entropy-action-representation} in terms of these simpler expectations as
\begin{align}\label{eq:entropy-action-variational-representation}
\entropy\!\le[p\!\le(z_1,\ldots, z_{m}\vert x\ri)\ri]=&\log Z +\E{\frac{1}{2G}\sum_{i=1}^{m}z_{i}^2}+\frac{1}{Z} \int\le[\prod_{i=1}^{m}dz_i\ri] e^{-\frac{1}{2G}\sum_{i=1}^{m}z_{i}^2}e^{-\acvar} \acvar\, \notag\\
=&\frac{m}{2}\log(2\pi G)+\log\!\le(\frac{Z}{Z_G}\ri)+\frac{m}{2}+\le(\frac{Z}{Z_G}\ri)^{-1}\brabra e^{-\acvar} \acvar\ketket_{G} \, ,
\end{align}
where in the first equality we just plugged in \eqref{eq:variational-action}, and in the second equality 
we rewrote all the full expectations in terms of the simpler Gaussian expectations using~\eqref{eq:multi-neuron-expectation-reprint}.
Then, by Taylor-expanding in $\acvar$, we can evaluate the ratio of partition functions as
\begin{align}\label{eq:variational-evaluation}
&\frac{Z}{Z_{G}}=\frac{1}{Z_{G}} \int\le[\prod_{i=1}^{m}dz_i\ri] e^{-\frac{1}{2G}\sum_{i=1}^{m}z_{i}^2}\le(e^{-\acvar} \ri)=\brabra e^{-\acvar}\ketket_{G}\, \\
=&1-\bra\!\bra \acvar\ket\!\ket_{G}+\frac{1}{2}\brabra \acvar^2\ketket_{G}-\frac{1}{6}\brabra \acvar^3\ketket_{G}+\o{\acvar^4}\, ,\notag
\end{align}
and similarly we can evaluate the needed Gaussian expectation as
\be
\brabra e^{-\acvar} \acvar\ketket_{G}=\bra\!\bra \acvar\ket\!\ket_{G}-\brabra \acvar^2\ketket_{G}+\frac{1}{2}\brabra \acvar^3\ketket_{G}+\o{\acvar^4}\, .
\ee
Plugging these back into the variational expression for the entropy~\eqref{eq:entropy-action-variational-representation} and organizing terms, for which you might find $\log(1+x)=x-\frac{x^2}{2}+\frac{x^3}{3}+\ldots$ and $1/(1+x)=1-x+x^2-x^3+\ldots$ helpful,  we get
\begin{align}\label{eq:entropy-action-variational-representation-practical}
\entropy\!\le[p\!\le(z_1,\ldots, z_{m}\vert x\ri)\ri]=&\frac{m}{2}\log(2\pi e G)-\frac{1}{2}\le[\brabra \acvar^2\ketket_{G}-\bra\!\bra \acvar\ket\!\ket_{G}^2\ri]\, \\
&+\frac{1}{3}\le[\brabra \acvar^3\ketket_{G}-3\brabra \acvar^2\ketket_{G}\bra\!\bra \acvar\ket\!\ket_{G}+2\bra\!\bra \acvar\ket\!\ket_{G}^3\ri]+\o{\acvar^4} \, .\notag
\end{align}
First, note that the first term is exactly the  \terminate{entropy} for a multivariate Gaussian distribution with a covariance $\delta_{i_1i_2}G$, cf.~\eqref{eq:entropy-preactivation-infinite-width-multi}.  Second, and most importantly, note that the would-be linear term proportional to $\bra\!\bra \acvar\ket\!\ket_{G}$ exactly cancelled out. In other words, our ansatz\index{variational ansatz} for the decomposition of the action \eqref{eq:variational-action}  with the choice \eqref{eq:variational-free-choice} automatically satisfies the \terminate{variational principle} \eqref{eq:variational-principle-equation}.

Finally, let us note in passing that the leading correction coming from the quadratic term is definitively negative.\footnote{To see this, notice that the expression in the square brackets of the quadratic term is the variance of the variational part of the action, and thus is positive: $\bra\!\bra \acvar^2\ket\!\ket_{G}-\bra\!\bra \acvar\ket\!\ket_{G}^2=\langle\!\langle\le(\acvar-\bra\!\bra \acvar\ket\!\ket_{G}\ri)^2\rangle\!\rangle_{G}\geq 0$.
}
This negativity is actually necessary for mathematical consistency: as the \terminate{Gaussian distribution} maximizes the entropy of any set of random variables with known and fixed first and second moments, any deformation\index{deformation!Gaussian distribution} of a distribution away from Gaussianity, while also respecting such moment constraints, must necessarily have less entropy.
In the current case, our variational ansatz \eqref{eq:variational-action} gives a nearly-Gaussian deformation of a zero-mean Gaussian distribution.\footnote{
    N.B.~the terms in the second set of the square brackets in \eqref{eq:entropy-action-variational-representation-practical} are necessary for computing the next-to-leading-order correction.
}











Now that we're done passing notes,
let's satisfy our constraints, \eqref{eq:variational-constraint} and \eqref{eq:variational-constraint-2}, and then use our variational expression \eqref{eq:entropy-action-variational-representation-practical} to compute the \terminate{entropy} and \terminate{mutual information} of the preactivations in a finite-width network.

\subsubsection{Leading-Order Correction: Nonzero Mutual Information}
At leading order, we've already worked out how to relate the couplings of the variational action $\acvar$ to the single-input metric $G$ and the single-input four-point correlator $V$.
Recall from our discussion of the running couplings\index{running coupling} when integrating out neurons in~\S\ref{sec:sum-rule} that the leading correction to the action was given by
\be\label{eq:variational-action-first-order}
\acvar\!\le(z_1,\ldots, z_{m}\ri)=\frac{1}{2}\le(g_{m}-\frac{1}{G}\ri)\sum_{i=1}^m z_i^2-\frac{v_m}{8}\sum_{i,j=1}^m z_i^2z_j^2+\o{\frac{1}{n^2}} \, ,
\ee
where 
the running quadratic coupling\index{running coupling!quadratic} was given by ~\eqref{eq:quadratic-reprint-m-emphasis}, 
\be\label{eq:quadratic-reprint-m-emphasis-reprint}
g_{m}=\frac{1}{G}+\le(\frac{m+2}{2}\ri)\frac{V}{n G^3 } +\o{\frac{1}{n^2}} \, ,
\ee
and the running quartic coupling\index{running coupling!quartic} was given by~\eqref{eq:quartic-single-input-coupling-for-vertex},
\be\label{eq:quartic-single-input-coupling-for-vertex-reprint}
v_{m}= \frac{V}{n G^4 }+\o{\frac{1}{n^2}}\, .
\ee
To get the expression \eqref{eq:variational-action-first-order}, look at our expression for the distribution of $m$ preactivations, \eqref{eq:m-neuron-action}, and then rearrange \eqref{eq:variational-action}  with \eqref{eq:variational-free-choice} to solve for $\acvar$.
If you don't recall how to get \eqref{eq:quadratic-reprint-m-emphasis-reprint} and \eqref{eq:quartic-single-input-coupling-for-vertex-reprint},  feel free to flip back and reread the last subsubsection of \S\ref{sec:sum-rule}, or feel free to flip forward to the next subsection where we'll have to derive these expressions again to higher order in the \terminate{$1/n$ expansion}, cf.~\eqref{eq:quadratic-reprint-m-emphasis-refined} and~\eqref{eq:quartic-single-input-coupling-for-vertex-refined}.

Given that this leading-order variational correction to the action $\acvar$~\eqref{eq:variational-action-first-order} now satisfies the constraints \eqref{eq:variational-constraint} and \eqref{eq:variational-constraint-2}, we can now evaluate the leading correction to the \terminate{entropy}~\eqref{eq:entropy-action-variational-representation-practical}:%
\begin{align}\label{eq:first-and-last-unrolling}
&\brabra \acvar^2\ketket_{G}-\bra\!\bra \acvar\ket\!\ket_{G}^2\, \\
=&\frac{1}{4}\le(g_{m}-\frac{1}{G}\ri)^2\sum_{i_1,i_2=1}^m\le[\brabra z_{i_1}^2 z_{i_2}^2 \ketket_{G}-\brabra z_{i_1}^2\ketket_{G}\brabra z_{i_2}^2 \ketket_{G}\ri]\notag\\
&-\frac{1}{8}\le(g_{m}-\frac{1}{G}\ri)v_m\sum_{i_1,i_2,i_3=1}^m \le[\brabra z_{i_1}^2 z_{i_2}^2 z_{i_3}^2 \ketket_{G}-\brabra z_{i_1}^2 z_{i_2}^2\ketket_{G}\brabra z_{i_3}^2 \ketket_{G}\ri]\, \notag\\
&+\frac{1}{64}v_m^2 \sum_{i_1,i_2,i_3,i_4=1}^m\le[\brabra z_{i_1}^2 z_{i_2}^2 z_{i_3}^2z_{i_4}^2 \ketket_{G}-\brabra z_{i_1}^2 z_{i_2}^2\ketket_{G}\brabra z_{i_3}^2z_{i_4}^2 \ketket_{G}\ri]+\o{\frac{1}{n^3}}\, \notag\\
=&\frac{m}{2}\le(g_{m}-\frac{1}{G}\ri)^2G^2\!-\frac{m(m+2)}{2}\le(g_{m}-\frac{1}{G}\ri)v_mG^3\!+\frac{m(m+2)(m+3)}{8}v_m^2 G^4\!+\o{\frac{1}{n^3}}\, .\notag
\end{align}
Here, in going from the second line to the last line, you may find this formula for these Gaussian expectations useful:
\be\label{eq:combinatorial-2m-resurrection}
\sum_{i_1,\ldots,i_{k}=1}^m\brabra z_{i_1}^2 \cdots z_{i_k}^2 \ketket_{G}=m(m+2)\cdots [m+2(k-1)] \, G^k \, ,
\ee
which is akin to~\eqref{eq:combinatorial-2m}
and will be  used again in the next subsubsection repeatedly, up to $k=6$.
To complete the computation, plug in the quadratic coupling, \eqref{eq:quadratic-reprint-m-emphasis-reprint}, and the quartic coupling, \eqref{eq:quartic-single-input-coupling-for-vertex-reprint}, into~\eqref{eq:first-and-last-unrolling}, giving 
\be
\brabra \acvar^2\ketket_{G}-\bra\!\bra \acvar\ket\!\ket_{G}^2
=\frac{m(m+2)}{8}\le(\frac{V}{n G^2 }\ri)^2+\o{\frac{1}{n^3}}\, ,
\ee
for the variance of the variational action.
Therefore, the entropy~\eqref{eq:entropy-action-variational-representation-practical} is given by
\be\label{eq:entropy-second-order}
\entropy\!\le[p\!\le(z_1,\ldots, z_{m}\vert x\ri)\ri]=\frac{m}{2}\log(2\pi e G)-\frac{\le(m^2+2m\ri)}{16}\le(\frac{V}{n G^2 }\ri)^2+\o{\frac{1}{n^3}}\, ,
\ee
exhibiting a nontrivial correction at finite width.


Let us reflect on this formula by making some comments. First, note that the correction is definitely negative, as we pointed out before: recall that the Gaussian distribution maximizes the entropy of a set of random variables with a fixed covariance, and since our first variational constraint \eqref{eq:variational-constraint} fixes the two-point correlator of the preactivations, the entropy for the nearly-Gaussian distribution \eqref{eq:probability-action-appendix} must be less than the entropy of a Gaussian distribution with the same variance. Second, note that unlike all our previous results, the leading correction here is  \emph{second order} in the inverse layer width as $\sim V^2/n^2$ and  \emph{not} $\sim V/n$. Indeed, since the quartic coupling\index{coupling!quartic} $v_m$ in a generic nearly-Gaussian action can take either sign -- corresponding to a distribution with either fat tails or thin tails -- the leading correction to the entropy must be proportional to the minus the \emph{square} of the quartic coupling, $v_m^2 \propto V^2/n^2$, in order to guarantee that the entropy decreases.\footnote{This same argument excludes a contribution of the form $\o{\SPC_m} = \o{\SPV/n^2}$ coming from the sextic coupling\index{coupling!sextic}, cf.~\eqref{eq:variational-action-second-order}, and similarly excludes any linear contributions from other higher-order couplings.
 }
Finally, we see that this correction breaks the perfect additivity\index{entropy!additivity} for the neurons that we found in the infinite-width limit \eqref{eq:entropy-preactivation-infinite-width-multi}. In particular, although the decrease in the entropy is perturbatively small with an order $1/n^2$ scaling, it also depends \emph{quadratically} on $m$. This 
nonlinearity
in the number of neurons signals the presence of nontrivial interactions at finite width.









Accordingly, we can characterize this \terminate{statistical dependence} by computing \terminate{mutual information} between two non-overlapping sets of neurons, $\mathcal{M}_1=\le\{1,\ldots,m_1\ri\}$ and $\mathcal{M}_2=\le\{m_1+1,\ldots,m_1+m_2\ri\}$, %
\begin{align}\label{eq:MIT-second-order}
\MI\le[p\!\le( \mathcal{M}_1 , \mathcal{M}_2 \vert x\ri)\ri]\equiv&\entropy\!\le[p\!\le(\mathcal{M}_1 \vert x\ri)\ri]+\entropy\!\le[p\!\le(\mathcal{M}_2 \vert x\ri)\ri]-\entropy\!\le[p\!\le(\mathcal{M}_1 , \mathcal{M}_2 \vert x\ri)\ri]\, \\
=&\frac{m_1m_2}{8}\le[\frac{V^{(\ell)}}{n_{\ell-1} \le(G^{(\ell)}\ri)^2 }\ri]^2+\o{\frac{1}{n^3}}\, ,\notag
\end{align}
where we used our entropy formula \eqref{eq:entropy-second-order} in three different ways and also restored layer indices 
to better interpret this formula.
This nonzero mutual information signifies that -- at finite width, \emph{only} --  for a given layer $\ell$, observing the activities of a group of neurons $\mathcal{M}_1$ will convey information about the activities of another group of neurons $\mathcal{M}_2$. This can be thought of as an information-theoretic reformulation and generalization of the \neo{Hebbian learning} principle that we saw for the conditional variance \eqref{eq:conditional-variance} in \S\ref{subsec:Hebbian}: there we more simply saw that the variance of one neuron $z_2^{(\ell)}$, conditioned on an atypical observation of a second neuron, $z_1^{(\ell)} = \check{z}_1^{(\ell)}$, will itself be atypical; here, we can directly characterize how much one group of neurons can know about another non-overlapping group.\footnote{
    More generally, it would be interesting to work out the mutual information for multiple inputs in order to understand its data dependence. In that case, we expect it to be mediated by the multi-input four-point vertex and depend on the details of different groupings of four samples from the dataset\index{input data} $\D$.
}

 


Finally, remembering our \neo{scaling law} for the normalized vertex \eqref{eq:k-star-equals-zero-normalized-four-point-scaling-law}, we see that the \terminate{mutual information} \eqref{eq:MIT-second-order} scales with the depth $\ell$ of the hidden layer \emph{squared}: 
\be\label{eq:MIT-second-order-depth}
\MI\le[p\!\le( \mathcal{M}_1 , \mathcal{M}_2 \vert x\ri)\ri] \propto \ell^2/n^2 \, . 
\ee
In terms of \emph{RG flow}\index{representation group flow}, this means that the mutual information is \emph{relevant}\index{relevant (RG flow)}, suggesting that a growing mutual information is helpful when coarse-graining representations: as the fine-grained features are marginalized over, deeper hidden layers will have a growing set of correlations between groups of neurons. In other words, by increasing the size of the nonlinear subadditive term in the entropy, this reduces the number of independently available degrees of freedom in these deeper layers.\footnote{
    It would be interesting to try to interpret this in terms of the \neo{optimal brain damage} of \cite{brain-damage} or the \neo{lottery ticket hypothesis} of \cite{frankle2018the}.
}




















\subsubsection{NLO Correction: Optimal Aspect Ratio and Tripartite Information}
Our leading-order result for the finite-width mutual information, \eqref{eq:MIT-second-order-depth}, naively grows in depth without bounds, suggesting that deeper is always better. Of course, if the depth becomes too large, this naive answer breaks down as the higher-order terms in the perturbation series start to become important. To understand the mutual information at even greater depths, we'll need to compute the \emph{next-to-leading-order} (NLO) correction.\index{entropy!next-to-leading-order correction}\index{mutual information!next-to-leading-order correction}


To push our calculations to the next level, we need to ensure that our two variational constraints, \eqref{eq:variational-constraint} and \eqref{eq:variational-constraint-2}, are satisfied to next-to-leading-order, $\o{1/n^2}$. In principle, this means that we will need to also include an $\o{1/n^2}$ \emph{sextic} term in our variational action $\acvar$ as
\be\label{eq:variational-action-second-order}
\acvar\!\le(z_1,\ldots, z_{m}\ri)=\frac{1}{2}\le(g_{m}-\frac{1}{G}\ri)\sum_{i=1}^m z_i^2-\frac{1}{8}v_m\sum_{i,j=1}^m z_i^2z_j^2+\frac{1}{24}\SPC_m\sum_{i,j,k=1}^m z_i^2z_j^2z_k^2+\o{\frac{1}{n^3}}\, .
\ee
Such a sextic term was originally introduced in \eqref{eq:general-L-action-sixth}; here, we've specialized it to focus on $m$ preactivations in a layer $\ell$, with $\SPC_m$ the running sextic coupling\index{running coupling!sextic}.



Now, let's explain how to explicitly satisfy the variational constraints.
First, just as we did before for the \terminate{entropy} in \eqref{eq:entropy-action-variational-representation-practical}, 
note that for a general observable we can express its full expectation in terms of simpler Gaussian expectations as
\begin{align}\label{eq:observable-with-variation}
\E{\O}\equiv&\frac{1}{Z}\int\le[\prod_{i=1}^{m}dz_i\ri] e^{-\ac}\O=\le(\frac{Z}{Z_G}\ri)^{-1}\brabra e^{-\acvar}\O\ketket_{G}\, \\
=&\bra\!\bra \O\ket\!\ket_{G}-\Big[\bra\!\bra \O\acvar\ket\!\ket_{G}-\bra\!\bra \O\ket\!\ket_{G}\bra\!\bra \acvar\ket\!\ket_{G}\Big]\, \notag\\
&+\frac{1}{2}\le[\brabra \O\acvar^2\ketket_{G}-2\bra\!\bra \O\acvar\ket\!\ket_{G}\bra\!\bra\acvar\ket\!\ket_{G}-\bra\!\bra \O\ket\!\ket_{G}\brabra \acvar^2\ketket_{G}+2\bra\!\bra \O\ket\!\ket_{G}\bra\!\bra\acvar\ket\!\ket_{G}^2\ri]+\o{\frac{1}{n^3}}\,, \notag
\end{align}
where in the last equality, we expanded $\brabra e^{-\acvar}\O\ketket_{G}$ in $\acvar$ and 
also
used the formula that we already evaluated  in \eqref{eq:variational-evaluation} for the ratio of partition functions. %
Physically, the first square brackets says that in an interacting theory\index{interacting theory!variational method}, the leading variational correction to an observable  is given by the correlation of the observable with the variational part of the action. 

To satisfy our first constraint for the metric, \eqref{eq:variational-constraint}, we can use this general expression \eqref{eq:observable-with-variation} with the observable 
\be
\O\equiv \frac{1}{m}\sum_{i=1}^m z_i^2 \,,
\ee 
for which we can use our constraint \eqref{eq:variational-constraint} to easily see that $\E{\O}=G$. Evaluating this same expectation using the variational sextic action, \eqref{eq:variational-action-second-order}, we find
\begin{align}\label{eq:metric-shifts-nlo-variational}
G=&\frac{1}{m}\sum_{i=1}^m\E{z_i^2}\, \\
=&G
-\le(g_m-\frac{1}{G}\ri)G^2
+\frac{(m+2)}{2}v_m G^3
+\le(g_m-\frac{1}{G}\ri)^2\!G^3
-\frac{(m+2)(m+4)}{4}\SPC_m G^4
\, \notag\\
&-\frac{3(m+2)}{2}\le(g_m-\frac{1}{G}\ri)v_m G^4
+\frac{(m+2)(m+3)}{2}v_m^2 G^5
+\o{\frac{1}{n^3}}\, .\notag
\end{align}
In evaluating this, you  might again find the formula \eqref{eq:combinatorial-2m-resurrection} helpful. This expression, \eqref{eq:metric-shifts-nlo-variational},  shows how the interacting theory modifies the two-point correlator.
Rearranging terms to solve for $g_m$ order by order, we find an NLO version of~\eqref{eq:quadratic-reprint-m-emphasis-reprint}, which determines the variational quadratic coupling in terms of the metric and the other higher-order couplings: 
\be\label{eq:quadratic-reprint-m-emphasis-refined}
g_m = \frac{1}{G}+\frac{(m+2)}{2}v_mG-\frac{(m+2)(m+4)}{4}\SPC_m G^2+\frac{(m+2)}{2} v_m^2 G^3 +\o{\frac{1}{n^3}}\, .
\ee

Next, to satisfy the second constraint for the four-point vertex, \eqref{eq:variational-constraint-2}, we can use our general expression \eqref{eq:observable-with-variation} with the observable 
\be
\O \equiv \frac{1}{m(m+2)}\sum_{i,j=1}^m z_i^2z_j^2 \, ,
\ee
for which we can use both our constraints \eqref{eq:variational-constraint} and \eqref{eq:variational-constraint-2} to show that $\E{\O}= G^2+V/n$.
Now evaluating $\O$ according to the variational sextic action, \eqref{eq:variational-action-second-order}, we get
\begin{align}
G^2+\frac{V}{n}=&\frac{1}{m(m+2)}\sum_{i,j=1}^m\E{z_i^2z_j^2}\, \\
=&G^2
-2\le(g_m-\frac{1}{G}\ri)G^3
+3\le(g_m-\frac{1}{G}\ri)^2\!G^4
+(m+3)v_m G^4
-\frac{(m+4)^2}{2}\SPC_m G^5
\, \notag\\
&-4(m+3)\le(g_m-\frac{1}{G}\ri)v_m G^5
+\frac{(5m^2+34m+60)}{4}v_m^2 G^6
+\o{\frac{1}{n^3}}\, \notag\\
=&G^2+v_mG^4-(m+4)\SPC_m G^5+\frac{(m+8)}{2}v_m^2G^6+\o{\frac{1}{n^3}}\, ,
\end{align}
where to get to the second line we again made heavy use of our formula \eqref{eq:combinatorial-2m-resurrection}, and to get to the final line we plugged in \eqref{eq:quadratic-reprint-m-emphasis-refined} for the quadratic coupling. 
Rearranging terms to solve for $v_m$ order by order, we find an NLO version of~\eqref{eq:quartic-single-input-coupling-for-vertex-reprint}, which determines the variational quartic coupling in terms of the two constraints and the sextic coupling: 
\be\label{eq:quartic-single-input-coupling-for-vertex-refined}
v_m =\frac{V}{nG^4}-\frac{(m+8)}{2}\le(\frac{V}{nG^3}\ri)^2+(m+4)\SPC_m G+\o{\frac{1}{n^3}}\, .
\ee






We now finally have all the pieces we need to evaluate the NLO correction to the \terminate{entropy} \eqref{eq:entropy-action-variational-representation-practical}.\footnote{
    In principle,  at this order we would need to continue and find an expression for $u_m$ in terms of an additional six-point vertex constraint, but as we will soon see, $u_m$ doesn't factor into any of our expressions for the mutual information. %
    This is the reason why we are able to analyze these next-to-leading-order corrections without otherwise evaluating  additional MLP recursions.
}
First, let's  reevaluate the variance of the variational action to NLO by
repeated use of the formula~\eqref{eq:combinatorial-2m-resurrection},
\begin{align}\label{eq:not-too-bad-1}
&\brabra \acvar^2\ketket_{G}-\bra\!\bra \acvar\ket\!\ket_{G}^2\, \\
=&\frac{m}{2}\le(g_{m}-\frac{1}{G}\ri)^2G^2\!-\frac{m(m+2)}{2}\le(g_{m}-\frac{1}{G}\ri)v_mG^3\!+\frac{m(m+2)(m+3)}{8}v_m^2 G^4\, \notag\\
&+\frac{m(m+2)(m+4)}{4}\le(g_{m}-\frac{1}{G}\ri)\SPC_mG^4-\frac{m(m+2)(m+4)^2}{8}v_m \SPC_m G^5+\o{\frac{1}{n^4}}\, \notag\\
=&\frac{m(m+2)}{8}\le[\le(v_mG^2\ri)^2-2(m+4)\le(v_mG^2\ri)\le(\SPC_mG^3\ri)\ri]+\o{\frac{1}{n^4}}\, \notag\\
=&\frac{m(m+2)}{8}\le[\le(\frac{V}{nG^2}\ri)^2-(m+8)\le(\frac{V}{nG^2}\ri)^3\ri]+\o{\frac{1}{n^4}}\, .\notag
\end{align}
Here, to get to the penultimate line we used our NLO expression for the quadratic coupling~\eqref{eq:quadratic-reprint-m-emphasis-refined}, and then to get to the final line we used our NLO expression for the quartic coupling~\eqref{eq:quartic-single-input-coupling-for-vertex-refined}.
Second, let's similarly evaluate the subleading term in our expression \eqref{eq:entropy-action-variational-representation-practical} for the  \terminate{entropy}:
\begin{align}\label{eq:not-too-bad-2}
&\brabra \acvar^3\ketket_{G}-3\brabra \acvar^2\ketket_{G}\bra\!\bra \acvar\ket\!\ket_{G}+2\bra\!\bra \acvar\ket\!\ket_{G}^3\, \\
=&m\le(g_{m}-\frac{1}{G}\ri)^3G^3-\frac{9m(m+2)}{4}\le(g_{m}-\frac{1}{G}\ri)^2v_mG^4\, \notag\\
&+\frac{3m(m+2)(m+3)}{2}\le(g_{m}-\frac{1}{G}\ri)v_m^2G^5-\frac{m(m+2)(5m^2+34m+60)}{16}v_m^3 G^6+\o{\frac{1}{n^4}}\, \notag\\
=&-\frac{m(m+2)(m+8)}{8}\le(v_mG^2\ri)^3+\o{\frac{1}{n^4}}\, \notag\\
=&-\frac{m(m+2)(m+8)}{8}\le(\frac{V}{nG^2}\ri)^3+\o{\frac{1}{n^4}}\, .\notag
\end{align}
Putting \eqref{eq:not-too-bad-1} and \eqref{eq:not-too-bad-2} back into our variational expression for the \terminate{entropy} \eqref{eq:entropy-action-variational-representation-practical}, we finally arrive at
\begin{align}\label{eq:entropy-third-order}
&\entropy\!\le[p\!\le(z_1,\ldots, z_{m}\vert x\ri)\ri]\\
=&\frac{m}{2}\log(2\pi e G)-\frac{\le(m^2+2m\ri)}{16}\le(\frac{V}{n G^2 }\ri)^2+\frac{\le(m^3+10m^2+16m\ri)}{48}\le(\frac{V}{n G^2 }\ri)^3+\o{\frac{1}{n^4}}\, .\notag
\end{align}
This result 
in turn lets us compute the NLO correction to the mutual information between two sets of neurons, $\mathcal{M}_1=\le\{1,\ldots,m_1\ri\}$ and $\mathcal{M}_2=\le\{m_1+1,\ldots,m_1+m_2\ri\}$:
\begin{align}\label{eq:MIT-third-order}
&\MI\le[p\!\le( \mathcal{M}_1 , \mathcal{M}_2 \vert x\ri)\ri] \, \\
\equiv&\entropy\!\le[p\!\le(\mathcal{M}_1 \vert x\ri)\ri]+\entropy\!\le[p\!\le(\mathcal{M}_2 \vert x\ri)\ri]-\entropy\!\le[p\!\le(\mathcal{M}_1 , \mathcal{M}_2 \vert x\ri)\ri]\, \notag \\
=&\frac{m_1m_2}{8}\le[\frac{ V^{(\ell)} }{ n_{\ell-1} \le(G^{(\ell)}\ri)^2} \ri]^2 -\frac{m_1 m_2(20+3m_1+3m_2)}{48}\le[\frac{ V^{(\ell)} }{ n_{\ell-1} \le(G^{(\ell)}\ri)^2} \ri]^3 +\o{\frac{1}{n^4}} \, \notag,
\end{align}
where once again we have restored  layer indices on the metric, the four-point vertex, and the layer width.
Note that as promised, the sextic coupling $\SPC_m$ dropped out in the end.\footnote{
This is another realization of the principle of maximum entropy\index{maximum entropy, principle} for nearly-Gaussian distributions. 
In particular, the entropy for a zero-mean distribution that satisfies constraints 
fixing its two-point correlator and its connected four-point correlator -- and otherwise leaves unfixed its connected six-point correlator -- 
is given to order $1/n^3$ by our expression \eqref{eq:entropy-third-order}. Thus, if we did add a third constraint that fixed the connected six-point correlator, with $\SPV = \o{1/n^2}$, then any terms of the form $\o{v_m \SPC_m } = \o{\SPV V/n^3}$ cannot appear in \eqref{eq:entropy-third-order}, as they could \emph{increase} the entropy depending on the sign of $\SPV V$.
} 


Excitingly, these two terms have opposite signs.
To explain our excitement, let us plug in our scaling solution for the normalized \terminate{four-point vertex}  \eqref{eq:k-star-equals-zero-normalized-four-point-scaling-law} evaluated at the final layer $\ell=L$:
\be\label{eq:scaling-solution-reprinted-for-MIT}
\frac{ V^{(L)} }{ n_{L-1} \le(G^{(L)}\ri)^2} \equiv \nu r \, .
\ee
Here, $r \equiv L/n$ is the overall depth-to-width aspect ratio of the network, and $\nu$ is an activation-function dependent constant: for the $K^\star=0$ universality\index{universality class!K@$K^\star=0$}, we have~\eqref{eq:k-star-equals-zero-normalized-four-point-tanh-univ}
\be\label{eq:tanh-nu}
\nu = \frac{2}{3}\, ,
\ee
independent of the details of the activation function itself; for scale-invariant activation functions\index{universality class!scale-invariant}, we have~\eqref{eq:k-star-equals-zero-normalized-four-point-relu-univ}
\be\label{eq:scale-invariant-nu}
\nu = \le(\frac{3A_4}{A_2^2} -1 \ri) \,,
\ee
with $A_2\equiv (a_+^2+a_-^2)/2$ and $A_4\equiv (a_+^4+a_-^4)/2$.\footnote{This gives $\nu=2$ for $\linear$ activations and $\nu=5$ for the $\relu$.} Plugging this scaling solution~\eqref{eq:scaling-solution-reprinted-for-MIT} into our NLO expression for the mutual informtion \eqref{eq:MIT-third-order}, we get
\begin{align}\label{eq:MIT-third-order-tangible}
\MI\le[p\!\le( \mathcal{M}_1 , \mathcal{M}_2 \vert x\ri)\ri]=&\frac{m_1m_2}{8}\nu^2r^2-\frac{m_1 m_2(20+3m_1+3m_2)}{48}\nu^3r^3+\o{r^4} \, .
\end{align}

Now, let us explain our excitement.
At one extreme, in the \terminate{infinite-width limit} $r\to 0$, this mutual information vanishes, as we already knew from \eqref{eq:MIT-zeroth-order}. Thus, for small aspect ratios $r \ll 1$, the first leading-order term dominates and the mutual information increases as depth increases. As the depth continues to increase, then the second term begins to dominate, \emph{decreasing} the mutual information. But we know by the \emph{subadditivity} of the entropy\index{entropy!additivity!subadditivity}, \eqref{eq:positivity-of-mutual-information}, that the \terminate{mutual information} is always bounded from below by zero,
and so for large enough aspect ratios $r$, this decreasing will be balanced by additional higher-order corrections. Taken altogether, we expect that at some nonzero but not too large $r$, the mutual information will reach a local maximum.
Indeed, maximizing \eqref{eq:MIT-third-order-tangible}, we find an \term{optimal aspect ratio} for the network:
\be\label{eq:Banks-Zaks}
r^{\star} = \le( \frac{4}{20+3 m_1 + 3m_2}\ri)\frac{1}{\nu} \, ,
\ee
with $\nu$ containing all of the details of the activation function.\footnote{Don't worry about the higher-order contributions $\o{r^4}$ that we neglected in obtaining the solution~\eqref{eq:Banks-Zaks}: for a particular choice of activation function, i.e.~for a choice of $\nu$, the estimate of the optimal value \eqref{eq:Banks-Zaks} is \emph{a posteriori} justified so long as the product $\nu r^{\star}$ is perturbatively small for a particular  $\nu$ and a particular grouping of neurons $(m_1, m_2)$.  FYI\index{for your information}, this argument is analogous 
the one given to justify the two-loop \emph{Banks-Zaks fixed point}\index{Banks-Zaks fixed point|see{fixed point}}\index{fixed point!Banks-Zaks}\index{fixed point!Banks-Zaks|seealso{optimal aspect ratio}} of the renormalization group \index{renormalization group flow} in \terminate{non-Abelian gauge theory}\index{non-Abelian gauge theory|seealso{Banks-Zaks fixed point}} \cite{banks1982phase}.}








Although it's not immediately obvious, maximizing a \terminate{mutual information} such as \eqref{eq:scaling-solution-reprinted-for-MIT} is closely related to well-known \term{unsupervised learning} objectives.\footnote{In particular, the \neo{InfoMax principle}\index{InfoMax principle|seealso{unsupervised learning}} \cite{linsker1988self} 
recommends
maximizing the \terminate{mutual information} between the input $x$ and a \terminate{representation} $z(x)$. A related notion involves maximizing the mutual information between different representations, $z_1(x)$ and $z_2(x)$, for the same input $x$  \cite{becker1992self}. %
This latter notion can be shown 
to
lower bound the InfoMax objective and thus motivates our analysis here.\label{footnote:info-max}} 
In contrast to a \emph{supervised} learning\index{supervised learning} setting where the goal is to predict the true output $y\equiv f(x)$ for any input sample $x$, in an \emph{unsupervised} learning setting the goal is to learn representations for a collection of input samples by observing patterns in the data.
For human-generated datasets, this has the advantage of eliminating the tedious task of labeling all the samples.
It should also be no surprise, given the benefit of \terminate{representation learning} (\S\ref{ch:features}), that models (pre)trained\index{pretraining}\index{unsupervised learning!as pretraining} with unsupervised learning algorithms  can often be 
efficiently fine-tuned on subsequent supervised learning tasks.


In the current context, rather than doing any actual learning,
we are understanding, a priori, which choice of the architecture and activation function can lead to a larger mutual information between neurons in deeper layers.\footnote{
    Similarly, we may think of our criticality analysis\index{criticality!as unsupervised learning} as a type of unsupervised learning or pretraining\index{pretraining} criteria in which we are understanding, a priori, which choice of \terminate{initialization hyperparameters} leads to an \terminate{ensemble} that generalizes most robustly after training, cf.~\S\ref{sec:generalization-at-infinity}.
} 
In particular, comparing the values of $\nu$ in \eqref{eq:tanh-nu} and \eqref{eq:scale-invariant-nu} for different choices of activation function, we see in principle that
networks built from $\tanhA$ activation function should be deeper than networks built from $\relu$ activation functions to have the same mutual information in a layer.


With this objective in mind, we should continue maximizing \eqref{eq:MIT-third-order-tangible} across all the possible partitions $(m_1, m_2)$. This picks out
a very natural choice of a partition spanning the whole layer and of equal sizes: $m_1=m_2=n_L/2$.
With this further maximization, we get
\be\label{eq:optimal-aspect-ratio-natural-choice}
r^{\star}= \le( \frac{4}{20+3n_L}\ri)\frac{1}{\nu} \, ,
\ee
for the optimal aspect ratio and
\be\label{eq:optimal-MI-value}
\MI\le[p\!\le( \mathcal{M}_1 , \mathcal{M}_2 \vert x\ri)\ri]= \frac{1}{6}\le(\frac{n_L}{20+3n_L}\ri)^2 \, ,
\ee
for the associated value of the maximized mutual information. In particular, this corresponds to a lower bound on the \emph{InfoMax}\index{InfoMax principle} objective that we discussed in footnote~\ref{footnote:info-max}.\footnote{
    While this value \eqref{eq:optimal-MI-value} may seem small, remember that our analysis is based entirely on the prior distribution  before any learning has taken place. 
}

As a final comment on \eqref{eq:optimal-aspect-ratio-natural-choice}, we can think of this optimal aspect ratio as defining a scale that separates the effectively-deep\index{effectively deep} regime for which our effective theory is valid from the overly-deep\index{overly deep} regime where the theory is strongly-coupled and networks are no longer trainable. We will push this interpretation further in the next appendix when we discuss residual networks\index{residual network}.





For a final computation, let's look at the \neo{tripartite information}~\eqref{eq:TI-definition} for mutually-exclusive subsystems $\mathcal{M}_1$, $\mathcal{M}_2$, and $\mathcal{M}_3$ of sizes $(m_1, m_2, m_3)$ neurons, respectively. Plugging in \eqref{eq:entropy-third-order} for various different combinations of the sizes, we find
\be\label{eq:TIM-third-order}
\MI_{3}\le[p\!\le( \mathcal{M}_1 , \mathcal{M}_2, \mathcal{M}_3 \vert x\ri)\ri]= \frac{m_1 m_2 m_3}{8} \le[\frac{ V^{(\ell)} }{ n_{\ell-1} \le(G^{(\ell)}\ri)^2} \ri]^3 +
 \o{\frac{1}{n^4}}\, ,
\ee
which, as per \eqref{eq:k-star-equals-zero-normalized-four-point-scaling-law}, scales cubically with depth, $\MI_{3}\le[p\!\le( \mathcal{M}_1 , \mathcal{M}_2, \mathcal{M}_3 \vert x\ri)\ri] \propto \ell^3/n_{\ell-1}^3$, and thus indicates that a nonzero result only first appears at $\o{1/n^3}$.\footnote{
    In hindsight, it's obvious that we needed the entropy to have at least a cubic dependence on the 
    sizes $m_i$ to find a nonzero answer for the \terminate{tripartite information}, just as we needed the entropy to have at least a quadratic dependence on the sizes to find a nonzero answer for the mutual information \eqref{eq:MIT-second-order}.
}
Importantly, we see that the tripartite information is exclusively \emph{positive}, meaning that any three groups of neurons in a layer will form \emph{redundant}\index{redundancy (information theory)} representations under RG flow: knowing the activities of any one group of neurons $\mathcal{M}_1$ means you would learn less information about a second group of neurons $\mathcal{M}_2$ from observing a third group $\mathcal{M}_3$ than you otherwise would have learned had you not already known $\mathcal{M}_1$. It would be interesting to try to understand further how this property relates to the coarse-graining mechanism of the \terminate{representation group flow} to the deeper layers.\footnote{It would also be interesting to try and interpret this in terms of the \neo{optimal brain damage} of \cite{brain-damage} or the \neo{lottery ticket hypothesis} of \cite{frankle2018the}.}
































