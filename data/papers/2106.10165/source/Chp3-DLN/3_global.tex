

\chapter{Effective Theory of Deep Linear Networks at Initialization}\label{ch:deep-linear-eft}
\epigraph{\dots a system which has spherical symmetry \dots certainly cannot result in an organism such as a horse, which is not spherically symmetrical. %
}{Alan Turing, on %
the limitations of toy models \cite{turing1952chemical}.\index{Turing, Alan}}

\index{deep linear network|textbf}\index{function approximation}\index{artificial intelligence}\index{brain}\index{deep linear network|seealso{$\texttt{linear}$}}
\noindent{}In this final warm-up chapter, we introduce and then solve a toy model of deep learning, the \textbf{deep linear network}.\footnote{For physicists, we give an analogy: the deep linear network is to deep learning as the \terminate{simple harmonic oscillator} is to \terminate{quantum mechanics}.} As will be explained in~\S\ref{sec:DLN}, the deep linear network is simply an MLP with $\linear$ activation functions. 
In particular, such a network can only 
compute
linear transformations of its inputs 
and certainly cannot result in a function such as a human, which is empirically known to be nonlinear.
Nonetheless, the study of deep linear networks\index{deep linear network} will serve as a useful blueprint for an \neo{effective theory of deep learning} that we will develop more generally over the subsequent chapters. %
Specifically, the exercises in this chapter illustrate how layer-to-layer recursions control the statistics of deep neural networks in a very intuitive way,
without getting bogged down by all the technical details.


To that end, in~\S\ref{sec:criticality_DLN} we obtain and then exactly solve a layer-to-layer recursion for the two-point correlator of preactivations in deep linear networks.
The result highlights that the statistics of the network sensitively depend on the setting of the \neo{initialization hyperparameters}, with the sensitivity increasing exponentially with depth.
This leads to the important concept of \neo{criticality}, which we will explore in \S\ref{ch:signalprop} in greater depth and sensitivity. In short, we learn that for networks to be well behaved, these hyperparameters need to be finely tuned. 

\index{connected correlator!four-point}\index{architecture hyperparameters}
Next, in~\S\ref{sec:fluctuations_DLN} we obtain and then 
solve a layer-to-layer recursion for the four-point correlator, 
albeit for a single input to further simplify the algebra. 
This showcases the way in which the behavior of the network can depend on the \emph{architecture hyperparameters}, particularly the width and depth of the network. 
In addition, we interpret the four-point \emph{connected} correlator as a measurement of the \emph{fluctuation}\index{fluctuations} of the network function from
draw to
draw of the model parameters. 
Such fluctuations can interfere with the tuning of the initialization hyperparameters
and need to be controlled so that networks
behave reliably for typical draws. 
The scale of the fluctuations are set by the depth-to-width ratio of the network, highlighting this important \neo{emergent scale} in the analysis of MLPs, and we'll see that 
the fluctuations can be kept under control by
keeping the depth-to-width ratio of the network sufficiently small.

\index{emergent scale}
Finally, in~\S\ref{sec:solution_DLN} we obtain a recursion for 
an arbitrary $M$-point
correlator for a deep linear network evaluated on a single input. Such recursions are all exactly solvable at any width 
$n$ and depth $L$, meaning we can fully determine the statistics of these networks at initialization.\footnote{This notion of \emph{solve} should not be confused with the solving of the training dynamics for a particular \terminate{learning algorithm}. In the context of deep linear networks\index{deep linear network}, the dynamics of \terminate{gradient descent} were analyzed in~\cite{saxe2013exact}.  In \S\ref{ch:NTHb} and \S\ref{ch:eot}, we will solve the training dynamics of gradient descent for MLPs with general activation functions in the context of our effective theory formalism.
}
Given these nonperturbative solutions, we
take the limit of large width, with fixed depth, and the limit of large depth, with fixed width, and show explicitly that these two limits do not commute. 
We also construct an interpolating solution with both large width and large depth, but fixed depth-to-width ratio $L/n$, and see how this 
\emph{scale}
serves as a perturbative parameter that controls 
all the 
interactions 
in the network
and 
controls
the validity of the perturbative analysis.








\section{Deep Linear Networks}
\label{sec:DLN}
\index{deep linear network}\index{linear transformations}\index{Gaussian distribution}
A deep linear network iteratively transforms an input $\x{i}{\alpha}$ through a sequence of simple linear transformations
\be\label{eq:deep-linear-foward-pass}
\z{i}{\alpha}{\ell+1} = \bias{i}{\ell+1} + \sum_{j=1}^{n_\ell}\W{ij}{\ell+1} \z{j}{\alpha}{\ell} \, ,
\ee
with $\z{i}{\alpha}{0}\equiv\x{i}{\alpha}$ and $\z{i}{\alpha}{\ell}\equiv z_i^{(\ell)}(x_{\alpha})$.
Since the $\linear$ activation function is the identity function, $\sigma(z) = z$, there's no distinction here between preactivations and activations.

\index{linear transformations}
In this chapter, we'll simplify matters a bit by turning off all the biases, $\bias{i}{\ell}=0$, so that the preactivations in layer $\ell$ are simply given by a repeated matrix multiplication of weight matrices as
\begin{align}\label{eq:deep-linear-preactivation-compact}
\z{i}{\alpha}{\ell}=\sum_{j_0=1}^{n_0} \sum_{j_1=1}^{n_1}\cdots \sum_{j_{\ell-1}=1}^{n_{\ell-1}} W_{i j_{\ell-1}}^{(\ell)} W_{j_{\ell-1}j_{\ell-2}}^{(\ell-1)} \cdots W_{j_{1} j_{0}}^{(1)} \x{j_{0}}{\alpha}\equiv \sum_{j=1}^{n_0}\PW_{ij}^{(\ell)} \x{j}{\alpha}\, .
\end{align}
Here we have introduced an $n_{\ell}$-by-$n_{0}$ matrix  
\be
\PW_{ij}^{(\ell)}= \sum_{j_1=1}^{n_1}\cdots \sum_{j_{\ell-1}=1}^{n_{\ell-1}}  W_{i j_{\ell-1}}^{(\ell)} W_{j_{\ell-1}j_{\ell-2}}^{(\ell-1)} \cdots W_{j_{1} j}^{(1)} \, ,
\ee
which highlights the fact that the preactivation at the $\ell$-th layer is simply a linear transformation of the input.
Additionally, let us set $\CW{\ell} \equiv C_W$ so that the order-one part of the weight variance is layer independent. All together, this means that the \terminate{initialization distribution} over the weights is characterized by the following expectations
\be\label{eq:deep-linear-weight-init}
\E{W^{(\ell)}_{i j}} = 0\,, \qquad \E{W^{(\ell)}_{i_1 j_1}W^{(\ell)}_{i_2 j_2}}=\delta_{i_1 i_2} \delta_{j_1 j_2}\frac{C_{W}}{n_{\ell-1}}\, .
\ee

\index{linear transformations}\index{deep linear network}
Somewhat counterintuitively, deep linear networks generically represent a smaller set of functions than fully general linear transformations, a.k.a.~one-layer networks of the same input-output dimensions.\footnote{This is not necessarily a bad thing, since there are often both computational and representational advantages to focusing on a specialized class of functions. For instance, we saw that convolutional networks represent a much smaller set of functions than MLPs, and yet they are known to perform better on \terminate{computer vision} tasks due to their translational-invariance-respecting \emph{inductive bias} %
as well as the fact that they require significantly less computation due to their sparse pattern of connections.\index{convolutional neural network}\index{translational invariance}
Having said that, it's not obvious if deep linear networks have a useful inductive bias when compared to general linear transformations.
} As an extreme example, let's take a two-layer deep linear network in which the first hidden layer consists of a single neuron $n_1=1$ and consider the network output in the second layer $\ell=2$. 
In this case, all the information in the input is compressed through a \neo{bottleneck} into a single number in the first layer before being converted into an $n_2$-dimensional vector in the output layer. Surely, 
such a deep linear network represents a
tinier subspace of linear transformations than those given by all the possible $n_2$-by-$n_0$ matrices, so long as $n_0, n_2 > 1$.

\index{deep linear network}
More importantly, we will show that the statistics of deep linear networks at initialization are also very different from those of one-layer networks.
In particular, while the statistics of each $W_{i j}^{(\ell)}$ are given by a simple \terminate{Gaussian distribution}, the statistics of their product $\PW_{ij}^{(\ell)}$ are non-Gaussian, depending in a complicated way on the depth $\ell$ and widths $n_{1},\ldots,n_{\ell}$ of the network. 


\index{correlator!$M$-point}
The goal of the rest of this chapter is to exactly work out this dependence. Concretely, we are going to compute the nontrivial distribution
\be
p\!\le(z^{(\ell)} \Big\vert \D\ri)\,  \equiv p\!\le(z^{(\ell)}\le(x_1\ri), \ldots, z^{(\ell)}\le(x_{\ND}\ri) \ri)\, ,%
\ee
of the preactivations $\z{i}{\alpha}{\ell}\equiv z_{i}^{(\ell)}\!\le(x_\alpha \ri)$
implied by the iterated multiplication \eqref{eq:deep-linear-preactivation-compact}
when evaluated on the entire dataset\index{input data} $\D$.
As mentioned in \S\ref{sec:not-Gauss}, a distribution is completely determined by the set of all its $M$-point correlators, and so our method for determining $p\!\le(z^{(\ell)} \Big\vert \D\ri)$ will be to directly compute these correlators.


Before moving onto the next section, let's consider the simplest observable, the mean of the preactivation $\z{i}{\alpha}{\ell}$.
Taking an expectation of the defining equation~\eqref{eq:deep-linear-preactivation-compact},
it's easy to see that the mean preactivation must vanish at any layer:
\begin{align}
\E{\z{i}{\alpha}{\ell}}&=\sum_{j_0=1}^{n_0} \sum_{j_1=1}^{n_1}\cdots \sum_{j_{\ell-1}=1}^{n_{\ell-1}} \E{W_{i j_{\ell-1}}^{(\ell)} W_{j_{\ell-1}j_{\ell-2}}^{(\ell-1)} \cdots W_{j_{1} j_{0}}^{(1)} \x{j_{0}}{\alpha}}  \, \\
&=\sum_{j_0=1}^{n_0} \sum_{j_1=1}^{n_1}\cdots \sum_{j_{\ell-1}=1}^{n_{\ell-1}}  \E{W_{i j_{\ell-1}}^{(\ell)}} \E{ W_{j_{\ell-1}j_{\ell-2}}^{(\ell-1)}} \cdots \E{W_{j_{1} j_{0}}^{(1)}} \x{j_{0}}{\alpha}= 0\, ,\notag
\end{align}
since the weight matrices are mutually independent -- and independent of the input -- and have zero mean \eqref{eq:deep-linear-weight-init}. By a similar argument, it's easy to see that any odd-point correlator of preactivations will vanish as well. Thus, going forward, we will only have to concern ourselves with the even-point correlators.









\section{Criticality}\label{sec:criticality_DLN}



Since the mean is trivial, 
the next simplest candidate for an interesting observable 
is the two-point correlator $\E{\z{i_1}{\alpha_1}{\ell}\z{i_2}{\alpha_2}{\ell}}$, which quantifies the typical magnitudes of the preactivations. We'll first go through the math, and then we'll discuss the physics.

\index{correlator!two-point}\index{Wick contraction}
\subsubsection{Math: recursion for the two-point correlator}
Let's start slowly by first considering the two-point correlator 
in the first layer. 
Using the defining equation~\eqref{eq:deep-linear-preactivation-compact} to express the first-layer preactivations in terms of the inputs as 
\be\label{eq:deep-linear-preactivations-first-layer}
\z{i}{\alpha}{1} = \sum_j^{n_0} \Ti{W}{ij}{1}\x{j}{\alpha} \, ,
\ee
we can express the two-point correlator as
\begin{align}\label{eq:deep-linear-two-point-first-layer}
\E{\z{i_1}{\alpha_1}{1}\z{i_2}{\alpha_2}{1}}&=\sum_{j_1,j_2=1}^{n_0}\E{\Ti{W}{i_1j_1}{1}\x{j_1}{\alpha_1}\Ti{W}{i_2j_2}{1}\x{j_2}{\alpha_2}}\\
&=\sum_{j_1,j_2=1}^{n_0}\E{\Ti{W}{i_1j_1}{1}\Ti{W}{i_2j_2}{1}} \x{j_1}{\alpha_1}\x{j_2}{\alpha_2} \notag \\
&=\sum_{j_1,j_2=1}^{n_0}\frac{C_W}{n_0}\delta_{i_1 i_2}\delta_{j_1 j_2}\x{j_1}{\alpha_1}\x{j_2}{\alpha_2}=\delta_{i_1 i_2}C_W\frac{1}{n_0}\sum_{j=1}^{n_0}\x{j}{\alpha_1}\x{j}{\alpha_2}\, , \notag
\end{align}
where
to go from the second line to the third line we Wick-contracted the two weights and inserted the variance~\eqref{eq:deep-linear-weight-init}.
Additionally, let us introduce the notation
\be\label{eq:deep-linear-normalized-inner-product-inputs}
\PH_{\alpha_1\alpha_2}\equiv \frac{1}{n_0}\sum_{i=1}^{n_0}\x{i}{\alpha_1}\x{i}{\alpha_2}\, ,
\ee
for the inner product of the two inputs, normalized by the input dimension $n_0$. In terms of this object, we can rewrite the first-layer two-point correlator \eqref{eq:deep-linear-two-point-first-layer} as
\be\label{eq:deep-linear-two-point-first-layer-simple}
\E{\z{i_1}{\alpha_1}{1}\z{i_2}{\alpha_2}{1}}=\delta_{i_1i_2} C_W \PH_{\alpha_1\alpha_2}\, .
\ee




\index{Wick contraction}\index{correlator!two-point}
Next, we could mindlessly repeat the same exercise to get the two-point correlator in any arbitrary layer, using the defining equation~\eqref{eq:deep-linear-preactivation-compact} to express $\z{i}{\alpha}{\ell}$ in terms of the input. Instead,
in order to practice our recursive approach,
let's 
evaluate the two-point correlator recursively.
To do so, we inductively assume that the two-point correlator at the $\ell$-th layer is known and then derive the two-point correlator at the $(\ell+1)$-th layer.
Using the iteration equation~\eqref{eq:deep-linear-foward-pass} with the bias set to zero, we find
\begin{align}
\label{eq:two-point-function-deep-linear-layer-ell}
\E{\z{i_1}{\alpha_1}{\ell+1}\z{i_2}{\alpha_2}{\ell+1}}=&\sum_{j_1,j_2=1}^{n_{\ell}}\E{\Ti{W}{i_1j_1}{\ell+1}\Ti{W}{i_2j_2}{\ell+1}\z{j_1}{\alpha_1}{\ell}\z{j_2}{\alpha_2}{\ell}}\, \\
=&\sum_{j_1,j_2=1}^{n_{\ell}}\E{\Ti{W}{i_1j_1}{\ell+1}\Ti{W}{i_2j_2}{\ell+1}}\E{\z{j_1}{\alpha_1}{\ell}\z{j_2}{\alpha_2}{\ell}}\, \notag \\
=&\delta_{i_1 i_2}C_W \frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\z{j}{\alpha_1}{\ell}\z{j}{\alpha_2}{\ell}}\, ,\nonumber%
\end{align}
where to go from the first line to the second line we used the fact that the weights $W^{(\ell+1)}$ of the $(\ell+1)$-th layer are statistically independent from the preactivations $z^{(\ell)}$ in the $\ell$-th layer, and to go from the second line to the third line we Wick-contracted the two weights and substituted in the variance \eqref{eq:deep-linear-weight-init}. Notice that at \emph{any} layer, the two-point correlator is proportional to the \terminate{Kronecker delta} $\delta_{i_1 i_2}$, vanishing unless the \terminate{neural indices} $i_1$ and $i_2$ are the same. 
With that in mind, let us decompose the two-point correlator as
\be\label{eq:deep-linear-covariance-notation}
\E{\z{i_1}{\alpha_1}{\ell}\z{i_2}{\alpha_2}{\ell}}\equiv  \delta_{i_1 i_2 }\Ti{G}{\alpha_1\alpha_2}{\ell}\, ,
\ee
and introduce a generalization of the above notation \eqref{eq:deep-linear-normalized-inner-product-inputs} for an arbitrary layer $\ell$.
Multiplying this equation by $\delta_{i_1i_2}$, summing over $i_1,i_2=1,\ldots,n_{\ell}$ and dividing it by $n_{\ell}$, the quantity $\Ti{G}{\alpha_1\alpha_2}{\ell}$ can also be expressed as 
\be\label{eq:deep-linear-covariance-notation-2}
\Ti{G}{\alpha_1\alpha_2}{\ell}= \frac{1}{n_{\ell}}\sum_{j=1}^{n_{\ell}}\E{\z{j}{\alpha_1}{\ell}\z{j}{\alpha_2}{\ell}}\, ,
\ee
and can thus be thought of as the average inner-product of preactivations in the $\ell$-th layer, divided by the number of neurons in the layer $n_{\ell}$. 
This inner product
depends on sample indices \emph{only} and lets us interpret $\Ti{G}{\alpha_1\alpha_2}{\ell} \equiv G^{(\ell)}\!\le(x_{\alpha_1},x_{\alpha_2}\ri)$ as the covariance of the two inputs, $x_{\alpha_1}$ and $x_{\alpha_2}$, after passing through an $\ell$-layer \terminate{deep linear network}.

With all this notation introduced and fully interpreted, it's easy to see that the above recursion~\eqref{eq:two-point-function-deep-linear-layer-ell} can be compactly summarized by %
\be\label{eq:deep-linear-kernel-recursion}
 \Ti{G}{\alpha_1\alpha_2}{\ell+1}=C_W  \Ti{G}{\alpha_1\alpha_2}{\ell}\, ,
\ee
which describes how the covariance $\Ti{G}{\alpha_1\alpha_2}{\ell}$ evolves from layer to layer.
Apparently, to transform the covariance from layer $\ell$ to layer $\ell+1$, we simply multiply by the constant $C_W$.  The initial condition $\Ti{G}{\alpha_1\alpha_2}{0}$ is given by the inner product of the two inputs \eqref{eq:deep-linear-normalized-inner-product-inputs}, and the solution is an exponential
\be\label{eq:deep-linear-exponential-solution}
\Ti{G}{\alpha_1\alpha_2}{\ell} = \le(C_W\ri)^{\ell}\Ti{G}{\alpha_1\alpha_2}{0}\, ,
\ee
as is typical for a repeated application of 
matrix multiplication. 
Note that the factor of the width $n_{\ell}$ in the variance of the weights~\eqref{eq:deep-linear-weight-init} nicely dropped out, indicating that this was in fact the proper way to scale the variance.




\subsubsection{Physics: criticality}
\index{criticality}
Already at this point our analysis illustrates an interesting and very general phenomenon. 
Considering the solution \eqref{eq:deep-linear-exponential-solution}, generically one of two things happens.
If $C_W> 1$, the covariance blows up exponentially, quickly being driven to a fixed point $\Tif{G}{\alpha_1\alpha_2} = \infty$ for all pairs of inputs and leading to a divergent network output.
If $C_W< 1$, the covariance exponentially decays to a fixed point $\Tif{G}{\alpha_1\alpha_2} = 0$ for all pairs of inputs, quickly curtailing any data dependence in the network output. 
Any time an observable approaches a value exponentially quickly, we'll refer to the limiting value as a \textbf{trivial fixed point}\index{fixed point!trivial|textbf}\index{trivial fixed point|see{fixed point}}. The value $\Tif{G}{\alpha_1\alpha_2} = \infty$ associated with $C_W>1$ and the value $\Tif{G}{\alpha_1\alpha_2} = 0$ associated with $C_W<1$ are prime examples of a trivial fixed point.

Exploring this further, first note that
the diagonal part of the covariance at the output layer $L$ estimates the typical magnitude of the output for a given input $\x{i}{\alpha}$
\be
\Ti{G}{\alpha\alpha}{L} =  \E{\frac{1}{n_{L}}\sum_{j=1}^{n_{L}}\le(\z{j}{\alpha}{L}\ri)^2} \, .
\ee
With this observable in mind, the aforementioned exponential behavior should immediately set off alarm bells, signaling either some sort of numerical instability ($C_W>1$) or loss of information ($C_W<1$). 
In addition, note that the target values for the different components of the network output are typically $\o{1}$ numbers, neither exponentially large nor small. Such exponential behavior of the network should thus make it extremely difficult to learn to approximate the desired function.
In this way, this \emph{exploding and vanishing covariance problem}\index{exploding and vanishing kernel problem} is a baby version of the infamous \terminate{exploding and vanishing gradient problem} -- a well-known obstacle to gradient-based training of deep networks -- which we shall make more precise in~\S\ref{ch:eft-ntk}. %

However, we were actually a little too quick in our analysis before: what happens if we tune the weight variance $C_W$ so that it's precisely equal to $1$? This is clearly a special point in the hyperparameter space of initialization, separating the exponentially growing solution from the exponentially decaying solution. Going back to the recursion \eqref{eq:deep-linear-kernel-recursion}, we see that if $C_W=1$ then the covariance is fixed $\Ti{G}{\alpha_1\alpha_2}{\ell}=\Ti{G}{\alpha_1\alpha_2}{0}\equiv\Tif{G}{\alpha_1\alpha_2}$, manifestly preserving the full covariance of the input data even after passing through many layers of the \terminate{deep linear network}.
This is a bona fide \textbf{nontrivial fixed point}\index{fixed point!nontrivial|textbf}\index{fixed point!nontrivial|seealso{criticality}}, as it doesn't exponentially trivialize the structure of input data.
Thus, at least at this heuristic level of analysis, choosing $C_W=1$  appears to be essential for preserving the structure of the input data in a numerically stable manner. More generally, flowing to a nontrivial fixed point seems to be a necessary condition for deep networks to do anything useful.

\index{spin|seealso{\texttt{bit} (unit of entropy)}}\index{critical initialization hyperparameters|see{initialization hyperparameters}}\index{initialization hyperparameters!critical|textbf}\index{criticality}\index{statistical physics}\index{critical phenomena}\index{magnetism}\index{spin}\index{temperature}\index{iron}\index{atom}\index{paramagnetism}\index{ferromagnetism}\index{phase transition}\index{magnetic field}
When we fine-tune\index{fine tuning} the \terminate{initialization hyperparameters} of a network so that the covariance avoids exponential behavior, we'll call them \textbf{critical initialization hyperparameters}.\footnote{This word choice is motivated by the analogy to critical phenomena in statistical physics. For instance, consider the prototypical example: a magnet made of iron. At high temperature, the magnetic moments -- or spins -- of the iron atoms point in random directions, leading to a paramagnetic phase without any coherent magnetic field. By contrast, at low temperature, the spins instead try to collectively orient in the same direction, leading to a ferromagnetic phase with coherent magnetic field -- think of the $\cap$-shaped cartoon magnet that children play with. A \terminate{critical temperature} separates these two phases of magnetism, and the magnet set to the critical temperature will exhibit very special behavior that is neither paramagnetism nor ferromagnetism but known as self-similarity.}
For deep linear networks\index{deep linear network}, the critical initialization hyperparameter $C_W=1$ separates two regimes, one with an exponentially growing covariance for $C_W>1$, and the other with an exponentially decaying covariance for $C_W<1$.
When the weight variance is tuned to criticality $C_W=1$, the network has a perfect self-similarity of the covariance, preserving it exactly through the evolution from layer to layer.


In \S\ref{ch:signalprop}, we will extend our analysis of \term{criticality} to MLPs that use any 
particular
activation function. And, as shall be seen further on
in~\S\ref{ch:NTHb} and~\S\ref{ch:eot},
tuning a network to criticality is \emph{critical} for any \emph{deep} network to be well behaved and perform useful tasks -- at least without otherwise employing ad-hoc
tricks to ensure that signals can propagate stably.






\section{Fluctuations}\label{sec:fluctuations_DLN}
\index{correlator!higher-point} \index{connected correlator!four-point}
Recall from~\S\ref{ch:tools} that if a distribution 
is Gaussian and has a zero mean, then the covariance 
completely specifies the distribution. 
If the preactivation distribution $p\!\le(z^{(\ell)}\Big\vert\D \ri)$ were Gaussian, this would
mean that the critical tuning of the one initialization hyperparameter $C_W = 1$ would be sufficient to ensure that any observable is well behaved. However, if the distribution $p\!\le(z^{(\ell)}\Big\vert\D \ri)$ is not Gaussian, then it's not clear a priori whether observables depending on higher-point connected correlators will be well behaved with the same tuning. In principle, such observables could require other tunings of $C_W$ that are incompatible with the critical setting $C_W = 1$ for the covariance $\Ti{G}{\alpha_1\alpha_2}{\ell}$. To settle this question, let's look at the next simplest observable, the connected four-point correlator. As before, we'll go through the math first and discuss the physics second.

In this section and the next, to simplify the algebra we'll focus on correlators of preactivations that are evaluated only on a single input $x_{\alpha}=x$. This is sufficient to qualitatively highlight the importance of the higher-point correlators while letting us avoid the interference of some annoying technical manipulations.
Accordingly, in these sections we will drop the \terminate{sample indices} on preactivations and  denote the covariance as
\be
G_{2}^{(\ell)}\equiv\Ti{G}{\alpha\alpha}{\ell}=G^{(\ell)}(x,x) \, .
\ee
In the next chapter, we'll consider the fully general case.

\subsubsection{Math: recursion for the four-point correlator}\index{correlator!four-point}
As we did for the two-point correlator in the previous section, we'll begin by working out the four-point correlator in the first layer and 
then next derive and solve a recursion for the correlator in the deeper layers.
First for the first layer, using the defining equation~\eqref{eq:deep-linear-preactivations-first-layer} with the sample index omitted,
we have
for the \emph{full} four-point correlator
\begin{align}\label{eq:deep-linear-four-point-first-layer}
&\E{z_{i_1}^{(1)}z_{i_2}^{(1)}z_{i_3}^{(1)}z_{i_4}^{(1)}}\, \\
=&\sum_{j_1,j_2,j_3,j_4=1}^{n_0}\E{\Ti{W}{i_1j_1}{1}\Ti{W}{i_2j_2}{1}\Ti{W}{i_3j_3}{1}\Ti{W}{i_4j_4}{1}}x_{j_1}x_{j_2}x_{j_3}x_{j_4}\, \nonumber\\
=&\frac{C_W^2}{n_0^2}\sum_{j_1,j_2,j_3,j_4=1}^{n_0}\le(\delta_{i_1i_2}\delta_{j_1j_2}\delta_{i_3i_4}\delta_{j_3j_4}+\delta_{i_1i_3}\delta_{j_1j_3}\delta_{i_2i_4}\delta_{j_2j_4}+\delta_{i_1i_4}\delta_{j_1j_4}\delta_{i_2i_3}\delta_{j_2j_3}\ri)x_{j_1}x_{j_2}x_{j_3}x_{j_4}\, \nonumber\\
=&C_W^2  \le(\delta_{i_1i_2}\delta_{i_3i_4}+\delta_{i_1i_3}\delta_{i_2i_4}+\delta_{i_1i_4}\delta_{i_2i_3}\ri)\le(\PH_2\ri)^2\, \notag.
\end{align}
where to go from line two to line three, we made three distinct pairings for the two Wick contractions\index{Wick contraction} of the four weights, 
and then used the weight variance~\eqref{eq:deep-linear-weight-init} to evaluate each contraction.
To get to the final line, we evaluated the sums over the $j$ indices and then substituted using our definition of the inner product~\eqref{eq:deep-linear-normalized-inner-product-inputs}, which 
for a single input simply reads
\be
\PH_2=\frac{1}{n_0}\sum_{j=1}^{n_0}x_j x_j\, .
\ee
Comparing this result~\eqref{eq:deep-linear-four-point-first-layer} with the two-point correlator in the first layer~\eqref{eq:deep-linear-two-point-first-layer-simple}, we note that this answer is precisely what we'd expect for the full four-point correlator if the preactivation distribution were exactly Gaussian. Thus, deep linear networks appear to be simply Gaussian after a single layer, at least at the four-point correlator level of analysis.\footnote{In the next chapter, we'll show very generally that the preactivation distribution is always Gaussian in the first layer.}
\index{deep linear network}

This Gaussianity does \emph{not} hold in deeper layers.  To see that, let's derive and solve a recursion for the four-point correlator. 
Beginning with the iteration equation~\eqref{eq:deep-linear-foward-pass} with zero bias, we find
\begin{align}\label{eq:deep-linear-four-point-a}
&\E{z_{i_1}^{(\ell+1)}z_{i_2}^{(\ell+1)}z_{i_3}^{(\ell+1)}z_{i_4}^{(\ell+1)}}\, \\
=&\sum_{j_1,j_2,j_3,j_4=1}^{n_{\ell}}\E{\Ti{W}{i_1j_1}{\ell+1}\Ti{W}{i_2j_2}{\ell+1}\Ti{W}{i_3j_3}{\ell+1}\Ti{W}{i_4j_4}{\ell+1}}\E{z_{j_1}^{(\ell)}z_{j_2}^{(\ell)}z_{j_3}^{(\ell)}z_{j_4}^{(\ell)}}\, \nonumber\\
=&\frac{C_W^2}{n_{\ell}^2}\sum_{j_1,j_2,j_3,j_4=1}^{n_{\ell}}\le(\delta_{i_1i_2}\delta_{j_1j_2}\delta_{i_3i_4}\delta_{j_3j_4}+\delta_{i_1i_3}\delta_{j_1j_3}\delta_{i_2i_4}\delta_{j_2j_4}+\delta_{i_1i_4}\delta_{j_1j_4}\delta_{i_2i_3}\delta_{j_2j_3}\ri)\, \nonumber\\
&\quad \quad \quad \quad \quad \quad \times \E{z_{j_1}^{(\ell)}z_{j_2}^{(\ell)}z_{j_3}^{(\ell)}z_{j_4}^{(\ell)}}\,  \nonumber \\
=&C_W^2\le(\delta_{i_1i_2}\delta_{i_3i_4}+\delta_{i_1i_3}\delta_{i_2i_4}+\delta_{i_1i_4}\delta_{i_2i_3}\ri)\frac{1}{n_{\ell}^2}\sum_{j,k=1}^{n_{\ell}}\E{z_{j}^{(\ell)}z_{j}^{(\ell)}z_{k}^{(\ell)}z_{k}^{(\ell)}}\, ,\nonumber
\end{align}
where on the second line we used the independence of the $(\ell+1)$-th-layer weights from the $\ell$-th-layer preactivations, 
on the third line we again made three distinct pairings for the two pairs of Wick contractions\index{Wick contraction} of 
the four weights, and on the last line we made judicious use of the Kronecker deltas to collapse the sums.  
\index{Kronecker delta} 

\index{tensor decomposition!four-point correlator}
Now, we see from this recursion that at \emph{any} layer the full four-point correlator is proportional to the factor $\le(\delta_{i_1i_2}\delta_{i_3i_4}+\delta_{i_1i_3}\delta_{i_2i_4}+\delta_{i_1i_4}\delta_{i_2i_3}\ri)$, a fixed tensor structure that specifies the neural-index dependence of the correlator. Thus by decomposing the full four-point correlator as
\be\label{eq:four-point-decompose}
\E{z_{i_1}^{(\ell)}z_{i_2}^{(\ell)}z_{i_3}^{(\ell)}z_{i_4}^{(\ell)}}\equiv \le(\delta_{i_1i_2}\delta_{i_3 i_4}+ \delta_{i_1i_3}\delta_{i_2 i_4}+ \delta_{i_1i_4}\delta_{i_2 i_3}\ri)G_4^{(\ell)} \, ,
\ee
we can put all of the layer dependence into this simpler object $\Ti{G}{4}{\ell}$ and not worry about \terminate{neural indices} in our recursion.
In terms of this decomposition, the result~\eqref{eq:deep-linear-four-point-first-layer} for the correlator in the first layer becomes
\be\label{eq:deep-linear-four-point-first-layer-after-decompose}
\Ti{G}{4}{1} = C_W^2 \le( \Ti{G}{2}{0}\ri)^2 \, ,
\ee
and the final factor in the above recursion~\eqref{eq:deep-linear-four-point-a} can be rewritten as
\be
\frac{1}{n_{\ell}^2}\sum_{j,k=1}^{n_{\ell}}\E{z_{j}^{(\ell)}z_{j}^{(\ell)}z_{k}^{(\ell)}z_{k}^{(\ell)}}=\frac{1}{n_{\ell}^2}\sum_{j,k=1}^{n_{\ell}} \le(\delta_{jj}\delta_{kk}+ \delta_{jk}\delta_{jk}+ \delta_{jk}\delta_{kj}\ri)G_4^{(\ell)}=\le(1+\frac{2}{n_{\ell}}\ri)G_4^{(\ell)}\, .
\ee
Using this, the entire recursion above~\eqref{eq:deep-linear-four-point-a} can be rewritten simply as a recursion for $\Ti{G}{4}{\ell}$ as
\be
G_4^{(\ell+1)}=C_W^2\le(1+\frac{2}{n_{\ell}}\ri)G_4^{(\ell)}\, .
\ee
This recursion, with the initial condition set by~\eqref{eq:deep-linear-four-point-first-layer-after-decompose}, has a simple solution
\begin{align}\label{eq:deep-linear-four-point-solution}
G_4^{(\ell)} =& C_W^{2\ell}\le[\prod_{\ell'=1}^{\ell-1}\le(1+\frac{2}{n_{\ell'}}\ri)\ri]\le(\Ti{G}{2}{0}\ri)^2 \, \\
=&\le[\prod_{\ell'=1}^{\ell-1}\le(1+\frac{2}{n_{\ell'}}\ri)\ri] \le(\Ti{G}{2}{\ell}\ri)^2\, ,\nonumber
\end{align}
where in the final line we substituted in the solution~\eqref{eq:deep-linear-exponential-solution} for the covariance.
Now let's 
extract some physics from this compact formula.


\subsubsection{Physics: large-$n$ expansion, non-Gaussianities, interactions, and fluctuations}
\index{$1/n$ expansion}\index{infinite-width limit!of deep linear networks}
To start, we note that the four-point correlator \eqref{eq:deep-linear-four-point-solution} drastically simplifies in the limit of an infinite number of neurons per hidden layer $(n_{\ell}\to\infty)$. In such a limit, the solution~\eqref{eq:deep-linear-four-point-solution} degenerates to
\be
G_4^{(\ell)}= \le(\Ti{G}{2}{\ell}\ri)^2 \, ,
\ee
and the full four-point correlator~\eqref{eq:four-point-decompose} becomes
\be
\E{z_{i_1}^{(\ell)}z_{i_2}^{(\ell)}z_{i_3}^{(\ell)}z_{i_4}^{(\ell)}} %
= \le(\delta_{i_1i_2}\delta_{i_3 i_4}+ \delta_{i_1i_3}\delta_{i_2 i_4}+ \delta_{i_1i_4}\delta_{i_2 i_3}\ri) \le(\Ti{G}{2}{\ell}\ri)^2 \, .%
\ee
This is exactly what we'd find if the preactivation distribution were Gaussian: the four-point correlator is determined entirely by the two-point correlator, with the tensor structure determined by \terminate{Wick's theorem}. In fact, as we will show
in the next chapter, for any MLP with any particular choice of a nonlinear activation function, the preactivation distribution is governed by Gaussian statistics in this infinite-width limit, implying no interactions between the neurons in such a limit.
\index{correlator!four-point}\index{infinite-width limit!of deep linear networks}
However, despite the rather large computational resources that \neo{big tech} can throw at machine-learning problems, realistic MLPs simply do not have an infinite number of neurons per layer. To understand such realistic MLPs, we'll have to back off this infinite-width limit.

To illustrate this most clearly, let's set all the hidden layer widths to be equal $n_1=n_2=\ldots=n_{L-1}\equiv n$. Then, evaluating \eqref{eq:deep-linear-four-point-solution}, the deviation from the infinite-width limit at the level of four-point correlator statistics is encoded by the difference
\begin{align}\label{eq:leading-four-point-correction}
G_4^{(\ell)}-\le(\Ti{G}{2}{\ell}\ri)^2=&\le[\le(1+\frac{2}{n}\ri)^{\ell-1} -1\ri]\le(\Ti{G}{2}{\ell}\ri)^2\, \\
=&\frac{2(\ell-1)}{n}\le(\Ti{G}{2}{\ell}\ri)^2+\o{\frac{1}{n^2}}\, ,\nonumber
\end{align}
where in the last line we expanded in $1/n$ and kept
the leading correction to the infinite-width limit.\footnote{This approximation is valid so long as the depth of the network doesn't grow too large.
Stay tuned for the analysis in the next section where we will discuss how this limit breaks down.} In particular, at criticality where $\Ti{G}{2}{\ell}$ is constant, this leading correction \eqref{eq:leading-four-point-correction} scales inversely proportionally with the width and proportionally with the depth.
Thus, the deviation from infinite width is proportional to the depth-to-width ratio of the network, our first encounter with this important \term{emergent scale}.
There are multiple ways to think about this finite-width correction.

\index{connected correlator!four-point}\index{nearly-Gaussian distribution}\index{coupling!quartic}
First, the connected four-point correlator~\eqref{eq:C4} is given by
\be\label{eq:four-point-decompose-connected}
\Ec{z_{i_1}^{(\ell)}z_{i_2}^{(\ell)}z_{i_3}^{(\ell)}z_{i_4}^{(\ell)}}{\Big|} = \le(\delta_{i_1i_2}\delta_{i_3 i_4}+ \delta_{i_1i_3}\delta_{i_2 i_4}+ \delta_{i_1i_4}\delta_{i_2 i_3}\ri)\le[G_4^{(\ell)}-\le(\Ti{G}{2}{\ell}\ri)^2\ri]  \, ,
\ee
which directly connects the difference~\eqref{eq:leading-four-point-correction} to our measure of non-Gaussianity for the distribution. We see that the non-Gaussianity grows as the network deepens, and the preactivation statistics in layer $\ell$ are \emph{nearly-Gaussian} so long as the emergent scale, the depth-to-width-ratio, remains perturbatively small.
From the action perspective, this means that the quartic coupling changes -- or \textbf{runs} -- as the layer at which we consider the preactivation distribution changes, with the coupling growing in proportion with layer $\ell$. \index{running coupling}

\index{connected correlator!four-point} 
Second,  in~\S\ref{sec:perturbation} we gave another interpretation for a nonzero connected four-point correlator as measuring \terminate{interactions} -- i.e.~the breakdown of \terminate{statistical independence} -- between the different components of the random vector.
To be very specific, let us look at a particular entry of the connected four-point correlator tensor with $i_1=i_2=j$ and $i_3=i_4=k$ for $j\ne k$. This entry can be expressed as
\be\label{eq:deep-linear-scale}
\E{\le(z_{j}^{(\ell)}z_{j}^{(\ell)}  -\Ti{G}{2}{\ell}\ri)\le(z_{k}^{(\ell)}z_{k}^{(\ell)}  -\Ti{G}{2}{\ell}\ri)} = \Ti{G}{4}{\ell} - \le(\Ti{G}{2}{\ell}\ri)^2\, , \quad \text{for}\ j\ne k\, .
\ee
This shows that the deviation of $z_{j}^{(\ell)}z_{j}^{(\ell)}$ from its mean value $\E{z_{j}^{(\ell)}z_{j}^{(\ell)}}=\Ti{G}{2}{\ell}$ on a particular neuron $j$ is correlated with the same deviation from the mean on a different neuron $k$.
We can thus interpret the finite-width difference~\eqref{eq:leading-four-point-correction} as controlling intralayer interactions between distinct neurons, with the strength of the interactions growing with depth.

\index{fluctuations!in deep linear networks}\index{typicality}
Third, we can see that some observables that are deterministic in the \terminate{infinite-width limit} start to fluctuate at finite width.
To this end, let us consider the simple observable
\be
\O^{(\ell)}\equiv \O\!\le(z^{(\ell)}\ri)\equiv \frac{1}{n}\sum_{j=1}^{n} z_{j}^{(\ell)}z_{j}^{(\ell)}\, , \quad \text{for}\ \ell< L\, ,
\ee
which captures the average magnitude of the preactivations over all the different neurons in a hidden layer $\ell$ for a given instantiation of the network weights. Its mean over different realizations of the weights is given by the expectation
\be
\E{\O^{(\ell)}}=\frac{1}{n}\sum_{j=1}^{n}\E{z_{j}^{(\ell)}z_{j}^{(\ell)}}=\Ti{G}{2}{\ell}\, ,
\ee
and the magnitude of this observable's fluctuation from instantiation to instantiation is measured by its variance
\begin{align}
\E{\le(\O^{(\ell)}-\E{\O^{(\ell)}}\ri)^2}=&\frac{1}{n^2}\sum_{j,k=1}^{n}\E{z_{j}^{(\ell)}z_{j}^{(\ell)}z_{k}^{(\ell)}z_{k}^{(\ell)}}-\le(\Ti{G}{2}{\ell}\ri)^2\, \\
=&\frac{1}{n^2}\sum_{j,k=1}^{n} \le(\delta_{jj}\delta_{kk}+ \delta_{jk}\delta_{jk}+ \delta_{jk}\delta_{kj}\ri)G_4^{(\ell)}-\le(\Ti{G}{2}{\ell}\ri)^2\, \nonumber\\
=&\frac{2}{n}G_4^{(\ell)}+ \le[G_4^{(\ell)}-\le(\Ti{G}{2}{\ell}\ri)^2\ri]\, \nonumber\\
=&\frac{2\ell}{n}\le(\Ti{G}{2}{\ell}\ri)^2+\o{\frac{1}{n^2}}\, ,\nonumber
\end{align}
where in the last step we recalled the expansion~\eqref{eq:leading-four-point-correction} for the finite-width difference.
As promised, $\O^{(\ell)}$ is deterministic at infinite width, since this variance is suppressed by $1/n$ and vanishes identically in the \terminate{infinite-width limit}.
However, as we back off the \terminate{infinite-width limit}, the variance grows linearly with depth at criticality due to the finite-width correction~\eqref{eq:leading-four-point-correction}.
As such depth increases, the fluctuation becomes larger, meaning that 
 the \emph{typical} magnitude of the preactivations $\O^{(\ell)}$ measured on any given realization of the deep linear network may deviate more from the mean value $\E{\O^{(\ell)}}=\Ti{G}{2}{\ell}$.












All these finite-width effects -- be they non-Gaussianities, intralayer interactions, or finite-width fluctuations --  are proportional to the depth-to-width ratio of the network.
This is perhaps the most important recurring theme of the book: the leading finite-width contributions at \terminate{criticality} grow linearly with depth, despite being suppressed by the inverse of the layer widths.
Since the depths of a real networks with at least one hidden layer are bounded from below as $L \ge 2$ -- that is, at minimum such networks have one hidden layer and one output layer -- in practice, networks of any finite size will express some amount of finite-width effects in their \terminate{output distribution} proportional to their aspect ratio $L/n$.
As we will see later in \S\ref{ch:signalprop}, this emergent scaling will hold very generally
at \terminate{criticality} for networks with any particular activation function.

\index{representation learning}
Thus, the deeper a network is, the less the infinite-width Gaussian description will apply, due to accumulation of finite-width \terminate{fluctuations}. This is actually a good thing because, as we shall emphasize more in~\S\ref{ch:features}, 
infinite-width networks do not have correlations among neurons within a layer 
and cannot learn %
nontrivial
representations from input data.
Real useful deep learning systems that are used in practice do both of these things, and our later analysis will show that
deeper networks have the capacity to do more of these things.

Depth, however, is a double-edged sword.
As the overall depth $L$ of a network becomes comparable to its hidden-layer width, fluctuations can begin to dominate. %
In particular, such extremely deep networks will have a huge variation in observables from instantiation to instantiation. Thus, even if we choose the critical initialization hyperparameter $C_W=1$, in some instantiations signals blow up, in other instantiations signals decay, and rarely do they stay tamed to be of order one. From a practical point of view, these networks are pretty useless.






\index{architecture hyperparameters}\index{initialization hyperparameters}
This set of circumstances is actually very fortuitous from a theorist's vantage point: our \neo{effective theory of deep learning} is most accurate when the 
aspect ratio
$L/n$ of the network is small but nonzero -- due to the applicability of the perturbative large-width expansion -- and this is exactly the setting of these architecture hyperparameters where networks work best
in practice.
In fact, one could expect that balancing the utility of nonzero depth for learning features\index{feature} against the cost of growing fluctuations 
could result in some optimal
aspect ratio  $L/n$ for MLPs of a particular activation, just as we saw that there is a correct tuning for the initialization hyperparameter $C_W$ for deep linear networks. We will return to this question of tuning $L/n$ when we discuss \terminate{inductive bias} in~\S\ref{ch:bayesian-inference} after first redoing our analysis of 
\terminate{criticality} and \terminate{fluctuations} %
for arbitrary activation functions
in the following two chapters, \S\ref{ch:ngp} and \S\ref{ch:signalprop}. In particular, in these chapters we will understand how the statistics of the preactivations run\index{running coupling} with depth, and see the emergence of the depth-to-width ratio as a scale that controls the validity of 
the perturbative \terminate{$1/n$ expansion}, as was the case here for deep linear networks\index{deep linear network}.



Quite generally, in the regime where perturbation theory works, the finite-width corrections grow linearly -- \emph{not} exponentially -- with depth and the network remains well behaved. By contrast, when the depth-to-width ratio becomes large, perturbation theory breaks down, making it very difficult to analyze such networks. However, in the special case of deep linear networks\index{deep linear network} a nonperturbative analysis is possible. In the next section we'll illustrate explicitly what happens to deep linear networks\index{deep linear network} when the depth-to-width ratio grows very large in order to paint an intuitive picture of the way networks behave in this regime.

























\section{Chaos}\label{sec:solution_DLN}
\index{chaos|seealso{overly deep}}\index{chaos}
\index{correlator!higher-point}
In the last two sections we used the method of Wick contractions\index{Wick contraction} to derive recursions for the two-point and four-point correlators of deep linear networks\index{deep linear network}, which we then easily solved. Now, we will use this same method 
to compute all the higher-point correlators
in order to complete our goal 
of determining the full distribution $p\!\le(z^{(\ell)}\Big| \D \ri)$. 
So that we may first simplify the algebra and then focus on the interesting properties of this distribution,
we'll again evaluate the correlators only on a single input $x_\alpha=x$
and drop the \terminate{sample indices} in all of the following equations.
Math then physics.




\subsubsection{Math: recursions for six-point and higher-point correlators}
Starting with the first layer, let's compute a general $2m$-point \emph{full} correlator. As this involves many Wick contractions\index{Wick contraction}, it might be helpful to remind yourself of the formal statement of \terminate{Wick's theorem} by flipping back to \S\ref{sec:Gauss} and consulting \eqref{eq:Wick-multi}\ldots. Good.

Now, using the defining equation~\eqref{eq:deep-linear-preactivations-first-layer} to express the first-layer preactivations in terms of the input, we get
\begin{align}\label{eq:deep-linear-gaussian-first-layer}
&\E{\Ti{z}{i_1}{1}\Ti{z}{i_2}{1}\cdots \Ti{z}{i_{2m-1}}{1}\Ti{z}{i_{2m}}{1}} \\
\notag
=&\sum_{j_1,\ldots,j_{2m}=1}^{n_0}\E{\Ti{W}{i_1j_1}{1}\Ti{W}{i_2j_2}{1}\cdots\Ti{W}{i_{2m-1}j_{2m-1}}{1}\Ti{W}{i_{2m}j_{2m}}{1}}x_{j_1}x_{j_2}\cdots x_{j_{2m-1}}x_{j_{2m}} \\
\notag
=&\le( \sum_{\text{all parings}}\delta_{i_{k_1} i_{k_2}} \cdots \delta_{i_{k_{2m-1}}i_{k_{2m}}}\ri)C_W^m\le(\Ti{G}{2}{0}\ri)^m \,  \notag\\
=&\le( \sum_{\text{all parings}}\delta_{i_{k_1} i_{k_2}} \cdots \delta_{i_{k_{2m-1}}i_{k_{2m}}}\ri)\le(\Ti{G}{2}{1}\ri)^m \, , \notag
\end{align}
where, as before we used \terminate{Wick's theorem} to determine the Wick contractions\index{Wick contraction} and then evaluated each contraction by substituting in \eqref{eq:deep-linear-weight-init} for the variance. Here, the sum is over all the possible pairing of the $2m$ auxiliary indices, $k_1,\ldots, k_{2m}$, resulting in $(2m-1)!!$ distinct terms, and on the final line  we substituted in the solution~\eqref{eq:deep-linear-exponential-solution} for the first-layer covariance.

The result \eqref{eq:deep-linear-gaussian-first-layer} confirms what we suspected in the last section,
that the preactivation distribution for the first layer is completely Gaussian. If this isn't clear by inspection, 
it's easy to check directly
-- via basically the same application of \terminate{Wick's theorem} -- 
that the correlators \eqref{eq:deep-linear-gaussian-first-layer} are precisely the $2m$-point correlators of a \terminate{Gaussian distribution} with zero mean and variance $\delta_{i_1i_2}G_2^{(1)}$.
In other words, the preactivation distribution in the first layer is governed by the quadratic action \index{action!quadratic}
\be
S\!\le(z^{(1)} \ri) = \frac{1}{2 G_2^{(1)}} \sum_{i=1}^{n_1} z_i^{(1)} z_i^{(1)}   \, .
\ee



\index{correlator!six-point}
Before presenting a recursion for general $2m$-point correlators, let us work out the recursion for the six-point correlator in detail. Beginning with the iteration equation~\eqref{eq:deep-linear-foward-pass} with the bias set to zero,
we find
\begin{align}
&\E{\Ti{z}{i_1}{\ell+1}\Ti{z}{i_2}{\ell+1} \Ti{z}{i_{3}}{\ell+1}\Ti{z}{i_{4}}{\ell+1}\Ti{z}{i_{5}}{\ell+1}\Ti{z}{i_{6}}{\ell+1}}\, \\
=&\sum_{j_1,j_2,j_3,j_4,j_5,j_6 = 1}^{n_{\ell}}\E{\Ti{W}{i_1j_1}{\ell+1}\Ti{W}{i_2j_2}{\ell+1} \Ti{W}{i_3j_3}{\ell+1}\Ti{W}{i_4j_4}{\ell+1}\Ti{W}{i_5j_5}{\ell+1}\Ti{W}{i_6j_6}{\ell+1}}\E{\Ti{z}{j_1}{\ell}\Ti{z}{j_2}{\ell} \Ti{z}{j_{3}}{\ell}\Ti{z}{j_{4}}{\ell}\Ti{z}{j_{5}}{\ell}\Ti{z}{j_{6}}{\ell}}\, \nonumber\\
=& C_W^3\Big(\delta_{i_1 i_2}\delta_{i_3 i_4}\delta_{i_5 i_6}+\delta_{i_1 i_3}\delta_{i_2 i_4}\delta_{i_5 i_6}+\delta_{i_1 i_4}\delta_{i_2 i_3}\delta_{i_5 i_6}\, \nonumber\\
&\quad +\delta_{i_1 i_2}\delta_{i_3 i_5}\delta_{i_4 i_6}+\delta_{i_1 i_3}\delta_{i_2 i_5}\delta_{i_4 i_6}+\delta_{i_1 i_5}\delta_{i_2 i_3}\delta_{i_4 i_6}\, \nonumber\\
&\quad +\delta_{i_1 i_2}\delta_{i_5 i_4}\delta_{i_3 i_6}+\delta_{i_1 i_5}\delta_{i_2 i_4}\delta_{i_3 i_6}+\delta_{i_1 i_4}\delta_{i_2 i_5}\delta_{i_3 i_6}\, \nonumber\\
&\quad +\delta_{i_1 i_5}\delta_{i_3 i_4}\delta_{i_2 i_6}+\delta_{i_1 i_3}\delta_{i_5 i_4}\delta_{i_2 i_6}+\delta_{i_1 i_4}\delta_{i_5 i_3}\delta_{i_2 i_6}\, \nonumber\\
&\quad +\delta_{i_5 i_2}\delta_{i_3 i_4}\delta_{i_1 i_6}+\delta_{i_5 i_3}\delta_{i_2 i_4}\delta_{i_1 i_6}+\delta_{i_5 i_4}\delta_{i_2 i_3}\delta_{i_1 i_6} \Big)\frac{1}{n_{\ell}^3}\sum_{i,j,k=1}^{n_{\ell}}\E{\Ti{z}{i}{\ell}\Ti{z}{i}{\ell} \Ti{z}{j}{\ell}\Ti{z}{j}{\ell}\Ti{z}{k}{\ell}\Ti{z}{k}{\ell}}\, ,\nonumber
\end{align}
noting again the independence of the $(\ell+1)$-th layer weights from the $\ell$-th layer preactivations.
On the final line, we see that there were fifteen distinct ways to make the three Wick contractions of six weights.

\index{Kronecker delta}
As we saw for the four-point correlator, the structure of \terminate{neural indices} for the full six-point correlator is the same for \emph{any} layer and proportional to a constant tensor, given by the object in the parenthesis above with all those Kronecker deltas. This suggests a decomposition of the six-point correlator as %
\be\label{eq:deep-linear-six-point-decomposition}\index{tensor decomposition!six-point correlator}
\E{\Ti{z}{i_1}{\ell}\Ti{z}{i_2}{\ell} \Ti{z}{i_{3}}{\ell}\Ti{z}{i_{4}}{\ell}\Ti{z}{i_{5}}{\ell}\Ti{z}{i_{6}}{\ell}}\equiv \le(\delta_{i_1 i_2}\delta_{i_3 i_4}\delta_{i_5 i_6}+\ldots+\delta_{i_5 i_4}\delta_{i_2 i_3}\delta_{i_1 i_6}\ri) \Ti{G}{6}{\ell}\, ,
\ee
with the neural-dependence encapsulated by that complicated sum-over-products of Kronecker deltas and the layer dependence captured solely by $ \Ti{G}{6}{\ell}$.

Now, to find a recursion for $\Ti{G}{6}{\ell}$, we need to perform the sum
\be
\frac{1}{n_{\ell}^3}\sum_{i,j,k=1}^{n_{\ell}}\E{\Ti{z}{i}{\ell}\Ti{z}{i}{\ell} \Ti{z}{j}{\ell}\Ti{z}{j}{\ell}\Ti{z}{k}{\ell}\Ti{z}{k}{\ell}}\, 
\ee
after substituting in the decomposition \eqref{eq:deep-linear-six-point-decomposition}.
With the given pattern of \terminate{neural indices}, there are 
really only three types of terms in the sum. In particular, there is one term that looks like this
\be
\frac{1}{n_{\ell}^3}\sum_{i,j,k=1}^{n_{\ell}} \delta_{ii}\delta_{jj}\delta_{kk}=1\, ,
\ee
six terms that look like this
\be
\frac{1}{n_{\ell}^3}\sum_{i,j,k=1}^{n_{\ell}} \delta_{ij}\delta_{ji}\delta_{kk}=\frac{1}{n_{\ell}}\, ,
\ee
and eight terms that look like this
\be
\frac{1}{n_{\ell}^3}\sum_{i,j,k=1}^{n_{\ell}} \delta_{i j}\delta_{jk}\delta_{ki}=\frac{1}{n_{\ell}^2}\, .
\ee
Putting all these terms together, we find a recursion for the layer-dependence of the full six-point correlator
\be
G_{6}^{(\ell+1)}=C_W^3 \le(1+\frac{6}{n_{\ell}}+\frac{8}{n_{\ell}^2}\ri) G_{6}^{(\ell)}\, ,
\ee
which has a simple solution
\begin{align}
G_{6}^{(\ell)}=&C_W^{3\ell} \le[\prod_{\ell'=1}^{\ell-1}\le(1+\frac{6}{n_{\ell'}}+\frac{8}{n_{\ell'}^2}\ri)\ri]\le(G_{2}^{(0)} \ri)^3\, \\
=&\le[ \prod_{\ell'=1}^{\ell-1}\le(1+\frac{6}{n_{\ell'}}+\frac{8}{n_{\ell'}^2}\ri)\ri]\le(G_{2}^{(\ell)} \ri)^3 \nonumber\, .
\end{align}
Here, we used the initial condition \eqref{eq:deep-linear-gaussian-first-layer} $\Ti{G}{6}{0}=\le(\Ti{G}{2}{0} \ri)^3$, and on the final line we substituted in our solution for the variance of a single input~\eqref{eq:deep-linear-exponential-solution}.

\index{correlator!$2m$-point}
Similarly, we can decompose an arbitrary $2m$-point correlator as
\be\label{eq:deep-linear-inductive-ansatz}
\E{\Ti{z}{i_1}{\ell}\Ti{z}{i_2}{\ell}\cdots \Ti{z}{i_{2m-1}}{\ell}\Ti{z}{i_{2m}}{\ell}}=\le(\sum_{\text{all parings}}\delta_{i_{k_1} i_{k_2}} \cdots \delta_{i_{k_{2m-1}}i_{k_{2m}}}\ri)G^{(\ell)}_{2m} \, ,
\ee 
and use a similar set of manipulations to show that the layer dependence $G^{(\ell)}_{2m}$ obeys a recursion
\be
G_{2m}^{(\ell+1)}=c_{2m}(n_{\ell})\, C_W^{m}  G_{2m}^{(\ell)}\, ,
\ee
with the combinatorial factor $c_{2m}(n)$ given by
\be\label{eq:combinatorial-2m}
 c_{2m}(n)=\le(1+\frac{2}{n}\ri)\le(1+\frac{4}{n}\ri)\cdots\le(1+\frac{2m-2}{n}\ri)=\frac{\le(\frac{n}{2}-1 + m\ri )!}{\le( \frac{n}{2} -1 \ri)!} \le( \frac{2}{n} \ri) ^{m}\, .
\ee
We included the explicit form of this factor only for completeness. If you insist on checking this factor, note that it reproduces the right combinatorial factors for $2m=2,4,6$,
though
we strongly suggest that you do not explicitly write out all of the terms for any other particular value of $m$.
Overall, this recursion is still just a simple sequence of multiplications, with a simple solution
\be\label{eq:2m-full-solution}
G_{2m}^{(\ell)}=\le[\prod_{\ell'=1}^{\ell-1} c_{2m}(n_{\ell}') \ri]  \le( G_{2}^{(\ell)}\ri)^m \ .
\ee
Enough with the math, time for the physics.\footnote{
If you \emph{do} want more math, 
check out \cite{ZavatonePehlevanFinite} for an alternative derivation of these $2m$-point correlators and a nonperturbative expression for the distribution $p\big(z^{(\ell)}\big| x \big)$.
}










\subsubsection{Physics: breakdown of perturbation theory and the emergence of chaos}
Let's play with this formula \eqref{eq:2m-full-solution} a bit by taking various limits. For simplicity, let's set all the hidden layer widths to be equal $n_1=n_2=\ldots=n_{L-1}\equiv n$,
and also focus only on \terminate{output distribution} $p\!\le(z^{(L)} \Big| x \ri)$. 



\bi
\item On the one hand, if we send the network width to infinity, $n\rightarrow\infty$, while keeping the depth $L$ fixed, then all the combinatorial factors~\eqref{eq:combinatorial-2m} become unity:
\be
\lim_{n\rightarrow\infty} c_{2m}(n)=1\, .
\ee
In this \terminate{infinite-width limit}, all the correlators~\eqref{eq:2m-full-solution} are given by their Gaussian values
\be\label{eq:deep-linear-higher-order-gaussian-limit}
G_{2m}^{(L)}=\le( G_{2}^{(L)}\ri)^m\, ,
\ee
and the \terminate{output distribution} $p\!\le(z^{(L)} \Big| x \ri)$ is precisely Gaussian.
More generally, even for multiple inputs the \terminate{output distribution} $p\!\le(z^{(L)} \Big| \D \ri)$ remains Gaussian,\index{Gaussian distribution} with covariance $\Ti{G}{\alpha_1\alpha_2}{L}= C_W^L\le(\frac{1}{n_0}\sum_{i=1}^{n_0}\x{i}{\alpha_1}\x{i}{\alpha_2}\ri)$.
As this distribution is equivalent to that of one-layer networks initialized with weight variance $C_W^L$, we see that such networks are not really deep after all.
\index{non-Gaussian distribution}\index{correlator!higher-point}
\item  On the other hand, if we send the depth to infinity, $L\rightarrow\infty$, while keeping the width $n$ fixed, then all the combinatorial factors are fixed and greater than one, $c_{2m}> 1$. This means that the higher-point correlators for $2m >2$ will all blow up exponentially as
\be\label{eq:deep-linear-higher-order-chaotic-limit}
G_{2m}^{(L)} = \Big[c_{2m}(n)\Big]^{L-1} \le(G_2^{(L)} \ri)^m \, .
\ee
Note that this behavior persists \emph{even if} we tune the network to \terminate{criticality} by setting $C_W=1$ so that the two-point correlator is fixed $G_2^{(\ell)}  = G_2^{(0)}$. This shows explicitly how our large-width analysis from the last section can break down if the network depth becomes too large.
Furthermore, the distribution implied by these correlators is extremely non-Gaussian, to say the least, and in practice 
the outputs of these networks will fluctuate chaotically from instantiation to instantiation. Such networks are entirely unusable. %
\item Clearly these limits do not commute, i.e., 
\be
\lim_{n \to \infty}  \lim_{L \to \infty} G_{2m}^{(L)}\neq \lim_{L \to \infty}  \lim_{n \to \infty}G_{2m}^{(L)}\, .
\ee
However, we can construct an \emph{interpolating solution} by sending both the width and depth to infinity, $n,L\rightarrow\infty$, while keeping 
their ratio fixed:
\be
\ratio\equiv \frac{L}{n}\, .
\ee
Noting that we can expand the combinatorial factors as
\be
c_{2m}(n)=1+\frac{1}{n}\le(\sum_{s=1}^{m-1}2s\ri)+\o{\frac{1}{n^2}}=1+\frac{m(m-1)}{n}+\o{\frac{1}{n^2}}\, ,
\ee
and then using the well-known formula for the exponential
\be
\lim_{L\rightarrow\infty}\le[1+\frac{a}{L}+\o{\frac{1}{L^2}}\ri]^{L}=e^{a}\, , 
\ee
we can construct a limiting value for any correlator at a given value of $m$ and fixed aspect ratio $r$:
\be\label{eq:interpolating}
G_{2m}^{(L)}\rightarrow e^{m(m-1)\ratio}\le( G_{2}^{(L)}\ri)^m\, .
\ee
This solution interpolates between the two extreme limits: by sending $r\to 0$ we recover the Gaussian limit \eqref{eq:deep-linear-higher-order-gaussian-limit}, and by sending $r\to \infty$ we recover the chaotic limit \eqref{eq:deep-linear-higher-order-chaotic-limit} that demonstrates the breakdown of \terminate{criticality}.\footnote{This \emph{double-scaling limit} corresponds to neglecting terms that scale like $\frac{L}{n^2}$, $\frac{L^3}{n^5}$, $\frac{L^{120}}{n^{157}}$, etc., which are all subleading when the depth and the width are large, $n,L\rightarrow\infty$, but their ratio $r$ is fixed.

Furthermore, there is a very subtle point in using this interpolating solution -- albeit a theoretical subtlety -- when we consider not just a particular correlator at a given $2m$, but the set of \emph{all} the correlators.
Namely, for any \emph{finite} $n,L$ -- no matter how big -- there always exist higher-point correlators
for which the exponential approximation~\eqref{eq:interpolating} is invalid because the factor of $m(m-1)$ becomes too big. %
That is, since we constructed this interpolating solution assuming fixed $m$, such a solution can break down if $m$ is large enough.
}
\ei

\index{connected correlator!four-point}\index{running coupling}\index{coupling!quartic}\index{connected correlator!six-point}\index{nearly-Gaussian distribution}
Let us play a tiny bit more with the last interpolating formula~\eqref{eq:interpolating} at \terminate{criticality}
where $G_{2}^{(L)}=G_{2}^{(0)}$.
Here, the finite-width difference~\eqref{eq:leading-four-point-correction} that governs the connected four-point correlator~\eqref{eq:four-point-decompose-connected} becomes
\begin{align}
G_{4}^{(L)}-\le(G_{2}^{(L)}\ri)^2&=\le(e^{2r}-1\ri)\le(G_{2}^{(0)}\ri)^2\, \\
&=2r \le(G_{2}^{(0)}\ri)^2+\o{r^2}\, . \notag
\end{align}
This reproduces the \emph{running} of the quartic coupling with the depth-to-width ratio~\eqref{eq:leading-four-point-correction}. Similarly,
the corresponding quantity governing the layer dependence of the connected six-point correlator~\eqref{eq:C6} is given by
\begin{align}
G_{6}^{(L)}-3G_{2}^{(L)}G_{4}^{(L)}+2\le(G_{2}^{(L)}\ri)^3&=\le(e^{6r}-3e^{2r}+2\ri)\le(G_{2}^{(0)}\ri)^3\\
&=12r^2 \le(G_{2}^{(0)}\ri)^3 +\o{r^3}\, , \notag
\end{align}
which scales like the depth-to-width ratio \emph{squared}. Therefore, the connected six-point correlator is even more suppressed than the connected four-point correlator for large networks with sufficiently small depth-to-width ratio $r$. This is in accord with the comments we made in~\S\ref{sec:perturbation}: neural networks obey \emph{nearly-Gaussian} statistics, and
the connected correlators 
have a hierarchical structure. In particular, we see here that the scaling of the correlators is controlled by the same small parameter $r$,  with the higher-point connected correlators suppressed by a higher power of that parameter. This means that for small $r$, we should be able to consistently truncate our distribution and only compute up to a fixed order in $r$.






 
 
























