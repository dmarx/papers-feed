
\chapter{Bayesian Learning}
\label{ch:bayesian-inference}

\epigraph{\dots the mathematical rules of probability theory are not merely rules for calculating frequencies of `random variables'; they are also the unique rules for conducting inference (i.e.~plausible reasoning) of any kind, and we shall apply them in full generality to that end.}{E. T. Jaynes, explaining the theme of his book~\cite{jaynes2003probability}.\index{Jaynes, Edwin T.}}

\noindent{}In the previous three chapters, we've spent a considerable amount of \terminate{spacetime} analyzing the ensemble of wide neural networks at initialization.
In particular, through the $1/n$ expansion\index{$1/n$ expansion} and
deep asymptotic analysis, we've obtained a rather thorough understanding of the interplay between the architecture, width, depth, and \terminate{initialization hyperparameters}
that together
define
the effective
distribution of preactivations.



\index{deep learning!deep but not yet learning}
In this study, we've paid very careful attention to the \emph{deep} 
of \neo{deep learning} to the total neglect of the \emph{learning}. 
But this is a \emph{deep learning book}, not just a \emph{deep book}. Thus, in this chapter we will begin to learn about learning 
and -- if the titles of our chapters are any real guide to their contents --  will continue learning about learning for
the rest of the book.


We'll begin on our learning quest with a discussion of \neo{Bayesian inference}, as it provides a natural framework for thinking about learning in general.
We'll first explain in \S\ref{sec:PP-Bayes} the Bayesian approach to probability, in which probabilities are reinterpreted to represent the strength of our beliefs about the world according to different hypotheses. There, we'll learn 
that
the rules of \terminate{Bayesian inference} 
-- really the rules of logic
extended to
probabilistic
reasoning --
pick out a logically consistent way of incorporating newly observed information into the
probabilistic models representing our hypotheses.


From \S\ref{sec:PP-Bayes-2} on out, we'll see why this simple yet powerful framework enables us to analyze and then understand how deep neural networks learn from observed data.



In \S\ref{subsec:ForIO}, we'll detail how Bayesian model fitting\index{Bayesian inference!model fitting} works for neural networks.
First, we'll reinterpret our well-studied effective preactivation distribution as a \emph{prior} distribution, encoding our initial beliefs about the model outputs before observing any data.
With this as a starting point, the rules of \terminate{Bayesian inference} then imply a \terminate{learning algorithm} for sharpening our beliefs
so as to best fit our
observations.
The result of inference -- the \emph{posterior} distribution\index{posterior} -- further lets us make Bayesian predictions\index{Bayesian inference!prediction}
on novel inputs whose outputs we haven't observed but need to infer.
This naturally segues into a discussion of practical implementations: first we'll discuss approximation methods -- giving a Bayesian
interpretation to the gradient-based learning methods that we'll explore in the epochs following this chapter
-- and then we'll discuss an exact method on which the rest of the current chapter will be based.



In \S\ref{subsec:bayesian-model-comparison}, we'll expand our horizons by contemplating 
the ultimate question of
Life, the Universe, and Everything\index{Life, the Universe, \& Everything, HHGTTG}: Bayesian model comparison.\index{Bayesian inference!model comparison} 
We'll explain how to use Bayesian \emph{evidence}\index{Bayesian inference!evidence} to select between different plausible hypotheses, organized according to different choices of hyperparameters and network architectures, in order to pick the best ones.
Bayesian model comparison also gives us a quantitative means to address \emph{inductive biases}\index{inductive bias}, the often hidden assumptions built into deep learning models. As a bonus, we'll further see how \neo{Occam's razor}\index{Occam's razor|seealso{sparsity, principle of}} is automatically incorporated in the rules of \terminate{Bayesian inference} applied to such model comparison.
With these tools, we can really begin to address one of the fundamental questions we posed at the beginning of the book:
why do some neural network models perform so well while others fail?

These abstract discussions are then followed by an onslaught of concrete calculations for infinite- and finite-width neural networks in \S\ref{sec:infinite-posterior} and \S\ref{sec:finite-posterior}, respectively.

Some of these calculations reinforce the themes of the previous chapter.
We'll first show that Bayesian model comparison prefers critical initialization hyperparameters, giving additional evidence for the \emph{principle of criticality}\index{criticality!principle of} (\S\ref{subsec:Occam-criticality}). 
We'll also illustrate another role of finite-width interactions. Specifically, the accumulation of correlated \neo{fluctuations} induces an \terminate{inductive bias} for \neo{neural association}\index{neural association|seealso{Hebbian learning}}, leading to a propensity for \terminate{Hebbian learning}\index{Hebbian learning|seealso{neural association}} -- a learning principle inspired by biological neurons\index{biological neuron} (\S\ref{subsec:Hebbian}).

Some of these calculations
contrast qualitatively different characteristics of infinite- and finite-width models that are trained with exact Bayesian learning.
Analyzing the posterior distribution of network outputs, we'll see that correlations among different components of the output are nonzero at finite width only (\S\ref{subsec:absence-FF-Bayes}~$\!\perp$\S\ref{subsec:presence-FF-Bayes}). 
The resulting expressions will also make it clear why -- while theoretically quite tractable --  exact Bayesian learning is impractical for any dataset of reasonable size.
Next, analyzing the posterior distribution of hidden-layer representations, we'll see the absence/presence of representation learning at infinite/finite width (\S\ref{subsec:absence-RL-Bayes}~$\!\perp$\S\ref{subsec:presence-RL-Bayes}).
Overall, this contrasting will provide a valuable blueprint for when we later consider infinite- and finite-width models trained with gradient-based learning (\S\ref{ch:NTHb}~$\!\perp$\S\ref{ch:eot}).







 
 
 


















\section{Bayesian Probability}\label{sec:PP-Bayes}
\index{Bayesian inference}\index{Bayesian probability}
A Bayesian
always starts with a \textbf{hypothesis}\index{hypothesis (Bayesian inference)|textbf}\index{Bayesian inference!hypothesis|see{hypothesis (Bayesian inference)}}\index{Bayesian probability!hypothesis|see{hypothesis (Bayesian inference)}} $\Hypo$. Mathematically, a hypothesis is a mechanism for assigning numbers $p(A\vert\Hypo)$ to \emph{statements}\index{Bayesian probability!statements}\index{statement|see{Bayesian probability}} $A$ about the world. These statements are logical propositions -- such as ``it will rain tomorrow" or ``this image $x$ contains a cat" or ``the output value for this function $f(x)$ evaluated on an input $x$ is $z$'' -- and these numbers $p(A\vert\Hypo)$ represent the relative plausibilities of those statements according to the assumptions or model of the world summarized by the hypothesis $\Hypo$.
In the context of \terminate{machine learning}, $p(A\vert\Hypo)$ is often called a \term{probabilistic model}.

As this notation and discussion 
should make clear, these beliefs $p(A\vert\Hypo)$ are expressed in the language of probability\index{probability (branch of mathematics)}. However, the \emph{Bayesian} interpretation of the probability $p(A\vert\Hypo)$ subtlety differs from the \neo{ensemble} interpretation that we gave in \S\ref{sec:MLP_distribution}. Namely, rather than representing the statistics\index{statistics (of a random variable)!Bayesian interpretation} of a random variable -- the relative frequency or chance observing $A$, given the conditions $\Hypo$ -- this probability instead 
constitutes
the strength of our belief in the proposition $A$ according to the assumptions $\Hypo$.\footnote{
The 
\neo{ensemble} interpretation is often called \term{frequentist probability} 
when contrasted with \term{Bayesian probability}.
In this book, we use the interpretation
that is most appropriate for the particular problem under consideration: if we're instantiating models by randomly drawing parameters from an \terminate{initialization distribution}, it makes sense to analyze an ensemble; if we're
making inferences based on a fixed hypothesis or comparing different hypotheses, it makes sense to adopt the Bayesian perspective.
}
\index{Bayesian inference}
Further, with such a Bayesian perspective all of probability theory and statistical inference can be uniquely derived as a consequence of logical constraints on these beliefs $p(A\vert\Hypo)$.\footnote{
    See Jaynes'\index{Jaynes, Edwin T.} book \cite{jaynes2003probability} for 
    an
    extended development of this perspective
    for which our brief summary does not give 
    justice.
}
We'll next brief you through these constraints as
they form the foundation of this chapter
but, as we have been using probabilities for quite a while now in this book, let us be brief.


Formally, the first logical constraint is known as the \textbf{product rule}\index{Bayesian probability!product rule},
\be\label{eq:logical-product}
p(A,B\vert\Hypo)=p(A\vert B,\Hypo)\,p(B\vert\Hypo)=p(B\vert A,\Hypo)\,p(A\vert\Hypo)\, ,
\ee
where $p(A,B\vert\Hypo)$ represents a \emph{joint} belief in both $A$ \emph{and} $B$ 
according to the hypothesis\index{hypothesis (Bayesian inference)} $\Hypo$, while $p(A\vert B,\Hypo)$ represents a \emph{conditional} belief 
in $A$
according to $\Hypo$ \emph{given} that $B$ 
has been observed.
The second logical constraint is known as the \textbf{sum rule}\index{Bayesian probability!sum rule},
\be\label{eq:logical-sum}
p(A\vert\Hypo) = \sum_{B} p(A,B\vert\Hypo) \, ,
\ee
and relates the joint belief in $A$ and $B$ to a marginal belief in just $A$.\footnote{We essentially discussed this sum rule as \eqref{eq:sum-rule-mlp}
under \S\ref{sec:sum-rule} \emph{Marginalization Rules}. \index{marginalization rule}
}
Here, the symbol $\sum_{B}$ represents a sum over all the logically possible values of a discrete variable $B$,
or for a continuous variable it represents an integral.\footnote{Though (Bayesian) probably it's already clear if you've made it this deep in the book, as we cannot be (Bayesian) certain, let us clarify the meaning of the \emph{statement}\index{Bayesian probability!statements} $A$ inside the belief system $p(A\vert\Hypo)$. Sometimes a statement represents a \emph{fixed} logical proposition, such as $A=$ ``Schr\"{o}dinger's cat is alive'' with $p(A\vert\Hypo)$ encoding the plausibility of cat's aliveness.\index{Schr\"{o}dinger's cat} Sometimes a statement represents a binary \emph{variable}, such as $B=$ ``the livelihood of Schr\"{o}dinger's cat'' which takes values in $\le\{\text{dead}, \text{alive}\ri\}$ with $p(B\vert\Hypo)$ giving the distribution over the two binary outcomes.
More generally, the statement can represent observable outcomes $\O$ of experiments -- a.k.a.~\emph{observables}\index{observable} -- with $p(\O\vert \Hypo)$ encoding our relative belief in the plausibilities of the different outcomes, where such observables can take on a discrete or continuous spectrum of values. Prominent examples of such general observables for us include the model parameters $\theta$ and preactivations $z^{(\ell)}$.\index{Bayesian probability!statements}
}
This sum rule in particular implies the normalization condition if we assign $p(C\vert\Hypo) \equiv 1$ for the statement $C$ that holds with \neo{absolute certainty} according to $\Hypo$: 
\be
\sum_{B} p(B\vert\Hypo) = \sum_{B} p(C,B\vert\Hypo)= p(C\vert\Hypo) = 1 \, .
\ee







With these rules in mind, after fixing a hypothesis\index{hypothesis (Bayesian inference)} a Bayesian then gathers information in order to refine the plausibilities of different beliefs.
For instance, after \emph{observing} $A$, we may want to \emph{update} our beliefs about $B$.
Such \term{Bayesian inference} can be accomplished by noting that an algebraic rearrangement of the product rule~\eqref{eq:logical-product} tells us how our beliefs should change as we condition on additional information $A$:
\be\label{eq:Bayes-rule}
p(B\vert A,\Hypo)=\frac{p(A\vert B,\Hypo)\, p(B\vert\Hypo)}{p(A\vert\Hypo)}\, . %
\ee
This rearrangement is so important that it's given its own name, \textbf{Bayes' rule}\index{Bayesian probability!Bayes' rule|textbf}, and even each individual factor of the equation
is named as well:
\bi
\item The factor $p(B\vert\Hypo)$ is called the \term{prior}\index{Bayesian inference!prior|see{prior}} of $B$, thusly named because it quantifies our belief in $B$ \emph{a priori}; that is, it encodes our belief in $B$  based entirely on our model $\Hypo$ before we observe any additional information.
\item The factor $p(B\vert A,\Hypo)$ is called the \term{posterior}\index{Bayesian inference!posterior|see{posterior}} of $B$ given $A$, thusly named because it quantifies our belief in $B$ \emph{a posteriori} upon learning $A$; that is, it encodes how our model $\Hypo$ updates its belief in $B$ after
 observing $A$.
\item The factor $p(A\vert B,\Hypo)$ is called the \textbf{likelihood}\index{likelihood|see{Bayesian inference}}\index{Bayesian inference!likelihood|textbf}.
We'll elaborate more on its name and interpretation later in \S\ref{subsec:ForIO} where we talk about model \emph{fitting}.\index{Bayesian inference!model fitting}
\item The factor $p(A\vert\Hypo)$ is called the \textbf{evidence}\index{evidence|see{Bayesian inference}}\index{Bayesian inference!evidence|textbf} for $\Hypo$.
We'll elaborate more on its name and interpretation later in \S\ref{subsec:bayesian-model-comparison} where we talk about model \emph{comparison}.\index{Bayesian inference!model comparison}
\ei
Note that the posterior is automatically normalized:
\be\label{eq:Bayes-consistency}
\sum_{B} p(B\vert A,\Hypo)=\sum_{B}\frac{p(A\vert B,\Hypo)\, p(B\vert\Hypo)}{p(A\vert\Hypo)}=\sum_{B}\frac{p(A, B\vert\Hypo)}{p(A\vert\Hypo)}=\frac{p(A\vert\Hypo)}{p(A\vert\Hypo)}=1\, .
\ee
More importantly, Bayes' rule %
 is the only logically consistent way to update a set of beliefs after making observations. 







\section{Bayesian Inference and Neural Networks}\label{sec:PP-Bayes-2}
The Bayesian framework for inference can be used for building, updating, and reasoning with powerful probabilistic models\index{probabilistic model} of the world.
Let's now see how we can apply the Bayesian framework to \terminate{deep learning}, first 
for 
model fitting (\S\ref{subsec:ForIO}) and then 
for 
model comparison (\S\ref{subsec:bayesian-model-comparison}).\index{Bayesian inference!model fitting}\index{Bayesian inference!model comparison}





\subsection{Bayesian Model Fitting}\label{subsec:ForIO}
For neural networks, it's most natural to begin by discussing the  prior distribution $p(\theta \vert\Hypo)$ of the model parameters $\theta_{\mu}=\le\{\bias{i}{\ell},\W{ij}{\ell}\ri\}$.
This prior lets us quantify our initial beliefs about the particular values of the model parameters that determine our neural-network function approximator $f(x; \theta)$. The most common choice  is to simply reinterpret the \terminate{initialization distribution} of the ensemble,
\be\label{eq:parameter-prior}
p(\theta | \Hypo) \equiv \prod_{\ell=1}^{L}\le\{ \le[\prod_{i=1}^{n_
\ell} p\!\le(\bias{i}{\ell}\ri) \ri] \le[ \prod_{i=1}^{n_\ell}\prod_{j=1}^{n_{\ell-1} } p\!\le(\W{ij}{\ell}\ri) \ri] \ri\} \, ,
\ee
as our Bayesian prior distribution\index{prior}.
Here we recall that $p\!\le(\bias{i}{\ell}\ri)$ and $p\!\le(\W{ij}{\ell}\ri)$ -- given by \eqref{eq:full-bias-initialization} and \eqref{eq:full-weights-initialization} -- are zero-mean Gaussian distributions with bias variance $\Cb{\ell}$ and weight variance $\CW{\ell}/n_{\ell-1}$, respectively.

From the Bayesian perspective, these \terminate{initialization hyperparameters} are part of the hypothesis\index{hypothesis (Bayesian inference)} $\Hypo$.
This hypothesis\index{hypothesis (Bayesian inference)} $\Hypo$ also contains
our choice of architecture -- MLP\index{multilayer perceptron}, CNN\index{convolutional neural network}, \terminate{transformer}, etc.~-- as well as all the \terminate{architecture hyperparameters} within that architecture class -- e.g.~for MLPs we need to further select the depth $L$, the hidden-layer widths $n_\ell$, and the activation function $\sigma(z)$.
In short, $\Hypo$ is for $\Hypo$yperparameters\index{hyperparameters@$\Hypo$yperparameters}\index{hyperparameters@$\Hypo$yperparameters|seealso{hypothesis (Bay\-es\-ian inference)}}.\footnote{
To be strict, we should have always conditioned on 
$\Cb{\ell}$, $\CW{\ell}$, and $n_{\ell}$
whenever we discussed the initialization distribution: $p(\theta) \to p\!\le(\theta\Big\vert  n_0, \Cb{1}, \CW{1}, \ldots, n_{L-1}, \Cb{L}, \CW{L} \ri)$. 
Thankfully we've so far left, and will continue to leave, this type of detailed dependence implicit for notational simplicity. However, to underscore the importance of the hypothesis for \terminate{Bayesian inference}, in this chapter we \emph{(i)} will leave the conditioning on the overall hypothesis $\Hypo$ explicit until the end of \S\ref{subsec:Occam-criticality} 
and at the same time 
\emph{(ii)} will move the dependence of a dataset\index{input data} $\D$ to an overall subscript of the preactivations. 
As a particular example, the prior distribution\index{prior} of the $\ell$-layer preactivations $p\big(z^{(\ell)}_{\D} \big| \Hypo \big)$, defined next paragraph in~\eqref{eq:gigantic-beast-that-we-tame-reprint}, is equivalent to
what we've been denoting as $p\big(z^{(\ell)} \big| \D \big)$ outside of this chapter.
}


Here, we've taken familiar objects -- the hyperparameters and the initialization distribution characterizing the frequency of potential network realizations -- and interpreted them in the way of Bayes -- as the hypothesis $\Hypo$ and as the prior distribution $p(\theta \vert\Hypo)$ characterizing our initial beliefs
about the value of the model parameters.
Another familiar object, of course, is the distribution of
$\ell$-th-layer
preactivations that we've spent last three chapters evaluating explicitly.
To give that a Bayesian interpretation, let us first denote by $z^{(\ell)}_{\D}\equiv\le\{\z{i}{\delta}{\ell}\ri\}$ the set of $\ell$-th-layer preactivations
evaluated on inputs $\x{j}{\delta}\in\D$ in some dataset\index{input data} $\D$.
Then, the prior distribution\index{prior} over these $\ell$-th-layer preactivations can be related to the prior distribution over the model parameters by
\begin{align}\label{eq:gigantic-beast-that-we-tame-reprint}
p\!\le(z^{(\ell)}_{\D} \Big| \Hypo \ri) &= \int \Bigg[  \prod_{\mu=1}^{P} d \theta_{\mu}\Bigg] p\!\le(z^{(\ell)}_{\D}, \theta\Big\vert \Hypo \ri)=\int \Bigg[\prod_{\mu=1}^{P} d \theta_{\mu}\Bigg] p\!\le(z^{(\ell)}_{\D} \Big|\theta, \Hypo \ri)\, p(\theta| \Hypo)\, ,
\end{align}
where we've applied the sum rule\index{Bayesian probability!sum rule}~\eqref{eq:logical-sum} in the first equality and the product rule\index{Bayesian probability!product rule}~\eqref{eq:logical-product} in the second. 
This \terminate{prior}
quantifies our initial beliefs about the different neural-network variables.
More specifically, for a hidden layer $\ell$, this distribution represents our beliefs about a particular \neo{feature} \neo{representation} of the input
and, for the output layer $L$, this represents our initial beliefs about 
the behavior of 
the 
\neo{function approximation} $f(x; \theta)$.
More generally, for any neural-network observable $\O=\O(\theta)$, our prior beliefs are determined by
\begin{align}\label{eq:prior-observable-predition}
p\!\le(\O \big| \Hypo \ri) &=\int \Bigg[\prod_{\mu=1}^{P} d \theta_{\mu}\Bigg] p\!\le(\O \big|\theta, \Hypo \ri)\, p(\theta| \Hypo)\, .
\end{align}

To better illustrate what these formal expressions represent, let us take the network output $z^{(L)}_{\D}$ as an observable.
Then, the prior distribution for the output layer $p\!\le(z^{(L)}_{\D} \Big| \Hypo \ri)$,~\eqref{eq:gigantic-beast-that-we-tame-reprint}, is the same distribution as the \terminate{output distribution} induced by the initialization ensemble,~\eqref{eq:gigantic-beast-that-we-tame-with-Dirac},
\emph{if and only if} we also pick the conditional distribution of the outputs given the parameters to be deterministic:
\be\label{eq:deterministic-conditional-Bayes}
p\!\le(z^{(L)}_{\D} \Big|\theta, \Hypo \ri)=\prod_{i=1}^{n_{L}}\prod_{\delta\in\D}\delta\Big(z^{(L)}_{i;\delta}-f_i(x_{\delta};\theta)\Big)\, .
\ee
Here, $f_i(x_{\delta};\theta)$ is an expression for the network output given in terms of the iteration equation that defines the MLP \eqref{eq:mlp-definition}, while $z^{(L)}_{i;\delta}$ is interpreted as a random variable.
The resulting prior distribution\index{prior} for the network outputs $p\!\le(z^{(L)}_{\D} \Big| \Hypo \ri)$ then characterizes our overall initial belief
about the joint set of output values for a given set of inputs $\D$ according to the hypothesis $\Hypo$,
instead of characterizing the relative frequency of such output values at initialization across different realizations of the model parameters. That said, operationally, the formalism developed in the previous chapters can be directly brought to bear on calculating 
with
these  
beliefs.

Importantly, note that the deterministic conditional distribution for the output~\eqref{eq:deterministic-conditional-Bayes} is a part of our hypothesis within the Bayesian framework: according to the hypothesis $\Hypo$, given the model parameters $\theta$, the outputs are \emph{definitely} the ones computed by the function $f(x;\theta)$.\index{hypothesis (Bayesian inference)!deterministic}\index{hypothesis (Bayesian inference)!deterministic|seealso{Dirac delta function}}
Another common hypothesis is the
\emph{uncertain hypothesis}\index{hypothesis (Bayesian inference)!uncertain}\index{hypothesis (Bayesian inference)!uncertain|seealso{mean squared error}}\index{MSE loss|see{loss}}\index{hypothesis (Bayesian inference)!uncertain|seealso{Gaussian distribution}}
\be\label{eq:noisy-conditional-Bayes}
p\!\le(z^{(L)}_{\D} \Big|\theta, \Hypo \ri)=\prod_{i=1}^{n_{L}}\prod_{\delta\in\D}\le\{\frac{1}{\sqrt{2\pi \sigma_{\varepsilon}^2}}\exp\!\le[-\frac{1}{2\sigma_{\varepsilon}^2}\le(z^{(L)}_{i;\delta}-f_i(x_{\delta};\theta)\ri)^2\ri]\ri\}\, ,
\ee
which reduces to the deterministic hypothesis~\eqref{eq:deterministic-conditional-Bayes} in the
limit of zero variance and \terminate{absolute certainty}: $\sigma_{\varepsilon}^2\rightarrow 0$.\footnote{
This hypothesis is equivalent to injecting random noise $\varepsilon_i$ with mean zero and variance $\sigma_{\varepsilon}^2$ into the network output.
This in turn is tantamount to shifting the last-layer biases as $\bias{i}{L}\to\bias{i}{L}+\varepsilon_i$, and hence we can easily incorporate this in our analysis by shifting the final bias variance as $\Cb{L}\to\Cb{L}+\sigma_{\varepsilon}^2$.
You should keep in mind, however, that $\varepsilon_i$ is separate from the bias $\bias{i}{L}$ and is \emph{not} a part of the adjustable model parameters $\theta_{\mu}$; instead, this noise is intended to embody an intrinsic uncertainty present in our observation of the model's output.

Before moving on, let us also mention one other common 
hypothesis for the network output, the \emph{categorical hypothesis}\index{hypothesis (Bayesian inference)!categorical}\index{hypothesis (Bayesian inference)!categorical|seealso{softmax distribution}}\index{hypothesis (Bayesian inference)!categorical|seealso{cross-entropy loss}}, defined for each input $x$ by
\be\label{eq:softmax}
p(i \vert\theta, \Hypo)\equiv \frac{\exp\!\le[f_i(x;\theta)\ri]}{\sum_{j=1}^{n_L}\exp\!\le[f_j(x;\theta)\ri]}\, .
\ee
This distribution is also sometimes known as the \emph{softmax}\index{softmax distribution}\index{softmax distribution|seealso{logistic function}}.
Here, instead of considering a continuous distribution over the $n_{L}$ output \emph{values} $z_i^{(L)}$, we consider a discrete distribution over output \emph{classes} $i$, such as \texttt{dog} or \texttt{cat} or \texttt{car};
then, for such classification tasks, each number $p(i \vert\theta, \Hypo)$ quantifies our belief about how likely the input $x$ represents the class $i$. Functionally, the softmax\index{softmax distribution} can be thought of as a generalization of the \terminate{logistic function} \eqref{eq:sigmoid} in the sense that it maps a vector of real numbers to 
a discrete probability distribution.
}








Having now thoroughly discussed the \terminate{prior}, let's next consider the \terminate{posterior}. As we gather more information $A$ about the true behavior of our desired function $f(x)$, we should update our beliefs about our \terminate{probabilistic model} for $f(x;\theta)$.
In order to incorporate this information in a logically consistent manner, we should use  Bayes' rule\index{Bayesian probability!Bayes' rule}.
Specifically, to update our belief about the model parameters, Bayes' rule \eqref{eq:Bayes-rule} instructs us to use
\be\label{eq:Bayes-rule-model-fitting}
p(\theta \vert A,\Hypo)=\frac{p(A \vert \theta,\Hypo) \, p(\theta\vert\Hypo)}{p(A\vert\Hypo)}\, .%
\ee
Here, to find the posterior distribution\index{posterior} $p(\theta \vert A,\Hypo)$,  the  prior distribution\index{prior} $p(\theta\vert\Hypo)$ gets multiplied by the likelihood\index{Bayesian inference!likelihood} $p(A \vert \theta,\Hypo)$ of the model parameters $\theta$ for the observation of $A$,
and divided by
the evidence\index{Bayesian inference!evidence} $p(A\vert\Hypo)$.
Consequently, with such a posterior distribution\index{posterior} of the model parameters, our beliefs about any neural-network observable $\O$ shifts from our \terminate{prior} $p\!\le(\O \big|\Hypo \ri)$ \eqref{eq:prior-observable-predition} to a \terminate{posterior} with the insertion of $A$,
\begin{align}\label{eq:posterior-observable-predition}
p\!\le(\O \big|A, \Hypo \ri) &=\int \Bigg[\prod_{\mu=1}^{P} d \theta_{\mu}\Bigg] p\!\le(\O \big|\theta, \Hypo \ri)\, p(\theta|A, \Hypo)\, .
\end{align}
These two equations~\eqref{eq:Bayes-rule-model-fitting} and~\eqref{eq:posterior-observable-predition} uniquely determine how new information $A$ can be incorporated to change our beliefs about the value of any neural-network observable.


For function approximation tasks, such information often
comes in the form of some dataset $\A$ containing observed input-output pairs:
\be\label{eq:input-output-information}
A\equiv \le\{(\x{j}{\tra},\y{i}{\tra})\ri\}\vert_{\tra\in\A} \,.
\ee 
Here, each input $\x{j}{\tra}\in{\A}$ is paired with its corresponding true output $\y{i}{\tra}\equiv f_i(x_{\tra})$ recorded from our desired function $f(x)$.\footnote{For maximal disambiguation, in this chapter we'll use \terminate{sample indices} of the form $\tra$ -- the Greek letter alpha with a tilde on top --  for elements of the dataset $\A$ corresponding to input-output pairs for which the \emph{true} output values from $f(x)$ are observed.} 
With our observation of the true values $y_{\A}\equiv\le\{\y{i}{\tra}\ri\}$, the likelihood\index{Bayesian inference!likelihood} and evidence\index{Bayesian inference!evidence} are then given by the conditional belief $p\!\le(y_{\A} \big|\theta, \Hypo \ri)$ and the belief $p\!\le(y_{\A} \big| \Hypo \ri)$ for outputs, respectively. Such beliefs appeared before when considering the prior distribution of the outputs, \eqref{eq:gigantic-beast-that-we-tame-reprint} with $\ell= L$, but are now evaluated on the \emph{fixed} values $y_{\A}$ associated with the given inputs $x_{\A}$. 

To develop some intuition for what this means, let's again take the deterministic hypothesis\index{hypothesis (Bayesian inference)!deterministic}~\eqref{eq:deterministic-conditional-Bayes}. In this case, the likelihood\index{Bayesian inference!likelihood} is given by
\be\label{eq:posterior-deterministic-hypothesis}
p(A \vert \theta,\Hypo)\equiv p(y_{\A} \vert \theta,\Hypo)=\prod_{\tra\in\A}\prod_{i=1}^{n_{L}}\delta\Big(\y{i}{\tra}-f_i(x_{\tra};\theta)\Big)\, .
\ee
This likelihood\index{Bayesian inference!likelihood} quite explicitly restricts
the model parameters to those \emph{exactly} satisfying the constraints $f_i(x_{\tra};\theta)=\y{i}{\tra}$ \emph{fitting}\index{Bayesian inference!model fitting} our observations.
Vice versa, the functions in our set that do \emph{not} satisfy these constraints are completely thrown away from the posterior distribution\index{posterior}, deemed \emph{unlikely}.
Note what has just happened. Naively, $p(y_{\A}  \vert \theta,\Hypo)$ represents our beliefs about the output values $y_{\A}$, given that we set the parameters of our model to $\theta$. However, here we \emph{first} observed the true output values $y_{\A}$ and \emph{then} interpreted $p(y_{\A}  \vert \theta,\Hypo)$ in terms of how likely the model parameters $\theta$ fit the observation $A$.
This is the origin of the name ``likelihood\index{Bayesian inference!likelihood}'' and why the proper way to refer to it is ``the likelihood\index{Bayesian inference!likelihood} of the model parameters $\theta$ for the observation $A$.''

To develop even more intuition, it's customary to introduce the \term{negative log-likelihood}\index{negative log-likelihood|seealso{loss}}  $\L_{\A}\!\le(\theta\ri)$
 -- or \neo{loss} -- representation of the likelihood\index{Bayesian inference!likelihood}:
 \be
p(y_{\A} \vert \theta,\Hypo)\equiv  \exp\!\le[-\L_{\A}\!\le(\theta\ri)\ri]\, .
\ee
Here, by parameterizing the loss as a function of the parameters $\theta$, we are emphasizing that it's the (negative log-)likelihood \emph{of} the parameters.\footnote{While the likelihood\index{Bayesian inference!likelihood} function -- and therefore the loss\index{loss!auxiliary} -- is considered auxiliary from the perspective of \terminate{function approximation}, 
from the perspective of \terminate{Bayesian inference} the form of the likelihood is considered to be part of the hypothesis, cf.~the deterministic hypothesis\index{hypothesis (Bayesian inference)!deterministic} \eqref{eq:deterministic-conditional-Bayes} vs.~the uncertain hypothesis\index{hypothesis (Bayesian inference)!uncertain}~\eqref{eq:noisy-conditional-Bayes}.
}
For the uncertain hypothesis\index{hypothesis (Bayesian inference)!deterministic}~\eqref{eq:noisy-conditional-Bayes}, the negative log-likelihood takes the form of the famous mean-squared-error or \textbf{MSE loss}\index{loss!MSE}:
\be\label{eq:MSE-loss-preview}
\L_{\text{MSE}}(\theta)= \sum_{\tra\in \A}\le\{\frac{1}{2\sigma_{\varepsilon}^2}\big[f_i(x_{\tra}; \theta)-\y{i}{\tra}\big]^2+\frac{1}{2}\log\!\le(2\pi \sigma_{\varepsilon}^2\ri)\ri\}\, .
\ee
In particular, as the network outputs $f_i(x_{\tra}; \theta)$ get closer to their target values $\y{i}{\tra}$,
the MSE loss decreases
and the likelihood increases.\footnote{
In the deterministic limit $\sigma_{\varepsilon}^2\rightarrow 0$, the \terminate{loss} $\L_{\A}\!\le(\theta\ri)$ would be infinite for functions that don't \emph{exactly} fit all the constraints $f_i(x_{\tra};\theta)=\y{i}{\tra}$ and negative infinite for those that do.
Thus, the uncertain hypothesis\index{hypothesis (Bayesian inference)!uncertain} softens these hard-fitting constraints of the deterministic hypothesis\index{hypothesis (Bayesian inference)!deterministic} by relaxing the \terminate{Dirac delta function} distribution
to a Gaussian distribution with a finite variance $\sigma_{\varepsilon}^2$.

When we consider the categorical hypothesis\index{hypothesis (Bayesian inference)!categorical}~\eqref{eq:softmax}, the negative log-likelihood of the softmax\index{softmax distribution} distribution gives the \emph{cross-entropy loss}\index{loss!cross-entropy}\index{cross-entropy loss|see{loss}}. 
We'll more systematically address the consequences of these different choices of \terminate{loss} functions in \S\ref{ch:NTHb}.
} 
As such, the loss is a natural measure of how well our model is approximating the true behavior of the function.
Additionally, since the loss \eqref{eq:MSE-loss-preview} involves an explicit sum over observations,
as the number of observed input-output pairs $\NR$  increases, the likelihood can dominate the prior; that is, if we gather enough information, eventually our prior beliefs  can become entirely replaced by what we learned from our observations. 












This is \textbf{Bayesian model fitting}\index{Bayesian inference!model fitting|textbf}: %
\terminate{Bayesian inference} \eqref{eq:Bayes-rule-model-fitting} is used
as a \neo{learning algorithm} to increase the accuracy of a \terminate{function approximation}. 
It gives greater preference to the functions that better fit  the constraints $f_i(x_{\tra};\theta)=\y{i}{\tra}$ and penalize the ones that don't.
The posterior~\eqref{eq:Bayes-rule-model-fitting} is then updated to reflect a balance between this preference for fitting our observations and an adherence to our prior beliefs about the values the model parameters should take.


Ultimately, we want to use our fit Bayesian model to make \textbf{Bayesian predictions}\index{Bayesian inference!prediction|textbf}. This is generically and abstractly embodied in~\eqref{eq:posterior-observable-predition}. Specifically and concretely, for function approximation tasks we are most often interested in posterior beliefs about the network outputs $\O = z^{(L)}$, for which~\eqref{eq:posterior-observable-predition} reads
\begin{align}\label{eq:posterior-observable-predition-but-actually}
p\!\le(z^{(L)} \Big|A, \Hypo \ri)  &=\int \Bigg[\prod_{\mu=1}^{P} d \theta_{\mu}\Bigg] p\!\le(z^{(L)} \Big|\theta, \Hypo \ri)\, p(\theta|A, \Hypo) \, .
\end{align}
Once we have this distribution, then we can in particular use its mean as our prediction and its variance as our level of confidence.
To compute any of these quantities, one way or another we need to perform  a gigantic integral over the model parameters $\theta$ in order to properly weight our different beliefs.
With that in mind, we'll now present two kinds of methods to tackle this model marginalization: \emph{(i)} approximate methods based on saddle-point approximations\index{saddle-point approximation} and \emph{(ii)} an exact method based on our \terminate{effective theory} approach.




\subsubsection{Approximation methods for model marginalization: MAP and MLE}\index{model fitting!Bayesian|see{Bayesian inference}}\index{Bayesian inference!model fitting!approximation methods}

One way to tackle such a gigantic integral is to presume that the integral measure, given by the posterior distribution $p(\theta \vert A,\Hypo)$ \eqref{eq:Bayes-rule-model-fitting}, is very concentrated around its \emph{mode}\index{mode}\index{mode|seealso{maximum a posteriori}}:
\be\label{eq:map-estimate}
\theta^\star_{\text{MAP}} \equiv \argmax_\theta p(\theta \vert A,\Hypo)= \argmax_\theta \le[p(y_{\A} \vert \theta,\Hypo) \, p(\theta\vert\Hypo)\ri] \, .
\ee
This maximum is known as the \term{maximum a posteriori} (MAP) estimate.
After such a maximization, we can use the function $f(x;\theta^\star_{\text{MAP}})$ for tasks and more generally approximate the full posterior distribution\index{posterior} $p\!\le(\O \big|A, \Hypo \ri)$  \eqref{eq:Bayes-rule-model-fitting} by the point estimate $\O\!\le(\theta^\star_{\text{MAP}}\ri)$.
This notion of approximating a probability distribution with single value of the random variable is known in statistics\index{statistics (branch of mathematics)}\index{statistics (branch of mathematics)|seealso{machine learning}} as a \neo{point estimate} and in \terminate{physics} as a \neo{saddle-point approximation}.\index{saddle-point approximation|seealso{point estimate}}\index{point estimate|seealso{mode}}
Another commonly-used saddle is given by the maximum of the likelihood\index{Bayesian inference!likelihood},
\be\label{eq:mle-estimate}
\theta^\star_{\text{MLE}} \equiv \argmax_\theta p(y_{\A} \vert \theta,\Hypo)  \, ,
\ee
known as the  \term{maximum likelihood estimation} (MLE)\index{MLE|see{maximum likelihood estimation}} of the model parameters.

In terms of the \terminate{negative log-likelihood}  $\L_{\A}\!\le(\theta\ri)$,
MLE is equivalent to the minimization of the \terminate{loss}
\be\label{eq:mle-estimate-loss}
\theta^\star_{\text{MLE}} = \argmin_\theta  \L_{\A}\!\le(\theta\ri) \, ,
\ee
while MAP\index{MAP|see{maximum a posteriori}}  estimate \eqref{eq:map-estimate} is a joint minimization of the loss and the negative log of the \terminate{prior},
\be\label{eq:map-estimate-loss}
\theta^\star_{\text{MAP}} = \argmin_\theta \, \le[\L_{\A}\!\le(\theta\ri)-\log p(\theta\vert\Hypo) \ri]\, .
\ee
In particular for a generic Gaussian prior of the form $p(\theta\vert\Hypo)\propto\exp\!\big(\!-\!\sum_{\mu=1}^P\! a_{\mu}\theta_\mu^2\big)$, the negative-log prior acts as a \neo{regularization} term of the form $\sum_{\mu=1}^P a_{\mu}\theta_\mu^2$ that has an effect of penalizing large parameter magnitudes.
Since the loss grows extensively\index{extensivity!of loss} with the size of the dataset $\A$ while this \terminate{regularization} term stays constant, when we've made sufficiently many observations, we naively expect that the prior will be eventually overwhelmed by the likelihood and that the MAP and MLE estimates will become similar.


If we are to apply these approximation methods to wide neural networks, there are certain things we need to keep in mind.\footnote{
In \S\ref{ch:NTHb}, we'll go through how all of this works in detail.
}
First of all,
there is actually no single optimal value for the \terminate{maximum likelihood estimation} $\theta^\star_{\text{MLE}}$. Instead, there are continuum of such optima, and we still have to consider a distribution over them. Importantly, such a distribution over maxima depends critically on how the maxima are obtained. For instance, it depends on the way you initialize model parameters $\theta_{\text{init}}$, the \terminate{learning algorithm} used to estimate these maxima -- such as \terminate{gradient descent} vs.~\emph{stochastic} gradient descent\index{stochastic gradient descent} -- and the \neo{training hyperparameters} controlling the learning algorithm.\index{learning algorithm} 
The study of this ensemble over optima and its dependence on the initialization\index{initialization hyperparameters} and \terminate{training hyperparameters} will more or less be the focus of the following chapters \S\ref{ch:training}--\S\ref{ch:eot}.\footnote{Since those following chapters will unsentimentally drop our Bayesian lens, let's interpret these different methods with fresh Bayesian eyes here.\label{foot:foretelling-GD}

In the \emph{impure} Bayesian approach -- that is MLE\index{maximum likelihood estimation} -- we have an \terminate{initialization distribution} $p(\theta_{\text{init}})$, but no prior distribution\index{prior} $p(\theta\vert\Hypo)$. By construction, the prior distribution\index{prior} does not enter into the estimate of the impure Bayesian~\eqref{eq:mle-estimate}, but the \terminate{initialization distribution}s \eqref{eq:full-bias-initialization} and \eqref{eq:full-weights-initialization} enters into their code to give particular realizations of networks acting as the starting points for optimization and \terminate{training}. Thus, such an initialization distribution induces a distribution over the resulting MLE estimates.

In the \emph{less impure} Bayesian approach -- that is MAP\index{maximum a posteriori} -- we have \emph{both} an \terminate{initialization distribution} $p(\theta_{\text{init}})$ \emph{and}  a prior distribution\index{prior} $p(\theta\vert\Hypo)$. For the former, we again use the initialization distributions \eqref{eq:full-bias-initialization} and \eqref{eq:full-weights-initialization} to provide starting points for optimization; for the latter, we typically use a Gaussian prior $p(\theta\vert\Hypo)\propto\exp\!\big(\!-\!\sum_{\mu=1}^P\! a_{\mu}\theta_\mu^2\big)$ which, as we said, serves as a \terminate{regularization} term when added to the optimization objective -- the loss -- as per \eqref{eq:map-estimate-loss}.

In the \emph{pure} Bayesian approach -- which is the focus of the rest of this chapter -- there is a prior distribution\index{prior} $p(\theta\vert\Hypo)$ but the \terminate{initialization distribution} $p(\theta_{\text{init}})$ isn't needed.
Pure Bayesians always integrate. What we really did with~\eqref{eq:parameter-prior} was pick a Gaussian prior over the parameters and then adopt the same conventions for the variances as we've been using for the \terminate{initialization distribution} \eqref{eq:full-bias-initialization} and \eqref{eq:full-weights-initialization}. %
We'll see in the rest of the chapter why this is sensible.
} 




























\subsubsection{Exact method for model marginalization: effective theory}\index{Bayesian inference!model fitting!exact marginalization}\index{integrating out}
For the \terminate{prior}~\eqref{eq:gigantic-beast-that-we-tame-reprint}, we know very well that it's possible to directly integrate out\index{integrating out} the model parameters through the use of a \terminate{$1/n$ expansion}. 
Such a gigantic marginalization was the focus of \S\ref{ch:ngp}, and in writing \eqref{eq:gigantic-beast-that-we-tame-reprint} we already reinterpreted our effective preactivation distribution at initialization as our prior beliefs about the preactivations.
For the \terminate{posterior}, the only hypothetical worry would be that we'd need to carry out entirely different sets of integrals. We'll show next that there is no such need.
Thus, in a very real sense the most painstaking theoretical part of \terminate{Bayesian inference} has already been taken care of for us!


Let's continue to suppose that we've made some observations $A$ of the true outputs $\y{i}{\tra}\equiv f_i(x_{\tra})$ of our function $f(x)$ for a given set of inputs $x_\A$ in a subsample $\A$ as defined by~\eqref{eq:input-output-information}.
We now want to incorporate what we've learned from these observations in order to update our beliefs about the output values
$z^{(L)}_{\B}\equiv\le\{\z{i}{\tea}{L}\ri\}$ for a potentially different set of inputs $\x{j}{\tea}\in\B$ in another subsample $\B$.\footnote{For maximal disambiguation, in this chapter we'll use \terminate{sample indices} of the form $\tea$ -- the Greek letter beta with a dot on top --  for elements of the dataset $\B$ corresponding to input-output pairs for which outputs values from $f(x)$ are \emph{not} observed but instead to be \emph{inferred}. 
}
Beginning with the joint \terminate{prior} for the network outputs over the union of both subsamples $\D\equiv\A\cup\B$, 
\be
p\!\le(z_{\D}^{(L)}\Big\vert  \Hypo \ri) \equiv p\!\le(z_{\A}^{(L)}\!,z_{\B}^{(L)}\Big\vert  \Hypo \ri)\, ,
\ee
we can set $z_{\A}^{(L)} \to y_\A$ and use the product rule~\eqref{eq:logical-product} to condition our beliefs about $z_{\B}^{(L)}$ on the observed \emph{true} values $y_\A$:
\be
p\!\le(y_{\A},z_{\B}^{(L)}\Big\vert  \Hypo \ri)=p\!\le(z_{\B}^{(L)}\Big\vert y_{\A}, \Hypo \ri)p\!\le(y_{\A}\Big\vert  \Hypo \ri)\, .
\ee
Then, rearranging terms like we are Reverend Thomas Bayes\index{Bayes, Reverend Thomas}, we get
\be\label{eq:BB-NN}
p\!\le(z_{\B}^{(L)}\Big\vert y_{\A}, \Hypo \ri)=\frac{p\!\le(y_{\A},z_{\B}^{(L)}\Big\vert  \Hypo \ri)}{p\!\le(y_{\A}\Big\vert  \Hypo \ri)}\, .
\ee
Since this iteration of Bayes' rule\index{Bayesian probability!Bayes' rule} is so important, let us be verbose and crystal clear about its interpretation: the denominator $p\!\le(y_{\A}\big\vert  \Hypo \ri)$ is the \terminate{prior} for the network outputs given the inputs $x_\A$ in the subsample $\A$, evaluated on the \emph{fixed} observed values $y_{\A}$, hence it is just a \emph{number}; the numerator $p\!\le(y_{\A},z_{\B}^{(L)}\Big\vert  \Hypo \ri)$ is the \terminate{prior} for the network outputs given the inputs $x_\D$ in
the joint dataset $\D\equiv\A\cup\B$, evaluated on the \emph{fixed} observed values $y_{\A}$ but with the network outputs $z_{\B}^{(L)}$ still \emph{variable}, hence it is a \emph{function} of the $z_{\B}^{(L)}$.\footnote{
    The reason we say \emph{given the inputs} here is that technically we should also be conditioning on $x_\A$ and $x_\B$ as well. In particular, while $y_{\A}$ is fixed and $z_{\B}^{(L)}$ is completely variable in the expression for the joint prior
    \be
    p\!\le(y_{\A},z_{\B}^{(L)}\Big\vert  \Hypo \ri)  \equiv p\!\le(y_{\A},z_{\B}^{(L)}\Big\vert x_\D, \Hypo \ri)  \,, 
    \ee
    the full set of inputs
    $x_\D \equiv x_\A \cup x_\B$ determines the \emph{data-dependent couplings}\index{data-dependent coupling} $g_{(L)}$ and $v_{(L)}$ -- or equivalently the metric  $G^{(L)}$ and the \terminate{four-point vertex} $V^{(L)}$ -- that parameterize the output distribution. We will see how this works in more detail in the following sections.
}
The numerator and denominator combine to make the \terminate{posterior} on the left-hand side, which is thus a function of the random variable $z_{\B}^{(L)}$ encoding our \terminate{posterior} beliefs about the plausible values of the network outputs $z_{\B}^{(L)}$ for the inputs $x_\B$ in $\B$, updated with our observations about the true values $y_{\A}$ of the outputs for the inputs $x_\A$ in $\A$.
In this way, rather than performing \terminate{Bayesian inference} to learn about the model parameters as way of maintaining different beliefs about the different functions $f(x; \theta)$ in our flexible set, here we simply update our beliefs about the behavior of the function $f(x)$ directly.

In this presentation of Bayes' rule \eqref{eq:BB-NN}, the marginalization over all the model parameters\index{marginalizing over} already occurred in our transition from \eqref{eq:parameter-prior}, the prior over the parameters, to \eqref{eq:gigantic-beast-that-we-tame-reprint}, the prior over the preactivations. The resulting posterior~\eqref{eq:BB-NN} is in fact exactly equivalent to what you'd get by explicitly doing a marginalization over a posterior distribution of the model parameters, e.g.~as in \eqref{eq:posterior-observable-predition}. %
To see why, consider the following set of manipulations:
\begin{align}
p\!\le(z_{\B}^{(L)}\Big\vert y_{\A}, \Hypo \ri)=&\int \le[  \prod_{\mu=1}^{P} d \theta_{\mu}\ri] p\!\le(z_{\B}^{(L)}, \theta \Big\vert y_{\A},\Hypo\ri)=\int \le[  \prod_{\mu=1}^{P} d \theta_{\mu}\ri] p\!\le(z_{\B}^{(L)}\Big\vert \theta, \Hypo \ri)\ p(\theta \vert y_{\A},\Hypo)\, \notag\\
=&\int \le[  \prod_{\mu=1}^{P} d \theta_{\mu}\ri] p\!\le(z_{\B}^{(L)}\Big\vert \theta, \Hypo \ri) \le[ \frac{p(y_{\A}\vert \theta,\Hypo) \, p(\theta\vert\Hypo)}{p(y_{\A}\vert\Hypo)}\ri] \, \notag\\
=&\frac{1}{p(y_{\A}\vert\Hypo)}\int \le[  \prod_{\mu=1}^{P} d \theta_{\mu}\ri] p\!\le(y_{\A}, z_{\B}^{(L)}\Big\vert \theta, \Hypo \ri) \, p(\theta\vert\Hypo) \notag\\
=&\frac{1}{p(y_{\A}\vert\Hypo)}\int \le[  \prod_{\mu=1}^{P} d \theta_{\mu}\ri] p\!\le(y_{\A}, z_{\B}^{(L)}, \theta\Big\vert \Hypo \ri) =\frac{p\!\le(y_{\A},z_{\B}^{(L)}\Big\vert  \Hypo \ri)}{p\!\le(y_{\A}\Big\vert  \Hypo \ri)}\,  .\label{eq:BB-NN-long}
\end{align}
The only nontrivial step is 
in the third line, where we reversed the factorization,
\be\label{eq:factorization-joint-network-outputs}
p\!\le(z_{\A}^{(L)}\!, z_{\B}^{(L)}\Big\vert \theta, \Hypo \ri)=p\!\le(z_{\A}^{(L)}\Big\vert \theta, \Hypo \ri)\, p\!\le(z_{\B}^{(L)}\Big\vert \theta, \Hypo \ri)\, ,
\ee
and evaluated at $z_{\A}^{(L)} \to y_\A$. 
This factorization \eqref{eq:factorization-joint-network-outputs} says that the network outputs are \emph{conditionally independent}, given the parameters.
This is a consequence of the fact that -- for a fixed set of network parameters -- the output on an example $x_{\tilde{\alpha}}$ is entirely independent from the output evaluated on any other example $x_{\dot{\beta}}$, which is manifestly true for all the hypotheses that we mentioned. (If it were not, neural networks would be pretty useless in practice.) 
The use of Bayes' rule\index{Bayesian probability!Bayes' rule} for the model parameters in the square brackets in the second line also makes manifest the connection between \emph{Bayesian model fitting}\index{Bayesian inference!model fitting}~\eqref{eq:Bayes-rule-model-fitting} on the one hand and \emph{Bayesian prediction}\index{Bayesian inference!prediction}~\eqref{eq:posterior-observable-predition} on the other hand.

















\index{integrating out}
As we already alluded to, this exact method for model marginalization is closely connected with our \terminate{effective theory} approach to understanding neural networks. 
In particular, while the model parameters are always part of the definition of our neural networks, we've always had to integrate them out in the process of determining the distribution over the network outputs. In this way, our \terminate{effective theory of deep learning} has always worked directly with the entire \terminate{ensemble} of network functions implied by the \terminate{initialization distribution} of the parameters~\eqref{eq:parameter-prior} rather than with any \emph{particular} network. Up until now, we've motivated this ensemble approach via the \emph{principle of typicality}\index{typicality!principle of}, in which we use the ensemble to analyze how a typical realization is likely to behave.\footnote{
    In \S\ref{ch:NTKa} and onwards, we'll see how this principle is manifested in neural networks \emph{trained} via gradient-based learning.
}
\index{marginalizing over}
Here we have a slightly different interpretation: rather than trying to make the ensemble describe a typical network, we actually want to consider the posterior predictions across the full set of potential networks, each weighted according to our posterior beliefs about how plausible those predictions are.

Now, after a brief detour into Bayesian model comparison\index{Bayesian inference!model comparison}, much of the focus of \S\ref{sec:infinite-posterior} and \S\ref{sec:finite-posterior} will be the explicit evaluation of these Bayesian predictions\index{Bayesian inference!prediction}~\eqref{eq:BB-NN} for infinite- and finite-width MLPs, respectively.























\subsection{Bayesian Model Comparison}\label{subsec:bayesian-model-comparison}\index{model comparison!Bayesian|see{Bayesian inference}}\index{marginalizing over}\index{integrating out}
In the context of Bayesian model fitting\index{Bayesian inference!model fitting} and Bayesian prediction\index{Bayesian inference!prediction}, the \emph{evidence}\index{Bayesian inference!evidence} $p\big(y_{\A}\big\vert  \Hypo\big)$ has thus far played essentially no role. In the context of our approximation methods, MAP and MLE\index{maximum a posteriori}\index{maximum likelihood estimation} and their respective maximizations \eqref{eq:map-estimate} and \eqref{eq:mle-estimate}, the value of the argument maximization is strictly independent of the evidence, since it doesn't depend on the \terminate{model parameters}. In the context of our exact method for Bayesian prediction\index{Bayesian inference!prediction}, the evidence is simply the \terminate{normalization factor} of the posterior, which is trivial for us to compute.

To actually see the role of the evidence\index{Bayesian inference!evidence} in action, %
\emph{you mustn't be afraid to dream a little bigger, darling}.\index{Eames (\emph{Inception} meme)}
That is, rather than being fixated on a single hypothesis $\Hypo$, we instead consider a multitude of different hypotheses $\Hypo_{a}$ as possible explanations for our data. This is the essence of \textbf{Bayesian model comparison}\index{Bayesian inference!model comparison|textbf}: using the \emph{evidence}\index{Bayesian inference!evidence} to weigh the plausibility of different probabilistic models\index{probabilistic model} as explanations for all of our observations. In the context of \terminate{deep learning}, this corresponds to comparing our relative beliefs in the different modeling choices encapsulated in each $\Hypo_{a}$ -- i.e.~comparing different hyperparameter settings -- and determining which modeling choice provides the best description of our observations $y_\A$.


To begin, let us again use Bayes' rule\index{Bayesian probability!Bayes' rule} -- this time on the evidence -- to invert the conditioning as
\be\label{eq:bayes-rule-model-comparison}
p\!\le( \Hypo_{a}\big\vert y_{\A}  \ri) = \frac{p\!\le(y_{\A}\big\vert \Hypo_{a}\ri) p\!\le(\Hypo_{a}\ri) }{p\!\le( y_{\A}\ri)  } \, .
\ee
In this form, the \terminate{posterior} $p\!\le( \Hypo_{a}\big\vert y_{\A}  \ri) $ on the left-hand side encodes our updated beliefs in the plausibility of the different hypotheses  $\Hypo_{a}$ -- the different hyperparameters settings -- given our observation $y_{\A}$, while the \terminate{prior} $p\!\le( \Hypo_a \ri)$ on the right-hand side encodes our initial beliefs about 
these hypotheses. 
Amusingly,  the \emph{old} \emph{evidence}\index{Bayesian inference!evidence} $p\!\le(y_{\A}\big\vert \Hypo_{a}\ri)$ for the hypothesis $\Hypo_{a}$ from our Bayesian model fitting\index{Bayesian inference!model fitting}
now appears as the \emph{new} \emph{likelihood}\index{Bayesian inference!likelihood} $p\!\le(y_{\A}\big\vert \Hypo_{a}\ri)$ of the hypothesis $\Hypo_{a}$ for the observation $y_{\A}$ in the context of Bayesian model comparison. Lastly, the \emph{new} \emph{evidence}\index{Bayesian inference!evidence} $p\!\le( y_{\A}\ri)$  
is just a \terminate{normalization factor} that we can safely ignore.\footnote{
    Unless, of course, we aren't afraid to dream even bigger.
    If we did -- \emph{narrator:} they won't\index{Narrator (\emph{Arrested Development})} -- 
    we'd need to introduce a \neo{meta hypothesis}\index{hypothesis (Bayesian inference)!meta hypothesis|see{meta hypothesis}},  $\mathcal{G}$, that encodes our \terminate{prior} beliefs about different hyperparameter configurations $p(\Hypo_{a}  \vert\mathcal{G})$. This is sometimes called \emph{Bayesian hierarchical modeling}\index{Bayesian inference!hierarchical modeling}.
    In this case, Bayesian model comparison in terms of this even grander evidence $p( y_{\A}) \to p( y_{\A}\vert\mathcal{G})$  in principle involves integrating overall \emph{all} the probabilistic models\index{probabilistic model} as $p( y_{\A}\vert\mathcal{G})=\sum_{a}p(y_{\A}\vert \Hypo_{a})\ p(\Hypo_{a}\vert\mathcal{G})$,  i.e.~any and all hypotheses $\Hypo_{a}$ that are encoded by $\mathcal{G}$. 
    The distinction between the \terminate{meta hypothesis} $\mathcal{G}$ and hypotheses $\Hypo_{a}$ is somewhat arbitrary, however; for instance, we could put into $\mathcal{G}$ our overall choice of architecture -- e.g.~MLP\index{multilayer perceptron}, CNN\index{convolutional neural network}, \terminate{transformer} -- and then let $\Hypo_{a}$ index the different settings of $\Hypo$yperparameters. Then, recursing again, a Bayesian model comparison over $\mathcal{G}$ would be a weighted evaluation of the best architecture for the data, taking into account all possible settings of the hyperparameters for those architectures.
}







To see how the model comparison works, let's use \eqref{eq:bayes-rule-model-comparison} to compare two different hypothesis, $\Hypo_1$ and $\Hypo_2$, in order to determine which is a better fit for our observations. 
Since our \emph{relative beliefs} are all that matter, let's take the ratio of the two  \terminate{posterior}s,
\be\label{eq:bayes-factor}
\frac{p\!\le( \Hypo_{1}\big\vert y_{\A}  \ri)}{p\!\le( \Hypo_{2}\big\vert y_{\A}  \ri)} = \le[\frac{p\!\le(y_{\A}\big\vert \Hypo_{1}\ri)}{p\!\le(y_{\A}\big\vert \Hypo_{2}\ri)}\ri] \frac{p\!\le(\Hypo_{1}\ri) }{p\!\le(\Hypo_{2}\ri) }\, ,
\ee
from which we see that the irrelevant \terminate{normalization factor} $p( y_{\A})$ simply drops out.
Here, the ratio in the square brackets is sometimes given the name the \textbf{Bayes' factor}\index{Bayesian inference!model comparison!Bayes' factor|textbf}, which in turn multiplies the ratio of our prior beliefs.
In particular, the Bayes' factor contains all of the observation dependence and characterizes how we should update our relative prior beliefs in each hypothesis given the new data $y_{\A}$.
A ratio 
greater than one indicates that the model specified by hypothesis $\Hypo_1$ is favored, while a ratio less than one indicates that the model specified by hypothesis $\Hypo_2$ is favored. In this way, the old evidence -- i.e.~the new likelihood -- $p\!\le(y_{\A}\big\vert \Hypo_{a}\ri)$ can be very useful, indeed.










\subsubsection{Occam's razor}
In order to further elaborate on the mechanism behind Bayesian model comparison\index{Bayesian inference!model comparison}~\eqref{eq:bayes-factor}, let us pick up \term{Occam's razor} \cite{occam},
which is the famous \emph{principle of sparsity}\index{sparsity, principle of}. It says that we should favor the simplest hypothesis that fits all the observations. In the context of \terminate{machine learning} and parameterized probabilistic modeling\index{probabilistic model}, this principle is often intended as a heuristic that guides us to favor models with fewer parameters, all else being equal. The intuitive explanation for this heuristic is that models with more parameters have greater flexibility to fit the observed data, making them more likely to \emph{overfit}\index{overfitting}\index{overfitting|seealso{generalization}} and less likely to \emph{generalize}\index{generalization} to explain new observations.\footnote{It's natural to wonder here how to interpret this overfitting in light of the fact that we've actually integrated out all our parameters! (In the \terminate{machine learning} literature, such ensembles are sometimes called \emph{non-parametric models}\index{non-parametric model|seealso{Gaussian process}}\index{non-parametric model}, though we really do not like such terminology, given the following explanation.) The culprit for this potential confusion is the overloaded usage of the word \emph{parameter}.
To illustrate this with the extreme, let's consider the \terminate{infinite-width limit}.
Despite formally starting with an infinite number of model parameters 
-- giving a model that is naively very \emph{overparameterized}\index{overparameterization}, to say the least -- 
the \terminate{effective theory} of the output distribution is completely characterized by the kernel $\ker^{(L)}$, which can be described by a finite number of \emph{data-dependent couplings}\index{data-dependent coupling} $\sim N_{\D}^2$. 
Thus, from the \terminate{macroscopic perspective} of Bayesian model comparison\index{Bayesian inference!model comparison}, it's these couplings that control the \terminate{model complexity} and not what we usually call the parameters, the tunable weights and biases. We will discuss this further and in greater detail in Epilogue~\ref{epi:overparameterization}, and in particular we'll highlight how the \terminate{$1/n$ expansion} for finite-width networks leads to a sequence of effective theories with increasing complexity.\label{footnote:occam-non-parametric}
}






Naively, Bayesian model comparison\index{Bayesian inference!model comparison}~\eqref{eq:bayes-rule-model-comparison} seems to give us a very natural way to implement this razor: we can \emph{subjectively} adjust the ratio of our prior beliefs $p(\Hypo_1) / p(\Hypo_2)$ to explicitly favor the simpler hypothesis, a priori penalizing more complicated models. However, as MacKay\index{MacKay, David} \cite{mackay1995probable} points out:

\vspace{-.30\baselineskip}
\setlength\epigraphwidth{.93\textwidth}
\epigraph{Coherent [Bayesian] inference embodies Occam's Razor automatically and quantitatively.}{}
\setlength\epigraphwidth{.9\textwidth}

\vspace{-1.80\baselineskip}
\noindent{}That is, Occam's razor is \emph{objectively} built into Bayesian model comparison\index{Bayesian inference!model comparison}~\eqref{eq:bayes-factor} through the Bayes' factor.\footnote{
See MacKay's\index{MacKay, David} excellent exposition \cite{mackay1995probable} for further details and examples,
with a particular emphasis on (pre-deep-learning-era) neural networks.
}



To understand why,
note that the prior distribution\index{prior} $p\!\le(z_\A^{(L)} \Big\vert \Hypo_{a}\ri)$
needs to be normalized. This means that for a given hypothesis\index{hypothesis (Bayesian inference)} $\Hypo_{a}$
to be complicated enough to explain an overwhelmingly wide variety of \emph{potential} observations $z_\A^{(L)}$, it must have small support on any \emph{particular} observation $y_\A$.
Hence the evidence $p(y_\A \vert \Hypo_{a})$  for such a hypothesis
will be small regardless of which actual observation we make.
In contrast, if the hypothesis is very simple, the \terminate{prior} $p\!\le(z_\A^{(L)} \Big\vert \Hypo_{a}\ri)$ will make a constrained set of predictions, but make them strongly, by concentrating its support on only a few plausible outcomes. Thus, the simplest models that still correctly predict the observation $y_{\A}$ are naturally preferred by the Bayes' factor\index{Bayesian inference!model comparison!Bayes' factor} $p(y_\A \vert \Hypo_{1}) / p(y_\A \vert \Hypo_{2})$ alone. In addition, the more observations we make that are correctly predicted, the more the Bayes' factor\index{Bayesian inference!model comparison!Bayes' factor} will amplify this preference for simpler models that still fit.\footnote{This is analogous to the way the likelihood\index{Bayesian inference!likelihood} factor will dominate the \terminate{prior} as observations accumulate when Bayesian model fitting\index{Bayesian inference!model fitting}.}











Since the Bayes' factor\index{Bayesian inference!model comparison!Bayes' factor} automatically and objectively implements \terminate{Occam's razor}, there's no need to subjectively express a preference for simpler models using the prior over hypotheses $p(\Hypo_a)$. This means that for a discrete set of hypothesis $\{\Hypo_a \}$, we can choose the prior distribution to be uniform, giving equal a priori preference to any particular hypothesis  $\Hypo_a$ regardless of their complexity.
With this choice our Bayesian model comparison\index{Bayesian inference!model comparison} is completely characterized by the  Bayes' factor\index{Bayesian inference!model comparison!Bayes' factor}:
\be\label{eq:bayes-factor-equal-hypo}
\frac{p\!\le( \Hypo_{1}\big\vert y_{\A}  \ri)}{p\!\le( \Hypo_{2}\big\vert y_{\A}  \ri)} = \frac{p\!\le(y_{\A}\big\vert \Hypo_{1}\ri)}{p\!\le(y_{\A}\big\vert \Hypo_{2}\ri)}\, .
\ee
Thus, we should really think of \terminate{Occam's razor} as the \emph{inductive bias} of \terminate{Bayesian inference} applied to model comparison.\index{Bayesian inference!model comparison}


 

















\subsubsection{Inductive Bias}
Given our last statement, we should clarify about something that we've been informally referring to since \S\ref{sec:MLP_intro} but now are finally ready to formally address:
\term{inductive bias}.



Way back in \S\ref{sec:MLP_intro}, inductive biases were introduced as something implicit that is 
built into a neural network architecture in order that the set of functions $\{ f(x;\theta)\}$ may better represent the properties of a particular dataset\index{input data} $\D$ and the
\terminate{function approximation} task at hand. From the Bayesian perspective, inductive biases represent the a priori assumptions made about the desired function $f(x)$ before any observations are made. More broadly, both hypotheses and \terminate{learning algorithm}s may have their own set of inductive biases; e.g.~we've just pointed out that \terminate{Occam's razor} is an inductive bias of \terminate{Bayesian inference}.


Throughout \S\ref{sec:infinite-posterior} and \S\ref{sec:finite-posterior}, we'll encounter various 
inductive biases  while performing concrete calculations for infinite- and finite-width MLPs.
Here, let's consider a very simple example for illustration: suppose that a Bayesian firmly believes with \neo{absolute certainty} that a statement $B$ is false such that their hypothesis $\Hypo_{\overline{B}}$ assigns an a priori probability of zero to this belief as $p(B\vert\Hypo_{\overline{B}})=0$; then, via Bayes' rule\index{Bayesian probability!Bayes' rule}~\eqref{eq:Bayes-rule}, there's no way that the \terminate{posterior} on $B$ can be updated to be anything other than zero, even if the Bayesian gathers some new information $A$ that would serve as positive evidence for $B$. In this case,
$\Hypo_{\overline{B}}$ is clearly a bad hypothesis; its inductive bias is leading to an absurdly stubborn set of beliefs. Alternatively, if $B$ turns out to be actually false, $\Hypo_{\overline{B}}$ is a good hypothesis because it can then assign more probability to other plausibly true statements. As this \neo{gedanken inference} illustrates, the advantage and disadvantage of an inductive bias depends on the ground truth.










Returning to our initial example in \S\ref{sec:MLP_intro} of the \terminate{inductive bias} of different neural-network architectures, the advantage of one architecture over another is a highly data- and task-dependent question.
In principle, we could use Bayesian model comparison\index{Bayesian inference!model comparison}~\eqref{eq:bayes-factor} to directly compare these different architectures -- MLPs\index{multilayer perceptron}, CNNs\index{convolutional neural network}, and transformers\index{transformer} -- for different sets of observations $y_{\A}$ if only we knew how to compute the evidence\index{Bayesian inference!evidence} $p\!\le(y_{\A}  \big\vert \Hypo \ri)$ for those architectures.\footnote{
Recall from \S\ref{sec:MLP_intro} that CNNs~\eqref{eq:conv-layer} are designed to capitalize on the fact that \terminate{computer vision} data organizes useful information in a spatially-local translationally-invariant manner.\index{translational invariance}
Incorporating this property into the architecture design is an \terminate{inductive bias} of the CNN; in particular, the assumption is that a cat is still a $\texttt{cat}$, even if it's shifted \emph{up up down down left right left right} $BA$\index{Konami Code}. The advantage of such an \terminate{inductive bias} as compared to MLPs should be directly encoded in a Bayes' factor\index{Bayesian inference!model comparison!Bayes' factor} $p\!\le( y_\A  \big\vert \Hypo_{\text{CNN}} \ri)/p\!\le( y_\A  \big\vert \Hypo_{\text{MLP}} \ri)$. This ratio should presumably be greater than one for any dataset with desired outputs $y_\A$ for which 
the assumption of spatial locality is a useful inductive bias.
}
The formalism of our \terminate{effective theory of deep learning} as laid out in the earlier chapters is precisely a blueprint for computing such factors for different architectures as a function of a particular dataset. We encourage you to give it a try.





 


















\section{Bayesian Inference at Infinite Width}\label{sec:infinite-posterior}
In this section, we'll give three lessons on Bayesian learning in the \terminate{infinite-width limit}. %
First, we'll calculate the evidence\index{Bayesian inference!evidence} $p(y_{\A}\vert\Hypo)$ 
and see 
that Bayesian model comparison\index{Bayesian inference!model comparison} prefers \terminate{criticality} for sufficiently deep networks (\S\ref{subsec:Occam-criticality}). 
Then, we'll calculate the posterior distribution for the network outputs $p\!\le(z_{\B}^{(L)}\Big\vert y_{\A},\Hypo\ri)$ and see that different output components are completely independent 
in this limit 
(\S\ref{subsec:absence-FF-Bayes}). 
Finally, we'll calculate the posterior distribution\index{posterior} of preactivations in the \emph{penultimate} layer $p\!\le(z_{\D}^{(L-1)}\Big\vert y_{\A},\Hypo\ri)$ 
and show that it's identical to the penultimate prior distribution $p\!\le(z_{\D}^{(L-1)}\Big\vert \Hypo\ri)$, %
thus implying that such infinitely-wide networks lack \terminate{representation learning} (\S\ref{subsec:absence-RL-Bayes}).

Before we begin, let's start with some reminiscence, recast through the lens of our new Bayesian glasses.\index{glasses (Bayesian)}
In the infinite-width limit, the prior distribution\index{prior} over the network outputs is given by a simple zero-mean Gaussian distribution
\be\label{eq:GP-for-once}
p\!\le(z^{(L)}_{\D}\Big\vert \Hypo\ri) = \frac{1}{\sqrt{\dete{2\pi \ker}^{n_{L}}}} \exp\!\le(-\frac{1}{2}\sum_{i=1}^{n_{L}}\sum_{\delta_1,\delta_2\in\D}\ker^{\delta_1 \delta_2}\z{i}{\delta_1}{L}\z{i}{\delta_2}{L}\ri)\, ,
\ee
with the variance $\ker_{\delta_1\delta_2}\equiv\ker_{\delta_1\delta_2}^{(L)}=\ker^{(L)}(x_{\delta_1},x_{\delta_2})$ given by the 
kernel at the output layer -- here with the layer index dropped -- 
depending
explicitly on pairs of inputs $x_{\delta_1}$ and $x_{\delta_2}$ from the dataset\index{input data} $\D$ and implicitly on the $\Hypo$yperparameters $C_b$ and $C_W$. %
Also recall that, as per our \emph{general relativistic}\index{general relativity} conventions, the matrix $\ker^{\delta_1 \delta_2}$ is the \emph{inverse} of the covariance matrix $\ker_{\delta_1\delta_2}$
\be\label{eq:whole-inverse}
\sum_{\delta_2\in\D}\ker^{\delta_1\delta_2}\ker_{\delta_2\delta_3}=\delta^{\delta_1}_{\ \delta_3}\, , %
\ee
where we are entertained by -- but also apologize for --  the collision of \terminate{sample indices} $\delta_1, \delta_2$ with the overall Kronecker delta\index{Kronecker delta},
and further recall that $\dete{2\pi \ker}$ is the determinant of the $\ND$-by-$\ND$ matrix $(2\pi \ker)_{\delta_1\delta_2}$. 

\subsection{The Evidence for Criticality}\label{subsec:Occam-criticality} %
As we elaborated on in the last section, the evidence\index{Bayesian inference!evidence} is just the prior distribution\index{prior} for the network outputs evaluated on the observed true output values $\y{i}{\tra}$ given the inputs $\x{i}{\tra}$ in the subsample $\A$:
\begin{align}\label{eq:Gaussian-evidence-general}
p(y_{\A}\vert\Hypo)= \frac{1}{\sqrt{\dete{2\pi \kersub}^{n_{L}}}} \exp\!\le(-\frac{1}{2}\sum_{i=1}^{n_{L}}\sum_{\tra_1,\tra_2\in\A}\kersub^{\tra_1 \tra_2}\y{i}{\tra_1}\y{i}{\tra_2}\ri)\, .
\end{align}
Here we've put tildes both on the \terminate{sample indices} $\tra$ and on the kernel as well, $\kersub_{\tra_1\tra_2}$, %
in order to indicate that it's an
$\NR$-by-$\NR$ submatrix built from the pairs of inputs $(x_{\tra_1},x_{\tra_2})$ in the subsample $\A$ of size $\NR$. Importantly, this means that the inverse $\kersub^{\tra_1 \tra_2}$ is taken with respect to the samples in the set $\A$ \emph{only},
\be\label{eq:inverse-subA}
\sum_{\tra_2\in\A}\kersub^{\tra_1\tra_2}\kersub_{\tra_2\tra_3}=\delta^{\tra_1}_{\ \tra_3}\, ,
\ee
and in particular that $\kersub^{\tra_1 \tra_2}\ne\ker^{\tra_1 \tra_2}$. In other words, $\kersub^{\tra_1 \tra_2}$ is \emph{not} the same as the inverse of the kernel $\ker^{\delta_1\delta_2}$ on the whole dataset\index{input data} $\D$~\eqref{eq:whole-inverse} evaluated on the sample indices $\delta_{1}=\tra_{1}$ and $\delta_{2}=\tra_{2}$; if you'd like, flip ahead and cf.~\eqref{eq:Kinv-tratra}.
Accordingly, the determinant $\dete{2\pi \kersub}$ is also computed from this
$N_{\A}$-by-$N_{\A}$ submatrix.
The usefulness of this notation and the essentialness of this distinction will become clearer when we consider the posterior in \S\ref{subsec:absence-FF-Bayes}.


Before we analyze the evidence \eqref{eq:Gaussian-evidence-general} in detail, we should establish our space of hypotheses.
Considering MLP architectures in the infinite-width limit, there's only three hyperparameters of relevance, the bias variance and rescaled weight variance $C_b$ and $C_W$, and the depth $L$. In principle, each combination of these three hyperparameters is a different hypothesis. However, in the asymptotic limit of large depth $L \gg 1$, we know from our discussion in \S\ref{sec:criticality_DLN} and our analysis in \S\ref{ch:eft-mlp} that generically the kernel recursion will either exponentially lead to a \emph{trivial fixed point}\index{fixed point!trivial} at zero $K^\star=0$ or at infinity $K^\star =\infty$, or slowly approach a \emph{nontrivial fixed point}\index{fixed point!nontrivial} at \terminate{criticality}.\footnote{Yes, we know, for some activation functions there exist hyperparameter settings that lead to trivial fixed point\index{fixed point!trivial}s at nonzero values of the kernel $K^{\star}\ne0$. We'll eventually consider -- and make a case against -- such hypotheses as well, though only in a future footnote,~\ref{foot:parallel-criticality}, and only after first considering the details of the two-input evidence\index{Bayesian inference!evidence}.}
Thus, for deep networks Bayesian model comparison\index{Bayesian inference!model comparison} essentially reduces to the comparison of three different hypotheses, $\Hypo_{0}$, $\Hypo_{\infty}$ and $\Hypo_{\text{critical}}$, corresponding to the two trivial fixed points and the one nontrivial fixed point, respectively.



Having established our space of hypotheses, let's first see how Bayesian model comparison\index{Bayesian inference!model comparison} works when we have only a single input $x$.
In this case the kernel is just a scalar, and the evidence is simply given by
\begin{align}\label{eq:Gaussian-evidence-single}
p(y\vert\Hypo) = \frac{1}{\le(2\pi \kersub\ri)^{\frac{n_{L}}{2}}} \exp\!\le(-\frac{1}{2\kersub}\sum_{i=1}^{n_{L}}y_{i}^2\ri)\, .
\end{align}
Here, the output norm $\sum_{i=1}^{n_{L}}y_{i}^2$ is fixed by a given function approximation task.\footnote{
Many common datasets for \neo{classification} tasks employ ``one-hot\index{one-hot encoding}'' true outputs in which all but one component $y_i$ of a particular output are zero, and the remaining single component -- corresponding to the \emph{correct} class -- is equal to one. For such datasets, the output norm is trivial $\sum_{i=1}^{n_{L}}y_{i}^2 = 1$.
}
Thus all the dependence on the hyperparameters $\Hypo$ is encoded in a single number: $\kersub$.



Let's start with $\Hypo_\infty$, for which $\kersub\to\infty$. In this case, the argument of the exponential in~\eqref{eq:Gaussian-evidence-single} vanishes and thus the exponential evaluates to unity, while the normalization factor in front 
vanishes. Therefore, the evidence will vanish polynomially:%
\be\label{eq:exploding-kernel-evidence}
p(y\vert\Hypo_\infty) 
= \lim_{\kersub\to\infty} \frac{1}{\le(2\pi \kersub\ri)^{\frac{n_{L}}{2}}} = 0\, .
\ee
In fact, in this limit the output distribution becomes an (unnormalizable) uniform distribution over all possible output norms.
Next, let's consider $\Hypo_0$ with $\kersub\to0$. In this case, while the normalization factor grows polynomially, the argument in the exponent approaches negative infinity. Thus, the evidence approaches zero exponentially quickly:
\be
p(y\vert\Hypo_0) 
= \lim_{\kersub\to 0} \exp\!\le[ 
    -\frac{1}{2\kersub}\sum_{i=1}^{n_{L}}y_{i}^2 + \o{\log \kersub}
\ri]  = 0\, .
\ee
Indeed, recalling \eqref{eq:gaussian-limit-delta-function}, the evidence~\eqref{eq:Gaussian-evidence-single} in this limit becomes a \terminate{Dirac delta function},
\be\label{eq:vanishing-kernel-evidence}
p(y\vert\Hypo_0) 
= \prod_{i=1}^{n_L}\delta\!\le( y_{i} \ri) \, ,
\ee
which is a fairly useless hypothesis unless all of the true outputs are the zero vector.
Therefore, for generic nonzero and finite output values, the maximal evidence should lie between these two extrema. 
Specifically, seen as a function of $\kersub$, the evidence~\eqref{eq:Gaussian-evidence-single} peaks at
$\kersub=\kersub^{(L)}(x,x)\equiv \sum_{i=1}^{n_{L}}y_{i}^2/n_{L}$.
Our remaining hypothesis,
\terminate{criticality} $\Hypo_{\text{critical}}$, comes the closest to realizing this maximum.



To reiterate, for a single input we just need the kernel $\kersub$ to be of order one. For deep neural networks, this is precisely the condition that we imposed in order to avoid the \neo{exploding and vanishing kernel problem} for a single input, which we satisfied with the \terminate{parallel susceptibility} condition $\chi_{\parallel}\!\le(\Tif{\ker}{}{}\ri)=1$. Physically, the exploding kernel gives a very flat distribution spread over a big range of output norms, yielding insubstantial evidence for any particular output norm; the vanishing kernel gives sharp support for the zero norm \eqref{eq:vanishing-kernel-evidence} and no support anywhere else. Clearly the Bayes' factor\index{Bayesian inference!model comparison!Bayes' factor}~\eqref{eq:bayes-factor-equal-hypo} will prefer any hypothesis that gives more focused support over reasonable output norms.
In the language of our \terminate{Occam's razor} discussion, $\Hypo_{\infty}$ is too complex, predicting every possible norm, while $\Hypo_{0}$ is too simple, predicting only one particular norm.
The only hypothesis that gives a finite and nonzero $\kersub$ in the deep asymptotic regime is $\Hypo_{\text{critical}}$, whereat the \terminate{initialization hyperparameters} are tuned to satisfy $\chi_{\parallel}\!\le(\Tif{\ker}{}{}\ri)=1$.\footnote{N.B.~polynomially vanishing kernels give finite evidence\index{Bayesian inference!evidence} for all practical depths. To be very pedantic about this, for such kernels  -- for instance, for the $\tanhA$ -- for absurdly deep networks the truly Bayesian-optimal $C_W$ would be ever so slightly above its critical value.}







Now that we see how this works, let's  extend our analysis of the evidence\index{Bayesian inference!evidence} to two inputs, with $\tra=\pm$.
Intuitively, we expect to find the \terminate{perpendicular susceptibility} condition $\chi_{\perp}\!\le(\Tif{\ker}{}{}\ri)=1$ and thus demonstrate a conclusive preference for the criticality hypothesis $\Hypo_{\text{critical}}$.
To rediscover $\chi_{\perp}\!\le(\Tif{\ker}{}{}\ri)=1$, it will be sufficient to consider the case where both inputs have the same norm 
\be\label{eq:same-norm-dah}
\sum_{i=1}^{n_0}\x{i}{+}^2=\sum_{i=1}^{n_0}\x{i}{-}^2\, .
\ee
Then, recalling our\index{$\gamma^{[a]}$ basis!kernel}
decomposition into the $\gamma^{[a]}_{\tra_1\tra_2}$ basis~\eqref{eq:kernel-expand-gamma}, we can write the kernel as
\be
\kersub_{\tra_1\tra_2}=\begin{pmatrix}
\kersub_{[0]}+\kersub_{[2]} & \kersub_{[0]}-\kersub_{[2]}  \\
\kersub_{[0]}-\kersub_{[2]}   & \kersub_{[0]}+\kersub_{[2]}
\end{pmatrix}\, ,%
\ee
where we've used the fact that $\kersub_{[1]}=0$ when both inputs have the same norm~\eqref{eq:same-norm-dah}.

In this basis, the determinant is given by $\dete{2\pi \kersub}=16\pi^2\kersub_{[0]}\kersub_{[2]}$, and the inverse of the kernel is given by
\be
\kersub^{\tra_1\tra_2}=\frac{1}{4\kersub_{[0]}\kersub_{[2]}}\begin{pmatrix}
\kersub_{[0]}+\kersub_{[2]} & -\kersub_{[0]}+\kersub_{[2]}  \\
-\kersub_{[0]}+\kersub_{[2]}   & \kersub_{[0]}+\kersub_{[2]}
\end{pmatrix}\, ,%
\ee
which in turn lets us evaluate the argument of the exponential in~\eqref{eq:Gaussian-evidence-general} as
\begin{align}
\sum_{i=1}^{n_{L}}\sum_{\tra_1,\tra_2=\pm}\kersub^{\tra_1 \tra_2}\y{i}{\tra_1}\y{i}{\tra_2}\, 
=&\sum_{i=1}^{n_{L}}\frac{1}{4\kersub_{[0]}\kersub_{[2]}}\le[\kersub_{[2]}\le(\y{i}{+}+\y{i}{-}\ri)^2+\kersub_{[0]}\le(\y{i}{+}-\y{i}{-}\ri)^2\ri] \notag \\
=&\frac{\LLmax_{[0]}}{\kersub_{[0]}}+\frac{\LLmax_{[2]}}{\kersub_{[2]}}\, ,
\end{align}
where in the last equality we introduced the components\index{output matrix!$\gamma^{[a]}$ basis|see{$\gamma^{[a]}$ basis}}
\be\label{eq:output-matrix-decomposition}
\LLmax_{[0]}\equiv\sum_{i}^{n_L}\le(\frac{\y{i}{+}+\y{i}{-}}{2}\ri)^2\, , \qquad \LLmax_{[2]}\equiv\sum_{i}^{n_L}\le(\frac{\y{i}{+}-\y{i}{-}}{2}\ri)^2 \, .
\ee
All together, this gives a simple expression for the two-input evidence\index{Bayesian inference!evidence},
\begin{align}\label{eq:Gaussian-evidence-double}
p\!\le(y_{+},y_{-} \vert\Hypo\ri)=&\le(16\pi^2 \kersub_{[0]}\kersub_{[2]}\ri)^{-\frac{n_{L}}{2}} \exp\!\le(-\frac{\LLmax_{[0]}}{2\kersub_{[0]}}-\frac{\LLmax_{[2]}}{2\kersub_{[2]}}\ri)\, \\
=&\le[\le(4\pi \kersub_{[0]}\ri)^{-\frac{n_{L}}{2}} \exp\!\le(-\frac{\LLmax_{[0]}}{2\kersub_{[0]}}\ri)\ri]\times\le[\le(4\pi \kersub_{[2]}\ri)^{-\frac{n_{L}}{2}}  \exp\!\le(-\frac{\LLmax_{[2]}}{2\kersub_{[2]}}\ri)\ri]\, .\notag
\end{align}




Now, let's consider a generic pair of input-output pairs $(x_{+}, y_{+})$ and $(x_{-}, y_{-})$ for which both the average and the difference of the true outputs, $\LLmax_{[0]}$ and $\LLmax_{[2]}$~\eqref{eq:output-matrix-decomposition}, are nonzero and of order one.  Then, running the same argument as we did for the single-input evidence\index{Bayesian inference!evidence}, we prefer a hypothesis that comes as close as possible to having both $\kersub_{[0]}\approx\LLmax_{[0]}/n_{L}=\o{1}$ -- from maximizing the object in the first square brackets of~\eqref{eq:Gaussian-evidence-double} -- and  $\kersub_{[2]}\approx\LLmax_{[2]}/n_{L}=\o{1}$ -- from maximizing the object in the second square brackets of~\eqref{eq:Gaussian-evidence-double}. And, as we learned in~\S\ref{ch:eft-mlp}, to keep both $\kersub_{[0]}$ \emph{and} $\kersub_{[2]}$ of order one, we need to set both the critical \terminate{parallel susceptibility} condition $\chi_{\parallel}\!\le(\Tif{\ker}{}{}\ri)=1$ \emph{and}  the critical \terminate{perpendicular susceptibility} condition $\chi_{\perp}\!\le(\Tif{\ker}{}{}\ri)=1$\index{$\gamma^{[a]}$ basis!output matrix}.\footnote{Finally, let's consider the trivial fixed point\index{fixed point!trivial}s with nonzero kernel values $K^{\star}\ne0$.\label{foot:parallel-criticality} 
(This can occur, e.g., for the $K^\star=0$ universality class, for which there exists fixed points $K^\star$ that have $\chi_\perp(K^\star)=1$ but $\chi_\parallel(K^\star)<1$.)%
For this analysis, we need to relax the same-norm condition~\eqref{eq:same-norm-dah} and consider the most general form of the two-input kernel. Projecting the kernel into the $\gamma^{[a]}_{\tra_1\tra_2}$ basis~\eqref{eq:kernel-expand-gamma} as
\be
\kersub_{\tra_1\tra_2}=\begin{pmatrix}
\kersub_{[0]}+\kersub_{[1]}+\kersub_{[2]} & \kersub_{[0]}-\kersub_{[2]}  \\
\kersub_{[0]}-\kersub_{[2]}   & \kersub_{[0]}-\kersub_{[1]}+\kersub_{[2]}
\end{pmatrix}\, ,%
\ee
we can similarly use~\eqref{eq:trace-projection} to decompose the \neo{output matrix}, $\LLmax_{\tra_1\tra_2}\equiv \sum_{i=1}^{n_L} \y{i}{\tra_1}\y{i}{\tra_2}$, into
components
\be
\LLmax_{[0]}=\sum_{i}\le(\frac{\y{i}{+}+\y{i}{-}}{2}\ri)^2\, ,\quad \LLmax_{[1]}=\frac{\sum_{i}\y{i}{+}^2-\sum_{i}\y{i}{-}^2}{2}\, ,\quad \LLmax_{[2]}=\sum_{i}\le(\frac{\y{i}{+}-\y{i}{-}}{2}\ri)^2\, .
\ee
Then, a quick calculations shows that the evidence\index{Bayesian inference!evidence} evaluates to
\begin{align}\label{eq:Gaussian-evidence-double-more}
p\!\le(y_{+},y_{-} \vert\Hypo\ri)=&\le[4\pi^2 (4\kersub_{[0]}\kersub_{[2]}-\kersub_{[1]}^2)\ri]^{-\frac{n_{L}}{2}} \exp\!\le[-\frac{(4\kersub_{[0]}\LLmax_{[2]}+4\kersub_{[2]}\LLmax_{[0]}-2\kersub_{[1]}\LLmax_{[1]})}{2(4\kersub_{[0]}\kersub_{[2]}-\kersub_{[1]}^2)}\ri]\, .
\end{align}
Now, we see from this expression that a hypothesis with $\kersub_{[1]}\LLmax_{[1]}>0$ has improved evidence compared to the one with non-positive $\kersub_{[1]}\LLmax_{[1]}$. In particular, if a fixed point is trivial then the parallel perturbation $\kersub_{[1]}$ always vanishes exponentially, even if the fixed-point value of the kernel is non-vanishing $\ker^{\star}\ne0$. Thus, such a hypothesis will be disfavored compared to $\Hypo_{\text{critical}}$,  completing our argument.

It should be noted that for this distinction to matter, we must have a nonzero $\LLmax_{[1]}$, meaning $\sum_{i}\y{i}{+}^2\ne \sum_{i}\y{i}{-}^2$. For networks used as generic function approximators -- or for tasks where the network outputs are general and used downstream for other tasks -- this may matter. For deep-learning tasks where all the true outputs have the same norm, this may not matter.}
Therefore, with this \emph{evidence for criticality}, Bayesian model comparison\index{Bayesian inference!model comparison} demonstrates a full preference for $\Hypo_{\text{critical}}$.\footnote{Technically, what we've shown here is a preference for criticality in the Bayesian prior distribution. In~\S\ref{sec:EVGP-WEP}, we'll also find a natural preference for \terminate{criticality} in the \terminate{initialization distribution}, by showing that such a tuning is necessary for controlling the exploding and vanishing \emph{gradient} problem\index{exploding and vanishing gradient problem} that arises with gradient-based learning.} \\






\noindent{}\emph{Programming note}\index{programming note}: since conditioning on $\Hypo$ is so deeply ingrained in our minds by now, for notational simplicity we'll re-start the suppression of this conditioning from here on out.







\subsection{Let's Not Wire Together}\label{subsec:absence-FF-Bayes}
Now, let's work out the full posterior distribution\index{posterior}~\eqref{eq:BB-NN} at infinite width.\footnote{
    The form of this distribution was first worked out by Williams in \cite{williams-infinite} for one-hidden-layer networks.
} As we already have an expression for the evidence $p\!\le(y_{\A}\ri)$~\eqref{eq:Gaussian-evidence-general} in the denominator, let's focus on the joint distribution $p\!\le(y_{\A},z_{\B}^{(L)}\ri)$ in the numerator. Recall also that to discuss the posterior we need to partition the data into two subsamples, $\D\equiv\A\cup\B$, one for which we have observed the true output values $y_{\A}$ and the other for which we are going to infer the output values. 

With such a data partitioning in mind, we can write out the joint distribution as
\begin{align}\label{eq:joint-numerator}
&p\!\le(y_{\A},z_{\B}^{(L)}\ri)\, \\
=&\frac{1}{\sqrt{\dete{2\pi \ker}^{n_{L}}}}\exp\!\Bigg[-\frac{1}{2}\sum_{i=1}^{n_{L}}\Bigg(\sum_{\tra_1,\tra_2\in\A}\!\!\!\ker^{\tra_1 \tra_2}\y{i}{\tra_1}\y{i}{\tra_2}+\sum_{\tra_1\in\A,\tea_2\in\B}\!\!\!\!\!\ker^{\tra_1 \tea_2}\y{i}{\tra_1}\z{i}{\tea_2}{L}\, \notag\\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad +\sum_{\tea_1\in\B, \tra_2\in\A}\!\!\!\!\!\ker^{\tea_1 \tra_2}\z{i}{\tea_1}{L}\y{i}{\tra_2}+\sum_{\tea_1,\tea_2\in\B}\!\!\!\ker^{\tea_1 \tea_2}\z{i}{\tea_1}{L}\z{i}{\tea_2}{L}\Bigg)\Bigg]\, ,\notag
\end{align}
where $\ker^{\tra_1 \tra_2}$, $\ker^{\tra_1 \tea_2}$, $\ker^{\tea_1 \tra_2}$, and $\ker^{\tea_1 \tea_2}$ are the blocks of %
\be\label{eq:kernel-inverse-joint-decompose}
\ker^{\delta_1\delta_2}\equiv\begin{pmatrix}
\ker^{\tra_1\tra_2} & \ker^{\tra_1\tea_2} \\
\ker^{\tea_1\tra_2}  &\ker^{\tea_1\tea_2} 
\end{pmatrix}\, ,
\ee
which is the inverse of the whole $\ND$-by-$\ND$ kernel matrix,
\be\label{eq:kernel-submatrix-decomposition}
\ker_{\delta_1\delta_2}
=\begin{pmatrix}
\kersub_{\tra_1\tra_2} & \ker_{\tra_1\tea_2} \\
\ker_{\tea_1\tra_2}  &\ker_{\tea_1\tea_2} 
\end{pmatrix}\, .
\ee
To make progress, we need to relate the submatrices in the inverse~\eqref{eq:kernel-inverse-joint-decompose} to the submatrices in the kernel decomposition \eqref{eq:kernel-submatrix-decomposition}, since, recalling 
\be
\ker_{\delta_1\delta_2} \equiv \frac{1}{n_L} \sum_i^{n_L} \E{z^{(L)}_i\!\le(x_{\delta_1}\ri) z^{(L)}_i\!\le(x_{\delta_2}\ri)} + \oninv \, ,
\ee
it's these blocks that are naturally defined in terms of the data.\footnote{
As we explained before, the over-tilde on $\kersub_{\tra_1\tra_2}$ indicates that it's a submatrix of the kernel evaluated on samples in the set $\A$, only. The inverse of that block was defined explicitly in~\eqref{eq:inverse-subA} and is symbolized as $\kersub^{\tra_1 \tra_2}$. Also note that the symmetry of the full kernel, $\ker_{\delta_1\delta_2} = \ker_{\delta_2\delta_1}$, endows a similar set of symmetries on the submatrices: $\kersub_{\tra_1\tra_2}=\kersub_{\tra_2\tra_1}$, $\ker_{\tea_1\tea_2} =\ker_{\tea_2\tea_1} $, and $\ker_{\tea\tra}=\ker_{\tra\tea}$. 
}


Explicitly inverting $\ker_{\delta_1\delta_2}$ according to the inverse formula~\eqref{eq:whole-inverse}, we find that the submatrices of \eqref{eq:kernel-inverse-joint-decompose} can be defined in terms of the blocks of the kernel~\eqref{eq:kernel-submatrix-decomposition} and the inverse submatrix $\kersub^{\tra_1 \tra_2}$ on $\A$ as
\begin{align}
\ker^{\tra_1\tra_2}&\equiv\kersub^{\tra_1\tra_2}+\sum_{\tra_3,\tra_4\in\A}\sum_{\tea_3,\tea_4\in\B} \kersub^{\tra_1\tra_3}\ker_{\tra_3\tea_3}\kerpos^{\tea_3\tea_4}\ker_{\tea_4\tra_4}\kersub^{\tra_4\tra_2}\, ,\label{eq:Kinv-tratra}\\
\ker^{\tra_1\tea_2}&\equiv-\sum_{\tra_3\in\A}\sum_{\tea_3\in\B}\kersub^{\tra_1\tra_3}\ker_{\tra_3\tea_3}\kerpos^{\tea_3\tea_2}\, ,\label{eq:Kinv-tratea}\\
\ker^{\tea_1\tra_2}&\equiv-\sum_{\tra_3\in\A}\sum_{\tea_3\in\B}\kerpos^{\tea_1\tea_3}\ker_{\tea_3\tra_3}\kersub^{\tra_3\tra_2}\, ,\label{eq:Kinv-teatra}\\
\ker^{\tea_1\tea_2}&\equiv\kerpos^{\tea_1\tea_2} \, ,\label{eq:Kinv-teatea}
\end{align}
where we've had to introduce (and name a posteori) the \emph{posterior covariance}\index{posterior!posterior covariance},
\be\label{eq:GP-posterior-variance}
\kerpos_{\tea_1\tea_2}\equiv \ker_{\tea_1\tea_2}-\sum_{\tra_3,\tra_4\in\A}\ker_{\tea_1\tra_3}\kersub^{\tra_3\tra_4}\ker_{\tra_4\tea_2}\, .
\ee
The expression for~\eqref{eq:Kinv-teatea} is defined implicitly by taking the inverse of \eqref{eq:GP-posterior-variance}:
\be\label{eq:inverse-posB}
\sum_{\tea_2\in\B}\kerpos^{\tea_1\tea_2}\, \kerpos_{\tea_2\tea_3}=\delta^{\tea_1}_{\ \tea_3}\, .
\ee



Since these are essential relations, let us check all the components of the inverse formula~\eqref{eq:whole-inverse}, one by one.
Firstly, considering the $\delta^{\delta_1}_{\ \delta_3} \to \delta^{\tra_1}_{\ \tra_3}$ component, we see
\begin{align}
\sum_{\delta_2\in\D}\ker^{\tra_1\delta_2}\ker_{\delta_2 \tra_3}&=\sum_{\tra_2\in\A}\ker^{\tra_1\tra_2}\ker_{\tra_2 \tra_3}+\sum_{\tea_2\in\B}\ker^{\tra_1\tea_2}\ker_{\tea_2 \tra_3}\, \notag \\
&=\sum_{\tra_2\in\A}\kersub^{\tra_1\tra_2}\kersub_{\tra_2 \tra_3}=\delta^{\tra_1}_{\ \tra_3}\, ,
\end{align}
where in the first line we decomposed the sum over $\delta_2 \in \D$ into separate sums over $\tra_2 \in \A$ and over $\tea_2 \in \B$ according to our partitioning $\D = \A \cup \B$, then in going to the second line we plugged in our expressions for the inverse blocks~\eqref{eq:Kinv-tratra} and~\eqref{eq:Kinv-tratea}, 
and finally in the last step we used the fact that $\kersub^{\tra_1\tra_2}$ is the inverse of the submatrix $\kersub_{\tra_1\tra_2}$~\eqref{eq:inverse-subA}.
Secondly, considering the $\delta^{\delta_1}_{\ \delta_3} \to \delta^{\tea_1}_{\ \tea_3}$ component, we see
\begin{align}
&\sum_{\delta_2\in\D}\ker^{\tea_1\delta_2}\ker_{\delta_2 \tea_3}=\sum_{\tra_2\in\A}\ker^{\tea_1\tra_2}\ker_{\tra_2 \tea_3}+\sum_{\tea_2\in\B}\ker^{\tea_1\tea_2}\ker_{\tea_2 \tea_3}\, \\
=&\sum_{\tea_2\in\B}\kerpos^{\tea_1\tea_2}\le(\ker_{\tea_2 \tea_3}-\sum_{\tra_3,\tra_2\in\A} \ker_{\tea_2\tra_3}\kersub^{\tra_3\tra_2}\ker_{\tra_2\tea_3}\ri)=\sum_{\tea_2\in\B}\kerpos^{\tea_1\tea_2}\kerpos_{\tea_2\tea_3}=\delta^{\tea_1}_{\ \tea_3}\, ,\notag
\end{align}
where as  before in the first line we decomposed the sum over $\delta_2 \in \D$ into separate sums over $\tra_2 \in \A$ and over $\tea_2 \in \B$ according to our partitioning $\D = \A \cup \B$, then in going to the second line we plugged in our expressions for the inverse blocks~\eqref{eq:Kinv-teatra} and~\eqref{eq:Kinv-teatea}, and finally, identifying the expression in the parenthesis as the definition of the posterior covariance\index{posterior!posterior covariance} $\kerpos_{\tea_1\tea_2}$~\eqref{eq:GP-posterior-variance}, we get the final result since $\kerpos^{\tea_1\tea_2}$ is the inverse of the posterior covariance~\eqref{eq:inverse-posB}.
Lastly, we consider the off-diagonal block:
\begin{align}
&\sum_{\delta_2\in\D}\ker^{\tra_1\delta_2}\ker_{\delta_2 \tea_3}=\sum_{\tra_2\in\A}\ker^{\tra_1\tra_2}\ker_{\tra_2 \tea_3}+\sum_{\tea_2\in\B}\ker^{\tra_1\tea_2}\ker_{\tea_2 \tea_3}\, \\
=&\sum_{\tra_2\in\A,\tea_2\in\B}\!\!\!\!\!\kersub^{\tra_1\tra_2}\ker_{\tra_2\tea_2}\le(\delta^{\tea_2}_{\ \tea_3}+\sum_{\tra_3,\tra_4\in\A}\sum_{\tea_4\in\B}\kerpos^{\tea_2\tea_4}\ker_{\tea_4\tra_4}\kersub^{\tra_4\tra_3}\ker_{\tra_3\tea_3}-\sum_{\tea_4\in\B}\kerpos^{\tea_2\tea_4}\ker_{\tea_4\tea_3}\ri)\, \notag\\
=&\sum_{\tra_2\in\A,\tea_2\in\B}\!\!\!\!\!\kersub^{\tra_1\tra_2}\ker_{\tra_2\tea_2}\le(\delta^{\tea_2}_{\ \tea_3}-\sum_{\tea_4\in\B}\kerpos^{\tea_2\tea_4}\kerpos_{\tea_4\tea_3}\ri)=0\, ,\notag
\end{align}
Here,  we follow the same pattern as before, \emph{(i)} decomposing the sum according to the partitioning $\D = \A \cup \B$, \emph{(ii)} plugging in expressions for inverse blocks \eqref{eq:Kinv-tratra} and \eqref{eq:Kinv-tratea}, and \emph{(iii)} using the posterior covariance\index{posterior!posterior covariance} \eqref{eq:GP-posterior-variance} and the inverse equation \eqref{eq:inverse-posB}. Everything checks out.










Now that we have some confidence in our inversions, let's plug our expressions for these submatrices~\eqref{eq:Kinv-tratra}--\eqref{eq:Kinv-teatea} into the joint prior~\eqref{eq:joint-numerator}. 
Since the posterior~\eqref{eq:BB-NN} is only a function of the outputs $z_{\B}^{(L)}$,
we can make things easier by limiting our focus to the $z_{\B}^{(L)}$ dependence only, ignoring the $y_{\A}$ terms independent of $z_{\B}^{(L)}$  and ignoring the normalization factor:
\begin{align}
&p\!\le(y_{\A},z_{\B}^{(L)}\ri)\propto \exp\Bigg[-\frac{1}{2}\sum_{i=1}^{n_{L}}\sum_{\tea_1,\tea_2\in\B}\kerpos^{\tea_1\tea_2}\z{i}{\tea_1}{L}\z{i}{\tea_2}{L}\, \\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \ +\sum_{i=1}^{n_{L}}\sum_{\tea_1\in\B,\tra_1\in\A}\z{i}{\tea_1}{L}\le(\sum_{\tra_2\in\A, \tea_2\in\B}\kerpos^{\tea_1 \tea_2}\ker_{\tea_2\tra_2}\kersub^{\tra_2\tra_1}\ri)\y{i}{\tra_1}\Bigg]\, .\notag
\end{align}
At this point you know what to do: completing the square\index{complete the square} -- as should be your second nature by now -- and ignoring the new $z_{\B}^{(L)}$-independent additive constant in the exponential, you get
\begin{align}\label{eq:mid-point-in-infinite-posterior}
p\!\le(y_{\A},z_{\B}^{(L)}\ri)\propto& \exp\Bigg[-\frac{1}{2}\sum_{i=1}^{n_{L}}\sum_{\tea_1,\tea_2\in\B}\kerpos^{\tea_1\tea_2}\Bigg(\z{i}{\tea_1}{L}-\sum_{\tra_3,\tra_4\in\A}\ker_{\tea_1\tra_3}\kersub^{\tra_3\tra_4}\y{i}{\tra_4}\Bigg)\, \\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad\  \times\Bigg(\z{i}{\tea_2}{L}-\sum_{\tra_5,\tra_6\in\A}\ker_{\tea_2\tra_5}\kersub^{\tra_5\tra_6}\y{i}{\tra_6}\Bigg)\Bigg]\, .\notag
\end{align}
This distribution \eqref{eq:mid-point-in-infinite-posterior} is still Gaussian, with a variance given by the posterior covariance\index{posterior!posterior covariance} $\kerpos_{\tea_1\tea_2}$ and a \emph{nonzero} posterior mean\index{posterior!posterior mean}:
\be\label{eq:GP-mean}
\posGPmean_{i;\tea}\equiv \sum_{\tra_1,\tra_2\in\A} \ker_{\tea \tra_1}\kersub^{\tra_1\tra_2}\y{i}{\tra_2}\, .
\ee
Here, the
superscript $\infty$ is used to remind us that we're in the infinite-width
limit.
Finally, we realize that the posterior distribution\index{posterior}~\eqref{eq:BB-NN} is proportional to the joint prior~\eqref{eq:mid-point-in-infinite-posterior},
\be 
p\!\le(z_{\B}^{(L)}\Big\vert y_{\A}\ri)\propto p\!\le(y_{\A},z_{\B}^{(L)}\ri) \, ,
\ee
and that the posterior distribution is automatically normalized~\eqref{eq:Bayes-consistency} as a function of the variable $z_{\B}^{(L)}$.
Thus, computing the normalization factor for \eqref{eq:mid-point-in-infinite-posterior} -- or really just writing it down, since at this point you know by heart how to normalize any Gaussian distribution -- we get the posterior at infinite width\index{posterior!infinite-width distribution}:
\begin{align}\label{eq:posterior-at-infinite-width}
&p\!\le(z_{\B}^{(L)}\Big\vert y_{\A}\ri)= \frac{1}{\sqrt{\dete{2\pi \kerpos}^{n_{L}}}} \exp\!\le[-\frac{1}{2}\sum_{i=1}^{n_{L}}\sum_{\tea_1,\tea_2\in\B}\kerpos^{\tea_1 \tea_2}\le(\z{i}{\tea_1}{L}-\posGPmean_{i;\tea_1}\ri)\le(\z{i}{\tea_2}{L}-\posGPmean_{i;\tea_2}\ri)\ri]\, .
\end{align}

The \emph{posterior mean}\index{posterior!posterior mean} $\posGPmean_{i;\tea}$ represents our updated belief about the expected network output for the input $\x{j}{\tea}\in\B$ after incorporating information about the true outputs $y_{\A}$ for all the inputs $\x{j}{\tra}\in\A$; as such, it is explicitly a function of the true input-output pairs $x_{\A}$ and $y_{\A}$ in the subsample $\A$, as we see in~\eqref{eq:GP-mean}. Importantly, our expected predictions were a priori zero -- indicating an \terminate{inductive bias} towards vanishing outputs on average -- and now a posteriori our predictions are shifted to something nonzero. Such a nonzero posterior mean\index{posterior!posterior mean} is a signature that learning is (finally!) happening.
In addition, the posterior covariance\index{posterior!posterior covariance} $\kerpos_{\tea_1\tea_2}$ encodes the confidence interval: the smaller the covariance is, the more sharply peaked the posterior is around its mean, and the more confident the model is about its predictions.


Practically speaking, note that in order to compute the mean prediction $\posGPmean_{i;\tea_1}$ according to its definition~\eqref{eq:GP-mean}, we'd in principle need to invert -- and then represent -- the $\NR$-by-$\NR$ submatrix $\kersub_{\tra_1\tra_2}$. As the size of our observations $\NR$ grows, the computational cost of such an inversion grows very fast.\footnote{For instance,~the computational cost of \neo{Gauss-Jordan elimination} scales as $\sim \NR^3$ and requires us to represent the $\NR \times \NR$-dimensional inverse in memory. Things can be improved a bit by realizing that to compute the posterior mean we only really require the \neo{matrix-vector product} of the inverse with the observations: $\sum_{\tra_2 \in \A} \kersub^{\tra_1\tra_2} y_{i;\tra_2}$. However, such an improvement is still not really sufficient for Bayesian learning to compete practically with gradient-based learning for large datasets $\A$.} This hidden catch is why -- though theoretically quite elegant -- (at least any naive implementation of) Bayesian learning is not practical for large datasets. Instead, for this reason we will essentially need to rely on approximation methods for model fitting, such as MLE \eqref{eq:mle-estimate}\index{maximum likelihood estimation}.\index{Bayesian inference!practicalities}
We'll comment more on this next chapter (\S\ref{ch:training}).

Theoretically \emph{and} practically speaking, there is another serious issue with the infinite-width posterior mean. Looking at its expression~\eqref{eq:GP-mean},
we see that the mean prediction on the output component $i$ is entirely independent from the observations $\y{j}{\alpha}$ that we made on the other components with $j\ne i$. Thus, our updated best estimate of these different output components are entirely uncorrelated, though in principle observations of different components $j$ may contain very useful information about a given component $i$.\footnote{\label{foot:distillation}The concept of \neo{knowledge distillation}\index{distillation|see{knowledge distillation}} \cite{hinton2015distilling} is predicated on this principle of correlations among the output components. For example, if a network is trying to classify\index{classification} images of hand-written digits, a certain example of a ``$2$'' may be more ``$7$''-like or more ``$3$''-like. Such feature information is quite useful, especially if the output of the network is used downstream for some other task.
}
In fact, we see from \eqref{eq:posterior-at-infinite-width} that the posterior distribution actually factorizes as
\be\label{eq:stat-independence-as-bad-posterior}
p\!\le(z_{i; \B}^{(L)}, z_{j; \B}^{(L)} \Big\vert y_{i;\A}, y_{j;\A}\ri)=p\!\le(z_{i; \B}^{(L)} \Big\vert y_{i;\A}\ri)\, p\!\le(z_{j; \B}^{(L)} \Big\vert y_{j;\A}\ri)\, , \qquad (i \neq j) \, ,
\ee
meaning that the different output components are entirely statistically independent.\index{statistical independence}\footnote{To be FAIR\index{FAIR|see{Facebook AI Research}}\index{Facebook AI Research}, the issue is with the \terminate{infinite-width limit} itself, as different output components are also decorrelated for infinite-width networks trained with gradient-based learning (\S\ref{ch:NTHb}).
}

We can trace this independence back to a similar property of the infinite-width prior distribution
\be\label{eq:stat-independence-as-bad-prior}
p\!\le(z_{i;\A}^{(L)},z_{j;\A}^{(L)} \ri)=p\!\le(z_{i;\A}^{(L)} \ri)p\!\le(z_{j;\A}^{(L)} \ri)\, , \qquad (i\neq j),
\ee
a property that we've recognized for a while now, see e.g.~\eqref{eq:infinite-distribution-factorization}.
Thus, with Bayesian learning output features do not \emph{wire together}: recalling our discussion of \terminate{inductive bias} before (\S\ref{subsec:bayesian-model-comparison}), we see that the prior endows on the posterior an absurdly stubborn set of beliefs, namely that the components of the output are completely independent with \neo{absolute certainty}. Such an \terminate{inductive bias} is incurable by any amount of learning, irregardless of how large the set of observations $\A$ are; the inductive bias of this prior can never be overwhelmed in the infinite width limit.

Luckily, this state of affiars is completely curable --  for both \terminate{learning algorithm}s, Bayesian learning and gradient-based learning -- by backing off of the \terminate{infinite-width limit} and working with finite-width networks \ldots the actual kind of networks that are used in practice.







\subsection{Absence of Representation Learning}\label{subsec:absence-RL-Bayes}
Considering the independence of the different components of the output in the posterior, a natural follow-up question is whether or not Bayesian learning at infinite width enables \terminate{representation learning}. Here, we will show decisively that it does \emph{not}.

As a representative avatar of this question, let's compute the \emph{posterior} distribution of preactivations in the penultimate layer $\ell=L-1$ on the full set of samples $\D$, given observations $y_\A$:
\be\label{eq:Bayes-posterior-general}
p\!\le(z_{\D}^{(L-1)}\Big\vert y_\A \ri)=\frac{p\!\le(y_\A \Big\vert z_{\D}^{(L-1)}\ri)p\!\le(z_{\D}^{(L-1)} \ri)}{p\!\le(y_\A \ri)}\, .
\ee
This is an application of Bayes' rule\index{Bayesian probability!Bayes' rule}~\eqref{eq:Bayes-rule}, following from applying the product rule~\eqref{eq:logical-product} to the joint distribution $p\!\le(y_\A, z_{\D}^{(L-1)} \ri)$
between the observations $y_\A$ and the penultimate preactivations $z_{\D}^{(L-1)}$.
Here, the likelihood\index{Bayesian inference!likelihood} $p\!\le(y_\A \Big\vert z_{\D}^{(L-1)} \ri)$ is the conditional distribution $p\!\le(z_\A^{(L)} \Big\vert z_{\D}^{(L-1)} \ri)$ evaluated on our set of observations $z_{\A}^{(L)} \to y_\A$. 

We already know the form of this conditional distribution, as it is the same object~\eqref{eq:general-layer-conditional} that we needed in order to work out the layer-to-layer RG flow\index{representation group flow} of the preactivations. In general, this distribution involves the \emph{stochastic} metric  $\Ti{\widehat{G}}{\tra_1 \tra_2}{L}= \Ti{\widehat{G}}{\tra_1 \tra_2}{L}\!\le( z_{\D}^{(L-1)}\ri)$. However, in the \terminate{infinite-width limit} the metric  is entirely \emph{deterministic} $\Ti{\widehat{G}}{\tra_1 \tra_2}{L} \to \Ti{G}{\tra_1 \tra_2}{L}$, with no dependence at all on the penultimate-layer preactivations $z_{\D}^{(L-1)}$.
Thus, the likelihood\index{Bayesian inference!likelihood} at infinite width -- swapping the deterministic metric for the kernel -- is given by
\be
p\!\le(y_\A \Big\vert z_{\D}^{(L-1)} \ri)= \frac{1}{\sqrt{\dete{2\pi \kersub^{(L)}}^{n_{L}}}} \exp\!\le(-\frac{1}{2}\sum_{i=1}^{n_{L}}\sum_{\tra_1,\tra_2\in\A}\TI{\kersub}{\tra_1 \tra_2}{L}\y{i}{\tra_1}\y{i}{\tra_2}\ri)=p\!\le(y_\A  \ri)\, ,
\ee
and our expression for the posterior of the penultimate layer~\eqref{eq:Bayes-posterior-general} reduces to the prior:
\be\label{eq:infinite-width-penultimate-posterior-equals-prior}
p\!\le(z_{\D}^{(L-1)}\Big\vert y_\A \ri)=p\!\le(z_{\D}^{(L-1)}\ri)\, .
\ee
Since the posterior equals the prior, our observation of $y_\A$ had no consequence on the penultimate-layer \terminate{representation}; thus, we conclude that there is no \terminate{representation learning} at infinite width. 

This lack of \terminate{representation learning} stems from the lack of  interlayer correlation in the joint distribution $p\!\le(z_{\D}^{(\ell)}, z_{\D}^{(\ell+1)} \ri)$ at infinite width, and thus it persists for all hidden layers with $\ell<L$. This is another bad \terminate{inductive bias} of the infinite-width hypotheses: regardless of the set of observations $y_\A$ that we make, there's no amount of new information that will allow the network to update its \terminate{representation}s in the hidden layers $\ell<L$. 

\index{wiring!in Bayesian inference|see{Bayesian inference}}
This state of affairs is somewhat tragic as the whole point of having many layers -- in fact, the main motivation given for \terminate{deep learning} on the whole -- is the learning of complex representations in those hidden layers.
As we will see next, we can solve this lack of \terminate{representation learning} -- as well as the lack of \emph{wiring together}\index{Bayesian inference!wiring!infinite width} in the output -- by backing off the \terminate{infinite-width limit} and looking at finite-width effects.\footnote{
    In \S\ref{ch:NTHb}, will also show the same lack of \terminate{representation learning} occurs for the ensemble of infinite-width networks that are (theoretically) trained with gradient-based learning. This issue is also resolved (practically) in \S\ref{ch:features} by going to finite width.
}













\section{Bayesian Inference at Finite Width}\label{sec:finite-posterior}
In this section, we'll give three lessons on Bayesian learning at finite width. 
To begin, we'll show that finite-width neural networks are automatically endowed with an \terminate{inductive bias} for \terminate{neural association} due to non-Gaussian \terminate{interactions} between neurons,
leading to a natural predisposition towards \terminate{Hebbian learning} (\S\ref{subsec:Hebbian}).
With that in mind, we'll in turn demonstrate how such learning works by first
calculating the mean of the posterior distribution for the network outputs  $p\!\le(z_{\B}^{(L)}\Big\vert y_{\A} \ri)$ -- showing how  \emph{intralayer} neural interactions in the prior give rise to nontrivial correlations among the components of the output (\S\ref{subsec:presence-FF-Bayes}) -- and then
calculating the posterior distribution\index{posterior} of preactivations in the penultimate layer $p\!\le(z_{\B}^{(L-1)}\Big\vert y_{\A}\ri)$ 
-- showing how \emph{interlayer} interactions
give rise to a nonzero shift between prior and posterior, thus signaling the presence of \terminate{representation learning} at finite width (\S\ref{subsec:presence-RL-Bayes}).









\subsection{Hebbian Learning, Inc.}\label{subsec:Hebbian}
In this subsection, we'll see that finite-width neural networks have an \terminate{inductive bias} that facilitates \neo{neural association}.
To explain \term{Hebbian learning}, let's begin first with a few words from our
honorary
guest speaker, Donald Hebb:

% \setlength\epigraphwidth{.93\textwidth}
\epigraph{The general idea is an old one, that any two cells or systems of cells that are repeatedly active at the same time will tend to become ``associated,'' so that activity in one facilitates activity in the other.}{Donald Hebb\index{Hebb, Donald}, in his 1949 classic \emph{The Organization of Behavior} \cite{hebb2005organization}.}
% \setlength\epigraphwidth{.9\textwidth}

\noindent{}(\emph{Applause}\index{applause}.) 

\epigraph{Thank you very much.}{Donald Hebb, apocryphal.}

\noindent{}While Hebb was originally thinking about biological neurons\index{biological neuron}, Hebbian learning has become a popular guiding principle for systems of artificial neurons\index{artificial neuron} as well. We've actually already seen this \terminate{inductive bias} for \terminate{neural association} any of the numerous times we've discussed the presence of neural interactions in the finite-width prior distribution. 
To make this manifest, we're now going to explicitly determine the neural influence of one preactivation on another in our effective preactivation distribution at initialization.

Concretely, let's suppose that a single input $x$ is fed into a network, and we've 
checked
that at layer $\ell$ the value of the first preactivation $z_1^{(\ell)}  = \check{z}_1^{(\ell)}$ is larger than typical; given this atypical value $\check{z}_1^{(\ell)}$, we can then ask whether the second preactivation $z_2^{(\ell)}$ is likely to be atypically large. This kind of \terminate{neural association} or influence is encoded in the conditional distribution
\be\label{eq:condition-from-one-to-two}
p\Big(z_2^{(\ell)}\Big\vert \check{z}_1^{(\ell)}\Big)=\frac{p\Big(\check{z}_1^{(\ell)},z_2^{(\ell)}\Big)}{p\Big(\check{z}_1^{(\ell)}\Big)}\, .
\ee
Note that at infinite width $p\Big(z_2^{(\ell)}\Big\vert \check{z}_1^{(\ell)}\Big)=p\Big(z_2^{(\ell)}\Big)$ due to the factorization of the prior on neurons~\eqref{eq:infinite-distribution-factorization},
 and so we see right away that there is a complete absence of neural association in such a limit.



To compute this association for finite-width networks, recall from~\S\ref{sec:sum-rule} the action representation~\eqref{eq:m-neuron-action} for a distribution over $m$ neurons
\be\label{eq:m-neuron-action-reprint-bayes}
p(z_1,\ldots, z_{m})\propto\exp\!\le(-\frac{g_{m}}{2}\sum_{i=1}^{m}z_{i}^2+\frac{v}{8}\sum_{i,j=1}^{m}z_{i}^2z_{j}^2\ri)\, ,
\ee
where we have temporarily dropped \terminate{layer indices} from the variables and couplings.
Here, the quadratic coupling\index{coupling!quadratic} $g_{m}$ is given implicitly by the expression~\eqref{eq:quadratic-reprint-m-emphasis},
\be\label{eq:quadratic-reprint-m-emphasis-reprinted}
\frac{1}{g_m}=G^{(\ell)}-\frac{(m+2)}{2n_{\ell-1}}\frac{ V^{(\ell)}}{G^{(\ell)}}+\o{\frac{1}{n^2}}\, ,
\ee
and we have emphasized the dependence of the coupling on $m$; similarly, the quartic coupling\index{coupling!quartic} is given by~\eqref{eq:quartic-single-input-coupling-for-vertex},
\be\label{eq:quartic-reprint-m-emphasis}
v=\frac{1}{n_{\ell-1}} \frac{V^{(\ell)}}{\le(G^{(\ell)}\ri)^4}+\o{\frac{1}{n^2}}\, ,
\ee 
which is independent of $m$ to this order in $1/n$.
Evaluating the action representation~\eqref{eq:m-neuron-action-reprint-bayes} on $m=1$ and $m=2$ neurons and plugging the resulting distributions into our expression for the conditional distribution~\eqref{eq:condition-from-one-to-two}, we get 
\begin{align}\label{eq:conditional-one-two-neuron}
p(z_2\vert \check{z}_1)&\propto \exp\!\le[-\frac{g_2}{2}z_2^2+\frac{v}{8}\le(z_2^4+2z_2^2\check{z}_1^2\ri)\ri] \, ,
\end{align}
where, similar to the last section, for such a conditional distribution we only need to keep track of the terms in the action that depend on $z_2$.

Now that we have a conditional distribution, let's evaluate some conditional expectations. Since this distribution is manifestly even in $z_2$, i.e.~invariant under a sign flip $z_2\leftrightarrow-z_2$, all the odd-point correlators vanish, including the conditional mean. This means that the first nontrivial observable is the two-point correlator or conditional variance:
\begin{align}
\int dz_2\ p(z_2\vert \check{z}_1) \, z_2^2=&\frac{\int dz_2\ \exp\!\le[-\frac{g_2}{2}z_2^2+\frac{v}{8}\le(z_2^4+2z_2^2\check{z}_1^2\ri)\ri]z_2^2}{\int dz_2\ \exp\!\le[-\frac{g_2}{2}z_2^2+\frac{v}{8}\le(z_2^4+2z_2^2\check{z}_1^2\ri)\ri]}\, \label{eq:Hebbian-calc-middle}\\
=&\frac{\int dz_2\ e^{-\frac{g_2 z_2^2}{2}}\le[z_2^2+\frac{v}{8}\le(z_2^6+2z_2^4\check{z}_1^2\ri)+\o{v^2}\ri]}{\int dz_2\ e^{-\frac{g_2 z_2^2}{2}}\le[1+\frac{v}{8}\le(z_2^4+2z_2^2\check{z}_1^2\ri)+\o{v^2}\ri]}\, \notag\\
=&\frac{g_2^{-1}+\frac{v}{8}\le(15g_2^{-3}+6g_2^{-2}\check{z}_1^2\ri)}{1+\frac{v}{8}\le(3 g_2^{-2}+2g_2^{-1} \check{z}_1^2\ri)}+\o{v^2}\, \notag\\
=&g_2^{-1}+\frac{v}{2}g_2^{-2}\le(3 g_2^{-1}+\check{z}_1^2\ri)+\o{v^2}\, .\notag
\end{align}
Above, on the first line we used \eqref{eq:conditional-one-two-neuron} in the numerator and at the same time computed its normalization in the denominator,
on the second line we expanded both the numerator and denominator in $v$, on the third line we  
computed the single-variable Gaussian integrals, and on the final line we expanded the denominator in $v$.
Plugging in our expressions for the quadratic coupling\index{coupling!quadratic}~\eqref{eq:quadratic-reprint-m-emphasis-reprinted} and the quartic coupling\index{coupling!quartic}~\eqref{eq:quartic-reprint-m-emphasis} and reimplementing \terminate{layer indices}, we find
\be\label{eq:conditional-variance}
\int dz_2^{(\ell)}\ p\!\le(z_2^{(\ell)}\Big\vert \check{z}_1^{(\ell)}\ri) \le(z_2^{(\ell)}\ri)^2=G^{(\ell)}+\frac{1}{2}\le[\le(\check{z}_1^{(\ell)}\ri)^2-G^{(\ell)}\ri]\le[\frac{ V^{(\ell)}}{n_{\ell-1} \le(G^{(\ell)}\ri)^2}\ri]+\o{\frac{1}{n^2}}\, .
\ee
In passing, note for later that
this result holds for any distinct pair of neurons by replacing \terminate{neural indices} as $1,2\to i_1,i_2$, with $i_1\ne i_2$.

This conditional variance \eqref{eq:conditional-variance} embodies some really interesting \terminate{physics}. 
If the observed value $\le(\check{z}_1^{(\ell)}\ri)^2$ is larger/smaller than its expected value $\E{\le(z_1^{(\ell)}\ri)^2}=G^{(\ell)}$, then
the variance of $z_2^{(\ell)}$ will itself be larger/smaller than is typical. Thus, $z_1^{(\ell)}$ and $z_2^{(\ell)}$ correlate their atypical firing.\footnote{You may or may not recall from footnote~\ref{footnote-kurtosis} in \S\ref{sec:not-Gauss} that having a nontrivial connected four-point correlator\index{connected correlator!four-point} serves as a measure of the potential for outliers. In statistics\index{statistics (branch of mathematics)}, for single-variable distributions this is called the \emph{excess kurtosis}\index{kurtosis, excess}; here, we see a multi-neuron generalization (which apparently can be called the \emph{cokurtosis}\index{cokurtosis|see{kurtosis, excess}}\index{kurtosis, excess!cokurtosis}). In particular, observing an outlying value $z_1^{(\ell)} = \check{z}_1^{(\ell)}$ implies that we are more likely to see outlying values for $z_2^{(\ell)}$ as well.
At the end of Appendix \ref{app:mi-stuff}, we'll provide an information-theoretic reformulation of this phenomenon that will also shed further light on how deep a network should be in order to best take advantage of it.
} This effect is proportional to the normalized \terminate{four-point vertex} in the second square brackets of \eqref{eq:conditional-variance}, which  as we know from \eqref{eq:k-star-equals-zero-normalized-four-point-scaling-law} and~\eqref{eq:vertex-scaling-law} is proportional to $\ell/n$ across our universality classes when at \terminate{criticality}.
In other words, deeper layers have an \terminate{inductive bias} to build more \terminate{neural association}s. Moreover, the presence of these associations is mediated by the interaction\index{interactions}s in the effective action induced at finite width \emph{only}. As we will soon show, nontrivial \terminate{representation learning} is a direct descendant of such associations.


Note that this result should be interpreted as a \emph{propensity} for atypicality rather than a \emph{guarantee}. Since the conditional variance~\eqref{eq:conditional-variance} applies to any pair of neurons, conditioned on a particular neuron $i_*$ having a larger/smaller norm than expected, then all of the other neurons with $i \neq i_*$ are more likely to have a larger/smaller norm, though not all will. In a given realization of a network in practice, the ones that happen to have a larger/smaller norm are the ones that are more likely to develop a correlation with $i_*$ as learning progresses.

\index{wiring|seealso{Hebbian learning}}
\terminate{Hebbian learning} is often summarized by the following slogan: \emph{neurons that fire together, wire together}. What we see here is that conditioned on an atypical firing $\check{z}_1$, another preactivation, e.g.~$z_2$, is much more likely to have an atypical firing itself. This propensity of finite-width networks to \emph{fire together} is an \terminate{inductive bias} of our \terminate{prior} beliefs before Bayesian learning as well as of our \terminate{initialization distribution} before gradient-based learning.
To understand the \emph{wire together} part, let's now consider the Bayesian \terminate{posterior}.\footnote{
For some models of \terminate{artificial neuron}s -- such as the \terminate{Hopfield network} -- \terminate{Hebbian learning} is often added in \emph{by hand}. For instance,  one learning rule for such networks that explicitly implements the Hebbian principle is updating the weights connecting two neurons $i$ and $j$ as $W_{ij} \propto z_i(x) z_j(x)$ when observing activities $z_i(x)$ and $z_j(x)$ for a given input $x$. 

In contrast, any finite-width feedforward neural network\index{feedforward network} 
should automatically incorporate \terminate{Hebbian learning} \emph{by nature}.
To underscore this point further, in~\S\ref{ch:eot} we'll perform an analogous computation for a gradient-descent update.\index{gradient descent} Since the prior has the same form as the initialization distribution, we expect that all learned finite-width networks will 
inc.~the \terminate{Hebbian learning} principle automatically, regardless of whether that learning is Bayesian or gradient-based.}











\subsection{Let's Wire Together}\label{subsec:presence-FF-Bayes}\index{Bayesian inference!wiring!finite width}
Let's start with some more reminiscing 
through our now well-adjusted Bayesian lens. Recall from~\eqref{eq:general-ell-action} that the prior distribution\index{prior} over peractivations is nearly-Gaussian\index{nearly-Gaussian distribution} at large-but-finite width:
\begin{align}\label{eq:general-ell-action-reprint}
p\!\le(z^{(L)}_{\D}\ri)
&\propto\exp\!\Bigg[-\frac{1}{2}\sum_{j=1}^{n_{L}}\sum_{\delta_1,\delta_2\in\D} g^{\delta_1\delta_2} \z{j}{\delta_1}{L}\z{j}{\delta_2}{L}\, \\
&\quad \quad \quad \ \ +\frac{1}{8}\sum_{j,k=1}^{n_{L}}\sum_{\delta_1,\ldots,\delta_4\in\D}v^{(\delta_1\delta_2)(\delta_3\delta_4)} \z{j}{\delta_1}{L}\z{j}{\delta_2}{L}\, \z{k}{\delta_3}{L}\z{k}{\delta_4}{L}+\ldots\Bigg]\, .\nonumber%
\end{align}
As a reminder, the quadratic coupling\index{coupling!quadratic} $g^{\delta_1\delta_2}\equiv g^{\delta_1\delta_2}_{(L)}$ \eqref{eq:two-point-match-general} and the quartic coupling\index{coupling!quartic}  $v^{(\delta_1\delta_2)(\delta_3\delta_4)}\equiv v^{(\delta_1\delta_2)(\delta_3\delta_4)}_{(L)}$ \eqref{eq:four-point-match-general} 
depend explicitly on groups of inputs
from the dataset\index{input data} $\D$ and implicitly on the $\Hypo$yperparameters $C_b$ and $C_W$, the widths $n_{\ell}$, and the depth $L$.
As a consequence of the nonzero \emph{intralayer} interaction between different output preactivations in the prior, there will be non-vanishing correlations between the components of the network outputs in the posterior.



As we did at infinite width, we'll start with the prior distribution~\eqref{eq:general-ell-action-reprint} and then obtain the posterior distribution $p\!\le(z_{\B}^{(L)}\Big\vert y_{\A}\ri)\propto p\!\le(y_{\A},z_{\B}^{(L)}\ri)$ by plugging in our observations $\z{i}{\tra}{L}\to \y{i}{\tra}$ and keeping track of the dependence on the remaining variables $\z{i}{\tea}{L}$. For the quadratic term in the action, with exactly the same set of manipulations as we did in the infinite-width limit (\S\ref{subsec:absence-FF-Bayes}), replacing the inverse kernel with the quadratic coupling at finite width $\ker^{\delta_1\delta_2}\to g^{\delta_1\delta_2}$, we find
\begin{align}\label{eq:quardratic-posterior}
&\frac{1}{2}\sum_{j=1}^{n_{L}}\sum_{\delta_1,\delta_2\in\D} g^{\delta_1\delta_2} \z{j}{\delta_1}{L}\z{j}{\delta_2}{L}\Big\vert_{\z{i}{\tra}{L}=\y{i}{\tra}}\, \\
=&\ \text{constant}+\frac{1}{2}\sum_{j=1}^{n_{L}}\sum_{\tea_1,\tea_2\in\B} \gpos^{\tea_1\tea_2} \le(\z{j}{\tea_1}{L}-\posmean_{j;\tea_1}\ri)\le(\z{j}{\tea_2}{L}-\posmean_{j;\tea_2}\ri)\, ,\notag
\end{align}
with the \emph{naive} posterior mean\index{posterior!posterior mean!finite width}
\be\label{eq:naive-mean}
\posmean_{i;\tea}\equiv \sum_{\tra_1,\tra_2\in\A} g_{\tea \tra_1}\gsub^{\tra_1\tra_2}\y{i}{\tra_2}\, ,
\ee
and the \emph{naive} posterior covariance\index{posterior!posterior covariance}\index{posterior!posterior covariance!finite width}
\be\label{eq:naive-posterior-variance}
\gpos_{\tea_1\tea_2}\equiv g_{\tea_1\tea_2}-\sum_{\tra_3,\tra_4\in\A}g_{\tea_1\tra_3}\gsub^{\tra_3\tra_4}g_{\tra_4\tea_2}\, .
\ee
We say \emph{naive} here because there are additional corrections we need to consider coming from the 
the quartic term\index{coupling!quartic} in the action. Let see explicitly how this works for the posterior mean\index{posterior!posterior mean}.



Given the observed true outputs $\y{i}{\tra}$ and the quadratic term~\eqref{eq:quardratic-posterior} centered at the naive posterior mean\index{posterior!posterior mean} $\posmean_{i;\tea}$, it is natural to center ourselves at 
\be\label{eq:posmean-definition}
\meanstring_{i;\delta}\equiv\le(\y{i}{\tra}\ ,\ \posmean_{i;\tea}\ri)=\Big(\y{i}{\tra}\ ,\ \sum_{\tra_1,\tra_2\in\A} g_{\tea \tra_1}\gsub^{\tra_1\tra_2}\y{i}{\tra_2}\Big)\, ,
\ee
and define a fluctuating variable $w_{i;\tea}\equiv\z{i}{\tea}{L}-\posmean_{i;\tea}$ so that we can plug the decomposition
\be\label{eq:posterior-mean-finite-width-decomposition}
\z{i}{\delta}{L}=\le(\z{i}{\tra}{L}\ ,\ \z{i}{\tea}{L}\ri) \to \le(\y{i}{\tra}\ ,\ \posmean_{i;\tea}+w_{i;\tea}\ri)=\meanstring_{i;\delta}+\le(0,\ w_{i;\tea}\ri)\, ,
\ee
into the action~\eqref{eq:general-ell-action-reprint}, thus making the partitioning into subsamples $\D = \A \cup \B$ manifest. In terms of this fluctuation, the quadratic term~\eqref{eq:quardratic-posterior} 
takes the form
\be
\text{constant}+\frac{1}{2}\sum_{j=1}^{n_{L}}\sum_{\tea_1,\tea_2\in\B} \gpos^{\tea_1\tea_2} w_{j;\tea_1} w_{j;\tea_2}\, ,
\ee
and the quartic term can be evaluated as
\begin{align}
&\mathcal{Q}\!\le(w\ri)\equiv \Bigg[\frac{1}{8}\sum_{j,k=1}^{n_{L}}\sum_{\delta_1,\ldots,\delta_4\in\D}v^{(\delta_1\delta_2)(\delta_3\delta_4)} \z{j}{\delta_1}{L}\z{j}{\delta_2}{L}\ \z{k}{\delta_3}{L}\z{k}{\delta_4}{L}\Bigg]\Bigg\vert_{\z{i}{\tra}{L}=\Phi_{i;\tra};\ \z{i}{\tea}{L}=\Phi_{i;\tea}+w_{i;\tea}}\, \notag \\
=&\ \text{constant}+\frac{4}{8}\sum_{j}\sum_{\tea_1\in\B}w_{j;\tea_1}\le(\sum_{k}\sum_{\delta_1,\delta_2,\delta_3\in\D}v^{(\tea_1\delta_1)(\delta_2\delta_3)} \meanstring_{j;\delta_1}\meanstring_{k;\delta_2}\meanstring_{k;\delta_3}\ri)\, \notag\\
&+\frac{2}{8}\sum_{j}\sum_{\tea_1,\tea_2\in\B}w_{j;\tea_1}w_{j;\tea_2}\le(\sum_{k}\sum_{\delta_1,\delta_2\in\D}v^{(\tea_1\tea_2)(\delta_1\delta_2)} \meanstring_{k;\delta_1}\meanstring_{k;\delta_2}\ri)\, \notag\\
&+\frac{4}{8}\sum_{j,k}\sum_{\tea_1,\tea_2\in\B}w_{j;\tea_1}w_{k;\tea_2}\le(\sum_{\delta_1,\delta_2\in\D}v^{(\tea_1\delta_1)(\tea_2\delta_2)} \meanstring_{j;\delta_1}\meanstring_{k;\delta_2}\ri)\, \notag\\
&+\frac{4}{8}\sum_{j,k}\sum_{\tea_1,\tea_2,\tea_3\in\B}w_{j;\tea_1}w_{j;\tea_2}w_{k;\tea_3}\le(\sum_{\delta_1\in\D}v^{(\tea_1\tea_2)(\tea_3\delta)} \meanstring_{k;\delta_1}\ri)\, \notag\\
&+\frac{1}{8}\sum_{j,k}\sum_{\tea_1,\ldots,\tea_4\in\B}w_{j;\tea_1}w_{j;\tea_2}\ w_{k;\tea_3}w_{k;\tea_4}\le(v^{(\tea_1\tea_2)(\tea_3\tea_4)}\ri)\, .
\label{eq:posterior-quartic}
\end{align}

\index{finite-width prediction!Bayesian inference}
Given all these expressions, we can finally determine the true posterior mean\index{posterior!posterior mean} by computing the following expectation:
\begin{align}\notag
\int dz^{(L)}_{\B} p\!\le(z_{\B}^{(L)} \Big\vert y_{\A} \ri)  \z{i}{\tea}{L}=& \int dz^{(L)}_{\B} \frac{p\!\le(y_{\A},z_{\B}^{(L)} \ri)}{p\big(y_{\A} \big)} \z{i}{\tea}{L}\\
=&\posmean_{i;\tea}+\int dw_{\B}\, \frac{p\!\le(y_{\A},\ \posmean_{\B}+w_{\B} \ri)}{p(y_{\A} )} w_{i;\tea}\, \notag \\
=&\posmean_{i;\tea}+\frac{ \brabra w_{i;\tea} e^{\mathcal{Q}\le(w\ri)}\ketket_{\gpos}}{\bra\!\bra e^{\mathcal{Q}\le(w\ri)}\ket\!\ket_{\gpos}}\, \notag\\
=&\posmean_{i;\tea}+\frac{ \brabra w_{i;\tea} \le[1+\mathcal{Q}\!\le(w\ri)\ri]\ketket_{\gpos}}{\bra\!\bra 1+\mathcal{Q}\!\le(w\ri)\ket\!\ket_{\gpos}}+\o{v^2}\, \notag\\
=&\posmean_{i;\tea}+\brabra w_{i;\tea}\mathcal{Q}\!\le(w\ri)\ketket_{\gpos}+\o{v^2}\, ,
\label{eq:posterior-mean-at-finite-width}
\end{align}
where on the first line we used Bayes' rule for the posterior~\eqref{eq:BB-NN}, on the second line we inserted our decomposition \eqref{eq:posterior-mean-finite-width-decomposition} in two places, on the third line we separated out the quartic term in order to rewrite the posterior expectation as a Gaussian expectation\index{Gaussian expectation} with respect to the naive posterior covariance\index{posterior!posterior covariance} $\gpos$ divided by the distribution's normalization,
on the fourth line we expanded the exponential, and on the final line we used the fact that the fluctuation has zero mean $\brabra w_{i;\tea} \ketket_{\gpos}=0$ in Gaussian expectation\index{Gaussian expectation}. We can now evaluate the remaining Gaussian expectation\index{Gaussian expectation} by plugging in our expression for the quartic term~\eqref{eq:posterior-quartic} and making Wick contractions\index{Wick contraction}:
\begin{align}\label{eq:mean-posterior-prediction-by-exact-Bayesian-at-finite-width}
&\posmean_{i;\tea}+\brabra w_{i;\tea}\mathcal{Q}\le(w\ri)\ketket_{\gpos}\, \\
=&\posmean_{i;\tea}+\frac{1}{2}\sum_{\tea_1\in\B}\gpos_{\tea \tea_1}\le(\sum_{k}\sum_{\delta_1,\delta_2,\delta_3\in\D}v^{(\tea_1\delta_1)(\delta_2\delta_3)} \meanstring_{i;\delta_1}\meanstring_{k;\delta_2}\meanstring_{k;\delta_3}\ri)\, \notag\\
&+\frac{1}{2}\sum_{\tea_1,\tea_2,\tea_3\in\B}\le(n_{L}\gpos_{\tea_1 \tea_2}\gpos_{\tea \tea_3}+2\gpos_{\tea \tea_1}\gpos_{\tea_2 \tea_3} \ri)\le(\sum_{\delta_1\in\D}v^{(\tea_1\tea_2)(\tea_3\delta_1)} \meanstring_{i;\delta_1}\ri)\, .\notag
\end{align}
Thus, we see that the naive posterior mean \eqref{eq:naive-mean} is further corrected by a number of $v$-dependent terms.

To extract some \terminate{physics} from this complicated expression, note from the definition of $\meanstring$~\eqref{eq:posmean-definition} that the $i$-th component of $\meanstring_{i;\delta}$ depends on
the $i$-th component of our observation $\y{i}{\tra}$. This in particular means that the term above  $\propto \sum_{k}\meanstring_{i;\delta_1}\meanstring_{k;\delta_2}\meanstring_{k;\delta_3}$ does incorporate information from all of the components of the observed true outputs. In other words, information from the $k$-th component of the observed outputs successfully influences the posterior mean\index{posterior!posterior mean} prediction on the $i$-th component for $i\ne k$. This means that at finite width we have a dependence among the components of the posterior outputs
\be\label{eq:stat-dependence-as-good-posterior}
p\!\le(z_{i; \B}^{(L)}, z_{k; \B}^{(L)} \Big\vert y_{i;\A}, y_{k;\A}\ri) \neq p\!\le(z_{i; \B}^{(L)} \Big\vert y_{i;\A}\ri)\, p\!\le(z_{k; \B}^{(L)} \Big\vert y_{k;\A}\ri)\, .
\ee
This property of the posterior distribution\index{posterior} descends from the nontrivial \emph{fire-together} \terminate{inductive bias} $p\Big(z_i^{(L)}\Big\vert z_k^{(L)}\Big)$ present  in the finite-width prior as discussed in~\S\ref{subsec:Hebbian}. The dependence among the components of the posterior outputs~\eqref{eq:stat-dependence-as-good-posterior} is a signature of our posterior beliefs' learning to \emph{wire together}, and we will see a further manifestation of this when we again consider \terminate{representation learning} in the next section.

Before we move on, we should address practical matters.
Practically speaking, it is even more computationally infeasible 
to evaluate the finite-width predictions of Bayesian learning\index{Bayesian inference!practicalities}~\eqref{eq:posterior-mean-at-finite-width} than it was at infinite width. In particular, evaluating the quartic coupling\index{coupling!quartic} involves first \emph{representing} the \terminate{four-point vertex} -- a $\NR\times\NR\times\NR\times\NR$-dimensional tensor -- and then multiply contracting it with inverse kernels. Thus, both the cost of computation and the memory requirements of Bayesian learning grow terrifyingly quickly with 
our observations, i.e.~with size of our dataset $\A$. However, please \emph{Don't Panic}\index{Don't Panic, HHGTTG}: we are getting ever closer to the point where we can show you how gradient-based learning resolves all these practical difficulties at finite width.




\subsection{Presence of Representation Learning}\label{subsec:presence-RL-Bayes}
The fact that the individual components of the finite-width posterior mean\index{posterior!posterior mean} prediction can incorporate information from our observations of the other components is suggestive of the idea that these observations might also be used to build up representations\index{representation} in the hidden layers. Here we will show that such \neo{representation learning} 
actually does occur at finite width as a direct consequence of the nonzero \emph{interlayer} interaction\index{interactions}s. %


Analogous to our parallel subsection  at infinite width (\S\ref{subsec:absence-RL-Bayes}),  we can investigate representation learning by considering the posterior distribution in the penultimate layer $\ell = L-1$ on the full set of samples $\D$, given observations $y_\A$. In particular, to show how the \emph{features}\index{feature} of the penultimate-layer representation evolve, our goal will be to compute the change in the expectation of a penultimate-layer observable $\O\!\le(z^{(L-1)}_{\D}\ri)$ taken with respect to the posterior as compared to the expectation taken with respect to the prior
\be\label{eq:posterior-minus-prior}
\dO\equiv \int dz^{(L-1)}_{\D} p\!\le(z^{(L-1)}_{\D}\Big\vert y_\A\ri) \O\!\le(z^{(L-1)}_{\D}\ri)-\int dz^{(L-1)}_{\D} p\!\le(z^{(L-1)}_{\D} \ri) \O\!\le(z^{(L-1)}_{\D}\ri)\, ,
\ee
where $p\!\le(z^{(L-1)}_{\D} \ri)$ and $p\!\le(z^{(L-1)}_{\D}\Big\vert y_\A \ri)$ are the prior and posterior distributions, respectively. This expectation difference was strictly zero in the infinite-width limit 
since the penultimate-layer posterior was exactly equal to the penultimate-layer prior \eqref{eq:infinite-width-penultimate-posterior-equals-prior}. A non-vanishing difference in contrast will mean that the penultimate-layer preactivations are being updated after making  observations $y_{\A}$. Such an \emph{update} is a direct avatar of \terminate{representation learning}.




As before, by Bayes' rule\index{Bayesian probability!Bayes' rule} we can write the posterior distribution of the penultimate preactivations $z_\D^{(L-1)}$ given our observations $y_\A$ as
\be\label{eq:Bayes-posterior-general-reprint}
p\!\le(z_{\D}^{(L-1)}\Big\vert y_\A \ri)=\frac{p\!\le(y_\A \Big\vert z_{\D}^{(L-1)}\ri)p\!\le(z_{\D}^{(L-1)} \ri)}{p\!\le(y_\A \ri)}\, .
\ee
Just as before, the likelihood\index{Bayesian inference!likelihood} $p\!\le(y_\A \Big\vert z_{\D}^{(L-1)} \ri)$ is the conditional distribution $p\!\le(z_\A^{(L)} \Big\vert z_{\D}^{(L-1)} \ri)$ evaluated on our set of observations $z_{\A}^{(L)} \to y_\A$. With this expression for the posterior \eqref{eq:Bayes-posterior-general-reprint}, 
we can express the update $\dO$ after Bayesian learning as
\be\label{eq:different-of-expectations}
\dO = \E{\frac{p\!\le(y_\A \Big\vert z_{\D}^{(L-1)}\ri)}{p\!\le(y_\A \ri)} \O\!\le(z^{(L-1)}_{\D}\ri)}- \E{\O\!\le(z^{(L-1)}_{\D}\ri)}\, .
\ee
As always, the full expectation $\E{\,\Vdot\,}$ is to be evaluated with respect to the \emph{prior} or initialization distribution $p\!\le(z_{\D}^{(L-1)} \ri)$; all learning will always be represented explicitly with the insertion of other factors as we did above.

Let's now determine how this insertion, the likelihood-to-evidence ratio 
\be\label{eq:likelihood-to-evidence-ratio}
\frac{p\!\le(y_\A \Big\vert z_{\D}^{(L-1)} \ri)}{p\!\le(y_\A \ri)}\, ,
\ee 
depends on the preactivations $z^{(L-1)}_{\D}$. As we pointed out when working through the infinite-width example, we already worked out the form of this likelihood in~\eqref{eq:general-layer-conditional} as the conditional distribution between layers. In our current context and notation, the likelihood\index{Bayesian inference!likelihood} reads
\be\label{eq:general-layer-conditional-reprint}
p\!\le(y_\A \Big\vert z_{\D}^{(L-1)} \ri)= \frac{1}{\sqrt{\dete{2\pi \widehat{G}^{(L)}}^{n_{L}}}} \exp\!\le(-\frac{1}{2}\sum_{i=1}^{n_{L}}\sum_{\tra_1,\tra_2\in\A}\TI{\widehat{G}}{\tra_1 \tra_2}{L}\y{i}{\tra_1}\y{i}{\tra_2}\ri)\, ,
\ee
where as a reminder the \emph{stochastic metric} $\Ti{\widehat{G}}{\tra_1 \tra_2}{L}= \Ti{\widehat{G}}{\tra_1 \tra_2}{L}\!\le( z_{\A}^{(L-1)}\ri)$ depends explicitly on the preactivations in the penultimate layer $\z{i}{\A}{L-1}$.\footnote{
    \label{foot:gross-tilde-hat}Strictly speaking, we should really denote the stochastic metric here as $\Ti{\widehat{\widetilde{G}}}{\tra_1 \tra_2}{L}$ to indicate that we're focusing on the $\NR$-by-$\NR$ submatrix of the full stochastic metric on $\D$, $\Ti{\widehat{G}}{\delta_1 \delta_2}{L}$. It's the matrix inverse of this submatrix $\Ti{\widehat{\widetilde{G}}}{\tra_1 \tra_2}{L}$ -- and not the $(\tra_1, \tra_2)$ block of the inverse of the full matrix $\Ti{\widehat{G}}{\delta_1 \delta_2}{L}$ -- that appears in \eqref{eq:general-layer-conditional-reprint}.
    Since  this tilde-with-a-hat looks ridiculous -- and since we are already heavily overburdened on the notational front -- if you promise to keep this caveat in mind, we'll do everyone a favor and temporarily suppress this tilde.
} 
Thus, the stochastic metric acts as a coupling here, inducing interlayer interactions between the $(L-1)$-th-layer preactivations and the observations $y_\A$. As we will see, this endows the updated distribution over $z_{\D}^{(L-1)}$ with a dependence on $y_\A$. 







\index{Schwinger-Dyson equations}
As should be fairly familiar at this point, we can decompose the stochastic metric into a mean and a fluctuation,\index{tensor decomposition!metric mean and fluctuation} 
\be
\Ti{\widehat{G}}{\tra_1 \tra_2}{L}\equiv\Ti{G}{\tra_1 \tra_2}{L}+\Ti{\widehat{\Delta G}}{\tra_1 \tra_2}{L} \, ,
\ee 
in terms of which the likelihood \eqref{eq:general-layer-conditional-reprint} can be Taylor-expanded \`{a} la Schwinger-Dyson as we did before in~\eqref{eq:second-layer-stochastic-exponential} and~\eqref{eq:second-layer-stochastic-determinant}. At first nontrivial order, we find for the likelihood-to-evidence ratio \eqref{eq:likelihood-to-evidence-ratio}
\begin{align}\label{eq:SD-for-likelihood-to-evidence-ratio}
&\frac{p\!\le(y_\A \Big\vert z_{\D}^{(L-1)}\ri)}{p\!\le(y_\A \ri)} \\
=& \frac{1}{p\!\le(y_\A \ri)\sqrt{\dete{2\pi G^{(L)}}^{n_{L}}}} \le[1\!+\frac{1}{2}\!\sum_{\tra_1,\ldots,\tra_4\in\A}\!\!\!\!\!\!\!\!\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}\TI{G}{\tra_1\tra_3}{L}\TI{G}{\tra_2\tra_4}{L}\!\sum_{i=1}^{n_{L}}\!\le(\y{i}{\tra_3}\y{i}{\tra_4}\!\!-\!\Ti{G}{\tra_3\tra_4}{L}\ri)\!+\!\o{\Delta^2}\!\ri]\,\! . \notag
\end{align}
Here, the prefactor 
before the square brackets
is constant with respect to the variables $z^{(L-1)}_{\D}$, and so all of the relevant dependence needed to evaluate update $\dO$~\eqref{eq:different-of-expectations} is contained implicitly in the metric fluctuation $\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}$.
We can thus compute a posterior expectation -- i.e.~the first expectation in \eqref{eq:different-of-expectations} -- of any observable by integrating against the quantity in the square bracket, so long as we also divide by an integral of ``$1$'' against the same quantity in order to properly normalize.
With this by-now familiar trick in mind, we can rewrite the posterior expectation  as
\begin{align}
&\frac{\mathbb{E}\le\{\O\!\le(z^{(L-1)}_{\D}\ri)\!\le[1+\frac{1}{2}\sum_{\tra_1,\ldots,\tra_4}\!\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}\TI{G}{\tra_1\tra_3}{L}\TI{G}{\tra_2\tra_4}{L}\sum_{i}\le(\y{i}{\tra_3}\y{i}{\tra_4}\!-\!\Ti{G}{\tra_3\tra_4}{L}\ri)\!+\!\o{\Delta^2}\ri]\ri\}}{\E{1+\frac{1}{2}\sum_{\tra_1,\ldots,\tra_4}\!\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}\TI{G}{\tra_1\tra_3}{L}\TI{G}{\tra_2\tra_4}{L}\sum_{i}\le(\y{i}{\tra_3}\y{i}{\tra_4}\!-\!\Ti{G}{\tra_3\tra_4}{L}\ri)\!+\!\o{\Delta^2}}}\, \\
=&\E{\O\!\le(z^{(L-1)}_{\D}\ri)}\, \notag\\
&+\frac{1}{2}\sum_{\tra_1,\ldots,\tra_4\in\A}\E{\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}\O\!\le(z^{(L-1)}_{\D}\ri)}\TI{G}{\tra_1\tra_3}{L}\TI{G}{\tra_2\tra_4}{L}\sum_{i}\le(\y{i}{\tra_3}\y{i}{\tra_4}\!-\!\Ti{G}{\tra_3\tra_4}{L}\ri)+\o{\frac{1}{n^2}}\, ,\notag
\end{align}
where the details of what we actually did are hidden in this here footnote.\footnote{The reason that we treated the additional $\o{\Delta^2}$ pieces as $\o{1/n^2}$ is hidden under the rug in the main body. To peak under that rug, first let us schematically express the likelihood-to-evidence ratio~\eqref{eq:SD-for-likelihood-to-evidence-ratio} as $\text{constant}\times \le[1+\sharp_1\Delta G+\sharp_2 (\Delta G)^2+\o{\Delta^3}\ri]$. Then, the posterior expectation becomes
\begin{align}
&\frac{\mathbb{E}\le\{\O \le[1+\sharp_1\Delta G+\sharp_2 (\Delta G)^2+\o{\Delta^3}\ri]\ri\}}{\E{1+\sharp_1\Delta G+\sharp_2 (\Delta G)^2+\o{\Delta^3}}}=\frac{\E{\O}+\sharp_1\E{\O\Delta G}+\sharp_2\E{\le(\Delta G\ri)^2\O}+\o{1/n^2}}{1+\sharp_2 \E{(\Delta G)^2}+\o{1/n^2}}\, \\
=&\E{\O}+\sharp_1\E{\O \Delta G}+\sharp_2 \le\{\E{\le(\Delta G\ri)^2\O}-\E{\le(\Delta G\ri)^2}\E{\O}\ri\}+\o{1/n^2}\, .\notag
\end{align}
Decomposing the observable into a mean and a fluctuation as $\O=\E{\O}+\Delta \O$, we see that the term proportional to the coefficient $\sharp_2$ is $\E{\o{\Delta^3}}=\o{1/n^2}$ and thus can be neglected, while the leading finite-width correction cannot be neglected: $\sharp_1\E{\O \Delta G}=\sharp_1\E{\Delta\O \Delta G}=\o{1/n}$.
} We see that the first term is just the prior expectation, while the second term expresses the update $\dO$ \eqref{eq:posterior-minus-prior}.
Finally, 
taking only the leading finite-width corrections at order $1/n$ and 
restoring the tildes to correctly represent the submatrices on $\A$ alone, we can write down a very general expression for the update to any penultimate-layer observable at leading nontrivial order in $1/n$:
\be\label{eq:update-bayes-penultimate-formula}
\dO = \frac{1}{2}\sum_{\tra_1,\ldots,\tra_4\in\A}\E{\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}\O\!\le(z^{(L-1)}_{\D}\ri)} \TI{\kersub}{\tra_1\tra_3}{L}\TI{\kersub}{\tra_2\tra_4}{L}\sum_{i}^{n_L}\le(\y{i}{\tra_3}\y{i}{\tra_4}\!-\!\Ti{\kersub}{\tra_3\tra_4}{L}\ri)\, . %
\ee
Again, please be careful and remember that the $\E{\,\Vdot\,}$ in \eqref{eq:update-bayes-penultimate-formula} is to be evaluated with respect to the prior distribution $p\!\le(z_{\D}^{(L-1)} \ri)$.
Note also that the lone expectation in the update \eqref{eq:update-bayes-penultimate-formula} is just the covariance of the stochastic metric with the observable:
\be
\E{\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}\O\!\le(z^{(L-1)}_{\D}\ri)}=\E{\Ti{\widehat{G}}{\tra_1\tra_2}{L}\O\!\le(z^{(L-1)}_{\D}\ri)}-\E{\Ti{\widehat{G}}{\tra_1\tra_2}{L}}\E{\O\!\le(z^{(L-1)}_{\D}\ri)}\, .
\ee
As we addressed in that rugly footnote, for a general order-one observable this covariance is $1/n$-suppressed but nonzero. Thus, we see that at large-but-finite width $(1 \ll n < \infty)$, such observables get updated: representations are learned.\index{representation learning}






In order to see how this works, let's consider a concrete example. The simplest observable turns out to be the average norm of the activations
\be
\O\!\le(z^{(L-1)}_{\D}\ri) \equiv \frac{1}{n_{L-1}}\sum_{j=1}^{n_{L-1}}\s{j}{\delta_1}{L-1}\s{j}{\delta_2}{L-1} \, ,
\ee
which we can decompose in terms of a mean and a fluctuation as
\be
\O\!\le(z^{(L-1)}_{\D}\ri)  = \E{\O\!\le(z^{(L-1)}_{\D}\ri)}+\frac{1}{\CW{L}}\Ti{\widehat{\Delta G}}{\delta_1\delta_2}{L}\, ,
\ee
if we also recall the explicit form of the metric fluctuation~\eqref{eq:metric-fluctuation-general-layer}
\be
\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}=\CW{L}\frac{1}{n_{L-1}}\sum_{j=1}^{n_{L-1}}\le(\s{j}{\tra_1}{L-1}\s{j}{\tra_2}{L-1}-\E{\s{j}{\tra_1}{L-1}\s{j}{\tra_2}{L-1}}\ri)\, .
\ee
Then, plugging into our expression for the leading-order finite-width update \eqref{eq:update-bayes-penultimate-formula}, we find
\begin{align}\notag
\dO=&\frac{1}{2 \CW{L}  }\sum_{\tra_1,\ldots,\tra_4\in\A}\E{\Ti{\widehat{\Delta G}}{\delta_1\delta_2}{L}\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}  } \TI{\kersub}{\tra_1\tra_3}{L}\TI{\kersub}{\tra_2\tra_4}{L}\sum_{i}\le(\y{i}{\tra_3}\y{i}{\tra_4}\!-\!\Ti{\kersub}{\tra_3\tra_4}{L}\ri)\, \\
=& \frac{1}{2n_{L-1}\CW{L}}\sum_{\tra_1,\ldots,\tra_4\in\A}V^{(L)}_{(\delta_1\delta_2)(\tra_1\tra_2)}\TI{\kersub}{\tra_1\tra_3}{L}\TI{\kersub}{\tra_2\tra_4}{L}\sum_{i}\le(\y{i}{\tra_3}\y{i}{\tra_4}\!-\!\Ti{\kersub}{\tra_3\tra_4}{L}\ri)\, ,
\end{align}
where to go to the second line we used the definition of the four-point vertex in terms of the two-point function of the metric fluctuation \eqref{eq:vertex-in-terms-of-metric-fluctuation}. As this vertex characterizes the non-Gaussianity of the output distribution, we see explicitly here how interactions are mediating updates to the penultimate-layer activations. In addition, the leading factor of $1/n_{L-1}$ makes it clear that this update is a finite-width effect. Further,  the term in the last parenthesis shows that the update depends explicitly on the difference between our observations of the outputs, $\y{i}{\tra_3}\y{i}{\tra_4}$, and our prior expectations of them, $\E{z^{(L)}_{i;\tra_3} z^{(L)}_{i;\tra_4} } \equiv \Ti{\kersub}{\tra_3\tra_4}{L} + \o{1/n}$. This means that the observations are in fact propagating \emph{backward} to induce changes in the hidden-layer representations.\footnote{This kind of backward-propagation or \neo{backpropagation}, if you will, persists further into the shallower hidden layers as well.
However, in the $(L-2)$-th layer, the posterior update turns out to be of order $\o{1/n^2}$. Intuitively this makes sense because the change in the representation in the penultimate layer $(L-1)$ is already down by a factor of $1/n$, and it gets further suppressed due to the $1/n$-suppression of the interlayer interaction in going back to the $(L-2)$-th layer.

Mathematically, we can consider 
the update to 
an $(L-2)$-th-layer observable $\O\!\le(z^{(L-2)}_{\D}\ri)$ 
as
\be\label{eq:posterior-minus-prior-two-layers-back}
\dO\equiv \int dz^{(L-2)}_{\D} p\!\le(z^{(L-2)}_{\D}\Big\vert y_\A\ri) \O\!\le(z^{(L-2)}_{\D}\ri)-\int dz^{(L-2)}_{\D} p\!\le(z^{(L-2)}_{\D} \ri) \O\!\le(z^{(L-2)}_{\D}\ri)\, .
\ee
Through the chain of Bayes', sum, and product rules, the posterior insertion in this formula is given in terms of the following marginalization:
\be
p\!\le(z^{(L-2)}_{\D}\Big\vert y_\A\ri)=\frac{p\!\le(y_\A\Big\vert z^{(L-2)}_{\D}\ri)p\!\le(z^{(L-2)}_{\D}\ri)}{p\!\le(y_\A \ri)}=\int dz^{(L-1)}_{\D} \frac{p\!\le(y_\A\Big\vert z^{(L-1)}_{\D}\ri)}{p\!\le(y_\A \ri)}p\!\le(z^{(L-1)}_{\D}, z^{(L-2)}_{\D}\ri)\, .
\ee
From here, through the same set of manipulations that led to the update equation for the penultimate layer \eqref{eq:update-bayes-penultimate-formula}, we get
\be\label{eq:update-bayes-two-layers-back-formula}
\dO = \frac{1}{2}\sum_{\tra_1,\ldots,\tra_4\in\A}\E{\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}\!\le(z^{(L-1)}_{\D}\ri)\O\!\le(z^{(L-2)}_{\D}\ri)} \TI{\kersub}{\tra_1\tra_3}{L}\TI{\kersub}{\tra_2\tra_4}{L}\sum_{i}^{n_L}\le(\y{i}{\tra_3}\y{i}{\tra_4}\!-\!\Ti{\kersub}{\tra_3\tra_4}{L}\ri)+\o{\frac{1}{n^2}}\, . %
\ee
Thus, to show that this change is of order $\o{1/n^2}$, we need to show that the \neo{interlayer correlation},
\be\label{eq:interlayer-next-layer}
\E{\Ti{\widehat{\Delta G}}{\tra_1\tra_2}{L}\!\le(z^{(L-1)}_{\D}\ri)\O\!\le(z^{(L-2)}_{\D}\ri)} \, ,
\ee
is of order $\o{1/n^2}$. This is most swiftly carried out in the future, first by the application of the formula~\eqref{eq:no-weight-insertion-general} with $\ell=L-2$ and then with the associated trickery~\eqref{eq:SD-again}. If you are up for a challenge, please flip forward and write a note next to \eqref{eq:SD-again} reminding yourself to come back to footnote~\ref{foot:come-back-to-me} in \S\ref{subsec:presence-RL-Bayes}.\label{foot:come-back-to-me}
\emph{Spoiler alert:}\index{spoiler alert} you should in fact find that~\eqref{eq:interlayer-next-layer}
is of order $\o{1/n^2}$.
}

Although perhaps not practically useful, this Bayesian analysis of representation learning at finite width 
will serve as a theoretically useful blueprint for studying a similar type of \terminate{representation learning} that occurs with gradient-based learning at finite width in \S\ref{ch:features}. Now, with all these allusions to gradient-based learning having accrued with interest, you must be really excited to flip the page to the next chapter!
















