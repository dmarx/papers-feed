---
abstract: |
  Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce **SSMBA**, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English. [^1]
author:
- |
  Nathan Ng  
  University of Toronto  
  Vector Institute Kyunghyun Cho  
  New York University Marzyeh Ghassemi  
  University of Toronto  
  Vector Institute  
bibliography:
- anthology.bib
- emnlp2020.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness"
---





# Introduction

# Background and Related Work

# SSMBA: Self-Supervised Manifold Based Augmentation

# Datasets

# Experimental Setup

# Results

# Analysis and Discussion

# Conclusion

# Acknowledgements

Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute <a href="www.vectorinstitute.ai/#partners" class="uri">www.vectorinstitute.ai/#partners</a>. This work was partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Research (Improving Deep Learning using Latent Structure). We thank Julian McAuley, Vishaal Prasad, Taylor Killian, Victoria Cheng, and Aparna Balagopalan for helpful comments and discussion.

[^1]: Code is availble at <https://github.com/nng555/ssmba>
