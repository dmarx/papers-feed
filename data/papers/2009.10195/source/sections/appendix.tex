\section{Datasets}
\label{app:data}
Full dataset statistics and details are provided in table \ref{tab:data_summ_big}.
All data splits for all tasks can be downloaded at \url{https://nyu.box.com/s/henvmy17tkyr6npl7e1ltw8j46baxsml}. 

\begin{table*}[t]
\small
\centering
\begin{tabular}{llcccccc}
\toprule
Dataset & Domain & Reference & $c$ & $l$ & Train & Valid & Test \\
\midrule
\multirow{4}{*}{AR-Clothing}
& Men & \citealt{jianmo}  & 5 & 31 & 25k$^\dagger$ & 2k & 2k\\
& Women & \citealt{jianmo}  & 5 & 40 & 25k$^\dagger$ & 2k & 2k\\
& Baby & \citealt{jianmo} & 5 & 29 & 25k$^\dagger$ & 2k & 2k\\
& Shoes & \citealt{jianmo} & 5 & 41 & 25k$^\dagger$ & 2k & 2k\\
\midrule
\multirow{10}{*}{AR-Full}
& Books & \citealt{jianmo} & 5 & 101 & 25k$^\dagger$ & 2k & 2k\\
& Clothing, Shoes \& Jewelry & \citealt{jianmo} & 5 & 39 & 25k$^\dagger$ & 2k & 2k\\
& Home and Kitchen & \citealt{jianmo} & 5 & 53 & 25k$^\dagger$ & 2k & 2k\\
& Kindle Store & \citealt{jianmo} & 5 & 104 & 25k$^\dagger$ & 2k & 2k\\
& Movies \& TV & \citealt{jianmo} & 5 & 83 & 25k$^\dagger$ & 2k & 2k\\
& Pet Supplies & \citealt{jianmo} & 5 & 57 & 25k$^\dagger$ & 2k & 2k\\
& Sports \& Outdoors & \citealt{jianmo} & 5 & 55 & 25k$^\dagger$ & 2k & 2k\\
& Electronics & \citealt{jianmo} & 5 & 73 & 25k$^\dagger$ & 2k & 2k\\
& Tools \& Home Improvement & \citealt{jianmo} & 5 & 57 & 25k$^\dagger$ & 2k & 2k\\
& Toys \& Games & \citealt{jianmo} & 5 & 50 & 25k$^\dagger$ & 2k & 2k\\
\midrule
\multirow{4}{*}{Yelp} 
& American & \citealt{yelp} & 5 & 138 & 25k$^\dagger$ & 2k & 2k \\
& Chinese & \citealt{yelp} & 5 & 135 & 25k$^\dagger$ & 2k & 2k\\
& Italian & \citealt{yelp} & 5 & 139 & 25k$^\dagger$ & 2k & 2k\\
& Japanese & \citealt{yelp} & 5 & 138 & 25k$^\dagger$ & 2k & 2k\\
\midrule
\multirow{10}{*}{MNLI}
& Slate & \citealt{williams2018broad} & 3 & 35 & 75k & 2k & 2k \\
& Fiction & \citealt{williams2018broad} & 3 & 25 & 73k & 2k & 2k \\
& Telephone & \citealt{williams2018broad} & 3 & 37 & 81k & 2k & 2k \\
& Travel & \citealt{williams2018broad} & 3 & 42 & 75k& 2k & 2k  \\
& Government & \citealt{williams2018broad} & 3 & 39 & 75k & 2k & 2k  \\
& Verbatim & \citealt{williams2018broad} & 3 & 43 & - & 1k & 1k \\
& Face-to-Face & \citealt{williams2018broad} & 3 & 29 & - & 1k & 1k \\
& OUP & \citealt{williams2018broad} & 3 & 41 & - & 1k & 1k \\
& 9/11 & \citealt{williams2018broad} & 3 & 36 & - & 1k & 1k \\
& Letters & \citealt{williams2018broad} & 3 & 34 & - & 1k & 1k \\
\midrule
\multirow{2}{*}{Movies}
& SST2 & \citealt{socher2013recursive} & 2 & 11 & 66k & 1k & 1k\\
& IMDb & \citealt{maas2011learning} & 2 & 296 & 46k & 2k & 2k\\
\midrule
\multirow{3}{*}{ANLI}
& R1 & \citealt{nie2019adversarial} & 3 & 92 & 17k & 1k & 1k \\
& R2 & \citealt{nie2019adversarial} & 3 & 90 & 46k & 1k & 1k \\
& R3 & \citealt{nie2019adversarial} & 3 & 82 & 100k & 1k & 1k \\
\midrule
IWSLT & IWSLT & \citealt{cettolo2014proceedings} & - & 24 & 160k & 7k & 7k \\
\midrule
\multirow{5}{*}{OPUS}
& Medical & \citealt{TIEDEMANN12.463} & - & 13 & 1.1m & 2k & 2k\\
& IT & \citealt{TIEDEMANN12.463} & - & 14 & - & 2k & 2k\\
& Koran & \citealt{TIEDEMANN12.463} & - & 23 & - & 2k & 2k\\
& Law & \citealt{TIEDEMANN12.463} & - & 31 & - & 2k & 2k\\
& Subtitles & \citealt{TIEDEMANN12.463} & - & 10 & - & 2k & 2k\\
\midrule
\multirow{2}{*}{de$\to$rm}
& Law & \citealt{scherrer-cartoni-2012-trilingual} & - & 22 & 101k & 2k & 2k \\
& Blogs & \citealt{Muller2019DomainRI} & - & 24 & - & 2k & 2k \\
\bottomrule
\end{tabular}
\caption{Summary statistics for datasets. For detailed information, see references. $n$: number of domains. $c$: number of target classes. $l$: average training example length, or average test example length, for datasets without training sets. Training sets marked with a $\dagger$ are sampled randomly from a larger dataset.}
\label{tab:data_summ_big}
\end{table*}

\section{Data Preprocessing}
\label{app:preprocess}
We use the same preprocessing steps across all sentiment analysis and NLI experiments.
All data is first tokenized using a GPT-2 style tokenizer and BPE vocabulary provided by \texttt{fairseq} \citep{ott2019fairseq}.
This BPE vocabulary consists of 50263 types. 
Corresponding labels are encoded using a label dictionary consisting of as many types as there are classes.
Input text and labels are then binarized for model training. 
Although all models share the same vocabulary, we randomly initialize each model's embeddings and train the entire model end-to-end.
For machine translation experiments, we follow \citealt{Muller2019DomainRI} and learn a 16k BPE on OPUS and a 32k BPE on de$\to$rm. 
On IWSLT14 we learn a 10k BPE.
We use a separate vocabulary for the source and target side.

\section{Model Architecture and Training Hyperparameters}
\label{app:hyperparams}


All models are written and trained within the \texttt{fairseq} framework \citep{ott2019fairseq} with T4 GPUs.
LSTM and CNN models were trained on a single GPU, RoBERTa models were trained with 4 GPUs, and tranfsormer models were trained with 2 GPUs. 
On average, when trained on augmented data, LSTM and CNN models took an hour to train to convergence, RoBERTa models took 12 hours to train to convergence, and transformer models took 24 hours to train to convergence. 
Models trained on unaugmented data took roughly 20\% of the time of models trained on augmented data to reach convergence. 
For each model we investigate, we present first the model architecture and then the training hyperparameters.

\subsection{LSTM}
\label{subapp:lstms}
Our LSTM models are a single layer of 512 nodes. 
Input embeddings are 512 dimensions.
The output embedding from the last time step is fed into a MLP classifier with a single hidden layer of 512 dimensions.
Models contain 28M parameters.
Dropout of 0.3 is applied to the input and output of our encoder, and dropout of 0.1 is applied to the MLP classifier.

We train with Adam optimizer \citep{kingma2014method} with $\beta = (0.9, 0.98)$ and $\epsilon =$ \num{1e-6}.
Our learning rate is set to \num{1e-4} and is first warmed up for 2 epochs before it is decayed using an inverse square root scheduler.

\subsection{CNN}
\label{subapp:cnn}
Our CNN models are based on the architecture in \cite{kim2014convolutional}. 
As in our LSTM models, our input embeddings are 512 dimensional, which we treat as our channel dimension.
We apply three convolutions of kernel size 3, 4, and 5, with 256 output channels. 
Models contain 27M parameters.
Convolutional outputs are max-pooled over time then concatenated to a 768-dimensional encoded representation.
Again, we feed this representation into a MLP classifier with a single hidden layer of 512 dimensions.
We apply dropout of 0.2 to our inputs and MLP classifier.

We train with Adam optimizer \citep{kingma2014method} with $\beta = (0.9, 0.98)$ and $\epsilon =$ \num{1e-6}.
Our learning rate is set to \num{1e-3} and is first warmed up for 2 epochs before it is decayed using an inverse square root scheduler.

\subsection{RoBERTa}
\label{subapp:roberta}
Our RoBERTa models use a pre-trained RoBERTa\textsubscript{BASE} model provided by \texttt{fairseq}. 
As in other models, classification token embeddings are fed into an MLP classifier with a single hidden layer of 512 dimensions.
Models contain 125M parameters.
We follow the MNLI fine-tuning procedures in \texttt{fairseq}, training with learning rate \num{1e-5} with Adam optimizer \citep{kingma2014method} with $\beta = (0.9, 0.98)$ and $\epsilon =$ \num{1e-6}.
We warmup the learning rate for 2 epochs before decaying with an inverse square root scheduler.

\subsection{Transformer}
\label{subapp:transformer}
Transformer models are trained with label-smoothed cross-entropy and label smoothing $0.1$. 
Due to the dataset sizes, we use a slightly smaller transformer architecture with embedding dimension 512, feed forward embedding dimension 1024, 4 encoder heads, and 6 encoder and decoder layers.
Models contain 52M parameters.
We also apply dropout of 0.3 and weight decay of 0.0001.
All other hyperparameters follow the base architecture in \citealt{vaswani2017attention}.

As in other models, we train with Adam optimizer \citep{kingma2014method} with $\beta = (0.9, 0.98)$ and $\epsilon =$ \num{1e-6}.
Our learning rate is set to \num{5e-4} and is first warmed up for 4000 updates before it is decayed using an inverse square root scheduler.

\section{\ssmba\ Hyperparameters}
\label{app:ssmba}
\ssmba\ hyperparameters for each dataset and domain are provided in table \ref{tab:ssmba_hyperparams}. 
Hyperparameters are chosen based on in-domain validation performance. A detailed analysis of hyperparameter tuning is provided in section \ref{sec:discussion}.

\begin{table*}[t]
\small
\centering
\begin{tabular}{llccccc}
\toprule
Dataset & Domain & Model & Corruption \% & Sampling Method & Labelling Method & \# Generated\\
\midrule
\multirow{2}{*}{AR-Clothing}
& * & RNN & 40\% & Unrestricted Sampling & Preserve Label & 5\\
& * & CNN & 40\% & Unrestricted Sampling & Soft Label & 5\\
\midrule
\multirow{2}{*}{AR-Full}
& * & RNN & 50\% & Unrestricted Sampling & Preserve Label & 5\\
& * & CNN & 40\% & Unrestricted Sampling & Soft Label & 5\\
\midrule
\multirow{2}{*}{Yelp}
& * & RNN & 60\% & Unrestricted Sampling & Preserve Label & 5\\
& * & CNN & 40\% & Unrestricted Sampling & Soft Label & 5\\
\multirow{4}{*}{Movies}
& SST2 & RNN & 10\% & Unrestricted Sampling & Soft Label & 5\\
& IMDb & RNN & 20\% & Unrestricted Sampling & Preserve Label & 5\\
& SST2 & CNN & 60\% & Unrestricted Sampling & Hard Label & 5\\
& IMDb & CNN & 30\% & Unrestricted Sampling & Soft Label & 5\\
\midrule
MNLI & * & RoBERTa & 10\% & Unrestricted Sampling & Soft Label & 5\\
\midrule
\multirow{3}{*}{ANLI}
& R1 & RoBERTa & 5\% & Unrestricted Sampling & Preserve Label & 5\\
& R2 & RoBERTa & 5\% & Unrestricted Sampling & Hard Label & 5\\
& R3 & RoBERTa & 10\% & Unrestricted Sampling & Hard Label & 5\\
\midrule
IWSLT & IWSLT & Transformer & 10\% & Unrestricted Sampling & Beam 5 & 5\\
\midrule
OPUS & Medical & Transformer & 15\% & Unrestricted Sampling & Beam 5 & 5\\
\midrule
de$\to$rm & Law & Transformer & 15\% & Unrestricted Sampling & Beam 5 & 5\\
\bottomrule
\end{tabular}
\caption{\ssmba\ hyperparameters used to generate augmented data for each dataset and domain. Hyperparameters were selected by in-domain validation performance. A * in the domain indicates that hyperparameters are the same for all domains in that dataset.}
\label{tab:ssmba_hyperparams}
\end{table*}

\section{Statistical Testing}
\label{app:stats}
For the statistical tests on sentiment analysis and NLI tasks, we use a Wilcoxon ranked-sum test. 
Specifically, we compare averages of model performances on pairs of training and test domains.
For example, in a dataset with 3 domains, D1, D2, and D3, we have 3 in-domain train-test pairs (D1-D1, D2-D2, D3-D3), and 6 out-of-domain train-test pairs (D1-D2, D1-D3, D2-D1, D2-D3, D3-D1, D3-D2).
We calculate the average performance for each model on each pair, then compare the matched in-domain and out-of-domain pairs. 
Since the number of samples we can compare depends on the total number of domains in the dataset, a larger number of datasets gives us a better sense of our statistical significance.

For the statistical tests on machine translation tasks, we use a paired bootstrap resampling approach \citep{koehn-2004-statistical}. 
Since the test works only on a single system's output, we run the test on every pairing of seeds and test domains for the two comparison models.
We report the significance level only if all tests result in a small enough probability.