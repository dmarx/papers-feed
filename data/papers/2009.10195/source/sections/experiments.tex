\subsection{Model Types}
%For each task we select a set of models to investigate.
For sentiment analysis tasks, we investigate LSTMs \citep{hochreiter1997long} and convolutional neural networks (CNNs).
For NLI tasks, we investigate fine-tuned RoBERTa\textsubscript{BASE} models \citep{liu2019roberta}, which are pretrained bidirectional transformers \citep{vaswani2017attention}.
On both tasks, representations from the encoder are fed into an feed-forward neural network for classification.
%We follow the fine-tuning procedure for NLI tasks detailed in \texttt{fairseq} \citep{ott2019fairseq}.
For MT tasks, we train transformers \citep{vaswani2017attention}.
For all models, word embeddings are initialized randomly and trained end-to-end with the model.
We do not initialize with pre-trained word embeddings to maintain consistency across all models and tasks.
%Data is preprocessed following \citep{Muller2019DomainRI}, \citep{wang2020exposure}, and \citep{edunov-etal-2018-classical}.
Model hyperparameters are tuned to maximize performance on in-domain validation data. 
Training details and hyperparameters for all models are provided in Appendix \ref{app:hyperparams}.

\subsection{\ssmba\ Settings}
For all experiments we use the MLM corruption function as our corruption function $q$.
We tune tune the total percentage of tokens corrupted, leaving the percentages of specific corruption operations (80\% masked, 10\% random, 10\% unmasked) the same.
For sentiment analysis and NLI experiments we use a pre-trained RoBERTa\textsubscript{BASE} model as our reconstruction function $r$, and for translation experiments we use a pre-trained German BERT model \citep{chan2020deepset}.
For each input example, we generate 5 augmented examples using unrestricted sampling.
For translation experiments, target side translations are generated with beam search with width 5.
\ssmba\ hyperparameters, including augmented example labelling method and corruption percentage, are chosen based on in-domain validation performance.
%For homogenous domain datasets, we use the same set of hyperparameters across domains, but for nonhomogenous domain datasets (Movies, ANLI), we select a set of hyperparameters for each individual domain.
Hyperparameters for each dataset are provided in Appendix \ref{app:ssmba}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[t!]
\small
\centering
\begin{tabular}{llcccccccccc}
\toprule
& & \multicolumn{2}{c}{\textbf{AR-Full}} & \multicolumn{2}{c}{\textbf{AR-Clothing}} & \multicolumn{2}{c}{\textbf{Movies}} & \multicolumn{2}{c}{\textbf{Yelp}} & \multicolumn{2}{c}{\textbf{Average}}\\
\cmidrule(lr){3-4}
\cmidrule(lr){5-6}
\cmidrule(lr){7-8}
\cmidrule(lr){9-10}
\cmidrule(lr){11-12}
Model & Augmentation & ID & OOD & ID & OOD & ID & OOD & ID & OOD & ID & OOD\\
\midrule
\multirow{5}{*}{RNN}
& None & 69.46 & 66.32 & 69.25 & 67.80 & 90.74 & 71.94 & 62.51 & 61.28 & 70.16 & 66.17 \\
& EDA & 67.32 & 64.47 & 66.87 & 65.21 & 88.43 & 68.3 & 58.39 & 57.19 & 67.56 & 63.55 \\
& CBERT & 69.94 & 66.77 & 69.56 & 68.10 & \textbf{91.01} & 72.11 & 63.17 & 61.75 & 70.17 & 66.57 \\
& UDA & 69.92 & 66.97 & 69.98 & 68.24 & 90.05 & 69.73 & 63.40 & 62.13 & 70.64 & 66.53 \\
\cmidrule{2-12}
& \ssmba & \textbf{70.38\rlap{$^{*\dagger}$} } & \textbf{67.41\rlap{$^{*\dagger}$}} & \textbf{70.19} & \textbf{68.60\rlap{$^{*\dagger}$}} & 89.61 & \textbf{73.20} & \textbf{63.85} & \textbf{62.83\rlap{$^{*\dagger}$}} & \textbf{70.96} & \textbf{67.31} \\
\midrule
\multirow{5}{*}{CNN}
& None & 70.67 & 67.64 & 70.14 & 68.52 & 92.92 & 72.11 & 65.13 & 64.46 & 71.68 & 67.63 \\
& EDA & 68.52 & 66.03 & 67.76 & 66.17 & 91.22 & 74.20 & 60.99 & 59.88 & 69.13 & 65.65 \\
& CBERT & 70.62 & 67.70 & 70.13 & 68.23 & 92.92 & 71.56 & 65.09 & 64.19 & 71.65 & 67.49 \\
& UDA & 70.80 & 68.06 & 70.29 & 68.70 & 92.63 & 72.55 & 65.22 & 64.32 & 71.77 & 67.89 \\
\cmidrule{2-12}
& \ssmba & \textbf{71.10\rlap{$^*$} } & \textbf{68.18\rlap{$^*$} } & \textbf{70.74} & \textbf{69.04\rlap{$^*$} } & \textbf{92.93} & \textbf{74.67} & \textbf{65.59} & \textbf{64.81\rlap{$^{*\dagger}$}} & \textbf{72.11} & \textbf{68.33} \\
\bottomrule
\end{tabular}
\caption{Average in-domain (ID) and out-of-domain (OOD) accuracy (\%) for models trained on sentiment analysis datasets. Average performance across datasets is weighted by number of domains contained in each dataset. Accuracies marked with a $*$ and $\dagger$ are statistically significantly higher than unaugmented models and the next best model respectively, both with $p<0.01$.}
\label{tab:sent_results}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Baselines}
On sentiment analysis and NLI tasks, we compare against 3 data augmentation methods.
%To compare with heuristic based methods, we use 
Easy Data Augmentation (EDA) \citep{wei-zou-2019-eda} is a heuristic method that randomly replaces synonyms and inserts, swaps, and deletes words.
Conditional Bert Contextual Augmentation (CBERT) \citep{wu2019conditional} finetunes a class-conditional BERT model and uses it to generate sentences in a process similar to our own.
Unsupervised Data Augmentation (UDA) \citep{xie2020unsupervised} translates data to and from a pivot language to generate paraphrases. We adapt UDA for supervised classification tasks by training directly on the backtranslated data.

On translation tasks, we compare only against methods which do not require additional target side monolingual data.
Word dropout \citep{sennrich-etal-2016-edinburgh} randomly chooses words in the source sentence to set to zero embeddings.
Reward Augmented Maximum Likelihood (RAML) \citep{norouzi2016reward} samples noisy target sentences based on an exponential of their Hamming distance from the original sentence.
SwitchOut \citep{wang-etal-2018-switchout} applies a noise function similar to RAML to both the source and target side.
%TDA \citep{fadaee2017data} trains a language model on the source and target languages to generate word substitutions. 
We use publicly available implementations for all methods. % and follow best practices as described in each paper.


\subsection{Evaluation Method}
%To evaluate \ssmba's performance on our specified set of tasks and models, 
We train LSTM and CNN models with 10 random seeds, RoBERTa models with 5 random seeds, and transformer models with 3 random seeds.
Models are trained separately on each domain then evaluated on all domains, and performance is averaged across seeds and test domains.
We report the average in-domain (ID) and OOD performance across all train domains.
On sentiment analysis and NLI tasks we report accuracy, and on translation we report uncased tokenized BLEU \citep{papineni2002bleu} for IWSLT and cased, detokenized BLEU  with SacreBLEU\footnote{Signature: BLEU+c.mixed+\#1+s.exp+tok.13a+v.1.4.3} \citep{post-2018-call} for all others.
Statistical testing details are in Appendix \ref{app:stats}.