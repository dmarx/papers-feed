To empirically evaluate our proposed algorithm, we select 9 datasets -- 4 sentiment analysis datasets, 2 natural language inference (NLI) datasets, and 3 machine translation (MT) datasets.
Table \ref{tab:data_summ_small} and Appendix \ref{app:data} provide dataset summary statistics.
All datasets either contain metadata that can be used to split the samples into separate domains or similar datasets that are treated as separate domains. 
%By splitting our selected datasets, we can train on a specific data domain, then test on the remaining domains to simulate domain shift.

\subsection{Sentiment Analysis}        
The Amazon Review Dataset \citep{jianmo} contains product reviews from Amazon. 
Following \citealt{hendrycks2020pretrained}, we form two datasets: 
\textbf{AR-Full} contains reviews from the 10 largest categories, and 
\textbf{AR-Clothing} contains reviews in the clothing category separated into subcategories by metadata. % (Men, Women, Baby, Shoes).
Since the reviews in AR-Clothing come from the same top-level category, the amount of domain shift is much less than that of AR-Full.
Models predict a review's 1 to 5 star rating.
    
SST2 \citep{socher2013recursive} contains movie review excerpts. 
Following \citealt{hendrycks2020pretrained} we pair this dataset with the IMDb dataset \citep{maas2011learning}, which contains full length movie reviews.
We call this pair the \textbf{Movies} dataset.
Models predict a movie review's binary sentiment. 
    
The \textbf{Yelp Review Dataset} contains restaurant reviews with associated business metadata which we preprocess following \citealt{hendrycks2020pretrained}.
Models predict a review's 1 to 5 star rating.

\begin{table}[t]
\small
\centering
\begin{tabular}{llllll}
\toprule
\textbf{Dataset} & \textbf{Domain} & \pmb{$n$} & \pmb{$l$} & \textbf{Train} & \textbf{Test} \\
\toprule
AR-Clothing & * & 4 & 35 & 25k$^\dagger$ & 2k \\
\midrule
AR-Full & * & 10 & 67 & 25k$^\dagger$ & 2k \\
\midrule
Yelp & * &  4 & 138 & 25k$^\dagger$ & 2k \\
\midrule
\multirow{2}{*}{Movies}
& SST2 &-  & 11 & 66k & 1k \\
& IMDb & - & 296 & 46k & 2k \\
\midrule
MNLI & *  & 10 & 36 & 80k & 1k \\
\midrule
\multirow{3}{*}{ANLI}
& R1 & - & 92 & 17k & 1k \\
& R2 & -  & 90 & 46k & 1k \\
& R3 & -  & 82 & 100k & 1k \\
\midrule
IWSLT & - & 1 & 24 & 160k & 7k \\
\midrule
OPUS & Medical & 5 & 15 & 1.1m & 2k \\
\midrule
\multirow{2}{*}{de-rm}
& Law & - & 22 & 100k & 2k\\
& Blogs & - & 25 & - & 2k \\
\bottomrule
\end{tabular}
\caption{Dataset summary statistics. $n$: number of domains. $l$: average tokenized input length. A * in the domain column indicates that the statistics are identical across domains within that dataset. Training sets marked with a $\dagger$ are sampled randomly from a larger dataset. Refer to Appendix \ref{app:data} for more information.}
\label{tab:data_summ_small}
\end{table}

\subsection{Natural Language Inference}
\textbf{MNLI} \citep{williams2018broad} is a corpus of NLI data from 10 distinct genres of written and spoken English.
We train on the 5 genres with training data and test on all 10 genres.
Since the dataset does not include labeled test data, we use the validation set as our test set and sample 2000 examples from each training set for validation.

\textbf{ANLI} \citep{nie2019adversarial} is a corpus of NLI data designed adversarially by humans such that state-of-the-art models fail to classify examples correctly.
The dataset consists of three different levels of difficulty which we treat as separate textual domains.

\subsection{Machine Translation}
Following \citealt{Muller2019DomainRI}, we consider two translation directions, German$\to$English (de$\to$en) and German$\to$Romansh (de$\to$rm). Romansh is a low-resource language with an estimated 40,000 native speakers where OOD robustness is of practical relevance \citep{Muller2019DomainRI}.

In the de$\to$en direction, we use \textbf{IWSLT14 de$\to$en} \citep{cettolo2014proceedings} as a widely-used benchmark to test in-domain performance. 
We also use the \textbf{OPUS} \citep{TIEDEMANN12.463} dataset to test OOD generalization. 
We train on highly specific in-domain data (medical texts) and disparate out-of-domain data (Koran text, Ubuntu localization files, movie subtitles, and legal text).
Since domains share very little similarities in language, generalization to out-of-domain text is extremely difficult.
In the \textbf{de$\to$rm} direction, we use a training set consisting of the Allegra corpus \citep{scherrer-cartoni-2012-trilingual} and Swiss press releases. We use blog posts from Convivenza as a test domain. 