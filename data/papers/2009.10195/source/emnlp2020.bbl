\begin{thebibliography}{63}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Anaby-Tavor et~al.(2020)Anaby-Tavor, Carmeli, Goldbraich, Kantor,
  Kour, Shlomov, Tepper, and Zwerdling}]{lambada}
Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour,
  Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2020.
\newblock Do not have enough data? deep learning to the rescue!
\newblock In \emph{Proceedings of the 2020 AAAI}.

\bibitem[{Bachman et~al.(2014)Bachman, Alsharif, and
  Precup}]{bachman2014learning}
Philip Bachman, Ouais Alsharif, and Doina Precup. 2014.
\newblock Learning with pseudo-ensembles.
\newblock In \emph{NIPS}.

\bibitem[{Bengio et~al.(2013)Bengio, Yao, Alain, and
  Vincent}]{bengio2013generalized}
Yoshua Bengio, Li~Yao, Guillaume Alain, and Pascal Vincent. 2013.
\newblock Generalized denoising auto-encoders as generative models.
\newblock In \emph{Proceedings of the 26th International Conference on Neural
  Information Processing Systems - Volume 1}, NIPS’13, page 899–907, Red
  Hook, NY, USA. Curran Associates Inc.

\bibitem[{Blitzer et~al.(2007)Blitzer, Dredze, and
  Pereira}]{blitzer-etal-2007-biographies}
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
\newblock \href {https://www.aclweb.org/anthology/P07-1056} {Biographies,
  {B}ollywood, boom-boxes and blenders: Domain adaptation for sentiment
  classification}.
\newblock In \emph{Proceedings of the 45th Annual Meeting of the Association of
  Computational Linguistics}, pages 440--447, Prague, Czech Republic.
  Association for Computational Linguistics.

\bibitem[{Cettolo et~al.(2014)Cettolo, Niehues, St\"{u}ker, Bentivogli, and
  Federico}]{cettolo2014proceedings}
Mauro Cettolo, Jan Niehues, Sebastian St\"{u}ker, Luisa Bentivogli, and
  Marcello Federico. 2014.
\newblock Report on the 11th iwslt evaluation campaign, iwslt 2014.
\newblock In \emph{Proceedings of the 11$^{th}$ International Workshop on
  Spoken Language Translation}.

\bibitem[{Chan et~al.(2020)Chan, Möller, Pietsch, Soni, and
  Yeung}]{chan2020deepset}
Branden Chan, Timo Möller, Malte Pietsch, Tanay Soni, and Chin~Man Yeung.
  2020.
\newblock \href {https://deepset.ai/german-bert} {Open sourcing german bert}.

\bibitem[{Chapelle et~al.(2006)Chapelle, Schölkopf, and
  Zien}]{chapelle2006semi}
Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien. 2006.
\newblock \emph{Semi-Supervised Learning (Adaptive Computation and Machine
  Learning)}.
\newblock The MIT Press.

\bibitem[{Chapelle et~al.(2000)Chapelle, Weston, Bottou, and
  Vapnik}]{vicinal200olivier}
Olivier Chapelle, Jason Weston, L\'{e}on Bottou, and Vladimir Vapnik. 2000.
\newblock Vicinal risk minimization.
\newblock In \emph{NIPS}.

\bibitem[{Daum{\'e}~III(2007)}]{daume-iii-2007-frustratingly}
Hal Daum{\'e}~III. 2007.
\newblock \href {https://www.aclweb.org/anthology/P07-1033} {Frustratingly easy
  domain adaptation}.
\newblock In \emph{Proceedings of the 45th Annual Meeting of the Association of
  Computational Linguistics}, pages 256--263, Prague, Czech Republic.
  Association for Computational Linguistics.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{devlin2018}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock \href {http://arxiv.org/abs/1810.04805} {{BERT:} pre-training of deep
  bidirectional transformers for language understanding}.
\newblock \emph{CoRR}, abs/1810.04805.

\bibitem[{Edunov et~al.(2018)Edunov, Ott, Auli, Grangier, and
  Ranzato}]{edunov-etal-2018-classical}
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc{'}Aurelio
  Ranzato. 2018.
\newblock \href {https://doi.org/10.18653/v1/N18-1033} {Classical structured
  prediction losses for sequence to sequence learning}.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 355--364, New Orleans,
  Louisiana. Association for Computational Linguistics.

\bibitem[{Fadaee et~al.(2017)Fadaee, Bisazza, and Monz}]{fadaee2017data}
Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017.
\newblock \href {https://doi.org/10.18653/v1/P17-2090} {Data augmentation for
  low-resource neural machine translation}.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 567--573,
  Vancouver, Canada. Association for Computational Linguistics.

\bibitem[{Hendrycks et~al.(2020)Hendrycks, Liu, Wallace, Dziedzic, Krishnan,
  and Song}]{hendrycks2020pretrained}
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and
  Dawn Song. 2020.
\newblock Pretrained transformers improve out-of-distribution robustness.
\newblock In \emph{Association for Computational Linguistics}.

\bibitem[{Hochreiter and Schmidhuber(1997)}]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber. 1997.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9(8):1735--1780.

\bibitem[{Kafle et~al.(2017)Kafle, Yousefhussien, and
  Kanan}]{kafle-etal-2017-data}
Kushal Kafle, Mohammed Yousefhussien, and Christopher Kanan. 2017.
\newblock \href {https://doi.org/10.18653/v1/W17-3529} {Data augmentation for
  visual question answering}.
\newblock In \emph{Proceedings of the 10th International Conference on Natural
  Language Generation}, pages 198--202, Santiago de Compostela, Spain.
  Association for Computational Linguistics.

\bibitem[{Kanbak et~al.(2018)Kanbak, Seyed-Mohsen, and
  Frossard}]{kanbak2018geometric}
Can Kanbak, Moosavi-Dezfooli Seyed-Mohsen, and Pascal Frossard. 2018.
\newblock \href {https://doi.org/10.1109/CVPR.2018.00467} {Geometric robustness
  of deep networks: Analysis and improvement}.
\newblock pages 4441--4449.

\bibitem[{Kim(2014)}]{kim2014convolutional}
Yoon Kim. 2014.
\newblock Convolutional neural networks for sentence classification.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}.

\bibitem[{Kingma and Ba(2014)}]{kingma2014method}
Diederik~P. Kingma and Jimmy Ba. 2014.
\newblock \href {http://arxiv.org/abs/1412.6980} {Adam: A method for stochastic
  optimization}.
\newblock Cite arxiv:1412.6980Comment: Published as a conference paper at the
  3rd International Conference for Learning Representations, San Diego, 2015.

\bibitem[{Kobayashi(2018)}]{kobayashi-2018-contextual}
Sosuke Kobayashi. 2018.
\newblock \href {https://doi.org/10.18653/v1/N18-2072} {Contextual
  augmentation: Data augmentation by words with paradigmatic relations}.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, pages 452--457, New Orleans,
  Louisiana. Association for Computational Linguistics.

\bibitem[{Koehn(2004)}]{koehn-2004-statistical}
Philipp Koehn. 2004.
\newblock \href {https://www.aclweb.org/anthology/W04-3250} {Statistical
  significance tests for machine translation evaluation}.
\newblock In \emph{Proceedings of the 2004 Conference on Empirical Methods in
  Natural Language Processing}, pages 388--395, Barcelona, Spain. Association
  for Computational Linguistics.

\bibitem[{Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton}]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton. 2012.
\newblock \href
  {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
  {Imagenet classification with deep convolutional neural networks}.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, \emph{Advances in Neural Information Processing Systems 25}, pages
  1097--1105. Curran Associates, Inc.

\bibitem[{{Kumar} et~al.(2020){Kumar}, {Choudhary}, and {Cho}}]{kumar20202data}
Varun {Kumar}, Ashutosh {Choudhary}, and Eunah {Cho}. 2020.
\newblock \href {http://arxiv.org/abs/2003.02245} {{Data Augmentation using
  Pre-trained Transformer Models}}.
\newblock \emph{arXiv e-prints}, page arXiv:2003.02245.

\bibitem[{Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts}]{maas2011learning}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts. 2011.
\newblock \href {http://www.aclweb.org/anthology/P11-1015} {Learning word
  vectors for sentiment analysis}.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pages 142--150,
  Portland, Oregon, USA. Association for Computational Linguistics.

\bibitem[{Miyato et~al.(2017)Miyato, Maeda, Koyama, and
  Ishii}]{miyato2017virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2017.
\newblock \href {http://arxiv.org/abs/1704.03976} {{Virtual Adversarial
  Training: A Regularization Method for Supervised and Semi-Supervised
  Learning}}.

\bibitem[{Müller et~al.(2019)Müller, Gonzales, and
  Sennrich}]{Muller2019DomainRI}
Mathias Müller, Annette~Rios Gonzales, and Rico Sennrich. 2019.
\newblock Domain robustness in neural machine translation.
\newblock \emph{ArXiv}, abs/1911.03109.

\bibitem[{Ni et~al.(2019)Ni, Li, and McAuley}]{jianmo}
Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019.
\newblock Justifying recommendations using distantly-labeled reviews and
  fined-grained aspects.
\newblock In \emph{Proceedings of EMNLP}.

\bibitem[{Nie et~al.(2019)Nie, Williams, Dinan, Bansal, Weston, and
  Kiela}]{nie2019adversarial}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
  Kiela. 2019.
\newblock \href {http://arxiv.org/abs/1910.14599} {{Adversarial NLI: A New
  Benchmark for Natural Language Understanding}}.

\bibitem[{Norouzi et~al.(2016)Norouzi, Bengio, Chen, Jaitly, Schuster, Wu, and
  Schuurmans}]{norouzi2016reward}
Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster,
  Yonghui Wu, and Dale Schuurmans. 2016.
\newblock \href
  {http://papers.nips.cc/paper/6547-reward-augmented-maximum-likelihood-for-neural-structured-prediction.pdf}
  {Reward augmented maximum likelihood for neural structured prediction}.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 29}, pages
  1723--1731. Curran Associates, Inc.

\bibitem[{Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli}]{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli. 2019.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of NAACL-HLT 2019: Demonstrations}.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu}]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
\newblock \href {https://doi.org/10.3115/1073083.1073135} {Bleu: A method for
  automatic evaluation of machine translation}.
\newblock In \emph{Proceedings of the 40th Annual Meeting on Association for
  Computational Linguistics}, ACL ’02, page 311–318, USA. Association for
  Computational Linguistics.

\bibitem[{Paschali et~al.(2019)Paschali, Simson, Roy, Naeem, G\"{o}bl,
  Wachinger, and Navab}]{paschali2019data}
Magdalini Paschali, Walter Simson, Abhijit~Guha Roy, Muhammad~Ferjad Naeem,
  R\"{u}diger G\"{o}bl, Christian Wachinger, and Nassir Navab. 2019.
\newblock Data augmentation with manifold exploring geometric transformations
  for increased performance and robustness.
\newblock \emph{arXiv}.

\bibitem[{Perez and Wang(2017)}]{perez2017}
Luis Perez and Jason Wang. 2017.
\newblock \href {http://arxiv.org/abs/1712.04621} {{The Effectiveness of Data
  Augmentation in Image Classification using Deep Learning}}.

\bibitem[{Post(2018)}]{post-2018-call}
Matt Post. 2018.
\newblock \href {https://doi.org/10.18653/v1/W18-6319} {A call for clarity in
  reporting {BLEU} scores}.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pages 186--191, Belgium, Brussels. Association for
  Computational Linguistics.

\bibitem[{Quionero-Candela et~al.(2009)Quionero-Candela, Sugiyama,
  Schwaighofer, and Lawrence}]{quionera2009dataset}
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil~D.
  Lawrence. 2009.
\newblock \emph{Dataset Shift in Machine Learning}.

\bibitem[{Rico~Sennrich(2016)}]{sennrich2016improving}
Alexandra~Birch Rico~Sennrich, Barry~Haddow. 2016.
\newblock Improving neural machine translation models with monolingual data.
\newblock In \emph{Proc. of ACL}.

\bibitem[{Sajjadi et~al.(2016)Sajjadi, Javanmardi, and
  Tasdizen}]{sajjadi2016regularization}
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. 2016.
\newblock Regularization with stochastic transformations and perturbations for
  deep semi-supervised learning.
\newblock In \emph{NIPS}.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf}]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock In \emph{NeurIPS $EMC^2$ Workshop}.

\bibitem[{Scherrer and Cartoni(2012)}]{scherrer-cartoni-2012-trilingual}
Yves Scherrer and Bruno Cartoni. 2012.
\newblock \href
  {http://www.lrec-conf.org/proceedings/lrec2012/pdf/685_Paper.pdf} {The
  trilingual {ALLEGRA} corpus: Presentation and possible use for lexicon
  induction}.
\newblock In \emph{Proceedings of the Eighth International Conference on
  Language Resources and Evaluation ({LREC}-2012)}, pages 2890--2896, Istanbul,
  Turkey. European Languages Resources Association (ELRA).

\bibitem[{Sennrich et~al.(2016)Sennrich, Haddow, and
  Birch}]{sennrich-etal-2016-edinburgh}
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
\newblock \href {https://doi.org/10.18653/v1/W16-2323} {{E}dinburgh neural
  machine translation systems for {WMT} 16}.
\newblock In \emph{Proceedings of the First Conference on Machine Translation:
  Volume 2, Shared Task Papers}, pages 371--376, Berlin, Germany. Association
  for Computational Linguistics.

\bibitem[{Simard et~al.(1998)Simard, LeCun, Denker, and Victorri}]{Simard1998}
Patrice~Y. Simard, Yann~A. LeCun, John~S. Denker, and Bernard Victorri. 1998.
\newblock \href {https://doi.org/10.1007/3-540-49430-8_13}
  {\emph{Transformation Invariance in Pattern Recognition --- Tangent Distance
  and Tangent Propagation}}, pages 239--274. Springer Berlin Heidelberg,
  Berlin, Heidelberg.

\bibitem[{Simonyan and Zisserman(2015)}]{simonyan2014very}
Karen Simonyan and Andrew Zisserman. 2015.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{Proceedings of the 2015 International Conference on Learning
  Representations}.

\bibitem[{Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts}]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts. 2013.
\newblock \href {https://www.aclweb.org/anthology/D13-1170} {Recursive deep
  models for semantic compositionality over a sentiment treebank}.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}, pages 1631--1642, Seattle, Washington, USA.
  Association for Computational Linguistics.

\bibitem[{Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus}]{szegedy2014intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus. 2014.
\newblock \href {http://arxiv.org/abs/1312.6199} {Intriguing properties of
  neural networks}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Tiedemann(2012)}]{TIEDEMANN12.463}
J\"{o}rg Tiedemann. 2012.
\newblock Parallel data, tools and interfaces in opus.
\newblock In \emph{Proceedings of the Eight International Conference on
  Language Resources and Evaluation (LREC'12)}, Istanbul, Turkey. European
  Language Resources Association (ELRA).

\bibitem[{{Torralba} and {Efros}(2011)}]{torralba2011unbiased}
A.~{Torralba} and A.~A. {Efros}. 2011.
\newblock Unbiased look at dataset bias.
\newblock In \emph{CVPR 2011}, pages 1521--1528.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Proceedings of the 2017 Conference on Neural Information
  Processing Systems}.

\bibitem[{Vincent et~al.(2008)Vincent, Larochelle, Bengio, and
  Manzagol}]{vincent2008extracting}
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
  2008.
\newblock Extracting and composing robust features with denoising autoencoders.
\newblock In \emph{Proceedings of the 25th International Conference on Machine
  Learning}.

\bibitem[{Wang and Yang(2015)}]{wangyang2015thats}
William~Yang Wang and Diyi Yang. 2015.
\newblock \href {https://doi.org/10.18653/v1/D15-1306} {That{'}s so
  annoying!!!: A lexical and frame-semantic embedding based data augmentation
  approach to automatic categorization of annoying behaviors using {\#}petpeeve
  tweets}.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pages 2557--2563, Lisbon, Portugal. Association
  for Computational Linguistics.

\bibitem[{Wang et~al.(2018)Wang, Pham, Dai, and
  Neubig}]{wang-etal-2018-switchout}
Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018.
\newblock \href {https://doi.org/10.18653/v1/D18-1100} {{S}witch{O}ut: an
  efficient data augmentation algorithm for neural machine translation}.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 856--861, Brussels, Belgium. Association
  for Computational Linguistics.

\bibitem[{Wei and Zou(2019)}]{wei-zou-2019-eda}
Jason Wei and Kai Zou. 2019.
\newblock \href {https://www.aclweb.org/anthology/D19-1670} {{EDA}: Easy data
  augmentation techniques for boosting performance on text classification
  tasks}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 6383--6389, Hong Kong,
  China. Association for Computational Linguistics.

\bibitem[{Williams et~al.(2018)Williams, Nangia, and
  Bowman}]{williams2018broad}
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.
\newblock \href {http://aclweb.org/anthology/N18-1101} {A broad-coverage
  challenge corpus for sentence understanding through inference}.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122. Association for
  Computational Linguistics.

\bibitem[{Wu et~al.(2019{\natexlab{a}})Wu, Fan, Baevski, Dauphin, and
  Auli}]{wu2018pay}
Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli.
  2019{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/1901.10430} {Pay less attention with
  lightweight and dynamic convolutions}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Wu et~al.(2019{\natexlab{b}})Wu, Lv, Zang, Han, and
  Hu}]{wu2019conditional}
Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and Songlin Hu.
  2019{\natexlab{b}}.
\newblock Conditional bert contextual augmentation.
\newblock In \emph{International Conference on Computational Science}, pages
  84--95. Springer.

\bibitem[{Xie et~al.(2019)Xie, Dai, Hovy, Luong, and Le}]{xie2019unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc~V Le. 2019.
\newblock Unsupervised data augmentation for consistency training.
\newblock \emph{arXiv preprint arXiv:1904.12848}.

\bibitem[{Xie et~al.(2020)Xie, Dai, Hovy, Luong, and Le}]{xie2020unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc~V. Le. 2020.
\newblock \href {https://openreview.net/forum?id=ByeL1R4FvS} {Unsupervised data
  augmentation for consistency training}.

\bibitem[{Xie et~al.(2017)Xie, Wang, Li, Levy, Nie, Jurafksy, and
  Ng}]{xie2017data}
Ziang Xie, Sida~I. Wang, Jiwei Li, Daniel Levy, Aiming Nie, Dan Jurafksy, and
  Andrew~Y. Ng. 2017.
\newblock Data noising as smoothing in neural network language models.
\newblock In \emph{Proceedings of the 2017 International Conference on Learning
  Representations}.

\bibitem[{{Yang} et~al.(2020){Yang}, {Malaviya}, {Fernandez}, {Swayamdipta},
  {Le Bras}, {Wang}, {Bhagavatula}, {Choi}, and {Downey}}]{yang2020g-daug}
Yiben {Yang}, Chaitanya {Malaviya}, Jared {Fernandez}, Swabha {Swayamdipta},
  Ronan {Le Bras}, Ji-Ping {Wang}, Chandra {Bhagavatula}, Yejin {Choi}, and
  Doug {Downey}. 2020.
\newblock \href {http://arxiv.org/abs/2004.11546} {{G-DAUG: Generative Data
  Augmentation for Commonsense Reasoning}}.
\newblock \emph{arXiv e-prints}, page arXiv:2004.11546.

\bibitem[{Yelp()}]{yelp}
Yelp.
\newblock Yelp open dataset.
\newblock \url{https://www.yelp.com/dataset}.

\bibitem[{Yu et~al.(2018)Yu, Dohan, Le, Luong, Zhao, and Chen}]{wei2018fast}
Adams~Wei Yu, David Dohan, Quoc Le, Thang Luong, Rui Zhao, and Kai Chen. 2018.
\newblock \href {https://openreview.net/forum?id=B14TlG-RW} {Fast and accurate
  reading comprehension by combining self-attention and convolution}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz}]{zhang2018mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N. Dauphin, and David Lopez-Paz. 2018.
\newblock \href {https://openreview.net/forum?id=r1Ddp1-Rb} {mixup: Beyond
  empirical risk minimization}.
\newblock \emph{International Conference on Learning Representations}.

\bibitem[{Zhang et~al.(2015)Zhang, Zhao, and LeCun}]{zhang2015character}
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
\newblock \href
  {http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf}
  {Character-level convolutional networks for text classification}.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 28}, pages
  649--657. Curran Associates, Inc.

\end{thebibliography}
