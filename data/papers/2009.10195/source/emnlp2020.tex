%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{csquotes}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{tablefootnote}
\usepackage{longtable}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{cleveref}
\usepackage{multirow}
\usepackage{dblfloatfix}
\crefformat{section}{\S#2#1#3} % see manual of cleveref, section 8.2.1
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
\usetikzlibrary{intersections,shapes.arrows}

\makeatletter
\let\OldStatex\Statex
\renewcommand{\Statex}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \OldStatex\hskip\dimexpr#1\@tempdima\relax}
\makeatother

\algrenewcommand\algorithmicindent{1.3em}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\comment}[1]{}

\def\D{\mathcal{D}}
\def\R{\mathbb{R}}
\def\ssmba{SSMBA}
\def\nn#1{\textcolor{blue}{NN: #1}}
\def\M{\mathcal{M}}

\title{SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness}

\author{Nathan Ng\\
  University of Toronto \\
  Vector Institute
  \And
  Kyunghyun Cho \\
  New York University
  \And
  Marzyeh Ghassemi \\
  University of Toronto \\
  Vector Institute\\ 
  }

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples.
  Data augmentation is a common method used to prevent overfitting and improve OOD generalization.
  However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold.
  We introduce \textbf{\ssmba}, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold.
  We investigate the use of \ssmba\ in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. % in three tasks: sentiment classification, natural language inference, and machine translation.
  In experiments on robustness benchmarks across 3 tasks and 9 datasets, \ssmba\ consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8\% accuracy on OOD Amazon reviews, 1.8\% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English. 
  \footnote{Code is availble at \url{https://github.com/nng555/ssmba}}
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{sections/introduction}

\section{Background and Related Work}
\label{sec:background}
\input{sections/background}

\section{SSMBA: Self-Supervised Manifold Based Augmentation}
\label{sec:ssmba}
\input{sections/ssmba}

\section{Datasets}
\label{sec:data}
\input{sections/data}

\section{Experimental Setup}
\label{sec:experiments}
\input{sections/experiments}

\section{Results}
\label{sec:results}
\input{sections/results}

\section{Analysis and Discussion}
\label{sec:discussion}
\input{sections/discussion}

\section{Conclusion}
\label{sec:conclusion}
\input{sections/conclusion}

\section*{Acknowledgements}
Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute \url{www.vectorinstitute.ai/#partners}.
This work was partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Research (Improving Deep Learning using Latent Structure).
We thank Julian McAuley, Vishaal Prasad, Taylor Killian, Victoria Cheng, and Aparna Balagopalan for helpful comments and discussion.

\bibliography{anthology,emnlp2020}
\bibliographystyle{acl_natbib}

\clearpage
\appendix
\input{sections/appendix}

\end{document}

