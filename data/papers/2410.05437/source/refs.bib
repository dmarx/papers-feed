@article{xue2024repeat,
  title={To repeat or not to repeat: Insights from scaling llm under token-crisis},
  author={Xue, Fuzhao and Fu, Yao and Zhou, Wangchunshu and Zheng, Zangwei and You, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  year={2023}
}

@article{liu2024understanding,
  title={Understanding llms: A comprehensive overview from training to inference},
  author={Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and others},
  journal={arXiv preprint arXiv:2401.02038},
  year={2024}
}

@article{xu2023tensorgpt,
  title={Tensorgpt: Efficient compression of the embedding layer in llms based on the tensor-train decomposition},
  author={Xu, Mingxue and Xu, Yao Lei and Mandic, Danilo P},
  journal={arXiv preprint arXiv:2307.00526},
  year={2023}
}

@article{gu2022heat,
  title={Heat: Hardware-efficient automatic tensor decomposition for transformer compression},
  author={Gu, Jiaqi and Keller, Ben and Kossaifi, Jean and Anandkumar, Anima and Khailany, Brucek and Pan, David Z},
  journal={arXiv preprint arXiv:2211.16749},
  year={2022}
}

@article{yuan2023asvd,
  title={Asvd: Activation-aware singular value decomposition for compressing large language models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}

@misc{Badri_Shaji_2024, title={Low-Rank Pruning of Llama2}, 
howpublished = {\url{https://mobiusml.github.io/low-rank-llama2/}},
journal={Mobius ML Github}, author={Badri, Hicham and Shaji, Appu}, year={2024}}

@article{zhao2024galore,
  title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{ashkboos2024quarot,
  title={QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2404.00456},
  year={2024}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{lin2023awq,
  title={Awq: Activation-aware weight quantization for llm compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{ma2023llm_pruner,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}


@article{mirzadeh2023relu,
  title={Relu strikes back: Exploiting activation sparsity in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Mehta, Sachin and Del Mundo, Carlo C and Tuzel, Oncel and Samei, Golnoosh and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2310.04564},
  year={2023}
}

@inproceedings{kwon2023pagedattn,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{zheng2023radixattn,
  title={Efficiently programming large language models using sglang},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Huang, Jeff and Sun, Chuyue and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2312.07104},
  year={2023}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@inproceedings{yu2022continuousbatching,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@article{kim2024speculativedecoding,
  title={Speculative decoding with big little decoder},
  author={Kim, Sehoon and Mangalam, Karttikeya and Moon, Suhong and Malik, Jitendra and Mahoney, Michael W and Gholami, Amir and Keutzer, Kurt},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{edalati2021kronecker,
  title={Kronecker decomposition for gpt compression},
  author={Edalati, Ali and Tahaei, Marzieh and Rashid, Ahmad and Nia, Vahid Partovi and Clark, James J and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:2110.08152},
  year={2021}
}

@article{chen2023tsvd,
  title={Ternary singular value decomposition as a better parameterized form in linear mapping},
  author={Chen, Boyu and Chen, Hanxuan and He, Jiao and Sun, Fengyu and Jui, Shangling},
  journal={arXiv preprint arXiv:2308.07641},
  year={2023}
}

@article{ashkboos2024slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2401.15024},
  year={2024}
}

@book{johnson2004clt,
  title={Information theory and the central limit theorem},
  author={Johnson, Oliver},
  year={2004},
  publisher={World Scientific}
}

@inproceedings{sakr2022octav,
  title={Optimal clipping and magnitude-aware differentiation for improved quantization-aware training},
  author={Sakr, Charbel and Dai, Steve and Venkatesan, Rangha and Zimmer, Brian and Dally, William and Khailany, Brucek},
  booktitle={International Conference on Machine Learning},
  pages={19123--19138},
  year={2022},
  organization={PMLR}
}

@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}

@inproceedings{dai2023efficient,
  title={Efficient Transformer Inference with Statically Structured Sparse Attention},
  author={Dai, Steve and Genc, Hasan and Venkatesan, Rangharajan and Khailany, Brucek},
  booktitle={2023 60th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}

@article{abdi2010pca,
  title={Principal component analysis},
  author={Abdi, Herv{\'e} and Williams, Lynne J},
  journal={Wiley interdisciplinary reviews: computational statistics},
  volume={2},
  number={4},
  pages={433--459},
  year={2010},
  publisher={Wiley Online Library}
}

@inproceedings{sakr2017analytical,
  title={Analytical guarantees on numerical precision of deep neural networks},
  author={Sakr, Charbel and Kim, Yongjune and Shanbhag, Naresh},
  booktitle={International Conference on Machine Learning},
  pages={3007--3016},
  year={2017},
  organization={PMLR}
}

@misc{shoeybi2020megatronlm,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models}, 
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@inproceedings{clark2019boolq,
  title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle = {NAACL},
  year =      {2019}
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@inproceedings{Bisk2020piqa,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}

@inproceedings{lai-etal-2017-race,
    title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",
    author = "Lai, Guokun  and
      Xie, Qizhe  and
      Liu, Hanxiao  and
      Yang, Yiming  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1082",
    doi = "10.18653/v1/D17-1082",
    pages = "785--794",
}

@InProceedings{ai2:winogrande,
title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
author={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi
},
year={2019}
}

@article{smith2022mtnlg,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{hinton2012rmsprop,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  pages={2},
  year={2012}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{ma2024one_bit_llm,
  title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits},
  author={Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  journal={arXiv preprint arXiv:2402.17764},
  year={2024}
}

@article{liu2023chipnemo,
  title={Chipnemo: Domain-adapted llms for chip design},
  author={Liu, Mingjie and Ene, Teodor-Dumitru and Kirby, Robert and Cheng, Chris and Pinckney, Nathaniel and Liang, Rongjian and Alben, Jonah and Anand, Himyanshu and Banerjee, Sanmitra and Bayraktaroglu, Ismet and others},
  journal={arXiv preprint arXiv:2311.00176},
  year={2023}
}

@inproceedings{park2020profit,
  title={Profit: A novel training method for sub-4-bit mobilenet models},
  author={Park, Eunhyeok and Yoo, Sungjoo},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part VI 16},
  pages={430--446},
  year={2020},
  organization={Springer}
}

@inproceedings{nagel2022overcoming,
  title={Overcoming oscillations in quantization-aware training},
  author={Nagel, Markus and Fournarakis, Marios and Bondarenko, Yelysei and Blankevoort, Tijmen},
  booktitle={International Conference on Machine Learning},
  pages={16318--16330},
  year={2022},
  organization={PMLR}
}

@article{kuzmin2024versus,
  title={Pruning vs Quantization: Which is Better?},
  author={Kuzmin, Andrey and Nagel, Markus and Van Baalen, Mart and Behboodi, Arash and Blankevoort, Tijmen},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dery2024everybody,
  title={Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes},
  author={Dery, Lucio and Kolawole, Steven and Kagey, Jean-Francois and Smith, Virginia and Neubig, Graham and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2402.05406},
  year={2024}
}

@article{nemotron,
  title={Nemotron-4 15B Technical Report},
  author={Parmar, Jupinder and Prabhumoye, Shrimai and Jennings, Joseph and Patwary, Mostofa and Subramanian, Sandeep and Su, Dan and Zhu, Chen and Narayanan, Deepak and Jhunjhunwala, Aastha and Dattagupta, Ayush and others},
  journal={arXiv preprint arXiv:2402.16819},
  year={2024}
}

@inproceedings{sakr2019per,
  title={Per-tensor fixed-point quantization of the back-propagation algorithm},
  author={Sakr, Charbel and Shanbhag, Naresh},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}

@inproceedings{sakr2018analytical,
  title={An analytical method to determine minimum per-layer precision of deep neural networks},
  author={Sakr, Charbel and Shanbhag, Naresh},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1090--1094},
  year={2018},
  organization={IEEE}
}