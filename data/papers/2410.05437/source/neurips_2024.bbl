\begin{thebibliography}{10}

\bibitem{xue2024repeat}
F.~Xue, Y.~Fu, W.~Zhou, Z.~Zheng, and Y.~You, ``To repeat or not to repeat: Insights from scaling llm under token-crisis,'' {\em Advances in Neural Information Processing Systems}, vol.~37, 2023.

\bibitem{liu2024understanding}
Y.~Liu, H.~He, T.~Han, X.~Zhang, M.~Liu, J.~Tian, Y.~Zhang, J.~Wang, X.~Gao, T.~Zhong, {\em et~al.}, ``Understanding llms: A comprehensive overview from training to inference,'' {\em arXiv preprint arXiv:2401.02038}, 2024.

\bibitem{xiao2023smoothquant}
G.~Xiao, J.~Lin, M.~Seznec, H.~Wu, J.~Demouth, and S.~Han, ``Smoothquant: Accurate and efficient post-training quantization for large language models,'' in {\em International Conference on Machine Learning}, pp.~38087--38099, PMLR, 2023.

\bibitem{frantar2023sparsegpt}
E.~Frantar and D.~Alistarh, ``Sparsegpt: Massive language models can be accurately pruned in one-shot,'' in {\em International Conference on Machine Learning}, pp.~10323--10337, PMLR, 2023.

\bibitem{lin2023awq}
J.~Lin, J.~Tang, H.~Tang, S.~Yang, X.~Dang, and S.~Han, ``Awq: Activation-aware weight quantization for llm compression and acceleration,'' {\em arXiv preprint arXiv:2306.00978}, 2023.

\bibitem{frantar2022gptq}
E.~Frantar, S.~Ashkboos, T.~Hoefler, and D.~Alistarh, ``Gptq: Accurate post-training quantization for generative pre-trained transformers,'' {\em arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{ma2023llm_pruner}
X.~Ma, G.~Fang, and X.~Wang, ``Llm-pruner: On the structural pruning of large language models,'' {\em Advances in neural information processing systems}, vol.~36, pp.~21702--21720, 2023.

\bibitem{mirzadeh2023relu}
I.~Mirzadeh, K.~Alizadeh, S.~Mehta, C.~C. Del~Mundo, O.~Tuzel, G.~Samei, M.~Rastegari, and M.~Farajtabar, ``Relu strikes back: Exploiting activation sparsity in large language models,'' {\em arXiv preprint arXiv:2310.04564}, 2023.

\bibitem{yu2022continuousbatching}
G.-I. Yu, J.~S. Jeong, G.-W. Kim, S.~Kim, and B.-G. Chun, ``Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models,'' in {\em 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)}, pp.~521--538, 2022.

\bibitem{kim2024speculativedecoding}
S.~Kim, K.~Mangalam, S.~Moon, J.~Malik, M.~W. Mahoney, A.~Gholami, and K.~Keutzer, ``Speculative decoding with big little decoder,'' {\em Advances in Neural Information Processing Systems}, vol.~36, 2024.

\bibitem{kwon2023pagedattn}
W.~Kwon, Z.~Li, S.~Zhuang, Y.~Sheng, L.~Zheng, C.~H. Yu, J.~Gonzalez, H.~Zhang, and I.~Stoica, ``Efficient memory management for large language model serving with pagedattention,'' in {\em Proceedings of the 29th Symposium on Operating Systems Principles}, pp.~611--626, 2023.

\bibitem{zheng2023radixattn}
L.~Zheng, L.~Yin, Z.~Xie, J.~Huang, C.~Sun, C.~H. Yu, S.~Cao, C.~Kozyrakis, I.~Stoica, J.~E. Gonzalez, {\em et~al.}, ``Efficiently programming large language models using sglang,'' {\em arXiv preprint arXiv:2312.07104}, 2023.

\bibitem{dao2022flashattention}
T.~Dao, D.~Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}, ``Flashattention: Fast and memory-efficient exact attention with io-awareness,'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~16344--16359, 2022.

\bibitem{edalati2021kronecker}
A.~Edalati, M.~Tahaei, A.~Rashid, V.~P. Nia, J.~J. Clark, and M.~Rezagholizadeh, ``Kronecker decomposition for gpt compression,'' {\em arXiv preprint arXiv:2110.08152}, 2021.

\bibitem{chen2023tsvd}
B.~Chen, H.~Chen, J.~He, F.~Sun, and S.~Jui, ``Ternary singular value decomposition as a better parameterized form in linear mapping,'' {\em arXiv preprint arXiv:2308.07641}, 2023.

\bibitem{xu2023tensorgpt}
M.~Xu, Y.~L. Xu, and D.~P. Mandic, ``Tensorgpt: Efficient compression of the embedding layer in llms based on the tensor-train decomposition,'' {\em arXiv preprint arXiv:2307.00526}, 2023.

\bibitem{gu2022heat}
J.~Gu, B.~Keller, J.~Kossaifi, A.~Anandkumar, B.~Khailany, and D.~Z. Pan, ``Heat: Hardware-efficient automatic tensor decomposition for transformer compression,'' {\em arXiv preprint arXiv:2211.16749}, 2022.

\bibitem{Badri_Shaji_2024}
H.~Badri and A.~Shaji, ``Low-rank pruning of llama2.'' \url{https://mobiusml.github.io/low-rank-llama2/}, 2024.

\bibitem{hu2021LoRa}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen, ``Lora: Low-rank adaptation of large language models,'' {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{yuan2023asvd}
Z.~Yuan, Y.~Shang, Y.~Song, Q.~Wu, Y.~Yan, and G.~Sun, ``Asvd: Activation-aware singular value decomposition for compressing large language models,'' {\em arXiv preprint arXiv:2312.05821}, 2023.

\bibitem{ashkboos2024slicegpt}
S.~Ashkboos, M.~L. Croci, M.~G.~d. Nascimento, T.~Hoefler, and J.~Hensman, ``Slicegpt: Compress large language models by deleting rows and columns,'' {\em arXiv preprint arXiv:2401.15024}, 2024.

\bibitem{zhao2024galore}
J.~Zhao, Z.~Zhang, B.~Chen, Z.~Wang, A.~Anandkumar, and Y.~Tian, ``Galore: Memory-efficient llm training by gradient low-rank projection,'' {\em arXiv preprint arXiv:2403.03507}, 2024.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,'' {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{hinton2012rmsprop}
G.~Hinton, N.~Srivastava, and K.~Swersky, ``Neural networks for machine learning lecture 6a overview of mini-batch gradient descent,'' {\em Cited on}, vol.~14, no.~8, p.~2, 2012.

\bibitem{johnson2004clt}
O.~Johnson, {\em Information theory and the central limit theorem}.
\newblock World Scientific, 2004.

\bibitem{sakr2022octav}
C.~Sakr, S.~Dai, R.~Venkatesan, B.~Zimmer, W.~Dally, and B.~Khailany, ``Optimal clipping and magnitude-aware differentiation for improved quantization-aware training,'' in {\em International Conference on Machine Learning}, pp.~19123--19138, PMLR, 2022.

\bibitem{wu2020integer}
H.~Wu, P.~Judd, X.~Zhang, M.~Isaev, and P.~Micikevicius, ``Integer quantization for deep learning inference: Principles and empirical evaluation,'' {\em arXiv preprint arXiv:2004.09602}, 2020.

\bibitem{dai2023efficient}
S.~Dai, H.~Genc, R.~Venkatesan, and B.~Khailany, ``Efficient transformer inference with statically structured sparse attention,'' in {\em 2023 60th ACM/IEEE Design Automation Conference (DAC)}, pp.~1--6, IEEE, 2023.

\bibitem{abdi2010pca}
H.~Abdi and L.~J. Williams, ``Principal component analysis,'' {\em Wiley interdisciplinary reviews: computational statistics}, vol.~2, no.~4, pp.~433--459, 2010.

\bibitem{sakr2017analytical}
C.~Sakr, Y.~Kim, and N.~Shanbhag, ``Analytical guarantees on numerical precision of deep neural networks,'' in {\em International Conference on Machine Learning}, pp.~3007--3016, PMLR, 2017.

\bibitem{sakr2018analytical}
C.~Sakr and N.~Shanbhag, ``An analytical method to determine minimum per-layer precision of deep neural networks,'' in {\em 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.~1090--1094, IEEE, 2018.

\bibitem{sakr2019per}
C.~Sakr and N.~Shanbhag, ``Per-tensor fixed-point quantization of the back-propagation algorithm,'' in {\em 7th International Conference on Learning Representations, ICLR 2019}, 2019.

\bibitem{shoeybi2020megatronlm}
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro, ``Megatron-lm: Training multi-billion parameter language models using model parallelism,'' 2020.

\bibitem{touvron2023llama}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, {\em et~al.}, ``Llama 2: Open foundation and fine-tuned chat models,'' {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{nemotron}
J.~Parmar, S.~Prabhumoye, J.~Jennings, M.~Patwary, S.~Subramanian, D.~Su, C.~Zhu, D.~Narayanan, A.~Jhunjhunwala, A.~Dattagupta, {\em et~al.}, ``Nemotron-4 15b technical report,'' {\em arXiv preprint arXiv:2402.16819}, 2024.

\bibitem{merity2016pointer}
S.~Merity, C.~Xiong, J.~Bradbury, and R.~Socher, ``Pointer sentinel mixture models,'' 2016.

\bibitem{clark2019boolq}
C.~Clark, K.~Lee, M.-W. Chang, T.~Kwiatkowski, M.~Collins, and K.~Toutanova, ``Boolq: Exploring the surprising difficulty of natural yes/no questions,'' in {\em NAACL}, 2019.

\bibitem{zellers2019hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi, ``Hellaswag: Can a machine really finish your sentence?,'' in {\em Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 2019.

\bibitem{Bisk2020piqa}
Y.~Bisk, R.~Zellers, R.~L. Bras, J.~Gao, and Y.~Choi, ``Piqa: Reasoning about physical commonsense in natural language,'' in {\em Thirty-Fourth AAAI Conference on Artificial Intelligence}, 2020.

\bibitem{lai-etal-2017-race}
G.~Lai, Q.~Xie, H.~Liu, Y.~Yang, and E.~Hovy, ``{RACE}: Large-scale {R}e{A}ding comprehension dataset from examinations,'' in {\em Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, (Copenhagen, Denmark), pp.~785--794, Association for Computational Linguistics, Sept. 2017.

\bibitem{ai2:winogrande}
S.~Keisuke, L.~B. Ronan, B.~Chandra, and C.~Yejin, ``Winogrande: An adversarial winograd schema challenge at scale,'' 2019.

\bibitem{eval-harness}
L.~Gao, J.~Tow, B.~Abbasi, S.~Biderman, S.~Black, A.~DiPofi, C.~Foster, L.~Golding, J.~Hsu, A.~Le~Noac'h, H.~Li, K.~McDonell, N.~Muennighoff, C.~Ociepa, J.~Phang, L.~Reynolds, H.~Schoelkopf, A.~Skowron, L.~Sutawika, E.~Tang, A.~Thite, B.~Wang, K.~Wang, and A.~Zou, ``A framework for few-shot language model evaluation,'' 12 2023.

\bibitem{smith2022mtnlg}
S.~Smith, M.~Patwary, B.~Norick, P.~LeGresley, S.~Rajbhandari, J.~Casper, Z.~Liu, S.~Prabhumoye, G.~Zerveas, V.~Korthikanti, {\em et~al.}, ``Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model,'' {\em arXiv preprint arXiv:2201.11990}, 2022.

\bibitem{park2020profit}
E.~Park and S.~Yoo, ``Profit: A novel training method for sub-4-bit mobilenet models,'' in {\em Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part VI 16}, pp.~430--446, Springer, 2020.

\bibitem{nagel2022overcoming}
M.~Nagel, M.~Fournarakis, Y.~Bondarenko, and T.~Blankevoort, ``Overcoming oscillations in quantization-aware training,'' in {\em International Conference on Machine Learning}, pp.~16318--16330, PMLR, 2022.

\bibitem{kuzmin2024versus}
A.~Kuzmin, M.~Nagel, M.~Van~Baalen, A.~Behboodi, and T.~Blankevoort, ``Pruning vs quantization: Which is better?,'' {\em Advances in Neural Information Processing Systems}, vol.~36, 2024.

\bibitem{dery2024everybody}
L.~Dery, S.~Kolawole, J.-F. Kagey, V.~Smith, G.~Neubig, and A.~Talwalkar, ``Everybody prune now: Structured pruning of llms with only forward passes,'' {\em arXiv preprint arXiv:2402.05406}, 2024.

\bibitem{horn2012matrix}
R.~A. Horn and C.~R. Johnson, {\em Matrix analysis}.
\newblock Cambridge university press, 2012.

\bibitem{ma2024one_bit_llm}
S.~Ma, H.~Wang, L.~Ma, L.~Wang, W.~Wang, S.~Huang, L.~Dong, R.~Wang, J.~Xue, and F.~Wei, ``The era of 1-bit llms: All large language models are in 1.58 bits,'' {\em arXiv preprint arXiv:2402.17764}, 2024.

\bibitem{liu2023chipnemo}
M.~Liu, T.-D. Ene, R.~Kirby, C.~Cheng, N.~Pinckney, R.~Liang, J.~Alben, H.~Anand, S.~Banerjee, I.~Bayraktaroglu, {\em et~al.}, ``Chipnemo: Domain-adapted llms for chip design,'' {\em arXiv preprint arXiv:2311.00176}, 2023.

\bibitem{men2024shortgpt}
X.~Men, M.~Xu, Q.~Zhang, B.~Wang, H.~Lin, Y.~Lu, X.~Han, and W.~Chen, ``Shortgpt: Layers in large language models are more redundant than you expect,'' {\em arXiv preprint arXiv:2403.03853}, 2024.

\end{thebibliography}
