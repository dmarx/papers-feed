\vspace{-3mm}
\section{Auto Device Mapping} \label{sec:auto_mapping}

Our hybrid programming model %
requires users to input the following configurations, which are referred to as a \textit{mapping} of the RLHF dataflow to the given devices: %
(a) device placement of the models in the dataflow; (b) the corresponding parallelism strategy for running each model in each stage. %



We provide an efficient %
algorithm (Algorithm~\ref{alg:mapping})  
for users to identify the optimized mapping of executing the RLHF dataflow on a given cluster of devices, that minimizes the end-to-end latency of each RLHF iteration. Given a dataflow $D$, we first explore all possible placement plans $\mathcal{P}$ %
for the models in the given cluster (Line~\ref{line:get_placement}).
For example, the PPO algorithm involves four models, resulting in 15 possible placements (from the Bell partition problem~\cite{bell1934exponential, rota1964number}),
ranging from a completely standalone placement where all models are placed on different devices (e.g., OpenRLHF's placement) to colocating all models on the same set of devices (e.g., DeepSpeed-Chat's placement). We refer to %
colocated models on the same set of GPUs as a colocated \textit{set}. Models in a colocated set can employ different parallelism strategies across the same set of GPUs. We identify the smallest number of GPUs to be allocated to each of the colocated model sets, $A_{min}$, based on memory consumption of colocated models, ensuring no out-of-memory errors (Line~\ref{line:get_min_alloc}).

Next, starting from the minimal GPU allocation in $A_{min}$, we enumerate all feasible device allocations to each colocated model set (Lines~\ref{line:enum_alloc}-\ref{line:enum_all_set}).
Given device allocation $A$ to the colocated set and computation workload $W$ of models in the set, we explore optimized parallelism strategies for each model in the \verb|auto_parallel| module, that minimizes model execution latency. 
The workload $W$ includes input and output shapes and computation (training, inference or generation) of each model. 
In \verb|auto_parallel|,  we utilize a simulator module \verb|simu| to estimate the latency of different parallel strategies, following previous research~\cite{zhongDistServeDisaggregatingPrefill2024, zheng2022alpa, yuan2024llmrooftline, llm-analysis} (outline in 
Appendix.~\ref{appendix:auto_parallel}).










The \verb|d_cost| module estimates the end-to-end latency of the RLHF dataflow under given model placement and %
parallelism strategies, by iterating through all stages in the dataflow graph and summing up latencies of all stages (Lines~\ref{line: d_cost_call}, \ref{line:d_cost}). %
For models in the same colocated set and involving computation in the same stage (such as actor and critic both performing model update in RLHF training stage),
their execution latencies are summed up (Line~\ref{alg:d_cost_sum_up}). For models in different colocated sets, their execution within the same stage can be parallelized, and the latency of the stage is determined by the maximum execution time among different sets (Line~\ref{alg:d_cost_max}). 
We identify the best device placement of the models with their corresponding parallelism strategies, achieving minimal execution time per RLHF iteration 
(Lines~\ref{line:start_compare_best}-\ref{line:end_compare_best}).

\begin{algorithm}[t]
\small
\caption{Device Mapping for an RLHF Dataflow}
\label{alg:mapping}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} RLHF dataflow graph $D$, LLMs in RLHF dataflow $L$=$[l_1, l_2, \ldots, l_k]$,
workload $W$ of LLMs in RLHF dataflow, total \# of GPUs $N$,
memory capacity per GPU $Q$ 
\STATE {\bfseries Output:} device mapping of models in RLHF dataflow
\STATE $\mathcal{P} \leftarrow \text{get\_placements}(D, L, N)$ \label{line:get_placement}
\STATE $C^{*} \leftarrow \infty$
\STATE $best\_mapping \leftarrow \emptyset$
\FORALL{$plm \in \mathcal{P}$}
    \STATE $C_{plm} \leftarrow \infty$
    \STATE $best\_plm\_alloc \leftarrow \emptyset$
    \STATE $A_{min} \leftarrow \text{get\_min\_alloc}(plm, Q, N)$ \label{line:get_min_alloc}
    \FORALL{$A \in \text{enum\_alloc}(N, A_{min})$} \label{line:enum_alloc}
        \STATE $\widehat{L} \leftarrow []$ 
        \FORALL{$\text{set} \in plm$} \label{line:enum_all_set}
            \FORALL{$l \in \text{set}$}
                \STATE $\widehat{l} \leftarrow \text{auto\_parallel}(A, A_{min}, l, W)$ \label{line:auto_parallel}
                \STATE $\widehat{L}.\text{append}(\widehat{l})$
            \ENDFOR
        \ENDFOR
        \STATE $plm.\text{update}(\widehat{L})$ 
        \STATE $C_{alloc} \leftarrow \text{d\_cost}(D, plm, W)$ \label{line: d_cost_call}
        \IF{$C_{alloc} < C_{plm}$} \label{line:start_compare_best}
            \STATE $C_{plm} \leftarrow C_{alloc}$
            \STATE $best\_plm\_alloc \leftarrow (plm, A)$
        \ENDIF
    \ENDFOR
    \IF{$C_{plm} < C^{*}$}
        \STATE $C^{*} \leftarrow C_{plm}$
        \STATE $best\_mapping \leftarrow best\_plm\_alloc$ \label{line:end_compare_best}
    \ENDIF
\ENDFOR
\RETURN $best\_mapping$


\vspace{0.5mm}
\STATE {\bfseries Procedure} d\_cost($D$, $plm$, $W$): \label{line:d_cost}
\STATE $\quad s \gets \text{number of stages in } D$
\STATE $\quad c \gets [0] \times s$ 
\textit{ // Initialize latency for each stage to 0} 
\STATE $\quad \textbf{for all}\  \text{set} \in plm \ \textbf{do}$
    \STATE $\quad \quad c_{g} \gets [0] \times s$
    \STATE $\quad \quad \textbf{for all}\  i \in \{0, ..., s-1\} \ \textbf{do}$
        \STATE $\quad \quad \quad \textbf{for all}\  \widehat{l} \in \text{set} \ \textbf{do}$
            \STATE $\quad \quad \quad \quad c_g[i] \gets  c_g[i] + \text{simu}(\widehat{l}, W[i])$
            \label{alg:d_cost_sum_up}
    \STATE $\quad \quad \quad c[i] \gets  max\{c[i], c_g[i]\}$ \label{alg:d_cost_max}
\STATE $\quad \textbf{return} \  \text{sum}(c)$ 

\end{algorithmic}
\end{algorithm}


The complexity of Algorithm~\ref{alg:mapping} 
is $O(\frac{(N-1)!}{(k-1)! (N-k)!})$, where $k$ is the number of models in the dataflow and $N$ is the total number of devices to run the dataflow.
This is the worst-case complexity for enumerating all possible device allocations for a placement strategy (i.e., the standalone placement), calculated by assigning $N$ devices to $k$ models (known as the integer partition problem~\cite{andrews2004integer}). 
For better efficiency, we cache parallelism strategies identified for each model on a number of devices $A$, to eliminate redundant searches for the same parallelism strategies when the model is placed on different sets of $A$ GPUs in different placement strategies. %

Though we assume $N$ homogeneous GPUs when running the auto mapping algorithm, Algorithm \ref{alg:mapping} can be readily extended for optimizing model mapping over heterogeneous devices, by considering heterogeneous devices in \verb|simu| and \verb|auto_parallel| modules~\cite{zhang2024hap}.



