\vspace{-1mm}
\section{Discussions} %
\noindent \textbf{Fault Tolerance.}
HybridFlow is orthogonal to existing fault-tolerance approaches~\cite{swift, wang2023gemini, jang2023oobleck, mohan2021checkfreq, eisenman2022check} and already incorporates checkpointing. Failures can be detected by NCCL errors and silent-data-corruption by checksums. Our programming model enables the single controller to coordinate checkpoint operations via RPC, allowing the saving of model states within each ParallWorker Group. 
This includes saving parameters of actor/critic models, dataloader IDs, and Random Number Generator (RNG) states to ensure system-wide consistency.
Moreover, \sysname{} can also employ redundancy-based fault-tolerance methods, such as broadcast parameters and CPU checkpoint, for fast recovery if enough healthy model replicas are available~\cite{swift, wang2023gemini}.





\noindent \textbf{Placement Insights.} We conclude three main insights for model placement and GPU allocation in RLHF training. 
\textbf{1)} Allocating more GPUs to the actor model can reduce the time-consuming generation latency, which cannot be parallelized with other models. 
\textbf{2)} When each model computation can fully utilize GPU resources, colocating all the models is most effective when training on relatively small-scale clusters.
\textbf{3)} When scaling up to large-scale clusters (i.e., strong scaling), distributing the actor and critic models on different devices for parallel execution in the training and preparation stages would help achieve higher throughput.

\noindent \textbf{Resource multiplexing.}
\sysname{} enables colocation of models on shared devices by utilizing time-sharing for GPU computation.
Recent research in DNN task scheduling has developed fine-grained resource multiplexing techniques, primarily aimed at achieving the service-level objectives of individual tasks~\cite{han2022microsecond, park2017multiplex-gpu, wang2016multiplex-gpu, liang2014multi-plexgpu, bai2020pipeswitch, han2022microsecond, cui2022dvabatch}.
Although the \texttt{ResourcePool} implementation supports parallel execution of collocated models, \sysname{} generally adheres to sequential execution to prevent GPU resource contention or OOM issues as discussed in Section~\ref{sec:2_3_rlhf_characterisitc}.
Applying GPU sharing and heterogeneous resources in RLHF training poses distinct challenges, as it seeks to balance the computation workload and manage complex data dependencies among various tasks.
Investigating fine-grained auto-mapping algorithms for GPU sharing in RLHF training, coupled with model offload optimization and integration of heterogeneous devices, would be a promising direction for future research.








\noindent \textbf{From alignment to reasoning.}
In RLHF for LLM alignment, the reward signal is generated by the reward model. Besides alignment tasks, similar algorithms (e.g., PPO and GRPO~\cite{shao2024deepseekmath}) can be applied to other domains, such as code generation and mathematical reasoning. 
For these tasks, a ground truth may exist for each prompt, which can be determined by assessing the correctness of the output value for each code test case and verifying the accuracy of mathematical results.
Therefore, the reward model can be replaced by non-neural-network reward modules, such as a sandbox environment~\cite{zhangframework} for evaluating generated code or a reward function~\cite{cobbe2021gsm8k, 2019math} to validate mathematical results. \sysname{} can seamlessly integrate these reward modules by wrapping them as remote functions and orchestrating their execution within the single-process script, providing a flexible and efficient framework for diverse reinforcement learning applications.



















