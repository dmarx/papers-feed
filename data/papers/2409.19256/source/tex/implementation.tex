\section{Implementation} \label{sec:impl}
\sysname{} is implemented in around 12k lines of Python code (LoC). %



\noindent \textbf{Hybrid programming model.}
The hierarchical APIs are implemented with 1.8k LoC. %
The centralized single controller is built on top of Ray~\cite{moritz2018ray} and uses Remote Process Calls (RPC) to coordinate the execution order of different models and transfer data between models following the dataflow.
These intermediate data are stored in TensorDict~\cite{paszke2019pytorch}.
{In our multi-controller paradigm for distributed computation, each model function runs on a separate process across various devices, with control messages relayed from each controller's CPU process to the corresponding GPU.}
Our implementation supports Megatron-LM, PyTorch FSDP, and DeepSpeed as the LLM training and inference engines, and vLLM for auto-regressive generation. %
In vLLM, we replace the centralized KVCache manager with a distributed manager to align with the multi-controller paradigm.




\noindent \textbf{3D-HybridEngine.} Its main logic %
is implemented with 2.4k LoC on top of Megatron-LM and vLLM. We %
store actor model weights for training and generation stages on separate memory buffers, offload generation weights to the CPU memory during training, reload generation weights back to GPU memory during the transition, and use both buffers in generation. 
We %
use NCCL communication primitives~\cite{jeaugey2017nccl} to collect and concatenate model parameters in each micro DP group during the transition between training and generation. We offload KVCache to CPU memory after generation and reload it back to GPU in the next iteration. %

\noindent\textbf{Auto-Mapping Algorithm}
is implemented with 1.9k LoC, together with three simulators for training, inference, and generation workloads. The algorithm is run before starting the RLHF dataflow on CPU, to generate device mapping and parallelism strategies for dataflow initialization. 





