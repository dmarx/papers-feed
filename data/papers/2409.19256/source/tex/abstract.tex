\begin{abstract}
Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment.
Traditional RL can be modeled as a %
dataflow, where each node represents computation of a neural network (NN) %
and each edge denotes data dependencies between the NNs. %
RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast.
Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, %
which can be \textit{inefficient} in RLHF due to large control dispatch overhead for distributed intra-node computation. %
Existing RLHF systems %
adopt a multi-controller paradigm, which can be \textit{inflexible} due to nesting distributed computation and data communication. %
We propose \textit{\sysname}, which combines single-controller and multi-controller paradigms in a \underline{\textit{\smash{hybrid}}} manner to enable
\textit{flexible} representation and \textit{efficient} execution of the RLHF data\underline{\textit{\smash{flow}}}.
We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies %
in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. %
We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead.
Our experimental results demonstrate 1.53$\times$$\sim$20.57$\times$ throughput improvement when running various RLHF algorithms using \sysname{}, as compared with state-of-the-art baselines. 
\sysname{} source code will be available at \href{https://github.com/volcengine/verl}{https://github.com/volcengine/verl}




\end{abstract}
