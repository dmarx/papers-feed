\section{Introduction} \label{sec:intro}

Large language models (LLMs) such as %
GPT~\cite{gpt3}, Llama~\cite{touvron2023llama} and Claude~\cite{bai2022training} have revolutionized various artificial intelligence (AI) applications, ranging from writing~\cite{openai2023gpt4}, searching~\cite{nakano2021webgpt} to coding~\cite{rozi√®re2023codellama}. 
LLMs are first pre-trained on trillions of tokens from books, websites, etc,. via next-word prediction to accumulate broad knowledge \cite{gpt3}. Next, LLMs are trained on domain-specific datasets via supervised fine-tuning (SFT), to be able to follow human instructions~\cite{gpt3}. Despite the outstanding capabilities of LLMs on natural language tasks after pre-training and SFT, the detrimental and biased contents in the training datasets may still mislead an LLM to generate toxic and undesirable content. Reinforcement Learning from Human Feedback (RLHF) %
is introduced to further align an LLM to human values, for building helpful and harmless AI applications~\cite{bai2022training, ouyang2022training}.

RLHF is built upon %
traditional RL algorithms~\cite{schulman2017proximal, williams1992REINFORCE, akrour2011preference_tradtionrl}, e.g., Proximal Policy Optimization 
(PPO)~\cite{schulman2017proximal} and REINFORCE~\cite{williams1992REINFORCE}.
The widely adopted PPO-based RLHF system typically consists of four LLMs~\cite{bai2022training, ouyang2022training}%
: an \textit{actor}, a \textit{critic}, a \textit{reference policy} network and a \textit{reward model}.
PPO-based RLHF proceeds in iterations, each with three stages: (1) response \textit{generation} using the actor model with a batch of prompts;
(2) \textit{preparation} of 
training data by scoring the generated responses through a single forward pass of the critic, reference policy, and reward models;
(3) \textit{learning} from human preference by updating actor and critic through forward and backward computation.
Other RLHF variants~\cite{daiSafeRLHFSafe2023, li2023remax} follow similar stages but involves different numbers of models and data dependencies among the models.


Traditional RL can be %
modeled as a %
dataflow%
~\cite{liang2021rllib}, which is a directed acyclic graph (DAG): each node in the RL dataflow represents computation of a neural network (e.g., actor or critic network which can be CNN or MLP);
 each edge denotes data dependency between NN computations (e.g., output of the critic is used as input to actor training~\cite{schulman2017proximal}.) %
RLHF dataflow is more complex, with more complicated models involved 
(e.g., LLMs for the actor/critic/reference/reward models),
each running distinct computation,
and more diverse data dependencies among them (i.e., multicast between distributed model partitions). %
Training and generation of an LLM in the RLHF dataflow requires %
distributed computation %
(e.g., using tensor/pipeline/data parallelism)~\cite{shoeybi2019megatron, kwon2023efficient}. %
Therefore, each node in the RLHF dataflow is a complex distributed program, corresponding to distributed computation of the respective LLM. 
Models in different nodes typically use different parallelism strategies as their workloads vary.
The edge represents data resharding, which is often a many-to-many multicast.
Consequently, 
\textit{\textbf{Flexible}} representation and \textit{\textbf{efficient}} execution of the complex and resource intensive RLHF is imperative.


Traditional RL frameworks such as RLLib~\cite{liang2018rllib} and RLLib Flow~\cite{liang2021rllib} utilize a hierarchical single-controller paradigm to run %
RL dataflows. 
A centralized controller %
assigns nodes in the dataflow to different processes and coordinates their execution order. %
Each node process can further spawn more workers to perform computation, again following the single-controller paradigm. 
However, they only provide primitives for data-parallel training and are constrained to neural networks that are at most hundreds of MB in size~\cite{liang2021rllib, liang2018rllib}.
In the RLHF dataflow, %
each node corresponds to %
an LLM with up to billions of operators, computed using some complex parallelism. 
A single-controller paradigm %
is inefficient due to the substantial overhead of dispatching operators to distributed accelerators%
~\cite{barham2022pathways, abadi2016tensorflow}.

Existing RLHF systems adopt a multi-controller paradigm to manage intra-node computation and inter-node data resharding~\cite{hu23openrlhf,NeMoAligner,xiao2023adaptive}.
Each controller independently manages the computation of one device and uses multiple point-to-point operations to coordinate data dependencies between different nodes.
This multi-controller paradigm introduces negligible dispatch overhead when performing LLM computation (detailed in \textsection\ref{sec:motivate_programming_model}).

However, without central control, it is \textit{inflexible} to implement various RLHF dataflow, as modifying a single node to adapt to different data dependencies requires
changing all dependent nodes' implementation, 
 hindering code reuse.


To address these limitations, we propose \textit{\sysname}, a flexible and efficient RLHF %
framework to easily represent and execute diverse RLHF dataflows, %
attaining high throughput.
Our key observation is that utilizing the single-controller paradigm on the inter-node level enables flexible expression of various data dependencies and easy coordination of inter-node data resharding with minimal overhead,
while integrating the multi-controller paradigm within intra-node computation enhances computation efficiency substantially.
We %
advocate a hierarchical hybrid programming model to generate RLHF dataflows.
At the node level, multiple model classes are provided that encapsulate distributed computation (training, inference and generation) of different LLMs in the dataflow into primitive APIs. %
These APIs %
can seamlessly support various parallelism strategies from the existing LLM %
frameworks, including 3D parallelism~\cite{shoeybi2019megatron}, ZeRO~\cite{rajbhandari2020zero}, and PyTorch FSDP~\cite{paszke2019pytorch}%
), and perform distributed computation under the multi-controller paradigm. %
Among the nodes, a set of transfer protocols are designed to hide the complexity of data resharding from users, as coordinated by a single controller.
This programming model abstracts away the complexity of distributed computing, allowing users to implement an RLHF dataflow in a few lines of code and run RLHF through a single process of the single controller.
It also effectively decouples intra-node computation and inter-node data transfer,
allowing %
independent optimization of each model 
without changing the code of other models in the dataflow.


Training and generation of the actor model represent major computation %
in the RLHF dataflow.
We further design a \textit{3D-HybridEngine} to enable efficient execution of training and generation of the actor model, introducing zero memory redundancy and significantly reduced communication overhead during model parameter resharding between the training and generation stages.
Our hybrid programming model also facilitates flexible placement of models onto the same or different sets of GPU devices. %
This allows us to provide an effective algorithm to optimize GPU allocation and placement of the models, with various model sizes and distinct workloads, for any RLHF dataflow.
Our contributions in designing \sysname{} are summarized as follows:




\noindent$\bullet$ We propose a hierarchical hybrid programming model for conveniently building the RLHF dataflow. This programming model enables efficient distributed execution of intra-node computation and flexible inter-node data resharding and transfer, for %
various RLHF algorithms (\textsection\ref{sec:programming_model}). 


\noindent$\bullet$ We design a 3D-HybridEngine that executes training and generation of the actor model with high computation efficiency and zero-redundancy transition between the training stage and the generation stage %
(\textsection\ref{sec:hybrid_engine}).

\noindent$\bullet$ We devise an effective mapping algorithm to automatically identify optimized %
GPU allocation and placement of each node (model) in the RLHF dataflow %
(\textsection\ref{sec:auto_mapping}).


\noindent$\bullet$ 
We conduct extensive experiments comparing \sysname{} with state-of-the-art RLHF systems~\cite{hu23openrlhf, yao2023deepspeedchat, NeMoAligner} under various RLHF algorithms, model sizes and %
cluster scales. %
Our evaluation 
demonstrates 1.53$\times$$\sim$20.57$\times$ throughput improvements.

We have open-sourced \sysname{} and believe that \sysname{} can boost future RLHF research and development.






