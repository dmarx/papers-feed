\section{Transfer Protocols} \label{appendix:transfer_protocols}
We implemented transfer protocols that cover all common use cases of data resharding between models in RLHF dataflow. 
Users can utilize these pre-defined protocols to generate any RLHF dataflow. Moreover, 
Users can easily define their own transfer protocols by implementing a collect function and a distribute function. Transfer protocols decoupled the complicated data resharding and distributed training. We denote \textit{p, t, d} as the rank of the worker in pipeline-, tensor- and data-parallel group respectively. We illustrate these predefined protocols in Table~\ref{tab:transfer_proto}.
\begin{table*}[htbp]
\caption{Key functions provided in each model class. The users can use these provided functions to construct various RLHF algorithms in a few lines of code.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|m{10cm}}
\toprule
Model & APIs & Computation & \multicolumn{1}{c}{Interpretation}  \\ 
\midrule
\multirow{3}{*}{Actor}  & \texttt{generate\_sequence}      & \makecell{auto-regressive\\generation} & Based on a batch of prompts, the actor model generates a batch of responses and returns the log probability of each token in the responses. \\
\cmidrule{2-4}
& \texttt{compute\_log\_prob}     & a forward pass   & The actor model computes the log probability of each token in the prompts and responses. This log probability is the same as the return log probability when performing generation using the same model precision. (Optional in PPO)  \\
\cmidrule{2-4}
& \texttt{compute\_loss}     & a forward pass   & The actor model computes the pretrain loss based on the pertaining dataset~\cite{bai2022training, daiSafeRLHFSafe2023, ouyang2022training}.  \\
\cmidrule{2-4}
& \texttt{update\_actor}  & \makecell{a forward, backward pass\\and model update} & Based on the advantages, returns (calculated from \texttt{compute\_advantage}) and pertaining loss, the actor model calculate the training loss and update its weights. We implement various loss for diverse RLHF algorithms including PPO~\cite{ouyang2022training}, Safe-RLHF~\cite{daiSafeRLHFSafe2023}, ReMax~\cite{li2023remax}, GRPO~\cite{shao2024deepseekmath} and others. \\
\midrule
\multirow{2}{*}{Critic} & \texttt{compute\_values}         & a forward pass & The critic model computes the values for each prompt and response.\\
\cmidrule{2-4}
& \texttt{update\_critic}  & \makecell{a forward, backward pass\\and model update} & Based on the values and returns, the critic computes a squared-error loss to update its weights. We also implement critic loss for diverse RLHF algorithms including PPO~\cite{ouyang2022training}, Safe-RLHF~\cite{daiSafeRLHFSafe2023}, ReMax~\cite{li2023remax}, GRPO~\cite{shao2024deepseekmath} and others. \\
\midrule
\makecell{Reference\\Policy}  & \texttt{compute\_ref\_log\_prob} & a forward pass  & The reference model computes the reference log probability of each token in the prompts and responses. This log probability is utilized as a benchmark to evaluate the divergence of the actor model and constrain its learning process. \\ 
\midrule
Reward  & \texttt{compute\_reward} & a forward pass      & The reward model conducts forward computation to calculate scores for a given set of prompts and responses. The rewards could be token-level or sample-level. \\
\midrule
- & \texttt{compute\_advantage}  & \makecell{numerical\\computation}    &  Based on the values rewards from the value model and reward model respectively, the function estimates the advantages on the given prompts and the current policy model's responses. This computation involves no model forward passes. \\ 

\bottomrule
\end{tabular}}
\label{tab:primitive_apis}
\end{table*}

