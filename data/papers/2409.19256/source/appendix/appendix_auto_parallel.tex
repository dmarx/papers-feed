\begin{algorithm}[htbp]
\caption{Auto Parallelism Algorithm}
\label{alg:auto_parallel}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} Device allocation $A$, minimal device allocation and model parallel size for each model in a set $A_{min}$, workload $W$, the number of GPUs per machine $U$

\STATE {\bfseries Output:} the parallelism strategy for the model in a set

\STATE {\bfseries Procedure} auto\_parallel($A$, $A_{min}$, $l$, $W$):
    \STATE $N_l = A[l]$ \textit{ // Get device allocation of the model}
    \STATE $t_{min} = A_{min}[l].t$ \textit{ // Get minimal model parallel size}
    \STATE $p_{min} = A_{min}[l].p$
    \STATE $\text{best\_para} \leftarrow \emptyset$
    \STATE $\text{best\_para.cost} \leftarrow \infty$
    \FORALL{$\text{t} \in \{ t_{min}, t_{min} + 1 ..., U$\}}
        \FORALL{$\text{p} \in \{ p_{min}, p_{min} + 1 ..., \frac{N_l}{U}$\}} 
            \STATE $\text{d} \leftarrow \frac{N_l}{\text{p} \times \text{t}}$
            \STATE $\text{para\_plan} \leftarrow (p, t, d)$ 
            \STATE $\text{cost} \leftarrow \text{simu}({para\_plan}, l, W[l])$
            \IF{$\text{best\_para}.cost > \text{cost}$}
                \STATE $\text{best\_para}.cost \leftarrow \text{cost}$
                \STATE $\text{best\_para} \leftarrow \text{para\_plan}$
            \ENDIF
        \ENDFOR
    \ENDFOR
\RETURN $\text{best\_para}$

\end{algorithmic}
\end{algorithm}

\section{Auto-Parallelism Algorithm}  \label{appendix:auto_parallel}

Algorithm~\ref{alg:auto_parallel} outlines the search process of the optimal parallelism strategy of each model.
Starting from the minimal model parallelism size of each model (to prevent OOM when colocating with multiple workers), we enumerate all feasible parallel configurations based on the number of GPUs and the number of GPUs per machine $U$. The default number of $U$ is set to 8. We use \texttt{simu} module to estimate the latency of each model based on their workload. This module includes three simulators for training, inference, and generation workload, all are analytical models following previous research~\cite{yuan2024llmrooftline, zhongDistServeDisaggregatingPrefill2024, llm-analysis}. The training and inference workload is compute-bound while the generation workload is memory-bound.
For the actor model, we first find the parallelism strategy for training and record the memory usage in the training stage. 
During actor generation, KVCache requirements are calculated using the batch size and max sequence length. If the model-parallel size for the generation stage cannot accommodate both parameters and KVCache, we increase it.
Then, we seek the optimal strategy with corresponding KVCache allocation by comparing the latency estimation.
Developing a comprehensive autoregressive generation simulator that accounts for variable KVCache sizes could further enhance the auto-mapping process in RLHF research.
