\begin{table*}[htbp]
\caption{The transfer protocols in HybridFlow.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|m{10cm}}
\toprule
Transfer Protocols & Distribute function & Collect function & \multicolumn{1}{c}{Use case}  \\ 
\midrule
\texttt{ONE\_TO\_ALL}  & Broadcast the data to all ranks.      & Gather the data from all ranks. & All the worker methods have the same input and run the ssme codes, e.g. model initialization.\\
\midrule
\texttt{3D\_PROTO}  & \makecell{Split the data, scatter across all DP ranks \\ and broadcast within the group.}      & \makecell{ Gather 
 and concatenate the data from\\  the \textit{p=-1, t=0} worker in all DP groups.}  & The model is sharded among multiple workers within each data-parallel group. The output of the model only exists in the last pipeline stage and is duplicated across the data-parallel groups. This is a typical scenario in 3D parallel training in Megatron-LM, Deepspeed, etc.\\
\midrule
\texttt{3D\_ALL\_MICRO\_DP}  & \makecell{Split the data by micro DP size, scatter across \\ all micro DP groups and broadcast \\ among all ranks within the group.}      & \makecell{ Gather 
 and concatenate the data from\\  the local\_rank=0 worker in all micro DP groups.}  & Used with HybridEngine. It is used to handle the 3D-parallel scheme of the policy model, when switching between training and inference.\\
\midrule
\texttt{3D\_PP\_ONLY}  & \makecell{Broadcast the data to all ranks.}      & \makecell{ Gather 
 and concatenate the data from\\  the \textit{t=0, d=0} worker in all PP groups.}  & Used to examine weight names as they are identical in TP and DP groups. \\
\midrule
\texttt{DP\_PROTO}  & \makecell{Split the data into batches and\\ scatter across all DP ranks.}      & \makecell{ Gather 
 and concatenate \\the data from all DP ranks.}  & Training model in data-parallel mode.\\
 \midrule
\texttt{ALL\_TO\_ALL}  & \makecell{No operation.}      & \makecell{ Gather the data from all ranks.}  & Used when debugging. Users can manually define the inputs of each worker and examine their outputs respectively.\\

\bottomrule
\end{tabular}}
\label{tab:transfer_proto}
\end{table*}

\section{Primitive APIs in HybridFlow} \label{appendix:primitive_apis}
In HybridFlow, we implemented the primitive of each model in RLHF training by inheriting the \texttt{3DParallelWorker}, \texttt{FSDP} \texttt{Worker} and \texttt{ZeROWorker}. The functions of these model classes are designed to decouple the distributed computation code and provide fundamental operations in RLHF for the users. This primitive design is compatible with the auto-regressive generation, forward pass, backward pass, and model update operations in the existing distributed inference and training frameworks. Users can easily customize the RLHF training dataflow (by adapting the numerical computation in the provided functions) according to the algorithm's design and benefit from reusing the underlying distributed computation implementation. We illustrate the meaning and the actual computations of these APIs in Table~\ref{tab:primitive_apis}. 






