\section{Introduction}
\label{sec:intro}

Creativity emerges over time in a complex interplay of factors. Human judgments of creativity are often biased by personal tastes, expectations, and hindsight. Scholarly interest in creativity has been found in a number of disciplines, primarily psychology, cognitive sciences and more recently computer science with the advent of large language models. As AI assisted writing gets more and more popular \cite{AIassistedWriting1,AIassistedWriting2} especially in the wake of an enormous movement in the writing industry \cite{WGA} it is rather timely and urgent to evaluate whether large language models understand what it takes to be creative. Creativity is often difficult to evaluate because it is a complex, multifaceted concept that is deeply subjective, contextual, and hard to judge objectively. Several empirical studies have been conducted in psychology and cognitive science of the processes through which creativity occurs.Interpretation of the results of these studies has led to several possible explanations of the sources and methods of creativity. Eminent psychologist J. P. Guilford  drew a distinction between convergent and divergent thinking \cite{guilford1967nature}. Convergent thinking involves aiming for a single, correct solution to a problem, whereas divergent thinking involves creative generation of multiple answers to a set problem. Divergent thinking is sometimes used as a synonym for creativity in psychology literature or is considered the necessary precursor to creativity \cite{RUNCO2011400}. Built on J.P. Guilford's work Ellis Paul Torrance devised one of the widely accepted test of creativity \textit{Torrance Tests of Creative Thinking} \cite{torrance1966torrance} that originally involved simple tests of divergent thinking and other problem-solving skills, which were scored on four scales: \textit{Fluency, Flexibility, Originality, Elaboration}

Creative writing constitutes a promising topic for interdisciplinary conversation \cite{Doyle1998TheWT}. Several researchers have shown the promise of large language models in creative writing tasks \cite{mirowski2023cowriting,yang2022re3,lee2022coauthor} however there has been little consensus in the field of Computer Science on how to empirically evaluate creative writing. To tackle this issue we propose a diagnostic suite of tests that aims to evaluate the creativity in a total of 48 short stories written by both professional writers as well large language models such as GPT3.5, GPT4 and Claude.We construct the foundation of our evaluation through the prism of the \textit{Torrance Tests of Creative Thinking (TTCT)}. Towards this we first we recruit 8 creative writing experts and elicit 4 empirical measures from them across each of the dimensions of \textit{Fluency, Flexibility, Originality} and \textit{Elaboration} in the context of Creative Writing (Section \ref{sec:approach}). These single sentence measures collected from the experts are then subsequently expanded to detailed and quantifiable natural language prompts with the help large language models, followed by careful scrutiny from human experts (Section \ref{sec:prompting}).We then recruit 8 different creative writing experts and ask them alongside leading LLMs such as GPT3.5, GPT4 and Claude to perform the tests described above. The primary objectives of our investigation encompass the resolution of the following research queries:

\begin{itemize}
    \item Can creative writing experts discriminate between stories written by professional writers vs LLMs? 
    \item Are there certain tests for creativity that emerge as discriminative thereby drawing the distinction between good and mediocre writing?
    \item Do LLMs align with expert human judgements across the various dimensions of Creativity and can they provide correct explanation for their judgements?
\end{itemize}


