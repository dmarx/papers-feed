\section{Discussion}

\subsection{Are experts good at detecting AI written texts or are they simply good at detecting lower quality texts?}
During our evaluation, expert annotators were tasked with predicting a story's author from three categories (Written by an Expert, Written by an Amateur, or AI-generated), even though none of the stories they annotated were selected from amateur authors. This mismatch was intended to reduce the likelihood of annotators tracing back the creation of the dataset and provide a more granular scale for their prediction.

Our goal is to design a rubric for creative writing evaluation that rates higher-quality text as better compared to lower-quality text. It is by no means designed to penalize AI-generated text. The primary reason for this was that it is really hard to detect AI-generated text because current large language models are good at generating fluent and human-like text. Certain traits are common in fiction written by an AI and an amateur writer which makes it difficult for the task of authorship attribution. One of our experts remarked \textit{There is, at best, little subtext in an amateur-written story, and likely none at all. This is the aspect that sometimes made me hesitate as to whether a story was written by AI or an amateur writer.} While both AI and amateur-written fiction gravitate towards clich√©s, several experts mentioned AI \textit{writing tics} that led them to believe it is not written by a human. Unlike text written by a mediocre writer, AI-written text would often contain predictable formations: strong topic sentences at the top of paragraphs; and summary sentences at the end of paragraphs; Some experts also mentioned peculiar sentence construction that was common in AI writing such as [time-orienting clause][comma][exposition]. Certain LLMs such as GPT4 also conflate good writing with ornamental use of language. This leads to generated text that is full of lofty superficial figurative language unlike text written by amateurs. Finally, AI's voice in writing tended to be overly moralizing, unlike amateur writers. This evidence somewhat makes us believe that experts might have an easier time discriminating when lower-quality text is produced by a human vs a machine.


\subsection{LLMs: from study subject to HCI research tool}

This work leveraged LLMs as a tool to facilitate research, such as generating the first pass of the ``expanded expert measure'' text, or clustering expert-written explanations. Recent work has defined researchers and model developers as a main user category of LLMs \cite{chen2023next}, with recent efforts for example making use of models to generate synthetic HCI research data \cite{hamalainen2023evaluating}. We reflect on the utility and limitations of using LLMs as a research tool in our work. We used an LLM in Section~\ref{sec:prompting} to assist in writing the ``expanded expert measure'' descriptions, filling in the missing detail and context in expert-written descriptions, by adding definitions of technical terms or providing context or an example. LLMs were also useful in situations where we had to process a large volume of textual data, such as the clustering of the 2016 expert-written explanations we performed in Section~\ref{sec:analysis}. Manually clustering such explanations would have been prohibitively time-consuming, and LLMs enabled us to swiftly extract initial insights, which we discussed and refined. In either situation, we approach the use of LLMs in a human-AI collaborative framework, where the output of the LLM is validated by humans (in this case, the authors of the paper) and serves as an initial step in the research process rather than the final product.

\subsection{Towards interactive LLM-based creative co-writing} 
Our experimental results and the analysis of the expert explanations highlight the limitations of current LLMs in generating both high-quality fictional stories as well as assessing the creativity of such existing stories. With the rapid progression in LLM development, we make available a corpus containing expert evaluations of TTCW assessments. We believe such a contribution will facilitate the evaluation of the upcoming model's abilities for creative writing assessment. If LLMs are capable of producing TTCW assessments that correlate positively with expert judgments, future work can explore new opportunities for creative LLM-based co-writing interfaces.

In particular, we envision LLMs assisting \textit{Planning} and \textit{Reviewing}, crucial phases in the cognitive process theory of writing \cite{flower1981cognitive}. In prior work \citet{gero2023social} states that writers expressed the importance of specificity in the feedback instead of generic feedback like \textit{this might be a bit boring}. We hope that the TTCW tests can provide the structure in future work looking to provide targeted feedback on writing. Further \citet{ippolito2022creative} recently pointed out that professional writers constantly felt that LLM-generated text is rife with cliches and overused tropes. Metrics that quantify elements like originality in theme, structural flexibility, or rhetorical complexity could guide creative writing support tools\footnote{\url{https://www.sudowrite.com/}} built using current LLMs, thereby improving \textit{planning} and \textit{translation} \cite{10.1145/3461778.3462050}.

\subsection{Is TTCW universal in nature?}

The development of TTCW was informed by the expertise of 8 field specialists, with each test reflecting the insights of 1-3 experts. However, TTCW cannot be considered a universal benchmark for creative writing due to its reliance on a narrow expert base, potentially echoing Western literary biases due to the panel's background in "highbrow" literary fiction. This might marginalize other cultural narratives and styles. Specifically, TTCW evaluates creative writing through various lenses: TTCW Flexibility1 values diverse narrative perspectives, potentially disadvantaging stories focused on a singular viewpoint. TTCW Fluency2 and TTCW Flexibility2 assess the balance in storytelling elements, which might not favor experimental works that blend narrative techniques. Despite focusing on short stories, TTCW also critiques story closure in TTCW Fluency4, contrasting with plays that seek non-cathartic endings for specific impacts, as seen in the Brechtian tradition. Moreover, TTCW Originality3 rewards formal innovation, possibly underrating stories that creatively navigate within strict formal boundaries, like those adhering to traditional mythic structures. Thus, while TTCW offers a structured approach to evaluating creative writing, its scope and applicability are affected by certain choices and specific literary conventions it prioritizes.