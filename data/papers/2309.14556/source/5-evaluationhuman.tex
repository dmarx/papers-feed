\subsection{Evaluation Protocol}

\subsubsection{Expanded Expert Measure and Prompt Design} \label{sec:prompting}

\begin{table*}[!ht]
\centering
\small
\begin{tabular}{|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Expert \\ Measure\end{tabular}               & Does the writer make the fictional world believable at the sensory level?                                                                                                                                     \\ \hline
\begin{tabular}[c]{@{}l@{}}Expanded\\ Expert\\ Measure (M)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sensory details pertain to the five senses - sight, sound, touch, taste, and smell. An effective \\ writer can use these elements to paint a detailed picture of the story's environment, making\\ it feel tangible and real to the reader.\\ \\ For example, describing the specific colors and shapes in a scene, the sounds that fill a space,the \\ textures and temperatures that a character comes into contact with, the flavors of the food they \\ eat, or the scents that fill the air, can all contribute to creating a sensory-rich and believable world.\\ \\ By stimulating the reader's senses, the writer can make the reader feel as though they're \\ experiencing the events of the story firsthand.This level of detail contributes to the believability of\\ the world, even if it's a completely fictional or fantastical setting. It helps the reader to suspend\\ disbelief and become more deeply invested in the narrative.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Human\\ Instruction\end{tabular}             & \begin{tabular}[c]{@{}l@{}}\{\{M\}\}\\ \\ Based on the story that you just read, answer the following question.\\ \textit{\color{blue}Does the writer make the fictional world believable at the sensory level?}\\ -Yes \\ -No \\\\ Reasoning : \end{tabular}                                                                       \\ \hline
\begin{tabular}[c]{@{}l@{}}LLM\\ Instruction\end{tabular}               & \begin{tabular}[c]{@{}l@{}}\{\{M\}\}\\ \\ Given the story above, list out the elements in the story that call to each of the\\ five senses. Then overall, give your reasoning about the question below and give\\ an answer to it between 'Yes' or 'No' only\\ \\ \textit{\color{blue} Q) Does the writer make the fictional world believable at the sensory level?}\end{tabular}                                                                                                                                                                                                                                 \\ \hline
\end{tabular}
\vspace{2ex}
\caption{\label{prompting1}Expert suggested question for World Building and setting (Row1) ; Expanded Expert Measure (Row2); Elucidated prompt designed for other expert humans
(Row3); Elucidated quantifiable prompt designed for Large Language Models that elicit Chain of Thought Reasoning(Row4) }
\vspace{-5ex}
\end{table*}

We want these tests described above to be understandable by both other creative writing experts or even LLMs, such that they can be used for evaluation purposes. An expert suggested questions for empirically evaluating creative writing might frequently elicit ambiguity in Large Language Models or even other creative writing experts. In order for LLMs or other experts to comprehend the suggested questions in Section
\ref{CreativityTest}, we attempt to expand them by adding more details.
Recent pre-trained LLMs (e.g., GPT-4 \cite{OpenAI2023GPT4TR} GPT3.5 \cite{ChatGPT}) can engage in fluent, multi-turn conversations out of the box, substantially lowering the data and programming-skill barriers to creating passable conversational user experiences. People can improve LLM outputs by prepending prompts—textual instructions and examples of their desired interactions—to LLM inputs. The prompts steer the model towards generating the desired outputs, raising the ceiling of what conversational UX is achievable for non-AI experts. To elucidate these questions we prompt GPT4 with the following instruction: \textit{What do creative experts mean when they say the following: \{\{expert question\}\}}. Once GPT4 gives a response 3 domain experts carefully verify the response and edit it where required. Table \ref{prompting1} (Row2) 
shows the human-verified GPT4 expanded expert measure in response to the input prompt. Table \ref{prompting1} (Row3; Human Instruction) shows the final instruction given to human experts during the evaluation of our stories that contains the expanded expert measure \{\{M\}\} in addition to the original yes/no question.More examples of Human Instructions for the remaining 13 tests are provided in Section \ref{allprompts} in the Appendix.

\begin{table}[!ht]
\centering
\small
\begin{tabular}{|l|l|l|l|}
\hline
ID & Profession  & Gender & Age                \\ \hline
E1 & Lecturer of Creative Writing & Male & 42 \\ \hline
E2 & Lecturer of Creative Writing & Male & 32 \\ \hline
E3 & Professor of Creative Writing & Male & 46  \\ \hline
E4 & Professor of Creative Writing & Female & 43 \\ \hline
E5 & Literary Agent & Male & 29 \\ \hline
E6 & Literary Agent & Female & 30  \\ \hline
E7 & Writer with an MFA in Fiction & Non-Binary &  25       \\ \hline
E8 & Writer with an MFA in Fiction & Male & 24      \\ \hline
E9 & Writer with an MFA in Fiction & Male & 28      \\ \hline
E10 & Writer with an MFA in Poetry  & Male & 30                      \\ \hline
\end{tabular}
\vspace{2ex}
\caption{\label{creativeexperts}Background of creative writing experts recruited for evaluating the stories from our test set}
\vspace{-3ex}
\end{table}

\begin{table*}[!ht]
\small
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Dimension & Test & GPT3.5 & GPT4 & Claudev1.3 & NewYorker & Expert Agreement \\ \hline
\multirow{5}{*}{Fluency} & Understandability \& Coherence & 22.2 & 33.3 & 55.6 & \textbf{91.7} & 0.27 \\ \cline{2-7}
& Narrative Pacing & 8.3 & 52.8 & 61.1 & \textbf{94.4} & 0.39 \\ \cline{2-7}
& Scene vs Exposition & 8.3 & 50.0 & 58.3 & \textbf{91.7} & 0.27 \\ \cline{2-7}
& Literary Devices \& Language Proficiency & 5.6 & 36.1 & 13.9 & \textbf{88.9} & 0.37 \\ \cline{2-7}
& Narrative Ending & 8.3 & 19.4 & 33.3 & \textbf{91.7} & 0.48 \\ \hline\hline
\multirow{3}{*}{Flexibility} & Emotional Flexibility & 16.7 & 19.4 & 36.1 & \textbf{91.7} & 0.32 \\ \cline{2-7}
& Perspective \& Voice Flexibility & 8.3 & 16.7 & 19.4 & \textbf{72.2} & 0.44 \\ \cline{2-7}
& Structural Flexibility & 11.1 & 19.4 & 30.6 & \textbf{88.9} & 0.39 \\ \hline\hline
\multirow{3}{*}{Originality} & Originality in Form & 2.8 & 8.3 & 0.0 & \textbf{63.9} & 0.41 \\ \cline{2-7}
& Originality in Thought & 2.8 & 44.4 & 19.4 & \textbf{91.7} & 0.40 \\ \cline{2-7}
& Originality in Theme \& Content & 0 & 19.4 & 11.1 & \textbf{75.0} & 0.66 \\ \hline\hline
\multirow{3}{*}{Elaboration} & World Building \& Setting & 16.7 & 41.7 & 58.3 & \textbf{94.4} & 0.33 \\ \cline{2-7}
& Character Development & 8.3 & 16.7 & 16.7 & \textbf{61.1} & 0.31 \\ \cline{2-7}
& Rhetorical Complexity & 2.8 & 11.1 & 5.6 & \textbf{88.9} & 0.66 \\ \hline\hline
\multicolumn{2}{|c|}{Average} & 8.7 &27.9 & 30.0 & \textbf{84.7} & 0.41 \\ \hline
\end{tabular}
\vspace{2ex}
\caption{\label{absolutehumaneval} Average passing rate on individual TTCW, based on annotations of 10 creative writing experts across the 48 stories in our collection, authored by GPT3.5, GPT4, Claude, and expert human writers published in the New Yorker along with agreement measures (Fleiss Kappa) on individual test.}
\end{table*}

\subsubsection{Expert Evaluation Protocol}
\label{experteval}
We developed an evaluation protocol tailored specifically for experts in the domain of creative writing. The protocol, designed to be completed in approximately 2 to 2.5 hours, centered around a rigorous assessment of tuples of four distinct stories (one New Yorker story and the associated LLM-generated stories by the three LLMs) using the TTCW. The study was structured as follows:

\begin{enumerate}
    \item The four stories in a group were shuffled, and anonymized (i.e., the author of the story was not visible to the evaluator).
    \item The expert evaluator read the first story in its entirety and then administered the fourteen TTCW tests by assigning a Yes/No label and providing a justification for the label.
    \item Upon completing the evaluation of the first story in the group, the evaluator proceeded to read and evaluate the second, third, and fourth stories in the group respectively. The evaluators were also allowed to edit their responses at any point of time during the entire process.
    \item Once the evaluator had completed the TTCW evaluation of the four stories within a group, they were asked to rank all four stories in terms of subjective preference, and were asked to make an estimated guess of each story's origin: choosing from ``An experienced writer'', ``An amateur writer'', or ``Written by AI.'' The exact formulation of each question is given in Figure~\ref{relev} in the Appendix.
\end{enumerate}

In preliminary trials conducted by the authors of the paper, the entire task completion was observed to range from 2-2.5 hours. Consequently, an \$80 remuneration was determined to appropriately acknowledge the expertise of participants and encourage them to provide detailed justifications in their TTCW assessments. We note that participants were not provided details on the process used to create the groups of four stories, and were not told that each group consisted of one human-written story and 3 LLM-generated stories. Participants were explicitly instructed to avoid using search engines, which might reveal the origin of the New Yorker story. We also ensured beforehand that the participants were not familiar with any of the stories within a given group. Finally, participants were permitted to take breaks during the study but were encouraged to complete the entire task within a 24-hour window, so they would clearly remember each story when completing the final comparative task.

\subsection{Participant Recruitment}
\label{participant}

To test the robustness and validity of TTCW-based evaluation, we chose to recruit a new set of experts to conduct the evaluation and not re-hire the ones from our formative study that played a role in the creation of the tests. We posit that such a choice demonstrates the fact that the TTCW can be administered by any expert provided solely with the tests and their expanded explanations. We recruited 10 participants on the \textit{User Interviews} platform and listed their background in Table~\ref{creativeexperts}. Four of these participants are associated with the creative writing departments at leading American academic institutions, with considerable experience conducting undergraduate and graduate-level courses. Two participants function as literary agents at a top-tier, full-service US literary agency representing well-recognized authors, and four are professional writers with a Master of Fine Arts in Fiction or Poetry.

To experimentally analyze the reproducibility and validity of the TTCW, we randomly assigned each story group to 3 distinct experts. This allows us to study the agreement levels between experts on individual tests as well as in aggregate. After each task, an expert participant was given the option to receive another group of stories, and our participants completed on average 3.6 tasks over 3 weeks, for a total of 36 assessments (i.e., 3 for each of the twelve groups). Because each assessment contains four stories, and each story was evaluated using the 14 TTCW, we collected a total of 2,016 binary labels and expert-written justifications for these labels.