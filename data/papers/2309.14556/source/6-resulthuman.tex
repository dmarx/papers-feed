\subsection{Results} \label{ref:humanresult}
\subsubsection{Analyzing Pass Rates of Stories}
\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/Creativity_Aggregate_Performance.pdf}
    \caption{Distribution of aggregate TTCW results, in which only the number of tests passed is retained. }
    \label{fig:ttcw_aggregate_histogram}
\end{figure*}

\begin{figure*}[!ht]
    \subfloat[Likert plot showing ranking of stories within an individual group based on all expert preferences]{\frame{\includegraphics[width=0.44\textwidth]{figures/relativerank.png}}} \quad
    \subfloat[Likert plot showing how often authors attributed the source of the stories correctly]{\frame{\includegraphics[width=0.53\textwidth]{figures/relativeauthor.png}}} 
    \caption{\label{relativehumaneval1} \textbf{Relative Evaluation} Left figure showing ranking preference assigned to each story within a group. Right figure showing how creative experts attributed any given story from The NewYorker or 3 LLMs to one of the options between An experienced writer, An amateur writer, or An AI }
\end{figure*}

Table ~\ref{absolutehumaneval} summarizes the average \textit{passing rate} on the 14 TTCW for each of the four story types (GPT3.5, GPT4, Claude-v1.3, and New Yorker). Passing rate here corresponds to the percentage of time expert participants answer `Yes' to an individual TTCW for any given story. The New Yorker stories widely achieve the highest passing rate on all fourteen tests, with an overall pass rate of 84.7\%. In other words, individual New Yorker stories are assessed to pass 11.9 of the 14 TTCW on average. When examining performance on individual tests, no test receives a pass rate of 100\%, confirming that no test is an absolute requirement in high-quality creative writing, and experimentally justifying the need to conduct the TTCW tests as a set (Design Principle 4).

Moving to the performance of the LLM-generated stories, passing rates are much lower, with GPT3.5 stories passing less than 10\% of TTCW, while GPT4 and Claude v1.3 are closer to 30.0\%. In other words, LLM-generated stories pass between a third and a tenth of the TTCW compared to human-written New Yorker stories. When breaking down LLM-story performance across the Torrance dimensions, all models achieve their highest pass rate on the Fluency dimension, and Claude v1.3 achieves the highest performance on average across Fluency, Flexibility, and Elaboration, while GPT4 scores highest on the Originality dimension. This experimental finding is surprising as Claude v1.3 is an LLM that is smaller in size(52B) than GPT4 \footnote{https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/}. 

\subsubsection{Reproducibility of TTCW}

Since we collected three independent TTCW evaluations for each story, we can report the agreement levels of experts when they conduct the tests individually, and their assessment in aggregate. We compute the Fleiss $\kappa$ agreement across all annotations and report the interrater agreement level of each test in Table ~\ref{absolutehumaneval}. Individual test agreement ranges from 0.27 to 0.66, and averages at 0.41, suggesting moderate agreement on most of the individual TTCW.

Since the TTCW are designed to be additive, we further compute an aggregate score for each story by counting the number of TTCW tests a story passes. We visualize the results of this aggregate measure in Figure ~\ref{fig:ttcw_aggregate_histogram}. Since the aggregate measure is numerical (ranges from 0 to 14), we use Pearson correlation to measure agreement among experts. At this aggregate level, we obtain a correlation($\rho$) of 0.69, showing strong agreement among experts on the number of tests a story passes. In other words, even though experts reach slightly lower agreement on which exact TTCW a story passes or fails, they achieve strong agreement on the number of tests a story passes overall. This experimental finding confirms the importance of Design Principle 4, and the need for the tests to be performed as a set to achieve a reproducible evaluation of creativity for a given short story. When the objective is to evaluate the broad creativity in a short story, we recommend that all fourteen tests be administered as a set by one expert annotator, rather than by different experts or administering only an individual test, as this increases the reproducibility of the results.
\subsubsection{Comparative Evaluation Results}

The final portion of the evaluation protocol asks expert participants to rank the four shuffled stories in terms of subjective preference, as well as guess each story's origin between ``An experienced writer'', ``An amateur writer'', or ``Written by AI''. Figure~\ref{relativehumaneval1} summarizes the results from this final portion of the study.

Looking at the ranking results, human-written New Yorker stories were ranked as the most preferred story 89\% of the time, while the GPT-3.5-generated stories ranked as least preferred roughly two-thirds of the time. When comparing GPT-4 and Claude, Claude is almost twice as likely to rank as second (behind the human-written story) and was the most preferred on three of the four assessments in which the New Yorker story was not chosen as the most favored. These ranking results confirm and accentuate the observation from the test passing rates analysis that Claude V1.3 generates higher-quality short stories than models in the GPT family.

The attribution results paint a similar picture, with New Yorker stories predominantly attributed to an experienced writer, while LLM-generated stories get attributed to AI or an amateur writer. Interestingly, Claude V1.3 is more likely to be attributed to an amateur writer than an AI, whereas GPT3.5 and GPT4 stories are 80\%+ attributed to AI. One hypothesis for such behavior could be that the participants in our study might be more familiar with text written by OpenAI models, as these models are commercially more successful, providing an element of surprise to Claude-generated text.

\subsection{What can we infer from expert explanations of administering the TTCW?} \label{sec:analysis}
\input{tables/expert_opinion_on_llm}
\input{tables/explanationthemes}

Our annotation effort in Section~\ref{experteval} required experts to not only annotate for binary labels but also provide a justification paragraph accounting for their assessment. In Table~\ref{expertfeedbackcluster}, we provide examples of such justification for one of the TTCW tests for Originality. To gain insights into the main justifications experts provide for a story to pass or fail a TTCW test, we performed a manual thematic analysis of the expert justifications. We organized the results into a set of minimal phrases that often appear when a story passes or fails each TTCW test. Recent work has shown the utility of LLMs in clustering \cite{viswanathan2023large}. Based on these findings we asked the GPT4 model to cluster explanations across a given TTCW dimension into recurrent and broader representative themes. Three authors of the paper then manually verified these themes to ensure correctness. The outcome is summarized in Table~\ref{expertexpl1} for Fluency and Flexibility tests, and Table~\ref{expertexpl2} in the Appendix for Originality and Elaboration tests. The underlying themes found across the explanations reaffirm prior findings from \citet{ippolito2022creative} where writers found LLM-generated stories experiencing difficulty in maintaining a style/voice and easily reverting to tropes and repetition as well as those from \citet{mirowski2023cowriting} where screenwriters complained about the lack of subtext and character motivation.

\input{tables/expert_differentiate}


\subsection{How do experts differentiate between human-written and LLM-generated stories?}
\label{expertvsai}

In an optional exchange with the expert participants (Section ~\ref{participant}) who participated in the annotation (Section ~\ref{experteval}), they were given the opportunity to describe how they differentiated between AI-generated and human-written stories. Collected responses showed that expert evaluators did not make a decision on which stories were AI-generated based on factors such as grammatical characteristics. Table \ref{expertfeedback} lists the replies of a few experts, which we color-coded to highlight the recurrent issues that led them to believe that a story is AI-generated. Feedback was often aligned with individual TTCW, demonstrating that experts discriminated AI vs human written stories based on creative execution rather than spurious cues.

In particular, E5, E4, E1 thought AI struggles at \textbf{\color{blue}\underline{Narrative Ending}}. E5 and E4 highlighted that AI-generated stories would forestall the ending by getting bigger in scope. E1 highlighted that AI-generated stories would have multiple disparate endings. E5, E4, E1, and E2 all highlighted that AI-generated stories would often contain abstruse and incoherent metaphors that do not add meaning or extremely cliched or simple metaphors thereby demonstrating poor \textbf{\color{ForestGreen}\underline{Language Proficiency and use of Literary Devices}}. E1 highlighted one such example in a story - \textit{However, she managed to laugh louder and louder until her laughter transformed into an embrace of the sun's atmosphere}.

E1 and E2 further highlighted that characters in AI-generated stories have poor \textbf{\color{red}\underline{Rhetorical Complexity}} and are often lacking in subtext. E1 further added that AI-generated stories operate in a nearly opposite and Drax-like fashion in which there is only literal meaning, and that literal meaning is often nonsensical, or at least presented without any of the context that might make it seem like something a human would say or do.
E1 highlighted an exchange below from an AI-generated story:

\begin{center}
    Sarah: \textit{``We've been avoiding the inevitable, Max. During our time here we've grown closer, and now that it's almost over, we can't just pretend like it never happened.''} \\
    Max:  \textit{``I understand, Sarah. But how do we move forward? How do we navigate this complexity without unraveling everything we've built?''}
\end{center}

where he exclaimed that these statements make hackish sense as clumsy exposition directed at the reader, and no sense at all as sentences spoken from one alleged human being to another.
Both E5, E3, and E1 agreed on poor \textbf{\color{orange}\underline{Character Development}} in AI-generated stories where a character would appear and then disappear without having any impact. E3 and E2 also highlighted issues in \textbf{\color{brown}\underline{Narrative Pacing}} where stories would either spiral into a repetitive pattern or rapidly accelerate through time after the first scene. E1 and E4 also highlighted \textbf{\color{purple}\underline{Unusual Syntax}} in sentence structure in AI-generated stories and repetition of certain words and phrases across stories.
