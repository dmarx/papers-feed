\section{TTCW Implementation with LLMs as Assessors}
\label{sec:llmeval}
Expert annotation such as the one we perform in Section~\ref{sec:data} is costly: based on our evaluation protocol, evaluating a 1500-2500 word short story with a qualified expert costs \$20, and requires roughly 30 minutes of the expert's time. Prior work has shown the promise of using LLMs in text evaluation. For instance, GPT3.5 and GPT4 are effective at evaluating the factual consistency of a summary to its document \cite{laban2023llms}, or measuring the coherence of a summary \cite{gao2023human}. GPTEval \cite{liu2023gpteval} employs the framework of using large language models with chain-of-thoughts (CoT) \cite{wei2022chain} to assess the quality of NLG outputs. Recent work has also applied LLM-based evaluation to the creative domain \cite{rajani2023llm_labels}, claiming that GPT4 can achieve a high correlation with humans when evaluating brainstorming or creative generation tasks. In this section, we describe our TTCW implementation with LLMs as assessors to understand LLM's ability to assess creative writing. 


We apply a similar evaluation protocol to the one described in Section \ref{sec:data}. We use the same data selection and the same three LLMs:
GPT3.5, GPT4, and Claude, and prompt them to answer the 14 individual TTCW tests for the 48 stories in our collection. 

Prior work \citet{wei2022chain} has shown how generating a \textit{chain of thought} -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. Taking advantage of this we design the prompts/instructions for large language models in a slightly different fashion than for the human experts as can be seen in Table \ref{prompting1}(Row 3 (Human Instruction) vs Row 4 LLM Instruction). To help the model make an informed decision we first ask it to list out elements specific to any given test such as ``elements in the story that call to each of the five senses" for the World Building and setting test followed by asking it to decide overall and then provide its reasoning before choosing an answer between `Yes' or `No'.
The exact prompt contains (1) the story, (2) the expanded TTCW context, (3) the TTCW question, and (4) an LLM-specific instruction guiding the model to perform the task in a chain-of-though manner. We then measure model agreement with the \textit{majority vote} of the three experts that conducted the test, using Cohen's Kappa.

\begin{table*}[!ht]
\small
\centering
\begin{tabular}{ll|lll|}
\hline
\multicolumn{1}{|l|}{Dimension}                    & Test                                                                               & \multicolumn{1}{l|}{GPT3.5} & \multicolumn{1}{l|}{GPT4}  & Claude \\ \hline
\multicolumn{1}{|l|}{\multirow{5}{*}{Fluency}}     & Understandability \& Coherence                                                     & \multicolumn{1}{l|}{-0.01}   & \multicolumn{1}{l|}{-0.01} & -0.17                                                      \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Narrative Pacing                                                                   & \multicolumn{1}{l|}{0.05}   & \multicolumn{1}{l|}{0.0}   &  -0.22                                                    \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Scene vs Exposition                                                                & \multicolumn{1}{l|}{-0.03}  & \multicolumn{1}{l|}{-0.08}  &  -0.23                                                          \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & \begin{tabular}[c]{@{}l@{}}Literary Devices \& Language Proficiency\end{tabular} & \multicolumn{1}{l|}{0.04}   & \multicolumn{1}{l|}{-0.09} &  -0.11                                                         \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Narrative Ending                                                                   & \multicolumn{1}{l|}{-0.02}  & \multicolumn{1}{l|}{0.02} &  0.02                                                          \\ \hline\hline
\multicolumn{1}{|l|}{\multirow{3}{*}{Flexibility}} & Emotional Flexibility                                                              & \multicolumn{1}{l|}{-0.04}  & \multicolumn{1}{l|}{0.0}   & 0.09                                                          \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Perspective \& Voice Flexibility                                                   & \multicolumn{1}{l|}{0.0}    & \multicolumn{1}{l|}{0.26}  & 0.14                                                       \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Structural Flexibility                                                             & \multicolumn{1}{l|}{-0.04}  & \multicolumn{1}{l|}{0.0}   &  -0.07                                                   \\ \hline\hline
\multicolumn{1}{|l|}{\multirow{3}{*}{Originality}}                             & Originality in Form                                                             & \multicolumn{1}{l|}{0.08}   & \multicolumn{1}{l|}{0.09}  &  0.03                                                         \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Originality in Thought                                                             & \multicolumn{1}{l|}{0.19}   & \multicolumn{1}{l|}{0.31}  &  0.15                                                          \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Originality in Theme \& Content                                                    & \multicolumn{1}{l|}{0.06}   & \multicolumn{1}{l|}{-0.01} &  0.18                                                        \\ \hline\hline
\multicolumn{1}{|l|}{\multirow{3}{*}{Elaboration}} & World Building \& Setting                                                          & \multicolumn{1}{l|}{0.0}   & \multicolumn{1}{l|}{0.00}  &  0.09                                                            \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Character Development                                                              & \multicolumn{1}{l|}{-0.08}  & \multicolumn{1}{l|}{0.02}  &  0.00                                                          \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                             & Rhetorical Complexity                                                              & \multicolumn{1}{l|}{0.0}    & \multicolumn{1}{l|}{0.0}   &  0.02                                                          \\ \hline\hline
\multicolumn{2}{|c|}{Average}                                                                                                           & \multicolumn{1}{l|}{0.016}   & \multicolumn{1}{l|}{0.035}  & -0.006                                                     \\ \hline
\end{tabular}
\vspace{2ex}
\caption{\label{llmhumancor} Correlation between LLM-administered TTCW and expert annotations (Cohen's Kappa) on all 48 stories}
\vspace{-3ex}
\end{table*}

\subsection{Results}

Table~\ref{llmhumancor} summarizes correlation results between LLM-based and expert-based TTCW assessments. On average, we find that none of the LLMs produce assessments that correlate positively with expert assessments, with correlation averages close to zero. GPT4 is the only model to obtain correlations above 0.2 on two of the fourteen tests, yet this still does not qualify as moderate agreement.This empirical result contrasts with prior work: even though GPT4 has been shown to have some ability to evaluate creativity in short-form tasks (such as responses with less than 100 words) \cite{rajani2023llm_labels}, our work shows that this result does not extend to longer-form evaluation. We note that although the prompts we utilized were zero-shot in nature (i.e., these prompts did not include example binary labels and justifications from experts), we experimented with few-shot prompts for a couple of the TTCW tests and did not obtain any significant correlation gains.

Yet LLM-administered TTCW would be a crucial building block in improving model-generated stories. Assuming that an automated method could produce reliable TTCW outcomes, it could be used in iterative algorithms such as Self-Refine \cite{madaan2023self} to iteratively edit a draft story until it passes a large proportion of tests. With this in mind, we release the TTCW benchmark which contains all binary judgments we collected and expert justifications, with the hope that the community can use it as a tool to track progress in the evaluation of the creative capabilities of LLMs.


To get a deeper understanding of how expert and LLM explanations differ we take a closer look at them. We explicitly prompted LLMs to do step-by-step reasoning before arriving at any verdict and this was often reflected in the explanations. The LLM-generated explanations were procedural and typically lengthier than expert explanations. While there were not any specific instructions given to experts about the length of the explanations we asked them to provide necessary details justifying their decision. Table~\ref{llmvsexpertexpl} in the Appendix shows such an example. 