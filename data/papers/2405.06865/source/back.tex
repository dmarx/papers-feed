
\section{Background and Related Work}
\label{sec:back}

We begin by providing background on style mimicry and existing image-based protection methods. Then we follow with an overview of publicly available tools that enable style mimicry attacks on the video domain.

\subsection{Style Mimicry and Existing Defenses}
\label{sec:back1}
In a style mimicry attack, a bad actor finetunes a text-to-image model to
generate art in a particular artist's style without their consent. 
Since the introduction of text-to-image diffusion~\cite{sd-release,podell2023sdxl,df,novelai-update,ramesh2022hierarchical} models in 2022, style mimicry has grown significantly. There have been multiple high-profile mimicry incidents involving human artists~\cite{hollie-steal,sarah-andersen,lensa-steal,sam-steal}, and new companies are founded that focus purely on style mimicry~\cite{aigame,lexica}. AI marketplaces have also recently gained traction, with websites like CivitAI~\cite{civitai} offering over 119K ready-to-use mimicry models for people to download and use.

\para{Image-based style mimicry. } 
Style mimicry relies on finetuning pretrained text-to-image models (\eg stable diffusion) on a small set of images from a specific style~\cite{ruiz2022dreambooth,finetune-c,gal2022image}. The quality of these images greatly impacts the mimicry result, and thus, attackers often scrape high quality images from artists' websites and online galleries~\cite{hollie-steal,sam-steal}. In practice, a bad actor does not need many (less than 20 images~\cite{shan2023glaze,gal2022image}) in order to successfully generate arbitrary artwork from a victim artist's style. Because of the risk of image-based mimicry, many artists choose to reduce the amount of art they post online~\cite{aiprotest}, reduce the quality of any posted art~\cite{lowres}, and apply protection (discussed in details below) on this artwork~\cite{shan2023glaze}. 

\para{Protecting images from style mimicry. } 
Existing work (Mist~\cite{mist}, Anti-Dreambooth~\cite{antidb}, and Glaze \cite{shan2023glaze}) has 
proposed methods that leverage clean-label poisoning~\cite{saha2020hidden, turner2018clean, zhu2019transferable} to prevent style mimicry. At a high level, these systems add small optimized perturbations to image artwork that modifies the perturbed image's feature space representation without altering its content. The altered feature space representation prevents models from learning the correct artistic style. In general, these protection tools calculate the perturbation $\delta_x$ for an image $x$ using the following objective: 

\secspace
\begin{eqnarray}
   &\min\limits_{\delta_x} Dist\left( \Phi(x + \delta_x), \Phi(T)\right),  \label{eq:cloakopt}\\
  & \text{subject to } \; |\delta_x|< p, \nonumber
\end{eqnarray} 

where $\Phi$ is a generic image feature extractor from a public text-to-image model, $Dist(.)$ computes the distance between two feature representations, $|\delta_x|$ measures the perceptual perturbation caused by protection, and $p$ is the perceptual perturbation budget. $T$ is 
a ``target image'' that the perturbation $\delta_x$ is optimized towards, such that $x + 
\delta_x$ resembles $T$ in feature space while being visually identical to $x$. Mist~\cite{mist} extends the optimization objective across the entire diffusion process, including gradient computations through the randomized diffusion denoising process. By default, Mist uses a predefined black and white patterned image as its target with the goal of producing chaotic patterns in generated images. Anti-DB (Anti-Dreambooth)~\cite{antidb} is most similar to Mist, but modifies the optimization objective to specifically target Dreambooth~\cite{ruiz2022dreambooth} text-to-image models. There, they find that training surrogate models alongside computing image perturbations results in stronger protection, though it incurs additional computation time. Glaze~\cite{shan2023glaze} introduces input-specific target images by performing style transfer on the input image using a contrasting artistic style. This method preserves the overall content of the input image, while changing mainly the style, which the authors argue leads to more robust protection. Glaze then attacks the image encoder of a diffusion model as detailed above. 

These protection tools have been positively received by the artist community,
with Glaze having been downloaded at least 2.3 million
times~\cite{shan2023glazewebsite}. While these systems are typically too
computationally expensive for artists, efforts have been made to improve
accessibility~\cite{mistgithub, shan2023webglaze}. Since these systems are
free and increasingly available, images may no longer be a viable data source
for attackers to access artwork for fine-tuning text-to-image models. 

Video protection, on the other hand, has yet to be explored. Computation time per image is already limiting for many artists, and applying the same algorithms to all frames would be many times more costly. Yet, videos represent a significant source of data, incentivizing attackers to explore publicly available video art, such as short animation, movies or video game trailers etc..

Until recently, most style mimicry models are trained on still images. This 
is no longer the case today because 1) artists are increasingly more reluctant to post their 
work on the Internet~\cite{aiprotest}, 2) existing defenses (\S\ref{sec:back1}) are effective at protecting still images against mimicry, 3) video frames offer a significantly more diverse range of images compared to still images. 

\para{Video content is a promising source for mimicry. } 
Video content (\eg game trailers, anime, short videos, documentary, ads) provides promising alternative data sources for two reasons. First, video contents often offer a more diverse (3D) shots of an object or style, \eg rotating shot of an object, panning across a scene. These diverse viewpoints 
enable models to better learn the content during the training process~\cite{videomotivate}. Second, there are significantly more video frames compared to still images and many of the videos contain unique art styles/characters. The entire Internet produces around 3.2 billions still images daily~\cite{manyvideos}, while YouTube alone sees over 271,000 hours of videos (\ie around 29 billions video frames) uploaded per day. Specifically, gaming companies and animation studios often use short videos as a way to promote new games, characters, and movies. Movie clip compilations and trailers are readily available on YouTube~\cite{movietrailers}, while video game companies like Riot and Mihoyo frequently post teasers and trailers showcasing new playable content, or highly anticipated characters~\cite{genshintrailer, leaguetrailer}. These videos are filled with original artwork, and contain image frames that are prime targets for style mimicry. 

\para{Video-based mimicry in the real-world. } 
Style mimicry using video content has already occurred in the real-world. Bad actors have created and distributed software that generates high quality text-to-image datasets from online videos. One GitHub tool~\cite{anime2sd} automates the process of downloading (\eg torrenting) Japanese Anime episodes and extracting high quality frames of desired characters. Another option~\cite{civitai-video} advertised on CivitAI does the same, with the additional capability of scraping frames from screencap websites such as FanCaps~\cite{fancaps}. These tools demonstrate that there already exists sophisticated technology aimed at creating text-to-image datasets from original video content. 

We also provide our own examples of this threat. We download and extract high quality frames from YouTube videos and train style mimicry models on them (Figure~\ref{fig:style-mimicry-scenario}). Figure~\ref{fig:style-mimicry-baseline} shows some examples of extracted video frames as well as mimicry results generated by the style mimicry model. We include human evaluation of the success of these style mimicry images later using user studies with both artists and the general public in \S\ref{sec:eval}. 

While there has been recent developments in text-to-video~\cite{videoworldsimulators2024} and image-to-video models~\cite{blattmann2023stable}, we leave them as a topic for future work, and focus solely on text-to-image mimicry where the source of data originates from video content. 

