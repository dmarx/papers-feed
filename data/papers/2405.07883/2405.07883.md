---
abstract: |
  Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (*tokens*). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding *embeddings* for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models’ performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.
author:
- |
  Benjamin Minixhofer$\, ^{\texttt{[SEP]}}$     Edoardo M. Ponti$\, ^{\texttt{[CLS]}}$     Ivan Vulić$\, ^{\texttt{[SEP]}}$  
  $~~~~~~~^{\texttt{[SEP]}}$University of Cambridge       $^{\texttt{[CLS]}}$University of Edinburgh
bibliography:
- bib.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Zero-Shot Tokenizer Transfer
---





# Introduction

Language Models[^1] typically operate on discrete tokens, so they need a means to map text into a sequence of tokens, namely a *tokenizer*. The vast majority of contemporary LMs use subword tokenizers , whereas others use byte-level  or character-level tokenizers . Regardless of the chosen tokenization ‘granularity’, these models share a fundamental limitation: once they are trained with a particular tokenizer, inference with a different tokenizer is impossible. In other terms, a pre-trained LM is *“bound”* to the tokenizer it was trained with. This has wide-ranging implications: since the focus during pretraining is typically primarily on the English language, the tokenizer often encodes languages besides English  or other domains, such as code, less efficiently. This leads to large disparities in the inference cost between English and non-English text . Tokenizers may also be sub-optimal for domains which they were not designed to be used with, e.g. fine-tunings of the Llama models performing subpar on coding tasks . Efficiency and performance are only some of the reasons to transfer models across tokenizers: methods of interaction between models, such as ensembling  and model merging , typically assume the same unit of representation (i.e., equivalent tokenization) across models; if two models adopt different tokenizers, they become unsuitable for ensembling or merging. Problematic artifacts of tokenization such as ‘Glitch tokens’  may also be fixed via transfer to a new tokenizer.

To address these issues, past work developed methods to equip an LM with a new tokenizer by retraining the embedding parameters, and optionally continuing to train the entire model . This adaptation can be made faster by initializing the embedding parameters through heuristics . In this work, we formulate a new problem: given an LM, can we create an embedding matrix on-the-fly for any arbitrary tokenizer, without ever observing data for it? While past work investigated $n$-shot tokenizer transfer, we refer to this new problem as *zero-shot tokenizer transfer* (ZeTT). If the performance of the model can be approximately preserved, ZeTT effectively "detaches" LMs from the tokenizer they were trained with. We first evaluate the efficacy of prior (heuristic-based) approaches for ZeTT, finding that, while heuristics can preserve performance to some extent, there is generally a large gap to the original LM performance.

To close this gap, we introduce a new paradigm: We train a *hypernetwork* on a diverse distribution of tokenizers to predict the embedding parameters for any given tokenizer. By investing into the one-time-cost of training the hypernetwork, we aim to subsequently enable effective ZeTT. This proves to be possible: ZeTT via the hypernetwork preserves performance to a few percent accuracy in many cases. Furthermore, the hypernetwork can learn to rapidly adapt to a given target tokenizer by continued training on a small amount (\<1B) of extra tokens, whereas previous work typically needed hundreds of billions of tokens . As such, our hypernetwork provides a state-of-the-art solution to $n$-shot tokenizer transfer, while also establishing a competitive baseline to our newly introduced zero-shot tokenizer transfer problem. This unlocks a range of new ways to combine language models with tokenizers. For example, in this work, we zero-shot substitute the Mistral-7B tokenizer  with a tokenizer that encodes code using 10% less tokens on average, while preserving functional code generation correctness to approx. 3% (Section ). We also evaluate zero-shot cross-lingual transfer of the multilingual XLM-R encoder model to a range of different languages by substituting the XLM-R tokenizer with a target-language specific tokenizer and reusing adapters trained for the original XLM-R. This leads to a \>16% speedup and preserves performance on XNLI  to 1% on average, although the language model has never been trained with the target-language tokenizers. Finally, we show that a hypernetwork trained for a base large LM (e.g. Mistral-7B) can also be applied to fine-tunings of the same model (e.g. Mistral-7B-Instruct-v0.1), preserving capabilities to a large extent (Section ). Our code and models are publicly available at [`github.com/bminixhofer/zett`](https://github.com/bminixhofer/zett).

<figure id="fig:training">
<span class="image placeholder" data-original-image-src="figures/training-cropped.pdf" data-original-image-title="" width="0.9\linewidth"></span>
<figcaption>The hypernetwork predicts input and output embeddings based on the tokenizer.</figcaption>
</figure>

# Background

Tokenizers operate as a *tokenization function* $T$ mapping a text to a sequence of elements in the *vocabulary* $\mathcal{V}$. By the term *tokenizer*, we henceforth refer to the tuple comprising the two crucial components, $(\mathcal{V}, T)$. Importantly, the vocabulary and the tokenization function are distinct components; given some vocabulary, there are many ways to encode text as a sequence of tokens in this vocabulary . After tokenization, the model represents the sequence of tokens via a function $E_\phi: \mathcal{V} \rightarrow \mathbb{R}^{d_{\text{model}}}$ (the *embeddings*). The embeddings are typically parametrized by a matrix $\phi$ as a lookup table which assigns a distinct $d_{\text{model}}$-dimensional vector (a row of the matrix) to every element in $\mathcal{V}$. Embeddings are used twice in the language model: once at the input to map tokens to a fixed-size vector, and again at the output to compute a logit for every token, typically via a dot-product of $E_\phi(t)$ with the final hidden state of the LM. Embedding parameters may or may not be shared between the input and the output;[^2] our method works with both. We denote the entire set of embedding parameters via $\phi$, denoting input embeddings as $\phi^{\text{in}}$ and output embeddings as $\phi^{\text{out}}$, if necessary.

Contemporary language models typically use subword tokenizers via BPE  or UnigramLM . Subword tokenization is a common choice since it can represent arbitrary sequences of text ("open-vocabulary" language modeling) while largely retaining the efficiency of word-level models . However, there are a number of problems with subword tokenization, e.g. models using subword tokenization struggle parsing sequences of numbers  and text with spelling mistakes . A recent strand of work aims to get rid of subword tokenization via byte-level (so-called "token-free") models . However, these models still operate on tokens, using the set of 256 bytes as the vocabulary, and Unicode as the tokenization function . In a similar vein, some models use character-level tokenization , optionally learning to pool characters into longer tokens .[^3] So far, byte- or character-level approaches have been unable to supplant subword tokenization due to reduced compute efficiency (because of longer sequences), and not necessarily being more robust . Thus, although our approach is in principle applicable to any tokenizer, we focus our experiments on subword tokenizers. Specifically, we use the UnigramLM parametrization of the tokenization function, and show that other tokenizers can be converted to this parametrization later in Section . UnigramLM sets $$T(x) := \mathop{\mathrm{argmax}}_{C \in \mathcal{C}_x} \sum_{t \in C} \log p(t)$$ where $\mathcal{C}_x$ is the set of all possible tokenizations of $x$ (i.e., all possible decompositions of $x$ in $\mathcal{V}$). UnigramLM provides a convenient way to represent tokens as a 2-tuple $(t, p(t)) \in (\mathcal{V}, \mathbb{R})$.

Prior work transfers LMs to a new tokenizer by initializing embedding parameters via a heuristic, then continuing to train the embeddings. We denote the original tokenizer as $(\mathcal{V}_a, T_a)$ and the original embedding parameters as $\phi_a$. Analogously, the target tokenizer is $(\mathcal{V}_b, T_b)$ with embedding parameters $\phi_b$. <span class="smallcaps">FVT</span>  initializes embeddings for any new token $t \in \mathcal{V}_b$ as the mean of the embeddings of $T_a(t)$ i.e. the mean of the sequence of embeddings the new token is decomposed into by the previous tokenizer $T_a$. <span class="smallcaps">RAMEN</span> , <span class="smallcaps">WECHSEL</span>  and <span class="smallcaps">OFA</span>  require auxiliary embeddings $E_{\text{aux}}: \mathcal{V}_{\text{aux}} \rightarrow \mathbb{R}^{d_{\text{aux}}}$ with $|\mathcal{V}_{\text{aux}} \cap \mathcal{V}_{a}| \not \ll |\mathcal{V}_{a}|$ and $|\mathcal{V}_{\text{aux}} \cap \mathcal{V}_{b}| \not \ll |\mathcal{V}_{b}|$. They use $E_{\text{aux}}$ to embed tokens in $\mathcal{V}_{a}$ and $V_{b}$ in the same semantic space, then initialize embeddings in $E_{\phi_b}$ as a weighted average of embeddings in $E_{\phi_a}$ with weights given by their similarity in $E_{\text{aux}}$. <span class="smallcaps">FOCUS</span>  initializes embeddings of tokens in $V_{b} \setminus V_{a}$ as a weighted combination of the overlapping tokens $V_{a} \cap V_{b}$, and copies the embeddings of the overlapping tokens. Weights are again computed using an auxiliary embedding matrix $E_{\text{aux}}$, but the only requirement is $|\mathcal{V}_{\text{aux}} \cap \mathcal{V}_{b}| \not \ll |\mathcal{V}_{b}|$. We use <span class="smallcaps">FOCUS</span> as the main baseline since show it obtains better performance without any training (i.e., zero-shot) than other heuristics, which we also confirm later in Section .

While a significant amount of prior work has investigated heuristics to initialize the embedding layer, there is also research into changing the training procedure to facilitate $n$-shot tokenizer transfer. show that forward- and backward-propagating through a subset of the model layers is sufficient for learning embeddings for a new tokenizer. find that regularly resetting the embedding parameters during pretraining boosts the speed at which they are relearnt upon transfer. These approaches can be seen as orthogonal to ours. They could be freely combined with our method; we leave this to future work.

Hypernetworks are networks that predict the parameters of another network . Prior work uses neural networks to predict embeddings for out-of-vocabulary  or rare words  of word embedding models . extend this approach to predict embeddings for rare words in BERT models . These methods can also be viewed as embedding prediction hypernetworks. In contrast, the hypernetwork we propose (i) approaches the more general problem of *transferring* to an arbitrary tokenizer, instead of *extending* the original tokenizer and (ii) can be applied to encoder, decoder, and encoder-decoder LMs, that is, it is *objective-agnostic*.

# Methodology

## Hypernetwork Training

We aim to find parameters $\theta$ of a hypernetwork $H_{\theta}: (\mathcal{V}_b, T_b) \rightarrow \phi_b$ for some pretrained LM. Let $\phi_a$ and $\psi$ be the embedding and inner (non-embedding) parameters of the language model, respectively. $\mathcal{L}$ is the loss of the language model as a function of the tokens, the embedding parameters, and the inner parameters, typically: $$\mathcal{L}(t, \phi_a, \psi) = \text{CrossEntropy}(\mathrm{LM}_{\psi}(E_{\phi_a}(t)), \mathrm{label}(t)),$$ where $\mathrm{LM}_{\psi}$ is the language model and $\mathrm{label}$ maps the sequence of tokens to corresponding labels, e.g., shifting the sequence in case of standard (autoregressive, causal) language modeling, or masking the sequence in case of Masked Language Modeling . Importantly, however, we do not make any specific assumptions on $\mathcal{L}$.

Note that the loss of the language model under the original tokenizer $T_a$ on a text $x$ is $\mathcal{L}(T_{a}(x), \phi_a,\psi)$. We train our hypernetwork to minimize the loss $\mathcal{L}_{\theta}(
T_b(x),
H_{\theta}(\mathcal{V}_b, T_b),
\psi
)$. That is, we substitute the original embedding parameters for the hypernet predictions, and substitute the original tokenizer for a tokenizer $(\mathcal{V}_b, T_b)$. Figure illustrates the flow of information.

We follow standard practice and sample texts uniformly from the training corpus. Tokenizer sampling is not as trivial: we would like a distribution over tokenizers $(\mathcal{V}_b, T_b)$ with high variance to encourage generalization to unseen tokenizers. To this end, we introduce a procedure to sample a diverse set of UnigramLM tokenizers. We show later in Section  that arbitrary tokenizers can be well-approximated via UnigramLM, motivating this choice.

We initially fill a queue $\bm{q}$ with $n$ texts sampled randomly from the training corpus and, at every step in the training loop, push the $m$ texts in the current batch and remove the $m$ least recently added texts. We then compute all substrings $t$ up to length $l$ and their frequency in $\bm{q}$.[^4][^5] We add Gaussian noise to the frequencies to arrive at a final score $p(t)$ for every token $t$. Finally, we assemble the tokenizer by taking the top $k$ tokens with the highest $p(t)$ as the vocabulary and UnigramLM parametrized by $p(t)$ as the tokenization function. The training loop is summarized in Algorithm . The ‘rolling’ queue of texts $\bm{q}$ ensures high variance in the vocabulary, while the Gaussian noise added to the frequencies ensures high variance in the tokenization function.

Importantly, the texts and the tokenizer are sampled *dependently*: the batch of $m$ texts used for training is a subset of the $n$ texts used for sampling the tokenizer. If they were sampled independently, the probability for a token to occur would be $p(\text{token}) \propto p(\text{token} \in \mathcal{V}_b) \times p(\text{token} \in \bm{x})$. Since both these factors are small for rare tokens, $p(\text{token})$ would get vanishingly small in this case.

<figure id="fig:architecture">
<span class="image placeholder" data-original-image-src="figures/architecture-cropped.pdf" data-original-image-title="" width="0.82\linewidth"></span>
<figcaption>The hypernetwork consists of a language model <span class="math inline">\(\mathrm{HLM}_\theta\)</span> learning to compose embeddings under the original tokenization into a new embedding and amortizes over the tokenization function.</figcaption>
</figure>

In practice, directly minimizing $\mathcal{L}_\theta$ starting from randomly initialized $\theta$ is difficult. Thus, we include a warmup stage where we train the hypernetwork to mimic the embedding parameters of the original tokenizer, akin to MIMICK . $$\mathcal{L}^{\text{warmup}}_\theta = \|H_\theta(\mathcal{V}_a, T_a) - \phi_a)\|_2$$

The warmup stage is substantially quicker than the main stage because there is no need to propagate through the main model. We found it prevents divergence in some cases. Afterwards, we add an auxiliary loss, which, for every token in the sampled vocabulary $\mathcal{V}_b$ that also exists in the original vocabulary $\mathcal{V}_a$, penalizes the distance to the corresponding embedding in $\phi_a$.

$$\mathcal{L}^{\text{aux}}_\theta = \frac{1}{|\mathcal{V}_a \cap \mathcal{V}_b|}\sum_{t \in |\mathcal{V}_a \cap \mathcal{V}_b|}\|H_\theta(\mathcal{V}_b, T_b)[\mathcal{V}_b[t]] - \phi_a[\mathcal{V}_a[t]]\|_2$$

This penalizes drift from the warmup stage. Combining it with the main loss yields the final loss.

$$\mathcal{L}^{\text{final}}_{\theta} = 
\mathcal{L}_{\theta}(
T_b(x),
H_{\theta}(\mathcal{V}_b, T_b),
\psi
) + \alpha \cdot  \mathcal{L}^{\text{aux}}_\theta$$

The hyperparameter $\alpha$ weighs the contribution of the auxiliary loss. Since $H_\theta(\mathcal{V}_b, T_b)$ is also required for the main loss, it requires negligible extra computation. The auxiliary loss is necessary especially for models with separate input and output embedding matrices as shown in Appendix .

## Hypernetwork Architecture

It remains to define the hypernetwork architecture, that is, how to map the tokenizer $(\mathcal{V}_b, T_b)$ to the embedding parameters $\phi_b$. To this end, we represent the new tokens $t_b \in \mathcal{V}_b$ by decomposing them using the original tokenization function $T_a$, and embedding them with the original embeddings $E_{\phi_a}$.[^6] This sequence of embeddings is passed through multiple Transformer layers, plus a separate prediction head for the input embeddings and output embeddings $\phi^{\text{in}}_b$ and $\phi^{\text{out}}_b$. The hypernetwork thus consists of *another language model* which is applied separately for every token. We refer to the hypernetwork’s language model as $\mathrm{HLM}_\theta$. $\mathrm{HLM}_\theta$ can be thought of as learning how to compose the sequence of tokens $T_a(t)$—which any given token is decomposed into—into one embedding, as illustrated in Figure . Importantly, we do not take the tokenization function into account. By sampling diverse tokenizers during the training process, we aim for the hypernetwork to learn to produce a single embedding suitable to a wide variety of different tokenization functions. We analyze the impact of this choice later in Section . We also experiment with hypernetworks which do take the tokenization function into account in Appendix .

The input to the hypernetwork consists of the sequence of tokens $T_a(t)$ that any given token is *decomposed* into. However, this decomposition is not always trivial: for example, $T_a$ could be character-level, while the token $t$ could be in the vocabulary of a byte-level tokenizer $T_b$. In this case, $t$ could be any arbitrary sequence of bytes (not necessarily valid UTF-8). To solve this issue, we introduce a procedure to convert tokenizers to the byte level by adding a small amount of extra tokens to the vocabulary (c.f. Section ). This guarantees that $T_a$ can decompose arbitrary tokens. The embeddings of the extra vocabulary are initialized randomly and trainable alongside the hypernetwork parameters.

# Experiments

## Setup

We use the English subset of the MADLAD-400 corpus  and code from the StarCoder data  for hypernetwork training. The sampling ratio of English to Code is 7:3 following . For the multilingual hypernetwork, we use a subset of 26 of the languages used in XGLM .[^7] with data from MADLAD-400. We sample languages using a multinomial distribution as in with $\alpha=0.1$. For the $n$-shot experiments, we also train on the StarCoder data, but substitute the English section of the MADLAD-400 corpus for Flan v2  sampled as in .[^8]

We use the standard benchmarks PiQA , HellaSwag , BoolQ , MMLU  and the “easy” subset of ARC  for evaluation in English and the synthesis task of HumanEvalPack  for coding evaluation. For multilingual evaluation, we use XNLI , XCOPA  and MMLU as machine-translated by .

To evaluate our method, we use Mistral-7B  as the main decoder-style language model and XLM-R  as a representative of encoder-style models.[^9] We also experiment with the smaller TinyLlama-1.1B model  in Appendix .

We transfer models to the GPT2 tokenizer  for evaluation on natural language benchmarks and to the StarCoder tokenizer  for evaluation on code benchmarks.[^10] For multilingual evaluation, we train language-specific monolingual tokenizers with a vocabulary size of 50k using SentencePiece  and evaluate transfer to these. We also verify that the hypernetwork is robust to the choice of vocabulary size in Appendix .

We train the hypernetwork for 200k gradient update steps (10k of which are MIMICK-style warmup) with a batch size of 128 tokens and a sequence length of 128 (we find it sufficient to use short sequence lengths for learning embedding parameters). For the multilingual decoder-style models, we start from the English + Code checkpoint and forgo MIMICK-style warmup, keeping other hyperparameters unchanged. We use a RoBERTa-style architecture i.e. bidirectional attention and Post-LayerNorm Transformer layers , but use a feedforward dimension of 2x the hidden dimension (instead of RoBERTa’s 4x) for the hypernetwork. See Appendix  for a full list of hyperparameters.

To keep runtime comparable between training the model with hypernetwork and direct training (without hypernetwork), we run hypernetwork inference only for a subset of $k=16384$ tokens in the continued training case. The subset consists of all tokens occurring in the batch, plus a uniform sample of those that do not occur. The language modeling loss is then only computed over this subset of tokens. We found in preliminary experiments that this causes only minor performance degradation. Furthermore, we use the zero-shot predicted embeddings as the target for the auxiliary loss instead of using the original embeddings. This stabilizes training. We train for 50k steps with a batch size of 32 and sequence length of 512, resulting in ‘seeing’ 819.2M tokens.

## Zero-Shot and n-shot Results

Results for XLM-R are shown in Table . We take task adapters trained for the original XLM-R model on the English XNLI dataset via and substitute the tokenizer for our language-specific one. We compare our hypernetwork against a simple lexical baseline (copying the embeddings of overlapping tokens and initializing the rest randomly), FVT, OFA, and FOCUS (c.f. Section ). We focus only on FOCUS in the following since it performs best among the baselines.[^11] Our hypernetwork consistently outperforms all baselines and preserves accuracy to 1% on average, losing 3% in the worst case and improving by 1% in the best case, while sequences are on average 14% shorter for the language-specific tokenizers; inference is thus more than 16% faster.[^12] We show in Appendix  that these results are robust to the target vocabulary size.

Table  shows results on English and Code for Mistral-7B. We find that ZeTT is more challenging in the decoder case: FOCUS performs roughly random in the worst case (-23.2% on BoolQ) and is reduced to 0% pass@1 on HumanEval in Python. The hypernetwork goes a long way in closing this gap but still falls behind on some benchmarks. However, continuing to train the hypernetwork with the target tokenizer closes the gap almost completely. In fact, continued training on 800M tokens with the StarCoder tokenizer performs *better* than continued training for the same amount of tokens with the original tokenizer, potentially because the StarCoder tokenizer is more well suited towards code; it results in approx. 10% less tokens on average. Also, notably, continued training with the original tokenizer slightly *degrades* performance on average; this may be due to a higher-quality data mix used for pretraining Mistral-7B, whereas we use public data sources (c.f. Section ).

Results of the multilingual hypernetwork for Mistral-7B are shown in Table  and Table . On XCOPA, the hypernetwork on average improves performance over the original model, while also more than halving sequence length. XCOPA performance is close to random in some languages (e.g. Southern Quechua (qu) and Estonian (et)), so we also evaluate on multilingual MMLU. Here, although the hypernetwork clearly outperforms FOCUS (which performs close to random), there is still a substantial gap to the original model; this could presumably be fixed via continued training.

## Applying a Hypernetwork trained for a Base Model to Fine-Tuned Models

So far, we have shown that the hypernetwork can be successfully applied for transferring the tokenizer of the base model[^13] it was trained on. However, a large amount of the models used by practitioners are fine-tuned versions of base models, e.g. via SFT or RLHF . We now attempt to answer the question: *Given a hypernetwork trained for a base model, can we apply this hypernetwork to fine-tuned versions of the same model without any extra training?* This would act as a multiplying factor for the hypernetwork’s applicability. First, we observe that the embedding space of a fine-tuned model is compatible with that of the base model: the embeddings of the fine-tuned Mistral-7B-Instruct-v0.1 have an average cosine similarity of 98.6% to the corresponding embedding in the base model while the average cosine similarity of the mean embedding vector is 17.4%.[^14] Embedding compatibility also holds true for other models (Appendix ). The predictions of a hypernetwork trained for a base model can thus be used out-of-the-box with fine-tuned models. We verify that this is the case by evaluating Mistral-7B-Instruct-v0.1 transferred to the GPT2 tokenizer on the corrected[^15] version of MT-Bench . For $n$-shot transfer, since we train the full model we also need a way to transfer the non-embedding parameters; we achieve this via Task Arithmetic . Results are shown in Table .

The transferred fine-tuned model performs well, coming within approx. 0.5 score of the original model. Also, curiously, the fine-tuned model with the original tokenizer performs *better* when using the embeddings of the (not fine-tuned) base model; this may be a prudent direction for future work.

# Discussion

As per Section , we need a procedure to convert tokenizers to the byte level to ensure that token decomposition is always possible. This is trivial in most cases; the bytes just need to be added to the vocabulary. BPE is an exception: here, we need to change the atomic units on which merges are defined from characters to bytes. This can be achieved by adding merges to assemble the characters used by the tokenizer from their constituent bytes to the beginning of the merge table. We measure the success of the conversion to byte level as the probability that, given some pretoken sampled from a corpus, this pretoken results in the same token sequence in the original and the converted tokenizer. Results are shown in Table .

We also introduce a procedure to convert arbitrary tokenizers to tokenizers using UnigramLM as the tokenization function. We refer to this process as *unigramifying* (details in Appendix ). An important assumption of the hypernetwork training is that by using the UnigramLM parametrization with scores distributed as Gaussians we can cover a sufficiently diverse distribution of tokenizers to enable the hypernetwork to generalize to e.g. BPE tokenizers. Unigramifying allows us to check if, in principle, this is possible. Luckily, we find that it is: unigramifying results in minimal performance degradation when substituting the original tokenizer with the corresponding UnigramLM tokenizer (Table ). Although this does not guarantee that our distribution of tokenizers is sufficiently diverse, our empirical results suggest it is (cf. Section ).

We believe our conversion methods to UnigramLM and to byte-level will simplify further research into tokenizer transfer, showing that *the wildly heterogeneous landscape of tokenizers can be well approximated via byte-level UnigramLM tokenizers*.

As described earlier in Section , we ‘amortize’ over the tokenization function, that is, the tokenization function is not an input to our hypernetwork. We find that the predicted amortized embeddings are robust to the choice of tokenization function. For example, the set of embeddings predicted for the GPT2 vocabulary has low bits-per-character for both the original GPT2 tokenization function and a different UnigramLM tokenization function with scores based on token frequencies (Table ). This is not the case for the original GPT2 embeddings: while they (as expected) perform well with the original GPT2 tokenizer, there is significant performance degradation when switching to the frequency-based UnigramLM tokenization function. This calls into question prior work copying the embeddings of overlapping tokens for transfer across tokenizers , indicating that *even if there is an exactly overlapping token in the original tokenizer, it is not necessarily the optimal initialization of the corresponding token in the new tokenizer*.

Although we amortize over most of the aspects of the tokenization function, in practice, tokenization functions rely on a considerable amount of engineering, so it is not possible to amortize over everything; we discuss remaining assumptions in Appendix .

We estimate the FLOPs per token of multiple hypernetworks in Table .[^16] Given a batch size $n$ and sequence length $s$ for the main model, and using the hypernetwork to compose $k$ token sequences of length $t$, the FLOPs per batch will be $n \times s \times (\frac{\text{FLOPs}}{\text{token}})_{\text{main}} + k \times t \times (\frac{\text{FLOPs}}{\text{token}})_{\text{hypernet}}$. Taking hypernet training for Mistral-7B as an example with $n = s = 128$, $k=32768$ and $t = 7$ the FLOPs per batch will be 252T + 30T i.e. a 12% overhead from applying the hypernet. Notably, we observed in preliminary experiments that a hypernetwork size of three layers is sufficient, regardless of model size, so the relative overhead decreases with increased amounts of layers in the main model (as also evident in Table ).

# Conclusion

We have established *Zero-Shot Tokenizer Transfer (ZeTT)*, the difficult problem of transferring language models to a new tokenizer without any training. We have found that prior heuristics for embedding initialization provide a first baseline for ZeTT, but fall short in many cases. To establish a much stronger baseline, we introduced a hypernetwork-based approach that closes the gap to a large extent, and can be further improved via continued training on a few (\<1B) tokens. Due to preserving the embedding space of the original model, ZeTT can be applied to e.g. reusing adapters trained for the original model with a different tokenizer, and to transferring fine-tuned models to a new tokenizer using a hypernetwork trained for the base model. In aggregate, this work is a substantial step towards *detaching* language models from their tokenizer, increasing their flexibility and reusability.

# Acknowledgments

This work has been supported by a Royal Society University Research Fellowship *‘Inclusive and Sustainable Language Technology for a Truly Multilingual World’* (no 221137; 2022-) awarded to Ivan Vulić. Research supported with Cloud TPUs from Google’s TPU Research Cloud (TRC). We thank Markus Frohmann, Marcell Fekete and Piotr Nawrot for helpful feedback on a draft of this paper, and Arduin Findeis for many valuable discussions during the entirety of this project.

# Unigramifying: Approximating Arbitrary Tokenizers via UnigramLM

We introduce a procedure to convert arbitrary tokenizers to UnigramLM in an optimal (but lossy) way which we refer to as *unigramifying*. Given a text $x$ and the sequence of tokens $T(x)$, for the UnigramLM tokenizer $\hat{T}$ to be equivalent to $T$, it is necessary that $\hat{T}$ fulfills $\sum_{t \in T(x)} \log p_{\hat{T}}(t) > \sum_{t \in C} \log p_{\hat{T}}(t)$ for all $C$ in $\mathcal{C}_x \setminus \{T(x)\}$.[^17] Thus, given a corpus of texts $X$ we can formulate a loss

$$\mathcal{L_{T}}(X, \hat{T}) = \sum_{x \in X}\,\sum_{C \in \mathcal{C}_x \setminus \{T(x)\}} \max\left(0, \sum_{t \in T(x)} \log p_{\hat{T}}(t) - \sum_{t \in C} \log p_{\hat{T}}(t)\right)$$

which is zero if and only if the condition above is satisfied for all texts in $X$. This objective is piecewise linear, so it can be converted to a standard Linear Programming (LP) form and solved via an LP solver. In practice, we use the `CPLEX v22.1`  solver. Since applying the procedure to a corpus directly would be costly, we first pre-tokenize the training corpus, then count the pretokens, and choose the top $n=1000000$ pretokens as the set $X$.

<figure id="fig:aux_loss">
<span class="image placeholder" data-original-image-src="figures/auxiliary_loss.pdf" data-original-image-title="" width="0.7\linewidth"></span>
<figcaption>Language modeling loss of GPT2, and GPT2 with untied weight embeddings with and without the auxiliary loss across the first 50k training steps, excluding MIMICK-style warmup.</figcaption>
</figure>

# Stabilization Effect of the Auxiliary Loss

We found in preliminary experiments that the auxiliary loss is necessary, especially for models that do not share embedding parameters between the input and the output (models with *untied* embeddings). To validate this hypothesis, we conducted an experiment where we manually untied the embeddings of GPT2 i.e. used a separate hypernetwork prediction head for the input and the output embeddings. Although everything else is kept the same, the untied GPT2 model diverges without the auxiliary loss, whereas the original GPT2 trains as expected, even without an auxiliary loss (Figure ).

# Non-Amortizing Hypernetworks

We experimented with hypernetworks taking the tokenization function into account by adding *sparse inter-token attention* blocks between the self-attention and the FFN in every hypernetwork layer. Sparse inter-token attention consists of two attention blocks. The first attention block attends from a fixed amount of learnable inter-token embeddings (e.g. 16, each a vector of size $d_{\text{model}}$) to the $i$th token representation of every token sequence passed to the hypernetwork. The second block attends from the $i$th token representation to the inter-token embeddings. This way, we factorize the attention to e.g. one $16 \times k$ attention and one $k \times 16$ attention, instead of regular the $k \times k$ self-attention which would be infeasibly slow for typical vocabulary sizes. We only add inter-token attention for the first token in every sequence. This improves performance on the sampled tokenizers, but does not improve performance on ‘real-world’ tokenizers (Table ); investigating this mismatch is a direction for future work.

# Additional Hyperparameters

Hyperparameters for hypernetwork training are shown in Table . For continued training, we use the same optimizer, but sequence length of 512, batch size of 32, training for 50k steps and a constant learning rate chosen among the set $\{1\mathrm{e}{-6},3\mathrm{e}{-6},6\mathrm{e}{-6},1\mathrm{e}{-5},3\mathrm{e}{-5}\}$ to maximize performance. The chosen learning rate is $1\mathrm{e}{-6}$ for the runs keeping the original tokenizer (*original@800M*), $6\mathrm{e}{-6}$ for continued training starting from FOCUS (*FOCUS@800M*) and $3\mathrm{e}{-6}$ for continued training with the hypernetwork (*ours@800M*).

# Sensitivity to Tokenizer Size

Since the tokenizers we experiment with have similar vocabulary sizes (50k for the language-specific tokenizers and for GPT2, 49k for the StarCoder tokenizer) we conduct an additional experiment to quantify the sensitivity of the performance of our hypernetwork to the size of the target tokenizer. We find that although there is slight performance degradation when increasing the size of the new tokenizers’ vocabulary, the hypernetwork is fairly robust to vocabulary size (Figure ).

<figure id="fig:vocabulary_size_sensitivity">
<span class="image placeholder" data-original-image-src="figures/vocabulary_size_sensitivity.pdf" data-original-image-title="" width="0.7\linewidth"></span>
<figcaption>Difference in accuracy to the original XLM-R model on XNLI of our method and FOCUS across vocabularies with size 30k, 50k, and 100k of the new tokenizer.</figcaption>
</figure>

# Reliance on Vocabulary Overlap

<figure id="fig:overlap_correlation">
<span class="image placeholder" data-original-image-src="figures/overlap_correlation.pdf" data-original-image-title="" width="0.7\linewidth"></span>
<figcaption>Correlation of the difference in accuracy to the original XLM-R model with Unigram overlap probability <span class="math inline">\(p(\text{overlap})\)</span> (left) and vocabulary overlap (right).</figcaption>
</figure>

Intuitively, transfer is easier the more the target has in common with the source. One way to measure commonality between the original (source) and the target tokenizer is the fraction of tokens of the target vocabulary which also exist in the source vocabulary (*vocabulary overlap*). Performance correlates with vocabulary overlap, but it correlates more strongly with the probability for tokens to overlap: that is, when randomly sampling some token from a corpus tokenized with $T_b$, the probability that this token also exists in the vocabulary of $T_a$. We refer to this metric as *$p(\text{overlap})$*. *$p(\text{overlap})$* has higher correlation with the performance of FOCUS, indicating that our hypernetwork depends less on overlap (Figure ).

# Additional LLM Results

Zero-shot and n-shot results for TinyLlama-1.1B are shown in Table  and MT-Bench results of transferring TinyLlama-1.1B-Chat-v1.0 in Table . We observe the same patterns as on Mistral-7B.

# Assumptions on the Tokenization Function

In practice, besides the tokenization algorithm itself (e.g. BPE, UnigramLM) tokenization functions also contain other steps, in particular *pretokenizing* text into smaller chunks (usually words) on which to apply the tokenization function . In our experiments, we assume fixed pretokenization given by a regular expression based on the regular expression used by GPT2 , adjusted to not over-segment text in languages using characters in the Unicode Mark category within words (e.g. Hindi and Tamil). We also add a *prefix space* (i.e., a whitespace at the start of the text to tokenize) if and only if the original tokenizer also uses a prefix space. Finally, we always add whitespace characters covering sequences of consecutive whitespaces up to 16 characters long similar to   to ensure code is tokenized efficiently. These light assumptions mostly preserve the generality of our method but could be further relaxed in future work.

[^1]: We adopt a broad definition of LMs that also includes models that do not define a probability distribution over finite-length sequences, such as text encoders.

[^2]: Some models share the input and the output embedding parameters , this has been shown to be problematic  and many recent LLMs  separate them.

[^3]: See also for a comprehensive overview of tokenizers.

[^4]: In practice, implementing $\bm{q}$ as a queue allows efficiently caching the substrings and their probability $p(t)$ at this step. They only need to be recomputed for the new $m$ texts encountered in every batch.

[^5]: To ensure substrings do not cross word boundaries we pretokenize the text before computing substrings.

[^6]: In the multilingual case, we also append an element containing a learnable language-specific embedding.

[^7]: We exclude languages without whitespace between words since they would require language-specific pretokenizers . Although our method is also applicable to this case, we leave this to future work.

[^8]: We use Flan v2 because we observed a strong decrease in accuracy from continuing to train on the MADLAD-400 data (even with the original tokenizer). The training data for most LLMs (including Mistral-7B) is not public, but it is plausible that this decrease stems from higher-quality data mixed in especially towards the end of training as in e.g. .

[^9]: Although (decoder-style) LLMs are the centerpiece of a large amount of current NLP research, encoder-style LMs have wide-ranging applications in e.g. retrieval  and LLM distillation  due to their lower computational cost.

[^10]: We chose these tokenizers due to their popularity and comparatively efficient encoding of the target domain.

[^11]: We note, however, that FVT comes close to FOCUS’ performance without requiring auxiliary embeddings so it may be a better choice for practical applications.

[^12]: 1/(1-14%)=16%, plus additional speedup due to attention scaling quadratically with sequence length.

[^13]: We refer to models purely pretrained on the Language Modeling task as *base models*.

[^14]: Averaged across the input and the output embeddings.

[^15]: Using the corrections from <https://github.com/InflectionAI/Inflection-Benchmarks>.

[^16]: We estimate FLOPs on the basis of XLA-compiled instructions using Jax .

[^17]: This is not sufficient for equivalence since order is ignored e.g. $T(x) = \{ab, a, b\}$ and $\hat{T}(x) = \{a, b, ab\}$ fulfill the criterion but are not equivalent.
