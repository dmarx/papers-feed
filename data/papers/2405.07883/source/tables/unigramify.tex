\begin{table}[!t]
\caption{Probability of pretokens sampled from the English MADLAD-400 data to be tokenized equivalently to the original tokenization when converting the tokenizer to byte-level (\textit{To Byte-Level}) or to UnigramLM (\textit{Unigramify}). Also shown is the LMs bits-per-character when applying the original vs. the corresponding UnigramLM tokenizer. Bits-per-character can not be measured for conversion to byte-level since extra tokens are added in this process (which there are no embeddings for).}
\centering
\small
\setlength\tabcolsep{3pt}
\begin{tabularx}{\linewidth}{llRRRR}
\toprule
& & \textbf{BERT} & \textbf{Mistral-7B} & \textbf{TinyLlama-1.1B} & \textbf{GPT2}\\
\multicolumn{2}{l}{Kind} & WordPiece & BPE & BPE & BBPE\\
\midrule
\multirow{2}{*}{Original} & $p(\text{preserved})$ & 100\% & 100\% & 100\% & 100\%\\
& bits per char & n/a & 0.675 & 0.747 & 0.930\\
\midrule
\multirow{2}{*}{To Byte-Level} & $p(\text{preserved})$ & 99.6\% & 99.9\% & 99.9\% & 100\%\\
& Extra Tokens & 162 & 522 & 362 & 0\\
\midrule
\multirow{2}{*}{Unigramify} & $p(\text{preserved})$ & 99.4\% & 99.8\% & 99.8\% & 99.7\%\\
& bits per char & n/a & 0.678 & 0.750 & 0.932\\
\bottomrule
\end{tabularx}
\label{table:unigramify}
\vspace{-0.3cm}
\end{table}