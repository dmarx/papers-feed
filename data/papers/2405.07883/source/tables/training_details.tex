\begin{table}[h]
\caption{Hypernetwork hyperparameters.}
\centering
\small
\begin{tabular}{lr}
\toprule
Optimizer & AdamW~\citep{loshchilov2018decoupled}\\
\quad $(\beta_1, \beta_2)$ & (0.9, 0.95)\\
\quad weight decay & 0.01 \\
Max. global gradient norm & 0.1\\
Sequence length & 128\\
Batch size & 128\\
Steps & 200000\\
\quad of which MIMICK-style warmup steps & 10000\\
MIMICK-style warmup learning rate schedule & linear warmup to 3-e4\\
Main learning rate schedule & linear warmup to 6e-5 until 10k, then cosine decay to 6e-6\\
Tokenizer sampling &\\
\quad Vocabulary size & 32768\\
\quad Distribution of noise level $z$ & $\mu=\ln(10^{-5}), \sigma=4$\\
\quad Batch size $m$ & 2048\\
Auxiliary loss weight & 0.5\\
Hypernetwork\\
\quad num. layers & 3\\
\quad max. sequence length & 7 (English + Code) or 15 (multilingual)\\
\quad hidden dimension & $d_{\text{model}}$\\
\quad FFN dimension & $2 d_{\text{model}}$\\
\quad num. attention heads & $\min(d_{\text{model}} / 64, 32)$\\

\bottomrule
\end{tabular}
\label{table:hypernetwork_hyperparameters}
\end{table}