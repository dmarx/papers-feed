\begin{table}[!t]
\caption{Parameter count and FLOPs estimates for our hypernetwork (and the corresponding main model) in different setups. The relatively lower computational cost compared to parameter count is mainly due to forgoing de-embedding which contributes significantly to FLOPs \citep{kaplan2020scaling}.}
\centering
\small
\setlength\tabcolsep{3pt}
\begin{tabular}{lrr@{\hskip 0.1in}rr}
\toprule
& \multicolumn{2}{c}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Hypernet}} \\ 
& \textbf{\#params} & \textbf{FLOPs / token} & \textbf{\#params} & \textbf{FLOPs / token} \\
\midrule
GPT2 & 124M & 253M & 21M (16\%) & 4.5M (1.8\%)\\
TinyLlama-1.1B & 1.1B & 2.1G & 170M (15\%) & 33.1M (1.6\%)\\
Mistral-7B & 7.2G & 15.4G & 678M\:\:\;(9\%) & 132.1M (0.9\%)\\
\bottomrule
\end{tabular}
\label{table:flops}
\end{table}