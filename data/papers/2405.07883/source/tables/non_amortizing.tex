\begin{table}[!h]
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\setlength{\extrarowheight}{.25ex}

\caption{Performance of the hypernetwork in bits-per-byte with and without inter-token attention. \textit{Sampled Tokenizers} are tokenizers as sampled during the training loop (c.f. Algorithm~\ref{alg:training}), \textit{en} is an English UnigramLM tokenizer. The respective vocabulary sizes are shown in brackets.}
\centering
\small
\begin{tabular}{lrrr}
\toprule
& \textbf{Sampled Tokenizers (32k)} & \textbf{GPT-NeoX (50k)} & \textbf{en (30k)}\\
\midrule
ours & 1.157 & \textbf{0.902} & \textbf{1.054}\\
ours (+ inter-token attention) & \textbf{1.118} & 0.904 & 1.103\\
\bottomrule
\end{tabular}
\label{table:non_amortizing}
\end{table}