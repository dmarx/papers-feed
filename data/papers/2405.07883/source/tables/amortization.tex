\begin{table}[!t]
\caption{Bits-per-character of GPT2 with the original tokenizer and the tokenization function being original (left), unigramified (middle) and UnigramLM with scores set to the substring frequency of the tokens (right). We compare the original embeddings with embeddings predicted from our hypernetwork, with or without Gaussian noise in the sampling process.}
\centering
\small
\setlength\tabcolsep{3pt}
\begin{tabularx}{\linewidth}{llRRR}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Embeddings}} & \multicolumn{3}{c}{\textbf{Tokenizer $\bm{(\mathcal{V}, T)}$}}\\
& & $(\text{GPT2}, \text{GPT2})$ & $(\text{GPT2}, \text{unigramify}(\text{GPT2}))$ & $(\text{GPT2}, \text{UnigramLM})$\\
\midrule
\multirow{3}{*}{GPT2} & original & 0.930 & 0.932 & 1.005\\
 & ours & \textbf{0.919} & \textbf{0.920} & \textbf{0.964}\\
& ours (no noise) & 0.925 & 0.926 & 0.978\\
\bottomrule
\end{tabularx}
\label{table:amortization}
\vspace{-0.3cm}
\end{table}