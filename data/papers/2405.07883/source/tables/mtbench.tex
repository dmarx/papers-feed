\begin{table}[t]
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\setlength{\extrarowheight}{.25ex}

\caption{Single model rating results on MT-Bench of transferring Mistral-7B-Instruct-v0.1 to the GPT2 tokenizer using the hypernetwork trained for the base Mistral-7B model. We use $\texttt{gpt-3.5-turbo-1106}$ as a judge. \textit{orig.} is the original fine-tuned model, \textit{base} the model with the same tokenizer but embeddings substituted for the base models' embeddings. $\lambda$ is the scaling factor for the weight differences in Task Arithmetic~\citep{ilharco2023editing}.}
\centering
\small
\begin{tabular}{l>{\columncolor{gray!20}}r>{\columncolor{gray!20}}rrrrrrrr}
\toprule
& \multicolumn{2}{c}{\cellcolor{gray!20}\textbf{original}} & \multicolumn{2}{c}{\textbf{0-shot}} & \multicolumn{4}{c}{\textbf{n-shot}}\\
\textbf{Embeddings} & orig. & base & FOCUS & ours & \multicolumn{4}{c}{ours@800}\\
\cmidrule{2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9}
$\lambda$ & - & - & - & - & 0.0 & 0.3 & 0.5 & 0.7\\
\cmidrule{2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9}
\textbf{Score (1 to 10)} & 7.33 & 7.48 & 5.03 & \textbf{6.56} & 6.59 & 6.75 & \textbf{6.82} & 6.77 &\\
\bottomrule
\end{tabular}
\label{table:mtbench}
\end{table}