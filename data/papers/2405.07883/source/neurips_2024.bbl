\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahia et~al.(2023)Ahia, Kumar, Gonen, Kasai, Mortensen, Smith, and Tsvetkov]{ahia-etal-2023-languages}
Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov.
\newblock Do all languages cost the same? tokenization in the era of commercial language models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.\  9904--9923, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.614}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.614}.

\bibitem[Ainsworth et~al.(2023)Ainsworth, Hayase, and Srinivasa]{ainsworth2023git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=CQsmMYmlP5T}.

\bibitem[Artetxe et~al.(2020)Artetxe, Ruder, and Yogatama]{artetxe-etal-2020-cross}
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
\newblock On the cross-lingual transferability of monolingual representations.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  4623--4637, Online, July 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.421}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.421}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{Bisk2020}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence}, 2020.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang, and Weinbach]{black-etal-2022-gpt}
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn~Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach.
\newblock {GPT}-{N}eo{X}-20{B}: An open-source autoregressive language model.
\newblock In Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Gall{\'e} (eds.), \emph{Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models}, pp.\  95--136, virtual+Dublin, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.bigscience-1.9}.
\newblock URL \url{https://aclanthology.org/2022.bigscience-1.9}.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary, Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs, 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Chen et~al.(2023)Chen, Marchisio, Raileanu, Adelani, Stenetorp, Riedel, and Artetxe]{chen2023improving}
Yihong Chen, Kelly Marchisio, Roberta Raileanu, David~Ifeoluwa Adelani, Pontus Stenetorp, Sebastian Riedel, and Mikel Artetxe.
\newblock Improving language plasticity via pretraining with active forgetting.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=jvEbQBxd8X}.

\bibitem[Chung et~al.(2021)Chung, Fevry, Tsai, Johnson, and Ruder]{chung2021rethinking}
Hyung~Won Chung, Thibault Fevry, Henry Tsai, Melvin Johnson, and Sebastian Ruder.
\newblock Rethinking embedding coupling in pre-trained language models.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=xpFFI_NtgpW}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark-etal-2019-boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock {B}ool{Q}: Exploring the surprising difficulty of natural yes/no questions.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  2924--2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1300}.
\newblock URL \url{https://aclanthology.org/N19-1300}.

\bibitem[Clark et~al.(2022)Clark, Garrette, Turc, and Wieting]{clark-etal-2022-canine}
Jonathan~H. Clark, Dan Garrette, Iulia Turc, and John Wieting.
\newblock Canine: Pre-training an efficient tokenization-free encoder for language representation.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 73--91, 2022.
\newblock \doi{10.1162/tacl_a_00448}.
\newblock URL \url{https://aclanthology.org/2022.tacl-1.5}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{allenai:arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv:1803.05457v1}, 2018.

\bibitem[Conneau \& Lample(2019)Conneau and Lample]{NEURIPS2019_c04c19c2}
Alexis Conneau and Guillaume Lample.
\newblock Cross-lingual language model pretraining.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf}.

\bibitem[Conneau et~al.(2018)Conneau, Rinott, Lample, Williams, Bowman, Schwenk, and Stoyanov]{conneau-etal-2018-xnli}
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov.
\newblock {XNLI}: Evaluating cross-lingual sentence representations.
\newblock In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun{'}ichi Tsujii (eds.), \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pp.\  2475--2485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1269}.
\newblock URL \url{https://aclanthology.org/D18-1269}.

\bibitem[Conneau et~al.(2020)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek, Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{conneau-etal-2020-unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  8440--8451, Online, July 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.747}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.747}.

\bibitem[Dagan et~al.(2024)Dagan, Synnaeve, and Rozière]{dagan2024getting}
Gautier Dagan, Gabriel Synnaeve, and Baptiste Rozière.
\newblock Getting the most out of your tokenizer for pre-training and domain adaptation, 2024.

\bibitem[de~Vries \& Nissim(2021)de~Vries and Nissim]{de-vries-nissim-2021-good}
Wietse de~Vries and Malvina Nissim.
\newblock As good as new. how to successfully recycle {E}nglish {GPT}-2 to make models for other languages.
\newblock In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pp.\  836--846, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-acl.74}.
\newblock URL \url{https://aclanthology.org/2021.findings-acl.74}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Dobler \& de~Melo(2023)Dobler and de~Melo]{dobler-de-melo-2023-focus}
Konstantin Dobler and Gerard de~Melo.
\newblock {FOCUS}: Effective embedding initialization for monolingual specialization of multilingual models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.\  13440--13454, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.829}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.829}.

\bibitem[Gee et~al.(2022)Gee, Zugarini, Rigutini, and Torroni]{gee-etal-2022-fast}
Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, and Paolo Torroni.
\newblock Fast vocabulary transfer for language model compression.
\newblock In Yunyao Li and Angeliki Lazaridou (eds.), \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track}, pp.\  409--416, Abu Dhabi, UAE, December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-industry.41}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-industry.41}.

\bibitem[Golkar et~al.(2023)Golkar, Pettee, Eickenberg, Bietti, Cranmer, Krawezik, Lanusse, McCabe, Ohana, Parker, Blancard, Tesileanu, Cho, and Ho]{golkar2023xval}
Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, Bruno R{\'e}galdo-Saint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, and Shirley Ho.
\newblock xval: A continuous number encoding for large language models.
\newblock In \emph{NeurIPS 2023 AI for Science Workshop}, 2023.
\newblock URL \url{https://openreview.net/forum?id=KHDMZtoF4i}.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, Arora, Atkinson, Authur, Chandu, Cohan, Dumas, Elazar, Gu, Hessel, Khot, Merrill, Morrison, Muennighoff, Naik, Nam, Peters, Pyatkin, Ravichander, Schwenk, Shah, Smith, Subramani, Wortsman, Dasigi, Lambert, Richardson, Dodge, Lo, Soldaini, Smith, and Hajishirzi]{Groeneveld2023OLMo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah~A. Smith, and Hannaneh Hajishirzi.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{Preprint}, 2024.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{ha2017hypernetworks}
David Ha, Andrew~M. Dai, and Quoc~V. Le.
\newblock Hypernetworks.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=rkpACe1lx}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendryckstest2021}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Hofmann et~al.(2022)Hofmann, Schuetze, and Pierrehumbert]{hofmann-etal-2022-embarrassingly}
Valentin Hofmann, Hinrich Schuetze, and Janet Pierrehumbert.
\newblock An embarrassingly simple method to mitigate undesirable properties of pretrained language model tokenizers.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  385--393, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-short.43}.
\newblock URL \url{https://aclanthology.org/2022.acl-short.43}.

\bibitem[Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister]{hsieh-etal-2023-distilling}
Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
\newblock Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pp.\  8003--8017, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.507}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.507}.

\bibitem[{IBM ILOG}(2022)]{cplex2022v22}
{IBM ILOG}.
\newblock V22.1: User’s manual for cplex.
\newblock 2022.

\bibitem[Ilharco et~al.(2023)Ilharco, Ribeiro, Wortsman, Schmidt, Hajishirzi, and Farhadi]{ilharco2023editing}
Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=6t0Kwf8-jrj}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Khattab \& Zaharia(2020)Khattab and Zaharia]{colbert}
Omar Khattab and Matei Zaharia.
\newblock Colbert: Efficient and effective passage search via contextualized late interaction over bert.
\newblock In \emph{Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval}, SIGIR '20, pp.\  39–48, New York, NY, USA, 2020. Association for Computing Machinery.
\newblock ISBN 9781450380164.
\newblock \doi{10.1145/3397271.3401075}.
\newblock URL \url{https://doi.org/10.1145/3397271.3401075}.

\bibitem[Kudo(2018)]{kudo-2018-subword}
Taku Kudo.
\newblock Subword regularization: Improving neural network translation models with multiple subword candidates.
\newblock In Iryna Gurevych and Yusuke Miyao (eds.), \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  66--75, Melbourne, Australia, July 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P18-1007}.
\newblock URL \url{https://aclanthology.org/P18-1007}.

\bibitem[Kudo \& Richardson(2018)Kudo and Richardson]{kudo-richardson-2018-sentencepiece}
Taku Kudo and John Richardson.
\newblock {S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
\newblock In Eduardo Blanco and Wei Lu (eds.), \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pp.\  66--71, Brussels, Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-2012}.
\newblock URL \url{https://aclanthology.org/D18-2012}.

\bibitem[Kudugunta et~al.(2023)Kudugunta, Caswell, Zhang, Garcia, Choquette-Choo, Lee, Xin, Kusupati, Stella, Bapna, and Firat]{kudugunta2023madlad400}
Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher~A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat.
\newblock Madlad-400: A multilingual and document-level large audited dataset, 2023.

\bibitem[Lai et~al.(2023)Lai, Nguyen, Ngo, Nguyen, Dernoncourt, Rossi, and Nguyen]{lai-etal-2023-okapi}
Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Nguyen.
\newblock Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback.
\newblock In Yansong Feng and Els Lefever (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pp.\  318--327, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-demo.28}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-demo.28}.

\bibitem[Land \& Bartolo(2024)Land and Bartolo]{land2024fishing}
Sander Land and Max Bartolo.
\newblock Fishing for magikarp: Automatically detecting under-trained tokens in large language models, 2024.

\bibitem[Li et~al.(2023)Li, allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, LI, Chim, Liu, Zheltonozhskii, Zhuo, Wang, Dehaene, Lamy-Poirier, Monteiro, Gontier, Yee, Umapathi, Zhu, Lipkin, Oblokulov, Wang, Murthy, Stillerman, Patel, Abulkhanov, Zocca, Dey, Zhang, Bhattacharyya, Yu, Luccioni, Villegas, Zhdanov, Lee, Timor, Ding, Schlesinger, Schoelkopf, Ebert, Dao, Mishra, Gu, Anderson, Dolan-Gavitt, Contractor, Reddy, Fried, Bahdanau, Jernite, Ferrandis, Hughes, Wolf, Guha, Werra, and de~Vries]{li2023starcoder}
Raymond Li, Loubna~Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry~Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh~Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason~T Stillerman, Siva~Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire~S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn~Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos~Mu{\~n}oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro~Von Werra, and Harm de~Vries.
\newblock Starcoder: may the source be with you!
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=KoFOg41haE}.
\newblock Reproducibility Certification.

\bibitem[Libovick{\'y} et~al.(2022)Libovick{\'y}, Schmid, and Fraser]{libovicky-etal-2022-dont}
Jind{\v{r}}ich Libovick{\'y}, Helmut Schmid, and Alexander Fraser.
\newblock Why don{'}t people use character-level machine translation?
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pp.\  2470--2485, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.findings-acl.194}.
\newblock URL \url{https://aclanthology.org/2022.findings-acl.194}.

\bibitem[Lin et~al.(2022)Lin, Mihaylov, Artetxe, Wang, Chen, Simig, Ott, Goyal, Bhosale, Du, Pasunuru, Shleifer, Koura, Chaudhary, O{'}Horo, Wang, Zettlemoyer, Kozareva, Diab, Stoyanov, and Li]{lin-etal-2022-shot}
Xi~Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit~Singh Koura, Vishrav Chaudhary, Brian O{'}Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li.
\newblock Few-shot learning with multilingual generative language models.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  9019--9052, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.616}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.616}.

\bibitem[Liu et~al.(2023)Liu, Lin, Wang, and Sch{\"u}tze]{liu2023ofa}
Yihong Liu, Peiqin Lin, Mingyang Wang, and Hinrich Sch{\"u}tze.
\newblock Ofa: A framework of initializing unseen subword embeddings for efficient large-scale multilingual continued pretraining.
\newblock \emph{arXiv preprint arXiv:2311.08849}, 2023.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, et~al.]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}, 2023.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Marchisio et~al.(2023)Marchisio, Lewis, Chen, and Artetxe]{marchisio-etal-2023-mini}
Kelly Marchisio, Patrick Lewis, Yihong Chen, and Mikel Artetxe.
\newblock Mini-model adaptation: Efficiently extending pretrained models to new languages via aligned shallow training.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pp.\  5474--5490, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.338}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.338}.

\bibitem[Mielke et~al.(2021)Mielke, Alyafeai, Salesky, Raffel, Dey, Gallé, Raja, Si, Lee, Sagot, and Tan]{mielke2021words}
Sabrina~J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson~Y. Lee, Benoît Sagot, and Samson Tan.
\newblock Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp, 2021.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and Dean]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space, 2013.

\bibitem[Minixhofer et~al.(2022)Minixhofer, Paischer, and Rekabsaz]{minixhofer-etal-2022-wechsel}
Benjamin Minixhofer, Fabian Paischer, and Navid Rekabsaz.
\newblock {WECHSEL}: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models.
\newblock In Marine Carpuat, Marie-Catherine de~Marneffe, and Ivan~Vladimir Meza~Ruiz (eds.), \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  3992--4006, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.293}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.293}.

\bibitem[Muennighoff et~al.(2023)Muennighoff, Liu, Zebaze, Zheng, Hui, Zhuo, Singh, Tang, von Werra, and Longpre]{muennighoff2023octopack}
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry~Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre.
\newblock Octopack: Instruction tuning code large language models.
\newblock \emph{arXiv preprint arXiv:2308.07124}, 2023.

\bibitem[Nawrot et~al.(2023)Nawrot, Chorowski, Lancucki, and Ponti]{nawrot-etal-2023-efficient}
Piotr Nawrot, Jan Chorowski, Adrian Lancucki, and Edoardo~Maria Ponti.
\newblock Efficient transformers with dynamic token pooling.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  6403--6417, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.353}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.353}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Gray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=TG8KACxEON}.

\bibitem[Parmar et~al.(2024)Parmar, Prabhumoye, Jennings, Patwary, Subramanian, Su, Zhu, Narayanan, Jhunjhunwala, Dattagupta, Jawa, Liu, Mahabaleshwarkar, Nitski, Brundyn, Maki, Martinez, You, Kamalu, LeGresley, Fridman, Casper, Aithal, Kuchaiev, Shoeybi, Cohen, and Catanzaro]{parmar2024nemotron4}
Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, and Bryan Catanzaro.
\newblock Nemotron-4 15b technical report, 2024.

\bibitem[Petrov et~al.(2023)Petrov, La~Malfa, Torr, and Bibi]{petrov2023language}
Aleksandar Petrov, Emanuele La~Malfa, Philip Torr, and Adel Bibi.
\newblock Language model tokenizers introduce unfairness between languages.
\newblock In A.~Oh, T.~Neumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine (eds.), \emph{Advances in Neural Information Processing Systems}, volume~36, pp.\  36963--36990. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/74bb24dca8334adce292883b4b651eda-Paper-Conference.pdf}.

\bibitem[Pinter et~al.(2017)Pinter, Guthrie, and Eisenstein]{pinter2017mimicking}
Yuval Pinter, Robert Guthrie, and Jacob Eisenstein.
\newblock Mimicking word embeddings using subword rnns.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pp.\  102--112, 2017.

\bibitem[Ponti et~al.(2020)Ponti, Glava{\v{s}}, Majewska, Liu, Vuli{\'c}, and Korhonen]{ponti-etal-2020-xcopa}
Edoardo~Maria Ponti, Goran Glava{\v{s}}, Olga Majewska, Qianchu Liu, Ivan Vuli{\'c}, and Anna Korhonen.
\newblock {XCOPA}: A multilingual dataset for causal commonsense reasoning.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  2362--2376, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.185}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.185}.

\bibitem[Poth et~al.(2023)Poth, Sterz, Paul, Purkayastha, Engl{\"a}nder, Imhof, Vuli{\'c}, Ruder, Gurevych, and Pfeiffer]{poth-etal-2023-adapters}
Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya Purkayastha, Leon Engl{\"a}nder, Timo Imhof, Ivan Vuli{\'c}, Sebastian Ruder, Iryna Gurevych, and Jonas Pfeiffer.
\newblock Adapters: A unified library for parameter-efficient and modular transfer learning.
\newblock In Yansong Feng and Els Lefever (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pp.\  149--160, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-demo.13}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-demo.13}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rust et~al.(2021)Rust, Pfeiffer, Vuli{\'c}, Ruder, and Gurevych]{rust-etal-2021-good}
Phillip Rust, Jonas Pfeiffer, Ivan Vuli{\'c}, Sebastian Ruder, and Iryna Gurevych.
\newblock How good is your tokenizer? on the monolingual performance of multilingual language models.
\newblock In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  3118--3135, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.243}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.243}.

\bibitem[Sagi \& Rokach(2018)Sagi and Rokach]{sagi2018ensemble}
Omer Sagi and Lior Rokach.
\newblock Ensemble learning: A survey.
\newblock \emph{Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery}, 8\penalty0 (4):\penalty0 e1249, 2018.

\bibitem[Schick \& Sch{\"u}tze(2019)Schick and Sch{\"u}tze]{schick-schutze-2019-attentive}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock Attentive mimicking: Better word embeddings by attending to informative contexts.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  489--494, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1048}.
\newblock URL \url{https://aclanthology.org/N19-1048}.

\bibitem[Schick \& Sch{\"u}tze(2020)Schick and Sch{\"u}tze]{schick-schutze-2020-bertram}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock {BERTRAM}: Improved word embeddings have big impact on contextualized model performance.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  3996--4007, Online, July 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.368}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.368}.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{sennrich-etal-2016-neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In Katrin Erk and Noah~A. Smith (eds.), \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1715--1725, Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1162}.
\newblock URL \url{https://aclanthology.org/P16-1162}.

\bibitem[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, Hofmann, Jha, Kumar, Lucy, Lyu, Lambert, Magnusson, Morrison, Muennighoff, Naik, Nam, Peters, Ravichander, Richardson, Shen, Strubell, Subramani, Tafjord, Walsh, Zettlemoyer, Smith, Hajishirzi, Beltagy, Groeneveld, Dodge, and Lo]{dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya~Harsh Jha, Sachin Kumar, Li~Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah~A. Smith, Hannaneh Hajishirzi, Iz~Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.
\newblock {Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}.
\newblock \emph{arXiv preprint}, 2024.

\bibitem[Sun(2012)]{sun2012jieba}
Junyi Sun.
\newblock Jieba chinese word segmentation tool.
\newblock 2012.

\bibitem[Tay et~al.(2022)Tay, Tran, Ruder, Gupta, Chung, Bahri, Qin, Baumgartner, Yu, and Metzler]{tay2022charformer}
Yi~Tay, Vinh~Q. Tran, Sebastian Ruder, Jai Gupta, Hyung~Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler.
\newblock Charformer: Fast character transformers via gradient-based subword tokenization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=JtBRnrlOEFN}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Tran(2020)]{tran2020english}
Ke~Tran.
\newblock From english to foreign languages: Transferring pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2002.07306}, 2020.

\bibitem[Uzan et~al.(2024)Uzan, Schmidt, Tanner, and Pinter]{uzan2024greed}
Omri Uzan, Craig~W. Schmidt, Chris Tanner, and Yuval Pinter.
\newblock Greed is all you need: An evaluation of tokenizer inference methods, 2024.

\bibitem[Wang et~al.(2024)Wang, Gangavarapu, Yan, and Rush]{wang2024mambabyte}
Junxiong Wang, Tushaar Gangavarapu, Jing~Nathan Yan, and Alexander~M Rush.
\newblock Mambabyte: Token-free selective state space model, 2024.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, and Schmidt]{pmlr-v162-wortsman22a}
Mitchell Wortsman, Gabriel Ilharco, Samir~Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt.
\newblock Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  23965--23998. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/wortsman22a.html}.

\bibitem[Xue et~al.(2022)Xue, Barua, Constant, Al-Rfou, Narang, Kale, Roberts, and Raffel]{xue-etal-2022-byt5}
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel.
\newblock {B}y{T}5: Towards a token-free future with pre-trained byte-to-byte models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 291--306, 2022.
\newblock \doi{10.1162/tacl_a_00461}.
\newblock URL \url{https://aclanthology.org/2022.tacl-1.17}.

\bibitem[Yadav et~al.(2023)Yadav, Tam, Choshen, Raffel, and Bansal]{yadav2023tiesmerging}
Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal.
\newblock {TIES}-merging: Resolving interference when merging models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=xtaX3WyCj1}.

\bibitem[Yu et~al.(2023)Yu, Simig, Flaherty, Aghajanyan, Zettlemoyer, and Lewis]{yu2023megabyte}
Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.
\newblock {MEGABYTE}: Predicting million-byte sequences with multiscale transformers.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=JTmO2V9Xpz}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers-etal-2019-hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock {H}ella{S}wag: Can a machine really finish your sentence?
\newblock In Anna Korhonen, David Traum, and Llu{\'\i}s M{\`a}rquez (eds.), \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pp.\  4791--4800, Florence, Italy, July 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1472}.
\newblock URL \url{https://aclanthology.org/P19-1472}.

\bibitem[Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
\newblock Tinyllama: An open-source small language model, 2024.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\end{thebibliography}
