\section{Background on Diffusion Models}

Score-based models and diffusion models allow to generate samples starting from easy-to-sample Gaussian noise to complex target distributions via
iteratively applying the score function of learned distribution during sampling, i.e., the gradient of underlying probability distribution $\nabla_{\mathbf{x}} \log p(\mathbf{x})$ with respect to $\mathbf{x}$. However, the exact estimations of the score functions are intractable. 

To bypass this problem, Yang Song et al. proposed slice score matching~\cite{song2020sliced}, and Ho et al. proposed Denoising Diffusion Probability Model (DDPM)~\cite{ho2020denoising} that first gradually perturbs the clean data with linear combinations of Gaussian noise and clean data, as $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x} + \sqrt{1 - \bar{\alpha}_t} \epsilon_t$ via the predefined timestep schedulers where $t \in[0, T]$ and $\epsilon_t \sim \mathcal{N}(0, \mathbf{I})$, then they finally become isotropic Gaussian noise as time reaches $T$, this is also referred as forward diffusion. The goal is to train a time-dependent neural network that can learn to denoise noisy samples given corresponding timestep $t$. Specifically, the training objective is the expectation over noise estimation MSE, which is formulated as $\mathbb{E}_{t, \mathbf{x}, \epsilon_t}[\| \epsilon_t - \epsilon_{\theta}(\mathbf{x}_t, t) \|_2^2]$, where $\epsilon_{\theta}$ denotes the parametrized neural network, DDPM adopted UNet~\cite{ronneberger2015u} as their noise estimating network. During inference time, we first generate a random Gaussian sample, then iteratively apply the noise estimation network $\epsilon_{\theta}$ and perform denoising operations to generate a new clean sample of the learned distribution. Particularly, Song et.al proposed DDIM~\cite{song2021denoisingdiffusionimplicitmodels} that generalized the DDPM sampling formulation as:

\begin{equation}
    \begin{aligned}
        \mathbf{x}_{t-1} &= \sqrt{\bar{\alpha}_{t - 1}}
        \left(\frac {\mathbf{x}_t - \sqrt{1- \bar{\alpha}_t}\epsilon_{\theta}(\mathbf{x}_t, t)}{\sqrt{\bar{\alpha}_t}}
        \right) \\
        &\quad + \sqrt{1-\bar{\alpha}_{t-1}-\sigma^{2}_t}\epsilon_{\theta}(\mathbf{x}_t, t) + \sigma_t \epsilon_t.
    \end{aligned}
    \label{eq:ddim}
\end{equation}

With $\sigma_t = \sqrt{(1-\bar{\alpha}_{t-1})/(1-\bar{\alpha}_t)} \cdot$ $\sqrt{1-\bar{\alpha}_t /\bar{\alpha}_{t-1} }$, Equation~\ref{eq:ddim} becomes DDPM, and when $\sigma_t = 0$, the sampling process become deterministic as proposed in DDIM since the added noise during each sampling step is null.
