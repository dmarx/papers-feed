\section{Related Work}


\subsection{Evasion Attack for Diffusion Model}
The prevalence of diffusion-based image editing techniques has posed the challenges of protecting images from maliciously editing.
These editing techniques are mainly based on SDEdit~\cite{meng2021sdedit} or its variations which can be applied to both PDM and LDM to produce the editing results. In general, the editing starts from first transforming the clean image (or the clean latent) into the noisy one by introducing Gaussian noise for the forward diffusion process followed by performing a series of reverse diffusion sampling steps with new conditions. In addition, based on different number of forward diffusion steps, the method could control the extent of the editing results obeying the new conditions while preserving the original image semantics. Notably, a small number of forward steps allow the editing results faithful to the original image, and more variations are introduced when larger forward step value is applied.

To counteract SDEdit-based editing, H. Salman et al. first proposed PhotoGuard~\cite{salman2023raisingcostmaliciousaipowered} to introduce two attacking paradigms based on Projected Gradient Descent (PGD)~\cite{madry2018towards}. The first is the Encoder Attack, which aims to disrupt the latent representations of the Variational Autoencoder (VAE) of the LDMs, and the second is the Diffusion Attack, which focuses more on disrupting the entire diffusion process of the LDMs. 

The Encoder Attack is simple yet effective, but the attacking results are sub-optimal due to its less flexibility for optimization than the Diffusion Attack. Although the Diffusion Attack achieves better attack results, it is prohibitively expensive due to its requirement of backpropagation through all the diffusion steps. In the following, we further introduce more relevant work for these attacks along with another common attack for diffusion models.

\subsubsection{Diffusion Attacks.}
Despite the cost of performing the Diffusion Attack, the higher generalizability and universally applicable nature drive previous works focusing on disrupting the process with lower cost. Liang et al.~\cite{liang2023adversarialexampledoesgood} proposed AdvDM to utilize the diffusion training loss as their attacking semantic loss. Then, AdvDM performs gradient ascent with the Monte Carlo method, aiming to disrupt the denoising process without calculating full backpropagation. Mist~\cite{liang2023mistimprovedadversarialexamples} also incorporates semantic loss and performs constrained optimization via PGD to achieve better attacking performance.

\subsubsection{Encoder Attacks.}
On the other hand, researchers found that VAEs in widely adopted LDMs are more vulnerable to attack at a lower cost while avoiding attacking the expensive diffusion process.~\cite{salman2023raisingcostmaliciousaipowered, liang2023mistimprovedadversarialexamples, shan2023glazeprotectingartistsstyle, xue2024effectiveprotectiondiffusionbased}, focus on disrupting the latent representation in LDM via PGD and highlights the encoder attacks are more effective against LDMs.

\subsubsection{Conditional Module Attacks.}
Most of the LDMs contain conditional modules for steering generation, previous works~\cite{shan2023glazeprotectingartistsstyle, shan2024nightshadepromptspecificpoisoningattacks, lo2024distraction} exploited the vulnerability of text conditioning modules. By disrupting the cross-attention between text concepts and image semantics, these methods could effectively interfere with the diffusion model for capturing the image-text alignment, thereby realizing the attack.

\subsubsection{Limitations of Current Methods.}
To our knowledge, previous works primarily focus on adversarial attacks for LDMs, while attacks on PDMs remain unexplored. Xue et al.~\cite{xue2024pixelbarrierdiffusionmodels} further emphasized the difficulty of attacking PDMs. However, in our work, we find that by crafting an adversarial image to corrupt the intermediate representation of diffusion UNet, we can achieve promising attack performance for PDMs, 
while the attack is also compatible with LDMs. Moreover, inspired by~\cite{laidlaw2021perceptual, liu2023instruct2attack} which utilizes LPIPS~\cite{zhang2018unreasonable} as distortion measure, we also propose a novel attacking loss as the measure to craft better adversarial images for PDMs.
