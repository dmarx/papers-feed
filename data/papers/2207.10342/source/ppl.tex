\section{\Cascades}

In this section, we show how to create
\cascades\ of LMs to tackle various
language-based reasoning problems.
A \emph{\cascade} is a probabilistic program that includes
string-valued random variables, sampled from an LM.
For example, Figure~\ref{fig:cot_cascade} is a simple \cascade\ for question answering.
%For example, Figure~\ref{fig:si_cascade} is a simple \cascade\ for the selection-inference algorithm.
Each of the \texttt{yield} expressions return a string distributed according to the language model \texttt{S}.\footnote{The first argument
to \texttt{S} defines a unique name for the random variable,
and the remaining arguments conditions the LM on a string prefix. A variable may be marked as observed within the program, \texttt{S('varname', obs='observed value')}, or at inference time} This program defines a joint distribution over the variables \texttt{question, thought}, and \texttt{answer}.  Programs with complex control flow and observations are included in Appendix~\ref{app:implementation}.

We implement \cascades\ as a trace-based probabilistic programming language embedded in Python via effect handlers, inspired by \citet{pyro, numpyro}, and via coroutines, inspired by \citet{pymc4}. A pretrained LM is used to parameterize all conditional distributions. A \cascade supports arbitrary control flow and recursion. While the current presentation is in terms of few-shot prompting of causal language models, we emphasize that the ideas are immediately applicable to finetuned models, masked LM setting, and other complex data types including images.

\subsection{Scratchpads and Chain of thought}
\label{sec:QTA}

As our first example, we show how to 
 represent a chain of thought \citep{scratchpads, chainofthought} as shown in 
\cref{fig:QTA} and subsequent graphical model figures; refer to the corresponding probabilistic programs in \cref{app:implementation}.
We condition the $A$ node not just on the test question $Q$,
but also on previous $(Q^m,T^m,A^m)$ triples, which 
constitute the few-shot prompting part of the model.
This is denoted by the shaded nodes inside the plate.
Inference can be implemented by ancestral
sampling.

%\input{figures/qta-simple.tex}
\input{figures/qta-plates.tex}


%In \cascades, we define the model in code in Figure~\ref{fig:cot_cascade}, where S is a (conditional) string distribution.
%The prompt for each variable is automatically constructed at inference time based on input variables and few shot examples.

% \charles{The program should not sample $q$, should it? Would it be more clear to leave off the 'question' etc arguments? Perhaps instead of question=q, we should use prefix=q, prefix=concat(q, t) to make it more generic? If we say this is a PPL, the reader is going to wonder whether we have observes. The program also does not explain how the prompt examples are used.}

% david: We can either say `S('question', obs='the question')`, or have the inference call inject the value (which is what I do in practice):
% `infer(program, observe(question='the question')`

% charles: Sure, the infer way is fine.  

% charles: I guess we can explain in the text that the first argument to S is a name for the r.v.

 % q = yield S('question')
  
\begin{figure}[h]
\begin{verbatim}
def qta():
  q = yield S('question')
  t = yield S('thought', question=q)
  a = yield S('answer', question=q, 
                        thought=t)
  return a
\end{verbatim}
\caption{Chain of thought \cascade\ in Python. Each \texttt{yield S(...)} statement samples a string from an LM. The name of the random variable is provided as the first argument to \texttt{S}.}
\label{fig:cot_cascade}
\end{figure}  

\subsection{Semi-supervised learning}
\label{sec:STAR}
In \cref{sec:QTA}, we provided a manually created set  $(Q^m,T^m,A^m)$ triples,
where the ``thoughts'' or ``rationalizations'' were provided.
A more scalable approach is to define a small set $D_S$
of such ``supervised'' triples,
but then to provide a larger set $D_L$ of $(Q^m,A^m)$ pairs,
which are easier to gather. % (e.g., by scraping question-answering web-sites).
We can augment the pairs in $D_L$ by adding 
the hidden $T^m$ variable to get a semi-supervised setup, shown in  
\cref{fig:QTAhidden}.

\input{figures/qta-hidden-plates}

The Self-Taught Reasoner (STaR) \citep{zelikman2022star} proposes a procedure for fine-tuning LMs in the chain-of-thought type setting.
We can interpret their method as a stochastic EM-like procedure in the cascade of \cref{fig:QTAhidden}.
In particular, they first fine-tune on the ``fully observed''
dataset $D_S = \{(Q^m,T^m,A^m)\}$.
Then they impute the unknown $T_i$ values in the 
``partially observed'' dataset  $D_L = \{(Q^m,T^m=?,A^m)\}$
during the ``E'' step by doing rejection sampling on $p(T, A | Q^m)$ until finding a thought which leads to the known correct answer. If sampling $(T,A)$ given the question fails to find the correct answer, they sample thoughts from $p(T | Q^m, A^m)$. This uses a recognition network to approximately sample from the posterior distribution over thoughts given the known correct answer.
%where the true answer is added to the prompt.
They call this approach  ``rationale generation with rationalization''. They then update the parameters in the ``M'' step based on these imputed thoughts.
By interpreting the rationale generation at this higher level of abstraction, we open up the possibility of applying this tuning method to other types of cascades.


%See \cref{sec:STAR} for details.

% We can fine-tune the model on this semi-supervised data
% using an EM-like procedure,
% as proposed for STaR (self-taught reasoner) \citep{zelikman2022star}.
% In the E step, they impute the unknown $T^m$ values in the $D_L$
% dataset, and in the M step, they update the LM parameters.
% 
% In the E step, thoughts are imputed by sampling from 
% $p(T | Q^m, A=A^m)$ using rejection sampling,
% where $p$ represents the language model,
% and they use $p(T,A|Q^m)$ as the proposal.
% If this fails to yield any correct answers,
% they use a more powerful proposal, 
%  $p(T,A|Q^m,A^m)$,
%  where the true answer is added to the prompt.
% They call this approach 
% ``rationale generation with rationalization''

%\input{star-old}

\subsection{Selection-Inference}
\label{sec:selection_inference}

Selection Inference \citep{selection_inference} is a recent example of multiple interacting LM modules. It proposes splitting reasoning into: the \textit{selection} module which selects a subset of facts given a question, and the \textit{inference} module which infers new facts given this subset.
%See \cref{sec:selection_inference} for details.

It may be represented by the model in \cref{fig:selective}. Here $S$ is the selection of a subset of ``facts''
from a pre-specified set of facts,
and $I$ is an inference driven by that fact.
The $S$ and $I$ nodes
can be iterated to do multistep reasoning.
The model is ``trained'' by giving it examples,
$D = \{ (Q^m, \{F^{mj}\}, S^m, I^m, A^m): m=1:M\}$,
as part of the prompt.
% It is then given a test question,
% and the answer is computed using
% $A \sim p(A|Q,D)$,
% ignoring (``marginalizing out'')\todo{ddohan: is this actually marginalizing out? It's just taking a single monte carlo sample by default. I would expect > 1 sample to marginalize}
% any intermediate $S$ or $I$ strings that
% might be generated by the model.

\input{figures/selective}

\subsection{Verifiers}
\label{sec:verifiers}

Although adding explicit ``thought'' variables to a model
has been found to improve performance, models still arrive at incorrect answers, or the correct answer for an erroneous reason.
An intuitive way to improve model performance is to train it to judge whether an answer and thought are likely to be ``valid''. \citet{verifiers} propose using a separate model as a verifier to filter solutions to reasoning tasks.
%sometimes
%these ``thoughts'' are erroneous,
%even though the answer may be correct.

We can create a ``labeled'' training
set of the form
$D = \{ (Q^m, T^m, A^m, V^m\}$,
where we add a ``verification'' label 
$V^m \in \{0, 1 \}$,
representing whether the thought $T^m$
is a valid form of reasoning for deriving
$A^m$ from $Q^m$, and $A^m$ is the correct answer.
This can be particularly helpful in settings
where there may be more than one way of deriving
the answer. The verifiers may be used to reject incorrect examples in ancestral sampling, and the thought generator may itself be conditioned on the verifiers being correct by finetuning or prompting, reminiscent of RL as inference \cite{rl_inference} and goal-conditioned policies such as decision-transformer \cite{decision_transformer}.

%\input{figures/verifier-david}
%\input{figures/verifier-buffer-line}
%\input{figures/verifier-ladder}
\input{figures/verifier-buffer-unlabeled}

We can extend this to  $N$-step reasoning as follows
(where  we drop conditioning on $D$
for brevity):
{\footnotesize
\begin{align*}
p(A|Q,V_{1:N}=1)
&\propto \sum_{T_{1:N}}
p(A,T_{1:N},V_{1:N}=1\,|\,Q),
\end{align*}
}
where
{\footnotesize
\begin{align*}
p(A,T_{1:N},V_{1:N}=1|Q)
&= \left[ \prod_{t=1}^N p(T_t|T_{1:t-1},Q)
p(V_t=1|T_{1:t},Q) \right] \\
& \times p(A|T_{1:N},Q).
\end{align*}
}
\noindent We can represent this 
as shown in \cref{fig:verifier}.
% where the 
% double-ringed nodes are deterministic
% buffer variables that accumulates all the past strings.

% TODO: Clarify this. bpoole didn't get & he understands ladder VAEs well
%(This restores the first-order Markov property,
%and is similar to how RNNs are used
%to define ladder VAEs \citep{Sonderby2016ladder}.)

To see why such a verification model can be useful,
consider (for simplicity) the case where $N=1$.
Suppose we have trained the model to generate valid
thoughts and answers by giving it suitable training examples,
and then we generate $K$ samples
$(T^k,V^k,A^k) \sim p(T,V,A|Q,D)$.
We can then rank the samples for validity by computing
$r^k = p(V^k=1|A^k, Q, D)$,
and then picking the  $A^k$ with largest score $r^k$.

\citet{verifiers} train the verifier to predict a binary correctness label.  \citet{language_feedback} incorporates natural language feedback, and finds that learning is significantly more sample efficient. Preliminary evidence suggests that LMs are capable of critiquing their own chain of reasoning in language, in which case the verifier produces natural language and $p(V_{1:N} = 1 | Q, A, T_{1:N})$ becomes the likelihood of the verifier taking on a particular string value, such as $p(V_{1:N}=\text{"The reasoning and solution are correct."} | ...)$. \citet{openai_critique} study model generated critiques in the context of summarization.

% \ddohan{We view this feedback as an auxiliary variable which can be conditioned to inform inference.}

% \kevin{Should we mention Rapha's preliminary results?}

\begin{comment}
One way to perform inference in this model 
would be to use
particle filtering. The hope is that, by conditioning
on $V_{1:N}=1$,
we increase the probability that the sampled
$T_{1:N}$ values constitute
a valid chain of reasoning
for deriving $A$ from $Q$ and $D$.
This can then provide higher quality training data
in an EM-like fine-tuning scheme, similar
to STaR in \cref{sec:STAR}.
However, we leave evaluation of this idea to future work.
\end{comment}

\subsection{Tool-use}
The applications discussed so far involve iterating a language model, within some control flow, without external feedback. There are many tasks of interest in which a model is interacting with external systems. \citet{verifiers} has an LM use a calculator to solve math tasks, while \citet{webgpt} put an LM in a loop with a web browser to answer questions. Using PPLs to represent these probabilistic models allows easily representing these cases, by writing the call to the external tool, such as the calculator, directly
into the program.
Then techniques from simulation based inference, for example, can be applied to do inference in such situations \cite{simulation_inference}.

\begin{comment}
\charles{Suggest cut for time.}
\kevin{Agreed}
\input{figures/webgpt}
\end{comment}