% \section{Scope and limitations}
% We frame many existing algorithms for composing models in terms of probabilistic programming. While this suggests the possibility of applying a variety of existing inference and train-time techniques to the resulting models, the present work does not evaluate methods beyond rejection sampling.
% 
% A challenge applying cascades in practice is the difficulty of probabilistic inference in models with string-valued variables. Previous work in particle based inference for probabilistic programs provides some hope in this direction \citep{anglican}.

% This perspective opens up exciting new directions in applications of language models, and in foundation models more generally.
% Existing fine-tuning methods can be described with a principled probabilistic programming language formalism representing structured distributions known as LM Cascades. This defines the distribution on the string-valued output of a large language model, the compositions of which may be adapted as specific inference algorithms underpinning various applications. 


% efficiency / expense
% We don't actually explore specific inference methods
% Hint of 
% hints @ lots of possibilities, explores very few of them
% - multimodality
% - train/test time inference beyond rejection sampling
% - tool use