%\section{Conclusion and future work}
\section{Discussion}
% TODO: Combine conclusion, limitations, and future work
We have shown how probabilistic programming provides a flexible formalism for composing models together to define complex probabilistic models over strings, placing many existing algorithms in a unified framework. While this suggests the possibility of applying a variety of existing inference and train-time techniques to the resulting models, the present work does not evaluate methods beyond rejection sampling.

%beyond the examples that we mentioned, one can also use this framework
%to represent lms that interact with external systems,
%such as calculators or search engines,
%in order to perform a variety of tasks.
We can also cast many planning and RL tasks in our framework, by using the perspective of control as inference.
While we restrict presentation to the string setting, the ideas presented here are applicable to multimodal settings as well, allowing us to combine image and text models into a larger system.

% We frame many existing algorithms for composing models in terms of probabilistic programming. While this suggests the possibility of applying a variety of existing inference and train-time techniques to the resulting models, the present work does not evaluate methods beyond rejection sampling.
A challenge applying cascades in practice is the difficulty of probabilistic inference in models with string-valued variables. Previous work in particle based inference for probabilistic programs provides some hope in this direction \citep{anglican}.

The core technical challenge is efficient inference, 
as is usually the case with PPLs. A key insight, which we intend
to explore in future work, is that we can emulate posterior
inference by training the LM 
to ``fill in the blanks'', corresponding to the unknown variables.
A similar idea is explored in 
foundation posteriors \citep{foundationposterior}, applied to Stan probabilistic programs, demonstrating that LMs are applicable to numerical data types as well.
In other words, we can use LMs as proposal distributions,
or guide networks.
%as well as a way of specifying the model.
We also intend to explore fine-tuning methods, going
beyond the few-shot prompting approach described here.

Recent advances in program synthesis suggest the possibility of \textit{probabilistic program induction} \citep{Lake2015,language_of_thought} to search for \cascades which solve a target task, rather than assuming a fixed probabilistic program structure.

