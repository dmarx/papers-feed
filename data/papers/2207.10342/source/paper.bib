@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@article{aqua,
  author    = {Wang Ling and
               Dani Yogatama and
               Chris Dyer and
               Phil Blunsom},
  title     = {Program Induction by Rationale Generation: Learning to Solve and Explain
               Algebraic Word Problems},
  journal   = {CoRR},
  volume    = {abs/1705.04146},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.04146},
  eprinttype = {arXiv},
  eprint    = {1705.04146},
  timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LingYDB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{esnli,
 author = {Camburu, Oana-Maria and Rockt\"{a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {e-SNLI: Natural Language Inference with Natural Language Explanations},
 url = {https://proceedings.neurips.cc/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{ortega2021shaking,
  title={Shaking the foundations: delusions in sequence models for interaction and control},
  author={Ortega, Pedro A and Kunesch, Markus and Del{\'e}tang, Gr{\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Veness, Joel and Buchli, Jonas and Degrave, Jonas and Piot, Bilal and Perolat, Julien and others},
  journal={arXiv preprint arXiv:2110.10819},
  year={2021}
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@misc{promptchainer,
  doi = {10.48550/ARXIV.2203.06566},
  
  url = {https://arxiv.org/abs/2203.06566},
  
  author = {Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J},
  
  keywords = {Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PromptChainer: Chaining Large Language Model Prompts through Visual Programming},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@ARTICLE{scratchpads,
  title         = "Show Your Work: Scratchpads for Intermediate Computation
                   with Language Models",
  author        = "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy
                   and Michalewski, Henryk and Austin, Jacob and Bieber, David
                   and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and
                   Luan, David and Sutton, Charles and Odena, Augustus",
  abstract      = "Large pre-trained language models perform remarkably well on
                   tasks that can be done ``in one pass'', such as generating
                   realistic text or synthesizing computer programs. However,
                   they struggle with tasks that require unbounded multi-step
                   computation, such as adding integers or executing programs.
                   Surprisingly, we find that these same models are able to
                   perform complex multi-step computations -- even in the
                   few-shot regime -- when asked to perform the operation
                   ``step by step'', showing the results of intermediate
                   computations. In particular, we train transformers to
                   perform multi-step computations by asking them to emit
                   intermediate computation steps into a ``scratchpad''. On a
                   series of increasingly complex tasks ranging from long
                   addition to the execution of arbitrary programs, we show
                   that scratchpads dramatically improve the ability of
                   language models to perform multi-step computations.",
  month         =  nov,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2112.00114"
}

@ARTICLE{chainofthought,
  title         = "Chain of Thought Prompting Elicits Reasoning in Large
                   Language Models",
  author        = "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma,
                   Maarten and Chi, Ed and Le, Quoc and Zhou, Denny",
  abstract      = "Although scaling up language model size has reliably
                   improved performance on a range of NLP tasks, even the
                   largest models currently struggle with certain reasoning
                   tasks such as math word problems, symbolic manipulation, and
                   commonsense reasoning. This paper explores the ability of
                   language models to generate a coherent chain of thought -- a
                   series of short sentences that mimic the reasoning process a
                   person might have when responding to a question. Experiments
                   show that inducing a chain of thought via prompting can
                   enable sufficiently large language models to better perform
                   reasoning tasks that otherwise have flat scaling curves.
                   When combined with the 540B parameter PaLM model, chain of
                   thought prompting achieves new state of the art of 58.1\% on
                   the GSM8K benchmark of math word problems.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.11903"
}


@ARTICLE{selfconsistency,
  title         = "{Self-Consistency} Improves Chain of Thought Reasoning in
                   Language Models",
  author        = "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le,
                   Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha
                   and Zhou, Denny",
  abstract      = "We explore a simple ensemble strategy, self-consistency,
                   that significantly improves the reasoning accuracy of large
                   language models. The idea is to sample a diverse set of
                   reasoning paths from a language model via chain of thought
                   prompting then return the most consistent final answer in
                   the set. We evaluate self-consistency on a range of
                   arithmetic and commonsense reasoning benchmarks, and find
                   that it robustly improves accuracy across a variety of
                   language models and model scales without the need for
                   additional training or auxiliary models. When combined with
                   a recent large language model, PaLM-540B, self-consistency
                   increases performance to state-of-the-art levels across
                   several benchmark reasoning tasks, including GSM8K (56.5\%
                   -> 74.4\%), SVAMP (79.0\% -> 86.6\%), AQuA (35.8\% ->
                   48.3\%), StrategyQA (75.3\% -> 81.6\%) and ARC-challenge
                   (85.2\% -> 88.7\%).",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2203.11171"
}


@ARTICLE{foundationposterior,
  title         = "Foundation Posteriors for Approximate Probabilistic
                   Inference",
  author        = "Wu, Mike and Goodman, Noah",
  abstract      = "Probabilistic programs provide an expressive representation
                   language for generative models. Given a probabilistic
                   program, we are interested in the task of posterior
                   inference: estimating a latent variable given a set of
                   observed variables. Existing techniques for inference in
                   probabilistic programs often require choosing many
                   hyper-parameters, are computationally expensive, and/or only
                   work for restricted classes of programs. Here we formulate
                   inference as masked language modeling: given a program, we
                   generate a supervised dataset of variables and assignments,
                   and randomly mask a subset of the assignments. We then train
                   a neural network to unmask the random values, defining an
                   approximate posterior distribution. By optimizing a single
                   neural network across a range of programs we amortize the
                   cost of training, yielding a ``foundation'' posterior able
                   to do zero-shot inference for new programs. The foundation
                   posterior can also be fine-tuned for a particular program
                   and dataset by optimizing a variational inference objective.
                   We show the efficacy of the approach, zero-shot and
                   fine-tuned, on a benchmark of STAN programs.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2205.09735"
}



% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{ELK,
  title        = "{ARC's} first technical report: Eliciting Latent Knowledge -
                  {AI} Alignment Forum",
  abstract     = "ARC has published a report on Eliciting Latent Knowledge, an
                  open problem which we believe is central to alignment. We
                  think reading this report is the clearest way to understand
                  what problems we areâ€¦",
  url = "https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge",
  year         = 2022,
}

@ARTICLE{gpt3,
  title   = "Language models are few-shot learners",
  author  = "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah,
             Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan,
             Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and
             {Others}",
  journal = "arXiv preprint arXiv:2005. 14165",
  year    =  2020
}


@ARTICLE{selection_inference,
  title         = "{Selection-Inference}: Exploiting Large Language Models for
                   Interpretable Logical Reasoning",
  author        = "Creswell, Antonia and Shanahan, Murray and Higgins, Irina",
  abstract      = "Large language models (LLMs) have been shown to be capable
                   of impressive few-shot generalisation to new tasks. However,
                   they still tend to perform poorly on multi-step logical
                   reasoning problems. Here we carry out a comprehensive
                   evaluation of LLMs on 50 tasks that probe different aspects
                   of logical reasoning. We show that language models tend to
                   perform fairly well at single step inference or entailment
                   tasks, but struggle to chain together multiple reasoning
                   steps to solve more complex problems. In light of this, we
                   propose a Selection-Inference (SI) framework that exploits
                   pre-trained LLMs as general processing modules, and
                   alternates between selection and inference to generate a
                   series of interpretable, casual reasoning steps leading to
                   the final answer. We show that a 7B parameter LLM used
                   within the SI framework in a 5-shot generalisation setting,
                   with no fine-tuning, yields a performance improvement of
                   over 100\% compared to an equivalent vanilla baseline on a
                   suite of 10 logical reasoning tasks. The same model in the
                   same setting even outperforms a significantly larger 280B
                   parameter baseline on the same suite of tasks. Moreover,
                   answers produced by the SI framework are accompanied by a
                   causal natural-language-based reasoning trace, which has
                   important implications for the safety and trustworthiness of
                   the system.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2205.09712"
}


@ARTICLE{webgpt,
  title         = "{WebGPT}: Browser-assisted question-answering with human
                   feedback",
  author        = "Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and
                   Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse,
                   Christopher and Jain, Shantanu and Kosaraju, Vineet and
                   Saunders, William and Jiang, Xu and Cobbe, Karl and
                   Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and
                   Knight, Matthew and Chess, Benjamin and Schulman, John",
  abstract      = "We fine-tune GPT-3 to answer long-form questions using a
                   text-based web-browsing environment, which allows the model
                   to search and navigate the web. By setting up the task so
                   that it can be performed by humans, we are able to train
                   models on the task using imitation learning, and then
                   optimize answer quality with human feedback. To make human
                   evaluation of factual accuracy easier, models must collect
                   references while browsing in support of their answers. We
                   train and evaluate our models on ELI5, a dataset of
                   questions asked by Reddit users. Our best model is obtained
                   by fine-tuning GPT-3 using behavior cloning, and then
                   performing rejection sampling against a reward model trained
                   to predict human preferences. This model's answers are
                   preferred by humans 56\% of the time to those of our human
                   demonstrators, and 69\% of the time to the highest-voted
                   answer from Reddit.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.09332"
}

@ARTICLE{verifiers,
  doi = {10.48550/ARXIV.2110.14168},
  
  url = {https://arxiv.org/abs/2110.14168},
  
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training Verifiers to Solve Math Word Problems},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{prompttuning,
  title         = "The Power of Scale for {Parameter-Efficient} Prompt Tuning",
  author        = "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
  abstract      = "In this work, we explore ``prompt tuning'', a simple yet
                   effective mechanism for learning ``soft prompts'' to
                   condition frozen language models to perform specific
                   downstream tasks. Unlike the discrete text prompts used by
                   GPT-3, soft prompts are learned through backpropagation and
                   can be tuned to incorporate signal from any number of
                   labeled examples. Our end-to-end learned approach
                   outperforms GPT-3's ``few-shot'' learning by a large margin.
                   More remarkably, through ablations on model size using T5,
                   we show that prompt tuning becomes more competitive with
                   scale: as models exceed billions of parameters, our method
                   ``closes the gap'' and matches the strong performance of
                   model tuning (where all model weights are tuned). This
                   finding is especially relevant in that large models are
                   costly to share and serve, and the ability to reuse one
                   frozen model for multiple downstream tasks can ease this
                   burden. Our method can be seen as a simplification of the
                   recently proposed ``prefix tuning'' of Li and Liang (2021),
                   and we provide a comparison to this and other similar
                   approaches. Finally, we show that conditioning a frozen
                   model with soft prompts confers benefits in robustness to
                   domain transfer, as compared to full model tuning.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.08691"
}

@article{bigbench,
   title = "Beyond the Imitation Game: Measuring and extrapolating the capabilities of language models",
   author = "{BIG-bench collaboration}", 
   year = "2021",
   journal = "In preparation",
   url = "https://github.com/google/BIG-bench/"
   }

@ARTICLE{Lake2015,
  title    = "Human-level concept learning through probabilistic program
              induction",
  author   = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  abstract = "People learning new concepts can often generalize successfully
              from just a single example, yet machine learning algorithms
              typically require tens or hundreds of examples to perform with
              similar accuracy. People can also use learned concepts in richer
              ways than conventional algorithms-for action, imagination, and
              explanation. We present a computational model that captures these
              human learning abilities for a large class of simple visual
              concepts: handwritten characters from the world's alphabets. The
              model represents concepts as simple programs that best explain
              observed examples under a Bayesian criterion. On a challenging
              one-shot classification task, the model achieves human-level
              performance while outperforming recent deep learning approaches.
              We also present several ``visual Turing tests'' probing the
              model's creative generalization abilities, which in many cases
              are indistinguishable from human behavior.",
  journal  = "Science",
  volume   =  350,
  number   =  6266,
  pages    = "1332--1338",
  month    =  dec,
  year     =  2015,
  language = "en"
}


@ARTICLE{kernelsearch,
  title    = "Structure Discovery in Nonparametric Regression through
              Compositional Kernel Search",
  author   = "Duvenaud, David and Lloyd, James Robert and Grosse, Roger and
              Tenenbaum, Joshua B and Ghahramani, Zoubin",
  abstract = "Despite its importance, choosing the structural form of the
              kernel in nonparametric regression remains a black art. We define
              a space of kernel structures which are built compositionally by
              adding and multiplying a small number of base kernels. We present
              a method for searching over this space of structures which
              mirrors the scientific discovery process. The learned structures
              can often decompose functions into interpretable components and
              enable long-range extrapolation on time-series datasets. Our
              structure search method outperforms many widely used kernels and
              kernel combination methods on a variety of prediction tasks.",
  journal  = "arXiv [stat.ML]",
  month    =  feb,
  year     =  2013
}


@misc{probmods2,
  title = {{Probabilistic Models of Cognition}},
  edition = {Second},
  author = {Goodman, Noah D and Tenenbaum, Joshua B. and The ProbMods Contributors},
  year = {2016},
  howpublished = {\url{http://probmods.org/v2}},
  note = {Accessed: 2022-5-24}
}
@misc{agentmodels,
  title = {{Modeling Agents with Probabilistic Programs}},
  author = {Evans, Owain and Stuhlm\"{u}ller, Andreas and Salvatier, John and Filan, Daniel},
  year = {2017},
  howpublished = {\url{http://agentmodels.org}},
  note = {Accessed: 2022-5-24}
}

@ARTICLE{amplification,
  title         = "Supervising strong learners by amplifying weak experts",
  author        = "Christiano, Paul and Shlegeris, Buck and Amodei, Dario",
  abstract      = "Many real world learning tasks involve complex or
                   hard-to-specify objectives, and using an easier-to-specify
                   proxy can lead to poor performance or misaligned behavior.
                   One solution is to have humans provide a training signal by
                   demonstrating or judging performance, but this approach
                   fails if the task is too complicated for a human to directly
                   evaluate. We propose Iterated Amplification, an alternative
                   training strategy which progressively builds up a training
                   signal for difficult problems by combining solutions to
                   easier subproblems. Iterated Amplification is closely
                   related to Expert Iteration (Anthony et al., 2017; Silver et
                   al., 2017), except that it uses no external reward function.
                   We present results in algorithmic environments, showing that
                   Iterated Amplification can efficiently learn complex
                   behaviors.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1810.08575"
}

@ARTICLE{debate,
  title         = "{AI} safety via debate",
  author        = "Irving, Geoffrey and Christiano, Paul and Amodei, Dario",
  abstract      = "To make AI systems broadly useful for challenging real-world
                   tasks, we need them to learn complex human goals and
                   preferences. One approach to specifying complex goals asks
                   humans to judge during training which agent behaviors are
                   safe and useful, but this approach can fail if the task is
                   too complicated for a human to directly judge. To help
                   address this concern, we propose training agents via self
                   play on a zero sum debate game. Given a question or proposed
                   action, two agents take turns making short statements up to
                   a limit, then a human judges which of the agents gave the
                   most true, useful information. In an analogy to complexity
                   theory, debate with optimal play can answer any question in
                   PSPACE given polynomial time judges (direct judging answers
                   only NP questions). In practice, whether debate works
                   involves empirical questions about humans and the tasks we
                   want AIs to perform, plus theoretical questions about the
                   meaning of AI alignment. We report results on an initial
                   MNIST experiment where agents compete to convince a sparse
                   classifier, boosting the classifier's accuracy from 59.4\%
                   to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4
                   pixels. Finally, we discuss theoretical and practical
                   aspects of the debate model, focusing on potential
                   weaknesses as the model scales up, and we propose future
                   human and computer experiments to test these properties.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1805.00899"
}

@MISC{factored_cognition,
  title        = "Factored Cognition",
  author = "Ought",
  abstract     = "Can we solve difficult problems by composing small
                  contributions from individual agents who don't know the big
                  picture?",
  howpublished = "\url{https://ought.org/research/factored-cognition}",
  note         = "Accessed: 2022-5-24",
  language     = "en"
}


@ARTICLE{iterated_amplification,
  title    = "Machine Learning Projects for Iterated Distillation and
              Amplification",
  author   = "Evans, Owain and Saunders, W and Stuhlm{\"u}ller, Andreas",
  abstract = "Iterated Distillation and Amplification (IDA) is a framework for
              training ML models. IDA is related to existing frameworks like
              imitation learning and reinforcement learning, but it aims to
              solve tasks for which humans cannot construct a suitable reward
              function or solve directly. This document reviews IDA and
              proposes three projects that explore aspects of IDA. Project 1
              applies IDA to problems in highschool mathematics and
              investigates whether learning to decompose problems can improve
              performance over supervised learning. Project 2 applies IDA to
              neural program interpretation, where neural nets are trained on
              the internal behavior (execution traces) of traditional computer
              programs. Project 3 investigates whether adaptive computation
              time (varying compute at inference time as a function of the
              input) can improve the robustness and efficiency of IDA. Our goal
              in outlining these projects is to generate discussion and
              encourage research on IDA. We are not (as of June 2019) working
              on these projects, but we are interested in collaboration.",
  year     =  2019,
  language = "en"
}



@ARTICLE{Fan2018-gg,
  title    = "Hierarchical Neural Story Generation",
  author   = "Fan, Angela and Lewis, M and Dauphin, Yann",
  abstract = "This work collects a large dataset of 300K human-written stories
              paired with writing prompts from an online forum that enables
              hierarchical story generation, where the model first generates a
              premise, and then transforms it into a passage of text. We
              explore story generation: creative systems that can build
              coherent and fluent passages of text about a topic. We collect a
              large dataset of 300K human-written stories paired with writing
              prompts from an online forum. Our dataset enables hierarchical
              story generation, where the model first generates a premise, and
              then transforms it into a passage of text. We gain further
              improvements with a novel form of model fusion that improves the
              relevance of the story to the prompt, and adding a new gated
              multi-scale self-attention mechanism to model long-range context.
              Experiments show large improvements over strong baselines on both
              automated and human evaluations. Human judges prefer stories
              generated by our approach to those from a strong non-hierarchical
              model by a factor of two to one.",
  journal  = "undefined",
  year     =  2018,
  language = "en"
}

@ARTICLE{Irving2018-uw,
  title    = "{AI} safety via debate",
  author   = "Irving, Geoffrey and Christiano, Paul and Amodei, Dario",
  abstract = "To make AI systems broadly useful for challenging real-world
              tasks, we need them to learn complex human goals and preferences.
              One approach to specifying complex goals asks humans to judge
              during training which agent behaviors are safe and useful, but
              this approach can fail if the task is too complicated for a human
              to directly judge. To help address this concern, we propose
              training agents via self play on a zero sum debate game. Given a
              question or proposed action, two agents take turns making short
              statements up to a limit, then a human judges which of the agents
              gave the most true, useful information. In an analogy to
              complexity theory, debate with optimal play can answer any
              question in PSPACE given polynomial time judges (direct judging
              answers only NP questions). In practice, whether debate works
              involves empirical questions about humans and the tasks we want
              AIs to perform, plus theoretical questions about the meaning of
              AI alignment. We report results on an initial MNIST experiment
              where agents compete to convince a sparse classifier, boosting
              the classifier's accuracy from 59.4\% to 88.9\% given 6 pixels
              and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss
              theoretical and practical aspects of the debate model, focusing
              on potential weaknesses as the model scales up, and we propose
              future human and computer experiments to test these properties.",
  month    =  may,
  year     =  2018,
  eprint   = "1805.00899"
}

@ARTICLE{Hewitt2020-vu,
  title    = "Learning to learn generative programs with Memoised {Wake-Sleep}",
  author   = "Hewitt, Luke B and Le, Tuan Anh and Tenenbaum, Joshua B",
  abstract = "We study a class of neuro-symbolic generative models in which
              neural networks are used both for inference and as priors over
              symbolic, data-generating programs. As generative models, these
              programs capture compositional structures in a naturally
              explainable form. To tackle the challenge of performing program
              induction as an 'inner-loop' to learning, we propose the Memoised
              Wake-Sleep (MWS) algorithm, which extends Wake Sleep by
              explicitly storing and reusing the best programs discovered by
              the inference network throughout training. We use MWS to learn
              accurate, explainable models in three challenging domains:
              stroke-based character modelling, cellular automata, and few-shot
              learning in a novel dataset of real-world string concepts.",
  month    =  jul,
  year     =  2020,
  eprint   = "2007.03132"
}

@ARTICLE{Rashkin2021-fn,
  title    = "Measuring Attribution in Natural Language Generation Models",
  author   = "Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and
              Collins, Michael and Das, Dipanjan and Petrov, Slav and Tomar,
              Gaurav Singh and Turc, Iulia and Reitter, David",
  abstract = "With recent improvements in natural language generation (NLG)
              models for various applications, it has become imperative to have
              the means to identify and evaluate whether NLG output is only
              sharing verifiable information about the external world. In this
              work, we present a new evaluation framework entitled Attributable
              to Identified Sources (AIS) for assessing the output of natural
              language generation models, when such output pertains to the
              external world. We first define AIS and introduce a two-stage
              annotation pipeline for allowing annotators to appropriately
              evaluate model output according to AIS guidelines. We empirically
              validate this approach on three generation datasets (two in the
              conversational QA domain and one in summarization) via human
              evaluation studies that suggest that AIS could serve as a common
              framework for measuring whether model-generated statements are
              supported by underlying sources. We release guidelines for the
              human evaluation studies.",
  month    =  dec,
  year     =  2021,
  eprint   = "2112.12870"
}

@ARTICLE{Varshney2021-fz,
  title    = "{Interviewer-Candidate} Role Play: Towards Developing
              {Real-World} {NLP} Systems",
  author   = "Varshney, Neeraj and Mishra, Swaroop and Baral, Chitta",
  abstract = "Standard NLP tasks do not incorporate several common real-world
              scenarios such as seeking clarifications about the question,
              taking advantage of clues, abstaining in order to avoid incorrect
              answers, etc. This difference in task formulation hinders the
              adoption of NLP systems in real-world settings. In this work, we
              take a step towards bridging this gap and present a multi-stage
              task that simulates a typical human-human questioner-responder
              interaction such as an interview. Specifically, the system is
              provided with question simplifications, knowledge statements,
              examples, etc. at various stages to improve its prediction when
              it is not sufficiently confident. We instantiate the proposed
              task in Natural Language Inference setting where a system is
              evaluated on both in-domain and out-of-domain (OOD) inputs. We
              conduct comprehensive experiments and find that the multi-stage
              formulation of our task leads to OOD generalization performance
              improvement up to 2.29\% in Stage 1, 1.91\% in Stage 2, 54.88\%
              in Stage 3, and 72.02\% in Stage 4 over the standard unguided
              prediction. However, our task leaves a significant challenge for
              NLP researchers to further improve OOD performance at each stage.",
  month    =  jul,
  year     =  2021,
  eprint   = "2107.00315"
}

@ARTICLE{Metzler2021-eg,
  title    = "Rethinking Search: Making Experts out of Dilettantes",
  author   = "Metzler, Donald and Tay, Yi and Bahri, Dara and Najork,
              Marc-Alexander",
  abstract = "This paper examines how ideas from classical information
              retrieval and large pre-trained language models can be
              synthesized and evolved into systems that truly deliver on the
              promise of expert advice. When experiencing an information need,
              users want to engage with an expert, but often turn to an
              information retrieval system, such as a search engine, instead.
              Classical information retrieval systems do not answer information
              needs directly, but instead provide references to (hopefully
              authoritative) answers. Successful question answering systems
              offer a limited corpus created on-demand by human experts, which
              is neither timely nor scalable. Large pre-trained language
              models, by contrast, are capable of directly generating prose
              that may be responsive to an information need, but at present
              they are dilettantes rather than experts -- they do not have a
              true understanding of the world, they are prone to hallucinating,
              and crucially they are incapable of justifying their utterances
              by referring to supporting documents in the corpus they were
              trained over. This paper examines how ideas from classical
              information retrieval and large pre-trained language models can
              be synthesized and evolved into systems that truly deliver on the
              promise of expert advice.",
  journal  = "undefined",
  year     =  2021,
  language = "en"
}

@ARTICLE{Wang2021-dc,
  title    = "Exploring Generalization Ability of Pretrained Language Models on
              Arithmetic and Logical Reasoning",
  author   = "Wang, Cunxiang and Zheng, Boyuan and Niu, Yuchen and Zhang, Yue",
  abstract = "To quantitatively and intuitively explore the generalization
              ability of pre-trained language models (PLMs), we have designed
              several tasks of arithmetic and logical reasoning. We both
              analyse how well PLMs generalize when the test data is in the
              same distribution as the train data and when it is different, for
              the latter analysis, we have also designed a cross-distribution
              test set other than the in-distribution test set. We conduct
              experiments on one of the most advanced and publicly released
              generative PLM - BART. Our research finds that the PLMs can
              easily generalize when the distribution is the same, however, it
              is still difficult for them to generalize out of the
              distribution.",
  month    =  aug,
  year     =  2021,
  eprint   = "2108.06743"
}

@ARTICLE{Uria2016-di,
  title    = "Neural Autoregressive Distribution Estimation",
  author   = "Uria, Benigno and C{\^o}t{\'e}, Marc-Alexandre and Gregor, Karol
              and Murray, Iain and Larochelle, Hugo",
  abstract = "We present Neural Autoregressive Distribution Estimation (NADE)
              models, which are neural network architectures applied to the
              problem of unsupervised distribution and density estimation. They
              leverage the probability product rule and a weight sharing scheme
              inspired from restricted Boltzmann machines, to yield an
              estimator that is both tractable and has good generalization
              performance. We discuss how they achieve competitive performance
              in modeling both binary and real-valued observations. We also
              present how deep NADE models can be trained to be agnostic to the
              ordering of input dimensions used by the autoregressive product
              rule decomposition. Finally, we also show how to exploit the
              topological structure of pixels in images using a deep
              convolutional architecture for NADE.",
  month    =  may,
  year     =  2016,
  eprint   = "1605.02226"
}

@ARTICLE{Christiano2018-uf,
  title    = "Supervising strong learners by amplifying weak experts",
  author   = "Christiano, Paul and Shlegeris, Buck and Amodei, Dario",
  abstract = "Many real world learning tasks involve complex or hard-to-specify
              objectives, and using an easier-to-specify proxy can lead to poor
              performance or misaligned behavior. One solution is to have
              humans provide a training signal by demonstrating or judging
              performance, but this approach fails if the task is too
              complicated for a human to directly evaluate. We propose Iterated
              Amplification, an alternative training strategy which
              progressively builds up a training signal for difficult problems
              by combining solutions to easier subproblems. Iterated
              Amplification is closely related to Expert Iteration (Anthony et
              al., 2017; Silver et al., 2017), except that it uses no external
              reward function. We present results in algorithmic environments,
              showing that Iterated Amplification can efficiently learn complex
              behaviors.",
  month    =  oct,
  year     =  2018,
  eprint   = "1810.08575"
}

@ARTICLE{Huang2021-bt,
  title    = "Recall and Learn: A Memory-augmented Solver for Math Word
              Problems",
  author   = "Huang, Shifeng and Wang, Jiawei and Xu, Jiao and Cao, Da and
              Yang, Ming",
  abstract = "In this article, we tackle the math word problem, namely,
              automatically answering a mathematical problem according to its
              textual description. Although recent methods have demonstrated
              their promising results, most of these methods are based on
              template-based generation scheme which results in limited
              generalization capability. To this end, we propose a novel
              human-like analogical learning method in a recall and learn
              manner. Our proposed framework is composed of modules of memory,
              representation, analogy, and reasoning, which are designed to
              make a new exercise by referring to the exercises learned in the
              past. Specifically, given a math word problem, the model first
              retrieves similar questions by a memory module and then encodes
              the unsolved problem and each retrieved question using a
              representation module. Moreover, to solve the problem in a way of
              analogy, an analogy module and a reasoning module with a copy
              mechanism are proposed to model the interrelationship between the
              problem and each retrieved question. Extensive experiments on two
              well-known datasets show the superiority of our proposed
              algorithm as compared to other state-of-the-art competitors from
              both overall performance comparison and micro-scope studies.",
  month    =  sep,
  year     =  2021,
  eprint   = "2109.13112"
}

@ARTICLE{Zhang2021-fr,
  title    = "A Critical Review of Inductive Logic Programming Techniques for
              Explainable {AI}",
  author   = "Zhang, Zheng and Yilmaz, L and Liu, Bo",
  abstract = "A critical review of the recent advances in ILP is summarized and
              potential avenues of further ILP-motivated research toward
              developing self-explanatory artificial intelligence systems are
              highlighted. Despite recent advances in modern machine learning
              algorithms, the opaqueness of their underlying mechanisms
              continues to be an obstacle in adoption. To instill confidence
              and trust in artificial intelligence systems, Explainable
              Artificial Intelligence has emerged as a response to improving
              modern machine learning algorithms' explainability. Inductive
              Logic Programming (ILP), a subfield of symbolic artificial
              intelligence, plays a promising role in generating interpretable
              explanations because of its intuitive logic-driven framework. ILP
              effectively leverages abductive reasoning to generate explainable
              first-order clausal theories from examples and background
              knowledge. However, several challenges in developing methods
              inspired by ILP need to be addressed for their successful
              application in practice. For example, existing ILP systems often
              have a vast solution space, and the induced solutions are very
              sensitive to noises and disturbances. This survey paper
              summarizes the recent advances in ILP and a discussion of
              statistical relational learning and neural-symbolic algorithms,
              which offer synergistic views to ILP. Following a critical review
              of the recent advances, we delineate observed challenges and
              highlight potential avenues of further ILP-motivated research
              toward developing self-explanatory artificial intelligence
              systems. keywords: differentiable inductive logic programming,
              neuro-symbolic AI, inductive logic programming, metainterpretive
              learning, machine learning, probabilistic inductive logic
              programming, statistical relational learning, XAI.",
  journal  = "ArXiv",
  year     =  2021,
  language = "en"
}

@ARTICLE{Liu2021-qe,
  title    = "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting
              Methods in Natural Language Processing",
  author   = "Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao
              and Hayashi, Hiroaki and Neubig, Graham",
  abstract = "This paper surveys and organizes research works in a new paradigm
              in natural language processing, which we dub ``prompt-based
              learning''. Unlike traditional supervised learning, which trains
              a model to take in an input x and predict an output y as P(y|x),
              prompt-based learning is based on language models that model the
              probability of text directly. To use these models to perform
              prediction tasks, the original input x is modified using a
              template into a textual string prompt x' that has some unfilled
              slots, and then the language model is used to probabilistically
              fill the unfilled information to obtain a final string x, from
              which the final output y can be derived. This framework is
              powerful and attractive for a number of reasons: it allows the
              language model to be pre-trained on massive amounts of raw text,
              and by defining a new prompting function the model is able to
              perform few-shot or even zero-shot learning, adapting to new
              scenarios with few or no labeled data. In this paper we introduce
              the basics of this promising paradigm, describe a unified set of
              mathematical notations that can cover a wide variety of existing
              work, and organize existing work along several dimensions,
              e.g.the choice of pre-trained models, prompts, and tuning
              strategies. To make the field more accessible to interested
              beginners, we not only make a systematic review of existing works
              and a highly structured typology of prompt-based concepts, but
              also release other resources, e.g., a website
              http://pretrain.nlpedia.ai/ including constantly-updated survey,
              and paperlist.",
  month    =  jul,
  year     =  2021,
  eprint   = "2107.13586"
}

@ARTICLE{Wu2021-kb,
  title    = "{AI} Chains: Transparent and Controllable {Human-AI} Interaction
              by Chaining Large Language Model Prompts",
  author   = "Wu, Tongshuang and Terry, Michael and Cai, Carrie J",
  abstract = "Although large language models (LLMs) have demonstrated
              impressive potential on simple tasks, their breadth of scope,
              lack of transparency, and insufficient controllability can make
              them less effective when assisting humans on more complex tasks.
              In response, we introduce the concept of Chaining LLM steps
              together, where the output of one step becomes the input for the
              next, thus aggregating the gains per step. We first define a set
              of LLM primitive operations useful for Chain construction, then
              present an interactive system where users can modify these
              Chains, along with their intermediate results, in a modular way.
              In a 20-person user study, we found that Chaining not only
              improved the quality of task outcomes, but also significantly
              enhanced system transparency, controllability, and sense of
              collaboration. Additionally, we saw that users developed new ways
              of interacting with LLMs through Chains: they leveraged sub-tasks
              to calibrate model expectations, compared and contrasted
              alternative strategies by observing parallel downstream effects,
              and debugged unexpected model outputs by ``unit-testing''
              sub-components of a Chain. In two case studies, we further
              explore how LLM Chains may be used in future applications.",
  month    =  oct,
  year     =  2021,
  eprint   = "2110.01691"
}

@ARTICLE{Van_de_Meent2015-jx,
  title         = "{Black-Box} Policy Search with Probabilistic Programs",
  author        = "van de Meent, Jan-Willem and Paige, Brooks and Tolpin, David
                   and Wood, Frank",
  abstract      = "In this work, we explore how probabilistic programs can be
                   used to represent policies in sequential decision problems.
                   In this formulation, a probabilistic program is a black-box
                   stochastic simulator for both the problem domain and the
                   agent. We relate classic policy gradient techniques to
                   recently introduced black-box variational methods which
                   generalize to probabilistic program inference. We present
                   case studies in the Canadian traveler problem, Rock Sample,
                   and a benchmark for optimal diagnosis inspired by Guess Who.
                   Each study illustrates how programs can efficiently
                   represent policies using moderate numbers of parameters.",
  month         =  jul,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1507.04635"
}

@ARTICLE{Austin2021-iu,
  title         = "Program Synthesis with Large Language Models",
  author        = "Austin, Jacob and Odena, Augustus and Nye, Maxwell and
                   Bosma, Maarten and Michalewski, Henryk and Dohan, David and
                   Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc
                   and Sutton, Charles",
  abstract      = "This paper explores the limits of the current generation of
                   large language models for program synthesis in general
                   purpose programming languages. We evaluate a collection of
                   such models (with between 244M and 137B parameters) on two
                   new benchmarks, MBPP and MathQA-Python, in both the few-shot
                   and fine-tuning regimes. Our benchmarks are designed to
                   measure the ability of these models to synthesize short
                   Python programs from natural language descriptions. The
                   Mostly Basic Programming Problems (MBPP) dataset contains
                   974 programming tasks, designed to be solvable by
                   entry-level programmers. The MathQA-Python dataset, a Python
                   version of the MathQA benchmark, contains 23914 problems
                   that evaluate the ability of the models to synthesize code
                   from more complex text. On both datasets, we find that
                   synthesis performance scales log-linearly with model size.
                   Our largest models, even without finetuning on a code
                   dataset, can synthesize solutions to 59.6 percent of the
                   problems from MBPP using few-shot learning with a
                   well-designed prompt. Fine-tuning on a held-out portion of
                   the dataset improves performance by about 10 percentage
                   points across most model sizes. On the MathQA-Python
                   dataset, the largest fine-tuned model achieves 83.8 percent
                   accuracy. Going further, we study the model's ability to
                   engage in dialog about code, incorporating human feedback to
                   improve its solutions. We find that natural language
                   feedback from a human halves the error rate compared to the
                   model's initial prediction. Additionally, we conduct an
                   error analysis to shed light on where these models fall
                   short and what types of programs are most difficult to
                   generate. Finally, we explore the semantic grounding of
                   these models by fine-tuning them to predict the results of
                   program execution. We find that even our best models are
                   generally unable to predict the output of a program given a
                   specific input.",
  month         =  aug,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL",
  eprint        = "2108.07732"
}

@INPROCEEDINGS{Swanson2021-ut,
  title      = "Story centaur: Large language model few shot learning as a
                creative writing tool",
  booktitle  = "Proceedings of the 16th Conference of the European Chapter of
                the Association for Computational Linguistics: System
                Demonstrations",
  author     = "Swanson, Ben and Mathewson, Kory and Pietrzak, Ben and Chen,
                Sherol and Dinalescu, Monica",
  abstract   = "Story Centaur is a user interface for prototyping few shot
                models and a set of recombinable web components that deploy
                them to expose creative writers to few shot learning with a
                simple but powerful interface that lets them compose their own
                co-creation tools that further their own unique artistic
                directions. Few shot learning with large language models has
                the potential to give individuals without formal machine
                learning training the access to a wide range of text to text
                models. We consider how this applies to creative writers and
                present Story Centaur, a user interface for prototyping few
                shot models and a set of recombinable web components that
                deploy them. Story Centaur's goal is to expose creative writers
                to few shot learning with a simple but powerful interface that
                lets them compose their own co-creation tools that further
                their own unique artistic directions. We build out several
                examples of such tools, and in the process probe the boundaries
                and issues surrounding generation with large language models.",
  publisher  = "Association for Computational Linguistics",
  year       =  2021,
  address    = "Stroudsburg, PA, USA",
  language   = "en",
  conference = "Proceedings of the 16th Conference of the European Chapter of
                the Association for Computational Linguistics: System
                Demonstrations",
  location   = "Online"
}

@PHDTHESIS{Kirk2019-tq,
  title    = "Learning Hierarchical Compositional Task Definitions through
              Online Situated Interactive Language Instruction",
  author   = "Kirk, James",
  abstract = "Artificial agents, from robots to personal assistants, have
              become competent workers in many settings and embodiments, but
              for the most part, they are limited to performing the
              capabilities and tasks with which they were initially programmed.
              Learning in these settings has predominately focused on learning
              to improve the agent's performance on a task, and not on learning
              the actual definition of a task. The primary method for imbuing
              an agent with the task definition has been through programming by
              humans, who have detailed knowledge of the task, domain, and
              agent architecture. In contrast, humans quickly learn new tasks
              from scratch, often from instruction by another human. If we
              desire AI agents to be flexible and dynamically extendable, they
              will need to emulate these learning capabilities, and not be
              stuck with the limitation that task definitions must be acquired
              through programming. This dissertation explores the problem of
              how an Interactive Task Learning agent can learn the complete
              definition or formulation of novel tasks rapidly through online
              natural language instruction from a human instructor. Recent
              advances in natural language processing, memory systems, computer
              vision, spatial reasoning, robotics, and cognitive architectures
              make the time ripe to study how knowledge can be automatically
              acquired, represented, transferred, and operationalized. We
              present a learning approach embodied in an ITL agent that
              interactively learns the meaning of task concepts, the goals,
              actions, failure conditions, and task-specific terms, for 60
              games and puzzles. In our approach, the agent learns hierarchical
              symbolic representations of task knowledge that enable it to
              transfer and compose knowledge, analyze and debug multiple
              interpretations, and communicate with the teacher to resolve
              ambiguity. Our results show that the agent can correctly
              generalize, disambiguate, and transfer concepts across variations
              of language descriptions and world representations, even with
              distractors present.",
  year     =  2019,
  keywords = "Interactive Task Learning; Thesis",
  language = "en"
}

@ARTICLE{Nye2021-cx,
  title    = "Show Your Work: Scratchpads for Intermediate Computation with
              Language Models",
  author   = "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and
              Michalewski, Henryk and Austin, Jacob and Bieber, David and
              Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan,
              David and Sutton, Charles and Odena, Augustus",
  abstract = "Large pre-trained language models perform remarkably well on
              tasks that can be done ``in one pass'', such as generating
              realistic text or synthesizing computer programs. However, they
              struggle with tasks that require unbounded multi-step
              computation, such as adding integers or executing programs.
              Surprisingly, we find that these same models are able to perform
              complex multi-step computations -- even in the few-shot regime --
              when asked to perform the operation ``step by step'', showing the
              results of intermediate computations. In particular, we train
              transformers to perform multi-step computations by asking them to
              emit intermediate computation steps into a ``scratchpad''. On a
              series of increasingly complex tasks ranging from long addition
              to the execution of arbitrary programs, we show that scratchpads
              dramatically improve the ability of language models to perform
              multi-step computations.",
  month    =  nov,
  year     =  2021,
  eprint   = "2112.00114"
}

@ARTICLE{Richardson2021-jl,
  title    = "Pushing the Limits of Rule Reasoning in Transformers through
              Natural Language Satisfiability",
  author   = "Richardson, Kyle and Sabharwal, Ashish",
  abstract = "Investigating the reasoning abilities of transformer models, and
              discovering new challenging tasks for them, has been a topic of
              much interest. Recent studies have found these models to be
              surprisingly strong at performing deductive reasoning over formal
              logical theories expressed in natural language. A shortcoming of
              these studies, however, is that they do not take into account
              that logical theories, when sampled uniformly at random, do not
              necessarily lead to hard instances. We propose a new methodology
              for creating challenging algorithmic reasoning datasets that
              focus on natural language satisfiability (NLSat) problems. The
              key idea is to draw insights from empirical sampling of hard
              propositional SAT problems and from complexity-theoretic studies
              of language. This methodology allows us to distinguish easy from
              hard instances, and to systematically increase the complexity of
              existing reasoning benchmarks such as RuleTaker. We find that
              current transformers, given sufficient training data, are
              surprisingly robust at solving the resulting NLSat problems of
              substantially increased difficulty. They also exhibit some degree
              of scale-invariance - the ability to generalize to problems of
              larger size and scope. Our results, however, reveal important
              limitations too: a careful sampling of training data is crucial
              for building models that generalize to larger problems, and
              transformer models' limited scale-invariance suggests they are
              far from learning robust deductive reasoning algorithms.",
  month    =  dec,
  year     =  2021,
  eprint   = "2112.09054"
}

@ARTICLE{Bostrom2021-fa,
  title    = "Flexible Generation of Natural Language Deductions",
  author   = "Bostrom, Kaj and Zhao, Xinyu and Chaudhuri, Swarat and Durrett,
              Greg",
  abstract = "An interpretable system for open-domain reasoning needs to
              express its reasoning process in a transparent form. Natural
              language is an attractive representation for this purpose -- it
              is both highly expressive and easy for humans to understand.
              However, manipulating natural language statements in logically
              consistent ways is hard: models must cope with variation in how
              meaning is expressed while remaining precise. In this paper, we
              describe ParaPattern, a method for building models to generate
              deductive inferences from diverse natural language inputs without
              direct human supervision. We train BART-based models (Lewis et
              al., 2020) to generate the result of applying a particular
              logical operation to one or more premise statements. Crucially,
              we develop a largely automated pipeline for constructing suitable
              training examples from Wikipedia. We evaluate our models using
              out-of-domain sentence compositions from the QASC (Khot et al.,
              2020) and EntailmentBank (Dalvi et al., 2021) datasets as well as
              targeted perturbation sets. Our results show that our models are
              substantially more accurate and flexible than baseline systems.
              ParaPattern achieves 85\% validity on examples of the
              'substitution' operation from EntailmentBank without the use of
              any in-domain training data, matching the performance of a model
              fine-tuned for EntailmentBank. The full source code for our
              method is publicly available.",
  month    =  apr,
  year     =  2021,
  eprint   = "2104.08825"
}

@ARTICLE{Zhao2020-ps,
  title    = "{Ape210K}: A {Large-Scale} and {Template-Rich} Dataset of Math
              Word Problems",
  author   = "Zhao, Wei and Shang, Mingyue and Liu, Yang and Wang, Liang and
              Liu, Jingming",
  abstract = "Automatic math word problem solving has attracted growing
              attention in recent years. The evaluation datasets used by
              previous works have serious limitations in terms of scale and
              diversity. In this paper, we release a new large-scale and
              template-rich math word problem dataset named Ape210K. It
              consists of 210K Chinese elementary school-level math problems,
              which is 9 times the size of the largest public dataset Math23K.
              Each problem contains both the gold answer and the equations
              needed to derive the answer. Ape210K is also of greater diversity
              with 56K templates, which is 25 times more than Math23K. Our
              analysis shows that solving Ape210K requires not only natural
              language understanding but also commonsense knowledge. We expect
              Ape210K to be a benchmark for math word problem solving systems.
              Experiments indicate that state-of-the-art models on the Math23K
              dataset perform poorly on Ape210K. We propose a copy-augmented
              and feature-enriched sequence to sequence (seq2seq) model, which
              outperforms existing models by 3.2\% on the Math23K dataset and
              serves as a strong baseline of the Ape210K dataset. The gap is
              still significant between human and our baseline model, calling
              for further research efforts. We make Ape210K dataset publicly
              available at https://github.com/yuantiku/ape210k",
  month    =  sep,
  year     =  2020,
  eprint   = "2009.11506"
}

@ARTICLE{Stuhlmuller2015-ni,
  title    = "Modeling Cognition with Probabilistic Programs: Representations
              and Algorithms",
  author   = "Stuhlm{\"u}ller, Andreas",
  abstract = "This thesis develops probabilistic programming as a productive
              metaphor for understanding cognition, both with respect to mental
              representations and the manipulation of such representations. In
              the first half of the thesis, I demonstrate the representational
              power of probabilistic programs in the domains of concept
              learning and social reasoning. I provide examples of richly
              structured concepts, defined in terms of systems of relations,
              subparts, and recursive embeddings, that are naturally expressed
              as programs and show initial experimental evidence that they
              match human generalization patterns. I then proceed to models of
              reasoning about reasoning, a domain where the expressive power of
              probabilistic programs is necessary to formalize our intuitive
              domain understanding due to the fact that, unlike previous
              formalisms, probabilistic programs allow conditioning to be
              represented in a model, not just applied to a model. I illustrate
              this insight with programs that model nested reasoning in game
              theory, artificial intelligence, and linguistics. In the second
              half, I develop three inference algorithms with the dual intent
              of showing how to efficiently compute the marginal distributions
              defined by probabilistic programs, and providing building blocks
              for process-level accounts of human cognition. First, I describe
              a Dynamic Programming algorithm for computing the marginal
              distribution of discrete probabilistic programs by compiling to
              systems of equations and show that it can make inference in
              models of ``reasoning about reasoning'' tractable by merging and
              reusing subcomputations. Second, I introduce the setting of
              amortized inference and show how learning inverse models lets us
              leverage samples generated by other inference algorithms to
              compile probabilistic models into fast recognition functions.
              Third, I develop a generic approach to coarse-to-fine inference
              in probabilistic programs and provide evidence that it can speed
              up inference in models with large state spaces that have
              appropriate hierarchical structure. Finally, I substantiate the
              claim that probabilistic programming is a productive metaphor by
              outlining new research questions that have been opened up by this
              line of investigation.",
  journal  = "undefined",
  year     =  2015
}

@ARTICLE{simulation_inference,
  title    = "The frontier of simulation-based inference",
  author   = "Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles",
  abstract = "Many domains of science have developed complex simulations to
              describe phenomena of interest. While these simulations provide
              high-fidelity models, they are poorly suited for inference and
              lead to challenging inverse problems. We review the rapidly
              developing field of simulation-based inference and identify the
              forces giving additional momentum to the field. Finally, we
              describe how the frontier is expanding so that a broad audience
              can appreciate the profound influence these developments may have
              on science.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  117,
  number   =  48,
  pages    = "30055--30062",
  month    =  dec,
  year     =  2020,
  keywords = "approximate Bayesian computation; implicit models;
              likelihood-free inference; neural density estimation; statistical
              inference",
  language = "en"
}

@ARTICLE{Liu2021-vv,
  title    = "What Makes Good {In-Context} Examples for {GPT-$3$}?",
  author   = "Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill
              and Carin, Lawrence and Chen, Weizhu",
  abstract = "GPT-$3$ has attracted lots of attention due to its superior
              performance across a wide range of NLP tasks, especially with its
              powerful and versatile in-context few-shot learning ability.
              Despite its success, we found that the empirical results of
              GPT-$3$ depend heavily on the choice of in-context examples. In
              this work, we investigate whether there are more effective
              strategies for judiciously selecting in-context examples
              (relative to random sampling) that better leverage GPT-$3$'s
              few-shot capabilities. Inspired by the recent success of
              leveraging a retrieval module to augment large-scale neural
              network models, we propose to retrieve examples that are
              semantically-similar to a test sample to formulate its
              corresponding prompt. Intuitively, the in-context examples
              selected with such a strategy may serve as more informative
              inputs to unleash GPT-$3$'s extensive knowledge. We evaluate the
              proposed approach on several natural language understanding and
              generation benchmarks, where the retrieval-based prompt selection
              approach consistently outperforms the random baseline. Moreover,
              it is observed that the sentence encoders fine-tuned on
              task-related datasets yield even more helpful retrieval results.
              Notably, significant gains are observed on tasks such as
              table-to-text generation (41.9\% on the ToTTo dataset) and
              open-domain question answering (45.5\% on the NQ dataset). We
              hope our investigation could help understand the behaviors of
              GPT-$3$ and large-scale pre-trained LMs in general and enhance
              their few-shot capabilities.",
  month    =  jan,
  year     =  2021,
  eprint   = "2101.06804"
}

@ARTICLE{Jones2021-vg,
  title    = "Scaling Scaling Laws with Board Games",
  author   = "Jones, Andy L",
  abstract = "The largest experiments in machine learning now require resources
              far beyond the budget of all but a few institutions. Fortunately,
              it has recently been shown that the results of these huge
              experiments can often be extrapolated from the results of a
              sequence of far smaller, cheaper experiments. In this work, we
              show that not only can the extrapolation be done based on the
              size of the model, but on the size of the problem as well. By
              conducting a sequence of experiments using AlphaZero and Hex, we
              show that the performance achievable with a fixed amount of
              compute degrades predictably as the game gets larger and harder.
              Along with our main result, we further show that the test-time
              and train-time compute available to an agent can be traded off
              while maintaining performance.",
  month    =  apr,
  year     =  2021,
  eprint   = "2104.03113"
}

@ARTICLE{Van_de_Meent2018-ea,
  title         = "An Introduction to Probabilistic Programming",
  author        = "van de Meent, Jan-Willem and Paige, Brooks and Yang,
                   Hongseok and Wood, Frank",
  abstract      = "This book is a graduate-level introduction to probabilistic
                   programming. It not only provides a thorough background for
                   anyone wishing to use a probabilistic programming system,
                   but also introduces the techniques needed to design and
                   build these systems. It is aimed at people who have an
                   undergraduate-level understanding of either or, ideally,
                   both probabilistic machine learning and programming
                   languages. We start with a discussion of model-based
                   reasoning and explain why conditioning is a foundational
                   computation central to the fields of probabilistic machine
                   learning and artificial intelligence. We then introduce a
                   first-order probabilistic programming language (PPL) whose
                   programs correspond to graphical models with a known,
                   finite, set of random variables. In the context of this PPL
                   we introduce fundamental inference algorithms and describe
                   how they can be implemented. We then turn to higher-order
                   probabilistic programming languages. Programs in such
                   languages can define models with dynamic computation graphs,
                   which may not instantiate the same set of random variables
                   in each execution. Inference requires methods that generate
                   samples by repeatedly evaluating the program. Foundational
                   algorithms for this kind of language are discussed in the
                   context of an interface between program executions and an
                   inference controller. Finally we consider the intersection
                   of probabilistic and differentiable programming. We begin
                   with a discussion of automatic differentiation, and how it
                   can be used to implement efficient inference methods based
                   on Hamiltonian Monte Carlo. We then discuss gradient-based
                   maximum likelihood estimation in programs that are
                   parameterized using neural networks, how to amortize
                   inference using by learning neural approximations to the
                   program posterior, and how language features impact the
                   design of deep probabilistic programming systems.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1809.10756"
}

@ARTICLE{Wray2021-lr,
  title    = "Language Models as a Knowledge Source for Cognitive Agents",
  author   = "Wray, Robert E and {III} and Kirk, James R and Laird, John E",
  abstract = "Language models (LMs) are sentence-completion engines trained on
              massive corpora. LMs have emerged as a significant breakthrough
              in natural-language processing, providing capabilities that go
              far beyond sentence completion including question answering,
              summarization, and natural-language inference. While many of
              these capabilities have potential application to cognitive
              systems, exploiting language models as a source of task
              knowledge, especially for task learning, offers significant,
              near-term benefits. We introduce language models and the various
              tasks to which they have been applied and then review methods of
              knowledge extraction from language models. The resulting analysis
              outlines both the challenges and opportunities for using language
              models as a new knowledge source for cognitive systems. It also
              identifies possible ways to improve knowledge extraction from
              language models using the capabilities provided by cognitive
              systems. Central to success will be the ability of a cognitive
              agent to itself learn an abstract model of the knowledge implicit
              in the LM as well as methods to extract high-quality knowledge
              effectively and efficiently. To illustrate, we introduce a
              hypothetical robot agent and describe how language models could
              extend its task knowledge and improve its performance and the
              kinds of knowledge and methods the agent can use to exploit the
              knowledge within a language model.",
  month    =  sep,
  year     =  2021,
  eprint   = "2109.08270"
}

@ARTICLE{Betz2021-mm,
  title    = "Thinking Aloud: Dynamic Context Generation Improves {Zero-Shot}
              Reasoning Performance of {GPT-2}",
  author   = "Betz, Gregor and Richardson, Kyle and Voigt, Christian",
  abstract = "Thinking aloud is an effective meta-cognitive strategy human
              reasoners apply to solve difficult problems. We suggest to
              improve the reasoning ability of pre-trained neural language
              models in a similar way, namely by expanding a task's context
              with problem elaborations that are dynamically generated by the
              language model itself. Our main result is that dynamic problem
              elaboration significantly improves the zero-shot performance of
              GPT-2 in a deductive reasoning and natural language inference
              task: While the model uses a syntactic heuristic for predicting
              an answer, it is capable (to some degree) of generating reasoned
              additional context which facilitates the successful application
              of its heuristic. We explore different ways of generating
              elaborations, including fewshot learning, and find that their
              relative performance varies with the specific problem
              characteristics (such as problem difficulty). Moreover, the
              effectiveness of an elaboration can be explained in terms of the
              degree to which the elaboration semantically coheres with the
              corresponding problem. In particular, elaborations that are most
              faithful to the original problem description may boost accuracy
              by up to 24\%.",
  month    =  mar,
  year     =  2021,
  eprint   = "2103.13033"
}

@ARTICLE{Brookes2019-eq,
  title         = "A view of Estimation of Distribution Algorithms through the
                   lens of {Expectation-Maximization}",
  author        = "Brookes, David H and Busia, Akosua and Fannjiang, Clara and
                   Murphy, Kevin and Listgarten, Jennifer",
  abstract      = "Estimation of Distribution Algorithms (EDAs) are class of
                   stochastic optimization algorithms that are widely used in
                   practice across many engineering disciplines. However, the
                   development of EDAs has remained largely a heuristic
                   endeavour, making them difficult to coherently reason about,
                   extend or manipulate. Our main contribution is to show that
                   a large class of EDAs can be written as a Monte Carlo
                   Expectation-Maximization (EM) algorithm, and as exact EM in
                   the infinite sample limit. Because EM sits on a rigorous
                   statistical foundation and has been thoroughly analyzed,
                   this connection provides a coherent framework with which to
                   reason about EDAs. To illustrate the potential benefits of
                   such a connection, we use it to show that (i) EDAs can be
                   seen as approximating natural gradient descent, and (ii) one
                   can directly take an adaptive EM algorithm built on rigorous
                   analysis, and apply it to EDAs. We expect others will build
                   on top of this framework.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1905.10474"
}

@PHDTHESIS{Paige2016-zc,
  title    = "Automatic inference for higher-order probabilistic programs",
  author   = "Paige, Timothy Brooks",
  abstract = "Probabilistic models used in quantitative sciences have
              historically co-evolved with methods for performing inference:
              specific modeling assumptions are made not because they are
              appropriate to the application domain, but because they are
              required to leverage existing software packages or inference
              methods. The intertwined nature of modeling and computational
              concerns leaves much of the promise of probabilistic modeling out
              of reach for data scientists, forcing practitioners to turn to
              off-the-shelf solutions. The emerging field of probabilistic
              programming aims to reduce the technical and cognitive overhead
              for writing and designing novel probabilistic models, by
              introducing a specialized programming language as an abstraction
              barrier between modeling and inference. The aim of this thesis is
              to develop inference algorithms that scale well and are
              applicable to broad model families. We focus particularly on
              methods that can be applied to models written in general-purpose
              higher-order probabilistic programming languages, where programs
              may make use of recursion, arbitrary deterministic simulation,
              and higher-order functions to create more accurate models of an
              application domain. In a probabilistic programming system,
              probabilistic models are defined using a modeling language; a
              backend implements generic inference methods applicable to any
              model written in this language. Probabilistic programs --- models
              --- can be written without concern for how inference will later
              be performed. We begin by considering several existing
              probabilistic programming languages, their design choices, and
              tradeoffs. We then demonstrate how programs written in
              higher-order languages can be used to define coherent probability
              models, describing possible approaches to inference, and
              providing explicit algorithms for efficient implementations of
              both classic and novel inference methods based on and extending
              sequential Monte Carlo. This is followed by an investigation into
              the use of variational inference methods within higher-order
              probabilistic programming languages, with application to policy
              learning, adaptive importance sampling, and amortization of
              inference.",
  year     =  2016,
  school   = "University of Oxford"
}

@MISC{Evans2017-yq,
  title        = "{Modeling Agents with Probabilistic Programs}",
  author       = "Evans, Owain and Stuhlm{\"u}ller, Andreas and Salvatier, John
                  and Filan, Daniel",
  year         =  2017,
  howpublished = "\url{http://agentmodels.org}",
  note         = "Accessed: 2022-1-7"
}

@ARTICLE{Yang2019-hj,
  title    = "{XLNet}: Generalized Autoregressive Pretraining for Language
              Understanding",
  author   = "Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell,
              Jaime and Salakhutdinov, Ruslan and Le, Quoc V",
  abstract = "With the capability of modeling bidirectional contexts, denoising
              autoencoding based pretraining like BERT achieves better
              performance than pretraining approaches based on autoregressive
              language modeling. However, relying on corrupting the input with
              masks, BERT neglects dependency between the masked positions and
              suffers from a pretrain-finetune discrepancy. In light of these
              pros and cons, we propose XLNet, a generalized autoregressive
              pretraining method that (1) enables learning bidirectional
              contexts by maximizing the expected likelihood over all
              permutations of the factorization order and (2) overcomes the
              limitations of BERT thanks to its autoregressive formulation.
              Furthermore, XLNet integrates ideas from Transformer-XL, the
              state-of-the-art autoregressive model, into pretraining.
              Empirically, under comparable experiment settings, XLNet
              outperforms BERT on 20 tasks, often by a large margin, including
              question answering, natural language inference, sentiment
              analysis, and document ranking.",
  month    =  jun,
  year     =  2019,
  eprint   = "1906.08237"
}

@ARTICLE{Cobbe2021-jt,
  title    = "Training Verifiers to Solve Math Word Problems",
  author   = "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen,
              Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias
              and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and
              Hesse, Christopher and Schulman, John",
  abstract = "State-of-the-art language models can match human performance on
              many tasks, but they still struggle to robustly perform
              multi-step mathematical reasoning. To diagnose the failures of
              current models and support research, we introduce GSM8K, a
              dataset of 8.5K high quality linguistically diverse grade school
              math word problems. We find that even the largest transformer
              models fail to achieve high test performance, despite the
              conceptual simplicity of this problem distribution. To increase
              performance, we propose training verifiers to judge the
              correctness of model completions. At test time, we generate many
              candidate solutions and select the one ranked highest by the
              verifier. We demonstrate that verification significantly improves
              performance on GSM8K, and we provide strong empirical evidence
              that verification scales more effectively with increased data
              than a finetuning baseline.",
  month    =  oct,
  year     =  2021,
  eprint   = "2110.14168"
}

@ARTICLE{Naesseth2019-ey,
  title         = "Elements of Sequential Monte Carlo",
  author        = "Naesseth, Christian A and Lindsten, Fredrik and Sch{\"o}n,
                   Thomas B",
  abstract      = "A core problem in statistics and probabilistic machine
                   learning is to compute probability distributions and
                   expectations. This is the fundamental problem of Bayesian
                   statistics and machine learning, which frames all inference
                   as expectations with respect to the posterior distribution.
                   The key challenge is to approximate these intractable
                   expectations. In this tutorial, we review sequential Monte
                   Carlo (SMC), a random-sampling-based class of methods for
                   approximate inference. First, we explain the basics of SMC,
                   discuss practical issues, and review theoretical results. We
                   then examine two of the main user design choices: the
                   proposal distributions and the so called intermediate target
                   distributions. We review recent results on how variational
                   inference and amortization can be used to learn efficient
                   proposals and target distributions. Next, we discuss the SMC
                   estimate of the normalizing constant, how this can be used
                   for pseudo-marginal inference and inference evaluation.
                   Throughout the tutorial we illustrate the use of SMC on
                   various models commonly used in machine learning, such as
                   stochastic recurrent neural networks, probabilistic
                   graphical models, and probabilistic programs.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1903.04797"
}

@ARTICLE{Shin2020-gh,
  title    = "{AutoPrompt}: Eliciting Knowledge from Language Models with
              Automatically Generated Prompts",
  author   = "Shin, Taylor and Razeghi, Yasaman and Logan, IV, Robert L and
              Wallace, Eric and Singh, Sameer",
  abstract = "The remarkable success of pretrained language models has
              motivated the study of what kinds of knowledge these models learn
              during pretraining. Reformulating tasks as fill-in-the-blanks
              problems (e.g., cloze tests) is a natural approach for gauging
              such knowledge, however, its usage is limited by the manual
              effort and guesswork required to write suitable prompts. To
              address this, we develop AutoPrompt, an automated method to
              create prompts for a diverse set of tasks, based on a
              gradient-guided search. Using AutoPrompt, we show that masked
              language models (MLMs) have an inherent capability to perform
              sentiment analysis and natural language inference without
              additional parameters or finetuning, sometimes achieving
              performance on par with recent state-of-the-art supervised
              models. We also show that our prompts elicit more accurate
              factual knowledge from MLMs than the manually created prompts on
              the LAMA benchmark, and that MLMs can be used as relation
              extractors more effectively than supervised relation extraction
              models. These results demonstrate that automatically generated
              prompts are a viable parameter-free alternative to existing
              probing methods, and as pretrained LMs become more sophisticated
              and capable, potentially a replacement for finetuning.",
  month    =  oct,
  year     =  2020,
  eprint   = "2010.15980"
}

@ARTICLE{Zhao2021-yy,
  title    = "Calibrate Before Use: Improving {Few-Shot} Performance of
              Language Models",
  author   = "Zhao, Tony Z and Wallace, Eric and Feng, Shi and Klein, Dan and
              Singh, Sameer",
  abstract = "GPT-3 can perform numerous tasks when provided a natural language
              prompt that contains a few training examples. We show that this
              type of few-shot learning can be unstable: the choice of prompt
              format, training examples, and even the order of the training
              examples can cause accuracy to vary from near chance to near
              state-of-the-art. We demonstrate that this instability arises
              from the bias of language models towards predicting certain
              answers, e.g., those that are placed near the end of the prompt
              or are common in the pre-training data. To mitigate this, we
              first estimate the model's bias towards each answer by asking for
              its prediction when given the training prompt and a content-free
              test input such as ``N/A''. We then fit calibration parameters
              that cause the prediction for this input to be uniform across
              answers. On a diverse set of tasks, this contextual calibration
              procedure substantially improves GPT-3 and GPT-2's average
              accuracy (up to 30.0\% absolute) and reduces variance across
              different choices of the prompt.",
  month    =  feb,
  year     =  2021,
  eprint   = "2102.09690"
}

@ARTICLE{Pascual2021-iy,
  title    = "A {Plug-and-Play} Method for Controlled Text Generation",
  author   = "Pascual, Damian and Egressy, Beni and Meister, Clara and
              Cotterell, Ryan and Wattenhofer, Roger",
  abstract = "Large pre-trained language models have repeatedly shown their
              ability to produce fluent text. Yet even when starting from a
              prompt, generation can continue in many plausible directions.
              Current decoding methods with the goal of controlling generation,
              e.g., to ensure specific words are included, either require
              additional models or fine-tuning, or work poorly when the task at
              hand is semantically unconstrained, e.g., story generation. In
              this work, we present a plug-and-play decoding method for
              controlled language generation that is so simple and intuitive,
              it can be described in a single sentence: given a topic or
              keyword, we add a shift to the probability distribution over our
              vocabulary towards semantically similar words. We show how
              annealing this distribution can be used to impose hard
              constraints on language generation, something no other
              plug-and-play method is currently able to do with SOTA language
              generators. Despite the simplicity of this approach, we see it
              works incredibly well in practice: decoding from GPT-2 leads to
              diverse and fluent sentences while guaranteeing the appearance of
              given guide words. We perform two user studies, revealing that
              (1) our method outperforms competing methods in human
              evaluations; and (2) forcing the guide words to appear in the
              generated text has no impact on the fluency of the generated
              text.",
  month    =  sep,
  year     =  2021,
  eprint   = "2109.09707"
}

@ARTICLE{Zheng2021-bh,
  title         = "Exploring Prompt-based Few-shot Learning for Grounded Dialog
                   Generation",
  author        = "Zheng, Chujie and Huang, Minlie",
  abstract      = "Dialog grounding enables conversational models to make full
                   use of external information to establish multiple desired
                   qualities, such as knowledgeable, engaging and empathetic.
                   However, naturally grounded dialog corpora are usually not
                   directly available, which puts forward requirements for the
                   few-shot learning ability of conversational models.
                   Motivated by recent advances in pre-trained language models
                   and prompt-based learning, in this paper we explore
                   prompt-based few-shot learning for grounded dialog
                   generation (GDG). We first formulate the prompt construction
                   for GDG tasks, based on which we then conduct comprehensive
                   empirical analysis on two common types of prompting methods:
                   template-based prompting and soft-prompting. We demonstrate
                   the potential of prompt-based methods in few-shot learning
                   for GDG and provide directions of improvement for future
                   work.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.06513"
}

@MISC{Goodman2016-yn,
  title        = "{Probabilistic Models of Cognition}",
  author       = "Goodman, Noah D and Tenenbaum, Joshua B and Contributors, The
                  Probmods",
  year         =  2016,
  howpublished = "\url{http://probmods.org/v2}",
  note         = "Accessed: 2022-1-9"
}

@ARTICLE{Askell2021-od,
  title    = "A General Language Assistant as a Laboratory for Alignment",
  author   = "Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and
              Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph,
              Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and
              Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and
              Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown,
              Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and
              Kaplan, Jared",
  abstract = "Given the broad capabilities of large language models, it should
              be possible to work towards a general-purpose, text-based
              assistant that is aligned with human values, meaning that it is
              helpful, honest, and harmless. As an initial foray in this
              direction we study simple baseline techniques and evaluations,
              such as prompting. We find that the benefits from modest
              interventions increase with model size, generalize to a variety
              of alignment evaluations, and do not compromise the performance
              of large models. Next we investigate scaling trends for several
              training objectives relevant to alignment, comparing imitation
              learning, binary discrimination, and ranked preference modeling.
              We find that ranked preference modeling performs much better than
              imitation learning, and often scales more favorably with model
              size. In contrast, binary discrimination typically performs and
              scales very similarly to imitation learning. Finally we study a
              `preference model pre-training' stage of training, with the goal
              of improving sample efficiency when finetuning on human
              preferences.",
  month    =  dec,
  year     =  2021,
  eprint   = "2112.00861"
}

@ARTICLE{Potapov2016-vc,
  title         = "A Step from Probabilistic Programming to Cognitive
                   Architectures",
  author        = "Potapov, Alexey",
  abstract      = "Probabilistic programming is considered as a framework, in
                   which basic components of cognitive architectures can be
                   represented in unified and elegant fashion. At the same
                   time, necessity of adopting some component of cognitive
                   architectures for extending capabilities of probabilistic
                   programming languages is pointed out. In particular,
                   implicit specification of generative models via declaration
                   of concepts and links between them is proposed, and
                   usefulness of declarative knowledge for achieving efficient
                   inference is briefly discussed.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1605.01180"
}

@ARTICLE{Cheng2015-ph,
  title    = "Break It Down: A Comparison of Macro- and Microtasks",
  author   = "Cheng, Justin and Teevan, J and Iqbal, Shamsi T and Bernstein,
              Michael S",
  abstract = "It is found that breaking these tasks into microtasks results in
              longer overall task completion times, but higher quality outcomes
              and a better experience that may be more resilient to
              interruptions, suggesting that microt tasks can help people
              complete high quality work in interruption-driven environments. A
              large, seemingly overwhelming task can sometimes be transformed
              into a set of smaller, more manageable microtasks that can each
              be accomplished independently. For example, it may be hard to
              subjectively rank a large set of photographs, but easy to sort
              them in spare moments by making many pairwise comparisons. In
              crowdsourcing systems, microtasking enables unskilled workers
              with limited commitment to work together to complete tasks they
              would not be able to do individually. We explore the costs and
              benefits of decomposing macrotasks into microtasks for three task
              categories: arithmetic, sorting, and transcription. We find that
              breaking these tasks into microtasks results in longer overall
              task completion times, but higher quality outcomes and a better
              experience that may be more resilient to interruptions. These
              results suggest that microtasks can help people complete high
              quality work in interruption-driven environments.",
  journal  = "undefined",
  year     =  2015,
  language = "en"
}

@ARTICLE{Betz2021-vj,
  title    = "{Natural-Language} {Multi-Agent} Simulations of Argumentative
              Opinion Dynamics",
  author   = "Betz, Gregor",
  abstract = "This paper develops a natural-language agent-based model of
              argumentation (ABMA). Its artificial deliberative agents (ADAs)
              are constructed with the help of so-called neural language models
              recently developed in AI and computational linguistics. ADAs are
              equipped with a minimalist belief system and may generate and
              submit novel contributions to a conversation. The
              natural-language ABMA allows us to simulate collective
              deliberation in English, i.e. with arguments, reasons, and claims
              themselves -- rather than with their mathematical representations
              (as in formal models). This paper uses the natural-language ABMA
              to test the robustness of formal reason-balancing models of
              argumentation [Maes \& Flache 2013, Singer et al. 2019]: First of
              all, as long as ADAs remain passive, confirmation bias and
              homophily updating trigger polarization, which is consistent with
              results from formal models. However, once ADAs start to actively
              generate new contributions, the evolution of a conservation is
              dominated by properties of the agents *as authors*. This suggests
              that the creation of new arguments, reasons, and claims
              critically affects a conversation and is of pivotal importance
              for understanding the dynamics of collective deliberation. The
              paper closes by pointing out further fruitful applications of the
              model and challenges for future research.",
  month    =  apr,
  year     =  2021,
  eprint   = "2104.06737"
}

@ARTICLE{Reif2021-ao,
  title    = "A Recipe For Arbitrary Text Style Transfer with Large Language
              Models",
  author   = "Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy
              and Callison-Burch, Chris and Wei, Jason",
  abstract = "In this paper, we leverage large language models (LMs) to perform
              zero-shot text style transfer. We present a prompting method that
              we call augmented zero-shot learning, which frames style transfer
              as a sentence rewriting task and requires only a natural language
              instruction, without model fine-tuning or exemplars in the target
              style. Augmented zero-shot learning is simple and demonstrates
              promising results not just on standard style transfer tasks such
              as sentiment, but also on arbitrary transformations such as
              ``make this melodramatic'' or ``insert a metaphor.''",
  month    =  sep,
  year     =  2021,
  eprint   = "2109.03910"
}

@ARTICLE{Larma2021-cw,
  title    = "Improving exploration in policy gradient search: Application to
              symbolic optimization",
  author   = "Larma, Mikel Landajuela and Petersen, Brenden K and Kim, Soo K
              and Santiago, Claudio P and Glatt, Ruben and Mundhenk, T Nathan
              and Pettit, Jacob F and Faissol, Daniel M",
  abstract = "Many machine learning strategies designed to automate
              mathematical tasks leverage neural networks to search large
              combinatorial spaces of mathematical symbols. In contrast to
              traditional evolutionary approaches, using a neural network at
              the core of the search allows learning higher-level symbolic
              patterns, providing an informed direction to guide the search.
              When no labeled data is available, such networks can still be
              trained using reinforcement learning. However, we demonstrate
              that this approach can suffer from an early commitment phenomenon
              and from initialization bias, both of which limit exploration. We
              present two exploration methods to tackle these issues, building
              upon ideas of entropy regularization and distribution
              initialization. We show that these techniques can improve the
              performance, increase sample efficiency, and lower the complexity
              of solutions for the task of symbolic regression.",
  month    =  jul,
  year     =  2021,
  eprint   = "2107.09158"
}

@ARTICLE{Chen2021-tz,
  title    = "Evaluating Large Language Models Trained on Code",
  author   = "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and
              Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards,
              Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and
              Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov,
              Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela
              and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov,
              Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian,
              Mohammad and Winter, Clemens and Tillet, Philippe and Such,
              Felipe Petroski and Cummings, Dave and Plappert, Matthias and
              Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel
              and Guss, William Hebgen and Nichol, Alex and Paino, Alex and
              Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji,
              Suchir and Jain, Shantanu and Saunders, William and Hesse,
              Christopher and Carr, Andrew N and Leike, Jan and Achiam, Josh
              and Misra, Vedant and Morikawa, Evan and Radford, Alec and
              Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer,
              Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and
              McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech",
  abstract = "We introduce Codex, a GPT language model fine-tuned on publicly
              available code from GitHub, and study its Python code-writing
              capabilities. A distinct production version of Codex powers
              GitHub Copilot. On HumanEval, a new evaluation set we release to
              measure functional correctness for synthesizing programs from
              docstrings, our model solves 28.8\% of the problems, while GPT-3
              solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that
              repeated sampling from the model is a surprisingly effective
              strategy for producing working solutions to difficult prompts.
              Using this method, we solve 70.2\% of our problems with 100
              samples per problem. Careful investigation of our model reveals
              its limitations, including difficulty with docstrings describing
              long chains of operations and with binding operations to
              variables. Finally, we discuss the potential broader impacts of
              deploying powerful code generation technologies, covering safety,
              security, and economics.",
  month    =  jul,
  year     =  2021,
  eprint   = "2107.03374"
}

@ARTICLE{Evans2019-vb,
  title    = "Machine Learning Projects for Iterated Distillation and
              Amplification",
  author   = "Evans, Owain and Saunders, W and Stuhlm{\"u}ller, Andreas",
  abstract = "Iterated Distillation and Amplification (IDA) is a framework for
              training ML models. IDA is related to existing frameworks like
              imitation learning and reinforcement learning, but it aims to
              solve tasks for which humans cannot construct a suitable reward
              function or solve directly. This document reviews IDA and
              proposes three projects that explore aspects of IDA. Project 1
              applies IDA to problems in highschool mathematics and
              investigates whether learning to decompose problems can improve
              performance over supervised learning. Project 2 applies IDA to
              neural program interpretation, where neural nets are trained on
              the internal behavior (execution traces) of traditional computer
              programs. Project 3 investigates whether adaptive computation
              time (varying compute at inference time as a function of the
              input) can improve the robustness and efficiency of IDA. Our goal
              in outlining these projects is to generate discussion and
              encourage research on IDA. We are not (as of June 2019) working
              on these projects, but we are interested in collaboration.",
  year     =  2019,
  language = "en"
}

@ARTICLE{Newman2021-jg,
  title    = "{P-Adapters}: Robustly Extracting Factual Information from
              Language Models with Diverse Prompts",
  author   = "Newman, Benjamin and Choubey, Prafulla Kumar and Rajani, Nazneen",
  abstract = "Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the
              quality of the factual information extracted from Large Language
              Models (LLMs) depends on the prompts used to query them. This
              inconsistency is problematic because different users will query
              LLMs for the same information using different wording, but should
              receive the same, accurate responses regardless. In this work we
              aim to address this shortcoming by introducing P-Adapters:
              lightweight models that sit between the embedding layer and first
              attention layer of LLMs. They take LLM embeddings as input and
              output continuous prompts that are used to query the LLM.
              Additionally, we investigate Mixture of Experts (MoE) models that
              learn a set of continuous prompts (``experts'') and select one to
              query the LLM. They require a separate classifier trained on
              human-annotated data to map natural language prompts to the
              continuous ones. P-Adapters perform comparably to the more
              complex MoE models in extracting factual information from BERT
              and RoBERTa while eliminating the need for additional
              annotations. P-Adapters show between 12-26\% absolute improvement
              in precision and 36-50\% absolute improvement in consistency over
              a baseline of only using natural language queries. Finally, we
              investigate what makes a P-adapter successful and conclude that
              access to the LLM's embeddings of the original natural language
              prompt, particularly the subject of the entity pair being asked
              about, is a significant factor.",
  month    =  oct,
  year     =  2021,
  eprint   = "2110.07280"
}

@INPROCEEDINGS{Tan2021-hk,
  title      = "Progressive generation of long text with pretrained language
                models",
  booktitle  = "Proceedings of the 2021 Conference of the North American
                Chapter of the Association for Computational Linguistics: Human
                Language Technologies",
  author     = "Tan, Bowen and Yang, Zichao and Al-Shedivat, Maruan and Xing,
                Eric and Hu, Zhiting",
  abstract   = "This work proposes a simple but effective method of generating
                text in a progressive manner, inspired by generating images
                from low to high resolution, and shows that it significantly
                improves upon the fine-tuned large LMs and various
                planning-then-generation methods in terms of quality and sample
                efficiency. Large-scale language models (LMs) pretrained on
                massive corpora of text, such as GPT-2, are powerful
                open-domain text generators. However, as our systematic
                examination reveals, it is still challenging for such models to
                generate coherent long passages of text (e.g., 1000 tokens),
                especially when the models are fine-tuned to the target domain
                on a small corpus. Previous planning-then-generation methods
                also fall short of producing such long text in various domains.
                To overcome the limitations, we propose a simple but effective
                method of generating text in a progressive manner, inspired by
                generating images from low to high resolution. Our method first
                produces domain-specific content keywords and then
                progressively refines them into complete passages in multiple
                stages. The simple design allows our approach to take advantage
                of pretrained LMs at each stage and effectively adapt to any
                target domain given only a small set of examples. We conduct a
                comprehensive empirical study with a broad set of evaluation
                metrics, and show that our approach significantly improves upon
                the fine-tuned large LMs and various planning-then-generation
                methods in terms of quality and sample efficiency. Human
                evaluation also validates that our model generations are more
                coherent.",
  publisher  = "Association for Computational Linguistics",
  year       =  2021,
  address    = "Stroudsburg, PA, USA",
  language   = "en",
  conference = "Proceedings of the 2021 Conference of the North American
                Chapter of the Association for Computational Linguistics: Human
                Language Technologies",
  location   = "Online"
}

@ARTICLE{Gotmare2021-gt,
  title    = "Cascaded Fast and Slow Models for Efficient Semantic Code Search",
  author   = "Gotmare, Akhilesh Deepak and Li, Junnan and Joty, Shafiq and Hoi,
              Steven C H",
  abstract = "The goal of natural language semantic code search is to retrieve
              a semantically relevant code snippet from a fixed set of
              candidates using a natural language query. Existing approaches
              are neither effective nor efficient enough towards a practical
              semantic code search system. In this paper, we propose an
              efficient and accurate semantic code search framework with
              cascaded fast and slow models, in which a fast transformer
              encoder model is learned to optimize a scalable index for fast
              retrieval followed by learning a slow classification-based
              re-ranking model to improve the performance of the top K results
              from the fast retrieval. To further reduce the high memory cost
              of deploying two separate models in practice, we propose to
              jointly train the fast and slow model based on a single
              transformer encoder with shared parameters. The proposed cascaded
              approach is not only efficient and scalable, but also achieves
              state-of-the-art results with an average mean reciprocal ranking
              (MRR) score of 0.7795 (across 6 programming languages) as opposed
              to the previous state-of-the-art result of 0.713 MRR on the
              CodeSearchNet benchmark.",
  month    =  oct,
  year     =  2021,
  eprint   = "2110.07811"
}

@ARTICLE{Le2021-jn,
  title    = "Hybrid Memoised {Wake-Sleep}: Approximate Inference at the
              {Discrete-Continuous} Interface",
  author   = "Le, Tuan Anh and Collins, Katherine M and Hewitt, Luke and Ellis,
              Kevin and {N, Siddharth} and Gershman, Samuel J and Tenenbaum,
              Joshua B",
  abstract = "Modeling complex phenomena typically involves the use of both
              discrete and continuous variables. Such a setting applies across
              a wide range of problems, from identifying trends in time-series
              data to performing effective compositional scene understanding in
              images. Here, we propose Hybrid Memoised Wake-Sleep (HMWS), an
              algorithm for effective inference in such hybrid
              discrete-continuous models. Prior approaches to learning suffer
              as they need to perform repeated expensive inner-loop discrete
              inference. We build on a recent approach, Memoised Wake-Sleep
              (MWS), which alleviates part of the problem by memoising discrete
              variables, and extend it to allow for a principled and effective
              way to handle continuous variables by learning a separate
              recognition model used for importance-sampling based approximate
              inference and marginalization. We evaluate HMWS in the GP-kernel
              learning and 3D scene understanding domains, and show that it
              outperforms current state-of-the-art inference methods.",
  month    =  jul,
  year     =  2021,
  eprint   = "2107.06393"
}

@ARTICLE{Paige2016-lg,
  title         = "Inference Networks for Sequential Monte Carlo in Graphical
                   Models",
  author        = "Paige, Brooks and Wood, Frank",
  abstract      = "We introduce a new approach for amortizing inference in
                   directed graphical models by learning heuristic
                   approximations to stochastic inverses, designed specifically
                   for use as proposal distributions in sequential Monte Carlo
                   methods. We describe a procedure for constructing and
                   learning a structured neural network which represents an
                   inverse factorization of the graphical model, resulting in a
                   conditional density estimator that takes as input particular
                   values of the observed random variables, and returns an
                   approximation to the distribution of the latent variables.
                   This recognition model can be learned offline, independent
                   from any particular dataset, prior to performing inference.
                   The output of these networks can be used as
                   automatically-learned high-quality proposal distributions to
                   accelerate sequential Monte Carlo across a diverse range of
                   problem settings.",
  month         =  feb,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1602.06701"
}

@ARTICLE{Lavin2021-dw,
  title         = "Simulation Intelligence: Towards a New Generation of
                   Scientific Methods",
  author        = "Lavin, Alexander and Zenil, Hector and Paige, Brooks and
                   Krakauer, David and Gottschlich, Justin and Mattson, Tim and
                   Anandkumar, Anima and Choudry, Sanjay and Rocki, Kamil and
                   Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Prunkl, Carina and
                   Paige, Brooks and Isayev, Olexandr and Peterson, Erik and
                   McMahon, Peter L and Macke, Jakob and Cranmer, Kyle and
                   Zhang, Jiaxin and Wainwright, Haruko and Hanuka, Adi and
                   Veloso, Manuela and Assefa, Samuel and Zheng, Stephan and
                   Pfeffer, Avi",
  abstract      = "The original ``Seven Motifs'' set forth a roadmap of
                   essential methods for the field of scientific computing,
                   where a motif is an algorithmic method that captures a
                   pattern of computation and data movement. We present the
                   ``Nine Motifs of Simulation Intelligence'', a roadmap for
                   the development and integration of the essential algorithms
                   necessary for a merger of scientific computing, scientific
                   simulation, and artificial intelligence. We call this merger
                   simulation intelligence (SI), for short. We argue the motifs
                   of simulation intelligence are interconnected and
                   interdependent, much like the components within the layers
                   of an operating system. Using this metaphor, we explore the
                   nature of each layer of the simulation intelligence
                   operating system stack (SI-stack) and the motifs therein:
                   (1) Multi-physics and multi-scale modeling; (2) Surrogate
                   modeling and emulation; (3) Simulation-based inference; (4)
                   Causal modeling and inference; (5) Agent-based modeling; (6)
                   Probabilistic programming; (7) Differentiable programming;
                   (8) Open-ended optimization; (9) Machine programming. We
                   believe coordinated efforts between motifs offers immense
                   opportunity to accelerate scientific discovery, from solving
                   inverse problems in synthetic biology and climate science,
                   to directing nuclear energy experiments and predicting
                   emergent behavior in socioeconomic settings. We elaborate on
                   each layer of the SI-stack, detailing the state-of-art
                   methods, presenting examples to highlight challenges and
                   opportunities, and advocating for specific ways to advance
                   the motifs and the synergies from their combinations.
                   Advancing and integrating these technologies can enable a
                   robust and efficient hypothesis-simulation-analysis type of
                   scientific method, which we introduce with several use-cases
                   for human-machine teaming and automated science.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2112.03235"
}

@ARTICLE{Varshney2020-er,
  title    = "It's better to say ``I can't answer'' than answering incorrectly:
              Towards Safety critical {NLP} systems",
  author   = "Varshney, Neeraj and Mishra, Swaroop and Baral, Chitta",
  abstract = "In order to make AI systems more reliable and their adoption in
              safety critical applications possible, it is essential to impart
              the capability to abstain from answering when their prediction is
              likely to be incorrect and seek human intervention. Recently
              proposed ``selective answering'' techniques model calibration as
              a binary classification task. We argue that, not all incorrectly
              answered questions are incorrect to the same extent and the same
              is true for correctly answered questions. Hence, treating all
              correct predictions equally and all incorrect predictions equally
              constraints calibration. In this work, we propose a methodology
              that incorporates the degree of correctness, shifting away from
              classification labels as it directly tries to predict the
              probability of model's prediction being correct. We show the
              efficacy of the proposed method on existing Natural Language
              Inference (NLI) datasets by training on SNLI and evaluating on
              MNLI mismatched and matched datasets. Our approach improves area
              under the curve (AUC) of risk-coverage plot by 10.22\% and 8.06\%
              over maxProb with respect to the maximum possible improvement on
              MNLI mismatched and matched set respectively. In order to
              evaluate our method on Out of Distribution (OOD) datasets, we
              propose a novel setup involving questions with a variety of
              reasoning skills. Our setup includes a test set for each of the
              five reasoning skills: numerical, logical, qualitative, abductive
              and commonsense. We select confidence threshold for each of the
              approaches where the in-domain accuracy (SNLI) is 99\%. Our
              results show that, the proposed method outperforms existing
              approaches by abstaining on 2.6\% more OOD questions at
              respective confidence thresholds.",
  month    =  aug,
  year     =  2020,
  eprint   = "2008.09371"
}

@ARTICLE{Kim2021-hf,
  title    = "What Changes Can Large-scale Language Models Bring? Intensive
              Study on {HyperCLOVA}: Billions-scale Korean Generative
              Pretrained Transformers",
  author   = "Kim, Boseop and Kim, Hyoungseok and Lee, Sang-Woo and Lee,
              Gichang and Kwak, Donghyun and Jeon, Dong Hyeon and Park,
              Sunghyun and Kim, Sungju and Kim, Seonhoon and Seo, Dongpil and
              Lee, Heungsub and Jeong, Minyoung and Lee, Sungjae and Kim,
              Minsub and Ko, Suk Hyun and Kim, Seokhun and Park, Taeyong and
              Kim, Jinuk and Kang, Soyoung and Ryu, Na-Hyeon and Yoo, Kang Min
              and Chang, Minsuk and Suh, Soobin and In, Sookyo and Park,
              Jinseong and Kim, Kyungduk and Kim, Hiun and Jeong, Jisu and Yeo,
              Yong Goo and Ham, Donghoon and Park, Dongju and Lee, Min Young
              and Kang, Jaewook and Kang, Inho and Ha, Jung-Woo and Park,
              Woomyoung and Sung, Nako",
  abstract = "GPT-3 shows remarkable in-context learning ability of large-scale
              language models (LMs) trained on hundreds of billion scale data.
              Here we address some remaining issues less reported by the GPT-3
              paper, such as a non-English LM, the performances of different
              sized models, and the effect of recently introduced prompt
              optimization on in-context learning. To achieve this, we
              introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a
              Korean-centric corpus of 560B tokens. Enhanced by our
              Korean-specific tokenization, HyperCLOVA with our training
              configuration shows state-of-the-art in-context zero-shot and
              few-shot learning performances on various downstream tasks in
              Korean. Also, we show the performance benefits of prompt-based
              learning and demonstrate how it can be integrated into the prompt
              engineering pipeline. Then we discuss the possibility of
              materializing the No Code AI paradigm by providing AI prototyping
              capabilities to non-experts of ML by introducing HyperCLOVA
              studio, an interactive prompt engineering interface. Lastly, we
              demonstrate the potential of our methods with three successful
              in-house applications.",
  month    =  sep,
  year     =  2021,
  eprint   = "2109.04650"
}

@ARTICLE{Nakano2021-mf,
  title    = "{WebGPT}: Browser-assisted question-answering with human feedback",
  author   = "Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu,
              Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher
              and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and
              Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger,
              Gretchen and Button, Kevin and Knight, Matthew and Chess,
              Benjamin and Schulman, John",
  abstract = "We fine-tune GPT-3 to answer long-form questions using a
              text-based web-browsing environment, which allows the model to
              search and navigate the web. By setting up the task so that it
              can be performed by humans, we are able to train models on the
              task using imitation learning, and then optimize answer quality
              with human feedback. To make human evaluation of factual accuracy
              easier, models must collect references while browsing in support
              of their answers. We train and evaluate our models on ELI5, a
              dataset of questions asked by Reddit users. Our best model is
              obtained by fine-tuning GPT-3 using behavior cloning, and then
              performing rejection sampling against a reward model trained to
              predict human preferences. This model's answers are preferred by
              humans 56\% of the time to those of our human demonstrators, and
              69\% of the time to the highest-voted answer from Reddit.",
  month    =  dec,
  year     =  2021,
  eprint   = "2112.09332"
}

@ARTICLE{Malkin2021-xz,
  title    = "Boosting coherence of language models",
  author   = "Malkin, Nikolay and Wang, Zhen and Jojic, Nebojsa",
  abstract = "Naturality of long-term information structure -- coherence --
              remains a challenge in language generation. Large language models
              have insufficiently learned such structure, as their long-form
              generations differ from natural text in measures of coherence. To
              alleviate this divergence, we propose coherence boosting, an
              inference procedure that increases the effect of distant context
              on next-token prediction. We show the benefits of coherence
              boosting with pretrained models by distributional analyses of
              generated ordinary text and dialog responses. We also find that
              coherence boosting with state-of-the-art models for various
              zero-shot NLP tasks yields performance gains with no additional
              training.",
  month    =  oct,
  year     =  2021,
  eprint   = "2110.08294"
}

@ARTICLE{Ritchie2015-bm,
  title     = "Controlling procedural modeling programs with
               stochastically-ordered sequential Monte Carlo",
  author    = "Ritchie, Daniel and Mildenhall, Ben and Goodman, Noah D and
               Hanrahan, Pat",
  abstract  = "We present a method for controlling the output of procedural
               modeling programs using Sequential Monte Carlo (SMC). Previous
               probabilistic methods for controlling procedural models use
               Markov Chain Monte Carlo (MCMC), which receives control feedback
               only for completely-generated models. In contrast, SMC receives
               feedback incrementally on incomplete models, allowing it to
               reallocate computational resources and converge quickly. To
               handle the many possible sequentializations of a structured,
               recursive procedural modeling program, we develop and prove the
               correctness of a new SMC variant, Stochastically-Ordered
               Sequential Monte Carlo (SOSMC). We implement SOSMC for
               general-purpose programs using a new programming primitive: the
               stochastic future. Finally, we show that SOSMC reliably
               generates high-quality outputs for a variety of programs and
               control scoring functions. For small computational budgets,
               SOSMC's outputs often score nearly twice as high as those of
               MCMC or normal SMC.",
  journal   = "ACM Trans. Graph.",
  publisher = "Association for Computing Machinery",
  volume    =  34,
  number    =  4,
  pages     = "1--11",
  month     =  jul,
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "procedural modeling, sequential Monte Carlo, directable
               randomness, probabilistic programming"
}

@MISC{Goodman2014-kl,
  title        = "{The Design and Implementation of Probabilistic Programming
                  Languages}",
  author       = "Goodman, Noah D and Stuhlm{\"u}ller, Andreas",
  year         =  2014,
  howpublished = "\url{http://dippl.org}",
  note         = "Accessed: 2022-1-7"
}

@ARTICLE{Knoll2011-gp,
  title         = "A Machine Learning Perspective on Predictive Coding with
                   {PAQ}",
  author        = "Knoll, Byron and de Freitas, Nando",
  abstract      = "PAQ8 is an open source lossless data compression algorithm
                   that currently achieves the best compression rates on many
                   benchmarks. This report presents a detailed description of
                   PAQ8 from a statistical machine learning perspective. It
                   shows that it is possible to understand some of the modules
                   of PAQ8 and use this understanding to improve the method.
                   However, intuitive statistical explanations of the behavior
                   of other modules remain elusive. We hope the description in
                   this report will be a starting point for discussions that
                   will increase our understanding, lead to improvements to
                   PAQ8, and facilitate a transfer of knowledge from PAQ8 to
                   other machine learning methods, such a recurrent neural
                   networks and stochastic memoizers. Finally, the report
                   presents a broad range of new applications of PAQ to machine
                   learning tasks including language modeling and adaptive text
                   prediction, adaptive game playing, classification, and
                   compression using features from the field of deep learning.",
  month         =  aug,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1108.3298"
}

@ARTICLE{Holtzman2019-wl,
  title    = "The Curious Case of Neural Text Degeneration",
  author   = "Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and
              Choi, Yejin",
  abstract = "Despite considerable advancements with deep neural language
              models, the enigma of neural text degeneration persists when
              these models are tested as text generators. The counter-intuitive
              empirical observation is that even though the use of likelihood
              as training objective leads to high quality models for a broad
              range of language understanding tasks, using likelihood as a
              decoding objective leads to text that is bland and strangely
              repetitive. In this paper, we reveal surprising distributional
              differences between human text and machine text. In addition, we
              find that decoding strategies alone can dramatically effect the
              quality of machine text, even when generated from exactly the
              same neural language model. Our findings motivate Nucleus
              Sampling, a simple but effective method to draw the best out of
              neural generation. By sampling text from the dynamic nucleus of
              the probability distribution, which allows for diversity while
              effectively truncating the less reliable tail of the
              distribution, the resulting text better demonstrates the quality
              of human text, yielding enhanced diversity without sacrificing
              fluency and coherence.",
  month    =  apr,
  year     =  2019,
  eprint   = "1904.09751"
}

@ARTICLE{Anthony2019-ln,
  title    = "Policy Gradient Search: Online Planning and Expert Iteration
              without Search Trees",
  author   = "Anthony, Thomas and Nishihara, Robert and Moritz, Philipp and
              Salimans, Tim and Schulman, John",
  abstract = "Monte Carlo Tree Search (MCTS) algorithms perform
              simulation-based search to improve policies online. During
              search, the simulation policy is adapted to explore the most
              promising lines of play. MCTS has been used by state-of-the-art
              programs for many problems, however a disadvantage to MCTS is
              that it estimates the values of states with Monte Carlo averages,
              stored in a search tree; this does not scale to games with very
              high branching factors. We propose an alternative
              simulation-based search method, Policy Gradient Search (PGS),
              which adapts a neural network simulation policy online via policy
              gradient updates, avoiding the need for a search tree. In Hex,
              PGS achieves comparable performance to MCTS, and an agent trained
              using Expert Iteration with PGS was able defeat MoHex 2.0, the
              strongest open-source Hex agent, in 9x9 Hex.",
  month    =  apr,
  year     =  2019,
  eprint   = "1904.03646"
}

@ARTICLE{Ritchie2016-ea,
  title         = "Deep Amortized Inference for Probabilistic Programs",
  author        = "Ritchie, Daniel and Horsfall, Paul and Goodman, Noah D",
  abstract      = "Probabilistic programming languages (PPLs) are a powerful
                   modeling tool, able to represent any computable probability
                   distribution. Unfortunately, probabilistic program inference
                   is often intractable, and existing PPLs mostly rely on
                   expensive, approximate sampling-based methods. To alleviate
                   this problem, one could try to learn from past inferences,
                   so that future inferences run faster. This strategy is known
                   as amortized inference; it has recently been applied to
                   Bayesian networks and deep generative models. This paper
                   proposes a system for amortized inference in PPLs. In our
                   system, amortization comes in the form of a parameterized
                   guide program. Guide programs have similar structure to the
                   original program, but can have richer data flow, including
                   neural network components. These networks can be optimized
                   so that the guide approximately samples from the posterior
                   distribution defined by the original program. We present a
                   flexible interface for defining guide programs and a
                   stochastic gradient-based scheme for optimizing guide
                   parameters, as well as some preliminary results on
                   automatically deriving guide programs. We explore in detail
                   the common machine learning pattern in which a 'local' model
                   is specified by 'global' random values and used to generate
                   independent observed data points; this gives rise to
                   amortized local inference supporting global model learning.",
  month         =  oct,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1610.05735"
}

@ARTICLE{Ben-Assayag2021-ux,
  title    = "Train on Small, Play the Large: Scaling Up Board Games with
              {AlphaZero} and {GNN}",
  author   = "Ben-Assayag, Shai and El-Yaniv, Ran",
  abstract = "Playing board games is considered a major challenge for both
              humans and AI researchers. Because some complicated board games
              are quite hard to learn, humans usually begin with playing on
              smaller boards and incrementally advance to master larger board
              strategies. Most neural network frameworks that are currently
              tasked with playing board games neither perform such incremental
              learning nor possess capabilities to automatically scale up. In
              this work, we look at the board as a graph and combine a graph
              neural network architecture inside the AlphaZero framework, along
              with some other innovative improvements. Our ScalableAlphaZero is
              capable of learning to play incrementally on small boards, and
              advancing to play on large ones. Our model can be trained quickly
              to play different challenging board games on multiple board
              sizes, without using any domain knowledge. We demonstrate the
              effectiveness of ScalableAlphaZero and show, for example, that by
              training it for only three days on small Othello boards, it can
              defeat the AlphaZero model on a large board, which was trained to
              play the large board for $30$ days.",
  month    =  jul,
  year     =  2021,
  eprint   = "2107.08387"
}

@ARTICLE{Schulman2015-yk,
  title         = "Gradient Estimation Using Stochastic Computation Graphs",
  author        = "Schulman, John and Heess, Nicolas and Weber, Theophane and
                   Abbeel, Pieter",
  abstract      = "In a variety of problems originating in supervised,
                   unsupervised, and reinforcement learning, the loss function
                   is defined by an expectation over a collection of random
                   variables, which might be part of a probabilistic model or
                   the external world. Estimating the gradient of this loss
                   function, using samples, lies at the core of gradient-based
                   learning algorithms for these problems. We introduce the
                   formalism of stochastic computation graphs---directed
                   acyclic graphs that include both deterministic functions and
                   conditional probability distributions---and describe how to
                   easily and automatically derive an unbiased estimator of the
                   loss function's gradient. The resulting algorithm for
                   computing the gradient estimator is a simple modification of
                   the standard backpropagation algorithm. The generic scheme
                   we propose unifies estimators derived in variety of prior
                   work, along with variance-reduction techniques therein. It
                   could assist researchers in developing intricate models
                   involving a combination of stochastic and deterministic
                   operations, enabling, for example, attention, memory, and
                   control actions.",
  month         =  jun,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1506.05254"
}

@INPROCEEDINGS{Jiang2021-pm,
  title      = "{GenLine} and {GenForm}: Two tools for interacting with
                generative language models in a code editor",
  booktitle  = "The Adjunct Publication of the 34th Annual {ACM} Symposium on
                User Interface Software and Technology",
  author     = "Jiang, Ellen and Toh, Edwin and Molina, Alejandra and Donsbach,
                Aaron and Cai, Carrie J and Terry, Michael",
  abstract   = "A macro system with two tools that allow users to invoke
                language model prompts as macros in a code editor, and a
                form-like interface where the user provides input that is then
                transformed into multiple pieces of output at the same time. A
                large, generative language model's output can be influenced
                through well-designed prompts, or text-based inputs that
                establish textual patterns that the model replicates in its
                output [6]. These capabilities create new opportunities for
                novel interactions with large, generative language models. We
                present a macro system with two tools that allow users to
                invoke language model prompts as macros in a code editor.
                GenLine allows users to execute macros inline as they write
                code in the editor (e.g., ``Make an OK button'' produces the
                equivalent HTML). GenForm provides a form-like interface where
                the user provides input that is then transformed into multiple
                pieces of output at the same time (e.g., a description of web
                code is transformed into HTML, CSS, and JavaScript).",
  publisher  = "ACM",
  month      =  oct,
  year       =  2021,
  address    = "New York, NY, USA",
  language   = "en",
  conference = "UIST '21: The 34th Annual ACM Symposium on User Interface
                Software and Technology",
  location   = "Virtual Event USA"
}

@article{language_of_thought,
	title = {Concepts in a Probabilistic Language of Thought.},
	number = {010},
	year = {2014},
	month = {06/2014},
	abstract = {<p>Knowledge organizes our understanding of the world, determining what we expect given what we have already seen. Our predictive representations have two key properties: they are productive, and they are graded. Productive generalization is possible because our knowledge decomposes into concepts{\textemdash}elements of knowledge that are combined and recombined to describe particular situations. Gradedness is the observable effect of accounting for uncertainty{\textemdash}our knowledge encodes degrees of belief that lead to graded probabilistic predictions. To put this a different way, concepts form a combinatorial system that enables description of many different situations; each such situation specifies a distribution over what we expect to see in the world, given what we have seen. We may think of this system as a probabilistic language of thought (PLoT) in which representations are built from language-like composition of concepts and the content of those representations is a probability distribution on world states. The purpose of this chapter is to formalize these ideas in computational terms, to illustrate key properties of the PLoT approach with a concrete example, and to draw connections with other views of<br />
conceptual structure.</p>

<p><em>Note: The book chapter is reprinted courtesy of The MIT Press, from the forthcoming edited collection {\textquotedblleft}The Conceptual Mind: New Directions in the Study of Concepts{\textquotedblright} edited by Eric Margolis and Stephen Laurence, print date Spring 2015.</em></p>
},
	keywords = {Development of Intelligence},
	author = {Noah D. Goodman and Joshua B. Tenenbaum and Tobias Gerstenberg}
}


@ARTICLE{language_feedback,
  title         = "Training Language Models with Language Feedback",
  author        = "Scheurer, J{\'e}r{\'e}my and Campos, Jon Ander and Chan, Jun
                   Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan",
  abstract      = "Pretrained language models often do not perform tasks in
                   ways that are in line with our preferences, e.g., generating
                   offensive text or factually incorrect summaries. Recent work
                   approaches the above issue by learning from a simple form of
                   human evaluation: comparisons between pairs of
                   model-generated task outputs. Comparison feedback conveys
                   limited information about human preferences per human
                   evaluation. Here, we propose to learn from natural language
                   feedback, which conveys more information per human
                   evaluation. We learn from language feedback on model outputs
                   using a three-step learning algorithm. First, we condition
                   the language model on the initial output and feedback to
                   generate many refinements. Second, we choose the refinement
                   with the highest similarity to the feedback. Third, we
                   finetune a language model to maximize the likelihood of the
                   chosen refinement given the input. In synthetic experiments,
                   we first evaluate whether language models accurately
                   incorporate feedback to produce refinements, finding that
                   only large language models (175B parameters) do so. Using
                   only 100 samples of human-written feedback, our learning
                   algorithm finetunes a GPT-3 model to roughly human-level
                   summarization.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.14146"
}


@INPROCEEDINGS{pymc4,
  title     = "{PyMC4}: Exploiting Coroutines for Implementing a Probabilistic
               Programming Framework",
  booktitle = "{NeurIPS} 2019 Workshop Program Transformations",
  author    = "Kochurov, Max and Carroll, Colin and Wiecki, T and Lao, Junpeng",
  year      =  2019,
  language  = "en"
}


@ARTICLE{palm,
  title         = "{PaLM}: Scaling Language Modeling with Pathways",
  author        = "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob
                   and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and
                   Barham, Paul and Chung, Hyung Won and Sutton, Charles and
                   Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and
                   Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek
                   and Barnes, Parker and Tay, Yi and Shazeer, Noam and
                   Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and
                   Hutchinson, Ben and Pope, Reiner and Bradbury, James and
                   Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin,
                   Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat,
                   Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia,
                   Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam
                   and Zhou, Denny and Ippolito, Daphne and Luan, David and
                   Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander
                   and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and
                   Omernick, Mark and Dai, Andrew M and Pillai, Thanumalayan
                   Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and
                   Moreira, Erica and Child, Rewon and Polozov, Oleksandr and
                   Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta,
                   Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele
                   and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas
                   and Dean, Jeff and Petrov, Slav and Fiedel, Noah",
  abstract      = "Large language models have been shown to achieve remarkable
                   performance across a variety of natural language tasks using
                   few-shot learning, which drastically reduces the number of
                   task-specific training examples needed to adapt the model to
                   a particular application. To further our understanding of
                   the impact of scale on few-shot learning, we trained a
                   540-billion parameter, densely activated, Transformer
                   language model, which we call Pathways Language Model PaLM.
                   We trained PaLM on 6144 TPU v4 chips using Pathways, a new
                   ML system which enables highly efficient training across
                   multiple TPU Pods. We demonstrate continued benefits of
                   scaling by achieving state-of-the-art few-shot learning
                   results on hundreds of language understanding and generation
                   benchmarks. On a number of these tasks, PaLM 540B achieves
                   breakthrough performance, outperforming the finetuned
                   state-of-the-art on a suite of multi-step reasoning tasks,
                   and outperforming average human performance on the recently
                   released BIG-bench benchmark. A significant number of
                   BIG-bench tasks showed discontinuous improvements from model
                   scale, meaning that performance steeply increased as we
                   scaled to our largest model. PaLM also has strong
                   capabilities in multilingual tasks and source code
                   generation, which we demonstrate on a wide array of
                   benchmarks. We additionally provide a comprehensive analysis
                   on bias and toxicity, and study the extent of training data
                   memorization with respect to model scale. Finally, we
                   discuss the ethical considerations related to large language
                   models and discuss potential mitigation strategies.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.02311"
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Goodman, Noah D},
  journal={arXiv preprint arXiv:2203.14465},
  year={2022}
}


@ARTICLE{learning_to_summarize,
  title         = "Training Language Models with Language Feedback",
  author        = "Scheurer, J{\'e}r{\'e}my and Campos, Jon Ander and Chan, Jun
                   Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan",
  abstract      = "Pretrained language models often do not perform tasks in
                   ways that are in line with our preferences, e.g., generating
                   offensive text or factually incorrect summaries. Recent work
                   approaches the above issue by learning from a simple form of
                   human evaluation: comparisons between pairs of
                   model-generated task outputs. Comparison feedback conveys
                   limited information about human preferences per human
                   evaluation. Here, we propose to learn from natural language
                   feedback, which conveys more information per human
                   evaluation. We learn from language feedback on model outputs
                   using a three-step learning algorithm. First, we condition
                   the language model on the initial output and feedback to
                   generate many refinements. Second, we choose the refinement
                   with the highest similarity to the feedback. Third, we
                   finetune a language model to maximize the likelihood of the
                   chosen refinement given the input. In synthetic experiments,
                   we first evaluate whether language models accurately
                   incorporate feedback to produce refinements, finding that
                   only large language models (175B parameters) do so. Using
                   only 100 samples of human-written feedback, our learning
                   algorithm finetunes a GPT-3 model to roughly human-level
                   summarization.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.14146"
}



@ARTICLE{anthropic_human_feedback,
  title         = "Training a Helpful and Harmless Assistant with Reinforcement
                   Learning from Human Feedback",
  author        = "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell,
                   Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and
                   Fort, Stanislav and Ganguli, Deep and Henighan, Tom and
                   Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson
                   and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and
                   Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan
                   and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and
                   Nanda, Neel and Olsson, Catherine and Amodei, Dario and
                   Brown, Tom and Clark, Jack and McCandlish, Sam and Olah,
                   Chris and Mann, Ben and Kaplan, Jared",
  abstract      = "We apply preference modeling and reinforcement learning from
                   human feedback (RLHF) to finetune language models to act as
                   helpful and harmless assistants. We find this alignment
                   training improves performance on almost all NLP evaluations,
                   and is fully compatible with training for specialized skills
                   such as python coding and summarization. We explore an
                   iterated online mode of training, where preference models
                   and RL policies are updated on a weekly cadence with fresh
                   human feedback data, efficiently improving our datasets and
                   models. Finally, we investigate the robustness of RLHF
                   training, and identify a roughly linear relation between the
                   RL reward and the square root of the KL divergence between
                   the policy and its initialization. Alongside our main
                   results, we perform peripheral analyses on calibration,
                   competing objectives, and the use of OOD detection, compare
                   our models with human writers, and provide samples from our
                   models using prompts appearing in recent related work.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.05862"
}



@ARTICLE{ppo,
  title         = "Proximal Policy Optimization Algorithms",
  author        = "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and
                   Radford, Alec and Klimov, Oleg",
  abstract      = "We propose a new family of policy gradient methods for
                   reinforcement learning, which alternate between sampling
                   data through interaction with the environment, and
                   optimizing a ``surrogate'' objective function using
                   stochastic gradient ascent. Whereas standard policy gradient
                   methods perform one gradient update per data sample, we
                   propose a novel objective function that enables multiple
                   epochs of minibatch updates. The new methods, which we call
                   proximal policy optimization (PPO), have some of the
                   benefits of trust region policy optimization (TRPO), but
                   they are much simpler to implement, more general, and have
                   better sample complexity (empirically). Our experiments test
                   PPO on a collection of benchmark tasks, including simulated
                   robotic locomotion and Atari game playing, and we show that
                   PPO outperforms other online policy gradient methods, and
                   overall strikes a favorable balance between sample
                   complexity, simplicity, and wall-time.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1707.06347"
}


@ARTICLE{gptf,
  title         = "Generative Language Modeling for Automated Theorem Proving",
  author        = "Polu, Stanislas and Sutskever, Ilya",
  abstract      = "We explore the application of transformer-based language
                   models to automated theorem proving. This work is motivated
                   by the possibility that a major limitation of automated
                   theorem provers compared to humans -- the generation of
                   original mathematical terms -- might be addressable via
                   generation from language models. We present an automated
                   prover and proof assistant, GPT-f, for the Metamath
                   formalization language, and analyze its performance. GPT-f
                   found new short proofs that were accepted into the main
                   Metamath library, which is to our knowledge, the first time
                   a deep-learning based system has contributed proofs that
                   were adopted by a formal mathematics community.",
  month         =  sep,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2009.03393"
}


@ARTICLE{likelihood_free_blackbox,
  title         = "Unifying Likelihood-free Inference with Black-box
                   Optimization and Beyond",
  author        = "Zhang, Dinghuai and Fu, Jie and Bengio, Yoshua and
                   Courville, Aaron",
  abstract      = "Black-box optimization formulations for biological sequence
                   design have drawn recent attention due to their promising
                   potential impact on the pharmaceutical industry. In this
                   work, we propose to unify two seemingly distinct worlds:
                   likelihood-free inference and black-box optimization, under
                   one probabilistic framework. In tandem, we provide a recipe
                   for constructing various sequence design methods based on
                   this framework. We show how previous optimization approaches
                   can be ``reinvented'' in our framework, and further propose
                   new probabilistic black-box optimization algorithms.
                   Extensive experiments on sequence design application
                   illustrate the benefits of the proposed methodology.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2110.03372"
}


@ARTICLE{intro_ppl,
  title         = "An Introduction to Probabilistic Programming",
  author        = "van de Meent, Jan-Willem and Paige, Brooks and Yang,
                   Hongseok and Wood, Frank",
  abstract      = "This book is a graduate-level introduction to probabilistic
                   programming. It not only provides a thorough background for
                   anyone wishing to use a probabilistic programming system,
                   but also introduces the techniques needed to design and
                   build these systems. It is aimed at people who have an
                   undergraduate-level understanding of either or, ideally,
                   both probabilistic machine learning and programming
                   languages. We start with a discussion of model-based
                   reasoning and explain why conditioning is a foundational
                   computation central to the fields of probabilistic machine
                   learning and artificial intelligence. We then introduce a
                   first-order probabilistic programming language (PPL) whose
                   programs correspond to graphical models with a known,
                   finite, set of random variables. In the context of this PPL
                   we introduce fundamental inference algorithms and describe
                   how they can be implemented. We then turn to higher-order
                   probabilistic programming languages. Programs in such
                   languages can define models with dynamic computation graphs,
                   which may not instantiate the same set of random variables
                   in each execution. Inference requires methods that generate
                   samples by repeatedly evaluating the program. Foundational
                   algorithms for this kind of language are discussed in the
                   context of an interface between program executions and an
                   inference controller. Finally we consider the intersection
                   of probabilistic and differentiable programming. We begin
                   with a discussion of automatic differentiation, and how it
                   can be used to implement efficient inference methods based
                   on Hamiltonian Monte Carlo. We then discuss gradient-based
                   maximum likelihood estimation in programs that are
                   parameterized using neural networks, how to amortize
                   inference using by learning neural approximations to the
                   program posterior, and how language features impact the
                   design of deep probabilistic programming systems.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1809.10756"
}


@ARTICLE{numpyro,
  title         = "Composable Effects for Flexible and Accelerated
                   Probabilistic Programming in {NumPyro}",
  author        = "Phan, Du and Pradhan, Neeraj and Jankowiak, Martin",
  abstract      = "NumPyro is a lightweight library that provides an alternate
                   NumPy backend to the Pyro probabilistic programming language
                   with the same modeling interface, language primitives and
                   effect handling abstractions. Effect handlers allow Pyro's
                   modeling API to be extended to NumPyro despite its being
                   built atop a fundamentally different JAX-based functional
                   backend. In this work, we demonstrate the power of composing
                   Pyro's effect handlers with the program transformations that
                   enable hardware acceleration, automatic differentiation, and
                   vectorization in JAX. In particular, NumPyro provides an
                   iterative formulation of the No-U-Turn Sampler (NUTS) that
                   can be end-to-end JIT compiled, yielding an implementation
                   that is much faster than existing alternatives in both the
                   small and large dataset regimes.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1912.11554"
}


@ARTICLE{pyro,
  title         = "Pyro: Deep Universal Probabilistic Programming",
  author        = "Bingham, Eli and Chen, Jonathan P and Jankowiak, Martin and
                   Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos,
                   Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall,
                   Paul and Goodman, Noah D",
  abstract      = "Pyro is a probabilistic programming language built on Python
                   as a platform for developing advanced probabilistic models
                   in AI research. To scale to large datasets and
                   high-dimensional models, Pyro uses stochastic variational
                   inference algorithms and probability distributions built on
                   top of PyTorch, a modern GPU-accelerated deep learning
                   framework. To accommodate complex or model-specific
                   algorithmic behavior, Pyro leverages Poutine, a library of
                   composable building blocks for modifying the behavior of
                   probabilistic programs.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1810.09538"
}


@ARTICLE{socraticmodels,
  title         = "Socratic Models: Composing {Zero-Shot} Multimodal Reasoning
                   with Language",
  author        = "Zeng, Andy and Wong, Adrian and Welker, Stefan and
                   Choromanski, Krzysztof and Tombari, Federico and Purohit,
                   Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny
                   and Vanhoucke, Vincent and Florence, Pete",
  abstract      = "Large foundation models can exhibit unique capabilities
                   depending on the domain of data they are trained on. While
                   these domains are generic, they may only barely overlap. For
                   example, visual-language models (VLMs) are trained on
                   Internet-scale image captions, but large language models
                   (LMs) are further trained on Internet-scale text with no
                   images (e.g. from spreadsheets, to SAT questions). As a
                   result, these models store different forms of commonsense
                   knowledge across different domains. In this work, we show
                   that this model diversity is symbiotic, and can be leveraged
                   to build AI systems with structured Socratic dialogue -- in
                   which new multimodal tasks are formulated as a guided
                   language-based exchange between different pre-existing
                   foundation models, without additional finetuning. In the
                   context of egocentric perception, we present a case study of
                   Socratic Models (SMs) that can provide meaningful results
                   for complex tasks such as generating free-form answers to
                   contextual questions about egocentric video, by formulating
                   video Q\&A as short story Q\&A, i.e. summarizing the video
                   into a short story, then answering questions about it.
                   Additionally, SMs can generate captions for Internet images,
                   and are competitive with state-of-the-art on zero-shot
                   video-to-text retrieval with 42.8 R@1 on MSR-VTT 1k-A. SMs
                   demonstrate how to compose foundation models zero-shot to
                   capture new multimodal functionalities, without
                   domain-specific data collection. Prototypes are available at
                   socraticmodels.github.io.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2204.00598"
}


@ARTICLE{upsidedown_rl,
  title         = "Reinforcement Learning Upside Down: Don't Predict Rewards --
                   Just Map Them to Actions",
  author        = "Schmidhuber, Juergen",
  abstract      = "We transform reinforcement learning (RL) into a form of
                   supervised learning (SL) by turning traditional RL on its
                   head, calling this Upside Down RL (UDRL). Standard RL
                   predicts rewards, while UDRL instead uses rewards as
                   task-defining inputs, together with representations of time
                   horizons and other computable functions of historic and
                   desired future data. UDRL learns to interpret these input
                   observations as commands, mapping them to actions (or action
                   probabilities) through SL on past (possibly accidental)
                   experience. UDRL generalizes to achieve high rewards or
                   other goals, through input commands such as: get lots of
                   reward within at most so much time! A separate paper [63] on
                   first experiments with UDRL shows that even a pilot version
                   of UDRL can outperform traditional baseline algorithms on
                   certain challenging RL problems. We also also conceptually
                   simplify an approach [60] for teaching a robot to imitate
                   humans. First videotape humans imitating the robot's current
                   behaviors, then let the robot learn through SL to map the
                   videos (as input commands) to these behaviors, then let it
                   generalize and imitate videos of humans executing previously
                   unknown behavior. This Imitate-Imitator concept may actually
                   explain why biological evolution has resulted in parents who
                   imitate the babbling of their babies.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1912.02875"
}

@ARTICLE{decision_transformer,
  title         = "Decision Transformer: Reinforcement Learning via Sequence
                   Modeling",
  author        = "Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee,
                   Kimin and Grover, Aditya and Laskin, Michael and Abbeel,
                   Pieter and Srinivas, Aravind and Mordatch, Igor",
  abstract      = "We introduce a framework that abstracts Reinforcement
                   Learning (RL) as a sequence modeling problem. This allows us
                   to draw upon the simplicity and scalability of the
                   Transformer architecture, and associated advances in
                   language modeling such as GPT-x and BERT. In particular, we
                   present Decision Transformer, an architecture that casts the
                   problem of RL as conditional sequence modeling. Unlike prior
                   approaches to RL that fit value functions or compute policy
                   gradients, Decision Transformer simply outputs the optimal
                   actions by leveraging a causally masked Transformer. By
                   conditioning an autoregressive model on the desired return
                   (reward), past states, and actions, our Decision Transformer
                   model can generate future actions that achieve the desired
                   return. Despite its simplicity, Decision Transformer matches
                   or exceeds the performance of state-of-the-art model-free
                   offline RL baselines on Atari, OpenAI Gym, and Key-to-Door
                   tasks.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.01345"
}

@ARTICLE{rl_inference,
  title         = "Reinforcement Learning and Control as Probabilistic
                   Inference: Tutorial and Review",
  author        = "Levine, Sergey",
  abstract      = "The framework of reinforcement learning or optimal control
                   provides a mathematical formalization of intelligent
                   decision making that is powerful and broadly applicable.
                   While the general form of the reinforcement learning problem
                   enables effective reasoning about uncertainty, the
                   connection between reinforcement learning and inference in
                   probabilistic models is not immediately obvious. However,
                   such a connection has considerable value when it comes to
                   algorithm design: formalizing a problem as probabilistic
                   inference in principle allows us to bring to bear a wide
                   array of approximate inference tools, extend the model in
                   flexible and powerful ways, and reason about
                   compositionality and partial observability. In this article,
                   we will discuss how a generalization of the reinforcement
                   learning or optimal control problem, which is sometimes
                   termed maximum entropy reinforcement learning, is equivalent
                   to exact probabilistic inference in the case of
                   deterministic dynamics, and variational inference in the
                   case of stochastic dynamics. We will present a detailed
                   derivation of this framework, overview prior work that has
                   drawn on this and related ideas to propose new reinforcement
                   learning and control algorithms, and describe perspectives
                   on future research.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1805.00909"
}


@ARTICLE{zeroshot_reasoners,
  title         = "Large Language Models are {Zero-Shot} Reasoners",
  author        = "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and
                   Matsuo, Yutaka and Iwasawa, Yusuke",
  abstract      = "Pretrained large language models (LLMs) are widely used in
                   many sub-fields of natural language processing (NLP) and
                   generally known as excellent few-shot learners with
                   task-specific exemplars. Notably, chain of thought (CoT)
                   prompting, a recent technique for eliciting complex
                   multi-step reasoning through step-by-step answer examples,
                   achieved the state-of-the-art performances in arithmetics
                   and symbolic reasoning, difficult system-2 tasks that do not
                   follow the standard scaling laws for LLMs. While these
                   successes are often attributed to LLMs' ability for few-shot
                   learning, we show that LLMs are decent zero-shot reasoners
                   by simply adding ``Let's think step by step'' before each
                   answer. Experimental results demonstrate that our
                   Zero-shot-CoT, using the same single prompt template,
                   significantly outperforms zero-shot LLM performances on
                   diverse benchmark reasoning tasks including arithmetics
                   (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning
                   (Last Letter, Coin Flip), and other logical reasoning tasks
                   (Date Understanding, Tracking Shuffled Objects), without any
                   hand-crafted few-shot examples, e.g. increasing the accuracy
                   on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to
                   40.7\% with an off-the-shelf 175B parameter model. The
                   versatility of this single prompt across very diverse
                   reasoning tasks hints at untapped and understudied
                   fundamental zero-shot capabilities of LLMs, suggesting
                   high-level, multi-task broad cognitive capabilities may be
                   extracted through simple prompting. We hope our work not
                   only serves as the minimal strongest zero-shot baseline for
                   the challenging reasoning benchmarks, but also highlights
                   the importance of carefully exploring and analyzing the
                   enormous zero-shot knowledge hidden inside LLMs before
                   crafting finetuning datasets or few-shot exemplars.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.11916"
}


@ARTICLE{memoised_wake_sleep,
  title         = "Learning to learn generative programs with Memoised
                   {Wake-Sleep}",
  author        = "Hewitt, Luke B and Le, Tuan Anh and Tenenbaum, Joshua B",
  abstract      = "We study a class of neuro-symbolic generative models in
                   which neural networks are used both for inference and as
                   priors over symbolic, data-generating programs. As
                   generative models, these programs capture compositional
                   structures in a naturally explainable form. To tackle the
                   challenge of performing program induction as an 'inner-loop'
                   to learning, we propose the Memoised Wake-Sleep (MWS)
                   algorithm, which extends Wake Sleep by explicitly storing
                   and reusing the best programs discovered by the inference
                   network throughout training. We use MWS to learn accurate,
                   explainable models in three challenging domains:
                   stroke-based character modelling, cellular automata, and
                   few-shot learning in a novel dataset of real-world string
                   concepts.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2007.03132"
}


@ARTICLE{Jung2022,
  title         = "Maieutic Prompting: Logically Consistent Reasoning with
                   Recursive Explanations",
  author        = "Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman,
                   Faeze and Bhagavatula, Chandra and Le Bras, Ronan and Choi,
                   Yejin",
  month         =  may,
  year          =  2022,
  url           = "http://arxiv.org/abs/2205.11822",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.11822"
}


@ARTICLE{lamda,
  title         = "{LaMDA}: Language Models for Dialog Applications",
  author        = "Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and
                   Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze
                   and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu
                   and Li, Yaguang and Lee, Hongrae and Zheng, Huaixiu Steven
                   and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping
                   and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and
                   Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts,
                   Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi
                   and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will
                   and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee
                   and Meier-Hellstern, Kathleen and Morris, Meredith Ringel
                   and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju
                   and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran,
                   Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson,
                   Kristen and Molina, Alejandra and Hoffman-John, Erin and
                   Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna,
                   Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton,
                   Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray
                   and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian
                   and Chi, Ed and Le, Quoc",
  abstract      = "We present LaMDA: Language Models for Dialog Applications.
                   LaMDA is a family of Transformer-based neural language
                   models specialized for dialog, which have up to 137B
                   parameters and are pre-trained on 1.56T words of public
                   dialog data and web text. While model scaling alone can
                   improve quality, it shows less improvements on safety and
                   factual grounding. We demonstrate that fine-tuning with
                   annotated data and enabling the model to consult external
                   knowledge sources can lead to significant improvements
                   towards the two key challenges of safety and factual
                   grounding. The first challenge, safety, involves ensuring
                   that the model's responses are consistent with a set of
                   human values, such as preventing harmful suggestions and
                   unfair bias. We quantify safety using a metric based on an
                   illustrative set of human values, and we find that filtering
                   candidate responses using a LaMDA classifier fine-tuned with
                   a small amount of crowdworker-annotated data offers a
                   promising approach to improving model safety. The second
                   challenge, factual grounding, involves enabling the model to
                   consult external knowledge sources, such as an information
                   retrieval system, a language translator, and a calculator.
                   We quantify factuality using a groundedness metric, and we
                   find that our approach enables the model to generate
                   responses grounded in known sources, rather than responses
                   that merely sound plausible. Finally, we explore the use of
                   LaMDA in the domains of education and content
                   recommendations, and analyze their helpfulness and role
                   consistency.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.08239",
  url = {https://arxiv.org/abs/2201.08239}
}


@ARTICLE{Levine2022,
  title         = "Standing on the Shoulders of Giant Frozen Language Models",
  author        = "Levine, Yoav and Dalmedigos, Itay and Ram, Ori and Zeldes,
                   Yoel and Jannai, Daniel and Muhlgay, Dor and Osin, Yoni and
                   Lieber, Opher and Lenz, Barak and Shalev-Shwartz, Shai and
                   Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav",
  month         =  apr,
  year          =  2022,
  url           = "http://arxiv.org/abs/2204.10019",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.10019"
}


@INPROCEEDINGS{Sonderby2016ladder,
  title     = "Ladder Variational Autoencoders",
  booktitle = nips,
  author    = "S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o} e, Lars
               and S{\o} nderby, S{\o} Ren Kaae and Winther, Ole",
  pages     = "3738--3746",
  year      =  2016,
  url       = "http://papers.nips.cc/paper/6275-ladder-variational-autoencoders.pdf"
}


@ARTICLE{gpt3,
  title         = "Language Models are {Few-Shot} Learners",
  author        = "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah,
                   Melanie and Kaplan, Jared and Dhariwal, Prafulla and
                   Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and
                   Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel
                   and Krueger, Gretchen and Henighan, Tom and Child, Rewon and
                   Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and
                   Winter, Clemens and Hesse, Christopher and Chen, Mark and
                   Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess,
                   Benjamin and Clark, Jack and Berner, Christopher and
                   McCandlish, Sam and Radford, Alec and Sutskever, Ilya and
                   Amodei, Dario",

  year          =  2020,
  url           = "http://arxiv.org/abs/2005.14165",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.14165"
}
@article{anglican,
  author    = {David Tolpin and
               Jan{-}Willem van de Meent and
               Hongseok Yang and
               Frank D. Wood},
  title     = {Design and Implementation of Probabilistic Programming Language Anglican},
  journal   = {CoRR},
  volume    = {abs/1608.05263},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.05263},
  eprinttype = {arXiv},
  eprint    = {1608.05263},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/TolpinMYW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{openai_critique,
  doi = {10.48550/ARXIV.2206.05802},
  
  url = {https://arxiv.org/abs/2206.05802},
  
  author = {Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Self-critiquing models for assisting human evaluators},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{inference_combinators,
  doi = {10.48550/ARXIV.2103.00668},
  
  url = {https://arxiv.org/abs/2103.00668},
  
  author = {Stites, Sam and Zimmermann, Heiko and Wu, Hao and Sennesh, Eli and van de Meent, Jan-Willem},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Proposals for Probabilistic Programs with Inference Combinators},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}