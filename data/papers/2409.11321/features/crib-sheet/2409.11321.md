- **Key Contribution**: Introduces SOAP (ShampoO with Adam in the Preconditioner's eigenbasis), a computationally efficient algorithm that combines Shampoo and Adam.
  
- **Equivalence**: Establishes that Shampoo (with exponent 1/2) is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner.

- **Performance Metrics**: SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements over Shampoo.

- **Hyperparameters**: SOAP introduces only one additional hyperparameter (preconditioning frequency) compared to AdamW, simplifying the tuning process.

- **Robustness**: SOAP shows greater robustness to large preconditioning frequency compared to Shampoo, mitigating performance degradation.

- **Implementation**: Available at [GitHub - SOAP](https://github.com/nikhilvyas/SOAP).

- **Notation**:
  - Weight matrix: \( W \in \mathbb{R}^{m \times n} \)
  - Gradient: \( G \in \mathbb{R}^{m \times n} \)
  - Loss function: \( \phi_{B_t}(W_t) \)
  - Vectorized gradient: \( g_t = \text{vec}(G_t) \)

- **Update Rules**:
  - **Adam**: 
    \[
    W_t \leftarrow W_{t-1} - \eta M_t \sqrt{V_t}
    \]
  - **Shampoo**:
    \[
    W_t \leftarrow W_{t-1} - \eta L^{-1/4}_t G_t R^{-1/4}_t
    \]
  - **Adafactor**:
    \[
    W_t \leftarrow W_{t-1} - \eta M_t V'_t
    \]

- **Algorithm Steps for SOAP**:
  1. Sample batch \( B_t \).
  2. Compute gradient \( G_t \).
  3. Update preconditioners \( L \) and \( R \).
  4. Update weights using the preconditioned gradient.

- **Empirical Evaluation**: Conducted on language model pre-training with models of sizes 360m and 660m.

- **Related Works**: Discusses connections to KFAC, E-KFAC, and other second-order optimization methods.

- **Theoretical Insights**: Explores the theoretical foundations of Shampoo and its modifications, including the optimal Kronecker approximation of Adagrad.

- **Complexity Analysis**: SOAP's space and time complexity are discussed, highlighting improvements over existing methods.