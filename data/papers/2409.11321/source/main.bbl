\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2020)Anil, Gupta, Koren, Regan, and Singer]{anil2020scalable}
Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer.
\newblock Scalable second order optimization for deep learning.
\newblock \emph{arXiv preprint arXiv:2002.09018}, 2020.

\bibitem[Ba et~al.(2017)Ba, Grosse, and Martens]{ba2017distributed}
Jimmy Ba, Roger Grosse, and James Martens.
\newblock Distributed second-order optimization using kronecker-factored approximations.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=SkkTMpjex}.

\bibitem[Chen et~al.(2023)Chen, Liang, Huang, Real, Wang, Pham, Dong, Luong, Hsieh, Lu, and Le]{lion}
Xiangning Chen, Chen Liang, Da~Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho{-}Jui Hsieh, Yifeng Lu, and Quoc~V. Le.
\newblock Symbolic discovery of optimization algorithms.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), \emph{Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}, 2023.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2023/hash/9a39b4925e35cf447ccba8757137d84f-Abstract-Conference.html}.

\bibitem[Dahl et~al.(2023)Dahl, Schneider, Nado, Agarwal, Sastry, Hennig, Medapati, Eschenhagen, Kasimbeg, Suo, Bae, Gilmer, Peirson, Khan, Anil, Rabbat, Krishnan, Snider, Amid, Chen, Maddison, Vasudev, Badura, Garg, and Mattson]{dahl2023benchmarking}
George~E. Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli~Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel~L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, Kongtao Chen, Chris~J. Maddison, Rakshith Vasudev, Michal Badura, Ankush Garg, and Peter Mattson.
\newblock Benchmarking neural network training algorithms, 2023.

\bibitem[Defazio et~al.(2024)Defazio, Yang, Mehta, Mishchenko, Khaled, and Cutkosky]{schedulefree}
Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, and Ashok Cutkosky.
\newblock The road less scheduled.
\newblock \emph{CoRR}, abs/2405.15682, 2024.
\newblock \doi{10.48550/ARXIV.2405.15682}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2405.15682}.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7480--7512. PMLR, 2023.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Shleifer, and Zettlemoyer]{8bitadam}
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
\newblock 8-bit optimizers via block-wise quantization.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=shpkpVXzo3h}.

\bibitem[Documentation(2024)]{eigh-vs-qr}
Documentation.
\newblock torch.linalg.eigh documentation.
\newblock \url{https://web.archive.org/web/20240519213242/https://pytorch.org/docs/stable/generated/torch.linalg.eigh.html}, 2024.

\bibitem[Duchi et~al.(2011{\natexlab{a}})Duchi, Hazan, and Singer]{JMLR:v12:duchi11a}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (61):\penalty0 2121--2159, 2011{\natexlab{a}}.
\newblock URL \url{http://jmlr.org/papers/v12/duchi11a.html}.

\bibitem[Duchi et~al.(2011{\natexlab{b}})Duchi, Hazan, and Singer]{adagrad}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2121--2159, 2011{\natexlab{b}}.
\newblock \doi{10.5555/1953048.2021068}.
\newblock URL \url{https://dl.acm.org/doi/10.5555/1953048.2021068}.

\bibitem[Duvvuri et~al.(2024)Duvvuri, Devvrit, Anil, Hsieh, and Dhillon]{caspr}
Sai~Surya Duvvuri, Fnu Devvrit, Rohan Anil, Cho-Jui Hsieh, and Inderjit~S Dhillon.
\newblock Combining axes preconditioners through kronecker approximation for deep learning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=8j9hz8DVi8}.

\bibitem[Eschenhagen et~al.(2023)Eschenhagen, Immer, Turner, Schneider, and Hennig]{eschenhagen2023kroneckerfactored}
Runa Eschenhagen, Alexander Immer, Richard~E Turner, Frank Schneider, and Philipp Hennig.
\newblock Kronecker-factored approximate curvature for modern neural network architectures.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Ex3oJEKS53}.

\bibitem[Gao et~al.(2021)Gao, Liu, Huang, Wang, Wang, Xu, and Yu]{Gao_Liu_Huang_Wang_Wang_Xu_Yu_2021}
Kaixin Gao, Xiaolei Liu, Zhenghai Huang, Min Wang, Zidong Wang, Dachuan Xu, and Fan Yu.
\newblock A trace-restricted kronecker-factored approximation to natural gradient.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 35\penalty0 (9):\penalty0 7519--7527, May 2021.
\newblock \doi{10.1609/aaai.v35i9.16921}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/16921}.

\bibitem[Gemini~Team(2024)]{gemini15}
Google Gemini~Team.
\newblock {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}.
\newblock \url{https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf}, 2024.
\newblock [Online; accessed 19-May-2024].

\bibitem[George et~al.(2018)George, Laurent, Bouthillier, Ballas, and Vincent]{ekfac}
Thomas George, C{\'{e}}sar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent.
\newblock Fast approximate natural gradient descent in a kronecker factored eigenbasis.
\newblock In Samy Bengio, Hanna~M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol{\`{o}} Cesa{-}Bianchi, and Roman Garnett (eds.), \emph{Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada}, pp.\  9573--9583, 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper/2018/hash/48000647b315f6f00f913caa757a70b3-Abstract.html}.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, et~al.]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{arXiv preprint arXiv:2402.00838}, 2024.

\bibitem[Gupta et~al.(2018{\natexlab{a}})Gupta, Koren, and Singer]{gupta2018shampoo}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1842--1850. PMLR, 2018{\natexlab{a}}.

\bibitem[Gupta et~al.(2018{\natexlab{b}})Gupta, Koren, and Singer]{shampoo}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock In Jennifer~G. Dy and Andreas Krause (eds.), \emph{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  1837--1845. {PMLR}, 2018{\natexlab{b}}.
\newblock URL \url{http://proceedings.mlr.press/v80/gupta18a.html}.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Ishikawa \& Yokota(2024)Ishikawa and Yokota]{ishikawa2024when}
Satoki Ishikawa and Rio Yokota.
\newblock When does second-order optimization speed up training?
\newblock In \emph{The Second Tiny Papers Track at ICLR 2024}, 2024.
\newblock URL \url{https://openreview.net/forum?id=NLrfEsSZNb}.

\bibitem[Kaddour et~al.(2023)Kaddour, Key, Nawrot, Minervini, and Kusner]{KaddourKNMK23}
Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt~J. Kusner.
\newblock No train no gain: Revisiting efficient training algorithms for transformer-based language models.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), \emph{Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}, 2023.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2023/hash/51f3d6252706100325ddc435ba0ade0e-Abstract-Conference.html}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun (eds.), \emph{3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Li(2018)]{li18psgd}
Xi-Lin Li.
\newblock Preconditioned stochastic gradient descent.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 29\penalty0 (5):\penalty0 1454--1466, 2018.
\newblock \doi{10.1109/TNNLS.2017.2672978}.

\bibitem[Li(2024)]{li2024stochastichessianfittingslie}
Xi-Lin Li.
\newblock Stochastic hessian fittings with lie groups, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.11858}.

\bibitem[Lin et~al.(2024)Lin, Dangel, Eschenhagen, Bae, Turner, and Makhzani]{wulin2024}
Wu~Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard~E. Turner, and Alireza Makhzani.
\newblock Can we remove the square-root in adaptive gradient methods? {A} second-order perspective.
\newblock In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), \emph{Proceedings of the 41st International Conference on Machine Learning}, volume 235 of \emph{Proceedings of Machine Learning Research}, pp.\  29949--29973. PMLR, 21--27 Jul 2024.
\newblock URL \url{https://proceedings.mlr.press/v235/lin24e.html}.

\bibitem[Liu et~al.(2024)Liu, Li, Hall, Liang, and Ma]{liu2024sophia}
Hong Liu, Zhiyuan Li, David Leo~Wright Hall, Percy Liang, and Tengyu Ma.
\newblock Sophia: A scalable stochastic second-order optimizer for language model pre-training.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=3xHDeA8Noi}.

\bibitem[Lv et~al.(2024{\natexlab{a}})Lv, Yan, Guo, Lv, and Qiu]{adalomo}
Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, and Xipeng Qiu.
\newblock Adalomo: Low-memory optimization with adaptive learning rate.
\newblock In Lun{-}Wei Ku, Andre Martins, and Vivek Srikumar (eds.), \emph{Findings of the Association for Computational Linguistics, {ACL} 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024}, pp.\  12486--12502. Association for Computational Linguistics, 2024{\natexlab{a}}.
\newblock URL \url{https://aclanthology.org/2024.findings-acl.742}.

\bibitem[Lv et~al.(2024{\natexlab{b}})Lv, Yang, Liu, Guo, and Qiu]{lomo}
Kai Lv, Yuqing Yang, Tengxiao Liu, Qipeng Guo, and Xipeng Qiu.
\newblock Full parameter fine-tuning for large language models with limited resources.
\newblock In Lun{-}Wei Ku, Andre Martins, and Vivek Srikumar (eds.), \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand, August 11-16, 2024}, pp.\  8187--8198. Association for Computational Linguistics, 2024{\natexlab{b}}.
\newblock URL \url{https://aclanthology.org/2024.acl-long.445}.

\bibitem[Martens(2010)]{martens10}
James Martens.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{Proceedings of the 27th International Conference on International Conference on Machine Learning}, ICML'10, pp.\  735–742, Madison, WI, USA, 2010. Omnipress.
\newblock ISBN 9781605589077.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens15}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate curvature.
\newblock In Francis Bach and David Blei (eds.), \emph{Proceedings of the 32nd International Conference on Machine Learning}, volume~37 of \emph{Proceedings of Machine Learning Research}, pp.\  2408--2417, Lille, France, 07--09 Jul 2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v37/martens15.html}.

\bibitem[Martens et~al.(2018)Martens, Ba, and Johnson]{martens2018kroneckerrecurrent}
James Martens, Jimmy Ba, and Matt Johnson.
\newblock Kronecker-factored curvature approximations for recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HyMTkQZAb}.

\bibitem[Morwani et~al.(2024)Morwani, Shapira, Vyas, Malach, Kakade, and Janson]{whyshampoo}
Depen Morwani, Itai Shapira, Nikhil Vyas, Eran Malach, Sham~M. Kakade, and Lucas Janson.
\newblock A new perspective on shampoo's preconditioner.
\newblock \emph{CoRR}, abs/2406.17748, 2024.
\newblock \doi{10.48550/ARXIV.2406.17748}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.17748}.

\bibitem[Osawa et~al.(2019)Osawa, Tsuji, Ueno, Naruse, Yokota, and Matsuoka]{kazuki19}
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.
\newblock Large-scale distributed second-order optimization using kronecker-factored approximate curvature for deep convolutional neural networks.
\newblock In \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  12351--12359, 2019.
\newblock \doi{10.1109/CVPR.2019.01264}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Peirson et~al.(2022)Peirson, Amid, Chen, Feinberg, Warmuth, and Anil]{peirson2022fishy}
Abel Peirson, Ehsan Amid, Yatong Chen, Vladimir Feinberg, Manfred~K Warmuth, and Rohan Anil.
\newblock Fishy: Layerwise fisher approximation for higher-order neural network optimization.
\newblock In \emph{Has it Trained Yet? NeurIPS 2022 Workshop}, 2022.
\newblock URL \url{https://openreview.net/forum?id=cScb-RrBQC}.

\bibitem[Pooladzandi \& Li(2024)Pooladzandi and Li]{pooladzandi2024curvatureinformed}
Omead Pooladzandi and Xi-Lin Li.
\newblock Curvature-informed {SGD} via general purpose lie-group preconditioners, 2024.
\newblock URL \url{https://openreview.net/forum?id=sawjxRnVpF}.

\bibitem[Porian et~al.(2024)Porian, Wortsman, Jitsev, Schmidt, and Carmon]{porian}
Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, and Yair Carmon.
\newblock Resolving discrepancies in compute-optimal scaling of language models.
\newblock \emph{CoRR}, abs/2406.19146, 2024.
\newblock \doi{10.48550/ARXIV.2406.19146}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.19146}.

\bibitem[Puiu(2022)]{Puiu22}
Constantin~Octavian Puiu.
\newblock Randomized k-facs: Speeding up k-fac with randomized numerical linear algebra.
\newblock In Hujun Yin, David Camacho, and Peter Tino (eds.), \emph{Intelligent Data Engineering and Automated Learning -- IDEAL 2022}, pp.\  411--422, Cham, 2022. Springer International Publishing.
\newblock ISBN 978-3-031-21753-1.

\bibitem[Puiu(2023)]{puiu2023brandnewkfacsspeeding}
Constantin~Octavian Puiu.
\newblock Brand new k-facs: Speeding up k-fac with online decomposition updates, 2023.
\newblock URL \url{https://arxiv.org/abs/2210.08494}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Ren \& Goldfarb(2021)Ren and Goldfarb]{yi21}
Yi~Ren and Donald Goldfarb.
\newblock Tensor normal training for deep learning models.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan (eds.), \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  26040--26052. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/dae3312c4c6c7000a37ecfb7b0aeb0e4-Paper.pdf}.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In Jennifer~G. Dy and Andreas Krause (eds.), \emph{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  4603--4611. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/shazeer18a.html}.

\bibitem[Shi et~al.(2023)Shi, Lee, Iwasaki, Gallego{-}Posada, Li, Rangadurai, Mudigere, and Rabbat]{distributedshampoo}
Hao{-}Jun~Michael Shi, Tsung{-}Hsien Lee, Shintaro Iwasaki, Jose Gallego{-}Posada, Zhijing Li, Kaushik Rangadurai, Dheevatsa Mudigere, and Michael Rabbat.
\newblock A distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale.
\newblock \emph{CoRR}, abs/2309.06497, 2023.
\newblock \doi{10.48550/ARXIV.2309.06497}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2309.06497}.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Vyas et~al.(2024)Vyas, Morwani, and Kakade]{adamem}
Nikhil Vyas, Depen Morwani, and Sham~M. Kakade.
\newblock Adamem: Memory efficient momentum for adafactor.
\newblock In \emph{2nd Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ICML 2024)}, 2024.
\newblock URL \url{https://openreview.net/forum?id=fZqMVTz7K5}.

\bibitem[Wang et~al.(2024)Wang, Li, Zhou, and Huang]{4bitshampoo}
Sike Wang, Jia Li, Pan Zhou, and Hua Huang.
\newblock 4-bit shampoo for memory-efficient network training.
\newblock \emph{CoRR}, abs/2405.18144, 2024.
\newblock \doi{10.48550/ARXIV.2405.18144}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2405.18144}.

\bibitem[Wortsman et~al.(2024)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, Pennington, Sohl-Dickstein, Xu, Lee, Gilmer, and Kornblith]{wortsman2024smallscale}
Mitchell Wortsman, Peter~J Liu, Lechao Xiao, Katie~E Everett, Alexander~A Alemi, Ben Adlam, John~D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=d8w0pmvXbZ}.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and Beyer]{zhai}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pp.\  1204--1213. {IEEE}, 2022.
\newblock \doi{10.1109/CVPR52688.2022.01179}.
\newblock URL \url{https://doi.org/10.1109/CVPR52688.2022.01179}.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue, and Grosse]{nqm}
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George~E. Dahl, Christopher~J. Shallue, and Roger~B. Grosse.
\newblock Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett (eds.), \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pp.\  8194--8205, 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper/2019/hash/e0eacd983971634327ae1819ea8b6214-Abstract.html}.

\bibitem[Zhao et~al.(2024{\natexlab{a}})Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{galore}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.
\newblock Galore: Memory-efficient {LLM} training by gradient low-rank projection.
\newblock \emph{CoRR}, abs/2403.03507, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2403.03507}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2403.03507}.

\bibitem[Zhao et~al.(2024{\natexlab{b}})Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{galoregithub}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.
\newblock (code) galore: Memory-efficient {LLM} training by gradient low-rank projection.
\newblock \url{https://github.com/jiaweizzhao/GaLore}, 2024{\natexlab{b}}.

\bibitem[Zhao et~al.(2024{\natexlab{c}})Zhao, Morwani, Brandfonbrener, Vyas, and Kakade]{zhaoscience}
Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, and Sham~M. Kakade.
\newblock Deconstructing what makes a good optimizer for language models.
\newblock \emph{CoRR}, abs/2407.07972, 2024{\natexlab{c}}.
\newblock \doi{10.48550/ARXIV.2407.07972}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2407.07972}.

\end{thebibliography}
