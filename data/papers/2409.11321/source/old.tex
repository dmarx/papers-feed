
{\color{blue} We probably want to try model scales/architectures?: ResNet50 and InceptionNeXt-T? trying to stick to batch norm networks since we think things might be off for layernorm networks in Vision.}

To launch new: AdamW 300M beta1, beta2, eps sweeps can just be 2 or 3 values each with 3 LRs.  

Confirm that we have GaLore std runs and take best amount full and std, maybe full is too unstable due to weird momentum .


Figure 1: 2x2 plot: 300M, 600M x iterations, time, datasetsize. Comparing AdamW, DistributedShampoo(10), StableShampooQR2(10).

600M, 75,: QR2, 1 and 3 missing. [Nikhil, launched] 
600M: Original Shampoo2, (3) LR sweep. [Depen]

300M, 600M QR2, 87.5, 62.5, 3 optimal LRs. 
300M: Depen
600M: Nikhil

300M, QR2 (full) LRs: 1e-2, 3.16e-2, 1.0e-1 [launched]


Figure 2: 2x2 plot: 150M, 300M x dataset iterations, dataset time, datasetsize. Comparing AdamW, DistributedShampoo(10), StableShampoo(10).

Fig 3: Frequency vs Performance; 1, 3, 10, 32, 100. 300M + 2M. This should have OrigShampoo i.e. with no layernorm.

300M: 3 (LRs) x 5 (freq) x 2 (optimizers).
Nikhil: QR2
Depen:ShampooOrig2 (send to Depen: 300-10-orig2sweep (1st group)



Fig 4: Learning stability for 300M, 2M bsz, freq 10.  AdamW, InverseShampoo(10), StableShampooQR2(10).

Subsection: One sided Shampoo.

Maybe do hessian measurements. - later.

Ablations:

LayerNorm + LastLayer. DistributedShampoo vs InverseShampoo. Need to launch both for different powers. 

Appendix: All stability plots to show people that we have tuned everything well.

ShampooAdafactor plot somewhere, maybe in theory.

To launch: 600M, 300M StableShampooQR2, 300M 50\% and 75\% runs of StableShampooQR2, InverseShampoo, AdamW. 








\section{Old}



\section{Ablations}