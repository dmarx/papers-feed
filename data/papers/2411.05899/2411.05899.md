---
abstract: |
  The abstract paragraph should be indented 1/2 inch (3 picas) on both left and right-hand margins. Use 10 point type, with a vertical spacing of 11 points. The word <span class="smallcaps">Abstract</span> must be centered, in small caps, and in point size 12. Two line spaces precede the abstract. The abstract must be limited to one paragraph.
author:
- |
  Antiquus S. Hippocampus, Natalia Cerebro & Amelie P. Amygdale [^1]  
  Department of Computer Science  
  Cranberry-Lemon University  
  Pittsburgh, PA 15213, USA  
  `{hippo,brain,jen}@cs.cranberry-lemon.edu`  
  Ji Q. Ren & Yevgeny LeNet  
  Department of Computational Neuroscience  
  University of the Witwatersrand  
  Joburg, South Africa  
  `{robot,net}@wits.ac.za`  
  Coauthor  
  Affiliation  
  Address  
  `email`
bibliography:
- iclr2021_conference.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: How powerful are GFlowNets?
---





# Flows for trees and uniform distributions

ping

<div style="color: blue">

**To-do list (sensitivity analysis):**

1.  Sensitivity analysis for regular trees and uniform distribution

2.  Generalization for DAGs

3.  Generalization for non-uniform distributions

</div>

<div style="color: orange">

**To-do list (policy networks):**

1.  Anonymous

    1.  Balance is impossible for some pairs of pointed DAGs and reward functions

    2.  Some characterization of rewards that are particularly hard to approximate

    3.  Sequences, Multisets, Anonymous and non-anonymous graphs (directed and undirected)

    4.  trade-off between invariances in the networks and built into the state graphtel

2.  Non-anonymous

</div>

<div style="color: red">

**To-do list (evaluation and diagnostic of GFLowNets):**

1.  Current evaluation protocols are crap (usually focus on covering modes rather than approximation). Doing the right thing is also computationally infeasible for larges state-spaces

2.  Convergence diagnostic based on some estimate of $\delta$, leveraging our theorems in the first part of the paper

3.  some diagonose based on, e.g, the estimates we can get from $R$ based on the trajectory-balance loss (in equilibrium, should be path independent, i.e., a constant)

</div>

## Uniform distributions and uniform flows

- A uniform flow of degree $g$ and height $h$ is a Markovian flow that models a uniform distribution, meaning all the leaf nodes have the same value. The policy of such flow consists in a function that takes the incoming flow and splits it equally between each of the g outgoing nodes.

To start let us consider the example of a flow trained on a uniform distribution and policy that takes the incoming flow and split it equally to all outgoing states. The resulting uniform distribution on the terminal nodes density is represented by $\pi^*(x)=\frac{1}{g^h}$, for each terminal object in the domain $x \in \mathcal{X}$ and $|\mathcal{X}|=g^h$.

Let us consider the case that the policy at the root of the network introduces an error in the flow of size $\delta$, meaning that one children node now will receive a flow $\frac{F}{g}+\delta$ and the other $g-1$ will continue with $\frac{F}{g}$, which is equivalent to the policy with a probability density (after normalizing) that assigns a probability $\frac{F+g\delta}{g(F+\delta)}$ to one branch and $\frac{F}{g(F+\delta)}$ to the other $g-1$ branches. The total variation distance between this new density and the original policy (uniform probability for each $g$ branches) is $\epsilon(\delta, g)=(1-\frac{1}{g})\frac{\delta}{F+\delta}$. Now we denote the resulting sampling distribution induced by this modified flow as $\pi(x)$.

<figure id="fig:tree_graph">
<p><span class="math display">\[\begin{tikzcd}
    &amp;&amp;&amp; {F+\delta} \\
    &amp;&amp; {\frac{F}{g}+\delta} &amp;&amp; {\frac{F}{g}\text{ }\triangle} \\
    &amp; \triangle &amp; \triangle &amp;&amp; \triangle &amp; \triangle \\
    {\frac{F}{g^h}+\delta_1} &amp; {\frac{F}{g^h}+\delta_2\text{  }\ldots } &amp; {\frac{F}{g^h}+\delta_{g^{h-1}}} &amp;&amp; {\frac{F}{g^h}} &amp; {\frac{F}{g^h}} &amp; {\frac{F}{g^h}}
    \arrow[&quot;{\text{degree g}}&quot;, from=1-4, to=2-3]
    \arrow[from=1-4, to=2-5]
    \arrow[from=2-5, to=3-6]
    \arrow[from=2-5, to=3-5]
    \arrow[from=2-3, to=3-3]
    \arrow[from=2-3, to=3-2]
    \arrow[from=3-2, to=4-1]
    \arrow[from=3-2, to=4-2]
    \arrow[from=3-3, to=4-3]
    \arrow[from=3-5, to=4-5]
    \arrow[from=3-6, to=4-6]
    \arrow[from=3-6, to=4-7]
\end{tikzcd}\]</span></p>
<figcaption>A flow network with a extra flow of <span class="math inline">\(\delta\)</span> in one of the branches of the initial state</figcaption>
</figure>

Let $\mu$ and $\nu$ be two probability measures, then we denote $||\mu - \nu||_{\scaleto{\textbf{TV}}{3pt}}$ as the total variation distance between them.

<div class="assumption">

**Assumption 1**. Let the pair $(G_T, F)$ be a flow network such that $G_T$ is a regular tree with degree $g$ and depth $h$. Furthermore, assume that $F$ spreads uniformly in the edges of $G_T$, then the target distribution $\pi^*$ generated by $(G_T, F)$ is uniform.

</div>

<div class="theorem">

**Theorem 1** (Total variation of the sampling distribution). *Let $\delta >0$ and $\sum_{i=1}^{g^{h-1}} \delta_i = \delta$, where $\delta_i \in [0, \delta]$ for all $i \in \{1,2, \dots, g^{h-1}\}$. Suppose that we have the flow network $(G_T, F+\delta)$ abiding by Assumption  besides the first edge from the root to a son where it has a $\delta$ increasing generating a new target distribution $\pi$. Then under these conditions describe the total variation distance between $\pi$ and $\pi^*$ is bounded above and below by the following $$\begin{aligned}
& \epsilon(\delta, g) \leq ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} \leq \epsilon(\delta, g^h) \quad \text{where}
\\
& \epsilon(a,b) := \Big(1 - \frac{1}{b} \Big) \frac{a}{F+a}\,.
\end{aligned}$$*

</div>

<div class="proof">

*Proof.* The terminal states of the modified flow network will have two types of nodes, with flow $\frac{F}{g^h}$ and $\frac{F}{g^h}+\delta_{i}$, with $\delta_i \geq 0$ and $\sum_{i=1}^{g^{h-1}} \delta_i = \delta$. We normalize those probabilities to obtain the individual probabilities for each terminal state, which determines the density of each sample. From that, we can proceed to compute the total variation distance between $\pi$ and $\pi^*$. $$\begin{aligned}
    ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} &= \frac{1}{2}\sum_{x \in \mathcal{X}} | \pi(x)- \pi^*(x) | \\
                          &= \frac{1}{2}\left[(g^h-g^{h-1})\left|\frac{F}{g^h}\frac{1}{F+\delta} - \frac{1}{g^h}\right|+ \sum_{i=1}^{g^{h-1}} \left|\frac{F+g^h\delta_i}{g^h}\frac{1}{F+\delta} - \frac{1}{g^h}\right| \right] \\
                          &= \frac{1}{2}\left[\frac{g^h\delta-g^{h-1}\delta+\sum_{i=1}^{g^{h-1}}|g^h\delta_i-\delta|}{g^h(F+\delta)}\right]
\end{aligned}$$

We can lower bound $\sum_{i=1}^{g^{h-1}}|g^h\delta_i-\delta|$, by considering that $\sum_{i=1}^{g^{h-1}}(g^h\delta_i-\delta)=g^{h}\delta-g^{h-1}\delta$, taking the absolute value of the result and each element of the sum to obtain $g^{h}\delta-g^{h-1}\delta \leq \sum_{i=1}^{g^{h-1}}|g^h\delta_i-\delta|$. Thus we obtain the lower bound $$\begin{aligned}
\frac{1}{2}\left[\frac{g^h\delta-g^{h-1}\delta+g^{h}\delta-g^{h-1}\delta}{g^h(F+\delta)}\right]&\leq \frac{1}{2}\left[\frac{g^h\delta-g^{h-1}\delta+\sum_{i=1}^{g^{h-1}}|g^h\delta_i-\delta|}{g^h(F+\delta)}\right] \\
                       \left(1-\frac{1}{g}\right)\frac{\delta}{F+\delta} &\leq  ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} 
\end{aligned}$$.

This lower bound is reached when all error terms in the terminal states have the same value $\delta_i = \frac{\delta}{g^h}$.

To upper bound $|g^h\delta_i-\delta|$ we apply the triangle inequality, obtaining $|g^h\delta_i-\delta| \leq g^h\delta_i+\delta$ and $\sum_{i=1}^{g^{h-1}}|g^h\delta_i-\delta| \leq g^h\delta+g^{h-1}\delta$, from which we obtain the upper bound $$\begin{aligned}
    ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} &\leq \frac{1}{2}\left[\frac{g^h\delta-g^{h-1}\delta + g^{h}\delta+g^{h-1}\delta}{g^h(F+\delta)}\right] \\
    ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} &\leq \frac{\delta}{F+\delta}
\end{aligned}$$.

To obtain a tighter bound we break the sum $\sum_{i=1}^{g^{h-1}}|g^h\delta_i-\delta|$ by partitioning the sum into the first $I$ terms $S_A=g^h\sum_{i=1}^{I}|\delta_i-\frac{\delta}{g^h}|$ with $\delta_i < \frac{\delta}{g^h}$ and subsequent $g^{h-1}-I$ terms $S_B=g^h\sum_{j=I+1}^{g^{h-1}}|\delta_j-\frac{\delta}{g^h}|$ with $\delta_j \geq \frac{\delta}{g^h}$. By construction, we know that $S_A+g^h\sum_{i=1}^{I}\delta_i+g^h\sum_{j=I+1}^{g^{h-1}}\delta_j-S_B=g^{h-1}\delta$, simplifying to $S_B-S_A=\delta(g^h-g^{h-1})$. We rewrite $S_A + S_B = S_B-S_A+2S_A=\delta(g^h-g^{h-1})+2S_A$, and by triangle inequality on $S_A$, we obtain the upper bound $\sum_{i=1}^{g^{h-1}}|g^h\delta_i-\delta|=S_A+S_B \leq g^h\delta-g^{h-1}\delta+2I\delta$. Setting $I=g^{h-1}-1$ (the biggest value it can have without breaking the constraints on $\delta_i$), it simplifies to $S_A+S_B \leq g^h\delta+g^{h-1}\delta-2\delta$

$$\begin{aligned}
    ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} &\leq \frac{1}{2}\left[\frac{g^h\delta-g^{h-1}\delta+\sum_{i=1}^{g^{h-1}}|g^h\delta_i-\delta|}{g^h(F+\delta)}\right] \\
    ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} &\leq \frac{1}{2}\left[\frac{g^h\delta-g^{h-1}\delta+g^h\delta+g^{h-1}\delta-2\delta }{g^h(F+\delta)}\right] \\
    ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} &\leq \left[\frac{g^h\delta-\delta }{g^h(F+\delta)}\right] \\
    ||\pi - \pi^*||_{\scaleto{\textbf{TV}}{3pt}} &\leq  \left(1-\frac{1}{g^h}\right)\frac{\delta}{F+\delta}
\end{aligned}$$. ◻

</div>

<div style="color: red">

<div class="theorem">

**Theorem 2** (Total variation of the sampling distribution). *Let $\delta >0$ and $\sum_{i=1}^{n} \delta_i = \delta$, where $\delta_i \in [0, \delta]$. Suppose that we have the flow network $(G_n, F)$ which generates a target distribution $\pi^*$ uniform in the number of final vertices. Then if we increase the flow $F$ by $\delta$ in the same graph, that is $(G_n, F + \delta)$, generating a new target distribution $\pi$, the total variation distance between $\pi$ and $\pi^*$ is bounded above and below by the following $$\begin{aligned}
& ||\pi -\pi^*||_{\scaleto{\textbf{TV}}{3pt}} \leq \epsilon(\delta, n) \quad \text{where}
\\
& \epsilon(a,b) := \Big(1 - \frac{1}{b} \Big) \frac{a}{F+a}\,.
\end{aligned}$$*

</div>

</div>

# Inherent limitations of policy networks

<figure id="fig:wl_graphs">
<span class="image placeholder" data-original-image-src="gflownets_wl_test.pdf" data-original-image-title=""></span>
<figcaption>A state graph whose downstream distribution is not learnable by a GFlowNet with a policy network is parametrized by a 1-WL GNN.</figcaption>
</figure>

<div class="theorem">

**Theorem 3** (Distributional constraints of GFlowNets). *Let $\mathcal{G} = \{(\mathbf{X}, \mathbf{A}) \colon \mathbf{A} \in \{0,
    1\}^{N \times N}\}$ be the set of equally featured graphs with adjacency matrix $\mathbf{A}$ and features $\mathbf{r} \in \mathbb{R}^{d}$ ($\mathbf{X} = \mathbf{1}\mathbf{r}^{T} \in \mathbb{R}^{N \times d}$). Let $F_{\theta} \colon \mathcal{G} \rightarrow \Delta_{2}$ be the *policy network* that maps a graph $G \in \mathcal{G}$ to a point within the simplex of action-probabilities $\Delta_{2} =
    \{(a^{(1)}, a^{(2)}) \colon a^{(1)} + a^{(2)} = 1 \text{ and
    } a^{(1)}, a^{(2)} \ge 0\}$. See Figure . Suppose that the policy network is parametrized by an 1-WL GNN with parameters $\theta$. Let $\pi$ be a distribution on the graphs $\{G_{i} \colon i \in \{1, 2, 3, 4\}\}$ of Figure  with $\pi(G_{1}) = \pi(G_{2}) = \pi(G_{3}) =
    \frac{1}{6}$ and $\pi(G_{4}) = \frac{1}{2}$. In these settings, there does not exist a $\theta$ such that the downstream distribution induced by the policy network equals $\pi$.*

</div>

<div class="proof">

*Proof.* Let $p_{\theta}(X | S_{o})$ be the marginal transition probability learned by the GFlowNet of reaching the state $X
    \in \mathcal{G}$ through the generative process characterized by the state graph of Figure  and the policy network $F_{\theta}$. We will show that $p_{\theta}(G_{i} | S_{o})$ is – for any $\theta$ – necessarily different of $\pi(G_{i})$ for at least two graphs in $\pi$’s support.

For this, notice that the Markovity of the stochastic transitions learned by the GFlowNet entails that $p_{\theta}(G_{1} | S_{o}) = p_{\theta}(N_{1} | S_{o}) p_{\theta}(G_{1} | N_{1})$, $p_{\theta}(G_{2} | S_{o}) = p_{\theta}(N_{1} | S_{o}) p_{\theta}(G_{2} | N_{1})$, $p_{\theta}(G_{3} | S_{o}) = p_{\theta}(N_{2} | S_{o}) p_{\theta}(G_{3} | N_{2})$ and $p_{\theta}(G_{4} | S_{o}) = p_{\theta}(N_{2} | S_{o}) p_{\theta}(G_{4} | N_{2})$. Notably, the indistinguishability of the graphs $N_{1}$ and $N_{2}$ according to the 1-WL isomorphism test implies that $F_{\theta}(N_{1}) = F_{\theta}(N_{2})$ and hence the transition probabilities must satisfy $p_{\theta}(G_{1} | N_{1}) = p_{\theta}(G_{3} | N_{2})$ and $p_{\theta}(G_{2} | N_{1}) =
    p_{\theta}(G_{4} | N_{2})$.

Contradictorily, suppose that there is a $\theta$ such that the policy network $F_{\theta}$ is perfectly adjusted to the target distribution $\pi$. Hence, $p_{\theta}(G_{i} | S_{o}) = \pi(G_{i})$ for each $i \in \{1, 2, 3, 4\}$. Nonetheless, the representational equivalence of $N_{1}$ and $N_{2}$ and the Markovian assumption imply that

$$\begin{split} 
        p_{\theta}(N_{1} | S_{o}) = \frac{\pi(G_{1})}{p_{\theta} (G_{1} | N_{1})} 
        = \frac{\pi(G_{3})}{p_{\theta} (G_{3} | N_{2})} = p_{\theta}(N_{2} | S_{o}) 
        \text{ and that } \\ 
        p_{\theta}(N_{1} | S_{o}) = \frac{\pi(G_{2})}{p_{\theta}(G_{2} | N_{1})} \neq 
        \frac{\pi(G_{4})}{p_{\theta}(G_{4} | N_{2})} = p_{\theta}(N_{2} | S_{o}).
        % p_{\theta}(N_{1} | S_{o}) = 
    \end{split}$$

This contradiction guarantees that $p(G_{i} | S_{o})$ is necessarily different from $\pi(G_{i})$ for at least a pair of graphs and asseverates that the distribution characterized by the state graph of Figure  is unlearnable by a GFlowNet parametrized by a 1-WL GNN. ◻

</div>

<div class="remark">

**Remark 1**. The previous theorem states the limitations of a GFlowNet parametrized by a 1-WL GNN. The alternative use of a more expressive yet not permutationally invariant flow parametrization would entail a factorially large increase of the size of the state graph, as equivalent graphs with different labelling would be treated differently by the flow estimator, and lead to a computationally untractable problem. The next theorem characterizes a weak relationship between the size of the state graph and the statistical efficiency of a maximally entropic exploratory policy within the state graph.

</div>

### Author Contributions

If you’d like to, you may include a section for author contributions as is done in many journals. This is optional and at the discretion of the authors.

### Acknowledgments

Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.

# Appendix

You may include other additional sections here.

<div class="theorem">

**Theorem 4** (Total variation of the sampling distribution). *Let $(G_n, F)$ be a flow network which should generates a target distribution $\pi$ uniform in the number of final vertices. Suppose there exists an edge in $G_n$, that is $s \to s' \in \mathbb{A}$ such that $$F(s)P_{F}(s' | s) - F(s')P_{B}(s|s') = \delta \,,$$ where $\delta > 0$. Then we have that $(G_n, F)$ generates a probability distribution $\mu_{\delta}$ such that $$\begin{aligned}
& \frac{\delta(n - d)}{2n(F + \delta)} \le ||\mu_{\delta} -\pi||_{\scaleto{\textbf{TV}}{3pt}} \leq \frac{\delta(n + dn - d)}{2n(F + \delta)} \,,
\end{aligned}$$ where $d \in \{1,2, \dots, n-1\}$ is the number of final vertices that are descendants of $s'$.*

</div>

<div class="proof">

*Proof.* We define $D_{s'}$ as the set of final vertices that are descendants of $s'$. Thus we have $$\begin{aligned}
\label{eq:tv_mu_pi}
||\mu_{\delta} -\pi||_{\scaleto{\textbf{TV}}{3pt}} = \frac{1}{2} \sum_{i = 1}^n | \mu_{\delta} - n^{-1}|  = \frac{1}{2} \Big( \sum_{s'' \in D_{s'}} | \mu(s'') - n^{-1}| + \sum_{s'' \in D_{s'}^c} | \mu(s'') - n^{-1}| \Big)\,.
\end{aligned}$$

Now we will analyze it separately the two sum portions in  starting with the second one. For $s'' \in D_{s'}^c$ we have $\mu_{\delta} (s'') = F/n(F + \delta)$ then we obtain $$\begin{aligned}
\sum_{s'' \in D_{s'}^c} | \mu(s'') - n^{-1}| = (n - d) \Big| \frac{F}{n(F+\delta)} - \frac{1}{n} \Big| = \frac{\delta(n - d)}{n(F + \delta)} \,.   
\end{aligned}$$

We can rewrite $\delta$ as $\sum_{j = 1}^d \delta_j = \delta$ where $\delta_j \in [0, \delta]$ for every $j \in \{1, 2, \dots, d\}$. Now for $s_j \in D_{s'}$, where $j \in \{1,2, \dots, d\}$ we have $$\label{eq:mu_dj}
\mu_{\delta} (s_j) = \frac{Fn^{-1} + \delta_j}{F + \delta} = \frac{F + n\delta_j}{n(F+\delta)} \,.$$

Now for the first sum portion in  by  we obtain $$\begin{aligned}
\sum_{s'' \in D_{s'}} \Big| \mu(s'') - \frac{1}{n} \Big| = \sum_{j = 1}^d \Big| \frac{F+n\delta_j}{n(F+\delta)} - \frac{1}{n}  \Big| = \frac{1}{n(F+\delta)} \sum_{j = 1}^d |n\delta_j - \delta|\,.    
\end{aligned}$$

Since $|n(\delta/n) - \delta | \le |n\delta_j - \delta| \le |n\delta - \delta|$ for all $j \in \{1,2, \dots, d\}$ we have the desired result. ◻

</div>

<div class="theorem">

**Theorem 5** (Total variation of the sampling distribution 1 mode). *Let $\pi$ be a probability distribution with one mode, that is, $\pi(x_M) = R/n$ and $\pi(x) = (n- R)/n(n-1)$ where $1 < R < n$.*

</div>

<div class="proof">

*Proof.* We define $D_{s'}$ as the set of final vertices that are descendants of $s'$. Thus we have $$\begin{aligned}
\label{eq:tv_mu_pi_2}
||\mu_{\delta} -\pi||_{\scaleto{\textbf{TV}}{3pt}} = \frac{1}{2} \sum_{i = 1}^n | \mu_{\delta} - n^{-1}|  = \frac{1}{2} \Big( \sum_{s'' \in D_{s'}} | \mu(s'') - n^{-1}| + \sum_{s'' \in D_{s'}^c} | \mu(s'') - n^{-1}| \Big)\,.
\end{aligned}$$

First case $x_M \in D_{s'}$.

Now we will analyze separately the two sum portions in  starting with the second one. For $s'' \in D_{s'}^c$ we have $\mu_{\delta} (s'') = F(n-R)/n(n-1)(F + \delta)$ then we obtain $$\begin{aligned}
\sum_{s'' \in D_{s'}^c} \Big| \mu(s'') - \frac{(n-R)}{n(n-1)}\Big| & = (n - d) \Big| \frac{F(n-R)}{n(n-1)(F+\delta)} - \frac{(n-R)}{n(n-1)} \Big| 
\\
& = \frac{\delta(n - d)(n-R)}{n(n-1)(F + \delta)} \,. 
\end{aligned}$$

We can rewrite $\delta$ as $\sum_{j = 1}^d \delta_j = \delta$ where $\delta_j \in [0, \delta]$ for every $j \in \{1, 2, \dots, d\}$. Now for $s_j \in D_{s'}\backslash x_M$ we have $$\begin{aligned}
\label{eq:mu_dj_2}
\begin{split}
& \mu_{\delta} (s_j) = \frac{F(n-R) + n(n-1)\delta_j}{n(n-1)(F+\delta)} \quad \text{and}
\\
& \mu_{\delta}(x_M) = \frac{FR + n\delta_M}{n(F+\delta)}\,.
\end{split}
\end{aligned}$$

Now for the first sum portion in  by  we obtain $$\begin{aligned}
\label{eq:tv_mu_pi_2a}
\begin{split}
& \sum_{s'' \in D_{s'}} | \mu(s'') - \pi(s'')| = \Big| \frac{FR + n\delta_M}{n(F+\delta)} - \frac{R}{n} \Big| +   \sum_{j = 1}^{d-1} \Big| \frac{F(n-R) + n(n-1)\delta_j}{n(n-1)(F+\delta)} - \frac{(n-R)}{n(n-1)}  \Big| 
\\
& \le \Big| \frac{n\delta_M - R\delta}{n(F+\delta)} \Big| + \sum_{j=1}^{d-1} \frac{\delta_j (n-1)n + \delta(n - R)}{n(n-1)(F+\delta)}
\\
& \le \Big| \frac{n\delta_M - R\delta}{n(F+\delta)} \Big| + \frac{(\delta - \delta_M)(n-1)n + (d-1)\delta(n - R)}{n(n-1)(F+\delta)}\,.
\end{split}
\end{aligned}$$ In  we obtain the first inequality by triangle inequality.

If we have $R \ge n/2$ we obtain $$\begin{split}
& \sum_{s'' \in D_{s'}} | \mu(s'') - \pi(s'')| = \Big| \frac{n\delta_M - R\delta}{n(F+\delta)} \Big| + \frac{(\delta - \delta_M)(n-1)n + (d-1)\delta(n - R)}{n(n-1)(F+\delta)}\,.
\\
& \le \frac{R\delta}{n(F+\delta)} + \frac{n\delta(n-1) + \delta(d-1)(n-R)}{n(n-1)(F+\delta)} \le \frac{2\delta}{F + \delta} \,.
\end{split}$$

Otherwise if $R < n/2$ we have $$\begin{split}
& \sum_{s'' \in D_{s'}} | \mu(s'') - \pi(s'')| = \Big| \frac{n\delta_M - R\delta}{n(F+\delta)} \Big| + \frac{(\delta - \delta_M)(n-1)n + (d-1)\delta(n - R)}{n(n-1)(F+\delta)}\,.
\\
& \le \frac{\delta (n - R)}{n(F+\delta)} + \frac{n\delta(n-1) + \delta(d-1)(n-R)}{n(n-1)(F+\delta)} 
\\
& \le \frac{\delta(n - 1)(n - R) + n \delta(n-1) + \delta(d-1)(n-R)}{n(n-1)(F+\delta)} \le \frac{\delta(n - 2R)}{n(F + \delta)} \,.
\end{split}$$ The last inequality we obtain by the fact that $d < n$.

Second case $x_M \in D_{s'}^c$.

Now we will analyze separately the two sum portions in  starting with the second one. $$\label{eq_tv_mu_pi_Dsc} 
\begin{split}
\sum_{s'' \in D_{s'}^c} | \mu(s'') - \pi(s'')| & = \Big| \frac{FR}{n(F + \delta)} - \frac{R}{n} \Big| + (n - d - 1) \Big| \frac{F(n-R)}{n(n-1)(F+\delta)} - \frac{(n-R)}{n(n-1)} \Big| 
\\
& = \frac{R\delta}{n(F+\delta)} +  \frac{\delta(n - d - 1)(n-R)}{n(n-1)(F + \delta)} \,.
\end{split}$$

Then for the first sum portion in  we obtain $$\label{eq:tv_mu_pi_Ds}
\begin{split}
& \sum_{s'' \in D_{s'}} \Big| \mu(s'') - \frac{(n - R)}{n(n-1)} \Big| =  \sum_{j = 1}^{d} \Big| \frac{F(n-R) + n(n-1)\delta_j}{n(n-1)(F+\delta)} - \frac{(n-R)}{n(n-1)}  \Big| 
\\
& = \sum_{j=1}^{d} \frac{\delta_j (n-1)n + \delta(n - R)}{n(n-1)(F+\delta)} = \frac{\delta(n-1)n + d\delta(n - R)}{n(n-1)(F+\delta)} \,.
\end{split}$$

Hence by  and  we obtain the desired result. ◻

</div>

<div class="theorem">

**Theorem 6** (Total variation of the sampling distribution multiple $K$ modes). *Let $\pi$ be a probability distribution, such that $K$ states have the following distribution $\pi(x_M) = R/n$ and the others $\pi(x) = (n- KR)/n(n-K)$ where $1 < R < n$ and $K \ge 2$. Suppose that $b$ is the number of modes that are descendants of $s'$.*

*Then we have that $(G_n, F)$ generates a probability distribution $\mu_{\delta}$ such that $$\frac{\delta(2n^2- 2nK + 2dKR - 2dn + bn- Rbn - Rn + RK)}{2n(n-K)(F + \delta)}\le ||\mu_{\delta} -\pi||_{\scaleto{\textbf{TV}}{3pt}} \le \frac{\delta(2n^2 - K^2R - 2nK - nb + bKR)}{2n(n - K)(F + \delta)}\,.$$*

</div>

<div class="proof">

*Proof.* We define $D_{s'}$ as the set of final vertices that are descendants of $s'$ and $M$ as the set of states that are modes, hence $|M| = K$. Thus we have $$\begin{aligned}
\label{eq:tv_mu_pi_3}
\begin{split}
||\mu_{\delta} -\pi||_{\scaleto{\textbf{TV}}{3pt}} & = \frac{1}{2} \sum_{i = 1}^n | \mu_{\delta}(x_i) - \pi(x_i)|  
\\
& = \frac{1}{2} \Big( \sum_{s'' \in D_{s'}} | \mu(s'') - \pi(s'')| + \sum_{s'' \in D_{s'}^c} | \mu(s'') - \pi(s'')| \Big)\,.
\end{split}
\end{aligned}$$

Now we will analyze separately the two sum portions in  starting with the second one. For $s'' \in D_{s'}^c \backslash M$, we have $\mu_{\delta} (s'') = F(n-KR)/n(n-1)(F + \delta)$. For $s'' \in D_{s'}^c \cap M$ we have $\mu_{\delta}(s'') = FR/n(F + \delta)$. Then we obtain $$\label{eq:tv_mu_pi_5}
\begin{split}
& \sum_{s'' \in D_{s'}^c} | \mu(s'') - \pi(s'')|  = \sum_{s'' \in D_{s'}^c \cap M} | \mu(s'') - \pi(s'')| - \sum_{s'' \in D_{s'}^c \backslash M} | \mu(s'') - \pi(s'')|
\\
& = \sum_{j = 1}^{K - b} \Big| \frac{FR}{n(F+\delta)} - \frac{R}{n} \Big| + \sum_{j = 1}^{n -K - d} \Big| \frac{F(n-KR)}{n(n-K)(F+\delta)} - \frac{(n-KR)}{n(n-K)} \Big| 
\\
& = \frac{\delta R(K-b)}{n(F+\delta)} + \frac{(n- K - d)(n - KR)\delta}{n(n - K)(F+\delta)}\,.
\end{split}$$

We can rewrite $\delta$ as $\sum_{s'' \in D_{s'}} \delta_{s''} = \delta$ where $\delta_{s''} \in [0, \delta]$ for every $s'' \in D_{s'}$ and $|D_s'| = d$. Now for $s \in D_{s'}\backslash M$ and $x \in D_{s'} \cap M$ we have $$\begin{aligned}
\label{eq:mu_dj_3}
\begin{split}
& \mu_{\delta} (s) = \frac{F(n-KR) + n(n-K)\delta_s}{n(n-K)(F+\delta)} \quad \text{and}
\\
& \mu_{\delta}(x) = \frac{FR + n\delta_x}{n(F+\delta)}\,.
\end{split}
\end{aligned}$$

Now for the first sum portion in  by  we obtain $$\begin{aligned}
\label{eq:tv_mu_pi_3a}
\begin{split}
& \sum_{s'' \in D_{s'}} | \mu(s'') - \pi(s'')| = \sum_{s'' \in D_{s'} \cap M}| \mu(s'') - \pi(s'')| + \sum_{s'' \in D_{s'} \backslash M} | \mu(s'') - \pi(s'')|   
\\
& = \sum_{s'' \in D_{s'} \cap M} \Big| \frac{FR + n\delta_{s''}}{n(F+\delta)} - \frac{R}{n} \Big| +  \sum_{s'' \in D_{s'} \backslash M} \Big| \frac{F(n-R) + n(n-K)\delta_{s''}}{n(n-K)(F+\delta)} - \frac{(n-KR)}{n(n-K)}  \Big| 
\\
& \le \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big| + \sum_{s'' \in D_{s'} \backslash M} \frac{\delta_{s''} (n-K)n + \delta(n - KR)}{n(n-K)(F+\delta)} \,.
\end{split}
\end{aligned}$$ In  we obtain the first inequality by triangle inequality.

We define $\delta_M = \sum_{s'' \in D_{s'} \cap M} \delta_{s''}$, hence $\delta_M \in [0, \delta]$ and from  we have $$\label{eq:tv_mu_pi_4}
\begin{split}
& \sum_{s'' \in D_{s'}} | \mu(s'') - \pi(s'')|  \le  \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big| +  \frac{(\delta - \delta_M) (n-K)n + \delta(d-b)(n - KR)}{n(n-K)(F+\delta)} 
\\
& \le  \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big|  - \frac{\delta_M}{F + \delta} + \frac{\delta n(n-K) + \delta(d-b)(n - KR)}{n(n-K)(F+\delta)} 
\\
& \le \frac{b\delta R}{n(F+\delta)} + \frac{n \delta (n - K) + \delta(d-b)(n -KR)}{n(n-K)(F + \delta)} \,.
\end{split}$$

Note that, in order to derive the last inequality in , it suffices to show that $\sum_{s'' \in D_{s'} \cap M} |n\delta_{s''} - R\delta| - n \delta_M \le b\delta R$ for all $\delta_M \in [0, \delta]$. Then we have $$\sum_{s'' \in D_{s'} \cap M} |n\delta_{s''} - R\delta| \le \sum_{s'' \in D_{s'} \cap M} (n\delta_{s''} + R\delta) = n\delta_M + b\delta R\,.$$

Hence from  and  we obtain the upper bound.

Now we will prove lower-bound. From the second equality in  we obtain $$\label{eq:tv_mu_pi_6}
\begin{split}
& \sum_{s'' \in D_{s'}} |\mu(s'') - \pi(s'')| \ge \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big| + \sum_{s'' \in D_{s'} \backslash M} \frac{\delta_{s''} (n-K)n + \delta(KR - n)}{n(n-K)(F+\delta)}
\\
& \ge \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big| +  \frac{(\delta - \delta_M) (n-K)n + \delta(d-b)(KR - n)}{n(n-K)(F+\delta)} 
\\
& \ge \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big|  - \frac{\delta_M}{F + \delta} + \frac{\delta n(n-K) + \delta(d-b)(KR - n)}{n(n-K)(F+\delta)}
\\
& \ge \frac{-R\delta}{n(F+\delta)} + \frac{\delta n(n-K) + \delta(d-b)(KR - n)}{n(n-K)(F+\delta)} \,.
\end{split}$$

Note that, to obtain the last inequality in , it suffices to prove that $\sum_{s'' \in D_{s'} \cap M} |n \delta_{s''} - R\delta| - n\delta_M \ge - R\delta$ for all $\delta_M \in [0, \delta]$. Then we have $$\label{eq:tv_mu_pi_7}
\begin{split}
\sum_{s'' \in D_{s'} \cap M} |n \delta_{s''} - R\delta| - n\delta_M & \ge \Big| \sum_{s'' \in D_{s'} \cap M} n \delta_{s''} - R\delta \Big|  - n\delta_M
\\
& \ge |n\delta_M - R\delta| - n\delta_M \ge -R\delta \,. 
\end{split}$$

Then we have the lower-bound by  and . Hence we finished the proof. ◻

</div>

<div class="theorem">

**Theorem 7** (Total variation of the sampling distribution). *Let $(G_n, F)$ be a flow network which should generates a target distribution $\pi$ with support $X$. Suppose there exists an edge in $G_n$, that is $s \to s' \in \mathbb{A}$ such that the detailed balanced condition is broken and we have $$\label{eq:loss_log_1}
(\log(F(s)P_{F}(s' | s)) - \log(F(s')P_{B}(s|s')))^2 = \varepsilon \,,$$ where $\varepsilon \ge 0$. Then we have that $(G_n, F)$ generates a probability distribution $\mu_{\varepsilon}$. Let $d \in \{1,2, \dots, n-1\}$ be the number of final vertices that are descendants of $s'$ and $F(s' \to s) = F(s')P_B(s|s')$.*

- *If $\pi$ is uniform in the number of final vertices we have $$\begin{aligned}
      & \frac{(e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s)(n - d)}{n(F + (e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s))} \le ||\mu_{\varepsilon} -\pi||_{\scaleto{\textbf{TV}}{3pt}} \leq \frac{(e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s)(n - 1)}{n(F + (e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s))} \,,
      
  \end{aligned}$$*

- *If $\pi$ is a probability distribution and there exists a subset $M \subset X$ with $|M| = K$ (where $K \ge 1)$, then for each $x \in M$ we have $\pi(x_M) = R/n$ and for each $y \in X \backslash M$ we have $\pi(y) = (n- KR)/n(n-K)$ where $R < n$. Hence we obtain*

  *$$\begin{split}
       &||\mu_{\delta} -\pi||_{\scaleto{\textbf{TV}}{3pt}} \ge \frac{(e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s)(2n^2- 2nK + 2dKR - 2dn + bn- Rbn - Rn + RK)}{2n(n-K)(F + (e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s))} 
       \\
       & ||\mu_{\delta} -\pi||_{\scaleto{\textbf{TV}}{3pt}} \le \frac{(e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s)(2n^2 - K^2R - 2nK - nb + bKR)}{2n(n - K)(F + (e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s))}\,.
      \end{split}$$*

</div>

<div class="proof">

*Proof.* First, we define $D_{s'}$ as the set of final vertices that are descendants of $s'$ and by  we obtain $F(s \to s') = (e^{\varepsilon^{\frac{1}{2}}} - 1)F(s' \to s)$. Thus, we have that the detailed balance condition is broken in the following way: $$\label{eq:ep_var}
\begin{split}
& F(s)P_F (s'|s) = F(s')P_B (s|s') + \delta \quad \text{where}
\\
& \delta = (e^{\varepsilon^{\frac{1}{2}}} - 1)F(s \to s') \,.
\end{split}$$ Hence $\delta$ is a positive constant.

Now we start the proof with part $i)$. Thus we have $$\begin{aligned}
\label{eq:tv_mu_pi}
||\mu_{\varepsilon} -\pi||_{\scaleto{\textbf{TV}}{3pt}} = \frac{1}{2} \sum_{i = 1}^n | \mu_{\varepsilon} - n^{-1}|  = \frac{1}{2} \Big( \sum_{s'' \in D_{s'}} | \mu_{\varepsilon}(s'') - n^{-1}| + \sum_{s'' \in D_{s'}^c} | \mu_{\varepsilon}(s'') - n^{-1}| \Big)\,.
\end{aligned}$$

Now we will analyze it separately the two sum portions in  starting with the second one. For $s'' \in D_{s'}^c$ we have $\mu_{\varepsilon} (s'') = F/n(F + \delta)$ then we obtain $$\begin{aligned}
\label{eq:D_s^c}
\sum_{s'' \in D_{s'}^c} | \mu_{\varepsilon}(s'') - n^{-1}| = (n - d) \Big| \frac{F}{n(F+\delta)} - \frac{1}{n} \Big| = \frac{\delta(n - d)}{n(F + \delta)} \,.   
\end{aligned}$$

We can rewrite $\delta$ as $\sum_{j = 1}^d \delta_j = \delta$ where $\delta_j \in [0, \delta]$ for every $j \in \{1, 2, \dots, d\}$. Now for $s_j \in D_{s'}$, where $j \in \{1,2, \dots, d\}$ we have $$\label{eq:mu_dj}
\mu_{\varepsilon} (s_j) = \frac{Fn^{-1} + \delta_j}{F + \delta} = \frac{F + n\delta_j}{n(F+\delta)} \,.$$

Now for the first sum portion in  by  we obtain $$\label{eq:lower}
\begin{split}
\sum_{s'' \in D_{s'}} \Big| \mu_{\varepsilon}(s'') - \frac{1}{n} \Big| & = \sum_{j = 1}^d \Big| \frac{F+n\delta_j}{n(F+\delta)} - \frac{1}{n}  \Big| = \frac{1}{n(F+\delta)} \sum_{j = 1}^d |n\delta_j - \delta|
\\
& \ge \frac{1}{n(F+\delta)} \sum_{j = 1}^d n\delta_j - \delta = \frac{\delta (n -d)}{n(F + \delta)} \,.
\end{split}$$ Important to notice that the lower bound that we compute in  is the same that we have $\delta_j = \delta/d$ for all $j \in \{1, 2, \dots, d \}$.

Now for the upper bound from  and  we obtain $$\label{eq:upper}
\begin{split}
& \sum_{s'' \in D_{s'}} \Big| \mu_{\varepsilon}(s'') - \frac{1}{n} \Big| =  \frac{1}{n(F+\delta)} \sum_{j = 1}^d |n\delta_j - \delta| = \frac{1}{n(F+\delta)} \sum_{j = 1}^d | n\delta_j - \delta_j + \delta_j -  \delta|
\\
& \le  \frac{1}{n(F+\delta)} \sum_{j = 1}^d |n\delta_j - \delta_j| + | \delta_j - \delta| =   \frac{1}{n(F+\delta)} \sum_{j = 1}^d (\delta_j (n-1) + \delta - \delta_j ) \le \frac{\delta(n + d - 2)}{n(F+\delta)} \,.
\end{split}$$ Important to notice that the upper bound that we compute in  is the same that we have $\delta_j = \delta$ for one $j \in \{1, 2, \dots, d \}$ and $\delta_i = 0$ for all $i \in \{1,2, \dots, d \} \backslash \{j\}$.

Hence, from , , ,  and  we obtain the desired result for part $i)$.

Now we will prove part $ii)$.

As in the proof of part $i)$ we will analyze separately the two sum portions in  starting with the second one. We set $|D_{s'}^c \cap M| = b$. For $s'' \in D_{s'}^c \backslash M$, we have $\mu_{\varepsilon} (s'') = F(n-KR)/n(n-1)(F + \delta)$. For $s'' \in D_{s'}^c \cap M$ we have $\mu_{\varepsilon}(s'') = FR/n(F + \delta)$. Then we obtain $$\label{eq:tv_mu_pi_5}
\begin{split}
& \sum_{s'' \in D_{s'}^c} | \mu_{\varepsilon}(s'') - \pi(s'')|  = \sum_{s'' \in D_{s'}^c \cap M} | \mu_{\varepsilon}(s'') - \pi(s'')| - \sum_{s'' \in D_{s'}^c \backslash M} | \mu_{\varepsilon}(s'') - \pi(s'')|
\\
& = \sum_{j = 1}^{K - b} \Big| \frac{FR}{n(F+\delta)} - \frac{R}{n} \Big| + \sum_{j = 1}^{n -K - d} \Big| \frac{F(n-KR)}{n(n-K)(F+\delta)} - \frac{(n-KR)}{n(n-K)} \Big| 
\\
& = \frac{\delta R(K-b)}{n(F+\delta)} + \frac{(n- K - d)(n - KR)\delta}{n(n - K)(F+\delta)}\,.
\end{split}$$

We can rewrite $\delta$ as $\sum_{s'' \in D_{s'}} \delta_{s''} = \delta$ where $\delta_{s''} \in [0, \delta]$ for every $s'' \in D_{s'}$ and $|D_s'| = d$. Now for $s \in D_{s'}\backslash M$ and $x \in D_{s'} \cap M$ we have $$\begin{aligned}
\label{eq:mu_dj_3}
\begin{split}
& \mu_{\varepsilon} (s) = \frac{F(n-KR) + n(n-K)\delta_s}{n(n-K)(F+\delta)} \quad \text{and}
\\
& \mu_{\varepsilon}(x) = \frac{FR + n\delta_x}{n(F+\delta)}\,.
\end{split}
\end{aligned}$$

Now for the first sum portion in  by  we obtain $$\begin{aligned}
\label{eq:tv_mu_pi_3a}
\begin{split}
& \sum_{s'' \in D_{s'}} | \mu_{\varepsilon}(s'') - \pi(s'')| = \sum_{s'' \in D_{s'} \cap M}| \mu_{\varepsilon}(s'') - \pi(s'')| + \sum_{s'' \in D_{s'} \backslash M} | \mu_{\varepsilon}(s'') - \pi(s'')|   
\\
& = \sum_{s'' \in D_{s'} \cap M} \Big| \frac{FR + n\delta_{s''}}{n(F+\delta)} - \frac{R}{n} \Big| +  \sum_{s'' \in D_{s'} \backslash M} \Big| \frac{F(n-R) + n(n-K)\delta_{s''}}{n(n-K)(F+\delta)} - \frac{(n-KR)}{n(n-K)}  \Big| 
\\
& \le \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big| + \sum_{s'' \in D_{s'} \backslash M} \frac{\delta_{s''} (n-K)n + \delta(n - KR)}{n(n-K)(F+\delta)} \,.
\end{split}
\end{aligned}$$ In  we obtain the first inequality by triangle inequality.

We define $\delta_M = \sum_{s'' \in D_{s'} \cap M} \delta_{s''}$, hence $\delta_M \in [0, \delta]$ and from  we have $$\label{eq:tv_mu_pi_4}
\begin{split}
& \sum_{s'' \in D_{s'}} | \mu_{\varepsilon}(s'') - \pi(s'')|  \le  \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big| +  \frac{(\delta - \delta_M) (n-K)n + \delta(d-b)(n - KR)}{n(n-K)(F+\delta)} 
\\
& \le  \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big|  - \frac{\delta_M}{F + \delta} + \frac{\delta n(n-K) + \delta(d-b)(n - KR)}{n(n-K)(F+\delta)} 
\\
& \le \frac{b\delta R}{n(F+\delta)} + \frac{n \delta (n - K) + \delta(d-b)(n -KR)}{n(n-K)(F + \delta)} \,.
\end{split}$$

Note that, in order to derive the last inequality in , it suffices to show that $\sum_{s'' \in D_{s'} \cap M} |n\delta_{s''} - R\delta| - n \delta_M \le b\delta R$ for all $\delta_M \in [0, \delta]$. Then we have $$\sum_{s'' \in D_{s'} \cap M} |n\delta_{s''} - R\delta| \le \sum_{s'' \in D_{s'} \cap M} (n\delta_{s''} + R\delta) = n\delta_M + b\delta R\,.$$

Hence from ,   and  we obtain the upper bound.

Now we will prove lower-bound. From the second equality in  we obtain $$\label{eq:tv_mu_pi_6}
\begin{split}
& \sum_{s'' \in D_{s'}} |\mu_{\varepsilon}(s'') - \pi(s'')| \ge \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big| + \sum_{s'' \in D_{s'} \backslash M} \frac{\delta_{s''} (n-K)n + \delta(KR - n)}{n(n-K)(F+\delta)}
\\
& \ge \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big| +  \frac{(\delta - \delta_M) (n-K)n + \delta(d-b)(KR - n)}{n(n-K)(F+\delta)} 
\\
& \ge \sum_{s'' \in D_{s'} \cap M} \Big| \frac{n\delta_{s''} - R\delta}{n(F+\delta)} \Big|  - \frac{\delta_M}{F + \delta} + \frac{\delta n(n-K) + \delta(d-b)(KR - n)}{n(n-K)(F+\delta)}
\\
& \ge \frac{-R\delta}{n(F+\delta)} + \frac{\delta n(n-K) + \delta(d-b)(KR - n)}{n(n-K)(F+\delta)} \,.
\end{split}$$

Note that, to obtain the last inequality in , it suffices to prove that $\sum_{s'' \in D_{s'} \cap M} |n \delta_{s''} - R\delta| - n\delta_M \ge - R\delta$ for all $\delta_M \in [0, \delta]$. Then we have $$\begin{split}
\sum_{s'' \in D_{s'} \cap M} |n \delta_{s''} - R\delta| - n\delta_M & \ge \Big| \sum_{s'' \in D_{s'} \cap M} n \delta_{s''} - R\delta \Big|  - n\delta_M
\\
& \ge |n\delta_M - R\delta| - n\delta_M \ge -R\delta \,. 
\end{split}$$

Then we have the lower-bound by ,   and . Hence we finished the proof. ◻

</div>

[^1]: Use footnote for providing further information about author (webpage, alternative address)—*not* for acknowledging funding agencies. Funding acknowledgements go at the end of the paper.
