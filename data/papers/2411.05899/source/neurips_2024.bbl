\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akhound-Sadegh et~al.(2024)Akhound-Sadegh, Rector-Brooks, Bose, Mittal, Lemos, Liu, Sendera, Ravanbakhsh, Gidel, Bengio, Malkin, and Tong]{akhound2024iterated}
T.~Akhound-Sadegh, J.~Rector-Brooks, A.~J. Bose, S.~Mittal, P.~Lemos, C.-H. Liu, M.~Sendera, S.~Ravanbakhsh, G.~Gidel, Y.~Bengio, N.~Malkin, and A.~Tong.
\newblock Iterated denoising energy matching for sampling from {Boltzmann} densities.
\newblock \emph{arXiv preprint arxiv:2402.06121}, 2024.

\bibitem[Bengio et~al.(2021)Bengio, Jain, Korablyov, Precup, and Bengio]{Bengio2021}
E.~Bengio, M.~Jain, M.~Korablyov, D.~Precup, and Y.~Bengio.
\newblock Flow network based generative models for non-iterative diverse candidate generation.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 2021.

\bibitem[Bengio et~al.(2023)Bengio, Lahlou, Deleu, Hu, Tiwari, and Bengio]{Foundations}
Y.~Bengio, S.~Lahlou, T.~Deleu, E.~J. Hu, M.~Tiwari, and E.~Bengio.
\newblock {GFlowNet} foundations.
\newblock \emph{Journal of Machine Learning Research ({JMLR})}, 24\penalty0 (210), 2023.

\bibitem[Berner et~al.(2024)Berner, Richter, and Ullrich]{berner2022optimal}
J.~Berner, L.~Richter, and K.~Ullrich.
\newblock An optimal control perspective on diffusion-based generative modeling.
\newblock \emph{arXiv preprint arxiv:2211.01364}, 2024.

\bibitem[Bissiri et~al.(2016)Bissiri, Holmes, and Walker]{Bissiri2016}
P.~G. Bissiri, C.~Holmes, and S.~Walker.
\newblock A general framework for updating belief distributions.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Methodological)}, 78\penalty0 (5), 2016.

\bibitem[Broderick et~al.(2013)Broderick, Boyd, Wibisono, Wilson, and Jordan]{Broderick13}
T.~Broderick, N.~Boyd, A.~Wibisono, A.~C. Wilson, and M.~I. Jordan.
\newblock Streaming variational {Bayes}.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 2013.

\bibitem[Bui et~al.(2017)Bui, Nguyen, and Turner]{bui2017GPs}
T.~D. Bui, C.~Nguyen, and R.~E. Turner.
\newblock Streaming sparse {Gaussian} process approximations.
\newblock \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 2017.

\bibitem[Cole(1993)]{Cole1993}
T.~J. Cole.
\newblock Algorithm {AS} 281: Scaling and rounding regression coefficients to integers.
\newblock \emph{Applied Statistics}, 42\penalty0 (1), 1993.

\bibitem[Cranmer et~al.(2020)Cranmer, Brehmer, and Louppe]{cranmer2020frontier}
K.~Cranmer, J.~Brehmer, and G.~Louppe.
\newblock The frontier of simulation-based inference.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (48), 2020.

\bibitem[Csiszár and Körner(2011)]{csiszar2011information}
I.~Csiszár and J.~Körner.
\newblock \emph{Information theory: coding theorems for discrete memoryless systems}.
\newblock Cambridge University Press, 2011.

\bibitem[Deleu et~al.(2022)Deleu, Góis, Emezue, Rankawat, Lacoste-Julien, Bauer, and Bengio]{deleu2022bayesian}
T.~Deleu, A.~Góis, C.~C. Emezue, M.~Rankawat, S.~Lacoste-Julien, S.~Bauer, and Y.~Bengio.
\newblock {Bayesian} structure learning with generative flow networks.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence ({UAI})}, 2022.

\bibitem[Deleu et~al.(2023)Deleu, Nishikawa-Toomey, Subramanian, Malkin, Charlin, and Bengio]{deleu2023joint}
T.~Deleu, M.~Nishikawa-Toomey, J.~Subramanian, N.~Malkin, L.~Charlin, and Y.~Bengio.
\newblock Joint {Bayesian} inference of graphical structure and parameters with a single generative flow network.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 2023.

\bibitem[Dinh et~al.(2017)Dinh, Darling, and Matsen]{Dinh2017}
V.~Dinh, A.~E. Darling, and F.~A. Matsen, IV.
\newblock Online {Bayesian} phylogenetic inference: Theoretical foundations via sequential {Monte} {Carlo}.
\newblock \emph{Systematic Biology}, 67\penalty0 (3), 2017.

\bibitem[Fang et~al.(2021)Fang, Wang, Pan, Liu, and Zhe]{Fang21Tensor}
S.~Fang, Z.~Wang, Z.~Pan, J.~Liu, and S.~Zhe.
\newblock Streaming {Bayesian} deep tensor factorization.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2021.

\bibitem[Felsenstein(1981)]{Felsenstein1981}
J.~Felsenstein.
\newblock Evolutionary trees from {DNA} sequences: A maximum likelihood approach.
\newblock \emph{Journal of Molecular Evolution}, 17, 1981.

\bibitem[Flajolet and Sedgewick(2009)]{Flajolet2009}
P.~Flajolet and R.~Sedgewick.
\newblock \emph{Analytic Combinatorics}.
\newblock Cambridge University Press, 1 edition, 2009.

\bibitem[Garipov et~al.(2023)Garipov, Peuter, Yang, Garg, Kaski, and Jaakkola]{garipov2023compositional}
T.~Garipov, S.~D. Peuter, G.~Yang, V.~Garg, S.~Kaski, and T.~S. Jaakkola.
\newblock Compositional sculpting of iterative generative processes.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 2023.

\bibitem[González et~al.(2017)González, Dai, Damianou, and Lawrence]{gonzalez2017preferential}
J.~González, Z.~Dai, A.~Damianou, and N.~D. Lawrence.
\newblock Preferential {Bayesian} optimization.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2017.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Mirza, Da, Courville, and Bengio]{Goodfellow2014}
I.~J. Goodfellow, M.~Mirza, X.~Da, A.~C. Courville, and Y.~Bengio.
\newblock An empirical investigation of catastrophic forgeting in gradient-based neural networks.
\newblock In \emph{International Conference on Learning Representations ({ICLR})}, 2014.

\bibitem[Han et~al.(2020)Han, Ding, Liu, Torresani, Peng, and Liu]{han2020stein}
J.~Han, F.~Ding, X.~Liu, L.~Torresani, J.~Peng, and Q.~Liu.
\newblock Stein variational inference for discrete distributions.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics ({AISTATS})}, 2020.

\bibitem[Hornberger et~al.(1995)Hornberger, Habraken, and Bloch]{Hornberger1995}
J.~C. Hornberger, H.~Habraken, and D.~A. Bloch.
\newblock Minimum data needed on patient preferences for accurate, efficient medical decision making.
\newblock \emph{Medical Care}, 33\penalty0 (3), 1995.

\bibitem[Hu et~al.(2023{\natexlab{a}})Hu, Jain, Elmoznino, Kaddar, Lajoie, Bengio, and Malkin]{hu2023amortizing}
E.~J. Hu, M.~Jain, E.~Elmoznino, Y.~Kaddar, G.~Lajoie, Y.~Bengio, and N.~Malkin.
\newblock Amortizing intractable inference in large language models.
\newblock \emph{arXiv preprint arxiv:2310.04363}, 2023{\natexlab{a}}.

\bibitem[Hu et~al.(2023{\natexlab{b}})Hu, Malkin, Jain, Everett, Graikos, and Bengio]{discretegfn_ii}
E.~J. Hu, N.~Malkin, M.~Jain, K.~E. Everett, A.~Graikos, and Y.~Bengio.
\newblock {GFlowNet-EM} for learning compositional latent variable models.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2023{\natexlab{b}}.

\bibitem[Jain et~al.(2023)Jain, Raparthy, Hernandez-Garcia, Rector-Brooks, Bengio, Miret, and Bengio]{mogfn}
M.~Jain, S.~C. Raparthy, A.~Hernandez-Garcia, J.~Rector-Brooks, Y.~Bengio, S.~Miret, and E.~Bengio.
\newblock Multi-objective {GFlowNet}s.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2023.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2017categorical}
E.~Jang, S.~Gu, and B.~Poole.
\newblock Categorical reparameterization with {Gumbel}-softmax.
\newblock In \emph{International Conference on Learning Representations ({ICLR})}, 2017.

\bibitem[Jukes and Cantor(1969)]{Jukes1969}
T.~H. Jukes and C.~R. Cantor.
\newblock Evolution of protein molecules.
\newblock In \emph{Mammalian Protein Metabolism}. Academic Press, 1969.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arxiv:1412.6980}, 2014.

\bibitem[Knoblauch et~al.(2022)Knoblauch, Jewson, and Damoulas]{Knoblauch2022}
J.~Knoblauch, J.~Jewson, and T.~Damoulas.
\newblock An optimization-centric view on {Bayes'} rule: Reviewing and generalizing variational inference.
\newblock \emph{Journal of Machine Learning Research ({JMLR})}, 23\penalty0 (132), 2022.

\bibitem[Kviman et~al.(2023)Kviman, Molén, and Lagergren]{kviman2024improved}
O.~Kviman, R.~Molén, and J.~Lagergren.
\newblock Improved variational {Bayesian} phylogenetic inference using mixtures.
\newblock \emph{arXiv preprint arxiv:2310.00941}, 2023.

\bibitem[Lahlou et~al.(2023)Lahlou, Deleu, Lemos, Zhang, Volokhova, Hernández-García, Ezzine, Bengio, and Malkin]{theory}
S.~Lahlou, T.~Deleu, P.~Lemos, D.~Zhang, A.~Volokhova, A.~Hernández-García, L.~N. Ezzine, Y.~Bengio, and N.~Malkin.
\newblock A theory of continuous generative flow networks.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2023.

\bibitem[Lau et~al.(2023)Lau, Vemgal, Precup, and Bengio]{lau2023dgfn}
E.~Lau, N.~M. Vemgal, D.~Precup, and E.~Bengio.
\newblock {DGFN}: Double generative flow networks.
\newblock In \emph{{NeurIPS} 2023 Generative AI and Biology ({GenBio}) Workshop}, 2023.

\bibitem[Li et~al.(2023)Li, Marinescu, and Musslick]{li2023gfnsr}
S.~Li, I.~Marinescu, and S.~Musslick.
\newblock Gfn-sr: Symbolic regression with generative flow networks.
\newblock \emph{arXiv preprint arxiv:2312.00396}, 2023.

\bibitem[Lindley(1972)]{lindley1972bayesian}
D.~V. Lindley.
\newblock \emph{Bayesian statistics: A review}.
\newblock SIAM, 1972.

\bibitem[Liu et~al.(2023)Liu, Jain, Dossou, Shen, Lahlou, Goyal, Malkin, Emezue, Zhang, Hassen, Ji, Kawaguchi, and Bengio]{liu2023dropout}
D.~Liu, M.~Jain, B.~F.~P. Dossou, Q.~Shen, S.~Lahlou, A.~Goyal, N.~Malkin, C.~C. Emezue, D.~Zhang, N.~Hassen, X.~Ji, K.~Kawaguchi, and Y.~Bengio.
\newblock {GFlowOut}: Dropout with generative flow networks.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2023.

\bibitem[Madan et~al.(2022)Madan, Rector-Brooks, Korablyov, Bengio, Jain, Nica, Bosc, Bengio, and Malkin]{Madan2022LearningGF}
K.~Madan, J.~Rector-Brooks, M.~Korablyov, E.~Bengio, M.~Jain, A.~C. Nica, T.~Bosc, Y.~Bengio, and N.~Malkin.
\newblock Learning {GFlowNets} from partial episodes for improved convergence and stability.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2022.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{maddison2017the}
C.~J. Maddison, A.~Mnih, and Y.~W. Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random variables.
\newblock In \emph{International Conference on Learning Representations ({ICLR})}, 2017.

\bibitem[Malkin et~al.(2022)Malkin, Jain, Bengio, Sun, and Bengio]{malkin2022trajectory}
N.~Malkin, M.~Jain, E.~Bengio, C.~Sun, and Y.~Bengio.
\newblock Trajectory balance: Improved credit assignment in {GFlowNets}.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 2022.

\bibitem[Malkin et~al.(2023)Malkin, Lahlou, Deleu, Ji, Hu, Everett, Zhang, and Bengio]{malkin2023gflownets}
N.~Malkin, S.~Lahlou, T.~Deleu, X.~Ji, E.~Hu, K.~Everett, D.~Zhang, and Y.~Bengio.
\newblock {GFlowNets} and variational inference.
\newblock \emph{International Conference on Learning Representations ({ICLR})}, 2023.

\bibitem[McCloskey and Cohen(1989)]{McCloskey1989}
M.~McCloskey and N.~J. Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential learning problem.
\newblock In \emph{Psychology of Learning and Motivation}, volume~24. Academic Press, 1989.

\bibitem[Mittal et~al.(2023)Mittal, Bracher, Lajoie, Jaini, and Brubaker]{mittal2023exploring}
S.~Mittal, N.~L. Bracher, G.~Lajoie, P.~Jaini, and M.~A. Brubaker.
\newblock Exploring exchangeable dataset amortization for {Bayesian} posterior inference.
\newblock In \emph{{ICML} 2023 Workshop on Structured Probabilistic Inference / Generative Modeling}, 2023.

\bibitem[Mnih and Rezende(2016)]{mnih2016variational}
A.~Mnih and D.~Rezende.
\newblock Variational inference for {Monte} {Carlo} objectives.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2016.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and Mnih]{DBLP:journals/jmlr/MohamedRFM20}
S.~Mohamed, M.~Rosca, M.~Figurnov, and A.~Mnih.
\newblock {Monte} {Carlo} gradient estimation in machine learning.
\newblock \emph{Journal of Machine Learning Research ({JMLR})}, 21\penalty0 (132), 2020.

\bibitem[Newman(2018)]{newman}
M.~Newman.
\newblock \emph{Networks}.
\newblock Oxford University Press, 2018.

\bibitem[Pan et~al.(2023{\natexlab{a}})Pan, Malkin, Zhang, and Bengio]{LingTrajectory}
L.~Pan, N.~Malkin, D.~Zhang, and Y.~Bengio.
\newblock Better training of {GFlowNets} with local credit and incomplete trajectories.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2023{\natexlab{a}}.

\bibitem[Pan et~al.(2023{\natexlab{b}})Pan, Zhang, Jain, Huang, and Bengio]{stochastic}
L.~Pan, D.~Zhang, M.~Jain, L.~Huang, and Y.~Bengio.
\newblock Stochastic generative flow networks.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence ({UAI})}, 2023{\natexlab{b}}.

\bibitem[Papini et~al.(2018)Papini, Binaghi, Canonaco, Pirotta, and Restelli]{papini18pg}
M.~Papini, D.~Binaghi, G.~Canonaco, M.~Pirotta, and M.~Restelli.
\newblock Stochastic variance-reduced policy gradient.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{Paszke_PyTorch_An_Imperative_2019}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and S.~Chintala.
\newblock {PyTorch}: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 2019.

\bibitem[Richter and Berner(2023)]{richter2023improved}
L.~Richter and J.~Berner.
\newblock Improved sampling via learned diffusions.
\newblock \emph{arXiv preprint arxiv:2307.01198}, 2023.

\bibitem[Richter et~al.(2020)Richter, Boustati, N{\"u}sken, Ruiz, and Akyildiz]{richter2020vargrad}
L.~Richter, A.~Boustati, N.~N{\"u}sken, F.~Ruiz, and O.~D. Akyildiz.
\newblock Vargrad: a low-variance gradient estimator for variational inference.
\newblock \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 33, 2020.

\bibitem[RoyChoudhury et~al.(2015)RoyChoudhury, Willis, and Bunge]{roychoudhury2015}
A.~RoyChoudhury, A.~Willis, and J.~Bunge.
\newblock Consistency of a phylogenetic tree maximum likelihood estimator.
\newblock \emph{Journal of Statistical Planning and Inference}, 161, 2015.

\bibitem[Schaeffer et~al.(2022)Schaeffer, Du, Liu, and Fiete]{Schaeffer22features}
R.~Schaeffer, Y.~Du, G.~K. Liu, and I.~Fiete.
\newblock Streaming inference for infinite feature models.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2022.

\bibitem[Sendera et~al.(2024)Sendera, Kim, Mittal, Lemos, Scimeca, Rector-Brooks, Adam, Bengio, and Malkin]{sendera2024diffusion}
M.~Sendera, M.~Kim, S.~Mittal, P.~Lemos, L.~Scimeca, J.~Rector-Brooks, A.~Adam, Y.~Bengio, and N.~Malkin.
\newblock On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling.
\newblock \emph{arXiv preprint arxiv:2307.01198}, 2024.

\bibitem[Shen et~al.(2023)Shen, Bengio, Hajiramezanali, Loukas, Cho, and Biancalani]{shen23gflownets}
M.~W. Shen, E.~Bengio, E.~Hajiramezanali, A.~Loukas, K.~Cho, and T.~Biancalani.
\newblock Towards understanding and improving {GFlowNet} training.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2023.

\bibitem[Walker(1969)]{Walker1969}
A.~M. Walker.
\newblock On the asymptotic behaviour of posterior distributions.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Methodological)}, 31\penalty0 (1), 1969.

\bibitem[Wojnowicz et~al.(2022)Wojnowicz, Aeron, Miller, and Hughes]{wojnowicz2022easy}
M.~T. Wojnowicz, S.~Aeron, E.~L. Miller, and M.~Hughes.
\newblock Easy variational inference for categorical models via an independent binary approximation.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2022.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka.
\newblock How powerful are graph neural networks?
\newblock \emph{International Conference on Learning Representations ({ICLR})}, 2019.

\bibitem[Xu et~al.(2020)Xu, Gao, and Gu]{xu20pg}
P.~Xu, F.~Gao, and Q.~Gu.
\newblock An improved convergence analysis of stochastic variance-reduced policy gradient.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence ({UAI})}, 2020.

\bibitem[Yang(2014)]{Yang2014}
Z.~Yang.
\newblock \emph{Molecular Evolution: A Statistical Approach}.
\newblock Oxford University Press, 2014.

\bibitem[Zhang and Matsen(2019)]{zhang2018variational}
C.~Zhang and F.~A. Matsen, IV.
\newblock Variational {Bayesian} phylogenetic inference.
\newblock In \emph{International Conference on Learning Representations ({ICLR})}, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Malkin, Liu, Volokhova, Courville, and Bengio]{discretegfn_iii}
D.~Zhang, N.~Malkin, Z.~Liu, A.~Volokhova, A.~Courville, and Y.~Bengio.
\newblock Generative flow networks for discrete probabilistic modeling.
\newblock In \emph{International Conference on Machine Learning ({ICML})}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Dai, Malkin, Courville, Bengio, and Pan]{Zhang2023}
D.~Zhang, H.~Dai, N.~Malkin, A.~Courville, Y.~Bengio, and L.~Pan.
\newblock Let the flows tell: Solving graph combinatorial optimization problems with {GFlowNets}.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, 2023.

\bibitem[Zhao et~al.(2023)Zhao, Nassar, Jordan, Bugallo, and Park]{Zhao2023}
Y.~Zhao, J.~Nassar, I.~Jordan, M.~Bugallo, and I.~M. Park.
\newblock Streaming variational {Monte} {Carlo}.
\newblock \emph{{IEEE} Transactions on Pattern Analysis and Machine Intelligence}, 45\penalty0 (1), 2023.

\bibitem[Zhou et~al.(2024)Zhou, Yan, Layne, Malkin, Zhang, Jain, Blanchette, and Bengio]{zhou2024phylogfn}
M.~Y. Zhou, Z.~Yan, E.~Layne, N.~Malkin, D.~Zhang, M.~Jain, M.~Blanchette, and Y.~Bengio.
\newblock Phylo{GFN}: Phylogenetic inference with generative flow networks.
\newblock In \emph{International Conference on Learning Representations ({ICLR})}, 2024.

\end{thebibliography}
