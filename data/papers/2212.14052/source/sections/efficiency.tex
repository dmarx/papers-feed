\section{\fastfft Evaluation \label{sec:eval_efficiency}}
We evaluate how well \fastfft speeds up SSMs.
\fastfft sets state-of-the-art performance on the long range arena benchmark~\citep{tay2020long} using S4~\citep{gu2022efficiently}.
We report performance of training \hthree module with \fastfft compared to attention at various sequence lengths, from 256 to 32K and demonstrate nearly linear scaling.

\begin{table}[t]
    \centering
    \input{tables/lra}
\end{table}

\paragraph{Long Range Arena}
The Long Range Arena (LRA) benchmark~\citep{tay2020long} is a benchmark for long-range sequence modeling.
The state-of-the-art approach, S4~\citep{gu2022train}, is an SSM.
Table~\ref{table:lra} shows that \fastfft accelerates S4 by 2$\times$, outperforming Transformers by 5.8$\times$.

\input{figs/efficiency.tex}
\paragraph{Benchmark \hthree Against Attention}
We benchmark the time to run the forward and backward pass of \hthree with \fastfft against attention.
\fastfft maintains nearly linear scaling, even to very long sequence lengths.
\cref{fig:fftconv_speed} shows overall 2-3$\times$ speedup over FFTConv with cuFFT using our techniques
(block FFT, state-passing).
Simple kernel fusion (even without block FFT) can yield speedup over cuFFT for short sequences, since memory reads/writes are the bottleneck for short sequences.
For long sequences, SSMs using state passing can be dozens of times faster
than even the fastest attention implementation.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
