%!TEX root = ../main.tex

\section{Conclusion}
\label{sec:conc}

Our main goal is to understand and narrow the gap between attention and SSMs in
language modeling in terms of modeling capabilities and hardware efficiency.
Our exploration based on synthetic language tasks motivated us to design the
\hthree layer, which is surprisingly competitive with attention.
Our \textsc{BlockFFTConv} algorithm exploits matrix multiplication units and the
dual recurrent--convolution view of SSMs to substantially speed up SSMs, reducing
the hardware barrier between attention and SSMs.
We are excited about several future directions.
Our \hthree layer is a simple combination of two SSMs, and more
sophisticated designs could be more expressive.
Our encouraging results on language models up to 1.3B parameters suggests that
scaling SSMs to larger sizes is a promising avenue.
Since simply adding two attention layers to \hthree models already
outperforms both the pure \hthree model and Transformers, we are optimistic
about combining the complementary strengths of SSMs and attention in the future.

% \pagebreak
\ifarxiv

\else
\textbf{Reproducibility Statement.} To facilitate the reproducibility of our
algorithms and results, (i) we include a link to downloadable source code in
supplementary materials, (ii) for our theoretical statements and results, we
include clear explanations of any assumptions and a complete proof of the claims
in~\cref{sec:proofs}; for any datasets used in the experiments, a complete description of the data processing steps is in~\cref{sec:app_exp_details}.
We will also release model checkpoints for all our models.

\textbf{Ethics Statement.}
Our work seeks to understand the fundamental capabilities and limitations of newly-emerging model architectures.
As the amount of data and model size grows, we also week to understand how to make training these models more efficient---and run inference more efficiently.
This potentially connects to energy savings during model development and deployment.
We also note that the relative underutilization of tensor cores in the FFT convolutions of state space models (even with our block FFT) suggests that consumer GPUs may be able to train models at a cheaper price point.

However, as with any language model training, developing new techniques may impact a wide range of applications, each with potential benefits and harms.
For example, making language model training cheaper and making inference more efficient make it cheaper to spread disinformation.
Similarly, improving the efficiency of model training may not reduce the overall environmental footprint of training, since the same resources may be used to train more models, or train the same models for longer.
While our work makes partial progress on the fronts of efficiency and understanding, it does not explicitly address the issues of fairness and bias in language models.
\fi
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
