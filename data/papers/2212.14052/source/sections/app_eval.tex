\section{Experimental Details \label{sec:app_exp_details}}

\subsection{Synthetics}
Our synthetic tasks, inspired by~\citep{olsson2022context}, are designed to mimic the in-context learning capability of large language models---the ability to learn from examples in the input sequence, and use information from the input to generate the right answer for the output.
For example, the induction head task requires memorizing the token that appears after the special $\vdash$ token in the input sequence, and the associative recall task requires learning the mapping from keys to tokens from the input sequence.

We evaluate synthetics by training two-layer versions of our GPT models, with different modules replacing attention.
We train models with inner dimension 32, and MLP dimension 128.
For all the synthetics, we use a learning rate of 5e-4 and a weight decay of 0.1.
We sample 5000 training examples and 500 test examples from the same distribution, and we train for 200 epochs.
Again, we use embedding dropout of 0.1 and residual dropout of 0.0.

\subsection{Model Architecture}
For our 125M models, we use 12 layers, with hidden dimension 1024, and MLP dimension 4096.
For our 355M models, we use 24 layers, with the same hidden dimension and MLP dimension.
The 1.3B models have 24 layers, with hidden dimension 2048, and MLP dimension 8192.
The 2.7B models have 32 layers, hidden dimension 2560, and MLP dimension 10240.
The hybrid models have 12, 16, 16, and 20 heads for the 125M, 355M, 1.3B, and 2.7B models, respectively.
The 125M hybrid model has an attention layers at layers 1 and 7, the 355M and 1.3B hybrid models have attention layers at layers 1 and 13, and the 2.7B hybrid model has attention layers at layers 10 and 21.
For both our hybrid models and our \hthree models, we use SSM state size 64.
Our hybrid model uses head dimension 1 for \hthree, while our pure \hthree model uses head dimension 8.
We run models with mixed-precision training, with bf16 for the MLP's and attention.
When training language models, we use fp32 for the FFTConv.

\subsection{OpenWebText Training}
For the 125M models trained on OpenWebText, we follow the training recipe of the Megatron-LM repo.

We use an effective batch size of 512, and use gradient accumulation to fit into
available GPU memory.
We use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4
for GPT-2 medium, and weight decay of 0.1.
All models are trained with the same hyperparameters for 100K steps.
We run all implementations with mixed-precision training (PyTorch AMP).
We train models with sequence length 1024.

We use the Openwebtext dataset, with the GPT-2 BPE tokenizer. We randomly select
0.5\% of the dataset as the validation set, with the rest being used as training
set.
This random selection of validation set is done once, and all models are evaluated
on the same validation set.

\subsection{The Pile Training}
For the 125M and 355M models trained on the Pile, we follow the training recipe of GPT-3.
We use batch size 256, with sequence length 2048.
We train our models for 800K steps.
We use residual dropout 0.0 and embedding dropout 0.1.
We use the AdamW optimizer, with learning rate 6e-4 for the 125M model and 3e-4 for the 355M model, and a weight decay of 0.1.
We use a cosine schedule with 8000 steps for linear warmup, and decay the
learning rate to 10\% by 300B tokens, then continue training at 10\% learning
rate for another 100B tokens.
We suspect that there exist better hyperparameters for \hthree language models, but we did not have the resources to tune them.

For the 1.3B models, we double the batch size to 512 (with sequence length
2048), again following the training recipe of GPT-3. The number of training
steps are halved so that we train on the same number of tokens.

For the Pile dataset, we again use the GPT-2 BPE tokenizer, similar to GPT-3 and GPT-Neo.

\subsection{SuperGLUE}
We follow the prompts used in the GPT-3 paper~\citep{brown2020language}.
For rank classification on the binary classification tasks, we use yes/no for WSC, WIC, MultiRC, and BoolQ, and we use true/false for RTE.
For CB, we use true/false/neither as the three choices.
For COPA and ReCoRD, we use the continuations provided by the task.

\subsection{Hardware}
All models were trained on either a single 16xA100-40GB node or a cluster of 8xA100-80GB nodes.

\section{Additional Experiments}
\label{sec:app_additional_experiments}

\subsection{LRA Accuracy}
We evaluate the accuracy of \hthree on LRA.
We compare accuracy to S4D~\citep{gu2022parameterization}, since \hthree uses an S4D kernel as a component in its layer.
We use the same hyperparameters as S4D, and make the layer bidirectional by making two copies and running them in opposite directions.

\input{tables/lra_acc.tex}

Table~\ref{table:lra_acc} shows that \hthree performs well on the LRA benchmark, even thought it was designed for autoregressive language modeling.
\hthree outperforms S4D on two of the LRA tasks, and comes within 1 point on the others.

\subsection{WikiText103}
We train 125M-sized models on WikiText103~\citep{merity2016pointer} and compare their test PPL to transformers, as well as other variants of efficient or long-range attention.
We use the same hyperparameters and setup as training on OpenWebText.
We also provide results from Transformer-XL and Perceiver AR for context, though the results may not be directly comparable due to differences in model size and tokenizer.

\input{tables/wikitext103.tex}

Table~\ref{table:wikitext103} shows that the Hybrid \hthree model is competitive with Transformers of the same size, as well as larger models such as the 358M Perceiver AR and 285M Transformer-XL.
The hybrid \hthree model also significantly outperforms transformers with performer, reformer, and linear attention.

We note that the Transformer-XL and Perceiver AR PPl numbers are from the original papers, and may not be directly comparable to our results.
In particular, they use a tokenizer with a different vocab size, which means that the PPLs are not directly comparable.
In addition, the larger vocab size necessitates a change in the model (adaptive softmax) that may affect performance.
The top five numbers in Table~\ref{table:wikitext103} are trained with the same setup and are directly comparable to each other.

\subsection{PG-19}
We evaluate models trained on the PG-19 dataset~\citep{rae2019compressive}, a natural language dataset comprised of texts from books.
We compare the performance of Hybrid \hthree compared against Transformers and linear attention.
We use the same setup as evaluating on OpenWebText.

\input{tables/pg19.tex}

Table~\ref{table:pg19} shows that Hybrid \hthree outperforms transformers and linear attention.

\subsection{Length Extrapolation}
One property of SSMs is that they can naturally extrapolate to sequence lengths longer than those seen during training.
We use the synthetic associative recall task to demonstrate that \hthree maintains this capability.
To do so, we train a two-layer \hthree model on sequences of length 20 drawn from the associative recall synthetic language.
Then, we evaluate accuracy of the last token prediction on sequences of length 20 and 40.

\input{tables/length_extrapolation.tex}

Table~\ref{table:length_extrapolation} shows that \hthree maintains accuracy on sequences of length 40, which is twice the length of the training sequences.

\subsection{Scaling in Number of Tokens}
We evaluate how well a Hybrid \hthree model scales with the number of tokens seen during training, compared to a Transformer.
For these experiments, we train a 125M Hybrid \hthree model and a 125M Transformer on the Pile for 5B, 10B, and 15B tokens.
We use a learning rate of 6e-4, adjusting the warmup to be 1\% of the total training time, and adjusting the decay rate to decay the learning rate to 6e-5 by the end of training.

\input{tables/scaling_tokens.tex}

Table~\ref{table:scaling_tokens} shows the results.
Both the Hybrid \hthree model and Transformer model improve as the number of training tokens increases.

\subsection{\hthree Language Model}
\input{tables/superglue_zero_shot_all.tex}
\input{tables/superglue_few_shot_all.tex}

We report the results of a pure \hthree language model on NLP evaluations.
We train a 125M model on the Pile for 400B tokens.
Tables~\ref{table:superglue_zeroshot_all} and~\ref{table:superglue_fewshot_all} show zero-shot and few-shot performance on SuperGLUE, respectively.

\subsection{Generation Performance\label{sec:app_generation}}
\input{tables/superglue_zero_shot.tex}
\input{tables/superglue_few_shot.tex}
We report results on SuperGLUE for generation.
Instead of taking rank classification, we instead let the model generate a response, and we search for the gold label (i.e., ``yes'' or ``no'' for the yes/no questions) in the output.
Tables~\ref{table:superglue_zeroshot} and~\ref{table:superglue_fewshot} report the results.
The trends for few-shot learning match with the logit results, but the hybrid and \hthree models perform very poorly in zero-shot performance on some tasks.
In these cases, the models tend to generate long text responses that are not relevant to the answer.
The few-shot learning examples help the models generate answers in a parsable format.

\subsection{Non-Text Sequence Modeling}
\label{subsec:non_text_sequence_modeling}

We show that \hthree outperforms Transformers on two non-text sequence modeling tasks: raw speech classification and seizure classification over raw EEG signals.
\hthree sets state-of-the-art performance on seizure classification and matches S4 on speech classification---which suggests that \hthree, or one of its hybrids, may be a strong candidate for a multimodal foundation model.
Appendix~\ref{sec:app_exp_details} gives experimental details, and Appendix~\ref{sec:app_additional_experiments} gives an additional experiment on brain fMRI data.

\paragraph{Seizure Classification from EEG}
Seizures, which are characterized by uncontrolled brain activity, are some of the most common neurological disorders~\citep{fisher2014ilae}. Chronic seizures, or epilepsy, cause a range of psychiatric and psycho-social disorders and impact the lives of roughly one percent of the global population~\citep{kerr2012impact}. The first step to treating epilepsy is manual analysis of scalp EEG by board-certified neurologists. However, the vast amount of EEG data produced by each patient (which can be up to days of data) makes manual EEG analysis a costly and time-consuming process.  

To mitigate the costs associated with EEG monitoring, recent deep learning techniques have began to show promise in flagging abnormal EEG segments for potential seizure events~\citep{siddiqui2020review}. A challenge with classifying EEG data is the trade-off between increasing input sequence length, where more context has been shown to improve seizure classification performance \cite{saab2020weak}, with the increased difficulty of training deep learning models on long sequences (e.g., an EEG signal sampled at $200$Hz produces $12{,}000$ time steps per minute). As a result, many techniques involve domain-specialized models and pre-processing steps, such as FFT transforms and graphical representations \cite{tang2021self}.

We use the largest publicly available EEG corpus, TUSZ v1.5.2~\citep{shah2018temple}, which includes $5{,}612$ EEG signals from 636 patients, with $3{,}050$ annotated seizures.
Signals are segmented into 60-second clips, and split into train/val/test by patient.
The train set contains 39765 clips, the val set contains 4351 clips, and the test set contains 10001 clips.

\input{tables/eeg.tex}

We evaluate binary seizure classification of $60$-sec EEG clips, sampled at $200$Hz, with 19 electrodes: $ x \in R^{12{,}000 \times 19}$ and $y \in \{0,1\}$ on the TUSZ v1.5.2~\citep{shah2018temple} corpus.
Transformers cannot process the long sequence length of EEG signals without running out of GPU memory, whereas \hthree can---and sets state-of-the-art performance.

\paragraph{Raw Speech Classification}
The SC10 speech commands task~\citep{warden2018speech} contains raw audio signals one second in length, sampled at 16kHz.
Similarly to EEG signals, Transformers cannot process the long sequence length.
Table~\ref{table:speech} shows that \hthree comes within half a point of S4, the state-of-the-art method.

\input{tables/speech.tex}
