\section{Linear Attention and Time-Varying Systems}
\label{app:linear_attention}

We draw some connections from linear attention to LTI systems and SSMs.

We first present linear attention as a linear time-varying system, and show how converting it to a linear time-invariant system matches \hthree.

\paragraph{Linear time-varying system and linear attention}

In general, a layer in a sequence model takes in a sequence and outputs a
sequence.
Many of these take the form of a linear time-varying system (thanks to the
Picard-Lindelof theorem, nonlinear systems can be approximated by a series of
linear system):
\begin{align*}
  x_i &= \vA_i x_{i-1} + \vB_i u_i, \\
  y_i &= \vC_i x_i + \vD_i u_i.
\end{align*}
This has the same form as SSMs (\cref{sec:background}), except that the matrices
can depend on the timestep.

Recall the form of linear attention from~\cref{sec:background}.
For the purpose of approximation, we ignore the denominator in linear
attention~\cref{sec:background} (i.e., assuming $d_i = 1$).
We see that $S_i$ is just a cumulative sum, satisfying the recurrence $S_{i+1} = S_{i} + \phi(K_{i+1}) V_{i+1}^T$.
Similarly, $O_i$ satisfies the recurrence $O_{i+1} = \phi(Q_{i+1})^T S_{i+1}$.
This is a linear time-varying system of the form $x_{i+1} = \vA x_i + \vB u_{i+1}$ and
$y_{i+1} = \vC_{i+1} x_{i+1}$ (with $\vA = I$, $\vB = I$, $u_{i} = \phi(K_{i}) V_i^T$,
$C_{i} = \phi(Q_{i})^T$).
That is, $\vA$ and $\vB$ are constant, but $C$ is time-variant.

To convert this into a linear time-invariant version, we treat the time-variant $\vC_i$ as a post-processing step.
We instead of a fixed $\vC$ for the LTI.
This yields an LTI:
\begin{align*}
  x_{i+1} &= \vA x_i + \vB \phi(K_{i}) V_i^T, \\
  y_{i+1} &= \vC x_{i},
\end{align*}
for some matrices $\vA, \vB, \vC$ that are learned.
We then apply post-processing by multiply $y_{i+1}$ with $\phi(Q_i)^T$.
Replacing $\phi(K_{i})$ with a shift SSM yields an analogue to \hthree.
