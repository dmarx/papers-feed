@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are {RNN}s: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{choromanski2020rethinking,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Quincy and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{choromanski2019unifying,
  title={Unifying Orthogonal {M}onte {C}arlo Methods},
  author={Choromanski, Krzysztof and Rowland, Mark and Chen, Wenyu and Weller, Adrian},
  booktitle={International Conference on Machine Learning},
  pages={1203--1212},
  year={2019}
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@inproceedings{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  booktitle = {The International Conference on Machine Learning ({ICML})},
  year={2020}
}

@article{daras2020smyrf,
  title={Smyrf-efficient attention using asymmetric clustering},
  author={Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6476--6489},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{shoeybi2019megatron,
  title={Megatron-{LM}: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{zhang2022opt,
  title={{OPT}: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@misc{nvidia2017nvidia,
  title={Nvidia {T}esla {V}100 {GPU} architecture},
  author={NVIDIA},
  year={2017},
  publisher={Aug}
}

@misc{nvidia2020nvidia,
  title={Nvidia {A}100 Tensor Core {GPU} Architecture},
  author={NVIDIA},
  year={2020}
}

@misc{nvidia2022nvidia,
  title={Nvidia {H}100 Tensor Core {GPU} Architecture},
  author={NVIDIA},
  year={2022}
}

@article{kao2021optimized,
  title={An Optimized Dataflow for Mitigating Attention Performance Bottlenecks},
  author={Kao, Sheng-Chun and Subramanian, Suvinay and Agrawal, Gaurav and Krishna, Tushar},
  journal={arXiv preprint arXiv:2107.06419},
  year={2021}
}

@inproceedings{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{gu2022efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R\'e, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2022}
}

@article{goel2022s,
  title={It's Raw! Audio Generation with State-Space Models},
  author={Goel, Karan and Gu, Albert and Donahue, Chris and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2202.09729},
  year={2022}
}

@inproceedings{gupta2022diagonal,
  title={Diagonal State Spaces are as Effective as Structured State Spaces},
  author={Gupta, Ankit and Gu, Albert and Berant, Jonathan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{mehta2022long,
  title={Long range language modeling via gated state spaces},
  author={Mehta, Harsh and Gupta, Ankit and Cutkosky, Ashok and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2206.13947},
  year={2022}
}

@inproceedings{gu2022parameterization,
  title={On the Parameterization and Initialization of Diagonal State Space Models},
  author={Gu, Albert and Gupta, Ankit and Goel, Karan and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{gu2020hippo,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}

@article{bommasani2021opportunities,
  title={On the Opportunities and Risks of Foundation Models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@inproceedings{nguyen2022s4nd,
  title={S4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces},
  author={Nguyen, Eric and Goel, Karan and Gu, Albert and Downs, Gordon and Shah, Preey and Dao, Tri and Baccus, Stephen and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@misc{brogan1974modern,
  title={Modern Control Theory},
  author={Brogan, Willian L},
  year={1974},
  publisher={Quantum Publishers, Inc.}
}

@article{gu2021combining,
  title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers},
  author={Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}

@article{rabe2021self,
  title={Self-attention Does Not Need $ {O} (n^2) $ Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
}


@article{cooley1965an,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2003354},
 author = {James W. Cooley and John W. Tukey},
 journal = {Mathematics of Computation},
 number = {90},
 pages = {297--301},
 publisher = {American Mathematical Society},
 title = {An Algorithm for the Machine Calculation of Complex Fourier Series},
 volume = {19},
 year = {1965}
}

@article{bailey1990ffts,
  title={{FFT}s in external or hierarchical memory},
  author={Bailey, David H},
  journal={The journal of Supercomputing},
  volume={4},
  number={1},
  pages={23--35},
  year={1990},
  publisher={Springer}
}

@inproceedings{dao2022monarch,
  title={Monarch: Expressive Structured Matrices for Efficient and Accurate Training},
  author={Dao, Tri and Chen, Beidi and Sohoni, Nimit and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022}
}

@article{kalman1960new,
  title={A new approach to linear filtering and prediction problems},
  author={Kalman, Rudolph Emil},
  year={1960}
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = {2021},
  note         = {{If you use this software, please cite it using 
                   these metadata.}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year={2020},
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@article{gu2022train,
  title={How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections},
  author={Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2206.12037},
  year={2022}
}

@article{saab2020weak,
  title={Weak supervision as an efficient approach for automated seizure detection in electroencephalography},
  author={Saab, Khaled and Dunnmon, Jared and R{\'e}, Christopher and Rubin, Daniel and Lee-Messer, Christopher},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{tang2021self,
  title={Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis},
  author={Tang, Siyi and Dunnmon, Jared and Saab, Khaled Kamal and Zhang, Xuan and Huang, Qianying and Dubost, Florian and Rubin, Daniel and Lee-Messer, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{hochreiter1996lstm,
title={LSTM can solve hard long time lag problems}, author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
journal={Advances in neural information processing systems},
volume={9},
year={1996}
}

@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{ba2016using,
  title={Using fast weights to attend to the recent past},
  author={Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{oppenheim1978applications,
  title={Applications of digital signal processing},
  author={Oppenheim, Alan V},
  journal={Englewood Cliffs},
  year={1978}
}

@book{oppenheim2001discrete,
  title={Discrete-time signal processing. Vol. 2},
  author={Oppenheim, Alan V and Buck, John R and Schafer, Ronald W},
  year={2001},
  publisher={Upper Saddle River, NJ: Prentice Hall}
}

@article{hooker2021hardware,
  title={The hardware lottery},
  author={Hooker, Sara},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={58--65},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{warden2018speech,
  title={Speech commands: A dataset for limited-vocabulary speech recognition},
  author={Warden, Pete},
  journal={arXiv preprint arXiv:1804.03209},
  year={2018}
}

@misc{cufft,
  title={cuFFT v11.7.1 Documentation},
  author={NVIDIA},
  year={2022},
  note={https://docs.nvidia.com/cuda/cufft/index.html}
}

@article{frigo2012fftw,
  title={FFTW: Fastest Fourier transform in the west},
  author={Frigo, Matteo and Johnson, Steven G},
  journal={Astrophysics Source Code Library},
  pages={ascl--1201},
  year={2012}
}

@misc{Gokaslan2019OpenWeb,
    title={OpenWebText Corpus},
    author={Aaron Gokaslan and Vanya Cohen and Ellie Pavlick and Stefanie Tellex},
    year={2019}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{shah2018temple,
  title={The temple university hospital seizure detection corpus},
  author={Shah, Vinit and Von Weltin, Eva and Lopez, Silvia and McHugh, James Riley and Veloso, Lillian and Golmohammadi, Meysam and Obeid, Iyad and Picone, Joseph},
  journal={Frontiers in neuroinformatics},
  volume={12},
  pages={83},
  year={2018},
  publisher={Frontiers}
}

@article{hawthorne2022general,
  title={General-purpose, long-context autoregressive modeling with Perceiver AR},
  author={Hawthorne, Curtis and Jaegle, Andrew and Cangea, C{\u{a}}t{\u{a}}lina and Borgeaud, Sebastian and Nash, Charlie and Malinowski, Mateusz and Dieleman, Sander and Vinyals, Oriol and Botvinick, Matthew and Simon, Ian and others},
  journal={arXiv preprint arXiv:2202.07765},
  year={2022}
}

@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@inproceedings{rae2019compressive,
  title={Compressive Transformers for Long-Range Sequence Modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Hillier, Chloe and Lillicrap, Timothy P},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{luo2021stable,
  title={Stable, fast and accurate: Kernelized attention with relative positional encoding},
  author={Luo, Shengjie and Li, Shanda and Cai, Tianle and He, Di and Peng, Dinglan and Zheng, Shuxin and Ke, Guolin and Wang, Liwei and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22795--22807},
  year={2021}
}

% FMRI BEGIN ----------------------------
@article{thomas_fmri_2022,
  title={Self-Supervised Learning Of Brain Dynamics From Broad Neuroimaging Data},
  author={Thomas, Armin W and R{\'e}, Christopher and Poldrack, Russell A},
  journal={arXiv preprint arXiv:2206.11417},
  year={2022}
}

@article{dadi_2020_fine,
  title={Fine-grain atlases of functional modes for fMRI analysis},
  author={Dadi, Kamalaker and Varoquaux, Ga{\"e}l and Machlouzarides-Shalit, Antonia and Gorgolewski, Krzysztof J and Wassermann, Demian and Thirion, Bertrand and Mensch, Arthur},
  journal={NeuroImage},
  volume={221},
  pages={117126},
  year={2020},
  publisher={Elsevier}
}

@article{brown_2020_language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{wolf_2020_transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{markiewicz_2021_openneuro,
  title={The OpenNeuro resource for sharing of neuroscience data},
  author={Markiewicz, Christopher J and Gorgolewski, Krzysztof J and Feingold, Franklin and Blair, Ross and Halchenko, Yaroslav O and Miller, Eric and Hardcastle, Nell and Wexler, Joe and Esteban, Oscar and Goncavles, Mathias and others},
  journal={Elife},
  volume={10},
  pages={e71774},
  year={2021},
  publisher={eLife Sciences Publications Limited}
}

@article{van_2013_wu,
  title={The WU-Minn human connectome project: an overview},
  author={Van Essen, David C and Smith, Stephen M and Barch, Deanna M and Behrens, Timothy EJ and Yacoub, Essa and Ugurbil, Kamil and Wu-Minn HCP Consortium and others},
  journal={Neuroimage},
  volume={80},
  pages={62--79},
  year={2013},
  publisher={Elsevier}
}

@article{king_2019_functional,
  title={Functional boundaries in the human cerebellum revealed by a multi-domain task battery},
  author={King, Maedbh and Hernandez-Castillo, Carlos R and Poldrack, Russell A and Ivry, Richard B and Diedrichsen, J{\"o}rn},
  journal={Nature neuroscience},
  volume={22},
  number={8},
  pages={1371--1378},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{fischl_2012_freesurfer,
  title={FreeSurfer},
  author={Fischl, Bruce},
  journal={Neuroimage},
  volume={62},
  number={2},
  pages={774--781},
  year={2012},
  publisher={Elsevier}
}
% FMRI END ----------------------------

@inproceedings{baevski2018adaptive,
  title={Adaptive Input Representations for Neural Language Modeling},
  author={Baevski, Alexei and Auli, Michael},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{keles2022computational,
  title={On The Computational Complexity of Self-Attention},
  author={Keles, Feyza Duman and Wijewardena, Pruthuvi Mahesakya and Hegde, Chinmay},
  journal={arXiv preprint arXiv:2209.04881},
  year={2022}
}

@article{fisher2014ilae,
  title={ILAE official report: a practical clinical definition of epilepsy},
  author={Fisher, Robert S and Acevedo, Carlos and Arzimanoglou, Alexis and Bogacz, Alicia and Cross, J Helen and Elger, Christian E and Engel Jr, Jerome and Forsgren, Lars and French, Jacqueline A and Glynn, Mike and others},
  journal={Epilepsia},
  volume={55},
  number={4},
  pages={475--482},
  year={2014},
  publisher={Wiley Online Library}
}

@article{kerr2012impact,
  title={The impact of epilepsy on patients' lives},
  author={Kerr, Michael Patrick},
  journal={Acta Neurologica Scandinavica},
  volume={126},
  pages={1--9},
  year={2012},
  publisher={Wiley Online Library}
}

@article{siddiqui2020review,
  title={A review of epileptic seizure detection using machine learning classifiers},
  author={Siddiqui, Mohammad Khubeb and Morales-Menendez, Ruben and Huang, Xiaodi and Hussain, Nasir},
  journal={Brain informatics},
  volume={7},
  number={1},
  pages={1--18},
  year={2020},
  publisher={SpringerOpen}
}
