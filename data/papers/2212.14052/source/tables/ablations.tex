\begin{table}[h]
    % \captionsetup{font=small}
    \small
    \centering
    % \vspace{-1em}
    \caption{\label{table:ablations} Perplexity of SSM variants compared to
      Transformers on OpenWebText. All models have 12 layers, with size around 125M, and are trained
      with the same hyperpameters, for 50B tokens.}
    %   \vspace{1em}
    {
        \begin{tabular}{@{}|ccccc|c|@{}}
            \hline
        %   \specialrule{.15em}{.05em}{.05em}
        \hthree & \hthree Hybrid (2 Attn) & S4D & GSS & GSS Hybrid (2 Attn) & Transformer  \\ % & Training time \\
        %   \specialrule{.15em}{.05em}{.05em}
        \hline
        21.0 & \textbf{19.6} & 24.9 & 24.0 & 19.8 & 20.6 \\ \hline
        \end{tabular}
    }
    % \vspace{-1.5em}
\end{table}