% \begin{table}[h]
    % \captionsetup{font=small}
    \small
    \centering
    % \vspace{-1em}
    \caption{\label{table:training_time} Inference throughput on A100 80GB, 1.3B models.
    Batch size 64, prompt length 512, 1024, or 1536, and generating 128 tokens
    per sequence in the batch (i.e., 64 $\times$ 128 tokens in a batch). Hybrid
    \hthree is up to 2.4$\times$ faster than a Transformer of similar size in inference. The
    difference is larger for longer sequences.}
    %   \vspace{1em}
    {
        \begin{tabular}{@{}|c|c|c|c|@{}}
            \hline
        %   \specialrule{.15em}{.05em}{.05em}
        Tokens/s & Prompt length 512 & Prompt length 1024 & Prompt length 1536 \\ % & Training time \\
        %   \specialrule{.15em}{.05em}{.05em}
        \hline
        Transformer-1.3B & 1340 & 770 & 520 \\
        % GPT-2 Small (FlashAttention~\citep{dao2022flashattention}) & -- & -- \\ 
        % GPT-2 Small (FasterTransformer~\citep{}) & N/A & -- \\ \hline
        Hybrid \hthree-1.3B & 1980 & 1580 & 1240 \\ \hline
        \end{tabular}
    }
    % \vspace{-1.5em}
% \end{table}