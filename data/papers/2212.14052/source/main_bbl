\begin{thebibliography}{10}

\bibitem{ba2016using}
Jimmy Ba, Geoffrey~E Hinton, Volodymyr Mnih, Joel~Z Leibo, and Catalin Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{bailey1990ffts}
David~H Bailey.
\newblock {FFT}s in external or hierarchical memory.
\newblock {\em The journal of Supercomputing}, 4(1):23--35, 1990.

\bibitem{gpt-neo}
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, March 2021.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{brogan1974modern}
Willian~L Brogan.
\newblock Modern control theory, 1974.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{cho2014properties}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Dzmitry Bahdanau, and Yoshua Bengio.
\newblock On the properties of neural machine translation: Encoder-decoder
  approaches.
\newblock {\em arXiv preprint arXiv:1409.1259}, 2014.

\bibitem{choromanski2020rethinking}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{cooley1965an}
James~W. Cooley and John~W. Tukey.
\newblock An algorithm for the machine calculation of complex fourier series.
\newblock {\em Mathematics of Computation}, 19(90):297--301, 1965.

\bibitem{dadi_2020_fine}
Kamalaker Dadi, Ga{\"e}l Varoquaux, Antonia Machlouzarides-Shalit, Krzysztof~J
  Gorgolewski, Demian Wassermann, Bertrand Thirion, and Arthur Mensch.
\newblock Fine-grain atlases of functional modes for fmri analysis.
\newblock {\em NeuroImage}, 221:117126, 2020.

\bibitem{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G Carbonell, Quoc Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 2978--2988, 2019.

\bibitem{dao2022monarch}
Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan,
  Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R{\'e}.
\newblock Monarch: Expressive structured matrices for efficient and accurate
  training.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2022.

\bibitem{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{daras2020smyrf}
Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros~G Dimakis.
\newblock Smyrf-efficient attention using asymmetric clustering.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6476--6489, 2020.

\bibitem{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In {\em International Conference on Machine Learning}, pages
  5793--5831. PMLR, 2022.

\bibitem{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn
  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
  Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
  Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock {\em Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem{fischl_2012_freesurfer}
Bruce Fischl.
\newblock Freesurfer.
\newblock {\em Neuroimage}, 62(2):774--781, 2012.

\bibitem{fisher2014ilae}
Robert~S Fisher, Carlos Acevedo, Alexis Arzimanoglou, Alicia Bogacz, J~Helen
  Cross, Christian~E Elger, Jerome Engel~Jr, Lars Forsgren, Jacqueline~A
  French, Mike Glynn, et~al.
\newblock Ilae official report: a practical clinical definition of epilepsy.
\newblock {\em Epilepsia}, 55(4):475--482, 2014.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{goel2022s}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'e}.
\newblock It's raw! audio generation with state-space models.
\newblock {\em arXiv preprint arXiv:2202.09729}, 2022.

\bibitem{Gokaslan2019OpenWeb}
Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex.
\newblock Openwebtext corpus, 2019.

\bibitem{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1474--1487, 2020.

\bibitem{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R\'e.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In {\em The International Conference on Learning Representations
  ({ICLR})}, 2022.

\bibitem{gu2022parameterization}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher R{\'e}.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{gu2021combining}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and
  Christopher R{\'e}.
\newblock Combining recurrent, convolutional, and continuous-time models with
  linear state-space layers.
\newblock {\em Advances in neural information processing systems}, 34, 2021.

\bibitem{gu2022train}
Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R{\'e}.
\newblock How to train your hippo: State space models with generalized
  orthogonal basis projections.
\newblock {\em arXiv preprint arXiv:2206.12037}, 2022.

\bibitem{gupta2022diagonal}
Ankit Gupta, Albert Gu, and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{hawthorne2022general}
Curtis Hawthorne, Andrew Jaegle, C{\u{a}}t{\u{a}}lina Cangea, Sebastian
  Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals,
  Matthew Botvinick, Ian Simon, et~al.
\newblock General-purpose, long-context autoregressive modeling with perceiver
  ar.
\newblock {\em arXiv preprint arXiv:2202.07765}, 2022.

\bibitem{hochreiter1996lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Lstm can solve hard long time lag problems.
\newblock {\em Advances in neural information processing systems}, 9, 1996.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{hooker2021hardware}
Sara Hooker.
\newblock The hardware lottery.
\newblock {\em Communications of the ACM}, 64(12):58--65, 2021.

\bibitem{kao2021optimized}
Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna.
\newblock An optimized dataflow for mitigating attention performance
  bottlenecks.
\newblock {\em arXiv preprint arXiv:2107.06419}, 2021.

\bibitem{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem{keles2022computational}
Feyza~Duman Keles, Pruthuvi~Mahesakya Wijewardena, and Chinmay Hegde.
\newblock On the computational complexity of self-attention.
\newblock {\em arXiv preprint arXiv:2209.04881}, 2022.

\bibitem{kerr2012impact}
Michael~Patrick Kerr.
\newblock The impact of epilepsy on patients' lives.
\newblock {\em Acta Neurologica Scandinavica}, 126:1--9, 2012.

\bibitem{king_2019_functional}
Maedbh King, Carlos~R Hernandez-Castillo, Russell~A Poldrack, Richard~B Ivry,
  and J{\"o}rn Diedrichsen.
\newblock Functional boundaries in the human cerebellum revealed by a
  multi-domain task battery.
\newblock {\em Nature neuroscience}, 22(8):1371--1378, 2019.

\bibitem{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em The International Conference on Machine Learning ({ICML})},
  2020.

\bibitem{luo2021stable}
Shengjie Luo, Shanda Li, Tianle Cai, Di~He, Dinglan Peng, Shuxin Zheng, Guolin
  Ke, Liwei Wang, and Tie-Yan Liu.
\newblock Stable, fast and accurate: Kernelized attention with relative
  positional encoding.
\newblock {\em Advances in Neural Information Processing Systems},
  34:22795--22807, 2021.

\bibitem{markiewicz_2021_openneuro}
Christopher~J Markiewicz, Krzysztof~J Gorgolewski, Franklin Feingold, Ross
  Blair, Yaroslav~O Halchenko, Eric Miller, Nell Hardcastle, Joe Wexler, Oscar
  Esteban, Mathias Goncavles, et~al.
\newblock The openneuro resource for sharing of neuroscience data.
\newblock {\em Elife}, 10:e71774, 2021.

\bibitem{mehta2022long}
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock {\em arXiv preprint arXiv:2206.13947}, 2022.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem{nguyen2022s4nd}
Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen
  Baccus, and Christopher R{\'e}.
\newblock S4nd: Modeling images and videos as multidimensional signals with
  state spaces.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{nvidia2017nvidia}
NVIDIA.
\newblock Nvidia {T}esla {V}100 {GPU} architecture, 2017.

\bibitem{nvidia2020nvidia}
NVIDIA.
\newblock Nvidia {A}100 tensor core {GPU} architecture, 2020.

\bibitem{cufft}
NVIDIA.
\newblock cufft v11.7.1 documentation, 2022.
\newblock https://docs.nvidia.com/cuda/cufft/index.html.

\bibitem{nvidia2022nvidia}
NVIDIA.
\newblock Nvidia {H}100 tensor core {GPU} architecture, 2022.

\bibitem{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
  Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
  Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
  Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock {\em Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem{oppenheim1978applications}
Alan~V Oppenheim.
\newblock Applications of digital signal processing.
\newblock {\em Englewood Cliffs}, 1978.

\bibitem{oppenheim2001discrete}
Alan~V Oppenheim, John~R Buck, and Ronald~W Schafer.
\newblock {\em Discrete-time signal processing. Vol. 2}.
\newblock Upper Saddle River, NJ: Prentice Hall, 2001.

\bibitem{rabe2021self}
Markus~N Rabe and Charles Staats.
\newblock Self-attention does not need $ {O} (n^2) $ memory.
\newblock {\em arXiv preprint arXiv:2112.05682}, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{rae2019compressive}
Jack~W Rae, Anna Potapenko, Siddhant~M Jayakumar, Chloe Hillier, and Timothy~P
  Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{saab2020weak}
Khaled Saab, Jared Dunnmon, Christopher R{\'e}, Daniel Rubin, and Christopher
  Lee-Messer.
\newblock Weak supervision as an efficient approach for automated seizure
  detection in electroencephalography.
\newblock {\em NPJ digital medicine}, 3(1):1--12, 2020.

\bibitem{shah2018temple}
Vinit Shah, Eva Von~Weltin, Silvia Lopez, James~Riley McHugh, Lillian Veloso,
  Meysam Golmohammadi, Iyad Obeid, and Joseph Picone.
\newblock The temple university hospital seizure detection corpus.
\newblock {\em Frontiers in neuroinformatics}, 12:83, 2018.

\bibitem{siddiqui2020review}
Mohammad~Khubeb Siddiqui, Ruben Morales-Menendez, Xiaodi Huang, and Nasir
  Hussain.
\newblock A review of epileptic seizure detection using machine learning
  classifiers.
\newblock {\em Brain informatics}, 7(1):1--18, 2020.

\bibitem{tang2021self}
Siyi Tang, Jared Dunnmon, Khaled~Kamal Saab, Xuan Zhang, Qianying Huang,
  Florian Dubost, Daniel Rubin, and Christopher Lee-Messer.
\newblock Self-supervised graph neural networks for improved
  electroencephalographic seizure analysis.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{thomas_fmri_2022}
Armin~W Thomas, Christopher R{\'e}, and Russell~A Poldrack.
\newblock Self-supervised learning of brain dynamics from broad neuroimaging
  data.
\newblock {\em arXiv preprint arXiv:2206.11417}, 2022.

\bibitem{van_2013_wu}
David~C Van~Essen, Stephen~M Smith, Deanna~M Barch, Timothy~EJ Behrens, Essa
  Yacoub, Kamil Ugurbil, Wu-Minn~HCP Consortium, et~al.
\newblock The wu-minn human connectome project: an overview.
\newblock {\em Neuroimage}, 80:62--79, 2013.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{warden2018speech}
Pete Warden.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock {\em arXiv preprint arXiv:1804.03209}, 2018.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock {OPT}: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\end{thebibliography}
