\begin{thebibliography}{10}

\bibitem{ba2016using}
Jimmy Ba, Geoffrey~E Hinton, Volodymyr Mnih, Joel~Z Leibo, and Catalin Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{bailey1990ffts}
David~H Bailey.
\newblock {FFT}s in external or hierarchical memory.
\newblock {\em The journal of Supercomputing}, 4(1):23--35, 1990.

\bibitem{gpt-neo}
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, March 2021.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{brogan1974modern}
Willian~L Brogan.
\newblock Modern control theory, 1974.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{cho2014properties}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Dzmitry Bahdanau, and Yoshua Bengio.
\newblock On the properties of neural machine translation: Encoder-decoder
  approaches.
\newblock {\em arXiv preprint arXiv:1409.1259}, 2014.

\bibitem{choromanski2020rethinking}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{cooley1965an}
James~W. Cooley and John~W. Tukey.
\newblock An algorithm for the machine calculation of complex fourier series.
\newblock {\em Mathematics of Computation}, 19(90):297--301, 1965.

\bibitem{dadi_2020_fine}
Kamalaker Dadi, Ga{\"e}l Varoquaux, Antonia Machlouzarides-Shalit, Krzysztof~J
  Gorgolewski, Demian Wassermann, Bertrand Thirion, and Arthur Mensch.
\newblock Fine-grain atlases of functional modes for fmri analysis.
\newblock {\em NeuroImage}, 221:117126, 2020.

\bibitem{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G Carbonell, Quoc Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 2978--2988, 2019.

\bibitem{dao2022monarch}
Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan,
  Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R{\'e}.
\newblock Monarch: Expressive structured matrices for efficient and accurate
  training.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2022.

\bibitem{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{daras2020smyrf}
Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros~G Dimakis.
\newblock Smyrf-efficient attention using asymmetric clustering.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6476--6489, 2020.

\bibitem{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In {\em International Conference on Machine Learning}, pages
  5793--5831. PMLR, 2022.

\bibitem{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn
  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
  Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
  Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock {\em Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem{fischl_2012_freesurfer}
Bruce Fischl.
\newblock Freesurfer.
\newblock {\em Neuroimage}, 62(2):774--781, 2012.

\bibitem{fisher2014ilae}
Robert~S Fisher, Carlos Acevedo, Alexis Arzimanoglou, Alicia Bogacz, J~Helen
  Cross, Christian~E Elger, Jerome Engel~Jr, Lars Forsgren, Jacqueline~A
  French, Mike Glynn, et~al.
\newblock Ilae official report: a practical clinical definition of epilepsy.
\newblock {\em Epilepsia}, 55(4):475--482, 2014.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{goel2022s}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'e}.
\newblock It's raw! audio generation with state-space models.
\newblock {\em arXiv preprint arXiv:2202.09729}, 2022.

\bibitem{Gokaslan2019OpenWeb}
Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex.
\newblock Openwebtext corpus, 2019.

\bibitem{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1474--1487, 2020.

\bibitem{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R\'e.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In {\em The International Conference on Learning Representations
  ({ICLR})}, 2022.

\bibitem{gu2022parameterization}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher R{\'e}.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{gu2021combining}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and
  Christopher R{\'e}.
\newblock Combining recurrent, convolutional, and continuous-time models with
  linear state-space layers.
\newblock {\em Advances in neural information processing systems}, 34, 2021.

\bibitem{gu2022train}
Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R{\'e}.
\newblock How to train your hippo: State space models with generalized
  orthogonal basis projections.
\newblock {\em arXiv preprint arXiv:2206.12037}, 2022.

\bibitem{gupta2022diagonal}
Ankit Gupta, Albert Gu, and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{hawthorne2022general}
Curtis Hawthorne, Andrew Jaegle, C{\u{a}}t{\u{a}}lina Cangea, Sebastian
  Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals,
  Matthew Botvinick, Ian Simon, et~al.
\newblock General-purpose, long-context autoregressive modeling with perceiver
  ar.
\newblock {\em arXiv preprint arXiv:2202.07765}, 2022.

\bibitem{hochreiter1996lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Lstm can solve hard long time lag problems.
\newblock {\em Advances in neural information processing systems}, 9, 1996.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{hooker2021hardware}
Sara Hooker.
\newblock The hardware lottery.
\newblock {\em Communications of the ACM}, 64(12):58--65, 2021.

\bibitem{kao2021optimized}
Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna.
\newblock An optimized dataflow for mitigating attention performance
  bottlenecks.
\newblock {\em arXiv preprint arXiv:2107.06419}, 2021.

\bibitem{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem{keles2022computational}
Feyza~Duman Keles, Pruthuvi~Mahesakya Wijewardena, and Chinmay Hegde.
\newblock On the computational complexity of self-attention.
\newblock {\em arXiv preprint arXiv:2209.04881}, 2022.

\bibitem{kerr2012impact}
Michael~Patrick Kerr.
\newblock The impact of epilepsy on patients' lives.
\newblock {\em Acta Neurologica Scandinavica}, 126:1--9, 2012.

\bibitem{king_2019_functional}
Maedbh King, Carlos~R Hernandez-Castillo, Russell~A Poldrack, Richard~B Ivry,
  and J{\"o}rn Diedrichsen.
\newblock Functional boundaries in the human cerebellum revealed by a
  multi-domain task battery.
\newblock {\em Nature neuroscience}, 22(8):1371--1378, 2019.

\bibitem{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em The International Conference on Machine Learning ({ICML})},
  2020.

\bibitem{luo2021stable}
Shengjie Luo, Shanda Li, Tianle Cai, Di~He, Dinglan Peng, Shuxin Zheng, Guolin
  Ke, Liwei Wang, and Tie-Yan Liu.
\newblock Stable, fast and accurate: Kernelized attention with relative
  positional encoding.
\newblock {\em Advances in Neural Information Processing Systems},
  34:22795--22807, 2021.

\bibitem{markiewicz_2021_openneuro}
Christopher~J Markiewicz, Krzysztof~J Gorgolewski, Franklin Feingold, Ross
  Blair, Yaroslav~O Halchenko, Eric Miller, Nell Hardcastle, Joe Wexler, Oscar
  Esteban, Mathias Goncavles, et~al.
\newblock The openneuro resource for sharing of neuroscience data.
\newblock {\em Elife}, 10:e71774, 2021.

\bibitem{mehta2022long}
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock {\em arXiv preprint arXiv:2206.13947}, 2022.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem{nguyen2022s4nd}
Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen
  Baccus, and Christopher R{\'e}.
\newblock S4nd: Modeling images and videos as multidimensional signals with
  state spaces.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{nvidia2017nvidia}
NVIDIA.
\newblock Nvidia {T}esla {V}100 {GPU} architecture, 2017.

\bibitem{nvidia2020nvidia}
NVIDIA.
\newblock Nvidia {A}100 tensor core {GPU} architecture, 2020.

\bibitem{cufft}
NVIDIA.
\newblock cufft v11.7.1 documentation, 2022.
\newblock https://docs.nvidia.com/cuda/cufft/index.html.

\bibitem{nvidia2022nvidia}
NVIDIA.
\newblock Nvidia {H}100 tensor core {GPU} architecture, 2022.

\bibitem{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
  Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
  Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
  Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock {\em Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem{oppenheim1978applications}
Alan~V Oppenheim.
\newblock Applications of digital signal processing.
\newblock {\em Englewood Cliffs}, 1978.

\bibitem{oppenheim2001discrete}
Alan~V Oppenheim, John~R Buck, and Ronald~W Schafer.
\newblock {\em Discrete-time signal processing. Vol. 2}.
\newblock Upper Saddle River, NJ: Prentice Hall, 2001.

\bibitem{rabe2021self}
Markus~N Rabe and Charles Staats.
\newblock Self-attention does not need $ {O} (n^2) $ memory.
\newblock {\em arXiv preprint arXiv:2112.05682}, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{rae2019compressive}
Jack~W Rae, Anna Potapenko, Siddhant~M Jayakumar, Chloe Hillier, and Timothy~P
  Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{saab2020weak}
Khaled Saab, Jared Dunnmon, Christopher R{\'e}, Daniel Rubin, and Christopher
  Lee-Messer.
\newblock Weak supervision as an efficient approach for automated seizure
  detection in electroencephalography.
\newblock {\em NPJ digital medicine}, 3(1):1--12, 2020.

\bibitem{shah2018temple}
Vinit Shah, Eva Von~Weltin, Silvia Lopez, James~Riley McHugh, Lillian Veloso,
  Meysam Golmohammadi, Iyad Obeid, and Joseph Picone.
\newblock The temple university hospital seizure detection corpus.
\newblock {\em Frontiers in neuroinformatics}, 12:83, 2018.

\bibitem{siddiqui2020review}
Mohammad~Khubeb Siddiqui, Ruben Morales-Menendez, Xiaodi Huang, and Nasir
  Hussain.
\newblock A review of epileptic seizure detection using machine learning
  classifiers.
\newblock {\em Brain informatics}, 7(1):1--18, 2020.

\bibitem{tang2021self}
Siyi Tang, Jared Dunnmon, Khaled~Kamal Saab, Xuan Zhang, Qianying Huang,
  Florian Dubost, Daniel Rubin, and Christopher Lee-Messer.
\newblock Self-supervised graph neural networks for improved
  electroencephalographic seizure analysis.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{thomas_fmri_2022}
Armin~W Thomas, Christopher R{\'e}, and Russell~A Poldrack.
\newblock Self-supervised learning of brain dynamics from broad neuroimaging
  data.
\newblock {\em arXiv preprint arXiv:2206.11417}, 2022.

\bibitem{van_2013_wu}
David~C Van~Essen, Stephen~M Smith, Deanna~M Barch, Timothy~EJ Behrens, Essa
  Yacoub, Kamil Ugurbil, Wu-Minn~HCP Consortium, et~al.
\newblock The wu-minn human connectome project: an overview.
\newblock {\em Neuroimage}, 80:62--79, 2013.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{warden2018speech}
Pete Warden.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock {\em arXiv preprint arXiv:1804.03209}, 2018.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock {OPT}: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\end{thebibliography}
