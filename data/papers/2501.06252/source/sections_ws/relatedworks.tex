\vspace{-1mm}
\section{Related works}
\label{sec:relatedworks}
\vspace{-1mm}
Self-adaptive LLMs operate at macro (multiple collaborating LLMs) and micro (internal adaptations) levels. 
Microview adaptations often use Mixture of Experts (MoE) for dynamic routing~\citep{fedus2022switch}. 
Low-rank adaptation methods like LoRA~\citep{hu2021lora} enable efficient fine-tuning. 
Recent approaches leverage SVD for LLM fine-tuning, either using minor components~\citep{wang2024milora} or top singular vectors~\citep{balazy2024lora}. 
The most related work~\citep{lingam2024svft} explores SVD-based sparsification but doesn't focus on self-adaptive LLMs or use reinforcement learning for efficiency.
We refer to Appendix~\ref{app:sec:extendedrelatedworks} for references to the wider literature.

\vspace{-2mm}