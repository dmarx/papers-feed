\section{Additional results}
\label{app:sec:additional_exp}

\subsection{Baseline Comparison to More PEFT Methods}

We conduct additional comparison studies against more parameter-efficient fine-tuning methods, including IA3\cite{liu2022few}, DORA. \cite{liu2024dora}.  
\input{tables/svf_addition_baseline}
As Table~\ref{tab:res:svf_additional_baselines} shows, \svdacro still outperforms other methods and shows promising generalized performance.

\subsection{Impact from number of few-shots}
\label{app:sec:ablation_few_shots}

\input{tables/ablation_few_shots}

We investigate the relationship between the number of samples available for few-shot adaptation and downstream performance.
Our analysis focused on the test task where \llama demonstrates the highest baseline performance, to prevent the potential for a null signal in our CEM-based search.

As Table~\ref{tab:ablation:few_shot_adaptation} shows, substantial benefits of our few-shot strategy are evident with as few as 3 to 5 test samples.
Moreover, performance appears to plateau beyond 10 samples, underscoring how our essential and inherently regularized \svdacro parameterization effectively complements self-adaptation.
This efficiency enables optimal use of data to enhance understanding of the test task.

For completeness, we have also conducted experiments with identical settings on IA$^3$~\citep{liu2022few}, another method that leverages few-shot examples.
All experiments were conducted with full batch size, a learning rate of $5 \times 10^{-5}$, with 100 and 1000 training steps.

Our results indicate that the performance of IA$^3$ on the unseen test tasks is inferior to CEM-based adaptation for all numbers of few shots considered.
We note that in our experiment, we have to considerably limit the number of optimization steps to avoid overfitting the 500,000 parameters of IA$^3$ on the few-shot samples. However, we believe overfitting might still be occurring to some degree even after only 100 steps, as also validated by the modelâ€™s perfect training accuracy on this extremely small dataset.
This limitation of fine-tuning-based adaptation highlights the superior generalization capability of our CEM-based adaptation approach in \implname.


\subsection{Cross-model svf transfer on the training tasks}
We provide complementary results to Table~\ref{tab:analysis:cross_model_main} in the main text, where we analyze the \svdacro cross-model transfer performance from training on GSM8K, MBPP-pro, and ARC-Easy to our considered test tasks. In Table~\ref{tab:analysis:cross_model_app}, we show the results in the same transfer setting this time evaluating \mistral on the same training tasks where the \llama \svdacro vectors were obtained from. Overall, we recognize a similar trend, albeit with less consistent improvement from the original model (only in 1 out of 3 tasks), but still much higher performance than the randomly shuffled baseline. These results further confirm that the canonical ordering of the \svdacro parameterization is key for cross-model transfer, highlighting once more its inherent suitability to empower self-adaptation.

\input{tables/analysis_cross_model_app}

\subsection{Training curve of LoRA and policy gradient}

Figure~\ref{app:fig:lora_learning_curves} gives the learning curves for LoRA training on the GSM8K task.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/lora_training_curve.pdf}
    \caption{\textbf{Training LoRA with policy gradient.} The dashed line shows the performance of \llama on the test split. LoRA collapses at the beginning of the training stage and fails to recover, leading to negative effects on test performance. We swept a wide range of learning rates $(2 \times 10^{-4}, 5 \times 10^{-4}, \dots, 2\times 10{-2}, 5\times 10^{-2})$, and all learning curves were similar to the one presented.}
    \label{app:fig:lora_learning_curves}
\end{figure}
