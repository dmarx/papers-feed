\begin{table}[h]
% \begin{table}[!b]

% \vspace{-2mm}
\centering
\caption{\textbf{Ablation studies.} We fine-tune \llama on the GSM8K training split with different settings and the results on the test split along with zero-shot transfer results on MATH.}

\vspace{-3.5mm}
\small
\begin{tabular}{llllccc}
\toprule

\textbf{\#} & \textbf{Method} & \textbf{Objective Function} & \textbf{Module} & \textbf{\#Params ($\downarrow$)} & \textbf{GSM8K ($\uparrow$)} & \textbf{MATH ($\uparrow$)} \\

\midrule
0 & \multicolumn{4}{c}{\textsc{LLAMA-3-8B-Instruct}} & {75.89 {\scriptsize (\grey{1.00})}} & {24.54 {\scriptsize (\grey{1.00})}} \\ 
\midrule

1 & \svdacro & Policy gradient & MLP & 0.39M & { 78.62 {\scriptsize (\green{1.04})}} & {24.20 {\scriptsize (\green{0.99})}} \\

2 & \svdacro & Policy gradient & attention & \textbf{0.16M} & {76.19 {\scriptsize (\green{1.00})}} & {24.20 {\scriptsize (\green{0.99})}} \\

3 & \svdacro & Policy gradient & MLP + attention & 0.58M & \textbf{{ 79.23 {\scriptsize (\green{1.04})}}} & \textbf{{25.04 {\scriptsize (\green{1.04)}}}} \\

4 & \svdacro & Next token pred & attention & \textbf{0.16M} & { 60.50 {\scriptsize (\green{0.80})}} & {18.52 {\scriptsize (\green{0.75})}} \\

5 & LoRA & Policy gradient & attention & 6.82M & { 57.92 {\scriptsize (\green{0.76})}} & {15.72 {\scriptsize (\green{0.64})}} \\

6 & LoRA & Next token pred & attention & 6.82M & { 77.18 {\scriptsize (\green{0.98})}} & {24.12 {\scriptsize (\green{0.96})}} \\

7 & LoRA & Next token pred & MLP + attention & 35.13M & { 75.66 {\scriptsize (\green{0.96})}} & {22.12 {\scriptsize (\green{0.91})}} \\

\bottomrule
\label{tab:res:ablation}
\end{tabular}

\vspace{-6mm}
\end{table}

