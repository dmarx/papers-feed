@misc{huggingface2023svdtraining,
  author = {Alberto Cetoli},
  title = {Fine-tuning LLMs with Singular Value Decomposition},
  howpublished = {Hugging Face Blog},
  year = {2024},
  month = {June},
  url = {https://huggingface.co/blog/fractalego/svd-training},
  note = {Accessed: 2024-07-01}
}

@article{balazy2024lora,
  title={LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters},
  author={Ba{\l}azy, Klaudia and Banaei, Mohammadreza and Aberer, Karl and Tabor, Jacek},
  journal={arXiv preprint arXiv:2405.17604},
  year={2024}
}

@article{lingam2024svft,
  title={SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors},
  author={Lingam, Vijay and Tejaswi, Atula and Vavre, Aditya and Shetty, Aneesh and Gudur, Gautham Krishna and Ghosh, Joydeep and Dimakis, Alex and Choi, Eunsol and Bojchevski, Aleksandar and Sanghavi, Sujay},
  journal={arXiv preprint arXiv:2405.19597},
  year={2024}
}

@article{sharma2023truth,
  title={The truth is in there: Improving reasoning in language models with layer-selective rank reduction},
  author={Sharma, Pratyusha and Ash, Jordan T and Misra, Dipendra},
  journal={arXiv preprint arXiv:2312.13558},
  year={2023}
}

@article{wang2024milora,
  title={MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning},
  author={Wang, Hanqing and Xiao, Zeguan and Li, Yixia and Wang, Shuo and Chen, Guanhua and Chen, Yun},
  journal={arXiv preprint arXiv:2406.09044},
  year={2024}
}

@article{zhang2023adalora,
  title={AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{kopiczko2023vera,
  title={Vera: Vector-based random matrix adaptation},
  author={Kopiczko, Dawid Jan and Blankevoort, Tijmen and Asano, Yuki Markus},
  journal={arXiv preprint arXiv:2310.11454},
  year={2023}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@article{liu2024dora,
  title={Dora: Weight-decomposed low-rank adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}

@inproceedings{nascimento2023self,
  title={Self-adaptive large language model (llm)-based multiagent systems},
  author={Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
  booktitle={2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)},
  pages={104--109},
  year={2023},
  organization={IEEE}
}

@article{zhuge2023mindstorms,
  title={Mindstorms in natural language-based societies of mind},
  author={Zhuge, Mingchen and Liu, Haozhe and Faccio, Francesco and Ashley, Dylan R and Csord{\'a}s, R{\'o}bert and Gopalakrishnan, Anand and Hamdi, Abdullah and Hammoud, Hasan Abed Al Kader and Herrmann, Vincent and Irie, Kazuki and others},
  journal={arXiv preprint arXiv:2305.17066},
  year={2023}
}

@inproceedings{NEURIPS2022_2f00ecd7,
 author = {Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Chen, zhifeng and Le, Quoc V and Laudon, James},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {7103--7114},
 publisher = {Curran Associates, Inc.},
 title = {Mixture-of-Experts with Expert Choice Routing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/2f00ecd787b432c1d36f3de9800728eb-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{du2023improving,
  title={Improving factuality and reasoning in language models through multiagent debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023}
}

@article{zhu2024llama,
  title={Llama-moe: Building mixture-of-experts from llama with continual pre-training},
  author={Zhu, Tong and Qu, Xiaoye and Dong, Daize and Ruan, Jiacheng and Tong, Jingqi and He, Conghui and Cheng, Yu},
  journal={arXiv preprint arXiv:2406.16554},
  year={2024}
}

@article{dai2024deepseekmoe,
  title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}

@inproceedings{zhang2024proagent,
  title={ProAgent: building proactive cooperative agents with large language models},
  author={Zhang, Ceyao and Yang, Kaijie and Hu, Siyi and Wang, Zihao and Li, Guanghe and Sun, Yihang and Zhang, Cheng and Zhang, Zhaowei and Liu, Anji and Zhu, Song-Chun and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17591--17599},
  year={2024}
}

@misc{ICML2024_MoE,
title={Mixture-of-Experts in the Era of LLMs: A New Odyssey},
author={Tianlong, Chen and Yu, Cheng and Beidi, Chen and Minjia, Zhang and Mohit, Bansal},
year={2024},
howpublished={ICML 2024 presentation slides},
note={International Conference on Machine Learning (ICML)}
}

@misc{qwen_team_2024,
title = {Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters},
author = {{Qwen Team}},
year = {2024},
month = {March},
url = {https://qwenlm.github.io/blog/qwen-moe/},
note = {Blog post},
publisher = {Qwen}
}

@inproceedings{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International conference on machine learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}

@article{loose2017switch,
  title={Switch-independent task representations in frontal and parietal cortex},
  author={Loose, Lasse S and Wisniewski, David and Rusconi, Marco and Goschke, Thomas and Haynes, John-Dylan},
  journal={Journal of Neuroscience},
  volume={37},
  number={33},
  pages={8033--8042},
  year={2017},
  publisher={Soc Neuroscience}
}

@article{davison2015brain,
  title={Brain network adaptability across task states},
  author={Davison, Elizabeth N and Schlesinger, Kimberly J and Bassett, Danielle S and Lynall, Mary-Ellen and Miller, Michael B and Grafton, Scott T and Carlson, Jean M},
  journal={PLoS computational biology},
  volume={11},
  number={1},
  pages={e1004029},
  year={2015},
  publisher={Public Library of Science San Francisco, USA}
}

@article{kang2024self,
  title={Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts},
  author={Kang, Junmo and Karlinsky, Leonid and Luo, Hongyin and Wang, Zhen and Hansen, Jacob and Glass, James and Cox, David and Panda, Rameswar and Feris, Rogerio and Ritter, Alan},
  journal={arXiv preprint arXiv:2406.12034},
  year={2024}
}

@book{rubinstein2004cross,
  title={The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation, and machine learning},
  author={Rubinstein, Reuven Y and Kroese, Dirk P},
  volume={133},
  year={2004},
  publisher={Springer}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
  pages={3195--3204},
  year={2019}
}

@inproceedings{yu2024language,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{goddard2024arcee,
  title={Arcee's MergeKit: A Toolkit for Merging Large Language Models},
  author={Goddard, Charles and Siriwardhana, Shamane and Ehghaghi, Malikeh and Meyers, Luke and Karpukhin, Vlad and Benedict, Brian and McQuade, Mark and Solawetz, Jacob},
  journal={arXiv preprint arXiv:2403.13257},
  year={2024}
}

@article{akiba2024evolutionary,
  title={Evolutionary optimization of model merging recipes},
  author={Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David},
  journal={arXiv preprint arXiv:2403.13187},
  year={2024}
}

# self adaptation system
@INPROCEEDINGS{7302492,
  author={Klös, Verena and Göthel, Thomas and Glesner, Sabine},
  booktitle={2015 41st Euromicro Conference on Software Engineering and Advanced Applications}, 
  title={Adaptive Knowledge Bases in Self-Adaptive System Design}, 
  year={2015},
  volume={},
  number={},
  pages={472-478},
  keywords={Adaptation models;Adaptive systems;Knowledge based systems;Analytical models;Topology;Data models;Temperature sensors;self-adaptive systems;modelling;feedback loop;knowledge base;adaptation rules;run-time models},
  doi={10.1109/SEAA.2015.48}}

