\vspace{-1mm}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-3mm}

In this paper, we introduced \implname, providing a novel blueprint toward realizing self-adaptive LLMs.
Within this framework, we first proposed \svdacro, offering superior performance than prior fine-tuning recipes, together with reduced costs, high compositionality, and overfitting regularization -- all crucial properties to achieve scalable self-adaptation.
Leveraging a set of \svdacro experts as building blocks, we developed three effective strategies for self-adaptation, each offering unique benefits and monotonic performance benefits with increasing access to the test-time conditions.

While \implname demonstrates promising results, there remain exciting opportunities for future work.
One limitation is that the capabilities of \svdacro experts are tied to the latent components of the base model.
To address this, model merging offers a promising direction~\citep{yu2024language,goddard2024arcee,akiba2024evolutionary}, enabling specialized models to be combined into a single, more capable model.
Additionally, while our CEM-based adaptation effectively balances performance and efficiency, scaling to a large number of specialized domains may introduce increased one-time computational costs.
However, this trade-off is offset by the benefits of improved performance and enhanced self-adaptation capabilities.
Advances in model merging and efficient adaptation techniques have produced models dominating open leaderboards, making them strong candidates as base models for \implname and opening new possibilities for adaptive LLMs.
