\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akiba et~al.(2024)Akiba, Shing, Tang, Sun, and Ha]{akiba2024evolutionary}
Takuya Akiba, Makoto Shing, Yujin Tang, Qi~Sun, and David Ha.
\newblock Evolutionary optimization of model merging recipes.
\newblock \emph{arXiv preprint arXiv:2403.13187}, 2024.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Ba{\l}azy et~al.(2024)Ba{\l}azy, Banaei, Aberer, and Tabor]{balazy2024lora}
Klaudia Ba{\l}azy, Mohammadreza Banaei, Karl Aberer, and Jacek Tabor.
\newblock Lora-xs: Low-rank adaptation with extremely small number of parameters.
\newblock \emph{arXiv preprint arXiv:2405.17604}, 2024.

\bibitem[Brown(2020)]{brown2020language}
Tom~B Brown.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Cetoli(2024)]{huggingface2023svdtraining}
Alberto Cetoli.
\newblock Fine-tuning llms with singular value decomposition.
\newblock Hugging Face Blog, June 2024.
\newblock URL \url{https://huggingface.co/blog/fractalego/svd-training}.
\newblock Accessed: 2024-07-01.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Davison et~al.(2015)Davison, Schlesinger, Bassett, Lynall, Miller, Grafton, and Carlson]{davison2015brain}
Elizabeth~N Davison, Kimberly~J Schlesinger, Danielle~S Bassett, Mary-Ellen Lynall, Michael~B Miller, Scott~T Grafton, and Jean~M Carlson.
\newblock Brain network adaptability across task states.
\newblock \emph{PLoS computational biology}, 11\penalty0 (1):\penalty0 e1004029, 2015.

\bibitem[Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and Mordatch]{du2023improving}
Yilun Du, Shuang Li, Antonio Torralba, Joshua~B Tenenbaum, and Igor Mordatch.
\newblock Improving factuality and reasoning in language models through multiagent debate.
\newblock \emph{arXiv preprint arXiv:2305.14325}, 2023.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (120):\penalty0 1--39, 2022.

\bibitem[Goddard et~al.(2024)Goddard, Siriwardhana, Ehghaghi, Meyers, Karpukhin, Benedict, McQuade, and Solawetz]{goddard2024arcee}
Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz.
\newblock Arcee's mergekit: A toolkit for merging large language models.
\newblock \emph{arXiv preprint arXiv:2403.13257}, 2024.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{arXiv preprint arXiv:2103.03874}, 2021.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kang et~al.(2024)Kang, Karlinsky, Luo, Wang, Hansen, Glass, Cox, Panda, Feris, and Ritter]{kang2024self}
Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, and Alan Ritter.
\newblock Self-moe: Towards compositional large language models with self-specialized experts.
\newblock \emph{arXiv preprint arXiv:2406.12034}, 2024.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Klös et~al.(2015)Klös, Göthel, and Glesner]{7302492}
Verena Klös, Thomas Göthel, and Sabine Glesner.
\newblock Adaptive knowledge bases in self-adaptive system design.
\newblock In \emph{2015 41st Euromicro Conference on Software Engineering and Advanced Applications}, pp.\  472--478, 2015.
\newblock \doi{10.1109/SEAA.2015.48}.

\bibitem[Kopiczko et~al.(2023)Kopiczko, Blankevoort, and Asano]{kopiczko2023vera}
Dawid~Jan Kopiczko, Tijmen Blankevoort, and Yuki~Markus Asano.
\newblock Vera: Vector-based random matrix adaptation.
\newblock \emph{arXiv preprint arXiv:2310.11454}, 2023.

\bibitem[Lingam et~al.(2024)Lingam, Tejaswi, Vavre, Shetty, Gudur, Ghosh, Dimakis, Choi, Bojchevski, and Sanghavi]{lingam2024svft}
Vijay Lingam, Atula Tejaswi, Aditya Vavre, Aneesh Shetty, Gautham~Krishna Gudur, Joydeep Ghosh, Alex Dimakis, Eunsol Choi, Aleksandar Bojchevski, and Sujay Sanghavi.
\newblock Svft: Parameter-efficient fine-tuning with singular vectors.
\newblock \emph{arXiv preprint arXiv:2405.19597}, 2024.

\bibitem[Liu et~al.(2022)Liu, Tam, Muqeeth, Mohta, Huang, Bansal, and Raffel]{liu2022few}
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin~A Raffel.
\newblock Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 1950--1965, 2022.

\bibitem[Liu et~al.(2024)Liu, Wang, Yin, Molchanov, Wang, Cheng, and Chen]{liu2024dora}
Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang~Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen.
\newblock Dora: Weight-decomposed low-rank adaptation.
\newblock \emph{arXiv preprint arXiv:2402.09353}, 2024.

\bibitem[Loose et~al.(2017)Loose, Wisniewski, Rusconi, Goschke, and Haynes]{loose2017switch}
Lasse~S Loose, David Wisniewski, Marco Rusconi, Thomas Goschke, and John-Dylan Haynes.
\newblock Switch-independent task representations in frontal and parietal cortex.
\newblock \emph{Journal of Neuroscience}, 37\penalty0 (33):\penalty0 8033--8042, 2017.

\bibitem[Marino et~al.(2019)Marino, Rastegari, Farhadi, and Mottaghi]{marino2019ok}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external knowledge.
\newblock In \emph{Proceedings of the IEEE/cvf conference on computer vision and pattern recognition}, pp.\  3195--3204, 2019.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[{Qwen Team}(2024)]{qwen_team_2024}
{Qwen Team}.
\newblock Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters, March 2024.
\newblock URL \url{https://qwenlm.github.io/blog/qwen-moe/}.
\newblock Blog post.

\bibitem[Rajbhandari et~al.(2022)Rajbhandari, Li, Yao, Zhang, Aminabadi, Awan, Rasley, and He]{rajbhandari2022deepspeed}
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza~Yazdani Aminabadi, Ammar~Ahmad Awan, Jeff Rasley, and Yuxiong He.
\newblock Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale.
\newblock In \emph{International conference on machine learning}, pp.\  18332--18346. PMLR, 2022.

\bibitem[Rubinstein \& Kroese(2004)Rubinstein and Kroese]{rubinstein2004cross}
Reuven~Y Rubinstein and Dirk~P Kroese.
\newblock \emph{The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation, and machine learning}, volume 133.
\newblock Springer, 2004.

\bibitem[Sharma et~al.(2023)Sharma, Ash, and Misra]{sharma2023truth}
Pratyusha Sharma, Jordan~T Ash, and Dipendra Misra.
\newblock The truth is in there: Improving reasoning in language models with layer-selective rank reduction.
\newblock \emph{arXiv preprint arXiv:2312.13558}, 2023.

\bibitem[Singh et~al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh, and Rohrbach]{singh2019towards}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  8317--8326, 2019.

\bibitem[Tianlong et~al.(2024)Tianlong, Yu, Beidi, Minjia, and Mohit]{ICML2024_MoE}
Chen Tianlong, Cheng Yu, Chen Beidi, Zhang Minjia, and Bansal Mohit.
\newblock Mixture-of-experts in the era of llms: A new odyssey.
\newblock ICML 2024 presentation slides, 2024.
\newblock International Conference on Machine Learning (ICML).

\bibitem[Wang et~al.(2024)Wang, Xiao, Li, Wang, Chen, and Chen]{wang2024milora}
Hanqing Wang, Zeguan Xiao, Yixia Li, Shuo Wang, Guanhua Chen, and Yun Chen.
\newblock Milora: Harnessing minor singular components for parameter-efficient llm finetuning.
\newblock \emph{arXiv preprint arXiv:2406.09044}, 2024.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock \emph{Machine learning}, 8:\penalty0 229--256, 1992.

\bibitem[Yu et~al.(2024)Yu, Yu, Yu, Huang, and Li]{yu2024language}
Le~Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li.
\newblock Language models are super mario: Absorbing abilities from homologous models as a free lunch.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Zhang et~al.(2024)Zhang, Yang, Hu, Wang, Li, Sun, Zhang, Zhang, Liu, Zhu, et~al.]{zhang2024proagent}
Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, et~al.
\newblock Proagent: building proactive cooperative agents with large language models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pp.\  17591--17599, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Bukharin, Karampatziakis, He, Cheng, Chen, and Zhao]{zhang2023adalora}
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu~Cheng, Weizhu Chen, and Tuo Zhao.
\newblock Adalora: Adaptive budget allocation for parameter-efficient fine-tuning.
\newblock \emph{arXiv preprint arXiv:2303.10512}, 2023.

\bibitem[Zhu et~al.(2024)Zhu, Qu, Dong, Ruan, Tong, He, and Cheng]{zhu2024llama}
Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu~Cheng.
\newblock Llama-moe: Building mixture-of-experts from llama with continual pre-training.
\newblock \emph{arXiv preprint arXiv:2406.16554}, 2024.

\bibitem[Zhuge et~al.(2023)Zhuge, Liu, Faccio, Ashley, Csord{\'a}s, Gopalakrishnan, Hamdi, Hammoud, Herrmann, Irie, et~al.]{zhuge2023mindstorms}
Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan~R Ashley, R{\'o}bert Csord{\'a}s, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al~Kader Hammoud, Vincent Herrmann, Kazuki Irie, et~al.
\newblock Mindstorms in natural language-based societies of mind.
\newblock \emph{arXiv preprint arXiv:2305.17066}, 2023.

\end{thebibliography}
