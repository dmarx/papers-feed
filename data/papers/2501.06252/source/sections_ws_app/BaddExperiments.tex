\section{Additional results}
\label{app:sec:additional_exp}


\subsection{Ablation studies}
\label{app:sec:ablation_studies}

\textit{Module sensitivity:} We first compare the performance of \svdacro when it is applied to different modules (see trials 1-3).
Under consistent conditions, both individual MLP and attention updates improve performance, with MLP updates resulting in more pronounced gains.
Simultaneous updates to both module types yield even more significant enhancements.

\textit{Objective function:} We are interested in the performance impact from different objective functions, and we compare the RL objective with next-token prediction loss (see trials 2 and 4).
For the latter, we use instruction fine-tuning with official GSM8K solutions as target tokens.
Results show clear performance gains with RL, demonstrating its effectiveness in task-specific fine-tuning.
Conversely, next-token prediction even hinders performance.
This highlights RL's ability to handle cases lacking detailed solutions, suggesting its superiority in this context.

\textit{\svdacro vs LoRA:} Finally, we also evaluate LoRA using the RL objective (see trials 2 and 5).
A significant performance disparity is observed, primarily attributed to the severe instability of the LoRA training process.
Despite exploring a wide range of learning rates, LoRA's performance consistently lagged behind.
For further illustrations, see Figure~\ref{app:fig:lora_learning_curves} in the appendix.

\input{tables_ws/table_ablation}

\subsection{Impact from number of few-shots}
\label{app:sec:ablation_few_shots}

We investigate the relationship between the number of samples available for few-shot adaptation and downstream performance.
Our analysis focused on the test task where \llama demonstrates the highest baseline performance, to prevent the potential for a null signal in our CEM-based search.

\input{tables_ws/ablation_few_shots}

As Table~\ref{tab:ablation:few_shot_adaptation} shows, substantial benefits of our few-shot strategy are evident with as few as 3 to 5 test samples.
Moreover, performance appears to plateau beyond 10 samples, underscoring how our essential and inherently regularized \svdacro parameterization effectively complements self-adaptation.
This efficiency enables optimal use of data to enhance understanding of the test task.

\subsection{Cross-model svf transfer on the training tasks}
We provide complementary results to Table~\ref{tab:analysis:cross_model_main} in the main text, where we analyze the \svdacro cross-model transfer performance from training on GSM8K, MBPP-pro, and ARC-Easy to our considered test tasks. In Table~\ref{tab:analysis:cross_model_app}, we show the results in the same transfer setting this time evaluating \mistral on the same training tasks where the \llama \svdacro vectors were obtained from. Overall, we recognize a similar trend, albeit with less consistent improvement from the original model (only in 1 out of 3 tasks), but still much higher performance than the randomly shuffled baseline. These results further confirm that the canonical ordering of the \svdacro parameterization is key for cross-model transfer, highlighting once more its inherent suitability to empower self-adaptation.

\input{tables_ws/analysis_cross_model_app}

\subsection{Training curve of LoRA and policy gradient}

Figure~\ref{app:fig:lora_learning_curves} gives the learning curves for LoRA training on the GSM8K task.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/lora_training_curve.pdf}
    \caption{\textbf{Training LoRA with policy gradient.} The dashed line shows the performance of \llama on the test split. LoRA collapses at the beginning of the training stage and fails to recover, leading to negative effects on test performance. We swept a wide range of learning rates $(2 \times 10^{-4}, 5 \times 10^{-4}, \dots, 2\times 10{-2}, 5\times 10^{-2})$, and all learning curves were similar to the one presented.}
    \label{app:fig:lora_learning_curves}
\end{figure}
