
\newpage
\section{Extended Related works}
\label{app:sec:extendedrelatedworks}


\textbf{Self-adaptive LLMs} We define self-adaptive LLMs as a group of LLMs or a standalone LLM that can evaluate and modify its behavior in response to changes in its operating environment or internal state, without external intervention.
This adaptation can be explored from two perspectives: a macroview, where multiple LLMs collaborate and/or compete, and a microview, where internal adaptations allow a single LLM to specialize in different tasks.

\textit{Macroview:} From this perspective, the system directs queries to LLMs with domain specific expertise, prioritizing outputs from expert models, thereby achieving higher accuracy and task-specific optimization.
Such task-specific ensembles can be realized through various mechanisms: multiple LLMs playing distinct roles and coordinate toward a shared goal~\citep{zhuge2023mindstorms}, engaging in mutual listening and debate~\citep{du2023improving}, or using meticulously crafted prompt constructions~\citep{zhang2024proagent} to integrate knowledge library and skill planning.
Naturally, the improvement in the specialization and adaptive capabilities of individual LLMs in the ensemble enhances the collective performance.
Thus, in this paper, we focus on the microview of self-adaptive LLMs.

\textit{Microview:} MoE in LLMs plays a critical role in this perspective~\citep{ICML2024_MoE}.
In MoE systems, inputs are dynamically routed to a subset of specialized modules or layers (e.g., MLPs) containing domain-specific knowledge~\citep{rajbhandari2022deepspeed,fedus2022switch}.
To reduce inference time, researchers introduce sparsely activated MoE where only a subset of the experts are selected per token~\cite{jiang2024mixtral,qwen_team_2024}.
While it is possible to view \implname loosely as a type of MoE, there are two major differences.
In the aforementioned systems, self-adaptation is achieved through token-level routing, whereas \implname employs a sample-level module selection strategy.
The second difference lies in the construction of expert modules.
In traditional MoE systems, expert modules are either trained from scratch~\citep{fedus2022switch,jiang2024mixtral} or dense models (e.g., upcycling)~\citep{qwen_team_2024,zhu2024llama}, without an auxiliary loss to ensure module specialization.
In contrast, \implname specifically trains expert vectors with RL to acquire domain specific-knowledge, making them true experts.


\textbf{Low-rank adaptation}
PEFT methods such as LoRA~\citep{hu2021lora} works by freezing the original model's parameters and introducing small trainable low-rank matrices for task-specific updates.
It significantly lowers the computational and memory costs while providing performance comparable to full fine-tuning.
Inspired by LoRA's design, various modifications have been proposed~\citep{zhang2023adalora,kopiczko2023vera,liu2024dora,balazy2024lora,huggingface2023svdtraining}.
\implname does not rely on low-rank matrices, and instead scales the singular vectors of the original parameter matrix that span the full rank space.


\textbf{SVD for LLM Fine-tuning}
SVD is increasingly being used as an inductive bias for PEFT in LLMs.
For example, \citet{wang2024milora} decompose a weight matrix and use the minor singular components, associated with noisy or long-tail information, to initialize low-rank matrices for LoRA fine-tuning.
In a similar vein, SVD is employed to approximate an original weight matrix with the top $r$ singular vectors, corresponding to the highest singular values.
A small trainable matrix is then introduced on top of the truncated singular value matrix to adjust the magnitude and orientations within this top-$r$ subspace~\citep{balazy2024lora,huggingface2023svdtraining}.
However, the drawback of this approach is that retaining only the top singular components can result in the loss of important information, particularly when the singular values distribution is less skewed.
The work most similar to ours is a concurrent effort by \citet{lingam2024svft}, where they introduce various sparsification methods that utilize the SVD of the weights.
However, it is not for self-adaptive LLMs and does not use RL to enhance learning efficiency.