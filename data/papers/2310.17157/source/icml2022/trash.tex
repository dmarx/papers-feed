

\iffalse
\begin{lemma}
A better bound 
\begin{align*}
(1-\epsilon) \cdot \| x \|_2 \leq \| f(x) \|_2 \leq (1+\epsilon) \cdot \| x \|_2
\end{align*}
\end{lemma}

What assumptions are we allowed to make
\begin{itemize}
    \item $W$,
    \item $K$
    \item $V$
    \item In practice, for big model $d = 10^4$ vs $s < 2000$, this means $d \geq s$
\end{itemize}
Where do we use ReLU?
\fi


\iffalse
\begin{definition}[Approximate Nearest Neighbor ($\ann$)]\label{def:app_nearest_neighbor}
Let $\ov{c} >1$ and $r \in (0,2)$ denote two parameters.  Given an $n$-vector set $Y \subset \mathbb{S}^{d-1}$ on a unit sphere, the objective of the $(\ov{c},r)$-Approximate Nearest Neighbor ($\ann$) is to construct a data structure that, for any query $x \in \mathbb{S}^{d-1}$ such that $\min_{y\in Y}\| y - x \|_2 \leq r$, it returns a vector $z$ from $Y$ that satisfies $\| z - x \|_2 \leq \ov{c} \cdot r$.
\end{definition}

In the iterative-type optimization algorithm, the cost per iteration could be dominated by the Approximate $\maxip$ problem (Definition~\ref{def:app_maxip}), which is the dual problem of the $(\ov{c},r)$-$\ann$.

\begin{definition}[Approximate $\maxip$]\label{def:app_maxip}
Let $c \in (0,1)$ and $\tau \in (0,1)$ denote two parameters.
Given an $n$-vector dataset $Y \subset \mathbb{S}^{d-1}$ on a unit sphere, the objective of the $(c,\tau)$-{$\maxip$} is to construct a data structure that, given a query $x \in \mathbb{S}^{d-1}$ such that $\max_{y\in Y}\langle x , y \rangle \geq \tau$, it retrieves a vector $z$ from $Y$ that satisfies $\langle x , z \rangle \geq c \cdot \max_{y \in Y} \langle x,y \rangle$.
\end{definition}

Next, we present the the primal-dual connection between $\ann$ and approximate $\maxip$. Given to unit vectors $x,y\in \R^d$ with both norm equal to $1$, $\|x-y\|_2^2 = 2 - 2\langle x , y \rangle$. Therefore, we could maximizing  $\langle x , y \rangle$ by minimizing $\|x-y\|_2^2$. Based on this connection, we present how to solve $(c,\tau)$-{$\maxip$} using $(\ov{c},r)$-{$\ann$}.  We start with showing how to solve $(\ov{c},r)$-{$\ann$} with {\lsh}.


\begin{theorem}[Andoni, Laarhoven, Razenshteyn and Waingarten~\cite{alrw17}]\label{thm:alrw17}
Let $\ov{c} > 1$ and $r \in (0,2)$ denote two parameters. One can solve $(\ov{c},r)$-$\ann$ on a unit sphere in query time $O(d \cdot n^{\rho})$ using preprocessing time $O(dn^{1+o(1)})$ and space $O(n^{1+o(1)} + d n)$, where $\rho = \frac{2}{\ov{c}^2} -\frac{1}{\ov{c}^4}+o(1)$.
\end{theorem}

Next, we solve $(c,\tau)$-{$\maxip$} by solving  $(\ov{c},r)$-{$\ann$} using Theorem~\ref{thm:alrw17}. We have 

\begin{corollary}[An informal statement of Corollary~\ref{coro:maxip_lsh_formal}]\label{coro:maxip_lsh_informal}
Let $c \in (0,1)$ and $\tau \in (0,1)$ denote two parameters. One can solve $(c,\tau)$-{$\maxip$} on a unit sphere $\mathcal{S}^{d-1}$ in query time $O(d \cdot n^{\rho})$, where $\rho\in(0,1)$, using {\lsh} with both preprocessing time and space in $O(d n^{1+o(1)})$.
\end{corollary}

In our work, we consider a generalized form of approximate $\maxip$, denoted as projected approximate $\maxip$.

\begin{definition}[Projected approximate $\maxip$]\label{def:proj_approximate_maxip_informal}
Let $\phi, \psi: \R^d \rightarrow \R^k$ denote two transforms. Given an $n$-vector dataset $Y \subset \R^d $ so that $\psi(Y) \subset \mathbb{S}^{k-1}$, the goal of the $(c,\phi, \psi,\tau)$-{$\maxip$} is to construct a data structure that, given a query $x\in \R^d$ and $\phi(x) \in \mathbb{S}^{k-1}$ such that $\max_{y\in Y}\langle \phi(x) , \psi(y) \rangle \geq \tau$, it retrieves a vector $z \in Y$ that satisfies $\langle \phi(x) , \psi(z) \rangle \geq c \cdot (\phi, \psi)\text{-}\maxip (x,Y)$.
\end{definition}

For details of space-time trade-offs, please refer to Section~\ref{sec:data_structure}. The following sections show how to use projected approximate $\maxip$ to accelerate the optimization algorithm by reducing the cost per iteration.
\fi

\iffalse
\begin{definition}[Regularized $\minip$]\label{def:reg_minip} 
Given a data set $Y\subseteq \R^d$ and a point $x\in\R^d$. Let $\phi:\R^d\rightarrow \R^d$ denote a mapping. Given a constant $\alpha$, we define regularized $\minip$ as follows:
\begin{align*}
    (\phi,\alpha)\text{-}\minip (x,Y) := \min_{y \in Y} \langle y-x,\phi(x) \rangle + \alpha\|x-y\|.
\end{align*}
\end{definition}
\fi




\iffalse
\Zhao{I don't think we need the following things}
\subsection{Definitions and  Properties for Optimization}

We start with listing definitions for optimization.
\begin{definition}[Convex hull and its diameter]\label{def:cvx_hull}
Given a set $A=\{x_i\}_{i\in [n]} \subset \R^d$, we define its convex hull $\mathcal{B}(A)$ to be the collection of all finite linear combinations $y$ that satisfies  $ y=\sum_{i\in [n]} a_i\cdot x_i$ where $a_i\in [0,1]$ for all $i\in[n]$ and $\sum_{i\in [n]}a_i= 1$.  Let $D_{\max}$ denote the maximum square of diameter of ${\cal B}(A)$ so that $\|x-y\|_2\leq D_{\max}$ for all $(x,y)\in {\cal B}(A)$.
\end{definition}




\begin{definition}[Smoothness]\label{def:smooth}
We say $L$ is $\beta$-smooth if 
\begin{align*}
L(y)\leq L(x)+\langle \nabla L(x),y-x \rangle+\frac{\beta}{2}\| y-x \|^2_2
\end{align*}
\end{definition}



\begin{definition}[Convex]\label{def:convex}
We say function $L$ is convex if 
\begin{align*}
L(x)\geq L(y)+\langle \nabla L(y),x-y \rangle
\end{align*}
\end{definition}



Next, we list properties for optimization.
\begin{corollary}\label{coro:hull_maxip}
For a set $A=\{x_i\}_{i\in [n]} \subset \R^d$, and its convex hull $\mathcal{B}(A)$, given a query $q\in \R^d$, if $x^*=\arg\max_{x\in A} q^\top x$. Then, $q^\top y \leq q^\top x^*$ for all $y\in \mathcal{B}(A)$.
\end{corollary}
\begin{proof}
We can upper bound $q^\top y$ as follows:
\begin{align*}
    q^\top y 
    =&~q^\top (\sum_{i\in [n]}a_i\cdot x_i)  \\
    = &~ \sum_{i\in [n]}a_i\cdot q^\top x_i\\
    \leq &~ \sum_{i\in [n]}a_i\cdot q^\top x^*\\
    \leq &~ q^\top x^*
\end{align*}
where the first step follows from the definition of convex hull in Definition~\ref{def:cvx_hull}, the second step is an reorganization, the third step follows the fact that $a_i\in [0,1]$ for all $i\in[n]$ and $q^\top x_i \leq q^\top x^*$ for all $x_i\in A$, the last step follows that $\sum_{i\in [n]}a_i= 1$.
\end{proof}




\begin{lemma}[$\maxip$ condition] \label{lemma:max_ip_condition}
Let $g : \R^d \rightarrow \R$ denote a convex function. Let $S \subset \R^d$ denote a set of points. Given a vector $x\in {\cal B}(S)$, we have
\begin{align*}
\min_{s \in S } \langle \nabla g(x) ,  s-x \rangle  \leq 0, ~~~ \forall x\in \cal{B}(S).
\end{align*}
\end{lemma}

\begin{proof}
Let $s_{\min}=\arg\min_{s\in S} \langle \nabla g(x) ,  s \rangle $. Then, we upper bound $\langle \nabla g(x) ,  s_{\min}-x \rangle$ as
\begin{align}\label{eq:maxip_s_0}
    \langle \nabla g(x) ,  s_{\min}-x \rangle =&~\langle \nabla g(x) ,  s_{\min}-\sum_{s\in S} a_i\cdot s \rangle\notag\\
    \leq &~\langle \nabla g(x) ,  \sum_{s\in S}a_i( s_{\min}- s_i) \rangle \notag\\
    = &~\sum_{s_i\in S}a_i \langle \nabla g(x) ,   s_{\min}- s_i \rangle \notag\\
    = &~\sum_{s_i\in S}a_i (\langle \nabla g(x) ,   s_{\min} \rangle -\langle \nabla g(x) ,   s_i \rangle )\notag\\
    \leq&~ 0
\end{align}
where the first step follows from the definition of convex hull in Definition~\ref{def:cvx_hull}, the second and third steps are reorganizations, the final steps follows that $\langle \nabla g(x) ,   s_0 \rangle \leq \langle \nabla g(x) ,   s \rangle$ for all $s\in S$.

Next, we upper bound $\min_{s \in S } \langle \nabla g(x) ,  s-x \rangle  \leq 0, ~~~ \forall x\in \cal{B}(S)$ as
\begin{align*}
    \min_{s \in S } \langle \nabla g(x) ,  s-x \rangle  \leq  \langle \nabla g(x) ,  s_0-x \rangle  \leq 0
\end{align*}
where the first step follows from the definition of function $\min$ and the second step follows from Eq~\eqref{eq:maxip_s_0}.
\end{proof}
\fi 