\section{Conclusion}

Our main goal is to make LLM inference efficient so that their powerful in-context learning abilities can be used in more application domains.
We observe that contextual sparsity can be accurately predicted with lightweight learning-based algorithms. This motivated us to design \name{} that uses asynchronous lookahead predictors and hardware-efficient sparsity to speed up LLM inference in wall-clock time. Our encouraging empirical results validate that contextual sparsity can reduce inference latency by over 2$\times$ compared to the state-of-the-art FasterTransformer without model quality drops. 
Our method is a step towards making LLMs more accessible to the general community, which could unlock exciting new AI applications.



\section*{Acknowledgements}
We would like to thank Ryan Spring, Laurel Orr, Guangxuan Xiao, Eric Han, Xun Huang, Daniel Y. Fu, Benjamin Spector, Ruan Silva, Diana Liskovich, and the anonymous reviewers for helpful discussions and feedback. We acknowledge the generous support by Together Computer, which enabled the necessary partial computations in this work.