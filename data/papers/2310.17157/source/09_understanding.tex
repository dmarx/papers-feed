\section{Self-attention layer as a clustering algorithm}
\label{sec:clustering understanding}
The self-attention layer in the Transformer looks like mean-shift clustering. Suppose $\{(\vx_j, \vv_j)\}$ are a bunch of key and value pairs and $\vq$ is the query. Note that $\vq=W_q\vx$, $\vk=W_k \vx$ and $\vv=W_v \vx$ are computed by three projection matrices $W_k$, $W_q$ and $W_v$ from a common $\vx$. Then from self-attention we have:
%~\cite{katharopoulos2020transformers}:
\begin{equation}
    \vv = \sum_j p_j \vv_j = \frac{\sum_j \exp(\vx^\t W_q^\t W_k \vx_j) W_v \vx_j}{\sum_j \exp(\vx^\t W_q^\t W_k \vx_j)} = W_v \frac{\sum_j \exp(\vx^\t W_q^\t W_k \vx_j) \vx_j}{\sum_j \exp(\vx^\t W_q^\t W_k \vx_j)} 
\end{equation}
where $\sim(\vq, \vk_j) := \exp(\vq^\t \vk_j) = \exp(\vx^\t W_q^\t W_k\vx_j)$ and $p_j = \sim(\vq, \vk_j) / \sum_j \sim(\vq, \vk_j)$. 

On the other hand, mean-shift clustering looks like the following:
\begin{equation}
    m(\vx) = \frac{\sum_j K(\vx_j,\vx) \vx_j}{\sum_j K(\vx_j,\vx)}
\end{equation}
where $K(\vx_j,\vx)$ is a kernel matrix that measure the similarity between $\vx_j$ and $\vx$. According to the mean-shift algorithm, in the next iteration, we will simply replace $\vx$ with $m(\vx)$.

So in some sense, self-attention is just to do some kind of clustering for the input embedding $\vq$ and $\vk$, plus a transformation of the embedding to another place. The term ``projection'' is due to the fact that there is a projection matrix $W_v$ on $\vx$ for the next level.  

% Note that if $W_q = W_k$ then we have a positive definite kernel $\exp(\vx^\t W_q^\t W_q \vx_j)$. {\exper{Not sure whether this would help the performance or not?}}

\textbf{Residue connection and LayerNorm}. Compared to mean-shift, Transformer layer has residue connection. Therefore, for single-headed attention, what you actually get is $\vv + \vx$, followed by a LayerNorm. 
% If all input $\vx$ are zero-mean vector, and $W_v$ has zero-mean columns (i.e., $\vone^\t W_v = 0$), then $\vv$ is also zero-mean and LayerNorm reduces to just $\ell_2$ normalization. {\exper{So this suggests that we can reduce the number of parameter for $W_v$? Also if $\vx$ is zero-mean, then $W_q \vx$ is the same as $(W_q+ \vu\vone^\t)\vx$ for any $\vu$, so we could further remove $1$ parameter for each row of $W_q$ and $W_k$.}} Note that RMSNorm~\cite{zhang2019root} shows that the zero-mean property in LayerNorm is not necessarily useful.  
For the residue connection, the mean-shift analog already shows the output $m(\vx)$ contains $\vx+$ part. The reason why we need residue connection is that the self-attention part might only model the ``change'' of $\vx$ in the mean-shift picture, rather than the full update of $\vx$.
% As a result, maybe we don't need the similarity (or the kernel) to be positive-definite in the self-attention layer?

\section{The role of self-attention}
Consider we have a vocabulary of size $m$ and $d$ dimensional embedding space. In practice, many papers in NLP have reported clustering behaviors of word embeddings: such a clustering of word embedding naturally occurs after training.  

% Why that's the case? Why these embedding needs to be clustered towards each other? 
An explanation for the above phenomenon is that, by grouping these word embedding together, we might generalize better, since similarity in word now can transfer (e.g., A linked to B, B linked to C, then A might link to C as well) and generalization follows. 

Let's treat it as a fact and focus on how this is achieved and how self-attention plays a role here.  

\subsection{The capacity of embedding layer}
First let us take a look at the following pairwise distance constraints between word embedding (e.g., some words should be close to each other, some should be far away from each other) as the following:
\begin{equation}
    \|\vx_i - \vx_j\| = D(i, j) \label{eq:mds}
\end{equation}
where $D(i,j)$ is large for $i$ and $j$ that should be far apart and $D(i,j)$ is small for $i$ and $j$ that are close to each other. In visualization, this is called Multidimensional Scaling (MDS)~\cite{cox2008multidimensional}. 

Note that in neural network training, the constraint (Eqn.~\ref{eq:mds}) is not directly enforced during training, but the clustering naturally happens. Since we talk about capacity, how we achieve Eqn.~\ref{eq:mds} doesn't matter for now. 

In general we cannot find a \emph{fixed} low-dimensional embedding ($d \ll m$) to satisfy these constraints, since we only have $md$ parameters ($m$ vectors, each has $d$ entries), but $m^2$ constraint. So two vectors that are supposed to be close may not be close enough (but hopefully they remain close to each other). 

\subsection{The role of self-attention}
For this, the self-attention mechanism comes to the rescue, trading model-size with additional computation. It fulfills what (static) embedding cannot achieve: to further group the embedding vectors together in a multi-layer structure.

Note that one sentence never covers all $d$ vocabularies. Once the words in the sentence are picked, they are grouped together via self-attention layers to collectively represent a concept that can be useful for the task. 

\subsection{How the clustering happens through self-attention?}
Now one fundamental questions arise: How the static clustering of embedding happens during end-to-end training? In practice, no one explicitly enforces the MDS constraint (Eqn.~\ref{eq:mds}). 

Let's start with a simple example. we have two unit embedding: $\vx$ and $\vy$ with the normalization condition that $\|\vx\|_2 = 1$ and $\|\vy\|_2 = 1$, and a simple self-attention layer (without projection) which output $\vz$:
\begin{equation}
    \vz = (1 - p)\vx + p \vy 
\end{equation}
Where the attention map is:
\begin{equation}
    p = \frac{e^{\vx^\t\vy}}{e^{\vx^\t\vx} + e^{\vx^\t\vy}} = \frac{1}{1 + e^{1 - \vx^\t\vy}}
\end{equation}
Note that here we attend to $\vx$ so $0 < p < 1/2$ always. The last two is due to normalization condition. 

Now we consider a loss function $L = -\frac{1}{2}\|\vz\|_2^2$. The intuition behind is that ``for some reason, we found that $\vz$ is a good representation for our task, and want to make sure its length is as long as possible''. 

Under this context, what would be the gradient rule for $\vx$ and $\vy$? Will they cluster together? 

The answer is yes! We could compute 
\begin{eqnarray}
    \frac{\partial \vz}{\partial \vx} &=& (1-p) I + \frac{\partial p}{\partial \vx} (\vy - \vx)^\t \\
    \frac{\partial \vz}{\partial \vy} &=& p I + \frac{\partial p}{\partial \vy} (\vy - \vx)^\t 
\end{eqnarray}
Let $t := 1 - \vx^\t\vy$ and define the following function with respect to $t$: 
\begin{equation}
    f(t) := (\vx-\vy)^\t\vz = (1-2p)(1 - \vx^\t\vy) > 0
\end{equation}
Therefore, we can compute the gradient for $\vx$ and gradient for $\vy$:
\begin{eqnarray}
    -\vg_\vx &:=& -\frac{\partial L}{\partial \vx} = -\frac{\partial \vz}{\partial \vx} \frac{\partial L}{\partial \vz} = (1 - p)^2\vx + p(1-p)(1 - f(t))\vy \\ 
    -\vg_\vy &:=& -\frac{\partial L}{\partial \vy} = -\frac{\partial \vz}{\partial \vy} \frac{\partial L}{\partial \vz} = p^2\vy + p(1-p)(1 - f(t))\vx  
\end{eqnarray}
Note that since $\vx$ and $\vy$ are kept to be normalized, the term $(1-p)^2\vx$ in $\partial L / \partial \vx$ is gone (and similarly $p^2\vy$ for $\vg_\vy$). So how $\vx$ and $\vy$ move depends on the sign of $1 - f(t)$. 

With some computation, we could see $0 < f(t) < 1$ when $t < 1.5424$. In summary, if $\vx^\t\vy > -0.4576$, then the (negative) gradient of $\vx$ pushes it towards $\vy$ and pushes $\vx$ towards $\vy$, and the clustering of static embedding happens during training. Note that since both $\vx$ and $\vy$ are normalized, $-1 \le \vx^\t\vy \le 1$, so this is a quite loose condition and can be easily satisfied.  

\subsection{Multiple embeddings}
People might wonder what happen to multiple unit embeddings $\vx, \vy_1, \vy_2, \ldots, \vy_K$? In this case, we can similarly define self-attention probability $p_i$ (note that here we consider the case that every embedding attends to $\vx$):
\begin{equation}
    p_i := \frac{e^{\vx^\t\vy_i}}{e^{\vx^\t\vx} + \sum_j e^{\vx^\t\vy_j}} = \frac{e^{\vx^\t\vy_i}}{1 + \sum_j e^{\vx^\t\vy_j}}
\end{equation}
Define $p_S := \sum_{i=1}^K p_i = 1 - \frac{1}{1 + \sum_j e^{\vx^\t\vy_j}} < 1$ and we have:
\begin{equation}
    \vz = (1 - p_S) \vx + \sum_i p_i \vy_i
\end{equation}
Let $\tilde p_i := p_i / p_S$ be the (normalized) probability on $\vy_i$ and $\bar\vy := \frac{1}{p_S}\sum_i p_i \vy_i = \sum_i \tilde p_i \vy_i$ be the weighted mean of $\{\vy_i\}$ other than $\vx$, then we have: 
\begin{equation}
    \vz = (1 - p_S)\vx + p_S \bar\vy
\end{equation}
Now we can still compute the partial derivative:
\begin{eqnarray}
    \frac{\partial p_j}{\partial \vx} &=& p_j \left[-p_S \bar \vy + \vy_j\right] \\
    \frac{\partial p_j}{\partial \vy_i} &=& p_i \left[-p_j + \mathbb{I}(i = j)\right]\vx 
\end{eqnarray}
which gives
\begin{eqnarray}
\frac{\partial \vz}{\partial \vx} &=& (1 - p_S)I + \sum_j \frac{\partial p_j}{\partial \vx}(\vy_j - \vx)^\t  \\
\frac{\partial \vz}{\partial \vy_i} &=& p_i I + \sum_j \frac{\partial p_j}{\partial \vy_i}(\vy_j - \vx)^\t
\end{eqnarray}
After some manipulation, we have:
\begin{equation}
\frac{\partial \vz}{\partial \vx} = (1-p_S) [I + p_S \bar\vy (\bar \vy - \vx)^\t ] + p_S Q 
\end{equation}
where $Q := \sum_j \tilde p_j (\vy_j - \bar \vy)(\vy_j - \bar\vy)^\t$ is the weighted covariance matrix of data points $\{\vy_j\}$. 

Similar to the two unit case, we want to check $-\vg_\vx$ to see how the embedding $\vx$ changes over time.
\begin{eqnarray}
    -\vg_\vx &=& -\frac{\partial L}{\partial \vx} = -\frac{\partial \vz}{\partial \vx} \frac{\partial L}{\partial \vz} \\
    &=& (1 - p_S)^2\vx + p_S \left[(1-2p_S)\vx^\t\bar\vy - (1 - p_S) + p_S \|\bar\vy\|^2\right]\bar\vy + p_S Q\vz \nonumber 
\end{eqnarray}
If things are already quite clustered, then $\|\bar\vy\| \approx 1$ (usually $\|\bar\vy\|_2 < 1$ since sphere is a convex set), $Q\vz \approx 0$ (since $Q$ spans on the tangent space of $\vz$ at the sphere and $\vz$ is perpendicular to it), and we have:
\begin{equation}
    -\vg_\vx \approx (1 - p_S)^2 \vx + p_S(1-2p_S)(\vx^\t\bar\vy - 1) \bar\vy
\end{equation}
It is clear that $\vx^\t\bar\vy < 1$. When $p_S > 1/2$, which is high likely for large $K$, then $-\vg_\vx$ has positive component of $\bar\vy$ and $\vx$ will move towards $\bar\vy$. 

On the other hand, we could also check
\begin{equation}
    \frac{\partial \vz}{\partial \vy_i} = p_i \left[I + (1 - p_S)\vx (\bar\vy - \vx)^\t\right] + p_i \vx (\vy_i - \bar\vy)^\t
\end{equation}
which gives an expression of $-\vg_\vy$:
\begin{equation}
\cdot    
\end{equation}
With the same argument, it moves towards $\bar\vy$ (so all $\vy_i$ will cluster together) and towards $\vx$. 

When there is a $W_k$ and $W_q$ before the embedding, following the same logic, only the column subspace of $W_k$ (or $W_q$) will be clustered together. On the other hand, the value part will be different in order to enable encoding of more complicated concepts based on co-occurrence of multiple tokens. 

\def\pr{\mathbb{P}}

\section{Link self-attention with generative models.}
Consider the following self-attention structure. Consider an embedding matrix $X\in \rr^{n\times d}$ and for embedding $\vx_i$ and $\vx_j$, let
\begin{equation}
    \vy_{ij} = \phi(\vx_i; \vx_j) := (1 - \beta_{ij})\vx_i + \beta_{ij} \vx_j, \quad \beta_{ij} := \frac{e^{\vx_i^\t \vx_j}}{e^{\vx_i^\t \vx_i} + e^{\vx_i^\t \vx_j}} \\ 
\end{equation}
Here $\phi(\vx_i;\vx_j) := \vx_i + \beta_{ij}(\vx_j-\vx_i)$ is the self-attention operation. More properties of this operator $\phi$ need to be explored. Then we want to maximize the following objective:
\begin{equation}
    \max_{X, \|\vx_i\|_2=1} \sum_{ijk} \pr(k|i,j) \vy^\t_{ij} \vx_k 
\end{equation}
or more formally, using a softmax to avoid trivial solution $\vx_i \equiv \vx$, we have:
\begin{equation}
    \max_{X, \|\vx_i\|_2=1} J := \max_{X, \|\vx_i\|_2=1} \sum_{ijk} \pr(k|i,j) \log\delta_{ijk}, \quad \delta_{ijk} := \frac{e^{\vy^\t_{ij} \vx_k}}{\sum_k e^{\vy^\t_{ij} \vx_k}} 
\end{equation}
which is:
\begin{equation}
    \max_{X, \|\vx_i\|_2=1} \sum_{ijk} \pr(k|i,j) \left[\vy^\t_{ij} \vx_k - \log \sum_k e^{\vy^\t_{ij} \vx_k} \right] 
\end{equation}
We can compute its gradient update. Here we assume the index $k$ never appears in index $i$ and $j$ (encoding and decoding matrices are decoupled), then by gradient rule, we have:
\begin{equation}
    \dot \vx_k = \frac{\partial L}{\partial \vx_k} = P^\perp_{\vx_k} \sum_{ij} \pr(k|i,j) (1 - \delta_{ijk}) \vy_{ij}
\end{equation}
where $P^{\perp}_{\vx_k}$ is the projection matrix that projects a vector to the orthogonal complement space of $\vx_k$. The projection is due to the constraint $\|\vx_k\|_2=1$. If the training converges ($\dot \vx_k = 0$), then we know that 
\begin{equation}
    \sum_{ij} \pr(k|i,j) (1 - \delta_{ijk}) \vy_{ij} = \gamma \vx_k
\end{equation}
for some $\gamma > 0$ (note that $\gamma < 0$ will be an unstable stationary point).  

Depending on different structure of the generative model specified by $P(k|i, j)$, we might end up learning different embedding matrix $X$. 

The first thing we want to check is independency. Assume that for some specific token $k$ and $i$, we have $\pr(k|i, j) = \pr(k|i)$ for any $j$, which means that the frequency of token $k$ has nothing to do with the second entry $j$. Furthermore, token $k$ is not connected with other token $i'\neq i$, i.e, $\pr(k|i', j) \equiv 0$. If we just let $\delta_{ijk} = \delta > 0$, then we have:
\begin{equation}
    \pr(k|i) \sum_j \vy_{ij} = \gamma' \vx_k
\end{equation}
which yields
\begin{equation}
    \pr(k|i) n\vx_i + \sum_j \beta_{ij}(\vx_j-\vx_i) = \gamma' \vx_k
\end{equation}
And we could possibly show that $\sum_j \beta_{ij}(\vx_j-\vx_i) \approx 0$ since $\beta_{ij} = 1 / (1 + e^{1 - \vx_i^\t\vx_j})$ applies equal weights for embeddings around $\vx_i$ and they cancel out. Therefore, $\vx_k$ is aligned with $\vx_i$.  

Another thing we might want to check is identification of two tokens. Assume that there exists two tokens $j_1$ and $j_2$ and specific $k$ and $i$, so that $\pr(k|i,j_1) = \pr(k|i,j_2)$. For other $k, i, j$ combination $\pr(k|i,j) \equiv 0$, then we have:
\begin{eqnarray}
    \pr(k|i,j_1) \vy_{ij_1} = \gamma_1 \vx_k
\end{eqnarray}
(not sure how to continue). 

If we have $W_q$, $W_k$ and $W_v$, then the formulation doesn't change that much. The only difference here is that now 
\begin{equation}
    \beta_{ij} := \frac{e^{\vx_i^\t W_{pq} \vx_j}}{e^{\vx_i^\t W_{pq} \vx_i} + e^{\vx_i^\t W_{pq} \vx_j}}
\end{equation}
and $\vy_{ij}^\t \vx_k$ now becomes $\vy_{ij}^\t W_v \vx_k$. 

\iffalse
Alternatively, we could think about $L = |\vw^\t \vz - y|$ where $y$ is some predefined target. 
\fi