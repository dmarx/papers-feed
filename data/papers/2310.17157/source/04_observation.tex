
\section{Pre-trained LLMs are Contextually Sparse} \label{sec:obs}
In this section, we present several key observations and theoretical understandings of sparsity in LLMs, upon which the \name{} design is based.
We first test the contextual sparsity hypothesis and verify that contextual sparsity exists in pre-trained LLMs in Section~\ref{sec:sparse_obs}. Then, we build an understanding of why contextual sparsity happens naturally even when LLMs are densely trained in Section~\ref{sec:obs_att_cluster}. Finally, we present an observation on residual connections and explain their relationship to contextual sparsity analytically in Section~\ref{sec:obs_slowly_changing}.

\begin{figure}[t]
  \centering
    \subfigure[Contextual sparsity in Attention Head]{
    \hspace{0mm}\includegraphics[width=0.36\textwidth]{figure/head_sparsity.pdf}
    \label{obs:175b-sparsity-att} 
  }\\
  \vspace{-2mm}
  \subfigure[Contextual sparsity in MLP Block]{
    \includegraphics[width=0.36\textwidth]{figure/mlp_sparsity.pdf}
    \label{obs:175b-sparsity-mlp} 
  }
    \vspace{-2mm}
  \caption{ In Figure (a), we plot the percentage of not-activated attention heads. By only keeping heads that yield large output norms, we can silence over 80\% attention heads for a given token. In Figure (b), we plot the average sparsity we impose on MLP layers. We can zero out over 95\% of MLP parameters for a given token.}
    \vspace{-4mm}
  \label{observation:sparsity} 
\end{figure}

\subsection{Contextual Sparsity Hypothesis}
% \subsection{High Contextual Sparsity in MLP and Attention}
\label{sec:sparse_obs}
% Sparsity is a naturally occurring phenomenon in large language models. 
Inspired by prior pruning literature~\cite{molchanov2016pruning}, we find a surprisingly simple method is sufficient to study and verify our hypothesis. In this section, we describe the testing procedure, observation details, and insights of this study.   

\textbf{Verification:} Our test is performed on OPT-175B, 66B, and 30B models and various downstream datasets such as OpenBookQA~\cite{OpenBookQA2018} and Wiki-Text~\cite{merity2016pointer}. We find the contextual sparsity for every input example with two forward passes of the model. In the first pass, we record a subset of parameters, specifically which attention heads and MLP neurons yield large output norms for the input. In the second pass, each input example only uses the recorded subset of parameters for the computation. Surprisingly, these two forward passes lead to similar prediction or performance on all in-context learning and language modeling tasks.

\textbf{Observation:}  Figure~\ref{observation:sparsity} shows that on average, we can impose up to 80\% sparsity on attention heads and 95\% sparsity on MLP neurons. As mentioned in Section~\ref{sec:obs_computation}, OPT-175B model has $2\times$ MLP parameters than those of attention blocks. Therefore total sparsity here is around 85\%. Since these are all structured sparsity (heads and neurons), predicting them accurately could potentially lead to $7 \times$ speedup.   

\textbf{Insight:} It is intuitive that we can find contextual sparsity in MLP blocks at inference time because of their activation functions, e.g., ReLU or GeLU~\cite{pmlr-v119-kurtz20a}. Similar observations were made by~\cite{sanjiv}. However, it is surprising that we can find contextual sparsity in attention layers. Note that, finding contextual sparsity in attention is not the same as head pruning. We cross-check that different examples have different contextual sparsity. Although $80\%$ of the parameters are not included in the paths for a given example, they might be used by other examples. Next, we will try to understand why contextual sparsity exists in attention blocks.

% 

% Here, we summarize our findings on contextual sparsity~\ref{observation:sparsity}. 


% As shown in  over 95\% of parameters in the Expand MLP output zero values after the activation function, suggesting a high contextual sparsity ratio and a significant waste in compute.  In this Multi-Head-Attention Block, a given input is passed through multiple attention heads, and their outputs are concatenated together for output projection. At every head, a token will attend to every other token in the sequence, and the attention weight is decided based on the softmax probability. During training, each head is optimized for different sensitivities, like different convolution filters measure different characteristics~\cite {cordonnier2019relationship}. At each head, there may exist two outcomes depending on the head behavior: (1) the current token has high attention weights with a few tokens in the sequence, and (2) the current token has similar attention weights for the entire sequences. We consider the second situation as a sparsity head for this token because this token fails to find something it needs to pay attention to. In Figure~\ref{obs:sparsity-att}, we plot the percentage of the uniform head. Specifically, we calculate how many tokens are needed for the softmax score to accumulate to over 0.99. And we set 20\% as the threshold. We find that in the middle layers, over 80\% heads perform token mixing in a rather uniform manner, which suggests that all the sparse heads are performing rather similar computations.

% At the same time, a majority of neurons are activated, just by different input sequence. As for OPT-175b, we can barely find ``dead'' neurons can be pruned without any consequences after 40 layers.

% (3) \textit{Dead neurons at lower layers.} Some neurons in lower transformer layers never get activated for any input tokens, and thereby can be safely pruned without any consequences. It should be noted that the portion gets larger for larger models.

% Additionally, dynamic sparsity is prevalent across most layers.
% By dynamic sparsity, we mean that a majority of neurons are activated but by varying input tokens.

% shows that on average each token only activates 5\% rows (or columns) in FFN matrices and 10\% of attention heads. Furthermore, since token generation latency is one of the major challenges of LLM deployment for its token-by-token serial computation (details in Section~\ref{sec:related_work}), the above observation provides a huge opportunity to generate tokens $15\times$ faster.

\subsection{Token Clustering in Attention Layers}
\label{sec:obs_att_cluster}

In the previous section, we have verified that there exists contextual sparsity for a given input in LLMs. In this section, we try to understand the reason for such phenomena, especially in attention layers. We first show an in-depth observation of attention. Then we present a hypothesis that self-attentions are conceptually clustering algorithms. Last we show analytical evidence to support this hypothesis. 


\textbf{Observation:} Figure~\ref{fig:head_uniform} shows the attention map of three different heads from the same layer for an example input. The next token it should predict is ``Truck''. Darker color represents higher attention scores. We observe that the middle head is a relatively uniform token-mixing head while the top and bottom ones are ``heavy hitter'' attention heads (with high attention to ``like'' and ``shipping''). Unsurprisingly, only selecting heavy hitter heads but not uniform heads does not affect the prediction, since uniform heads do not model or encode important token interactions. In the next section, we will also explain in detail how the criteria for selecting uniform attention heads and heads with small output norms are highly correlated.    

\def\vone{\mathbf{1}}

\textbf{Hypothesis:} We hypothesize that the attention head is performing mean-shift clustering~\cite{derpanis2005mean}. 

Recall the notation defined in Section~\ref{sec:formulation}. For $i$-th head at current layer, $X = [x_1, \ldots, x_n]^{\top} \in \mathbb{R}^{n\times d}$ are the token embeddings in the previous time steps. $X W_i^K $ and $X W_i^V $ are the projection of embedding. For an input embedding $y$, the output $\tilde y_i = H_i(y)$, where $H_i(y)$ is defined in Eq.~\ref{eq:H_i_y}. 

\def\vm{\mathbf{m}}
For each $i \in [h]$, if we let $K_i(x_j,y) := \exp(y W_i^Q(W_i^K)^\top x_j)$ measure the similarity between $x_j$ and $y$, and define $m_i(y) := \frac{\sum_j K_i(x_j,y) x_j}{\sum_j K_i(x_j,y)}$, then we have $\tilde y_i =  m_i(y) W_i^V$. Further, if we set $W^V_i=I$ and consider the residue connection followed by layer norm, then in the next layer, the embedding $\hat y_i$ of the current token becomes $\hat y_i = \mathrm{Normalize}(y + \tilde y_i) = \mathrm{Normalize}(y + m_i(y))$, which has a fixed point $y = \gamma m_i(y)$ for any scalar $\gamma$. This iteration bears a resemblance to mean-shift clustering, which simply performs iteration $y \leftarrow m_i(y)$ until convergence. This has an obvious fixed point $y = m_i(y)$. 

Therefore, the self-attention head can be regarded as \emph{one mean-shift step} to push input embeddings of different tokens together, if they are already neighbors in a projection space specified by $W_i^Q (W_i^K)^\top $. Different heads learn different projection spaces to perform clustering. These dynamics explain the precise reason why token embeddings tend to cluster after going through more layers, resulting in high attention scores among cluster members, and low scores for non-members. Furthermore, the cluster patterns are different at different heads (More details in Appendix~\ref{sec:clustering understanding}).

The above analysis not only provides an understanding of why contextual sparsity exists naturally in pre-trained LLMs, but also inspires our design of ``similarity''-based sparsity prediction for \name{} in Section~\ref{sec:method}.  

%At head $i$, if the attention weights for a token $x$ are rather dense on a few tokens, it suggests that $x$ belongs to one cluster under head $i$'s clustering standard, and tokens in this cluster will have high attention weights on each other. 

\begin{figure}[]
 \vspace{-2mm}
  \centering
    \includegraphics[width=0.47\textwidth]{figure/Attention_clustering.pdf}
    \vspace{-3mm}
  \caption{ We visualize the attention scores of three different heads for an exemplary sentence. Head 42 and Head 44 give heavy attention scores on particular tokens while Head 43 is more uniform.     }
  \label{fig:head_uniform} 
     \vspace{-4mm}
\end{figure}
% In the previous example that when generating the fifth token, it does not go through head 1 in layer 10: if when generating the sixth token, it is similar and highly correlated with the fifth one at layer 10, it will go through the same paths / heads and highly likely not going through head 1; if the sixth token is not similar or correlated with the fifth one, not attending with the fifth token does not lose too much information.

\subsection{Slowly Changing Embeddings across Layers}
\label{sec:obs_slowly_changing}

\begin{figure}[]
\vspace{-2mm}
  \centering
 \subfigure[Model Comparison]{
    \includegraphics[width=0.22\textwidth]{figure/observation/cos_across_model.pdf}
    \label{obs:slowlyevoloving-all}
    }
  \subfigure[Across Layer]{
    \includegraphics[width=0.22\textwidth]{figure/observation/175b_between_layer_cos.pdf}
    \label{obs:slowlyevoloving-175b}
  } \\
  \vspace{-4mm}
    \subfigure[Residual Around Attention]{
    \includegraphics[width=0.22\textwidth]{figure/175b_norm_residual_attention_sqaure.pdf}
    \label{obs:attention_residual}
  } 
    \subfigure[Residual Around MLP]{
    \hspace{1mm}\includegraphics[width=0.22\textwidth]{figure/175b_norm_residual_mlp_sqaure.pdf}
    \label{obs:mlp_residaul}
  } 
  \vspace{-0.3em}
  \caption{\textbf{Slowly Changing Embedding.} Figure (a) shows the median cosine similarity between representations at two consecutive layers across all layers for different OPT models. All models show a similarity greater than 95\%. Figure (b) shows cosine similarity stays high even a few layers apart. For the residual connection $X' = X + F(X)$ inside each block, we plot the $\ell_2$ norm of $X$ and $F(X)$ in Figure (c) and Figure (d). $\|X\|$ is significantly higher than $\|F(X)\|$, which explains the slowly changing embedding.\vspace{-1em}}
  % \caption{ \textbf{Slowly Changing Embedding} We look at the cosine similarity between token embeddings at transformer layer $i$ and transformer layer $i + n$ for the same input.  Figure (a) shows the median cosine similarity between representations at two consecutive layers ($n = 1$) across all layers for different OPT models. All models show a similarity greater than 95\%. Also, similarity increases as the model grow larger. Figure (b), (c), and (d) shows the cosine similarity with various choices of $n$ at every layer. The similarity is near zero at the first layer. Starting at the second layer, cosine similarity remains high and drops a little at later layers. The similarity is lower when $n$ is larger.  \textbf{Residual connection maintains the angle.} There are two residual connections $X' = X + F(X)$ inside each transformer layer, one around the attention block and one around the MLP block. In this figure, we plot the cosine similarity between $X$ and $F(X)$, and the cosine similarity between $X$ and $X'$ in orange color. The similarity between $X'$ and $X$ is extremely high at all layers( except the first layer), while the similarity between $X$ and $F(X)$ is low, almost orthogonal in the majority of layers. To explain this, we plot the $L2$ norm of $X$ and $F(X)$. Except on the first layer, $\|X\|$ is significantly higher than $\|F(X)$. $\|F(X)\|$ is higher at the first layer, which corresponds to the low cosine similarity at the first layer. }
  \label{observation_residual} 
\end{figure}

We first present our observation that embeddings change slowly across consecutive layers. Then we provide a detailed analysis on the phenomenon. Finally, we show its close connection with contextual sparsity.  Details are in Section~\ref{sec:appendix-obs}. 

\textbf{High similar embeddings in consecutive layers:} In Figure~\ref{obs:slowlyevoloving-all}, we show that for the same given input, the cosine similarity between embeddings or activations in two consecutive layers is exceptionally high on 7 different sizes of OPT models. Specifically, we collect activations from each layer while performing OPT model inference on C4 validation set~\cite{2019t5}. Taking OPT-175B as an example, starting from the second layer, the similarity between any two consecutive layers is around 0.99, which indicates that when an input is passed through the model, the direction of its embedding  changes slowly. Interestingly, the most drastic change happens in the first layer. Furthermore, we increase the gap and investigate the similarity between the embedding at layer $l$ and at layer $l + n$ shown in Figure~\ref{obs:slowlyevoloving-175b}. As we increase the gap, the similarity decreases as expected while the differences in cosine similarity between various choices of $n$ are smaller at the shallower layer. We plot the mean similarity, and the standard deviation is indicated by the shading. Similar plots on more models are presented in Appendix~\ref{sec:appendix-obs}.  

\textbf{Connection to residuals:} We verify that the high similarity in embeddings in LLM inference is due to the residual connection. We first dissect the computation graph inside each transformer layer to understand the cause behind this phenomenon. There are two residual connections inside a transformer layer, one around the attention block, and the other one around the MLP block. The residual connection can be written as $X + F(X)$, where $F$ is either the Multi-Head Attention or two MLP Layers.  In Figure~\ref{obs:attention_residual} and Figure~\ref{obs:mlp_residaul},  indeed we can see that $\|X\|$ is significantly greater than $\|F(X)\|$, confirming that embeddings are changing slowly because the residual norm is large.   

\textbf{Connection to Contextual Sparsity:} We take a step deeper trying to understand the reason behind the large residual norm with mathematical modeling.  
We discover that one possible reason for small $\|F(X)\|$ is due to high sparsity. For the MLP Block, high sparsity may contribute to the small norm of $F(X)$ because a large portion of outputs have small norms. Similar reasoning applies to the Attention Block, and thus a large number of attention heads yield small norm outputs.

% uniform attention weights suggest that token embeddings are dissimilar or orthogonal, which can lead to a small norm.

% Figure~\ref{observation:residual} plots the cosine similarity between $X$ and $X + F(X)$, which is close to 1.0, and the cosine similarity between $X$ and $F(X)$, which is close to 0.0. This happens because $\|X\|$ is significantly greater than $\|F(X)\|$, shown in the purple in Figure~\ref{observation:residual}. At the first layer, $\|F(X)\|$ is larger, which explains the low cosine similarity. The magnitude of $L2$ norm is different across models, however, we observe a similar trend with models of different sizes. 

% We further investigate the reason for the small $\|F(X)\|$. Even though there exists a normalization layer before $F(X)$, as shown in Figure~\ref{observation:residual}, layer normalization scale $\|X\|$ to a consistent magnitude across layers (e.g. 85 for OPT-30B, 110 for OPT175B), but not necessarily scale down $\|X\|$. 






% Large language models are typically repetitions of the same architectures. For example, GPT3-175B consists of 96 transformer layers, where each layer is made of an Attention block and an MLP Block. We record the activation after every block and investigate the similarity between these activations in Figure~\ref{obs:slowlyevoloving}. Activation is collected by running OPT models~\cite{} on C4 validation set~\cite{}. We plot the mean similarity and the standard deviation is indicated by the shading. Similar plots on more models are presented in Appendix~\ref{sec:appendix-obs}.

% Our most surprising finding is that the cosine similarity between activations of two consecutive layers is extremely high. Taking OPT175B as an example in Figure~\ref{obs:slowlyevoloving-175b}, the similarity is low, around 0, between the first and the second layer. Starting from the second layer, the similarity between any two consecutive layers is around 0.99 with a small drop to 0.96 toward the ending layers. This indicates that when input data is passed through the model, the direction of its representation vector changes slowly. The exception is the first layer, in which we suspect that token embedding is uniformly mixed with each other, as suggested in~\cite{}.

% Further, we increase the gap and investigate the similarity between the representation at layer $i$ and at layer $i + n$. Generally, as we increase the gap, the similarity decreases as expected. One interesting finding is that the differences in cosine similarity between various choices of $n$ are smaller at the shallower layer. 

% \subsubsection{Empirical Reason}
% % \begin{wrapfigure}{r}{0.15\textwidth}
% %   \begin{center}
% %     \includegraphics[width=0.15\textwidth]{figure/GPT_Blog_Diagram.pdf}
% %   \end{center}
% %     \label{fig:arch}
% % \end{wrapfigure}
% We dissect the computation graph inside each transformer layer to understand the cause behind this phenomenon. There are two residual connections inside a transformer layer, one around the attention block, and the other one around the MLP block. The residual connection can be written as $X + F(X)$, where $F$ is either the Multi-Head Attention or two MLP Layer, shown in Figure~\ref{fig:arch}. 

% Figure~\ref{observation:residual} plots the cosine similarity between $X$ and $X + F(X)$, which is close to 1.0, and the cosine similarity between $X$ and $F(X)$, which is close to 0.0. This happens because $\|X\|$ is significantly greater than $\|F(X)\|$, shown in the purple in Figure~\ref{observation:residual}. At the first layer, $\|F(X)\|$ is larger, which explains the low cosine similarity. The magnitude of $L2$ norm is different across models, however, the we observe a similar trend with models of different sizes. 


% We further investigate the reason for the small $\|F(X)\|$. Even though there exists a normalization layer before $F(X)$, as shown in Figure~\ref{observation:residual}, layer normalization scale $\|X\|$ to a consistent magnitude across layers (e.g. 85 for OPT-30B, 110 for OPT175B), but not necessarily scale down $\|X\|$.  See Theoretical analysis in Section~\ref{sec:obs_theory}.%??? \Zhao{I will double check this once appendix is finished.}

% High sparsity may also contribute to the small norm of $F(X)$ in the MLP block. We observe high sparsity as a naturally occurring phenomenon in MLP blocks.
% As demonstrated in \cref{observation:sparsity}, we observed that the majority of neurons in the Expand MLP output zero values after the activation function. In the case of OPT-175b, over 95\% of neurons are not activated during inference. 

% \textbf{Theoretical Understanding}
% \label{sec:obs_theory}
% %\textbf{Residual Lower Bound}
% \iffalse
% \begin{lemma}
% Let $\epsilon_1 \in (0,1)$ denote the lower bound of shrinking factor.
%  We denote $x$ as input and denote $y$ as output. We have the residual connection $y = x + F(x)$. For the MLP block $F(x)$,  
%  we have $ \| y - x \|_2 \geq \epsilon_1$. For the attention block $F(x)$,  we have $\| y - x \|_2 \geq \epsilon_1 $.  
% %$ \| x' - x \|_2 \geq \epsilon_1 $
% \end{lemma}
% \fi
%\iffalse
%F(x) is the original MHA, F'(x) is the sparse computation as follows
%\begin{equation*}
%    MHA(x) = [H_1(x), H_2(x)], ...; H_n(x)]W_o
%\end{equation*}
%where $W_o$ is the output projection and $H_n(x)=\vec{0}, H_n(x) \notin S_h$

%F(x) - F'(x) preserve information
%\fi

\textbf{Residual Two Sides Bound:} Besides empirical reasoning, we formally define the computation of LLMs mathematically. Under our computation model, we can show that a shrinking property which is observed by our practical experiments. Proofs are in Appendix \ref{sec:subspace_embedding}, \ref{sec:distances_angles}, \ref{sec:function_approx}.

\begin{lemma}[Informal]
Let $0 < \epsilon_1 < \epsilon_2< 1$ be the lower and upper bound of the shrinking factor.
Let $x$ be the $y$ be the output. We have the residual connection $y = x + F(x)$. For the MLP block $F(x)$,  
 we have $\epsilon_1 \leq \| y - x \|_2 \leq \epsilon_2$. For the attention block $F(x)$,  we have $\epsilon_1 \leq \| y - x \|_2 \leq \epsilon_2 $.  
\end{lemma}






%describe the IO bound, low utilization. What is the latency challenge?



% \begin{figure}[]
%   \centering
%     \includegraphics[width=0.5\textwidth]{figure/diagram.pdf}
%   \caption{ \textbf{Sparsified Transformer Layer} }
%   \label{fig:workflow} 
% \end{figure}

%\input{08_theory.tex}
% \subsection{Insights}

% \textbf{Relationship to Smoothquant}


% \textbf{Relationship to Training}








%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
