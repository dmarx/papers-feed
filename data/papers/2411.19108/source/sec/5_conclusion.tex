\section{Conclusion}
In this study, we introduce \textbf{TeaCache}, a novel, training-free approach designed to significantly accelerate video synthesis inference while maintaining high-quality output. We analyze the correlation between model input and output,  observing that similarity of timsetep embedding modulated noisy input in consecutive timesteps shows strong correlation with similarity of model output. We propose to utilize similarity of timsetep embedding modulated noisy input as an indicator of output similarity, allowing for dynamic caching of model outputs. Further, we propose a rescaling strategy to refine the estimation of model output similarity,  optimizing the selection process for timestep caching. Extensive experiments demonstrate TeaCache's robust performance in terms of both efficiency and visual quality across diverse video generation models and image generation models, sampling schedules, video lengths, and resolutions, underscoring its potential for real-world applications.
\vspace{-0.4cm}