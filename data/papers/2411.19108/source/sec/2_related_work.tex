\section{Related Work}
\subsection{Diffusion Model}
%
In the realm of generative models, diffusion models \cite{ho2020denoising, sohl2015deep} have become foundational due to their exceptional ability to produce high-quality and diverse outputs. Initially developed with the U-Net architecture, these models have demonstrated impressive performance in image and video generation \cite{ramesh2022hierarchical, rombach2022high, ho2022video, saharia2022photorealistic, wei2024dreamvideo, wei2024dreamvideo2, wang2023modelscope, chen2023videocrafter1, chen2024videocrafter2}. 
%

However, the scalability of U-Net-based diffusion models is inherently constrained, posing challenges for applications requiring larger model capacities for enhanced performance. To address this limitation, Diffusion transformers (DiT) \cite{peebles2023scalable} represent a significant advancement. By utilizing the scalable architecture of transformers \cite{vaswani2017attention}, DiT provides an effective means to increase model capacity.
%
A notable achievement in this field is the advancement in generating long videos through the large-scale training of Sora \cite{Sora}, which employs a transformer-based Diffusion architecture for comprehensive simulations of the physical world. This underscores the considerable impact of scaling transformer-based Diffusion models.
%
An increasing number of studies have adopted the Diffusion transformer as the noise estimation network~\cite{chen2023pixart, chen2024pixart, Open-Sora, Open-Sora-Plan, ma2024latte, yang2024cogvideox}.

\subsection{Diffusion Model Acceleration}
%
Despite the notable performance of Diffusion models in image and video synthesis, their significant inference costs hinder practical applications. Efforts to accelerate Diffusion model inference fall into two primary categories. First, techniques such as DDIM~\cite{song2020denoising} allow for fewer sampling steps without sacrificing quality. Additional research has focused on efficient ODE or SDE solvers~\cite{song2019generative, jolicoeur2021gotta, lu2022dpm, karras2022elucidating, lu2022dpm++}, using pseudo numerical methods for faster sampling. Second, approaches include distillation~\cite{salimans2022progressive, wang2023videolcm}, quantization~\cite{li2024q, he2024ptqd, so2024temporal, shang2023post}, and distributed inference~\cite{li2024distrifusion} are employed to reduce the workload and inference time.  

However, these methods often demand additional resources for fine-tuning or optimization. Some training-free approaches~\cite{bolya2023token, wang2024attention} streamline the sampling process by reducing input tokens, thereby eliminating redundancy in image synthesis. Other methods reuse intermediate features between successive timesteps to avoid redundant computations~\cite{wimbauer2024cache, so2023frdiff, zhang2024cross}. DeepCache~\cite{xu2018deepcache} and Faster Diffusion~\cite{li2023faster} utilize feature caching to modify the UNet Diffusion, thus enhancing acceleration. FORA~\cite{selvaraju2024fora} and $\triangle$-DiT~\cite{chen2024delta} adapts this mechanism to DiT by caching residuals between attention layers. PAB~\cite{zhao2024real} caches and broadcasts intermediate features at various timestep intervals based on different attention block characteristics for video synthesis. While these methods have improved Diffusion efficiency, enhancements for DiT in visual synthesis remain limited.