\section{Geometric Stability}
\label{sec:geometry}

This Section describes the notion of geometric stability in signal representations. We begin with the Euclidean setting (subsection \ref{sec:euclideangeom}), where this stability is expressed in terms of diffeomorphisms of the signal domain. We then discuss how to extend this notion to general metric domains in subsection \ref{sec:noneuclideanstab}, and then highlight the limitations of several standard high-dimensional signal representations in regards to geometric stability (subsection \ref{sec:geomexamples}). 


% This section reviews some of the 
% existing tools for signal representation and
% discrimination in recognition tasks. 
% Invariance and stability are formulated 
% in terms of Lipschitz continuity conditions, 
% and are then studied on a variety of
% representations.

\subsection{Euclidean Geometric Stability}
\label{sec:euclideangeom}
%define the problems at hand. 
Consider a compact $d$-dimensional Euclidean domain $\Omega = [0,1]^d \subset \mathbb{R}^d$ on which  
square-integrable functions $\bbx\in L^2(\Omega)$ are defined (for example, in image analysis applications, images can be thought of as functions on the unit square $\Omega = [0,1]^2$). 
%
We consider a generic supervised learning setting, in which an unknown function 
$f : ~ L^2(\Omega) \to \mathcal{Y}$ is observed on a training set 
%\begin{equation}
$\{ \bbx_i \in L^2(\Omega), f_i = f(\bbx_i) \}_{i \in \mathcal{I}}.$ 
%\end{equation} 
%
% In a supervised classification setting, the target space $\mathcal{Y}$ can be thought discrete with $| \mathcal{Y} |$ being 
% the number of classes, 
%  In a {\em multiple object recognition} setting, we can replace $\mathcal{Y}$ by the $K$-dimensional simplex, 
%  which represents the posterior class probabilities $p( y | x)$. 
% %
% In {\em regression} tasks, we may consider $\mathcal{Y}=\mathbb{R}^m$. 
% %
% %$y$ is often referred to as {\em label}. 
In the vast majority of computer vision and speech analysis tasks, 
the unknown function $f$ satisfies crucial regularity properties 
expressed in terms of the signal domain $\Omega$.
%prior information 
%prior assumptions on the unknown function $y$. 
%
%As we will see in the following, these assumptions are effectively exploited by convolutional neural network architectures. 
% that is effectively exploited by Convolutional Neural Network architectures. 
%We will now describe these priors mathematically with the objective of extending them to non-Euclidean 
%domains later on. 

%First, many tasks are presumed to enjoy 
{{\em Global Translation Invariance:}} 
Let 
%\begin{equation}
${\cal T}_v \bbx(u) = \bbx(u - v), \hspace{3mm} u, v \in \Omega,$ 
%\end{equation}
%$T_v$ for some $v \in \mathbb{R}^d$ 
be a {\em translation operator}\footnote{
Assuming periodic boundary conditions to 
ensure that the operation is well-defined over $L^2(\Omega)$.
} acting on functions $\bbx \in L^2(\Omega)$. %as 
%$T_v f(x) = f(x - v)$ for 
%
Our first assumption is that the function $f$ is either {\em invariant}, ie $f({\cal T}_v \bbx) = f(  \bbx) $ for any $\bbx \in L^2(\Omega)$ and $v\in \Omega$, or {\em equivariant}, ie $f({\cal T}_v \bbx) = {\cal T}_v f(\bbx)$,  with 
respect to translations, depending on the task. 
 Translation invariance is typical in object classification tasks, whereas equivariance  
 arises when the output of the model is a space in which translations can act upon (for example, in problems of object localization, 
 semantic segmentation, or motion estimation). 
 
 The notion of global invariance/equivariance can be 
 easily extended to other transformation groups beyond translations. Section \ref{sec:rigid} discusses one such extension, to the group of rigid motions generated by translations and rotations in $\Omega$. 
 
%Our definition of invariance should
%not be confused with the traditional notion of \emph{translation invariant systems} in signal processing, 
%which corresponds to translation equivariance in our language (since the output translates whenever the input translates).
% Equivalently, we can describe this property in terms of the statistics of natural images. 
% If one considers that natural images are drawn from an underlying probability distribution, 
% the invariance/equivariance property implies that this distribution describes a stationary source \cite{simoncelli2001natural} in the 

However, global invariance is not a strong prior in the face of high-dimensional estimation. Ineed, global transformation groups are typically low-dimensional; in particular, in signal processing, they often correspond to subgroups of the affine group $\mathrm{Aff}(\Omega)$, with dimension $O(d^2)$. A much stronger prior may be defined by specifying how the function $f$ behaves under geometric perturbations of the domain which are `nearby' these global transformation groups.

{{\em Local deformations and scale separation}:} 
In particular, given a smooth vector field $\tau: \Omega \to \Omega$, a deformation by $\tau$ acts on $L^2(\Omega)$ as 
$\bbx_\tau(u) := \bbx(u -\tau(u))$. Deformations can model local translations, changes in point of view, rotations and
frequency transpositions \cite{bruna2013invariant}, and have 
been extensively used as models of image variability in computer vision 
\cite{jain1996object, felzenszwalb2010object, girshick2014rich}.
% Similarly, a deformation ${\cal L}_\tau$, where $\tau: \Omega \to \Omega$ is a smooth vector field,  acts on $L^2(\Omega)$ as 
% ${\cal L}_\tau f(u) = f(u -\tau(u))$. Deformations can model local translations, changes in point of view, rotations and
% frequency transpositions \cite{bruna2013invariant}. 
Most tasks studied in computer vision are not only translation invariant/equivariant, but also stable with respect to local deformations \cite{mallat2016understanding,bruna2013invariant}. In tasks that are translation invariant, this prior may be expressed informally as 
\begin{equation}
\label{deformstab}
| f( \bbx_\tau) - f( \bbx) | \approx \| \tau \|,
\end{equation}
for all $\bbx, \tau$. 
 Here, $\| \tau \| $ measures the distance 
 of the associated diffeomorphism $\varphi(u):= u - \tau(u)$ to the translation group; we will see in next section how to specify this metric in the space of diffeomorphisms. In other words, 
 the target to be predicted does not change much if the input image is slightly deformed. 
%
In tasks that are translation equivariant, we have $| f( \bbx_\tau ) -  f_\tau( \bbx) | \approx \|  \tau \| $
%  \begin{equation}
%  | y( {\cal L}_\tau f) - {\cal L}_\tau y( f) | \approx \| \nabla \tau \|.
%  \end{equation}
 instead.
The deformation stability property is much stronger than the global invariance one, since the space of local deformations has high dimensionality, 
as opposed to the group of global invariants. 

% It follows from (\ref{deformstab}) that we can extract sufficient statistics at a lower spatial resolution by 
% downsampling demodulated localized filter responses without losing approximation power. 
As we will see later, a key consequence of (\ref{deformstab}) is that long-range dependencies may be broken into multi-scale 
local interaction terms, leading to hierarchical models in which spatial resolution is progressively reduced.
 To illustrate this principle,
denote by 
 \begin{equation}
q(z_1, z_2;v) = \mathrm{Prob}( \bbx(u) = z_1 \,\,\, \mathrm{and} \,\,\, \bbx(u+v)=z_2)
 \end{equation}
 the joint distribution of two image pixels at an offset $v$ from each other, where we have assumed a stationary statistical model for natural images (hence $q$ does not depend upon the location $u$).
 In presence of long-range dependencies, this joint distribution will not be separable for any $v$. However, the deformation stability prior states that $q(z_1, z_2; v) \approx q(z_1, z_2; v(1+\epsilon))$ for small $\epsilon$. In other words, whereas long-range dependencies indeed exist in natural images and are critical to object recognition, they can be captured and down-sampled at different scales.
This principle of stability to local deformations has been exploited in the computer vision community 
in models other than CNNs, for instance, deformable parts models \cite{felzenszwalb2010object}, as we will review next.
% are graphical models that incorporate 
%a similar prior in their model specification. 
In practice, the Euclidean domain $\Omega$ is discretised using a regular grid with $n$ points; the translation and deformation operators are still well-defined so the above properties hold in the discrete setting. 

\subsection{Representations with Euclidean Geometric Stability}

Motivated by the previous geometric stability prior, we are 
interested in building signal representations that are compatible with such a prior. Specifically, suppose our estimation for $f$, the target function, takes the form 
\begin{equation}
    \hat{f}(\bbx) := \langle \Phi(\bbx), \theta \rangle~,
\end{equation}
where $\Phi: L^2(\Omega) \to \R^K$ corresponds to the signal representation and $\theta \in \R^K$ the classification or regression coefficients, respectively. 
In a CNN, one would associate $\Phi$ with the operator that maps the input to the last hidden layer, and $\theta$ with the very last output layer of the network. 

% are stable with respect to small deformations. 
% If $\bbx_\tau(u):= \bbx(u - \tau(u))$ denotes a change of variables 
% with a differentiable field $\tau : \Omega \to \Omega$ 
% such that $\| \nabla \tau \| < 1$, then we ask

The linear relationship between $\Phi(\bbx)$ and $\hat{f}(\bbx)$ above implies that geometric stability in the representation is sufficient to guarantee a predictor which is also geometrically stable. Indeed, if we assume that
\begin{equation}
\label{ble}
\forall~\bbx,\tau~,\, \| \Phi(\bbx) - \Phi(\bbx_\tau) \| \lesssim \| \bbx \| \| \tau \|~,    
\end{equation}
then by Cauchy-Schwartz, it follows that 
$$| \hat{f}(\bbx) - \hat{f}(\bbx_\tau) | \leq \| \theta\| \| \Phi(\bbx) - \Phi(\bbx_\tau) \| \lesssim \| \theta\| \| \bbx \| \| \tau \|~.$$

This motivates the study of signal representations where one can certify (\ref{ble}), while ensuring that $\Phi$ captures enough information so that $\| \Phi(\bbx) - \Phi(\bbx') \|$ is large whenever $|f(\bbx) - f(\bbx')| $ is large.  
 In this setting, a notorious challenge to achieving (\ref{ble}) 
while keeping enough discriminative power in $\Phi(\bbx)$ 
is to transform the high-frequency content of $\bbx$ in such a way that it becomes stable. 



In recognition tasks, one may not only want to 
consider geometric stability, but also 
stability with respect to the Euclidean metric 
in $L^2(\Omega)$: 
\begin{equation}
\label{additive_stability}
\forall\, \bbx,\bbx' \in L^2(\Omega)~,~\| \Phi(\bbx) - \Phi(\bbx') \| \lesssim \| \bbx - \bbx' \| ~.
\end{equation}
This stability property ensures that additive noise 
in the input will not drastically change the feature 
representation. 

The stability desiderata (\ref{ble}) and (\ref{additive_stability}) may also be interpreted in terms of robustness to adversarial examples \cite{szegedy2013intriguing}. Indeed, the general 
setup of adversarial examples consists in producing 
small perturbations $\bbx'$ of a given input $\bbx$ (measured by appropriate norms) such that $| \langle \Phi(\bbx) - \Phi(\bbx'), \theta \rangle|$ is large. Stable representations certify that those adversarial examples cannot be obtained with small additive or geometric perturbations. 

\subsection{Non-Euclidean Geometric Stability}
\label{sec:noneuclideanstab}

Whereas Euclidean domains may be used to model many signals of interest, such as images, videos or speech, a wide range of high-dimensional data across physical and social sciences is naturally defined on more general geometries. For example, signals measured on social networks have rich geometrical structure, encoding locality and multiscale properties, yet they on a non-Euclidean geometry. An important question is thus how to extend the notion of geometrical stability to more general domains. 

Deformations provide the natural framework to describe geometric stability in Euclidean domains, but their generalization to non-Euclidean, non-smooth 
domains is not straightforward.
Let $\bbx \in L^2(\mathcal{X})$ be a signal defined on a domain $\mathcal{X}$. If $\mathcal{X}$ is embedded into a low-dimension Euclidean space $\Omega \subset \R^d$, such as a 2-surface within a three-dimensional space, then one can still define meaningful deformations on $\mathcal{X}$ via \emph{extrinsic} deformations of $\Omega$. Indeed, if $\tau: \R^d \to \R^d$ is a smooth field and $\varphi(v) = v - \tau(v)$ the corresponding diffeomorphism (assuming $\| \tau\| < 1/2)$, then we can define $\bbx_\tau \in L^2( \mathcal{X}_\tau)$ as
$$\bbx_\tau(u) := \bbx(\varphi^{-1}(u))~,u \in \mathcal{X}~.$$
Such deformation models have been studied in \citep{kostrikov2017surface} with applications in surface representation, in which the notion of geometric stability relies on its ambient Euclidean structure. 

In more general applications, however, we may be interested in intrinsic notions of geometric stability, that do not 
necessarily rely on a pre-existent low-dimensional embedding of the domain. 
The change of variables $\varphi(u) = u - \tau(u)$ defining the deformation can be seen as a perturbation of the Euclidean metric in $L^2(\R^d)$. Indeed, 
\begin{equation} \nonumber
    \langle \bbx_\tau, \bby_\tau \rangle_{L^2(\R^d,\mu)} = \int_{\R^d} \bbx_\tau(u) \bby_\tau(u) d\mu(u) = \int_{\R^d} \bbx(u) \bby(u) | I - \nabla \tau(u)| d\mu(u) = \langle \bbx, \bby \rangle_{L^2(\R^d, \tilde{\mu})}~,
\end{equation}
with $d\tilde{\mu}(u) = | I - \nabla \tau(u)| d\mu(u)$, and $| I - \nabla \tau(u)| \approx 1$ if $\| \nabla \tau \|$ is small, where $I$ is the identity. 
Therefore, a possible way to extend the notion of deformation stability to general domains $L^2(\mathcal{X})$ is 
to think of $\mathcal{X}$ as a metric space and reason in terms of stability of $\Phi: L^2(\mathcal{X}) \to \R^K$
to \emph{metric changes} in $\mathcal{X}$. 
This requires a representation that can be defined on generic metric spaces, as well 
as a criteria to compare how close two metric spaces are. We will describe 
a general approach for discrete metric spaces based on diffusion operators in Section \ref{sec:graph}.

\subsection{Examples}
\label{sec:geomexamples}

\subsubsection{Kernel Methods}
\label{kernelmethods}

Kernel methods refer to a general
theory in the machine learning framework,
whose main purpose consists in
embedding data in a high dimensional space,
in order to express complex relationships 
in terms of linear scalar products. 

For a generic input space $\mathcal{Z}$ (which 
can be thought of as $\mathcal{Z} = L^2(\mathcal{X})$ 
corresponding to the previous discussion),  
a \emph{feature map}
$\Phi: \mathcal{Z} \longrightarrow \mathcal{H}$ maps 
data into a Hilbert space $\mathcal{H}$ with 
the reproducing property: for each $f \in \mathcal{H}$ and
$\bbx \in \mathcal{Z}$, $f(\bbx) = \langle f, \Phi(\bbx) \rangle$.
Linear classification methods access the 
transformed data $\Phi(\bbx)$ only through 
scalar products of the form \cite{kernel_methods}
$$\langle \Phi(\bbx), \Phi(\bbx') \rangle~.$$

Rather than building the mapping 
explicitly,
the popular ``Kernel Trick" exploits 
Mercer's theorem. It states that a continuous,
symmetric and positive
definite kernel $K: \mathcal{Z} \times \mathcal{Z} \to \mathbb{R}$ 
defines an integral
operator of
$L^2(\mathcal{Z})$, which diagonalises in an orthonormal
basis \cite{mercer_theorem} $\{\phi_n\}_n$ of
$L^2(\mathcal{Z})$, with non-negative
eigenvalues. As a result, $K(\bbx,\bbx')$ admits 
a representation
$$K(\bbx,\bbx') = \sum_{n\geq 1} \lambda_n \phi_n(\bbx)\phi_n(\bbx')~,$$
which yields
$$K(\bbx,\bbx') = \langle \Phi(\bbx), \Phi(\bbx') \rangle~,$$
with $\Phi(\bbx)=(\lambda_n^{1/2} \phi_n(\bbx)  )_n$.
In Kernel methods 
it is thus sufficient 
to construct positive definite kernels $K$ on $\mathcal{Z}^2$
in order to extend linear classification tools
to more complex relationships.
%Support Vector Machines (SVMs) are particular
%instances of kernel methods, which construct 
%separating hyperplanes in supervised 
%learning tasks. %We shall discuss these methods
%in further detail on Chapter \ref{classif_chapt}.

%supervised learning.
%High dimensional problem. 
%svm
% be careful because it works well in some areas.
Despite their success and effectiveness in 
a number of machine learning tasks, the high
dimensional embeddings induced by kernel
methods do not automatically 
enjoy the stability
properties to additive noise or deformations.
%On supervised learning applications, 
The kernel needs to be chosen accordingly. 
\emph{Convolutional Kernels Networks} \cite{mairal2014convolutional,bietti2017group} 
have been developed to capture the geometric stability properties 
and offer competitive empirical performance to modern deep architectures. 
These kernels contrast with another recent family of \emph{Neural Tangent Kernels} 
\cite{jacot2018neural}, which linearize a generic deep architecture around 
its parameter initialization, and which do not offer the same amount of geometric stability \cite{bietti2019inductive}. 

\begin{comment}


\subsubsection{Deformable Templates}

The theory of deformable templates, 
pioneered by Grenader in \cite{grenader_orig}, 
 introduced the notion of group action 
to construct metrics in a generic object space. 
A deformable template is defined as an 
element $x \in \mathcal{X}$ on which a group action 
$G \times \mathcal{X}$, $(g, x)\mapsto g.x \in \mathcal{X}$ is defined. 
This action defines through the orbits $\{g.x\,,\, g \in G\}$ 
a family of ``deformed" objects. 

This structure allows us to 
incorporate the group action into 
the notion of similarity between elements
of $\mathcal{X}$.
A metric $d$ on $\mathcal{X}$ can be 
constructed from a metric $\tilde{d}$ on the lifted product
space $G \times \mathcal{X}$ \cite{miller_younes}. 
If $\tilde{d}$ is \emph{left-invariant} \footnote{A distance $\tilde{d}$ 
on $G \times \mathcal{X}$ is left-invariant \cite{miller_younes} 
if, for all $h,g,g' \in G$ 
and all $x$, $x' \in \mathcal{X}$, 
$$\tilde{d}(h.(g,x) , h.(g',x')) = \tilde{d}( (g,x), (g',x'))$$ } , 
then
\begin{equation}
\label{def_templates_metric}
d(x, x') = \inf \{\tilde{d}( (id,x), (g,g.x') )\,,\, g \in G   \}~
\end{equation}
defines a metric from the set distance
between the orbits of 
$x$ and $x'$ under the action of $G$.

If $G$ denotes the group of $\mathbb{R}^d$ 
diffeomorphisms, acting on 
$\mathcal{X}=\LD$ by composition, then
the construction (\ref{def_templates_metric})
has the capacity to express the similarity 
between $x$ and its deformed version $g.x$
in terms of the amount of deformation 
$g$. 
%metric $\tilde{d}$ 
%has the capacity to express the similarity 
%between two elements $(1,x)$, $(g,x)$ of 
%$G \times \mathcal{X}$ in terms of the amount
%of deformation $g$ applied to $x$. 
%In general,  
%there is no simple way to construct 
% left-invariant metrics on the product space
%$G \times \mathcal{X}$.
% In \cite{trouve-younes}, Trouvï¿½
%and Younes construct a differential structure on 
%the product space, based on infinitesimal deformations 
%and amplitude variations. This structure then 
%enables the definition and computation of
%geodesics.

The computation of the distance 
$d(x,x')$ in (\ref{def_templates_metric}) requires
to optimize a deformation cost function, 
which in general is an ill-posed inverse problem.
In some applications, however, such as medical
imaging, it uncovers essential information 
about the underlying deformation process. 


%differential approach in \cite{trouve_younes}
%define geodesics
In general, there is no simple way to construct 
meaningful left-invariant metrics on the product space
$G \times \mathcal{X}$. In \cite{trouve-younes}, Trouve
and Younes construct a differential structure on 
this space, based on infinitesimal deformations 
and amplitude variations. This structure then 
enables the definition and computation of
geodesics. More specifically, the authors 
consider infinitesimal perturbations of 
a template $I \in \mathcal{X}$ of the form
\begin{eqnarray*}
G \times \mathcal{X} &\longrightarrow& \mathcal{X}~, \\
(\tau,\sigma) &\longmapsto& \tilde{I}^{\epsilon}_{(v,z)}(u) = I(u-\epsilon \tau(u)) + \beta \epsilon \sigma(u) + o(\epsilon)~,
\end{eqnarray*}
for small $\epsilon >0$. 
The vector field $\tau(u)$ thus carries the geometrical transformation,
whereas the scalar function $\sigma(u)$ represents infinitesimal 
amplitude variations. One then constructs the tangent 
space $T_I$ from these infinitesimal variations:
$$T_I = \left\{ \lim_{\epsilon\to 0} \frac{\tilde{I}^{\epsilon}_{(\tau,\sigma)} - I}{\epsilon} \,,\, (\tau,\sigma) \in G\times \mathcal{X} \right\}~.$$
By considering a norm $|\,\cdot \,|_{W}$ on 
$G \times \mathcal{X}$, one can define a metric $|\, \cdot\, |_{W'}$ on 
the tangent space $T_I$, which leads to the geodesic
distance 
$$d_W(I_0, I_1) = \inf \left\{ \int_0^1 |\dot{\gamma}(t) |_{W'} dt~, \gamma(0)=I_0\,\,,\gamma(1)=I_1 \right\} ~,$$
where the infimum is taken over all paths $\gamma$ joining the two 
images $I_0$, $I_1$.

The computation of such geoedesics thus requires to solve a
variational problem, which in particular estimates deformations
minimizing a trade-off between the geometric and photometric
components of the perturbation of the form
$\hat{\varphi} = \argmin \| I_1 - I_0 \circ \varphi \|_2^2 + D_G(Id, \varphi)^2~,$
where $D_G$ is a metric on the space of diffeomorphisms.

The estimation of diffeomorphisms is a difficult inverse problem. 
Several methods have been proposed, for
instance in \cite{trouve_diffgroups, trouve_stats}, with applications
in medical imaging \cite{allassonniere_1} and classification \cite{allassonniere_2}.
As we shall see, the scattering metric $d(x,x')=\| \Phi(x) - \Phi(x')\|$ 
where $\Phi$ linearises small deformations is a powerful alternative that
bypasses the need to estimate this inverse problem.

\end{comment}

\subsubsection{Power Spectra, Autocorrelation and Registration Invariants}
\label{transdef}

Translation invariant representations
can be obtained from
registration, auto-correlation or Fourier modulus operators.
However, the resulting representations are
not Lipschitz continuous to deformations.

A representation $\Phi (\bbx)$ is translation invariant %to global translations 
if it maps global translations $\bbx_c(u) = \bbx(u-c)$ by $c \in \R^d$ 
of any function $x \in \LD$ to the same image:   
\begin{equation}
\label{invar}
\forall \, \bbx \in \LD\,,\,\forall \, c \in \R^d~,~\Phi (\bbx_c) = \Phi (\bbx)~.
\end{equation}

The Fourier transform modulus is an example of a translation invariant
representation. Let  $\hat \bbx (\omega)$ be the Fourier transform of $\bbx(u) \in \LD$.
Since  
$\widehat {\bbx_c} (\omega) = e^{-i c.\omega}\, \hat \bbx(\omega)$, it follows that
$|\widehat {\bbx_c} | = |\hat \bbx|$ does not depend upon $c$.

A  Fourier modulus is translation invariant and stable to additive noise,
but unstable to small deformations at high frequencies \cite{stephane},
as illustrated with the following dilation example.
Let $\tau(u)=su$ denote a linear 
displacement field where $|s|$ is small,
%a small factor $|s|$, % such that $| 1- s |$ is small, 
and let $\bbx(u)=e^{i \xi u} \theta(u)$ be a modulated
version of a lowpass window $\theta(u)$. 
Then the dilation $\bbx_\tau(u) = \dL{\tau} \bbx(u)=\bbx((1+s)u)$ 
moves the central
frequency of $\hat{\bbx}$ from $\xi$ to $(1+s) \xi$. 
If $\sigma^2_\theta = \int |\om|^2 |\hat{\theta}(\om)|^2 d\om$ 
measures the frequency spread of $\theta$, 
then 
$$\sigma^2_x=\int |\om-\xi|^2 |\hat{x}(\om)|^2 d\om= \sigma^2_\theta~,$$
and 
\begin{eqnarray*}
\sigma^2_{x_\tau} &=& (1+s)^{-d} \int (\om-(1+s)\xi)^2 |\hat{x}((1+s)^{-1}\om)|^2 d\om \\
&=& \int |(1+s)(\om-\xi)|^2 |\hat{x}(\om)|^2 d\om = (1+s)^2 \sigma^2_x~.
\end{eqnarray*}
It follows that if the distance between 
the central frequencies of $\bbx$ and $\bbx_\tau$,
$s \xi$,  is large compared to their frequency 
spreads, $ (2+s) \sigma_\theta$,
 then the frequency supports of $\bbx$ and $\bbx_\tau$ 
 are nearly disjoint and hence 
 $$\| | \hat{\bbx}_\tau | - | \hat{\bbx} | \| \sim \| \bbx \| ~,$$
which shows that $\Phi(\bbx) = |\hat{\bbx}|$ is not Lipschitz
continuous to deformations, 
since $\xi$ can be arbitrarily large.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figures/fourier_unstab.pdf}
\caption{Dilation of a complex bandpass window. If $\xi \gg \sigma_x s^{-1}$, then the supports are nearly
disjoint.}
\label{fourier_unstab}
\end{figure}

%auto-correlation
The autocorrelation of $\bbx$
$$R_\bbx(v) = \int \bbx(u) \bbx^*(u-v) du$$ 
is also translation invariant: $R_\bbx = R_{\bbx_c}$.
Since $R_\bbx(v) = \bbx \star \overline{\bbx} (v)$, 
with $\overline{\bbx}(u) = \bbx^*(-u)$, it follows that
%the autocorrelation representation $\Phi (\bbx) = R_\bbx$ satisfies
$\widehat {R_x} (\om) = |\hat x (\omega)|^2~.$
The Plancherel formula thus proves that it has the same instabilities
as a Fourier transform:
\[
\| R_\bbx - R_{\bbx_\tau} \|= (2 \pi)^{-1} \| |\hat{\bbx}|^2 - |\hat{\bbx}_\tau|^2 \|~.
\]

Besides deformation instabilities, 
the Fourier modulus and the autocorrelation lose 
too much information.
For example,
a Dirac $\delta (u)$ and a linear chirp $e^{i u^2}$ are two
signals having Fourier
transforms whose moduli are equal and constant. 
Very different signals
may not be discriminated from their Fourier modulus.

%canonical invariant
A canonical invariant \cite{mnist_deformation,Soatto}
$\Phi(\bbx) = \bbx(u -a(\bbx))$ registers $\bbx \in \LD$ with 
an anchor point $a(\bbx)$, which is translated when $\bbx$ is translated:
$$a(\bbx_c) = a(\bbx) + c~.$$ 
It thus defines a translation invariant representation: $\Phi \bbx_c = \Phi \bbx$.
For example, the anchor point may be a filtered
maximum $a(\bbx) = \arg \max_{u} |\bbx \star h(u)|$,
for some filter $h(u)$.
A canonical invariant $\Phi \bbx(u) = \bbx(u - a(\bbx))$ 
carries more information
than a Fourier modulus, and characterizes $\bbx$ up to 
a global absolute position information \cite{Soatto}.
However, it has the same high-frequency instability
as a Fourier modulus transform. Indeed, for any choice of anchor
point $a(\bbx)$, applying
the Plancherel formula proves that
\[
\| \bbx(u-a(\bbx)) - \bbx'(u-a(\bbx')) \|\geq (2 \pi)^{-1}\,\||\hat \bbx(\om)| - |\hat \bbx'(\om)| \| ~.
\]
If $\bbx' = \bbx_\tau$, 
the Fourier transform instability at high frequencies implies that
$\Phi \bbx = \bbx(u-a(\bbx))$ is also unstable with respect to deformations.

