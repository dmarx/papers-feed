\section{Experiment Details}
\subsection{Dataset Details}
\label{sec:dataset_detail}

\paragraph{GSM8K}
A widely used math reasoning dataset comprising 8,000 problems, including a 1,300 problems test set that assesses models' capabilities in arithmetic reasoning and formulating mathematical steps using language~\cite{cobbe2021training}. For this dataset, we employ the complex multi-step CoT prompt~\cite{fu2023complexitybased}\footnote{https://github.com/FranxYao/chain-of-thought-hub} as the original prompt.

\paragraph{BBH}
A suite of language and symbolic reasoning tasks, consisting of 6,500 problems across 23 subsets, specifically designed to evaluate chain-of-thought prompting. In our experiment, we adopt the 3-shot CoT prompt\footnote{https://github.com/suzgunmirac/BIG-Bench-Hard} as the original prompts, following the approach described by \citet{suzgun2022challenging}.

\paragraph{ShareGPT}
A conversation dataset from ShareGPT.com platform~\cite{sharegpt} which includes users sharing conversations with ChatGPT in different languages and in various scenarios (e.g., coding, chitchat, writing assistant, etc.). We use a dataset of 575 samples provided by \citet{li2023unlocking} as our test set. We use all dialogues except the final round as the prompt and generate results with GPT-3.5-Turbo as the reference.

\paragraph{Arxiv-March23}
A dataset consisting of latest academic papers created in March 2023 from the arXiv preprint repository. We use 500 data items collected by \citet{li2023unlocking} as the test set. Due to the excessive length of some articles, we take the first five sections of each article and truncate each section to 10,000 characters. Then, we concatenate these sections to form the original prompt and use GPT-3.5-Turbo to generate the summary as the reference.

\subsection{Other Implementation Details}
All experiments were conducted using a Tesla V100 (32GB). We trained the GPT2-Alpaca model on the Alpaca dataset\footnote{https://github.com/tatsu-lab/stanford\_alpaca} for eight epochs using a learning rate of 1e-4 and the AdamW optimizer~\cite{loshchilov2018decoupled}. The training process took approximately 150 minutes to complete.
We use tiktoken\footnote{https://github.com/openai/tiktoken} and GPT-3.5-Turbo model to count all the tokens.

% \section{Prompt Examples}

\section{Economic Cost}
\input{figures/cost}

Table~\ref{tab:cost} displays the estimated inference costs for various datasets, according to the pricing of GPT-3.5-Turbo. Our approach showcases significant savings in computational resources and monetary expenditures, with cost reductions of \$4.7, \$8.0, \$0.4, and \$0.8 observed in the GSM8K, BBH, ShareGPT, and Arxiv datasets, respectively.


% \section{Application: KV Cache Compression}
% \input{figures/kv_cache}


\section{{Instructions used in GPT-4 Generation}}
\label{sec:gpt4_generation_instructions}

The instructions we used in the GPT-4 Generation are shown below:
\begin{enumerate}
    \setlength{\itemsep}{-0.1cm}
    % \vspace{-0.2cm}
    \item \textit{Could you please rephrase the paragraph to make it short, and keep 5\% tokens?}
    \item \textit{Condense the passage to retain only 5\% of its original tokens, while preserving its meaning.}
% Could you please rephrase the paragraph to make it short, and keep 5\% tokens?
% Condense the passage to retain only 5\% of its original tokens, while preserving its meaning.
\item \textit{Short the sentences to 200 tokens.}
\item \textit{Trim the text down to 200 tokens in total.}
\item \textit{Please provide a concise summary of the given examples in several sentences, ensuring that all reasoning information is included.}
\item \textit{Summarize the provided examples in a few sentences, maintaining all essential reasoning aspects.}
\item \textit{Remove redundancy and express the text concisely in English, ensuring that all key information and reasoning processes are preserved.}
\item \textit{Eliminate repetitive elements and present the text concisely, ensuring that key details and logical processes are retained.}
\item \textit{Follow these steps to shorten the given text content: 1. First, calculate the amount of information contained in each sentence, and remove sentences with less information. 2. Next, further condense the text by removing stop words, unnecessary punctuation, and redundant expressions. Refine the content while ensuring that all key information is retained. Let's do it step by step.}
\item \textit{To shorten the given text, follow these steps: a) Determine the information value of each sentence and remove those with lower value. b) Further reduce the text by removing stop words, unneeded punctuation, and superfluous expressions, while making sure to keep all vital information intact. Let's do it step by step.}
\end{enumerate}




\section{Recovering Compressed Prompts with Large Language Model}
\label{sec:recover_cases}

In this section, we showcase several examples of employing black-box LLMs to reconstruct compressed prompts. Specifically, we have selected three compressed prompts with varying compression ratios, produced by distinct small language models, on different datasets. These prompts, accompanied by guiding instructions, will serve as input for the GPT-4 model.

\begin{figure*}[htb]
    \begin{tcolorbox}
    \textbf{Original Prompt(9-steps Chain-of-Thought):} \\
    Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for \$10 each box. He rearranged five of these boxes into packages of six highlighters each and sold them for \$3 per package. He sold the rest of the highlighters separately at the rate of three pens for \$2. How much profit did he make in total, in dollars?\\\\
    Let's think step by step\\
    Sam bought 12 boxes x \$10 = \$120 worth of highlighters.\\
    He bought 12 * 30 = 360 highlighters in total.\\
    Sam then took 5 boxes × 6 highlighters/box = 30 highlighters.\\
    He sold these boxes for 5 * \$3 = \$15\\
    After selling these 5 boxes there were 360 - 30 = 330 highlighters remaining.\\
    These form 330 / 3 = 110 groups of three pens.\\
    He sold each of these groups for \$2 each, so made 110 * 2 = \$220 from them.\\
    In total, then, he earned \$220 + \$15 = \$235.\\
    Since his original cost was \$120, he earned \$235 - \$120 = \$115 in profit.\\
    The answer is 115\\
    \textbf{Compressed Prompt:}\\
    : Sam bought a dozen boxes each 30 highl pens inside, \$10 each. He reanged five of boxes into of six each \$3 per. He sold the thelters separately at the of three \$2. much make total,\\
    Lets think step\\
     bought boxes x0 oflters\\
    He 2 3ters in\\
    Sam then boxes 6lters/box 0ters\\
    He sold these boxes 5\\
    Afterelling these boxes there 36030lters\\
    ese00 of three\\
     sold groups2 each so made *2 \$20 from\\
    In total, he015\\
    Since his he \$ - \$120 = \$115 in profit.\\
    The answer is 115\\
    \textbf{Recovered Compressed Prompt(9-steps Chain-of-Thought):}\\
    % Recover Prompt: Please rewrite the sentence to recover the detail information, temp=0.7, max=800, top_p:0.95
    Question: Sam purchased a dozen boxes, each containing 30 highlighter pens, for \$10 each. He rearranged five of these boxes to have six pens each and sold them for \$3 per box. He then sold the remaining highlighters individually in groups of three for \$2 each. Let's calculate the total profit.\\
    \\
    Initially, Sam bought 12 boxes with 30 highlighters each, which equals 360 highlighters. He spent \$10 per box, so his total cost was \$120.\\
    Sam then rearranged 5 boxes to have 6 highlighters per box, which equals 30 highlighters. He sold these boxes for \$3 each, making \$15.\\
    After selling these boxes, he had 330 highlighters left (360 - 30). He sold these in groups of three, which equals 110 groups. He sold each group for \$2, making \$220 from this.\\
    \\
    In total, Sam made \$235 (\$15 + \$220). Since his initial cost was \$120, he earned a profit of \$115 (\$235 - \$120).\\
    \\
    The answer is \$115.
    \end{tcolorbox}
    \caption{Recovering the compressed prompt($1/\tau$=17x, Alpaca-7B as small language model) from GSM8K using GPT-4.}
    \label{fig:prompt_recovered}
\end{figure*}

\begin{figure*}
	\centering
	\begin{minipage}[t]{\linewidth}
       \begin{tcolorbox}
        \textbf{Compressed Prompt:}\\
        Question: Sam a dozen boxes with 30ighter pens, each Heanged into of3 the separately the of three. much total, in\\
        's think step bought boxes \$120l\\
         bought 12 30 = 360\\
        Sam took boxes 6ighters for \$15\\
         these boxes were 360 - =ighters\\
        These form 330 = 110 of three these so made * \$220 from them.\\
        In total, then, he earned \$220 + \$15 = \$235.\\
        Since his original cost was \$120, he earned \$235 - \$120 = \$115 in profit.\\
        The answer is 115\\
        \textbf{Recovered Compressed Prompt(7-steps Chain-of-Thought):}\\
        % Recover Prompt: Could you help me recover the incomplete sentence, temp=0.7, max=800, top_p:0.95
        Question: Sam bought a dozen boxes with 30 lighters each. He arranged them into groups of three and sold them separately. In total, he spent \$120 on the lighters. How much profit did he make?\\
        Step 1: Sam bought 12 boxes * 30 lighters each = 360 lighters in total.\\
        Step 2: Sam took 6 boxes of lighters and sold them for \$15.\\
        Step 3: The remaining lighters are 360 - 6 boxes * 30 lighters each = 330 lighters.\\
        Step 4: These 330 lighters form groups of three, so he made 110 groups * \$2 per group = \$220 from them.\\
        Step 5: In total, he earned \$220 + \$15 = \$235.\\
        Step 6: Since his original cost was \$120, he earned \$235 - \$120 = \$115 in profit.\\
        The answer is \$115.
        \end{tcolorbox}
        \caption{Recovering the compressed prompt ($1/\tau$=19x, GPT2-Alpaca as small language model) from GSM8K using GPT-4.}
        \label{fig:prompt_recovered_gpt2}
	\end{minipage}
	\\
	\begin{minipage}[t]{\linewidth}
        \begin{tcolorbox}
        \textbf{Original Prompt:} \\
        Questions that involve enumerating objects and asking the model to count them.\\
        Q: I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a flute, an orange, and a violin. How many fruits do I have?\\
        A: Let's think step by step.\\
        We first identify the fruits on the list and include their quantity in parentheses:\\
        - blackberry (1)\textbackslash n
        - nectarine (1)\textbackslash n
        - plum (1)\textbackslash n
        - strawberry (1)\textbackslash n
        - banana (1)\textbackslash n
        - orange (1)\\
        Now, let's add the numbers in parentheses: 1 + 1 + 1 + 1 + 1 + 1 = 6. So the answer is 6.\\
        \textbf{Compressed Prompt:}\\
        Questions that involve enumerating objects and asking the model to count them.\\
        : a blackberry, a a ne a a a a, many have\\
        :'s think\\
        We first theruits the list and include their in\\
        - (--\\
        's the numbers in parentheses:1 + 1 = 6. So the answer is 6.\\
        \textbf{Recovered Compressed Prompt:}\\
        % Recover Prompt: Please rewrite the sentence to recover the detail information, temp=0.7, max=800, top_p:0.95
        Question: In a basket, there is a blackberry, an apple, a pear, a plum, a peach, and an orange. How many fruits are in the basket?\\
        Let's think step by step. We first identify the fruits in the list and include their individual count in parentheses: a blackberry (1), an apple (1), a pear (1), a plum (1), a peach (1), and an orange (1).\\
        Now, let's add the numbers in parentheses: 1 + 1 + 1 + 1 + 1 + 1 = 6. So the answer is 6. There are 6 fruits in the basket.
        \end{tcolorbox}
        \caption{Recovering the compressed prompt($1/\tau$=7x, Alpaca-7B as small language model) from BBH using GPT-4.}
        \label{fig:prompt_recovered_bbh}
	\end{minipage}
\end{figure*}



\section{Cases Study}
\label{sec:cases_study}

We present various cases from multiple datasets, encompassing compressed prompts, outcomes derived from original prompts, outcomes derived from compressed prompts, and results achieved utilizing the selective-context approach.

% \subsection{GSM8K}

\begin{figure*}[htb]
    \begin{tcolorbox}
    \textbf{Compressed Prompt:}\\
    Follow the given examples and answer the question.\\
    : Sam bought a dozen boxes each 30 highl pens inside, \$10 each. He reanged five of boxes into of six each \$3 per. He sold the thelters separately at the of three \$2. much make total,\\
    Lets think step\\
     bought boxes x0 oflters\\
    He 2 3ters in\\
    Sam then boxes 6lters/box 0ters\\
    He sold these boxes 5\\
    Afterelling these boxes there 36030lters\\
    ese00 of three\\
     sold groups2 each so made *2 \$20 from\\
    In total, he015\\
    Since his he \$ - \$120 = \$115 in profit.\\
    The answer is 115\\\\
    Q: Toula went to the bakery and bought various types of pastries. She bought 3 dozen donuts which cost \$68 per dozen, 2 dozen mini cupcakes which cost \$80 per dozen, and 6 dozen mini cheesecakes for \$55 per dozen. How much was the total cost?\\
    \textbf{Reference Answer(4-steps):}\\
    The total charge for the doughnuts was 3 x \$68 = \$<<3*68=204>>204.\\
    The total charge for the mini cupcakes was 2 x \$80 = \$<<2*80=160>>160.\\
    The total charge for the mini cheesecakes was 6 x \$55 = \$<<6*55=330>>330.\\
    Therefore the total amount Toula paid for the pastries was \$204 + \$160 + \$330 = \$<<204+160+330=694>>\boxed{694}.\\
    \textbf{The Generation of Compressed Prompt(4-steps):}\\
    Toula bought 3 dozen donuts which cost \$68 per dozen. So, she spent 3 x 68 = \$204 on donuts.\\
    She also bought 2 dozen mini cupcakes which cost \$80 per dozen. So, she spent 2 x 80 = \$160 on mini cupcakes.\\
    Finally, she bought 6 dozen mini cheesecakes for \$55 per dozen. So, she spent 6 x 55 = \$330 on mini cheesecakes.\\
    Therefore, the total cost was 204 + 160 + 330 = \$694.\\
    The answer is \boxed{694}.\\
    \textbf{The Generation of Selective-Context:}\\
    A: Toula bought 3 x 12 = <<3*12=36>>36 donuts.\\
    Then, the total cost of the donuts is 36 x \$68 = \$<<36*68=2448>>2448.\\
    How many mini cupcakes did Toula buy in total?\\
    She bought 2 x 12 = <<2*12=24>>24 mini cupcakes.\\
    So, the total cost of the mini cupcakes is 24 x \$80 = \$<<24*80=1920>>1920.\\
    How many mini cheesecakes did Toula buy in total?\\
    She bought 6 x 12 = <<6*12=72>>72 mini cheesecakes.\\
    Thus, the total cost of the mini cheesecakes is 72 x \$55 = \$<<72*55=3960>>3960.\\
    Therefore, the total cost of all the pastries is \$2448 + \$1920 + \$3960 = \$<<2448+1920+3960=8328>>8328. Answer: \boxed{8328}.
    \end{tcolorbox}
    \caption{Cases study on GSM8K math reasoning dataset in half-shot constraint.}
    \label{fig:case_gsm8k}
\end{figure*}

% \subsection{BBH}

\begin{figure*}[htb]
    \begin{tcolorbox}
    \textbf{Compressed Prompt:}\\
    Evaluate a random boolean function expressed as a word problem.\\
    Q: Question: Fidel tells the truth.\\
    Jerry saysidel tells truth.\\
    Vina says Jerry\\
    Mill saysina\\
    Raymond saysent\\
    (3)ina says Jerry\\
    () saysina\\
    ) saysent\\
    We know from ( that Raymond truth.\\
    So the answer is Yes.\\
    Q: Question: Raymond tells the truth. Sal says Raymond lies. Alexis says Sal lies. Helene says Alexis lies. Elanor says Helene lies. Does Elanor tell the truth? \\
    A: Let's think step by step.\\
    \textbf{The Generation of Full-shot Prompt(5-steps):}\\
    (1) Raymond tells the truth. So, we know that Raymond tells the truth.\\
    (2) Sal says Raymond lies. Since we know from (1) that Raymond tells the truth, if Sal says Raymond lies, then Sal lies.\\
    (3) Alexis says Sal lies. Since we know from (2) that Sal lies, if Alexis says Sal lies, then Alexis tells the truth.\\
    (4) Helene says Alexis lies. Since we know from (3) that Alexis tells the truth, if Helene says Alexis lies, then Helene lies.\\
    (5) Elanor says Helene lies. Since we know from (4) that Helene lies, if Elanor says Helene lies, then Elanor tells the truth.\\
    Now, the question asks: Does Elanor tell the truth? We know from (5) that Elanor tells the truth. So the answer is \boxed{Yes}.\\
    \textbf{The Generation of Compressed Prompt(5-steps):}\\
    1. Raymond tells the truth.\\
    2. Sal says Raymond lies. This means Sal lies.\\
    3. Alexis says Sal lies. This means Alexis tells the truth.\\
    4. Helene says Alexis lies. This means Helene lies.\\
    5. Elanor says Helene lies. This means Elanor tells the truth.\\
    So the answer is \boxed{Yes}.\\
    \textbf{The Generation of Selective-Context:}\\
    1. Raymond says Millicent lies. If Raymond tells the truth, then Millicent lies. If Raymond lies, then Millicent tells the truth. We don't know which one is true.\\
    2. Millie says Kristian lies. If Millie tells the truth, then Kristian lies. If Millie lies, then Kristian tells the truth. We don't know which one is true.\\
    3. Michaela says Kristian lies. If Michaela tells the truth, then Kristian lies. If Michaela lies, then Kristian tells the truth. We don't know which one is true.\\
    4. Osvaldo says Raymond tells the truth. If Osvaldo tells the truth, then Raymond tells the truth. If Osvaldo lies, then Raymond lies. We don't know which one is true.\\
    5. Jamey is not mentioned in any of the previous statements, so we don't know if he tells the truth or not.\\
    Now let's move on to the second question.
    \end{tcolorbox}
    \caption{Cases study on web\_of\_lies of BBH reasoning dataset in quarter-shot constraint.}
    \label{fig:case_bbh}
\end{figure*}

% \subsection{ShareGPT}

\begin{figure*}[htb]
    \begin{tcolorbox}
    \textbf{Compressed Prompt:}\\
    Please provide the example of the radiation beam spectrum in the TOPAS format, and I will learn from it.\\
    The beam energy spectrum is composed of 12 energy levels with corresponding weights.\\
    dv:So/SARRPbeam/BeamEnergySpectrumValues: Lists the energy values in the spectrum, separated by tabs.\\
    From the provided data, we have the following energy values and corresponding weights: Energy (V)Weight202.15E-0520.12.29E-0520.22.43E0520.32.580520.42.74052.52.9152.63.0 952.7.29052.8.49E-0520.93.71E05213.94E-0521.14.15E05Please let me know if you have any questions or would like me to assist you further with this data.\\
    Now I'll give you a table with the spectrum of an ElektaAC and I want you to transform it to the topas format:\\
    Please provide the table with the spectrum of the Elekta LINAC, and I will transform it into theAS format for you.\\
    E (MeV)	Elekta (6 MV)\\
    0.25	0.173E—04(1.5\%)\textbackslash n
    .5	.100E03(0.5\%)\textbackslash n
    .75	.110E03(.5\%)\textbackslash n
    1	0.952E—04(0.5\%)\textbackslash n
    1.25	.827E04(.5\textbackslash n
    .5	.721E04(.\textbackslash n
    .	.63E04(.\textbackslash n
    2	.55E04(.\%)\textbackslash n
    .	.493E04(0.5\%)\textbackslash n
    2.5	.437E04(.6\textbackslash n
    2.5	.386E04(.6\textbackslash n
    3	.34E4(.6\textbackslash n
    .	.30E4(.6\textbackslash n
    .	.27E4(.7\textbackslash n
    3.7	0.240E—04(.7\textbackslash n
    4	.21E04(.7\textbackslash n
    .	.18E(.8\textbackslash n
    .	.16E(.8\textbackslash n
    .14E(.9\textbackslash n
    5	.121E04(.9\textbackslash n
    5.2	.101E4(1.0\textbackslash n
    .	.81E0(.1\textbackslash n
    .	.64E.2\textbackslash n
    6	.45E.4\textbackslash n
    .0.300E—05(1.8\textbackslash n
    6.5	.15E0(.4\textbackslash n
    .7	.73E0(.4\textbackslash n
    7	.29E0(.0\textbackslash n
    .	.88E0(.\textbackslash n
    	.18E—07(19.8\%)\textbackslash n
    7.75	0.277E—08(50.0\%)\textbackslash n
    8	0.159E—08(70.8\%)\\
    \textbf{The Generation of Full-shot Prompt:}\\
    Thank you for providing the table with the Elekta LINAC spectrum. Here's the TOPAS format for the same:\\
    \# Spectrum\\
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cccccccc}
    \toprule
        s:...Type & \multicolumn{5}{@{}l}{{"Discrete"}} \\
        \hline
        \multirow{4}{*}{{dv:...Values }} 
        & 0.25 & 0.5 & 0.75 & 1 & 1.25 & 1.5 & 1.75 & 2 \\& 2.25 & 2.5 & 2.75 & 3 & 3.25 & 3.5 & 3.75 & 4 \\& 4.25 & 4.5 &4.75 & 5 & 5.25 & 5.5 & 5.75 & 6 \\& 6.25 & 6.5 & 6.75 & 7 & 7.25 & 7.5 & 7.75 & 8  \\\hline
        \multirow{4}{*}{{uv:...Weights }} 
        & 0.173E-04 & 0.100E-03 & 0.110E-03 & 0.952E-04 & 0.827E-04 & 0.721E-04 & 0.635E-04 & 0.557E-04 \\& 0.493E-04 & 0.437E-04 & 0.386E-04 & 0.345E-04 & 0.302E-04 & 0.270E-04 & 0.240E-04 & 0.210E-04 \\& 0.186E-04 & 0.164E-04 & 0.142E-04 & 0.121E-04 & 0.101E-04 & 0.817E-05 & 0.645E-05 & 0.455E-05 \\& 0.300E-05 & 0.156E-05 & 0.734E-06 & 0.294E-06 & 0.882E-07 & 0.188E-07 & 0.277E-08 & 0.159E-08\\
        \bottomrule
    \end{tabular}}\\
    I hope this helps. Let me know if you have any questions or if you need further assistance. \\
    \textbf{The Generation of Compressed Prompt:}\\
    Thank you for providing the table of the EletaAC spectrum. Here's how it can be formatted in the TOPAS format:\\
    \# Spectrum\\
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cccccccc}
    \toprule
        s:...Type & \multicolumn{5}{@{}l}{{"Discrete"}} \\
        \hline
        \multirow{2}{*}{{dv:...Values }} 
        & 0.25 & 1.5 & 2 & 3.2 & 4 & 5 & 6 & 7 \\& 7.25 & 7.5 & 7.75 & 8  \\\hline
        \multirow{2}{*}{{uv:...Weights }} 
        % uv:ElektaLINAC/BeamEnergySpectrumWeights
        & 173E04 & .10E-0 & .11E1 & .950 & .82 & .63E-0 & .55E0 & .4943 \\& .345E0 & .30E0 & .27 & .24E0 & .16E0 & .714E52 & .64E0 & 6.45E0 \\& .30E0 & .15E0 & .73E0 & .294E-06 & .882E-07 & .188E-07 & .277E-08 & .159E-08 \\
        \bottomrule
    \end{tabular}}\\
    
    Please let me know if you have any questions or if you need further assistance with this data.\\
    \textbf{The Generation of Selective-Context:}\\
    I'm sorry, but I cannot see the table you are referring to. Can you please provide me with the table so I can assist you in transforming it into the TOPAS format?
    \end{tcolorbox}
    \caption{Cases study on ShareGPT conversation dataset in 2x constraint.}
    \label{fig:case_sharegpt}
\end{figure*}

% \subsection{Arxiv-March23}

\begin{figure*}[htb]
    \begin{tcolorbox}
    \textbf{Compressed Prompt:}\\
    Redshift drift in a universe with structure II: Light rays propagated through aian Nbody simulation\\\\koksbang@cp3.sdk\\	CP3-Orins,mark,, Mmark\\\\Theshiftift is computed alongs propagating through aulated universe on the Newtonian Nbody simulation code GGET2 with a perturbedmann-Lreer metric in theian gauge.\\It is that the meanshiftift is to theift of the meanshift to the precision of the numericalations and that this is to a high of cancellation between two dominant components theshiftift.\\This result is contrary to earlier findings based on inhomogeneous cosmological models exhibiting cosmic backreaction.\\For simplicity, the results neglect contributions from optical drift.\\Based on a study of the redshift drift in a Lemaitre-Tolman-Bondi model, the optical drift effects are estimated to be at most of order 10\% of the redshift drift signal.\\
    \textbf{The Generation of Full-shot Prompt:}\\
    The article discusses the computation of redshift drift along light rays in a simulated universe based on the Newtonian N-body simulation code GADGET-2 combined with a perturbed Friedmann-Lemaitre-Robertson-Walker metric in the Newtonian gauge. The study finds that the mean redshift drift is equal to the drift of the mean redshift to the precision of the numerical computations, and this is due to a high degree of cancellation between two dominant components of the redshift drift. The study neglects contributions from optical drift, but it is estimated that these effects are at most of order 10\% of the redshift drift signal. The study also finds that the redshift drift contribution from peculiar acceleration of the emitter is negligible in the simulation setup, but it is expected that this contribution will be larger for real observations.\\
    \textbf{The Generation of Compressed Prompt:}\\
    The document discusses the computation of redshift drift in a simulated universe using the Nbody simulation code GGET2 with a perturbed Lemaître-Lemaître metric. The results show that the mean shift is equivalent to the shift of the mean to a high degree of cancellation between two dominant components. This contradicts earlier findings based on inhomogeneous cosmological models exhibiting cosmic backreaction. The document also estimates that optical drift effects are at most 10\% of the redshift drift signal.\\
    \textbf{The Generation of Selective-Context:}\\
    The document discusses the use of redshift drift measurements, which were first proposed in 1962. The study involves analyzing non-radial light rays observed by three different observers with non-radial lines of sight. The present-day density profile of the LTB model is shown in a figure, and quadri-linear interpolation is used to interpolate between snapshots. The observation time is set to 30 years, and the mean and fluctuations of the individual components of the redshift drift are shown in another figure. The study does not address the effects of peculiar acceleration.
    \end{tcolorbox}
    \caption{Cases study on Arxiv-March23 summarization dataset in 200 tokens constraint.}
    \label{fig:case_arxiv}
\end{figure*}

