
\subsection{Handling Perturbations}
Now that we have shown that h-MDS recovers an embedding exactly, we
consider the impact of perturbations on the data. Given the necessity
of high precision for some embeddings, we expect that in some regimes
the algorithm should be very sensitive. Our results identify the
scaling of those perturbations.


First, we consider how to measure the effect of a perturbation on the
resulting embedding.  We measure the gap between two configurations of
points, written as matrices in $\mathbb{R}^{n \times r}$, by the sum
of squared differences $D(X,Y) = \tr((X-Y)^T(X-Y))$. Of course, this
is not immediately useful, since $X$ and $Y$ can be rotated or
reflected without affecting the distance matrix used for MDS--as these
are isometries, while scalings and Euclidean translations are
not. Instead, we measure the gap by
\[D_E(X,Y) = \inf \{D(X,PY) : P^T P = I\}.\]
In other words, we look for the configuration of $Y$ with the smallest
gap relative to $X$. For Euclidean MDS, \citet{Sibson1} provides an
explicit formula for $D_E(X,Y)$ and uses this formulation to build a
perturbation analysis for the case where $Y$ is a configuration
recovered by performing MDS on the perturbed matrix $XX^T+\Delta(E)$,
with $\Delta(E)$ symmetric.

\paragraph{Problem setup} In our case, the perturbations affect the hyperbolic distances. Let $H \in 
\mathbb{R}^{n \times  n}$ be the distance matrix for a set of points in hyperbolic space. Let $\Delta(H) \in \mathbb{R}^{n \times n}$ be the perturbation, with $H_{i,i}= 0$ and $\Delta(H)$ symmetric (so that $\hat H = H + \Delta_{H}$ remains symmetric).
% To simplify our derivations, we assume that the perturbation of $H$
% does not alter the dominant (Perron-Frobenius) eigenvalue and
% eigenvector of $Y$ from (\ref{eq:ydefn}).
The goal of our analysis is
to estimate the gap $D_E(X,Y)$ between $X$ recovered from $H$ with
h-MDS and $\hat X$ recovered from the perturbed distances
$H+\Delta(H)$.

\begin{lemma}
\label{lemma:hmds-perturb}
Under the above conditions, if $\lambda_{\min}$ denotes the smallest nonzero eigenvalue of $X X^T$ then up to second order in $\Delta(H)$,
\[
  D_E(X,\hat X) \le \frac{2 n^2}{\lambda_{\min}} \sinh^2\left( \| H \|_{\infty} \right) \| \Delta(H) \|_{\infty}^2.
\]
\end{lemma}

The key takeaway is that this upperbound matches our intuition for the
scaling: if all points are close to one another, then $\|H\|_{\infty}$
is small and the space is approximately flat (since $\sinh^2(z)$ is
dominated by $2z^2$ close to the origin). On the other hand, points at great
distance are sensitive to perturbations in an absolute sense.
% Note that far away points may be represented by having very large norms in
% some choices of coordinates.
% This leads us to consider other notions of recovery, which we do next.

\begin{proof}[Proof of Lemma~\ref{lemma:hmds-perturb}]
Similarly to our development of h-MDS, we proceed by accessing the underlying Euclidean distance matrix, and then apply the perturbation analysis from \citet{Sibson2}. There are three steps: first, we get rid of the $\acosh$ in the distances to leave us with scaled Euclidean distances. Next, we remove the scaling factors, and apply Sibson's result.
Finally, we bound the gap when projecting to the Poincar{\'e} sphere.
 
\paragraph*{Hyperbolic to scaled Euclidean distortion} Let $Y$ denote the scaled-Euclidean distance matrix, as in (\ref{eq:hmds-Y}), so that $Y_{i,j} = \cosh(H_{i,j})$. Let $\hat Y_{i,j} = \cosh(H_{i,j} + \Delta(H)_{i,j})$.
We write $\Delta(Y) = \hat Y - Y$ for the scaled Euclidean version of the perturbation. We can use the hyperbolic-cosine difference formula on each term to write
\begin{align*}
  \Delta(Y)_{i,j}
  &=
  \cosh(\hat H_{i,j}) - \cosh(H_{i,j}) \\
  &=
  (\cosh(H_{i,j} + \Delta(H)_{i,j}) - \cosh(H_{i,j})) \\
  &=
  2\sinh\left( \frac{ 2 H_{i,j} + \Delta(H)_{i,j} }{2} \right) \sinh\left( \frac{ \Delta(H)_{i,j} }{2} \right).
\end{align*}
In terms of the infinity norm, as long as $\| H \|_{\infty} \ge \| \Delta(H) \|_{\infty}$ (it is fine to assume this because we are only deriving a bound up to second order, so we can suppose that $\Delta(H)$ is small), we can simplify this to
% \begin{align*}
%   \| \Delta(Y) \|_{\infty}
%   &\le
%   \sinh\left( \frac{ 2 \| H \|_{\infty} + \| \Delta(H) \|_{\infty} }{2} \right) \sinh\left( \frac{ \| \Delta(H) \|_{\infty} }{2} \right).
% \end{align*}
% As long as $\| H \|_{\infty} \ge \| \Delta(H) \|_{\infty}$, we can simplify this to
\begin{align*}
  \| \Delta(Y) \|_{\infty}
  &\le
  2\sinh\left( \| H \|_{\infty} \right) \sinh\left( \| \Delta(H) \|_{\infty} / 2 \right).
\end{align*}

% on each term to write
% \begin{align*}
% (SE&+\Delta(SE))_{i,j} \leq \\
% & \frac{1}{2} (\cosh(H_{i,j}) -1) + \frac{1}{2}\left(\cosh\left(\frac{\Delta(H)_{i,j}}{H_{i,j}} \right) \right),
% \end{align*}
% or, 
% \[ \Delta(SE))_{i,j} \leq \frac{1}{2}\left(\cosh\left(\frac{\Delta(H)_{i,j}}{H_{i,j}} \right) \right).\]
%  Now we have the $\Delta(SE)$ perturbation in terms of $\Delta(H)$. Let us express this more conveniently with the $\infty$ norm. Let the smallest value of $H$ be denoted $h_{\min}$. Then,
%  \[ \|\Delta(SE)\|_{\infty} \leq\frac{1}{2}\cosh( \| \Delta(H)_\infty\| h_{\min}^{-1} ). \]
% %Note due to scaling the unit length, we can make this arbitrarirly small.

{\bf Scaled Euclidean to Euclidean inner product}.
Recall that if $X$ is the embedding in the hyperboloid model, then $Y = uu^T - XX^T$ from equation~\eqref{eq:hmds-Y2}, and furthermore $X^T u = 0$ so that $X$ can be recovered through PCA.
Now we are in the Euclidean setting, and can thus measure the result of the perturbation on the recovered $X$.
The proof of Theorem 4.1 in \citet{Sibson2} transfers to this setting.
This result states that if $\hat X$ is the configuration recovered from the perturbed inner products, then, the lowest-order term of the expansion of the error $D_E(X,\hat X)$ in the perturbation $\Delta(Y)$ is
\[
  D_E(X,\hat X) = \frac{1}{2} \sum_{j,k} \frac{(v_j^T \Delta(Y) v_k)^2}{\lambda_j + \lambda_k}.
\]
Here, the $\lambda_i$ and $v_i$ are the eigenvalues and corresponding orthonormal eigenvectors of $XX^T$ and the sum is taken over pairs of $\lambda_{j}, \lambda_k$ that are not both 0.
Let $\lambda_{\min}$ be the smallest nonzero eigenvalue of $XX^T$. Then,
\begin{align*}
  D_E(X,\hat X) 
  &\le 
  \frac{1}{2 \lambda_{\min}} \sum_{j,k} (v_j^T \Delta(Y) v_k)^2
  \le
  \frac{1}{2 \lambda_{\min}} \| \Delta(Y) \|_F^2 \\
  &\le
  \frac{n^2}{2 \lambda_{\min}} \| \Delta(Y) \|_{\infty}^2.
\end{align*}
Combining this with the previous bounds, and restricting to second-order terms in $\| \Delta(H) \|_{\infty}^2$ proves Lemma~\ref{lemma:hmds-perturb} for the embedding $X$ in the hyperboloid model.
\end{proof}

\paragraph{Projecting to the Poincar{\'e} disk}

Algorithm~\ref{alg:new_hmds} initially finds an embedding in $\mathbb{M}_r$, but optionally converts it to the Poincar{\'e} disk.
To convert a point $x$ in the hyperboloid model to $z$ in the Poincar{\'e} disk, take $z = \frac{x}{1 + \sqrt{1 + \|x\|_2^2}}$.
Let $Z \in \R^{n \times r}$ be the projected embedding.
Now we show that the same perturbation bound holds after projection.

\begin{lemma}
  \label{lmm:project-perturb}
  For any $x$ and $y$,
  $ \left\| \frac{x}{1 + \sqrt{1 + \|x\|_2^2}} - \frac{x}{1 + \sqrt{1 + \|x\|_2^2}} \right\| \le \| x-y \| $
\end{lemma}
\begin{proof}
  Let $u_x = \sqrt{1 + \|x\|^2}$ and define $u_y$ analogously.
  Note that $u_x \ge 2$, $u_x \ge \|x\|$, and
  \[
    u_y - u_x = \frac{u_y^2 - u_x^2}{u_y + u_x} = (\|y\|-\|x\|)\frac{\|y\|+\|x\|}{u_y + u_x} \le \|y\|-\|x\|.
  \]
  Combining these facts leads to the bound
  \begin{align*}
    \left\| \frac{x}{1 + \sqrt{1 + \|x\|_2^2}} - \frac{y}{1 + \sqrt{1 + \|y\|_2^2}} \right\| 
    &= \left\| \frac{x-y + x u_y - y u_y + y u_y - y u_x}{(1+u_x)(1+u_y)} \right\|
    \\&= \left\| \frac{(x-y)(1+u_y) + y(u_y - u_x)}{(1+u_x)(1+u_y)} \right\|
    \\&= \left\| \frac{x-y}{1+u_x} + \frac{y}{1+u_y}\frac{u_y-u_x}{1+u_x} \right\|
    \\&\le \frac{\left\| x-y \right\|}{1+u_x} + \frac{\left\| u_y-u_x \right\|}{1+u_x}
    \\&\le \left\| x-y \right\|.
  \end{align*}
\end{proof}

Lemma~\ref{lmm:project-perturb} is equivalent to the statement that $D(z, \hat z) \le D(x, \hat x)$ where $z, \hat z$ are the projections of $x, \hat x$.
Since orthogonal matrices $P$ preserve $\ell_2$ norm, $P\hat z$ is the projection of $P \hat x$ so $D(z, P \hat z) \le D(x, P \hat x)$ for any $P$.
Finally, $D(Z, P\hat Z)$ is just a sum over all columns and therefore $D(Z, P\hat Z) \le D(X, P\hat X)$.
This implies that $D_E(Z, \hat Z) \le D_E(X, \hat X)$ as desired.

\paragraph{The hyperbolic gap}
The gap $D(X,\hat X)$ can be written as a sum $\sum d_E(x_i, \hat{x}_i)^2$ over the vectors (columns) of $X,\hat X$.
We can instead ask about the hyperbolic gap
\[
  D_H(X, \hat X) = \inf \left\{ \sum d_H(x_i, P\hat{x}_i)^2 : P^T P = I \right\},
\]
which is a better interpretation of the perturbation error when recovering hyperbolic distances.

Note that for any points $x,y$ in the Gans model, we have
\[
  d_H(x,y) = \acosh\left( \sqrt{1 + \|x\|^2}\sqrt{1 + \|y\|^2} - \langle x,y \rangle \right) \le \acosh\left( \frac{2 + \|x\|^2 + \|y\|^2}{2} - \langle x,y \rangle \right) = \acosh\left( 1 + \frac{1}{2}\|x-y\|^2 \right).
\]
Furthermore, the function $\acosh(1 + t^2/2) - t$ is always negative except in a tiny region around $t=0$ (and attains a maximum here on the order of $10^{-10}$),
so effectively $\acosh\left( 1 + \frac{1}{2}\|x-y\|^2 \right) \le \|x-y\| = d_E(x,y)$,
and the same bound in Lemma~\ref{lemma:hmds-perturb} carries over to the hyperbolic gap.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hyperbolic_arxiv"
%%% End: