In this section, we explore a fundamental and more general question than we did in the previous section: if we are given the pairwise distances arising from a set of points in hyperbolic space, can we recover the points? The equivalent problem for Euclidean distances is solved with multidimensional scaling (MDS). The goal of this section is to analyze the \emph{hyperbolic MDS} (h-MDS) problem. We describe and overcome the additional technical challenges imposed by hyperbolic distances, and show that exact recovery is possible and interpretable.
Afterwards we propose a technique for dimensionality reduction using principal geodesics analysis (PGA) that provides optimization guarantees.
In particular, this addresses the shortcomings of h-MDS when recovering points that do not exactly lie on a hyperbolic manifold.
%proceed to analyze perturbations for h-MDS (i.e., recovery from noisy distances), mirroring the analysis of MDS robustness.

\subsection{Exact Hyperbolic MDS}
\label{sec:exactmds}


Suppose that there is a set of
hyperbolic points $x_1,\dots, x_n \in \mathbb{H}_r$, embedded in the Poincar{\'e} ball and written $X \in
\mathbb{R}^{n \times r}$ in matrix form.
We observe all the pairwise distances $d_{i,j} = d_H(x_i, x_j)$, but do not observe $X$:
our goal is use the observed $d_{i,j}$'s to recover $X$ (or some other set of points with the same pairwise distances $d_{i,j}$).

The MDS algorithm in the Euclidean setting makes an important
\emph{centering}%
\footnote{We say that points are centered at a particular mean
  if this mean is at $0$. The act of centering refers to applying an isometry
  that makes the mean of the points $0$.}
assumption.
That is it assumes the points have mean $0$, and it turns out that if an exact
embedding for the distances exists, it can be recovered from a matrix factorization.
In other words, Euclidean MDS always recovers a centered embedding.

In hyperbolic space, the same algorithm does not work, but we show that it is possible to find an embedding centered at a different mean. 
More precisely, we introduce a new mean which we call the \emph{pseudo-Euclidean mean}, that behaves like the Euclidean mean in that it enables recovery through matrix factorization.
Once the points are recovered in hyperbolic space, they can be recentered around a more canonical mean by translating it to the origin.

Algorithm~\ref{alg:new_hmds} is our complete algorithm, and for the remainder of
this section we will describe how and why it works.
We first describe the \emph{hyperboloid model}, an alternate but equivalent model of hyperbolic geometry in which h-MDS is simpler. Of course, we can easily convert between the hyperboloid model and the Poincar\'{e} ball model we have used thus far.
Next, we show how to reduce the problem to a standard PCA problem, which recovers an embedding centered at the points' pseudo-Euclidean mean.
Finally, we discuss the meaning and implications of centering and prove that the algorithm preserves submanifolds as well---that is, if there is an exact embedding in $k < r$ dimensions centered at their canonical mean,
then our algorithm will recover them.

\paragraph*{The hyperboloid model}
Define $Q$ to be the diagonal matrix in $\R^{r+1}$ where $Q_{00} = 1$ and $Q_{ii} = -1$ for $i > 0$.
For a vector $x \in \R^{r+1}$, $x^TQx$ is called the \emph{Minkowski quadratic form}.
The hyperboloid model is defined as
\[
  \mathbb{M}_r = \left\{ x \in \R^{r+1} \middle| x^T Q x = 1 \land x_0 > 0 \right\}.
\]
This manifold is endowed with a distance measure
\[
  d_H(x, y) = \acosh(x^T Q y).
\]
As a notational convenience, for a point $x \in \mathbb{M}_r$ we will let $x_0$ denote $0$th coordinate $e_0^T x$, and let $\vec x \in \R^r$ denote the rest of the coordinates.
Notice that $x_0$ is just a function of $\vec x$ (in fact, $x_0 = \sqrt{1 + \| \vec{x} \|^2}$), and so we can equivalently consider just $\vec x$ as being a member of a model of hyperbolic space: this model is sometimes known as the Gans model.
With this notation, the Minkowski quadratic form can be simplified to $x^T Q y = x_0 y_0 - \vec{x}^T \vec{y}$.

\paragraph*{A new mean}
We introduce the new mean that we will use.
Given points $x_1, x_2, \ldots, x_n \in \mathbb{M}_r$ in hyperbolic space,
define a variance term
\[
  \Psi(z; x_1, x_2, \ldots, x_n)
  =
  \sum_{i=1}^n \sinh^2(d_H(x_i, z)).
\]
Using this, we define a \emph{pseudo-Euclidean mean} to be any local minimum of this expression.
% This is a type of \emph{Karcher mean} in hyperbolic space.%
% \footnote{A Karcher mean is a local minimum of...}
% \[
%   A(x_1, \ldots, x_n)
%   =
%   \arg \min_{z \in \mathbb{M}_r} \Psi(z; x_1, \ldots, x_n).
% \]
Notice that this average is independent of the model of hyperbolic space that we are using, since it only is defined in terms of the hyperbolic distance function $d_H$.

\begin{lemma}
  \label{lmm:pe-centered}
  Define the matrix $X \in \R^{n \times r}$ such that $X^T e_i = \vec{x}_i$ and the vector $u \in \R^n$ such that $u_i = x_{0,i}$.
  Then
  \begin{align*}
    \left. \nabla_{\vec{z}} \Psi(z; x_1, x_2, \ldots, x_n) \right|_{\vec{z} = 0}
    =
    -2 \sum_{i=1}^n x_{0,i} \vec{x}_i
    =
    -2 X^T u.
  \end{align*}
\end{lemma}
This means that $0$ is a pseudo-Euclidean mean if and only if $0 = X^T u$.
Call some hyperbolic points $x_1, \ldots, x_n$ \emph{pseudo-Euclidean centered} if their average is $0$ in this sense: i.e. if $X^T u = 0$.
We can always center a set of points without affecting their pairwise distances by simply finding their average, and then sending it to $0$ through an isometry.

\paragraph*{Recovery via matrix factorization}
Suppose that there exist points $x_1, x_2, \ldots, x_n \in \mathbb{M}_r$
for which we observe their pairwise distances $d_H(x_i, x_j)$.
From these, we can compute the matrix $Y$ such that
\begin{equation}
  \label{eq:hmds-Y}
  Y_{i,j} = \cosh\left( d_H(x_i, x_j) \right) = x_i^T Q x_j = x_{0,i} x_{0,j} - \vec{x_i}^T \vec{x_j}.
\end{equation}
Furthermore, defining $X$ and $u$ as in Lemma~\ref{lmm:pe-centered},
then we can write $Y$ in matrix form as
\begin{equation}
  \label{eq:hmds-Y2}
  Y = u u^T - X X^T.
\end{equation}
Without loss of generality, we can suppose that the points we are trying to recover, $x_1, \ldots, x_n$, are centered at their pseudo-Euclidean mean, so that $X^T u = 0$ by Lemma~\ref{lmm:pe-centered}.

This implies that $u$ is an eigenvector of $Y$ with positive eigenvalue, and the rest of $Y$'s eigenvalues are negative.
Therefore an eigendecomposition of $Y$ will find $u,\hat{X}$ such that $Y = u u^T - \hat{X} \hat{X}^T$,
i.e. it will directly recover $X$ up to rotation.

In fact, running PCA on $-Y = X^T X - u u^T$ to find the $n$ most significant non-negative eigenvectors will recover $X$ up to rotation,
and then $u$ can be found by leveraging the fact that $x_0 = \sqrt{1 + \| \vec{x} \|^2}$.

This leads to Algorithm~\ref{alg:new_hmds}, with optional post-processing steps for converting the embedding to the Poincar{\'e} ball model and for re-centering the points.
% First, this algorithm returns an embedding in the Gans model; they can be converted to the Poincar{\'e} disk model with a simple projection.
% Second, once we've recovered the points centered at their pseudo-Euclidean mean, we can recover the points centered at any other mean by reflecting it onto the origin.


\paragraph*{A word on centering}
The MDS algorithm in Euclidean geometry returns points centered at their \emph{Karcher mean} $z$, which is a point minimizing $\sum d^2(z, x_i)$ (where $d$ is the distance metric).
The Karcher center is particularly useful for interpreting dimensionality reduction; for example, we use the analogous hyperbolic Karcher mean to perform PGA in Section~\ref{sec:PGA}.

Although Algorithm~\ref{alg:new_hmds} returns points centered at their pseudo-Euclidean mean instead of their Karcher mean, they can be easily recentered
by finding their Karcher mean and reflecting it onto the origin. 
Furthermore, we show that Algorithm~\ref{alg:new_hmds} \emph{preserves the dimension of the embedding}.
More precisely, we prove Lemma~\ref{lmm:hmds-centering} in Appendix~\ref{sec:mds-proof}.
\begin{lemma}
  \label{lmm:hmds-centering}
  If a set of points lie in a dimension-$k$ geodesic submanifold, then both their Karcher mean and their pseudo-Euclidean mean lie in the same submanifold.
\end{lemma}
This implies that centering with the pseudo-Euclidean mean preserves geodesic submanifolds:
If it is possible to embed distances in a dimension-$k$ geodesic submanifold centered and rooted at a Karcher mean, then it is also possible to embed the distances in a dimension-$k$ submanifold centered and rooted at a pseudo-Euclidean mean, and vice versa.


\begin{algorithm}[t]
% \caption{h-MDS}
\caption{ }
\begin{algorithmic}[1]
\STATE {\bfseries Input: Distance matrix $d_{i,j}$ and rank $r$}
\STATE Compute scaled distance matrix $Y_{i,j} = \cosh(d_{i,j})$
\STATE $X \rightarrow \text{PCA}(-Y,r)$
\STATE Project $X$ from hyperboloid model to Poincar\'{e} model: $x \to \frac{x}{1 + \sqrt{1 + \|x\|^2}}$
\STATE If desired, center $X$ at a different mean (e.g. the Karcher mean)
\STATE \textbf{return} $X$
\end{algorithmic}
\label{alg:new_hmds}
\end{algorithm}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hyperbolic_arxiv"
%%% End: