
\section{Glossary of Symbols}

\begin{table*}[h]
\centering
\begin{tabular}{l l}
\toprule
Symbol & Used for \\
\midrule
$x$, $y$, $z$ & vectors in the Poincar{\'e} ball model of hyperbolic space \\
$d_H$ & metric distance between two points in hyperbolic space \\
$d_E$ & metric distance between two points in Euclidean space \\
$d_U$ & metric distance between two points in metric space $U$ \\
$d$ & a particular distance value \\
$d_{i,j}$ & the distance between the $i$th and $j$th points in an embedding \\
$\mathbb{H}_r$ & the Poincar{\'e} ball model of $r$-dimensional Hyperbolic space \\
$r$ & the dimension of a Hyperbolic space \\
$\mathbb{H}$ & Hyperbolic space of an unspecified or arbitrary dimension \\
$\mathbb{M}_r$ & the Minkowski (hyperboloid) model of $r$-dimensional Hyperbolic space \\
$f$ & an embedding \\
$\mathcal{N}_a$ & neighborhood around node $a$ in a graph \\
$R_{a,b}$ & the smallest set of closest points to node $a$ in an embedding $f$ that contains node $b$ \\
$\text{MAP}(f)$ & the mean average precision fidelity measure of the embedding $f$ \\
$D(f)$ & the distortion fidelity measure of the embedding $f$ \\
$D_{\mathrm{wc}}(f)$ & the worst-case distortion fidelity measure of the embedding $f$ \\
$G$ & a graph, typically with node set $V$ and edge set $E$ \\
$T$ & a tree \\
$a, b, c$ & nodes in a graph or tree \\
$\operatorname{deg}(a)$ & the degree of node $a$ \\
$\operatorname{deg}_{\max}$ & maximum degree of a node in a graph \\
$\ell$ & the longest path length in a graph \\
$\tau$ & the scaling factor of an embedding \\
$\operatorname{reflect}_{x \rightarrow y}$ & a reflection of $x$ onto $y$ in hyperbolic space \\
$\operatorname{arg}(z)$ & the angle that the point $z$ in the plane makes with the $x$-axis \\
$X$ & matrix of points in hyperbolic space \\
$Y$ & matrix of transformed distances \\
% $S$ & diagonal scaling matrix used in h-MDS \\
% $v$ & vector of squared norm values used in h-MDS \\
% $u, \hat u$ & eigenvectors used in h-MDS \\
% $Z$ & reduced matrix used in h-MDS \\
% $\alpha, \beta$ & intermediate scalars used in h-MDS \\
$\gamma$ & geodesic used in PGA \\
$w_i$ & transformed points used in PGA \\
\bottomrule
\end{tabular}
\caption{Glossary of variables and symbols used in this paper.}
\label{table:glossary}
\end{table*}

\section{Related Work}
\label{sec:related}
Our study of representation tradeoffs for hyperbolic embeddings was motivated by
exciting recent approaches towards such embeddings in \citet{fb} and
\citet{ucl}.
Earlier efforts proposed using hyperbolic spaces for routing, starting with Kleinberg's work on geographic routing \cite{Kleinberg}.
\citet{Crovella} performed hyperbolic embeddings and routing for dynamic networks.
Recognizing that the use of hyperbolic space for routing required a large number of bits to store the vertex coordinates, \citet{Eppstein} introduced a scheme for succinct embedding and routing in the hyperbolic plane.
Another very recent effort also proposes using hyperbolic cones (similar to the cones that are the fundamental building block used in \citet{sarkar} and our work) as a heuristic for embedding entailment relations, i.e. directed acyclic graphs~\cite{ganea}.
The authors also propose to optimize on the hyperbolic manifold using its exponential map, as opposed to our approach of finding a closed form for the embedding should it exist (Section~\ref{sec:MDS}). An interesting avenue for future work is to compare both optimization methods empirically and theoretically, i.e., to understand the types of recovery guarantees under noise that such methods have.

There have been previous efforts to perform multidimensional scaling in hyperbolic space (the h-MDS problem), often in the context of visualization~\cite{lamping1994laying}. Most propose descent methods in hyperbolic space (e.g.~\cite{cvetkovski2016multidimensional}, \cite{walter2004}) and fundamentally differ from ours.
Arguably the most relevant is~\citet{wilson2014spherical}, which mentions exact recovery as an intermediate result, but ultimately suggests a heuristic optimization.
Our h-MDS analysis characterizes the recovered embedding and manifold and obtains the correctly centered one---a key issue in MDS.
For example, this allows us to properly find the components of maximal variation.
Furthermore, we discuss robustness to noise and produce optimization guarantees when a perfect embedding doesn't exist.

Several papers have studied the notion of hyperbolicity of networks, starting with the seminal work on hyperbolic graphs \citet{Gromov}. More recently, \citet{Mahoney} considered the hyperbolicity of small world graphs and tree-like random graphs. \citet{Dragan} performed a survey that examines how well real-world networks can be approximated by trees using a variety of tree measures and tree embedding algorithms. To motivate their study of tree metrics, \citet{Abraham} computed a measure of tree likeness on a Internet infrastructure network. 

We use matrix completion (closure) to perform embeddings with incomplete data. Matrix completion is a celebrated problem. \citet{TaoMatrix} derive bounds on the minimum number of entries needed for completion for a 
fixed rank matrix; they also introduce a convex program for matrix completion operating at near the optimal rate.

Principal geodesic analysis (PGA) generalizes principal components analysis (PCA) for the manifold setting. It was introduced and applied to shape analysis in \cite{PGA} and extended to a probabilistic setting in \cite{ProbPGA}.
There are other variants; the geodesic principal components analysis (GPCA) of~\citet{GPCA} uses our loss function.



\section{Low-Level Formulation Details}
We plan to release our PyTorch code, high precision solver, and other
routines on Github. A few comments are helpful to understand the
reformulation. In particular, we simply minimize the squared
hyperbolic distance with a learned scale parameter, $\tau$, e.g., :
\[ \min_{x_1,\dots,x_n,\tau}\sum_{1 \leq i < j \leq n} \left(\tau d_{H}(x_i,x_j) - d_{i,j}\right)^2 \]
We typically require that $\tau \geq 0.1$.

\begin{itemize}
\item On continuity of the derivative of the loss: Note that
  \[ \partial_{x} \mathsf{acosh}(1+x) = \frac{1}{\sqrt{(1+x)^2 -1 }}
= \frac{1}{\sqrt{x(x+2)}} \text{ hence } \lim_{x \to 0} \partial_{x}
\mathsf{acosh}(1+x) = \infty. \] Thus, $\lim_{y \to x} \partial_{x}
d_{H}(x,y) = \infty$. In particular, if two points happen to get near
to one another during execution, gradient-based optimization becomes
unstable. Note that $\exp\{\mathrm{acosh(1+x)}\}$ suffers from a
similar issue, and is used in both~\cite{fb, ucl}. This change may
increase numerical instability, and the public code for these
approaches does indeed take steps like masking out updates to mitigate
\textsc{NaN}s. In contrast, the following may be more stable:
\[ \partial_{x} \mathsf{acosh}(1+x)^2 = 2 \frac{\mathsf{acosh}(1+x)}{\sqrt{x(x+2)}} \text{ and in particular } \lim_{x \to 0} \partial_{x} \mathsf{acosh}(1+x)^2 = 2\]
The limits follows by simply applying L'Hopital's rule. In turn, this
implies the square formulation is continuously differentiable. Note
that it is not convex.
  \item One challenge is to make sure the gradient computed by PyTorch
    has the appropriate curvature correction (the Riemannian metric),
    as is well explained by \citet{fb}. The modification is
    straightforward: we create a subclass of \textsc{nn.Parameter}
    called \textsc{Hyperbolic\_Parameter}. This wrapper class allows
    us to walk the tree to apply the appropriate correction to the
    metric (which amounts to multiplying $\nabla_{w} f(w)$ by
    $\frac{1}{4}(1-\|w\|^2)^2$. After calling the \textsc{backward}
    function, we call a routine to walk the autodiff tree to find such
    parameters and correct them. This allows
    \textsc{Hyperbolic\_Parameter} and traditional parameters to be
    freely mixed.

    \item We project back on the hypercube following \citet{fb} and
      use gradient clipping with bounds of $[-10^{5},10^5]$. This
      allows larger batch sizes to more fully utilize the GPU.
\end{itemize}

\section{Combinatorial Construction Proofs}
\label{app:CombinatorialProofs}

\input{comb-proofs}

\section{Proof of h-MDS Results}
\label{sec:mds-proof}

% \input{app-hmds-old}
\input{app-hmds}


\section{Perturbation Analysis}

\input{hmds-perturb}

\section{Proof of Lemma~\ref{lemma:pga}}

In this section, we prove Lemma~\ref{lemma:pga}, which gives a setting under which we can guarantee that the hyperbolic PGA objective is locally convex.

\begin{proof}[Proof of Lemma~\ref{lemma:pga}]
We begin by considering the component function
\[
  f_i(\gamma) = \acosh^2(1 + d_E^2(\gamma, v_i)).
\]
Here, the $\gamma$ is a geodesic through the origin.
We can identify this geodesic on the Poincar{\'e} disk with a unit vector $u$ such that $\gamma(t) = (2t-1) u$.
In this case, simple Euclidean projection gives us
\[
  d_E^2(\gamma, v_i) = \| (I - u u^T) v_i \|^2.
\]
Optimizing over $\gamma$ is equivalent to optimizing over $u$, and so
\[
  f_i(u) = \acosh^2\left(1 + \| (I - u u^T) v_i \|^2 \right).
\]
If we define the functions
\[
  h(\gamma) = \acosh^2(1 + \gamma)
\]
and
\[
  R(u) = \| (I - u u^T) v_i \|^2 =  \| v_i \|^2 - (u^T v_i)^2
\]
then we can rewrite $f_i$ as
\[
  f_i(u) = h(R(u)).
\]
Now, optimizing over $u$ is an geodesic optimization problem on the hypersphere.
Every goedesic on the hypersphere can be isometrically parameterized in terms of an angle $\theta$ as
\[
  u(\theta) = x \cos(\theta) + y \sin(\theta)
\]
for orthogonal unit vectors $x$ and $y$.
Without loss of generality, suppose that $y^T v_i = 0$ (we can always choose such a $y$ because there will always be some point on the geodesic that is orthogonal to $v_i$).
Then, we can write
\[
  R(\theta)
  =
  \| v_i \|^2 - (x^T v_i)^2 \cos^2(\theta)
  =
  \| v_i \|^2 - (x^T v_i)^2 + (x^T v_i)^2 \sin^2(\theta).
\]
Differentiating the objective with respect to $\theta$,
\begin{align*}
  \frac{d}{d \theta} h(R(\theta))
  &=
  h'(R(\theta)) R'(\theta) \\
  &=
  2 h'(R(\theta)) \cdot (v_i^T x)^2 \cdot \sin(\theta) \cos(\theta).
\end{align*}
Differentiating again,
\begin{align*}
  \frac{d^2}{d \theta^2} h(R(\theta))
  &=
  4 h''(R(\theta)) \cdot (v_i^T x)^4 \cdot \sin^2(\theta) \cos^2(\theta)
  +
  2 h'(R(\theta)) \cdot (v_i^T x)^2 \cdot \left( \cos^2(\theta) - \sin^2(\theta) \right).
\end{align*}
Now, suppose that we are interested in the Hessian at a point $z = x \cos(\theta) + y \sin(\theta)$ for some fixed angle $\theta$.
Here, $R(\theta) = R(z)$, and as always $v_i^T z = v_i^T x \cos(\theta)$, so
\begin{align*}
  \frac{d^2}{d \theta^2} h(R(\theta)) \big|_{u(\theta) = z}
  &=
  4 h''(R(\theta)) \cdot (v_i^T x)^4 \cdot \sin^2(\theta) \cos^2(\theta)
  +
  2 h'(R(\theta)) \cdot (v_i^T x)^2 \cdot \left( \cos^2(\theta) - \sin^2(\theta) \right) \\
  &=
  4 h''(R(z)) \cdot \frac{ (v_i^T z)^4 }{\cos^4(\theta)} \cdot \sin^2(\theta) \cos^2(\theta)
  +
  2 h'(R(z)) \cdot \frac{ (v_i^T x)^2 }{\cos^2(\theta)} \cdot \left( \cos^2(\theta) - \sin^2(\theta) \right) \\
  &=
  4 h''(R(z)) \cdot (v_i^T z)^4 \cdot \tan^2(\theta)
  +
  2 h'(R(z)) \cdot (v_i^T z)^2 \cdot \left( 1 - \tan^2(\theta) \right) \\
  &=
  2 h'(R(z)) \cdot (v_i^T z)^2 
  +
  \left(
    4 h''(R(z)) \cdot (v_i^T z)^4
    -
    2 h'(R(z)) \cdot (v_i^T z)^2
  \right)
  \tan^2(\theta).
\end{align*}
But we know that since $h$ is concave and increasing, this last expression in parenthesis must be negative.
It follows that a lower bound on this expression for fixed $z$ will be attained when $\tan^2(\theta)$ is maximized.
For any geodesic through $z$, the angle $\theta$ is the distance along the geodesic to the point that is (angularly) closest to $v_i$.
By the Triangle inequality, this will be no greater than the distance $\theta$ along the Geodesic that connects $z$ with the normalization of $v_i$.
On this worst-case geodesic,
\[
  v_i^T z = \|v_i\| \cos(\theta),
\]
and so
\[
  \cos^2(\theta) = \frac{(v_i^T z)^2}{\|v_i\|^2}
\]
and
\[
  \tan^2(\theta) = \sec^2(\theta) - 1 = \frac{\|v_i\|^2}{(v_i^T z)^2} - 1 = \frac{R(z)}{(v_i^T z)^2}.
\]
Thus, for any geodesic, for the worst-case angle $\theta$,
\begin{align*}
  \frac{d^2}{d \theta^2} h(R(\theta)) \big|_{u(\theta) = z}
  &\ge
  2 h'(R(z)) \cdot (v_i^T z)^2 
  +
  \left(
    4 h''(R(z)) \cdot (v_i^T z)^4
    -
    2 h'(R(z)) \cdot (v_i^T z)^2
  \right)
  \tan^2(\theta) \\
  &=
  2 h'(R(z)) \cdot (v_i^T z)^2 
  +
  \left(
    4 h''(R(z)) \cdot (v_i^T z)^2
    -
    2 h'(R(z))
  \right)
  R(z).
\end{align*}
From here, it is clear that this lower bound on the second derivative (and as a consequence local convexity) is a function solely of the norm of $v_i$ and the residual to $z$.
From simple evaluation, we can compute that
\[
  h'(\gamma) = 2 \frac{\acosh(1+\gamma)}{\sqrt{\gamma^2 + 2\gamma}}
\]
and
\[
  h''(x)
  =
  2 \frac{
    \sqrt{\gamma^2 + 2\gamma}
    -
    (1 + \gamma) \acosh(1 + \gamma)
  }{
    (\gamma^2 + 2\gamma)^{3/2}
  }.
\]
As a result
\begin{align*}
  4 \gamma h''(\gamma) + h'(\gamma)
  &=
  8 \frac{
    \gamma \sqrt{\gamma^2 + 2\gamma}
    -
    (\gamma^2 + \gamma) \acosh(1 + \gamma)
  }{
    (\gamma^2 + 2\gamma)^{3/2}
  }
  +
  2 \frac{
    (\gamma^2 + 2\gamma) \acosh(1+\gamma)
  }{
    (\gamma^2 + 2\gamma)^{3/2}
  } \\
  &=
  2 \frac{
    4 \gamma \sqrt{\gamma^2 + 2\gamma}
    -
    4 (\gamma^2 + \gamma) \acosh(1 + \gamma)
    +
    (\gamma^2 + 2\gamma) \acosh(1+\gamma)
  }{
    (\gamma^2 + 2\gamma)^{3/2}
  } \\
  &=
  2 \frac{
    4 \gamma \sqrt{\gamma^2 + 2\gamma}
    -
    (3 \gamma^2 + 2 \gamma) \acosh(1 + \gamma)
  }{
    (\gamma^2 + 2\gamma)^{3/2}
  }.
\end{align*}
For any $\gamma$ that satisfies $0 \le \gamma \le 1$,
\[
  4 \gamma \sqrt{\gamma^2 + 2\gamma}
  \ge
  (3 \gamma^2 + 2 \gamma) \acosh(1 + \gamma)
\]
and so
\[
  4 \gamma h''(\gamma) + h'(\gamma) \ge 0.
\]
Thus, if $0 \le R(z) \le 1$,
\begin{align*}
  \frac{d^2}{d \theta^2} h(R(\theta)) \big|_{u(\theta) = z}
  &\ge
  2 h'(R(z)) \cdot (v_i^T z)^2 
  +
  \left(
    4 h''(R(z)) \cdot (v_i^T z)^2
    -
    2 h'(R(z))
  \right)
  R(z) \\
  &=
  h'(R(z)) \cdot (v_i^T z)^2 
  +
  \left(
    4 h''(R(z)) \cdot R(z)
    +
    h'(R(z))
  \right) \cdot (v_i^T z)^2 
  -
  2 h'(R(z)) \cdot R(z) \\
  &\ge
  h'(R(z)) \cdot (v_i^T z)^2 
  -
  2 h'(R(z)) \cdot R(z) \\
  &=
  h'(R(z)) \cdot \left( \| v_i \|^2 - R(z) \right)
  -
  2 h'(R(z)) \cdot R(z) \\
  &=
  h'(R(z)) \cdot \left( \| v_i \|^2 - 3 R(z) \right).
\end{align*}
Thus, a sufficient condition for convexity is for (as we assumed above) $R(z) \le 1$ and
\[
  \| v_i \|^2 \ge 3 R(z).
\]
Combining these together shows that if
\[
  \acosh^2\left(1 + d_E(\gamma, v_i)^2 \right)
  =
  R(z)
  \le
  \min\left(1, \frac{1}{3} \| v_i \|^2 \right)
\]
then $f_i$ is locally convex at $z$.
The result of the lemma now follows from the fact that $f$ is the sum of many $f_i$ and the sum of convex functions is also convex.
\end{proof}

\section{Experimental Results}
In this section, we provide some additional experimental results. We also present results on an additional less tree-like graph (a search engine query response graph for the search term `California' \cite{ca-data}.)
 
\paragraph*{Combinatorial Construction: Parameters}
To improve the intuition behind the combinatorial construction, we report some additional parameters used by the construction. For each of the graphs, we report the maximum degree, the scaling factor $\nu$ that the construction used (note how these vary with the size of the graph and the maximal degree), the time it took to perform the embedding, in seconds, and the number of bits needed to store a component for $\varepsilon=0.1$ and $\varepsilon=1.0$.

\begin{table*}[h]
\centering
\begin{tabular}{|l|c|c||c|c|c|c|c|c|} \hline
                              &               &                  &                            &   &              \multicolumn{2}{c|}{$\varepsilon=0.1$} &   \multicolumn{2}{c|}{$\varepsilon=1.0$} \\ \hline
Dataset     	          &  Nodes & Edges   &   $d_{\max}$ & Time    & Scaling Factor &  Precision      & Scaling Factor &  Precision \\ \hline\hline
Bal. Tree 1          & 40  	      &  39          & 4             &  3.78      & 23.76                & 102   & 4.32 & 18 \\ \hline
Phylo. Tree          & 344      & 343        & 16           & 3.13       & 55.02               & 2361  &  10.00   &  412   \\ \hline \hline
WordNet              & 74374 &  75834  &  404       & 1346.62 & 126.11           & 2877  & 22.92 & 495\\ \hline
CS PhDs              & 1025     &  1043    &  46          &  4.99       & 78.30               & 2358  & 14.2 & 342 \\ \hline \hline
Diseases              & 516      & 1188     & 24          &    3.92       & 63.97             & 919 & 13.67 & 247  \\ \hline
Protein - Yeast   & 1458   & 1948     & 54           &  6.23       &  81.83             & 1413  & 15.02 & 273 \\ \hline \hline
Gr-QC                   & 4158    &  13428  & 68          & 75.41      & 86.90             &  1249 &  16.14 & 269 \\ \hline
California           & 5925     &   15770 &  105      & 114.41   & 96.46               & 1386  & 19.22 & 245\\ \hline
\end{tabular}
\caption{Combinatorial construction parameters and results.}
\label{table:comb_setup}
\end{table*}

%We also comment on the possibility of reducing the precision by increasing the dimension (using the extended $\mathbb{H}_r$ construction from Section~\ref{sec:combinatorial}). We noted the largest dimension that reduces the precision (beyond this additional dimensions do not help). We see that the $\varepsilon=0.1$ remains challenging to store, while with $\varepsilon=1$, 64 bits suffices.

%\begin{table*}[h]
%\centering
%\begin{tabular}{|l|c|c||c|c|c|c|} \hline
%                              &               &        &          &                             & $\varepsilon=0.1$ & $\varepsilon=1.0$ \\ \hline
%Dataset     	          &  Nodes & Edges   &   $d_{\max}$ & Dimension    &  Precision      &  Precision \\ \hline\hline
%Bal. Tree 1          & 40  	      &  39          & 4   &  3  & 26     & 5         \\ \hline
%Phylo. Tree          & 344      & 343        & 16       & 5  &  251   & 46             \\ \hline \hline
%WordNet              & 74374 &  75834  &  404      & 10 & 133 & 25        \\ \hline
%CS PhDs              & 1025     &  1043    &  46      & 7   & 176    & 32        \\ \hline \hline
%Diseases              & 516      & 1188     & 24       &  6  &85     & 16           \\ \hline
%Protein - Yeast   & 1458   & 1948     & 54           &  7  & 101   &  19         \\ \hline \hline
%Gr-QC                   & 4158    &  13428  & 68      & 8   &86  & 16            \\ \hline
%California           & 5925     &   15770 &  105      & 8  &84  & 16             \\ \hline
%\end{tabular}
%\caption{Reducing the precision with larger dimensions.}
%\label{table:comb_reduction}
%\end{table*}


%\begin{table*}
%\centering
%\begin{tabular}{|l|c||c|c||c|c|c|c|} \hline
%Dataset     	          &  Nodes  & Combinatorial $\mathbb{H}_2$ &  FB $\mathbb{H}_2$ & h-MDS  & PCA & FB                  \\ \hline\hline
%Bal. Tree 1          & 40  	&   {\bf 1.0}              &    0.846    &    {\bf 1.0}           &  {\bf 1.0}           & 0.859     \\ \hline
%Phylo. Tree          & 344      &     {\bf 1.0}                &    0.718  &   0.675              &    {\bf 1.0}          &       0.811     \\ \hline \hline
%WordNet              & 74374 &    {\bf 0.989}            &  0.821                          &     &            &   0.870\\ \hline
%Gr-QC                   & 4158       &    {\bf 0.759}      &  0.635               &    0.710   &  0.738  &  {\bf 0.999}  \\ \hline 
%
%CS PhDs              & 1025     &    {\bf 0.991}             &  0.567          &  0.463    &                   0.541 & {\bf 0.786}   \\ \hline \hline
%Diseases              & 516      &     {\bf 0.888}             & 0.788          &     0.949           &         {\bf 0.999}      &    0.934                \\ \hline 
%Protein - Yeast   & 1458   &      {\bf 0.872 }       &   0.4718          &     0.525       &            0.606      &  {\bf 0.867}          \\ \hline \hline
%California           & 5925        &        {\bf 0.582}           &  0.5249       &   0.316              &     0.337  &                         \\ \hline 
%\end{tabular}
%\caption{MAP measures using combinatorial and h-DMS techniques, compared against PCA and results from \citet{fb}. Closer to 1 is better.}
%\label{app:table:map_results}
%\end{table*}
%\begin{table*}[h]
%\centering
%\begin{tabular}{|l|c||c|c||c|c|c|} \hline
%Dataset     	          &  Nodes & Combinatorial $\mathbb{H}_2$ &  FB $\mathbb{H}_2$ & h-MDS & PCA & FB                  \\ \hline\hline
%Bal. Tree 1          & 40  	       &   {\bf 0.019}         &      0.425               &    {\bf 0.077}        &  0.496    & 0.236 \\ \hline
%Phylo. Tree          & 344      &   {\bf 0.006}             &     0.832               &     {\bf 0.039}     &   0.746        &       0.583     \\ \hline \hline
%WordNet              & 74374 &   {\bf 0.054}           &     0.793                &               &  & 0.503\\ \hline 
%CS PhDs              & 1025    &  {\bf 0.243}    &   0.542                           &  {\bf 0.149}     &  0.708  & 0.336   \\ \hline \hline
%Diseases              & 516     &  {\bf 0.178}    &    0.410                          &  {\bf 0.111}      &    0.595 &  0.764              \\ \hline 
%Protein - Yeast   & 1458   &  {\bf 0.560}    &    0.581                            &{\bf 0.392}    &  0.591  &  0.765    \\ \hline \hline
%Gr-QC               & 4158      &   0.603    &   {\bf 0.387}                               & {\bf 0.530}    &   0.546 &   0.713\\ \hline 
%California    	& 5925      &   0.714    &    {\bf 0.543}                          &   0.603  &     0.481               &      \\ \hline 
%\end{tabular}
%\caption{Distortion measures using combinatorial and h-DMS techniques, compared against PCA and results from \citet{fb}. Closer to 0 is better.}
%\label{app:table:distortion_results}
%\end{table*}




% We show the MAP and average distortion for the resulting embedding and compare against the results from \citet{fb} for $\mathbb{H}_2$. 
\begin{table*}[]
\centering
\begin{tabular}{|c||c|c|c||c|c|c||c|c|c|}
\hline   
& \multicolumn{3}{c|}{MAP} &   \multicolumn{3}{c|}{2-MAP} &  \multicolumn{3}{c|}{$d_{avg}$}  \\ \hline
Rank       & h-MDS    & PCA      & FB      & h-MDS    & PCA      &  FB  & h-MDS    & PCA   & FB \\    \hline     \hline
Rank 2    & 0.346      &  0.614  &  {\bf 0.718}         & 0.754      &  {\bf 0.874}  &       0.802       & {\bf0.317}     &0.888  & 0.575  \\ \hline
Rank 5    & 0.439      & 0.627   &  {\bf 0.761}         & 0.844      &   0.905 & {\bf 0.950}       &{\bf 0.083}      &0.833 &  0.583 \\ \hline
Rank 10  & 0.471      & 0.632   &  {\bf 0.777}        & 0.857      &   0.912 &  {\bf 0.953}      & {\bf 0.048}      &0.804  & 0.586\\  \hline
Rank 50  & 0.560      & 0.687   &  {\bf 0.784}          & 0.880      &  0.962  &  {\bf 0.974}      & {\bf 0.036}     &0.768 & 0.584 \\ \hline
Rank 100 &0.645      & 0.698   &   {\bf 0.795}        & 0.926      &  {\bf 0.999}  &   0.981     &  {\bf 0.036}     &0.760 & 0.583 \\ \hline
Rank 200 &0.823      & {\bf 1.0}       &   0.811           & 0.968      & {\bf 1.0}      &   0.986      & {\bf 0.039}      & 0.746 & 0.583\\ \hline
\end{tabular}
\caption{Phylogenetic tree dataset.  Variation with rank, measured with MAP, 2-MAP, and $d_{avg}$. }
\label{table:rank_results}
\end{table*}

\paragraph*{Hyperparameter: Effect of Rank}
We also considered the influence of the dimension on the perfomance of
h-MDS, PCA, and FB. On the Phylogenetic tree dataset, we measured
distortion and MAP metrics for dimensions of 2,5,10,50,100, and
200. The results are shown in Table~\ref{table:rank_results}. We
expected all of the techniques to improve with better rank, and this
was the case as well. Here, the optimization-based approach typically
produces the best MAP, optimizing the fine details accurately. We
observe that the gap is closed when considering 2-MAP (that is, MAP
where the retrieved neighbors are at distance up to 2 away). In
particular we see that the main limitation of h-MDS is at the finest
layer, confirming the idea MAP is heavily influenced by local
changes. In terms of distortion, we found that h-MDS offers
good performance even at a very low dimension ($0.083$ at 5 dimensions).

\paragraph*{Precision Experiment}  (cf Table~\ref{table:mds-precision}).
Finally, we considered the effect of precision on h-MDS for a balanced tree and fixed dimension 10.
\begin{table}[ht!]
\centering
\begin{tabular}{|c||c|c|}
\hline 
  Precision   & $D_{avg}$ & MAP \\    \hline   
128 & 0.357  & 0.347 \\ \hline
256 & 0.091 &0.986 \\ \hline
512 & 0.076 & 1.0  \\  \hline
1024 &0.064 & 1.0  \\ \hline
\end{tabular}
\caption{h-MDS recovery at different precision levels for a $3$-ary tree and rank 10.}
\label{table:mds-precision}
\end{table}