Recently, hyperbolic embeddings have been proposed as a way to capture
hierarchy information for use in link prediction and natural language
processing tasks~\cite{fb, ucl}. These approaches are an exciting new
way to fuse rich structural information (for example, from knowledge graphs or
synonym hierarchies) with the continuous representations favored by
modern machine learning.


To understand the intuition behind hyperbolic embeddings' superior
capacity, note that trees can be embedded with arbitrarily low
distortion into the Poincar\'e disk, a model of hyperbolic space
with only two dimensions~\cite{sarkar}. In contrast, Bourgain's
theorem \cite{Lineal} shows that Euclidean space is unable to obtain
comparably low distortion for trees---even using an unbounded number of
dimensions.
Moreover, hyperbolic space can preserve certain properties;
for example, angles between embedded vectors are the same in
both Euclidean space and the Poincar\'e model (the mapping is conformal),
which suggests embedded data may be easily able to integrate with downstream
tasks. 

Many graphs, such as complex networks~\cite{krioukov2010hyperbolic}, including the Internet~\cite{krioukov2009curvature} and social networks~\cite{verbeek2016metric}) are known to have hyperbolic structure and thus befit hyperbolic embeddings. Recent works show that hyperbolic representations are indeed suitable for many hierarchies (e.g, the question answering system HyperQA proposed in \cite{tay2018hyperbolic}, vertex classifiers in \cite{ucl}, and link prediction \cite{fb}). However, the optimization problem underlying these embeddings is
challenging, and we seek to understand the subtle tradeoffs involved.

We begin by considering the situation in which we are given an input
graph that is a tree or nearly tree-like, and our goal is to produce
a low-dimensional hyperbolic embedding that preserves all distances. This
leads to a simple strategy that is combinatorial in that it does not
minimize a surrogate loss function using gradient descent. It is both
fast (nearly linear time) and has formal quality guarantees. The
approach proceeds in two phases: (1) we produce an embedding of a
graph into a weighted tree, and (2) we embed that tree into the
hyperbolic disk. In particular, we consider an extension of an elegant embedding of trees
into the Poincar\'e disk by Sarkar~\cite{sarkar} and recent work on
low-distortion graph embeddings into tree metrics~\cite{Abraham}. For trees, this approach has nearly perfect
quality. On the WordNet hypernym graph reconstruction, this obtains
nearly perfect mean average precision (MAP) $0.989$ using just two
dimensions, which outperforms the best published numbers in \citet{fb}
by almost $0.12$ points with $200$ dimensions.

We analyze this construction to extract fundamental tradeoffs. One tradeoff involves  the
dimension, the properties of the graph, and the number of bits of precision - an important hidden cost. For example, on the WordNet graph, we require almost 500 bits of precision to store values from the combinatorial embedding. We can reduce this number to 32 bits, but at the cost of using 10 dimensions instead of two. We show that for a fixed precision, the dimension
required scales linearly with the length of the longest path. On the
other hand, the dimension scales logarithmically with the maximum
degree of the tree. This suggests that hyperbolic embeddings should
have high quality on hierarchies like WordNet but require large
dimensions or high precision on graphs with long chains---which is supported by our
experiments. A second observation is that in contrast to Euclidean
embeddings, hyperbolic embeddings are not scale invariant. This
motivates us to add a learnable scale term into a stochastic gradient descent-based Pytorch algorithm
described below, and we show that it allows us to empirically improve the
quality of embeddings.

To understand how hyperbolic embeddings perform for metrics that are
far from tree-like, we consider a more general problem: given a matrix
of distances that arise from points that are embeddable in hyperbolic
space of dimension $d$ (not necessarily from a graph), find a set of
points that produces these distances. In Euclidean space, the problem
is known as multidimensional scaling (MDS) which is solvable using
PCA.%
\footnote{There is no perfect analogue of PCA in hyperbolic
space~\cite{annals:stats}.}
A key step is a transformation that
effectively centers the points--without knowledge of their exact
coordinates. It is not obvious how to center points in hyperbolic
space, which is curved.
% We show that in hyperbolic space, a centering
% operation is possible using the Perron-Frobenius theorem. In
% particular, the largest eigenvalue of the distance matrix is positive
% and corresponds to a component-wise positive eigenvector. The
% components of this eigenvector allow us to define a transformation to
% center the points.
We show that in hyperbolic space, a centering operation is still possible with respect to a non-standard mean.
In turn, this allows us to reduce the hyperbolic
MDS problem (h-MDS) to a standard eigenvalue problem, and so
it can be solved with scalable power methods.
Further, we extend
classical perturbation analysis~\cite{Sibson1,Sibson2}. When applied to distances from real data,
h-MDS obtains low distortion on graphs that are far from tree
like. However, we observe that these solutions may require high
precision, which is not surprising in light of our previous analysis.

Finally, we consider handling increasing amounts of noise in the
model, which leads naturally into new SGD-based formulations. In
traditional PCA, one may discard eigenvectors that have
correspondingly small eigenvalues to cope with noise. In hyperbolic
space, this approach may produce suboptimal results. Like PCA, the
underlying problem is nonconvex. In contrast to PCA, the optimization
problem is more challenging: the underlying problem has local minima
that are not global minima. Our main technical result is that an
SGD-based algorithm initialized with a h-MDS solution can recover the
submanifold the data is on--even in some cases in which the data is
perturbed by noise that can be full dimensional. Our algorithm
essentially provides new recovery results for convergence for
Principal Geodesic Analysis (PGA) in hyperbolic space~\cite{PGA, GPCA}.
We discuss the nuances between our optimization algorithms and previous attempts 
at these problems in Appendix~\ref{sec:related}.

All of our results can handle incomplete distance information through standard
techniques. Using the observations above, we
implemented an SGD algorithm that minimizes
the loss derived from the PGA loss using PyTorch.\footnote{A minor
  instability with \citet{fb,ucl}'s formulation is that one must guard
  against \textsc{NaN}s. This instability may be unavoidable in
  formulations that minimize hyperbolic distance with gradient
  descent, as the derivative of the hyperbolic distance has a
  singularity, that is, $\lim_{y \to x} \partial_x |d_H(x, y)| \to
  \infty$ for any $x \in \mathbb{H}$ in which $d_H$ is the hyperbolic
  distance function. This issue can be mitigated by minimizing
  $d_H^2$, which does have a continuous derivative throughout
  $\mathbb{H}$. We propose to do so in Section~\ref{sec:PGA} and
discuss this further in the Appendix.}


%% In Section~\ref{sec:background}, we give background on hyperbolic
%% geometry and embeddings. We describe our results on combinatorial
%% embeddings in Section~\ref{sec:combinatorial}. In
%% Section~\ref{sec:MDS}, we introduce hMDS and PGA. Experimental results
%% in Section~\ref{sec:experiments} illustrate the regimes where they are
%% most useful.

