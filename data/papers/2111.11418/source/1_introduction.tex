\vspace{-30pt}
\section{Introduction}
Transformers have gained much interest and success in the computer vision field~\cite{double_attention, stand_alone_attention, vaswani2021scaling, detr}.  
Since the seminal work of Vision Transformer (ViT) \cite{vit} that adapts pure Transformers to image classification tasks, many follow-up models are developed to make further improvements and achieve promising performance in various computer vision tasks \cite{deit, t2t, swin}. 


The Transformer encoder, as shown in Figure \ref{fig:first_figure}(a), consists of two  components. One is the attention module for mixing information among tokens and we term it as \textit{token mixer}. The other component contains the remaining modules, such as channel MLPs and residual connections. By regarding the attention module as a specific token mixer, we further abstract the overall Transformer into a general architecture \textit{MetaFormer} where the token mixer is not specified, as shown in Figure \ref{fig:first_figure}(a).


The success of Transformers has been long attributed to the attention-based token mixer \cite{transformer}. Based on this common belief, many variants of the attention modules  \cite{convit, pvt, refiner, tnt} have been developed to improve the Vision Transformer. However, a very recent work~\cite{mlp-mixer}  replaces the attention module completely with spatial MLPs as token mixers, and finds the derived  MLP-like model can readily attain competitive performance on image classification benchmarks. The follow-up works \cite{resmlp, gmlp, vip} further improve MLP-like models by data-efficient training and specific MLP module design, gradually narrowing the performance gap to ViT and challenging the dominance of attention as token mixers.


Some recent approaches~\cite{fnet, infinite_former, gfnet, continus_attention} explore other types of token mixers within the MetaFormer architecture, and have demonstrated encouraging performance. For example, \cite{fnet} replaces attention with Fourier Transform and still achieves around 97\% of the accuracy of vanilla Transformers. Taking all these results together, it seems as long as a model adopts MetaFormer as the general architecture, promising results could be attained. We thus hypothesize \textit{compared with specific token mixers, MetaFormer is more essential for the model to achieve competitive performance}. 


To verify this hypothesis, we apply an extremely simple non-parametric operator, \emph{pooling}, as the token mixer to conduct only basic token mixing. Astonishingly, this derived model, termed \emph{PoolFormer}, achieves competitive performance, and even consistently outperforms well-tuned Transformer and MLP-like models, including DeiT \cite{deit} and ResMLP \cite{resmlp}, as shown in Figure \ref{fig:first_figure}(b). More specifically, PoolFormer-M36 achieves 82.1\% top-1 accuracy on ImageNet-1K classification benchmark, surpassing well-tuned vision Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3\%/1.1\% accuracy with 35\%/52\% fewer parameters and 50\%/62\% fewer MACs. These results demonstrate that MetaFormer, even with a naive token mixer, can still deliver promising performance. We thus argue that MetaFormer is our \emph{de facto} need for vision models which is more essential to achieve competitive performance rather than specific token mixers. Note that it does not mean the token mixer is insignificant. MetaFormer still has this abstracted component. It means token mixer is not limited to a specific type, e.g. attention.


The contributions of our paper are two-fold. Firstly, we abstract Transformers into a general architecture MetaFormer, and empirically demonstrate that the success of Transformer/MLP-like models is largely attributed to the  MetaFormer architecture. Specifically, by only employing a simple non-parametric operator, pooling, as an extremely weak token mixer for MetaFormer, we build a simple model named PoolFormer and find it can still achieve highly competitive performance. We hope our findings inspire more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Secondly, we evaluate the proposed PoolFormer on multiple vision tasks including image classification \cite{imagenet}, object detection \cite{coco}, instance segmentation \cite{coco}, and semantic segmentation \cite{ade20k}, and find it achieves competitive performance compared with the SOTA models using sophistic design of token mixers. The PoolFormer can readily serve as a good starting baseline for future MetaFormer architecture design.







