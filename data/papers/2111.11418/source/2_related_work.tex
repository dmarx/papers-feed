\section{Related work}
Transformers are first proposed by \cite{transformer} for translation tasks and then rapidly become popular in various NLP tasks. 
In language pre-training tasks, Transformers are trained on large-scale unlabeled text corpus and achieve amazing performance \cite{bert, gpt3}. 
Inspired by the success of Transformers in NLP, many researchers apply attention mechanism and Transformers to vision tasks \cite{double_attention, stand_alone_attention, vaswani2021scaling, detr}. Notably, Chen \etal introduce iGPT \cite{igpt} where the Transformer is trained to auto-regressively predict pixels on images for self-supervised learning. Dosovitskiy \etal propose Vision Transformer (ViT) with hard patch embedding as input\cite{vit}. They show that on supervised image classification tasks, a ViT pre-trained on a large propriety dataset (JFT dataset with 300 million images) can achieve excellent performance. DeiT \cite{deit} and T2T-ViT \cite{t2t} further demonstrate that the ViT pre-trained on only ImageNet-1K ($\sim$ 1.3 million images) from scratch can achieve promising performance. A lot of works have been focusing on improving the token mixing approach of Transformers by shifted windows~\cite{swin}, relative position encoding~\cite{wu2021rethinking}, refining attention map~\cite{refiner}, or incorporating convolution \cite{guo2021cmt, wu2021cvt, d2021convit}, \etc. In addition to attention-like token mixers, \cite{mlp-mixer, resmlp} surprisingly find that merely adopting MLPs as token mixers can still achieve competitive performance. This discovery challenges the dominance of attention-based token mixers and triggers a heated discussion in the research community about which token mixer is better \cite{vip, chen2021cyclemlp}. However, the target of this work is neither to be engaged in this debate nor to design new complicated token mixers to achieve new state of the art. Instead, we examine a fundamental question: What is truly responsible for the success of the Transformers and their variants? Our answer is the general architecture \ie, MetaFormer. We simply utilize pooling as basic token mixers to probe the power of MetaFormer.

Contemporarily, some works contribute to answering the same question. Dong \etal prove that without residual connections or MLPs, the output converges doubly exponentially to a rank one matrix \cite{dong2021attention}. Raghu \etal \cite{raghu2021vision} compare the feature difference between ViT and CNNs, finding that self-attention allows early gathering of global information while residual connections greatly propagate features from lower layers to higher ones. Park \etal \cite{park2022how} shows that multi-head self-attentions improve accuracy and generalization by flattening the loss landscapes. Unfortunately, they do not abstract Transformers into a general architecture and study them from the aspect of general framework.