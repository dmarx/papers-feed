\begin{tabular}{@{}l|ccccc@{}}
\toprule
 & \multicolumn{5}{c}{PoolFormer} \\ 
 & S12 & S24 & S36 & M36 & M48 \\
\midrule
Peak drop rate of stoch. depth $d_r$ & 0.1 & 0.1 & 0.2 & 0.3 & 0.4 \\
LayerScale initialization $\epsilon$ & $10^{-5}$ & $10^{-5}$ & $10^{-6}$ & $10^{-6}$ & $10^{-6}$ \\
\hline
Data augmentation & \multicolumn{5}{c}{AutoAugment} \\
Repeated Augmentation & \multicolumn{5}{c}{off} \\
Input resolution & \multicolumn{5}{c}{224} \\
Epochs & \multicolumn{5}{c}{300} \\
Warmup epochs & \multicolumn{5}{c}{5} \\
Hidden dropout & \multicolumn{5}{c}{0} \\
GeLU dropout & \multicolumn{5}{c}{0} \\
Classification dropout & \multicolumn{5}{c}{0} \\
Random erasing prob & \multicolumn{5}{c}{0.25} \\
EMA decay & \multicolumn{5}{c}{0} \\
Cutmix $\alpha$ & \multicolumn{5}{c}{1.0} \\
Mixup $\alpha$ & \multicolumn{5}{c}{0.8} \\
Cutmix-Mixup switch prob & \multicolumn{5}{c}{0.5} \\
Label smoothing & \multicolumn{5}{c}{0.1} \\
\tabincell{l}{Relation between peak learning \\ \qquad rate and batch size} & \multicolumn{5}{c}{$\mathrm{lr} = \frac{\mathrm{batch\_size}}{1024}\times 10^{-3}$} \\
Batch size used in the paper & \multicolumn{5}{c}{4096} \\
Peak learning rate used in the paper & \multicolumn{5}{c}{$4 \times 10^{-4}$} \\
Learning rate decay & \multicolumn{5}{c}{cosine} \\
Optimizer & \multicolumn{5}{c}{AdamW} \\
Adam $\epsilon$ & \multicolumn{5}{c}{1e-8} \\
Adam $(\beta_1, \beta_2)$ & \multicolumn{5}{c}{(0.9, 0.999)} \\
Weight decay & \multicolumn{5}{c}{0.05} \\
Gradient clipping & \multicolumn{5}{c}{None} \\
\bottomrule
\end{tabular}