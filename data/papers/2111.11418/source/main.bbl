\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{layer_norm}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H. Larochelle, M. Ranzato, R. Hadsell, M.~F. Balcan, and H. Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{detr}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European Conference on Computer Vision}, pages 213--229.
  Springer, 2020.

\bibitem{mmdetection}
Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li,
  Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng,
  Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,
  Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen~Change Loy, and
  Dahua Lin.
\newblock {MMDetection}: Open mmlab detection toolbox and benchmark.
\newblock {\em arXiv preprint arXiv:1906.07155}, 2019.

\bibitem{chen2017deeplab}
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan~L
  Yuille.
\newblock Deeplab: Semantic image segmentation with deep convolutional nets,
  atrous convolution, and fully connected crfs.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  40(4):834--848, 2017.

\bibitem{igpt}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em International Conference on Machine Learning}, pages
  1691--1703. PMLR, 2020.

\bibitem{chen2021cyclemlp}
Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo.
\newblock Cyclemlp: A mlp-like architecture for dense prediction.
\newblock {\em arXiv preprint arXiv:2107.10224}, 2021.

\bibitem{double_attention}
Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng.
\newblock A\^{} 2-nets: Double attention networks.
\newblock {\em Advances in Neural Information Processing Systems}, 31:352--361,
  2018.

\bibitem{chollet2017xception}
Fran{\c{c}}ois Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1251--1258, 2017.

\bibitem{mmseg2020}
MMSegmentation Contributors.
\newblock {MMSegmentation}: Openmmlab semantic segmentation toolbox and
  benchmark.
\newblock \url{https://github.com/open-mmlab/mmsegmentation}, 2020.

\bibitem{randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{d2021convit}
St{\'e}phane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio
  Biroli, and Levent Sagun.
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock {\em arXiv preprint arXiv:2103.10697}, 2021.

\bibitem{convit}
St{\'e}phane D'Ascoli, Hugo Touvron, Matthew~L Leavitt, Ari~S Morcos, Giulio
  Biroli, and Levent Sagun.
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 2286--2296. PMLR, 18--24 Jul 2021.

\bibitem{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL-HLT (1)}, 2019.

\bibitem{dong2021attention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly
  exponentially with depth.
\newblock {\em arXiv preprint arXiv:2103.03404}, 2021.

\bibitem{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{silu}
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock {\em Neural Networks}, 107:3--11, 2018.

\bibitem{fvcore}
fvcore Contributors.
\newblock fvcore.
\newblock \url{https://github.com/facebookresearch/fvcore}, 2021.

\bibitem{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{guo2021cmt}
Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe
  Wang.
\newblock Cmt: Convolutional neural networks meet vision transformers.
\newblock {\em arXiv preprint arXiv:2107.06263}, 2021.

\bibitem{tnt}
Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
\newblock Transformer in transformer.
\newblock {\em arXiv preprint arXiv:2103.00112}, 2021.

\bibitem{mask_rcnn}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2961--2969, 2017.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{gelu}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{vip}
Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, and Jiashi
  Feng.
\newblock Vision permutator: A permutable mlp-like architecture for visual
  recognition.
\newblock {\em arXiv preprint arXiv:2106.12368}, 2021.

\bibitem{stochastic_depth}
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em European conference on computer vision}, pages 646--661.
  Springer, 2016.

\bibitem{batch_norm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{fpn}
Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Panoptic feature pyramid networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6399--6408, 2019.

\bibitem{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems},
  25:1097--1105, 2012.

\bibitem{fnet}
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock {\em arXiv preprint arXiv:2105.03824}, 2021.

\bibitem{retinanet}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017.

\bibitem{coco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{gmlp}
Hanxiao Liu, Zihang Dai, David~R So, and Quoc~V Le.
\newblock Pay attention to mlps.
\newblock {\em arXiv preprint arXiv:2105.08050}, 2021.

\bibitem{swin}
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 10012--10022, October 2021.

\bibitem{adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{mamalet2012simplifying}
Franck Mamalet and Christophe Garcia.
\newblock Simplifying convnets for fast learning.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 58--65. Springer, 2012.

\bibitem{continus_attention}
Andr\'{e} Martins, Ant\'{o}nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro
  Aguiar, and Mario Figueiredo.
\newblock Sparse and continuous attention mechanisms.
\newblock In H. Larochelle, M. Ranzato, R. Hadsell, M.~F. Balcan, and H. Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 20989--21001. Curran Associates, Inc., 2020.

\bibitem{infinite_former}
Pedro~Henrique Martins, Zita Marinho, and Andr{\'e}~FT Martins.
\newblock $\infty$-former: Infinite memory transformer.
\newblock {\em arXiv preprint arXiv:2109.00301}, 2021.

\bibitem{relu}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em Icml}, 2010.

\bibitem{park2022how}
Namuk Park and Songkuk Kim.
\newblock How do vision transformers work?
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{raghu2021vision}
Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey
  Dosovitskiy.
\newblock Do vision transformers see like convolutional neural networks?
\newblock {\em arXiv preprint arXiv:2108.08810}, 2021.

\bibitem{stand_alone_attention}
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,
  and Jon Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{gfnet}
Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou.
\newblock Global filter networks for image classification.
\newblock {\em arXiv preprint arXiv:2107.00645}, 2021.

\bibitem{rosenblatt1961principles}
Frank Rosenblatt.
\newblock Principles of neurodynamics. perceptrons and the theory of brain
  mechanisms.
\newblock Technical report, Cornell Aeronautical Lab Inc Buffalo NY, 1961.

\bibitem{rumelhart1985learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California Univ San Diego La Jolla Inst for
  Cognitive Science, 1985.

\bibitem{gradcam}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 618--626, 2017.

\bibitem{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem{label_smoothing}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{mlp-mixer}
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
  Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob
  Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em arXiv preprint arXiv:2105.01601}, 2021.

\bibitem{resmlp}
Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin
  El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve,
  Jakob Verbeek, et~al.
\newblock Resmlp: Feedforward networks for image classification with
  data-efficient training.
\newblock {\em arXiv preprint arXiv:2105.03404}, 2021.

\bibitem{deit}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  10347--10357. PMLR, 2021.

\bibitem{cait}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock {\em arXiv preprint arXiv:2103.17239}, 2021.

\bibitem{vaswani2021scaling}
Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake
  Hechtman, and Jonathon Shlens.
\newblock Scaling local self-attention for parameter efficient visual
  backbones.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12894--12904, 2021.

\bibitem{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{pvt}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 568--578, October 2021.

\bibitem{timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{resnet_improved}
Ross Wightman, Hugo Touvron, and Herv{\'e} J{\'e}gou.
\newblock Resnet strikes back: An improved training procedure in timm.
\newblock {\em arXiv preprint arXiv:2110.00476}, 2021.

\bibitem{wu2021cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei
  Zhang.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock {\em arXiv preprint arXiv:2103.15808}, 2021.

\bibitem{wu2021rethinking}
Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao.
\newblock Rethinking and improving relative position encoding for vision
  transformer.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10033--10041, 2021.

\bibitem{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem{t2t}
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
  Francis~E.H. Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 558--567, October 2021.

\bibitem{cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6023--6032, 2019.

\bibitem{mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{cutout}
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.
\newblock Random erasing data augmentation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 13001--13008, 2020.

\bibitem{ade20k}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 633--641, 2017.

\bibitem{refiner}
Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie
  Jin, Qibin Hou, and Jiashi Feng.
\newblock Refiner: Refining self-attention for vision transformers.
\newblock {\em arXiv preprint arXiv:2106.03714}, 2021.

\end{thebibliography}
