% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{hinton2012deep}
G.~Hinton, L.~Deng, D.~Yu, G.~Dahl, A.-r. Mohamed, N.~Jaitly, A.~Senior,
  V.~Vanhoucke, P.~Nguyen, B.~Kingsbury \emph{et~al.}, ``Deep neural networks
  for acoustic modeling in speech recognition,'' \emph{IEEE Signal processing
  magazine}, vol.~29, 2012.

\bibitem{dahl-ieeetasl-2012}
G.~Dahl, D.~Yu, L.~Deng, and A.~Acero, ``{Context-Dependent Pre-Trained Deep
  Neural Networks for Large-Vocabulary Speech Recognition},'' \emph{{IEEE
  Transactions on Audio, Speech, and Language Processing}}, vol.~20, Jan 2012.

\bibitem{sainath-icassp-2013}
T.~Sainath, A.~rahman Mohamed, B.~Kingsbury, and B.~Ramabhadran, ``{Deep
  Convolutional Neural Networks for LVCSR},'' in \emph{{ICASSP}}, 2013.

\bibitem{graves-icassp-2013}
A.~Graves, A.~rahman Mohamed, and G.~Hinton, ``{Speech Recognition with Deep
  Recurrent Neural Networks},'' in \emph{{ICASSP}}, 2013.

\bibitem{graves-icml-2014}
A.~Graves and N.~Jaitly, ``{Towards End-to-End Speech Recognition with
  Recurrent Neural Networks},'' in \emph{{ICML}}, 2014.

\bibitem{Chan2016ListenAA}
W.~Chan, N.~Jaitly, Q.~V. Le, and O.~Vinyals, ``{Listen, Attend and Spell: A
  Neural Network for Large Vocabulary Conversational Speech Recognition},'' in
  \emph{{ICASSP}}, 2016.

\bibitem{bahdanau-icassp-2016}
D.~Bahdanau, J.~Chorowski, D.~Serdyuk, P.~Brakel, and Y.~Bengio, ``{End-to-End
  Attention-based Large Vocabulary Speech Recognition},'' in \emph{{ICASSP}},
  2016.

\bibitem{chiu-icassp-2018}
C.-C. Chiu, T.~N. Sainath, Y.~Wu, R.~Prabhavalkar, P.~Nguyen, Z.~Chen,
  A.~Kannan, R.~J. Weiss, K.~Rao, E.~Gonina, N.~Jaitly, B.~Li, J.~Chorowski,
  and M.~Bacchiani, ``{State-of-the-art Speech Recognition With
  Sequence-to-Sequence Models},'' in \emph{{ICASSP}}, 2018.

\bibitem{kanda-asru-2013}
N.~Kanda, R.~Takeda, and Y.~Obuchi, ``{Elastic spectral distortion for low
  resource speech recognition with deep neural networks},'' in \emph{{ASRU}},
  2013.

\bibitem{ragni-interspeech-2014}
A.~Ragni, K.~M. Knill, S.~P. Rath, and M.~J.~F. Gales, ``{Data augmentation for
  low resource languages},'' in \emph{{INTERSPEECH}}, 2014.

\bibitem{jaitly-2013-icml}
N.~Jaitly and G.~Hinton, ``{Vocal Tract Length Perturbation (VTLP) improves
  speech recognition},'' in \emph{{ICML Workshop on Deep Learning for Audio,
  Speech and Language Processing}}, 2013.

\bibitem{hannun-arxiv-2014}
A.~Hannun, C.~Case, J.~Casper, B.~Catanzaro, G.~Diamos, E.~Elsen, R.~Prenger,
  S.~Satheesh, S.~Sengupta, A.~Coates, and A.~Ng, ``{Deep Speech: Scaling up
  end-to-end speech recognition},'' in \emph{{arXiv}}, 2014.

\bibitem{ko-interspeech-2015}
T.~Ko, V.~Peddinti, D.~Povey, and S.~Khudanpur, ``{Audio Augmentation for
  Speech Recognition},'' in \emph{{INTERSPEECH}}, 2015.

\bibitem{kim-interspeech-2017}
C.~Kim, A.~Misra, K.~Chin, T.~Hughes, A.~Narayanan, T.~Sainath, and
  M.~Bacchiani, ``{Generation of large-scale simulated utterances in virtual
  rooms to train deep-neural networks for far-field speech recognition in
  Google Home},'' in \emph{{INTERSPEECH}}, 2017.

\bibitem{prabhavalkar-2015-icassp}
R.~{Prabhavalkar}, R.~{Alvarez}, C.~{Parada}, P.~{Nakkiran}, and T.~N.
  {Sainath}, ``Automatic gain control and multi-style training for robust
  small-footprint keyword spotting with deep neural networks,'' in
  \emph{{ICASSP}}, 2015.

\bibitem{raju-arxiv-2018}
A.~Raju, S.~Panchapagesan, X.~Liu, A.~Mandal, and N.~Strom, ``{Data
  Augmentation for Robust Keyword Spotting under Playback Interference},'' in
  \emph{arXiv}, 2018.

\bibitem{mallidi-icassp-2016}
S.~H. Mallidi and H.~Hermansky, ``{Novel neural network based fusion for
  Multistream ASR},'' in \emph{{ICASSP}}, 2016.

\bibitem{Cubuk2018AutoAugmentLA}
E.~D. Cubuk, B.~Zoph, D.~Man{\'e}, V.~Vasudevan, and Q.~V. Le, ``Autoaugment:
  Learning augmentation policies from data,'' in \emph{{CVPR}}, 2019.

\bibitem{devries-arxiv-2017}
T.~DeVries and G.~Taylor, ``{Improved Regularization of Convolutional Neural
  Networks with Cutout},'' in \emph{arXiv}, 2017.

\bibitem{Panayotov2015LibriSpeechAA}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``{Librispeech: An ASR
  corpus based on public domain audio books},'' in \emph{ICASSP}, 2015.

\bibitem{gulcehre-2015-arxiv}
{\c{C}}.~G{\"{u}}l{\c{c}}ehre, O.~Firat, K.~Xu, K.~Cho, L.~Barrault, H.~Lin,
  F.~Bougares, H.~Schwenk, and Y.~Bengio, ``On using monolingual corpora in
  neural machine translation,'' in \emph{{arxiv}}, 2015.

\bibitem{switchboard}
J.~Godfrey, E.~Holliman, and J.~McDaniel, ``{SWITCHBOARD: telephone speech
  corpus for research and development},'' in \emph{{ICASSP}}, 1992.

\bibitem{fisher}
C.~Cieri, D.~Miller, and K.~Walker, ``The fisher corpus: a resource for the
  next generations of speech-to-text,'' in \emph{{LREC}}, 2004.

\bibitem{zeyer-interspeech-2018}
A.~Zeyer, K.~Irie, R.~Schl{\"{u}}ter, and H.~Ney, ``Improved training of
  end-to-end attention models for speech recognition,'' in
  \emph{{INTERSPEECH}}, 2018.

\bibitem{irie-arxiv-2019}
K.~Irie, R.~Prabhavalkar, A.~Kannan, A.~Bruguier, D.~Rybach, and P.~Nguyen,
  ``{Model Unit Exploration for Sequence-to-Sequence Speech Recognition},'' in
  \emph{{arXiv}}, 2019.

\bibitem{schuster-icassp-2012}
M.~Schuster and K.~Nakajima, ``Japanese and korean voice search,'' in
  \emph{{ICASSP}}, 2012.

\bibitem{graves-nips-2011}
A.~Graves, ``{Practical Variational Inference for Neural Networks},'' in
  \emph{{NIPS}}, 2011.

\bibitem{lbsm}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna, ``Rethinking the
  inception architecture for computer vision,'' in \emph{{CVPR}}, 2016.

\bibitem{chorowski-2017-interspeech}
J.~Chorowski and N.~Jaitly, ``Towards better decoding and language model
  integration in sequence to sequence models,'' in \emph{{INTERSPEECH}}, 2017.

\bibitem{povey-interspeech-2016}
D.~Povey, V.~Peddinti, D.~Galvez, P.~Ghahrmani, V.~Manohar, X.~Na, Y.~Wang, and
  S.~Khudanpur, ``{Purely sequence-trained neural networks for ASR based on
  lattice-free MMI},'' in \emph{{INTERSPEECH}}, 2016.

\bibitem{han-arxiv-2017}
K.~J. Han, A.~Chandrashekaran, J.~Kim, and I.~Lane, ``{The CAPIO 2017
  Conversational Speech Recognition System},'' in \emph{{arXiv}}, 2017.

\bibitem{yang-arxiv-2018}
X.~Yang, J.~Li, and X.~Zhou, ``{A novel pyramidal-FSMN architecture with
  lattice-free MMI for speech recognition},'' in \emph{{arXiv}}, 2018.

\bibitem{collobert-arxiv-2016}
R.~Collobert, C.~Puhrsch, and G.~Synnaeve, ``{Wav2Letter: an End-to-End
  ConvNet-based Speech Recognition System},'' in \emph{arXiv}, 2016.

\bibitem{liptchinsky-arxiv-2017}
V.~Liptchinsky, G.~Synnaeve, and R.~Collobert, ``{Letter-Based Speech
  Recognition with Gated ConvNets},'' in \emph{{arXiv}}, 2017.

\bibitem{zhou-icassp-2018}
Y.~Zhou, C.~Xiong, and R.~Socher, ``{Improving End-to-End Speech Recognition
  with Policy Learning},'' in \emph{{ICASSP}}, 2018.

\bibitem{zeghidour-arxiv-2018}
N.~Zeghidour, Q.~Xu, V.~Liptchinsky, N.~Usunier, G.~Synnaeve, and R.~Collobert,
  ``{Fully Convolutional Speech Recognition},'' in \emph{{arXiv}}, 2018.

\bibitem{li-arxiv-2019}
J.~Li, V.~Lavrukhin, B.~Ginsburg, R.~Leary, O.~Kuchaiev, J.~M. Cohen,
  H.~Nguyen, and R.~T. Gadde, ``{Jasper: An End-to-End Convolutional Neural
  Acoustic Model},'' in \emph{{arXiv}}, 2019.

\bibitem{zeyer-nips-2018}
A.~Zeyer, A.~Merboldt, R.~Schl{\"{u}}ter, and H.~Ney, ``{A comprehensive
  analysis on attention models},'' in \emph{{NIPS: Workshop IRASL}}, 2018.

\bibitem{sabour-iclr-2019}
S.~Sabour, W.~Chan, and M.~Norouzi, ``{Optimal Completion Distillation for
  Sequence Learning},'' in \emph{{ICLR}}, 2019.

\bibitem{povey-asru-2011}
D.~Povey, A.~Ghoshal, G.~Boulianne, L.~Burget, O.~Glembek, N.~Goel,
  M.~Hannemann, P.~Motlicek, Y.~Qian, P.~Schwarz, J.~Silovsky, G.~Stemmer, and
  K.~Vesely, ``{The Kaldi Speech Recognition Toolkit},'' in \emph{{ASRU}},
  2011.

\bibitem{vesely-interspeech-2013}
K.~Vesely, A.~Ghoshal, L.~Burger, and D.~Povey, ``{Sequence-discriminative
  training of deep neural networks},'' in \emph{{INTERSPEECH}}, 2013.

\bibitem{hadian-interspeech-2018}
H.~Hadian, H.~Sameti, D.~Povey, and S.~Khudanpur, ``{End-to-end speech
  recognition using lattice-free MMI},'' in \emph{{INTERSPEECH}}, 2018.

\bibitem{zweig-icassp-2017}
G.~Zweig, C.~Yu, J.~Droppo, and A.~Stolcke, ``{Advances in All-Neural Speech
  Recognition},'' in \emph{{ICASSP}}, 2017.

\bibitem{audhkhasi-interspeech-2019}
K.~Audhkhasi, B.~Ramabhadran, G.~Saon, M.~Picheny, and D.~Nahamoo, ``{Direct
  Acoustics-to-Word Models for English Conversational Speech Recognition},'' in
  \emph{{INTERSPEECH}}, 2018.

\bibitem{audhkhasi-icassp-2018}
K.~Audhkhasi, B.~Kingsbury, B.~Ramabhadran, G.~Saon, and M.~Picheny, ``Building
  competitive direct acoustics-to-word models for english conversational speech
  recognition,'' in \emph{{ICASSP}}, 2018.

\bibitem{lu-icassp-2016}
L.~Lu, X.~Zhang, and S.~Renals, ``{On training the recurrent neural network
  encoder-decoder for large vocabulary end-to-end speech recognition},'' in
  \emph{{ICASSP}}, 2016.

\bibitem{toshniwal-interspeech-2017}
S.~Toshniwal, H.~Tang, L.~Lu, and K.~Livescu, ``{Multitask Learning with
  Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition},'' in
  \emph{{INTERSPEECH}}, 2017.

\bibitem{weng-interspeech-2018}
C.~Weng, J.~Cui, G.~Wang, J.~Wang, C.~Yu, D.~Su, and D.~Yu, ``{Improving
  Attention Based Sequence-to-Sequence Models for End-to-End English
  Conversational Speech Recognition},'' in \emph{{INTERSPEECH}}, 2018.

\bibitem{kovacs2017increasing}
G.~Kov{\'a}cs, L.~T{\'o}th, D.~Van~Compernolle, and S.~Ganapathy, ``Increasing
  the robustness of cnn acoustic models using autoregressive moving average
  spectrogram features and channel dropout,'' \emph{Pattern Recognition
  Letters}, vol. 100, pp. 44--50, 2017.

\bibitem{toth2018perceptually}
L.~T{\'o}th, G.~Kov{\'a}cs, and D.~Van~Compernolle, ``A perceptually inspired
  data augmentation method for noise robust cnn acoustic models,'' in
  \emph{SPECOM}, 2018.

\end{thebibliography}
