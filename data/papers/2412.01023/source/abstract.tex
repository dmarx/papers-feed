\begin{abstract}
    Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work \citep{zeng2022learning} proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context \citep{chen2013hyperbolicity}. In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach \texttt{HypStructure}: a \underline{Hyp}erbolic \underline{Structure}d regularization approach to accurately embed the label hierarchy into the learned representations. \texttt{HypStructure} is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss. It can be combined with any standard task loss to learn \emph{hierarchy-informed} features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of \texttt{HypStructure} in reducing distortion and boosting generalization performance, especially under low-dimensional scenarios. For a better understanding of structured representation, we perform an eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is available at \url{https://github.com/uiuctml/HypStructure}.
\end{abstract}
