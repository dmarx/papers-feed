\section{Introduction}
\label{sec:intro} 
Real-world datasets, such as ImageNet~\citep{imagenet} and CIFAR~\citep{krizhevsky2009learning}, often exhibit a natural hierarchy or an inherent label structure that describes a structured relationship between different classes in the data. In the absence of an existing hierarchy, it is often possible to cheaply construct or infer this hierarchy from the label space directly~\citep{nauata2019structured}. However, the majority of existing representation learning methods~\citep{kolesnikov2019large, chen2020simple, wu2018unsupervised, henaff2020data, tian2020contrastive, hjelm2018learning, he2020momentum, 2020supcon} treat the labels as permutation invariant, ignoring this semantically-rich hierarchical label information. Recently, Zeng et al.~\citep{zeng2022learning} offer a promising approach to embed the tree-hierarchy explicitly in representation learning using a tree-metric-based regularizer, leading to improvements in generalization performance. The approach uses a computation of shortest paths between two classes in the tree hierarchy to enforce the same structure in the feature space, by means of a \textbf{C}o\textbf{p}henetic \textbf{C}orrelation \textbf{C}oefficient (CPCC)~\citep{cpcc} based regularizer. However, their approach uses the $\ell_2$ distance in the Euclidean space, distorting the parent-child representations in the hierarchy~\citep{ ravasz2003hierarchical, li2023euclidean} owing to the bounded dimensionality of the Euclidean space~\citep{chen2013hyperbolicity}.  

Hyperbolic geometry has recently gained growing interest in the field of representation learning \citep{nickel2017poincare,nickel2018learning}. Hyperbolic spaces can be viewed as the continuous analog of a tree, allowing for embedding tree-like data in finite dimensions with minimal distortion \citep{2010hyperbolic,sala2018representation, Sarkar_2012, gulcehre2018hyperbolic}. Unlike Euclidean spaces with zero curvature and spherical spaces with positive curvature, the hyperbolic spaces have negative curvature enabling the length to grow exponentially with its radius. Owing to these advantages, hyperbolic geometry has been used for various applications such as natural language processing \citep{liu2020hyperbolic, sala2018representation, dhingra2018embedding}, image classification \citep{khrulkov2020hyperbolic, yue2023hyperbolic, ermolov2022hyperbolic}, object detection \citep{lang2022hyperbolic, ge2022hyperbolic}, action retrieval \citep{Long_2020_CVPR}, and hierarchical clustering \citep{yan2021unsupervised}.
However, the aim of using hyperbolic geometry in these approaches is often to \emph{implicitly} leverage the hierarchical nature of the data. 

In this work, given a label hierarchy, we argue that accurately and \emph{explicitly} embedding the hierarchical information into the representation space has several benefits, and for this purpose, we propose \texttt{HypStructure}, a hyperbolic label-structure based regularization approach that extends the proposed methodology in~\citet{zeng2022learning} for semantically structured learning in the hyperbolic space. \texttt{HypStructure}
can be easily combined with any standard task loss for optimization, and enables the learning of discriminative and \emph{hierarchy-informed} features. In summary, our contributions are as follows:
\begin{itemize}
    \item We propose \texttt{HypStructure} and demonstrate its effectiveness in the supervised hierarchical classification tasks on three real-world vision benchmark datasets, and show that our proposed approach is effective in both training from scratch, or fine-tuning if there are resource constraints.
    \item We qualitatively and quantitatively assess the nature of the learned representations and demonstrate that along with the performance gains, using \texttt{HypStructure} as a regularizer leads to more interpretable as well as tree-like representations as a side benefit. The low-dimensional representative capacity of hyperbolic geometry is well-known \citep{chami2020low}, and interestingly, we observe that training with \texttt{HypStructure} allows for learning extremely low-dimensional representations with distortion values lower than even their corresponding high-dimensional Euclidean counterparts.
    \item We argue that representations learned with an underlying hierarchical structure are beneficial not only for the in-distribution (ID) classification tasks but also for Out-of-distribution (OOD) detection tasks. We empirically demonstrate that learning ID representations with \texttt{HypStructure} leads to improved OOD detection on 9 real-world OOD datasets without sacrificing ID accuracy \citep{zhang2023openood}. 
    \item Inspired by the improvements in OOD detection, we provide a formal analysis of the eigenspectrum of the in-distribution \emph{hierarchy-informed} features learned with CPCC-style structured regularization methods, thus leading to a better understanding of the behavior of structured representations in general. 
\end{itemize}