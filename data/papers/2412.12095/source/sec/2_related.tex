\section{Related Works}
\label{sec:related}

\textbf{Diffusion Models}.
Diffusion models \cite{sohl2015deep, song0, ddpm} decompose the image generation task into a sequence of iterative denoising steps, gradually transforming noise into a coherent image. Early diffusion models \cite{adm, rombach2022high, sdxl, nichol2021glide, dalle2} with U-net architectures pioneered denoising techniques for high-quality image synthesis. Later works \cite{dit, bao2023all} like DiT shift from the U-net to transformer-based architectures, enabling greater compute scalability. Modern methods \cite{, chen2024pixart, flux} further extend DiT architectures leveraging significantly larger training resources to achieve impressive image generation quality.

\vspace{3pt}
\textbf{Autoregressive Generation}.
Another popular approach for image generation involves autoregressive (AR) transformers that predict images token by token. Early works \cite{dalle1, ding2021cogview, gafni2022make, parti} generated image tokens in raster order, progressing sequentially across the image grid. This rasterized approach was later identified as inefficient \cite{chang2022maskgit}, prompting researchers to explore random-order generation methods \cite{chang2022maskgit, muse}. 
AR methods are further evolved to include new modalities such as video generation~\cite{kondratyukvideopoet} and any-to-any generation~\cite{io2, anygpt}. 

\vspace{3pt}
\textbf{Combining Diffusion and Autoregressive Models}.
Recent models explore different methods for integrating AR and diffusion processes. DART \cite{dart} unifies AR and diffusion in a non-Markovian framework by conditioning on multiple historical denoising steps instead of only the current one. BiGR \cite{bigr} generates discrete binary image codes autoregressively using a Bernoulli diffusion process. MAR \cite{mar} employs an AR model with a small diffusion head to enable continuous-value generation. Emu2 \cite{emu2} applies an external diffusion module to decode its AR-based multimodal outputs. 
Compared to previous methods, CausalFusion focuses on autoregressive sequence factorization and decouples diffusion data processing across both sequential tokens and noise levels, achieving significant performance gains over traditional diffusion frameworks.
