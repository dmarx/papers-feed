@article{Santurkar2023WhoseOD,
  title={Whose Opinions Do Language Models Reflect?},
  author={Shibani Santurkar and Esin Durmus and Faisal Ladhak and Cinoo Lee and Percy Liang and Tatsunori Hashimoto},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.17548}
}

@article{Czarnowska_fairness,
    author = {Czarnowska, Paula and Vyas, Yogarshi and Shah, Kashif},
    title = "{Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1249-1267},
    year = {2021},
    month = {11},
    abstract = "{Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model’s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00425},
    url = {https://doi.org/10.1162/tacl\_a\_00425},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00425/1972677/tacl\_a\_00425.pdf},
} 

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{scialom2020coldgans,
  title={Coldgans: Taming language gans with cautious sampling strategies},
  author={Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18978--18989},
  year={2020}
}
@article{Touvron2023LLaMAOA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aur'elien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
}

@inproceedings{hartvigsen2022toxigen,
  title={ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3309--3326},
  year={2022}
}


@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@article{hosseini2023empirical,
  title={An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models},
  author={Hosseini, Saghar and Palangi, Hamid and Awadallah, Ahmed Hassan},
  journal={arXiv preprint arXiv:2301.09211},
  year={2023}
}

@inproceedings{deng2019residual,
  title={Residual Energy-Based Models for Text Generation},
  author={Deng, Yuntian and Bakhtin, Anton and Ott, Myle and Szlam, Arthur and Ranzato, Marc'Aurelio},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{gudibande2023false,
  title={The false promise of imitating proprietary llms},
  author={Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
  journal={arXiv preprint arXiv:2305.15717},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
@article{gilardi2023chatgpt,
  title={Chatgpt outperforms crowd-workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={arXiv preprint arXiv:2303.15056},
  year={2023}
}

@article{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{
nucleus_sampling,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}

@article{huang2023chatgpt,
  title={Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech},
  author={Huang, Fan and Kwak, Haewoon and An, Jisun},
  journal={arXiv preprint arXiv:2302.07736},
  year={2023}
}

@article{synnaeve2019growing,
  title={Growing Up Together: Structured Exploration for Large Action Spaces},
  author={Synnaeve, Gabriel and Gehring, Jonas and Lin, Zeming and Haziza, Daniel and Usunier, Nicolas and Rothermel, Danielle and Mella, Vegard and Ju, Da and Carion, Nicolas and Gustafson, Laura and others},
  year={2019}
}


@InProceedings{pmlr-v119-scialom20a,
  title = 	 {Discriminative Adversarial Search for Abstractive Summarization},
  author =       {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8555--8564},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/scialom20a/scialom20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/scialom20a.html},
  abstract = 	 {We introduce a novel approach for sequence decoding, Discriminative Adversarial Search (DAS), which has the desirable properties of alleviating the effects of exposure bias without requiring external metrics. Inspired by Generative Adversarial Networks (GANs), wherein a discriminator is used to improve the generator, our method differs from GANs in that the generator parameters are not updated at training time and the discriminator is used to drive sequence generation at inference time. We investigate the effectiveness of the proposed approach on the task of Abstractive Summarization: the results obtained show that a naive application of DAS improves over the state-of-the-art methods, with further gains obtained via discriminator retraining. Moreover, we show how DAS can be effective for cross-domain adaptation. Finally, all results reported are obtained without additional rule-based filtering strategies, commonly used by the best performing systems available: this indicates that DAS can effectively be deployed without relying on post-hoc modifications of the generated outputs.}
}

@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@inproceedings{hutto2014vader,
  title={Vader: A parsimonious rule-based model for sentiment analysis of social media text},
  author={Hutto, Clayton and Gilbert, Eric},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={8},
  pages={216--225},
  year={2014}
}

@inproceedings{dhamala2021bold,
  title={{BOLD}: Dataset and metrics for measuring biases in open-ended language generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={862--872},
  year={2021}
}

@article{deberta,
  title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020},
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{mialon2023augmented,
  title={Augmented language models: a survey},
  author={Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  journal={arXiv preprint arXiv:2302.07842},
  year={2023}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{h4stackexchange,
  title={HuggingFace H4 Stack Exchange Preference Dataset},
  author={Lambert, Nathan and Tunstall, Lewis and Rajani, Nazneen and Thrush, Tristan},
  year={2023},
  url={https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences},
}

@inproceedings{SHP,
  title = 	 {Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information},
  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5988--6008},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher = {PMLR},
}

@inproceedings{nakano2021webgpt,
  author = {Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Lonbrown Ouyanbrown and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
  title = {WebGPT: Browser-assisted question-answering with human feedback},
  booktitle = {arXiv},
  year = 2021,
}

@inproceedings{stienon2020learning,
  author = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  title = {Learning to summarize from human feedback},
  booktitle = {NeurIPS},
  year = 2020,
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@Misc{FairScale2021,
  author =       {{FairScale authors}},
  title =        {FairScale:  A general purpose modular PyTorch library for high performance and large scale training},
  howpublished = {\url{https://github.com/facebookresearch/fairscale}},
  year =         {2021}
}

@Misc{xFormers2022,
  author =       {Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza},
  title =        {xFormers: A modular and hackable Transformer modelling library},
  howpublished = {\url{https://github.com/facebookresearch/xformers}},
  year =         {2022}
}

@inproceedings{nadeem2021stereoset,
  title={StereoSet: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5356--5371},
  year={2021}
}

@article{nangia2020crows,
  title={CrowS-pairs: A challenge dataset for measuring social biases in masked language models},
  author={Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2010.00133},
  year={2020}
}

@inproceedings{dinan2020queens,
  title={Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation},
  author={Dinan, Emily and Fan, Angela and Williams, Adina and Urbanek, Jack and Kiela, Douwe and Weston, Jason},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={8173--8188},
  year={2020}
}

@inproceedings{dinan2020multi,
  title={Multi-Dimensional Gender Bias Classification},
  author={Dinan, Emily and Fan, Angela and Wu, Ledell and Weston, Jason and Kiela, Douwe and Williams, Adina},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={314--331},
  year={2020}
}

@inproceedings{blodgett2021stereotyping,
  title={Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets},
  author={Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1004--1015},
  year={2021}
}

@article{barocas2017fairness,
  title={Fairness in machine learning},
  author={Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  journal={Nips tutorial},
  volume={1},
  pages={2017},
  year={2017}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@article{openai2023gpt4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/arXiv.2303.08774},
  eprinttype    = {arXiv},
  eprint       = {2303.08774},
  timestamp    = {Mon, 20 Mar 2023 15:23:19 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-08774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{mitchellModelCards,
  author       = {Margaret Mitchell and
                  Simone Wu and
                  Andrew Zaldivar and
                  Parker Barnes and
                  Lucy Vasserman and
                  Ben Hutchinson and
                  Elena Spitzer and
                  Inioluwa Deborah Raji and
                  Timnit Gebru},
  title        = {Model Cards for Model Reporting},
  journal      = {CoRR},
  volume       = {abs/1810.03993},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.03993},
  eprinttype    = {arXiv},
  eprint       = {1810.03993},
}

@article{glaese-etal-2022-improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and Campbell-Gillingham, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Sanchez Elias, Jaume and  Green, Richard and Mokr{\'{a}}, So{\u{n}}a and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and
 Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}

@article{askell-etal-2021-general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage,  Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{christiano-etal-2017-deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}

@inproceedings{blodgett-etal-2020-language,
    title = "Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {``}bias{''} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {``}bias{''} is an inherently normative process. We further find that these papers{'} proposed quantitative techniques for measuring or mitigating {``}bias{''} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {``}bias{''} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {``}bias{''}{---}i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
}

@article{levy2022safetext,
  title={SafeText: A Benchmark for Exploring Physical Safety in Language Models},
  author={Levy, Sharon and Allaway, Emily and Subbiah, Melanie and Chilton, Lydia and Patton, Desmond and McKeown, Kathleen and Wang, William Yang},
  journal={arXiv preprint arXiv:2210.10045},
  year={2022}
}

@inproceedings{lin2022truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3214--3252},
  year={2022}
}

@inproceedings{choi2018quac,
  title={QuAC: Question Answering in Context},
  author={Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2174--2184},
  year={2018}
}

@inproceedings{cao2022intrinsic,
  title={On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations},
  author={Cao, Yang and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul and Kumar, Varun and Dhamala, Jwala and Galstyan, Aram},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={561--570},
  year={2022}
}

@inproceedings{delobelle2022measuring,
  title={Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models},
  author={Delobelle, Pieter and Tokpo, Ewoenam and Calders, Toon and Berendt, Bettina},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1693--1706},
  year={2022}
}

@inproceedings{orgad2022choose,
  title={Choose Your Lenses: Flaws in Gender Bias Evaluation},
  author={Orgad, Hadas and Belinkov, Yonatan},
  booktitle={Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)},
  pages={151--167},
  year={2022}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3356--3369},
  year={2020}
}

@article{honovich2022unnatural,
  title={Unnatural instructions: Tuning language models with (almost) no human labor},
  author={Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  journal={arXiv preprint arXiv:2212.09689},
  year={2022}
}

@misc{synthetic_gptj,
  title={synthetic-instruct-gptj-pairwise},
  author={Alex Havrilla},
  howpublished = {\url{https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise}},
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{mollas2020ethos,
  title={Ethos: an online hate speech detection dataset},
  author={Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios},
  journal={arXiv preprint arXiv:2006.08328},
  year={2020}
}

@article{Hendrycks2020MeasuringMM,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Xiaodong Song and Jacob Steinhardt},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020},
}
@article{Chung2022ScalingIL,
  title={Scaling Instruction-Finetuned Language Models},
  author={Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph and Yi Tay and William Fedus and Eric Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Wei Yu and Vincent Zhao and Yanping Huang and Andrew M. Dai and Hongkun Yu and Slav Petrov and Ed Huai-hsin Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022},
}

@inproceedings{weifinetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{Khot2019QASCAD,
  title={QASC: A Dataset for Question Answering via Sentence Composition},
  author={Tushar Khot and Peter Clark and Michal Guerquin and Peter Alexander Jansen and Ashish Sabharwal},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019}
}

@inproceedings{de2018hate,
  title={Hate Speech Dataset from a White Supremacy Forum},
  author={de Gibert, Ona and P{\'e}rez, Naiara and Garc{\'\i}a-Pablos, Aitor and Cuadros, Montse},
  booktitle={Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)},
  pages={11--20},
  year={2018}
}

@inproceedings{elsherief2021latent,
  title={Latent Hatred: A Benchmark for Understanding Implicit Hate Speech},
  author={ElSherief, Mai and Ziems, Caleb and Muchlinski, David and Anupindi, Vaishnavi and Seybolt, Jordyn and De Choudhury, Munmun and Yang, Diyi},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={345--363},
  year={2021}
}

@inproceedings{sheng2019woman,
  title={The Woman Worked as a Babysitter: On Biases in Language Generation},
  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Prem and Peng, Nanyun},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3407--3412},
  year={2019}
}

@inproceedings{smith2022m,
  title={“I’m sorry to hear that”: Finding New Biases in Language Models with a Holistic Descriptor Dataset},
  author={Smith, Eric Michael and Hall, Melissa and Kambadur, Melanie and Presani, Eleonora and Williams, Adina},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={9180--9211},
  year={2022}
}

@inproceedings{rudinger2018gender,
  title={Gender Bias in Coreference Resolution},
  author={Rudinger, Rachel and Naradowsky, Jason and Leonard, Brian and Van Durme, Benjamin},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  pages={8--14},
  year={2018}
}

@inproceedings{zhao2018gender,
  title={Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  pages={15--20},
  year={2018}
}

@inproceedings{parrish2022bbq,
  title={BBQ: A hand-built bias benchmark for question answering},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={2086--2105},
  year={2022}
}

@article{yang2022unified,
  title={Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization},
  author={Yang, Zonghan and Yi, Xiaoyuan and Li, Peng and Liu, Yang and Xie, Xing},
  journal={arXiv preprint arXiv:2210.04492},
  year={2022}
}

@article{mukherjee2023orca,
  title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4},
  author={Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
  journal={arXiv preprint arXiv:2306.02707},
  year={2023}
}

@inproceedings{caselli2021hatebert,
  title={HateBERT: Retraining BERT for Abusive Language Detection in English},
  author={Caselli, Tommaso and Basile, Valerio and Mitrovi{\'c}, Jelena and Granitzer, Michael},
  booktitle={Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)},
  pages={17--25},
  year={2021}
}

%% Dataset contamination refs
@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{
flan,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gEZrGCozdqR}
}


@InProceedings{glam,
  title = 	 {{GL}a{M}: Efficient Scaling of Language Models with Mixture-of-Experts},
  author =       {Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten P and Zhou, Zongwei and Wang, Tao and Wang, Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5547--5569},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/du22c/du22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/du22c.html},
}


@misc{palm1,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@software{llm-eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@inproceedings{suffixarrays,
      title={Deduplicating Training Data Makes Language Models Better}, 
      author={Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison-Burch and Nicholas Carlini},
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
    year = "2022",
    publisher = "Association for Computational Linguistics"
}

@inproceedings{prabhakaran-etal-2019-perturbation,
    title = "Perturbation Sensitivity Analysis to Detect Unintended Model Biases",
    author = "Prabhakaran, Vinodkumar  and
      Hutchinson, Ben  and
      Mitchell, Margaret",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1578",
    doi = "10.18653/v1/D19-1578",
    pages = "5740--5745",
    abstract = "Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models {---} a sentiment model and a toxicity model {---} applied on online comments in English language from four different genres.",
}

@misc{MosaicML2023Introducing,
  title={Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},
  author={{MosaicML NLP Team} and others},
  year={2023},
  publisher={Accessed}
}


@article{DBLP:journals/corr/abs-1908-10084,
  author       = {Nils Reimers and
                  Iryna Gurevych},
  title        = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  journal      = {CoRR},
  volume       = {abs/1908.10084},
  year         = {2019},
  url          = {http://arxiv.org/abs/1908.10084},
  eprinttype    = {arXiv},
  eprint       = {1908.10084},
  timestamp    = {Thu, 26 Nov 2020 12:13:54 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1908-10084.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{anil2023palm,
      title={PaLM 2 Technical Report}, 
      author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and Eric Chu and Jonathan H. Clark and Laurent El Shafey and Yanping Huang and Kathy Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hernandez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan Botha and James Bradbury and Siddhartha Brahma and Kevin Brooks and Michele Catasta and Yong Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and Clément Crepy and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and Mark Díaz and Nan Du and Ethan Dyer and Vlad Feinberg and Fangxiaoyu Feng and Vlad Fienber and Markus Freitag and Xavier Garcia and Sebastian Gehrmann and Lucas Gonzalez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and Andrea Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wenhao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Music Li and Wei Li and YaGuang Li and Jian Li and Hyeontaek Lim and Hanzhao Lin and Zhongtao Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Alex Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alex Castro Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniel Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Kelvin Xu and Yunhan Xu and Linting Xue and Pengcheng Yin and Jiahui Yu and Qiao Zhang and Steven Zheng and Ce Zheng and Weikang Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},
      year={2023},
      eprint={2305.10403},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{henderson2022pile,
      title={Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset}, 
      author={Peter Henderson and Mark S. Krass and Lucia Zheng and Neel Guha and Christopher D. Manning and Dan Jurafsky and Daniel E. Ho},
      year={2022},
      eprint={2207.00220},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sennrich2016neural,
      title={Neural Machine Translation of Rare Words with Subword Units}, 
      author={Rico Sennrich and Barry Haddow and Alexandra Birch},
      year={2016},
      eprint={1508.07909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kudo2018sentencepiece,
      title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing}, 
      author={Taku Kudo and John Richardson},
      year={2018},
      eprint={1808.06226},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2019root,
      title={Root Mean Square Layer Normalization}, 
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{palminf,
    title={Efficiently Scaling Transformer Inference}, 
    author={Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},
    year={2022},
    eprint={2211.05102},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{tp2019,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and
                  Mostofa Patwary and
                  Raul Puri and
                  Patrick LeGresley and
                  Jared Casper and
                  Bryan Catanzaro},
      year={2019},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gqa2023,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shazeer2019mq,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{shazeer2020glu,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{su2022roformer,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2022},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ziems2022moral,
  title={The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems},
  author={Ziems, Caleb and Yu, Jane and Wang, Yi-Chia and Halevy, Alon and Yang, Diyi},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3755--3773},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{choi2023llms,
  title={Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark},
  author={Choi, Minje and Pei, Jiaxin and Kumar, Sagar and Shu, Chang and Jurgens, David},
  journal={arXiv preprint arXiv:2305.14938},
  year={2023}
}

@article{rao2023tricking,
  title={Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks},
  author={Rao, Abhinav and Vashistha, Sachin and Naik, Atharva and Aditya, Somak and Choudhury, Monojit},
  journal={arXiv preprint arXiv:2305.14965},
  year={2023}
}

@article{frantar2023sparsegpt,
  title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  year={2023}
}

@article{mckenna2023sources,
  title={Sources of Hallucination by Large Language Models on Inference Tasks},
  author={McKenna, Nick and Li, Tianyi and Cheng, Liang and Hosseini, Mohammad Javad and Johnson, Mark and Steedman, Mark},
  journal={arXiv preprint arXiv:2305.14552},
  year={2023}
}

@article{ranaldi2023trip,
  title={A Trip Towards Fairness: Bias and De-Biasing in Large Language Models},
  author={Ranaldi, Leonardo and Ruzzetti, Elena Sofia and Venditti, Davide and Onorati, Dario and Zanzotto, Fabio Massimo},
  journal={arXiv preprint arXiv:2305.13862},
  year={2023}
}

@article{shi2023trusting,
  title={Trusting Your Evidence: Hallucinate Less with Context-aware Decoding},
  author={Shi, Weijia and Han, Xiaochuang and Lewis, Mike and Tsvetkov, Yulia and Zettlemoyer, Luke and Yih, Scott Wen-tau},
  journal={arXiv preprint arXiv:2305.14739},
  year={2023}
}

@inproceedings{cao2022can,
  title={Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View},
  author={Cao, Boxi and Lin, Hongyu and Han, Xianpei and Liu, Fangchao and Sun, Le},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5796--5808},
  year={2022}
}

@article{deng2023recent,
  title={Recent advances towards safe, responsible, and moral dialogue systems: A survey},
  author={Deng, Jiawen and Sun, Hao and Zhang, Zhexin and Cheng, Jiale and Huang, Minlie},
  journal={arXiv preprint arXiv:2302.09270},
  year={2023}
}

@inproceedings{rottger2022multilingual,
  title={Multilingual HateCheck: Functional Tests for Multilingual Hate Speech Detection Models},
  author={R{\"o}ttger, Paul and Seelawi, Haitham and Nozza, Debora and Talat, Zeerak and Vidgen, Bertie},
  booktitle={Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)},
  pages={154--169},
  year={2022}
}

@inproceedings{rottger2021hatecheck,
  title={HateCheck: Functional Tests for Hate Speech Detection Models},
  author={R{\"o}ttger, Paul and Vidgen, Bertie and Nguyen, Dong and Waseem, Zeerak and Margetts, Helen and Pierrehumbert, Janet},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={41--58},
  year={2021}
}

@inproceedings{huang2020reducing,
  title={Reducing Sentiment Bias in Language Models via Counterfactual Evaluation},
  author={Huang, Po-Sen and Zhang, Huan and Jiang, Ray and Stanforth, Robert and Welbl, Johannes and Rae, Jack and Maini, Vishal and Yogatama, Dani and Kohli, Pushmeet},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={65--83},
  year={2020}
}

@article{kirk2021bias,
  title={Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models},
  author={Kirk, Hannah Rose and Jun, Yennie and Volpin, Filippo and Iqbal, Haider and Benussi, Elias and Dreyer, Frederic and Shtedritski, Aleksandar and Asano, Yuki},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={2611--2624},
  year={2021}
}

@inproceedings{sotnikova2021analyzing,
  title={Analyzing stereotypes in generative text inference tasks},
  author={Sotnikova, Anna and Cao, Yang Trista and Daum{\'e} III, Hal and Rudinger, Rachel},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={4052--4065},
  year={2021}
}

@inproceedings{venkit2023nationality,
  title={Nationality Bias in Text Generation},
  author={Venkit, Pranav Narayanan and Gautam, Sanjana and Panchanadikar, Ruchi and Huang, Ting-Hao and Wilson, Shomir},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={116--122},
  year={2023}
}

@article{kocielnik2023autobiastest,
  title={AutoBiasTest: Controllable Sentence Generation for Automated and Open-Ended Social Bias Testing in Language Models},
  author={Kocielnik, Rafal and Prabhumoye, Shrimai and Zhang, Vivian and Alvarez, R Michael and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2302.07371},
  year={2023}
}

@article{dinan2021anticipating,
  title={Anticipating safety issues in e2e conversational ai: Framework and tooling},
  author={Dinan, Emily and Abercrombie, Gavin and Bergman, A Stevie and Spruit, Shannon and Hovy, Dirk and Boureau, Y-Lan and Rieser, Verena},
  journal={arXiv preprint arXiv:2107.03451},
  year={2021}
}

@inproceedings{dinan2022safetykit,
  title={SafetyKit: First Aid for Measuring Safety in Open-domain Conversational Systems},
  author={Dinan, Emily and Abercrombie, Gavin and Bergman, A and Spruit, Shannon L and Hovy, Dirk and Boureau, Y-Lan and Rieser, Verena},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={4113--4133},
  year={2022}
}

@inproceedings{ung2022saferdialogues,
  title={SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures},
  author={Ung, Megan and Xu, Jing and Boureau, Y-Lan},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6462--6481},
  year={2022}
}

@article{suzgun2022prompt,
  title={Prompt-and-rerank: A method for zero-shot and few-shot arbitrary textual style transfer with small language models},
  author={Suzgun, Mirac and Melas-Kyriazi, Luke and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2205.11503},
  year={2022}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2018}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{ma2021dynaboard,
  title={Dynaboard: An evaluation-as-a-service platform for holistic next-generation benchmarking},
  author={Ma, Zhiyi and Ethayarajh, Kawin and Thrush, Tristan and Jain, Somya and Wu, Ledell and Jia, Robin and Potts, Christopher and Williams, Adina and Kiela, Douwe},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10351--10367},
  year={2021}
}

@article{burnell2023rethink,
  title={Rethink reporting of evaluation results in AI},
  author={Burnell, Ryan and Schellaert, Wout and Burden, John and Ullman, Tomer D and Martinez-Plumed, Fernando and Tenenbaum, Joshua B and Rutar, Danaja and Cheke, Lucy G and Sohl-Dickstein, Jascha and Mitchell, Melanie and others},
  journal={Science},
  volume={380},
  number={6641},
  pages={136--138},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{qian2022perturbation,
  title={Perturbation augmentation for fairer nlp},
  author={Qian, Rebecca and Ross, Candace and Fernandes, Jude and Smith, Eric and Kiela, Douwe and Williams, Adina},
  journal={arXiv preprint arXiv:2205.12586},
  year={2022}
}

@inproceedings{liang2020towards,
  title={Towards Debiasing Sentence Representations},
  author={Liang, Paul Pu and Li, Irene Mengze and Zheng, Emily and Lim, Yao Chong and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5502--5515},
  year={2020}
}

@inproceedings{ravfogel2020null,
  title={Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection},
  author={Ravfogel, Shauli and Elazar, Yanai and Gonen, Hila and Twiton, Michael and Goldberg, Yoav},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7237--7256},
  year={2020}
}

@inproceedings{liang2021towards,
  title={Towards understanding and mitigating social biases in language models},
  author={Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={6565--6576},
  year={2021},
  organization={PMLR}
}

@inproceedings{liu2021mitigating,
  title={Mitigating political bias in language models through reinforced calibration},
  author={Liu, Ruibo and Jia, Chenyan and Wei, Jason and Xu, Guangxuan and Wang, Lili and Vosoughi, Soroush},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={14857--14866},
  year={2021}
}

@article{basu2022equi,
  title={Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models},
  author={Basu, Sourya and Sattigeri, Prasanna and Ramamurthy, Karthikeyan Natesan and Chenthamarakshan, Vijil and Varshney, Kush R and Varshney, Lav R and Das, Payel},
  journal={arXiv preprint arXiv:2210.06475},
  year={2022}
}

@inproceedings{das2022quantifying,
  title={Quantifying Bias from Decoding Techniques in Natural Language Generation},
  author={Das, Mayukh and Balke, Wolf Tilo},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={1311--1323},
  year={2022}
}

@inproceedings{dhamala2023analysis,
  title={An analysis of the effects of decoding algorithms on fairness in open-ended language generation},
  author={Dhamala, Jwala and Kumar, Varun and Gupta, Rahul and Chang, Kai-Wei and Galstyan, Aram},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)},
  pages={655--662},
  year={2023},
  organization={IEEE}
}

@misc{lee2022rsc,
  title={Introducing the AI Research SuperCluster — Meta’s cutting-edge AI supercomputer for AI research},
  year=2022,
  url={https://ai.facebook.com/blog/ai-rsc/},
  author={Lee, Kevin and Sengupta, Shubho}
}

@article{shi2022life,
  title={When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels},
  author={Shi, Weiyan and Dinan, Emily and Shuster, Kurt and Weston, Jason and Xu, Jing},
  journal={arXiv preprint arXiv:2210.15893},
  year={2022}
}

@inproceedings{bergman2022guiding,
  title={Guiding the Release of Safer E2E Conversational AI through Value Sensitive Design},
  author={Bergman, A Stevie and Abercrombie, Gavin and Spruit, Shannon L and Hovy, Dirk and Dinan, Emily and Boureau, Y-Lan and Rieser, Verena},
  booktitle={Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  pages={39--52},
  year={2022}
}

@inproceedings{xu2021bot,
  title={Bot-Adversarial Dialogue for Safe Conversational Agents},
  author={Xu, Jing and Ju, Da and Li, Margaret and Boureau, Y-Lan and Weston, Jason and Dinan, Emily},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2950--2968},
  year={2021}
}

@article{mielke2020linguistic,
  title={Linguistic calibration through metacognition: aligning dialogue agent responses with expected correctness},
  author={Mielke, Sabrina J and Szlam, Arthur and Boureau, Y-Lan and Dinan, Emily},
  journal={arXiv e-prints},
  pages={arXiv--2012},
  year={2020}
}

@article{smith2020controlling,
  title={Controlling style in generated dialogue},
  author={Smith, Eric Michael and Gonzalez-Rico, Diana and Dinan, Emily and Boureau, Y-Lan},
  journal={arXiv preprint arXiv:2009.10855},
  year={2020}
}

@article{keskar2019ctrl,
  title={Ctrl: A conditional transformer language model for controllable generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@inproceedings{krause2021gedi,
  title={GeDi: Generative Discriminator Guided Sequence Generation},
  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={4929--4952},
  year={2021}
}

@article{dathathri2019plug,
  title={Plug and play language models: A simple approach to controlled text generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  journal={arXiv preprint arXiv:1912.02164},
  year={2019}
}

@article{roller2020open,
  title={Open-domain conversational agents: Current progress, open problems, and future directions},
  author={Roller, Stephen and Boureau, Y-Lan and Weston, Jason and Bordes, Antoine and Dinan, Emily and Fan, Angela and Gunning, David and Ju, Da and Li, Margaret and Poff, Spencer and others},
  journal={arXiv preprint arXiv:2006.12442},
  year={2020}
}

@inproceedings{roller2021recipes,
  title={Recipes for Building an Open-Domain Chatbot},
  author={Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Smith, Eric Michael and Boureau, Y-Lan and others},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={300--325},
  year={2021}
}

@article{smith2019zero,
  title={Zero-shot fine-grained style transfer: Leveraging distributed continuous style representations to transfer to unseen styles},
  author={Smith, Eric Michael and Gonzalez-Rico, Diana and Dinan, Emily and Boureau, Y-Lan},
  journal={arXiv preprint arXiv:1911.03914},
  year={2019}
}

@inproceedings{dinan2019build,
  title={Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack},
  author={Dinan, Emily and Humeau, Samuel and Chintagunta, Bharath and Weston, Jason},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4537--4546},
  year={2019}
}

@article{xu2023improving,
  title={Improving Open Language Models by Learning from Organic Interactions},
  author={Xu, Jing and Ju, Da and Lane, Joshua and Komeili, Mojtaba and Smith, Eric Michael and Ung, Megan and Behrooz, Morteza and Ngan, William and Moritz, Rashel and Sukhbaatar, Sainbayar and others},
  journal={arXiv preprint arXiv:2306.04707},
  year={2023}
}

@article{shuster2022blenderbot,
  title={Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage},
  author={Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and others},
  journal={arXiv preprint arXiv:2208.03188},
  year={2022}
}

@article{smith2021hi,
  title={Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models},
  author={Smith, Eric Michael and Williams, Adina},
  journal={arXiv preprint arXiv:2109.03300},
  year={2021}
}

@article{welleck2019neural,
  title={Neural text generation with unlikelihood training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  journal={arXiv preprint arXiv:1908.04319},
  year={2019}
}

@article{schick2021self,
  title={Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1408--1424},
  year={2021}
}

@inproceedings{rashkin2019towards,
  title={Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset},
  author={Rashkin, Hannah and Smith, Eric Michael and Li, Margaret and Boureau, Y-Lan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5370--5381},
  year={2019}
}

@inproceedings{smith2020can,
  title={Can You Put it All Together: Evaluating Conversational Agents’ Ability to Blend Skills},
  author={Smith, Eric Michael and Williamson, Mary and Shuster, Kurt and Weston, Jason and Boureau, Y-Lan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={2021--2030},
  year={2020}
}

@article{bailey2022based,
  title={Based on billions of words on the internet, people= men},
  author={Bailey, April H and Williams, Adina and Cimpian, Andrei},
  journal={Science Advances},
  volume={8},
  number={13},
  pages={eabm2463},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{xu2022learning,
  title={Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback},
  author={Xu, Jing and Ung, Megan and Komeili, Mojtaba and Arora, Kushal and Boureau, Y-Lan and Weston, Jason},
  journal={arXiv preprint arXiv:2208.03270},
  year={2022}
}

@article{ju2022learning,
  title={Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring the trolls},
  author={Ju, Da and Xu, Jing and Boureau, Y-Lan and Weston, Jason},
  journal={arXiv preprint arXiv:2208.03295},
  year={2022}
}

@inproceedings{hancock2019learning,
  title={Learning from Dialogue after Deployment: Feed Yourself, Chatbot!},
  author={Hancock, Braden and Bordes, Antoine and Mazare, Pierre-Emmanuel and Weston, Jason},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3667--3684},
  year={2019}
}

@inproceedings{li2020don,
  title={Don’t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training},
  author={Li, Margaret and Roller, Stephen and Kulikov, Ilia and Welleck, Sean and Boureau, Y-Lan and Cho, Kyunghyun and Weston, Jason},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4715--4728},
  year={2020}
}

@article{shuster2020deploying,
  title={Deploying lifelong open-domain dialogue learning},
  author={Shuster, Kurt and Urbanek, Jack and Dinan, Emily and Szlam, Arthur and Weston, Jason},
  journal={arXiv preprint arXiv:2008.08076},
  year={2020}
}

@article{solaiman2023evaluating,
  title={Evaluating the Social Impact of Generative AI Systems in Systems and Society},
  author={Solaiman, Irene and Talat, Zeerak and Agnew, William and Ahmad, Lama and Baker, Dylan and Blodgett, Su Lin and Daum{\'e} III, Hal and Dodge, Jesse and Evans, Ellie and Hooker, Sara and others},
  journal={arXiv preprint arXiv:2306.05949},
  year={2023}
}

@article{kumar2022language,
  title={Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey},
  author={Kumar, Sachin and Balachandran, Vidhisha and Njoo, Lucille and Anastasopoulos, Antonios and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2210.07700},
  year={2022}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{zhuo2023exploring,
  title={Exploring ai ethics of chatgpt: A diagnostic analysis},
  author={Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  journal={arXiv preprint arXiv:2301.12867},
  year={2023}
}

@inproceedings{ganesh2023impact,
  title={On The Impact of Machine Learning Randomness on Group Fairness},
  author={Ganesh, Prakhar and Chang, Hongyan and Strobel, Martin and Shokri, Reza},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1789--1800},
  year={2023}
}


@article{bojanowski2016fasttext,
  author       = {Piotr Bojanowski and
                  Edouard Grave and
                  Armand Joulin and
                  Tom{\'{a}}s Mikolov},
  title        = {Enriching Word Vectors with Subword Information},
  journal      = {CoRR},
  volume       = {abs/1607.04606},
  year         = {2016},
  url          = {http://arxiv.org/abs/1607.04606},
  eprinttype    = {arXiv},
  eprint       = {1607.04606},
  timestamp    = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/BojanowskiGJM16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{shaham-etal-2022-scrolls,
title = "{SCROLLS}: Standardized {C}ompa{R}ison Over Long Language Sequences",
author = "Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and Levy, Omer",
booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
month = dec,
year = "2022",
address = "Abu Dhabi, United Arab Emirates",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2022.emnlp-main.823",
pages = "12007--12021"
}

@article{casper2023explore,
  title={Explore, Establish, Exploit: Red Teaming Language Models from Scratch},
  author={Casper, Stephen and Lin, Jason and Kwon, Joe and Culp, Gatlen and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2306.09442},
  year={2023}
}

@inproceedings{perez2022red,
  title={Red Teaming Language Models with Language Models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3419--3448},
  year={2022}
}

@article{korbak2023pretraining,
  title={Pretraining language models with human preferences},
  author={Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika and Buckley, Christopher L and Phang, Jason and Bowman, Samuel R and Perez, Ethan},
  journal={arXiv preprint arXiv:2302.08582},
  year={2023}
}

@article{solaiman2021process,
  title={Process for adapting language models to society (palms) with values-targeted datasets},
  author={Solaiman, Irene and Dennison, Christy},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5861--5873},
  year={2021}
}

@incollection{acemoglu2018artificial,
  title={Artificial intelligence, automation, and work},
  author={Acemoglu, Daron and Restrepo, Pascual},
  booktitle={The economics of artificial intelligence: An agenda},
  pages={197--236},
  year={2018},
  publisher={University of Chicago Press}
}

@techreport{autor2018automation,
  title={Is automation labor-displacing? Productivity growth, employment, and the labor share},
  author={Autor, David and Salomons, Anna},
  year={2018},
  institution={National Bureau of Economic Research}
}

@article{webb2019impact,
  title={The impact of artificial intelligence on the labor market},
  author={Webb, Michael},
  journal={Available at SSRN 3482150},
  year={2019}
}

@article{shumailov2023curse,
  title={The Curse of Recursion: Training on Generated Data Makes Models Forget},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  journal={arXiv preprint arxiv:2305.17493},
  year={2023}
}

@article{taylor2022galactica,
  title={Galactica: A large language model for science},
  author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  journal={arXiv preprint arXiv:2211.09085},
  year={2022}
}

@inproceedings{shuster-etal-2021-retrieval-augmentation,
    title = "Retrieval Augmentation Reduces Hallucination in Conversation",
    author = "Shuster, Kurt  and
      Poff, Spencer  and
      Chen, Moya  and
      Kiela, Douwe  and
      Weston, Jason",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.320",
    doi = "10.18653/v1/2021.findings-emnlp.320",
    pages = "3784--3803",
}

@inproceedings{si2023prompting,
    title={Prompting {GPT}-3 To Be Reliable},
    author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan Lee Boyd-Graber and Lijuan Wang},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=98p5x51L5af}
}

@inproceedings{smith2022human,
  title={Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents},
  author={Smith, Eric and Hsu, Orion and Qian, Rebecca and Roller, Stephen and Boureau, Y-Lan and Weston, Jason},
  booktitle={Proceedings of the 4th Workshop on NLP for Conversational AI},
  pages={77--97},
  year={2022}
}


@InProceedings{pmlr-v162-borgeaud22a,
  title = 	 {Improving Language Models by Retrieving from Trillions of Tokens},
  author =       {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2206--2240},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/borgeaud22a.html},
}

@inproceedings{liu2016not,
  title={How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation},
  author={Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian Vlad and Noseworthy, Mike and Charlin, Laurent and Pineau, Joelle},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2122--2132},
  year={2016}
}

@misc{menick2022teaching,
      title={Teaching language models to support answers with verified quotes}, 
      author={Jacob Menick and Maja Trebacz and Vladimir Mikulik and John Aslanides and Francis Song and Martin Chadwick and Mia Glaese and Susannah Young and Lucy Campbell-Gillingham and Geoffrey Irving and Nat McAleese},
      year={2022},
      eprint={2203.11147},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bhatt2022recontextualizing,
      title={Re-contextualizing Fairness in NLP: The Case of India}, 
      author={Shaily Bhatt and Sunipa Dev and Partha Talukdar and Shachi Dave and Vinodkumar Prabhakaran},
      year={2022},
      eprint={2209.12226},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{he2022rethinking,
  title={Rethinking with retrieval: Faithful large language model inference},
  author={He, Hangfeng and Zhang, Hongming and Roth, Dan},
  journal={arXiv preprint arXiv:2301.00303},
  year={2022}
}

@misc{gao2023rarr,
      title={RARR: Researching and Revising What Language Models Say, Using Language Models}, 
      author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Y. Zhao and Ni Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
      year={2023},
      eprint={2210.08726},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{dziri-etal-2021-neural,
    title = "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
    author = {Dziri, Nouha  and
      Madotto, Andrea  and
      Za{\"\i}ane, Osmar  and
      Bose, Avishek Joey},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.168",
    doi = "10.18653/v1/2021.emnlp-main.168",
    pages = "2197--2214",
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{lee2022factuality,
  title={Factuality enhanced language models for open-ended text generation},
  author={Lee, Nayeon and Ping, Wei and Xu, Peng and Patwary, Mostofa and Fung, Pascale N and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34586--34599},
  year={2022}
}

@misc{du2023improving,
      title={Improving Factuality and Reasoning in Language Models through Multiagent Debate}, 
      author={Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
      year={2023},
      eprint={2305.14325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{dinan2020second,
  title={The second conversational intelligence challenge (convai2)},
  author={Dinan, Emily and Logacheva, Varvara and Malykh, Valentin and Miller, Alexander and Shuster, Kurt and Urbanek, Jack and Kiela, Douwe and Szlam, Arthur and Serban, Iulian and Lowe, Ryan and others},
  booktitle={The NeurIPS'18 Competition: From Machine Learning to Intelligent Conversations},
  pages={187--208},
  year={2020},
  organization={Springer}
}

@article{deriu2021survey,
  title={Survey on evaluation methods for dialogue systems},
  author={Deriu, Jan and Rodrigo, Alvaro and Otegi, Arantxa and Echegoyen, Guillermo and Rosset, Sophie and Agirre, Eneko and Cieliebak, Mark},
  journal={Artificial Intelligence Review},
  volume={54},
  pages={755--810},
  year={2021},
  publisher={Springer}
}

@inproceedings{Carlini2020ExtractingTD,
  title={Extracting Training Data from Large Language Models},
  author={Nicholas Carlini and Florian Tram{\`e}r and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom B. Brown and Dawn Xiaodong Song and {\'U}lfar Erlingsson and Alina Oprea and Colin Raffel},
  booktitle={USENIX Security Symposium},
  year={2020}
}

@article{Carlini2022QuantifyingMA,
  title={Quantifying Memorization Across Neural Language Models},
  author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tram{\`e}r and Chiyuan Zhang},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.07646}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{bender-et-al-2021,
  title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author = {Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Mitchell, Margaret},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {610--623},
  year = {2021}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{wu2022sustainable,
  title={Sustainable ai: Environmental implications, challenges and opportunities},
  author={Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Aga, Fiona and Huang, Jinshi and Bai, Charles and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={795--813},
  year={2022}
}

@article{dodge2022measuring,
  title={Measuring the Carbon Intensity of AI in Cloud Instances},
  author={Dodge, Jesse and Prewitt, Taylor and Combes, Remi Tachet Des and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A and DeCario, Nicole and Buchanan, Will},
  journal={arXiv preprint arXiv:2206.05229},
  year={2022}
}

@article{gupta2022chasing,
  title={Chasing carbon: The elusive environmental footprint of computing},
  author={Gupta, Udit and Kim, Young Guen and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin Sean and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},
  journal={IEEE Micro},
  year={2022},
  publisher={IEEE}
}

@inproceedings{gupta2022act,
  title={ACT: designing sustainable computer systems with an architectural carbon modeling tool},
  author={Gupta, Udit and Elgamal, Mariam and Hills, Gage and Wei, Gu-Yeon and Lee, Hsien-Hsin S and Brooks, David and Wu, Carole-Jean},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={784--799},
  year={2022}
}

@article{rajpurkar2018know,
  title={Know what you don't know: Unanswerable questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1806.03822},
  year={2018}
}

@misc{zhao2023fsdp,
      title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, 
      author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Shen Li},
      year={2023},
      eprint={2304.11277},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{penedo2023refinedweb,
      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, 
      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
      year={2023},
      eprint={2306.01116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{welbl2021challenges,
      title={Challenges in Detoxifying Language Models}, 
      author={Johannes Welbl and Amelia Glaese and Jonathan Uesato and Sumanth Dathathri and John Mellor and Lisa Anne Hendricks and Kirsty Anderson and Pushmeet Kohli and Ben Coppin and Po-Sen Huang},
      year={2021},
      eprint={2109.07445},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2021recipes,
      title={Recipes for Safety in Open-domain Chatbots}, 
      author={Jing Xu and Da Ju and Margaret Li and Y-Lan Boureau and Jason Weston and Emily Dinan},
      year={2021},
      eprint={2010.07079},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhou-etal-2023-lima,
  title={LIMA: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@misc{chen2021Evaluating,
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  title = {Evaluating Large Language Models Trained on Code},
  year = {2021},
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@misc{austin2021program,
  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  title = {Program Synthesis with Large Language Models},
  year = {2021},
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  pages={7432--7439},
  year={2020}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}

@article{zellers2019hellaswag,
  title={HellaSwag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{suzgun2022challenging,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{zhong2023agieval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}

@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{zhou2022large,
  title={Large Language Models are Human-Level Prompt Engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{ganguli2023capacity,
  title={The capacity for moral self-correction in large language models},
  author={Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and others},
  journal={arXiv preprint arXiv:2302.07459},
  year={2023}
}

@article{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@inproceedings{kumar2016ask,
  title={Ask me anything: Dynamic memory networks for natural language processing},
  author={Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  booktitle={International conference on machine learning},
  pages={1378--1387},
  year={2016},
  organization={PMLR}
}

@article{mccann2018natural,
  title={The natural language decathlon: Multitask learning as question answering},
  author={McCann, Bryan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1806.08730},
  year={2018}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{nguyen-etal-2019-toward,
  title={Toward understanding catastrophic forgetting in continual learning},
  author={Nguyen, Cuong V. and Achille, Alessandro and Lam, Michael and Hassner, Tal and Mahadevan, Vijay and Soatto, Stefano},
  journal={arXiv preprint arXiv:1908.01091},
  year={2019}
}

@article{kirkpatrick-etal-2017-overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Academy of Sciences}
}

@inproceedings{ramasesh-etal-2021-effect,
  title={Effect of scale on catastrophic forgetting in neural networks},
  author={Ramasesh, Vinay Venkatesh and Lewkowycz, Aitor and Dyer, Ethan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{clark-etal-2021-thats,
    title = "All That{'}s {`}Human{'} Is Not Gold: Evaluating Human Evaluation of Generated Text",
    author = "Clark, Elizabeth  and
      August, Tal  and
      Serrano, Sofia  and
      Haduong, Nikita  and
      Gururangan, Suchin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.565",
    doi = "10.18653/v1/2021.acl-long.565",
    pages = "7282--7296",
}

@article{gehrmann-etal-2023-repairing,
  title={Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text},
  author={Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault},
  journal={Journal of Artificial Intelligence Research},
  volume={77},
  pages={103--166},
  year={2023}
}

@book{gwet-2014-handbook,
  title={Handbook of inter-rater reliability: The definitive guide to measuring the extent of agreement among raters},
  author={Gwet, Kilem L.},
  year={2014},
  publisher={Advanced Analytics, LLC}
}

@article{gwet-2008-computing,
  title={Computing inter-rater reliability and its variance in the presence of high agreement},
  author={Gwet, Kilem Li},
  journal={British Journal of Mathematical and Statistical Psychology},
  volume={61},
  number={1},
  pages={29--48},
  year={2008},
  publisher={Wiley Online Library}
}

@inproceedings{tal-etal-2022-fewer,
    title = "Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",
    author = "Tal, Yarden  and
      Magar, Inbal  and
      Schwartz, Roy",
    booktitle = "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gebnlp-1.13",
    doi = "10.18653/v1/2022.gebnlp-1.13",
    pages = "112--120"
}

@inproceedings{dodge-etal-2021-documenting,
    title = "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
    author = "Dodge, Jesse  and
      Sap, Maarten  and
      Marasovi{\'c}, Ana  and
      Agnew, William  and
      Ilharco, Gabriel  and
      Groeneveld, Dirk  and
      Mitchell, Margaret  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.98",
    doi = "10.18653/v1/2021.emnlp-main.98",
    pages = "1286--1305"
}