\begin{thebibliography}{135}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acemoglu and Restrepo(2018)]{acemoglu2018artificial}
Daron Acemoglu and Pascual Restrepo.
\newblock Artificial intelligence, automation, and work.
\newblock In \emph{The economics of artificial intelligence: An agenda}, pages
  197--236. University of Chicago Press, 2018.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebrón,
  and Sanghai]{gqa2023}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico
  Lebrón, and Sumit Sanghai.
\newblock Gqa: Training generalized multi-query transformer models from
  multi-head checkpoints, 2023.

\bibitem[Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli,
  Cojocaru, Debbah, Goffinet, Heslow, Launay, Malartic, Noune, Pannier, and
  Penedo]{falcon40b}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
  Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien
  Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme
  Penedo.
\newblock {Falcon-40B}: an open large language model with state-of-the-art
  performance.
\newblock 2023.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,
  Taropa, Bailey, Chen, Chu, Clark, Shafey, Huang, Meier-Hellstern, Mishra,
  Moreira, Omernick, Robinson, Ruder, Tay, Xiao, Xu, Zhang, Abrego, Ahn,
  Austin, Barham, Botha, Bradbury, Brahma, Brooks, Catasta, Cheng, Cherry,
  Choquette-Choo, Chowdhery, Crepy, Dave, Dehghani, Dev, Devlin, Díaz, Du,
  Dyer, Feinberg, Feng, Fienber, Freitag, Garcia, Gehrmann, Gonzalez, Gur-Ari,
  Hand, Hashemi, Hou, Howland, Hu, Hui, Hurwitz, Isard, Ittycheriah, Jagielski,
  Jia, Kenealy, Krikun, Kudugunta, Lan, Lee, Lee, Li, Li, Li, Li, Li, Lim, Lin,
  Liu, Liu, Maggioni, Mahendru, Maynez, Misra, Moussalem, Nado, Nham, Ni,
  Nystrom, Parrish, Pellat, Polacek, Polozov, Pope, Qiao, Reif, Richter, Riley,
  Ros, Roy, Saeta, Samuel, Shelby, Slone, Smilkov, So, Sohn, Tokumine, Valter,
  Vasudevan, Vodrahalli, Wang, Wang, Wang, Wang, Wieting, Wu, Xu, Xu, Xue, Yin,
  Yu, Zhang, Zheng, Zheng, Zhou, Zhou, Petrov, and Wu]{anil2023palm}
Rohan Anil, Andrew~M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
  Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
  Eric Chu, Jonathan~H. Clark, Laurent~El Shafey, Yanping Huang, Kathy
  Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,
  Sebastian Ruder, Yi~Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
  Gustavo~Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,
  James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng,
  Colin Cherry, Christopher~A. Choquette-Choo, Aakanksha Chowdhery, Clément
  Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
  Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus
  Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,
  Steven Hand, Hadi Hashemi, Le~Hou, Joshua Howland, Andrea Hu, Jeffrey Hui,
  Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
  Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine
  Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek
  Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma
  Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John
  Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek,
  Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker
  Riley, Alex~Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee
  Shelby, Ambrose Slone, Daniel Smilkov, David~R. So, Daniel Sohn, Simon
  Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,
  Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan
  Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
  Ce~Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.
\newblock Palm 2 technical report, 2023.

\bibitem[Askell et~al.(2021{\natexlab{a}})Askell, Bai, Chen, Drain, Ganguli,
  Henighan, Jones, Joseph, Mann, DasSarma, Elhage, Hatfield-Dodds, Hernandez,
  Kernion, Ndousse, Olsson, Amodei, Brown, Clark, McCandlish, and
  Olah]{askell-etal-2021-general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
  Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac
  Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
  Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, and Chris Olah.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021{\natexlab{a}}.

\bibitem[Askell et~al.(2021{\natexlab{b}})Askell, Bai, Chen, Drain, Ganguli,
  Henighan, Jones, Joseph, Mann, DasSarma, et~al.]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
  Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021{\natexlab{b}}.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, and Sutton]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles
  Sutton.
\newblock Program synthesis with large language models, 2021.

\bibitem[Autor and Salomons(2018)]{autor2018automation}
David Autor and Anna Salomons.
\newblock Is automation labor-displacing? productivity growth, employment, and
  the labor share.
\newblock Technical report, National Bureau of Economic Research, 2018.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,
  DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,
  Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Bailey et~al.(2022)Bailey, Williams, and Cimpian]{bailey2022based}
April~H Bailey, Adina Williams, and Andrei Cimpian.
\newblock Based on billions of words on the internet, people= men.
\newblock \emph{Science Advances}, 8\penalty0 (13):\penalty0 eabm2463, 2022.

\bibitem[Bender et~al.(2021{\natexlab{a}})Bender, Gebru, McMillan-Major, and
  Mitchell]{bender-et-al-2021}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 610--623, 2021{\natexlab{a}}.

\bibitem[Bender et~al.(2021{\natexlab{b}})Bender, Gebru, McMillan-Major, and
  Shmitchell]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, pages 610--623, 2021{\natexlab{b}}.

\bibitem[Bergman et~al.(2022)Bergman, Abercrombie, Spruit, Hovy, Dinan,
  Boureau, and Rieser]{bergman2022guiding}
A~Stevie Bergman, Gavin Abercrombie, Shannon~L Spruit, Dirk Hovy, Emily Dinan,
  Y-Lan Boureau, and Verena Rieser.
\newblock Guiding the release of safer e2e conversational ai through value
  sensitive design.
\newblock In \emph{Proceedings of the 23rd Annual Meeting of the Special
  Interest Group on Discourse and Dialogue}, pages 39--52, 2022.

\bibitem[Bhatt et~al.(2022)Bhatt, Dev, Talukdar, Dave, and
  Prabhakaran]{bhatt2022recontextualizing}
Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, and Vinodkumar
  Prabhakaran.
\newblock Re-contextualizing fairness in nlp: The case of india, 2022.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, pages 7432--7439, 2020.

\bibitem[Blodgett et~al.(2021)Blodgett, Lopez, Olteanu, Sim, and
  Wallach]{blodgett2021stereotyping}
Su~Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna
  Wallach.
\newblock Stereotyping norwegian salmon: An inventory of pitfalls in fairness
  benchmark datasets.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 1004--1015, 2021.

\bibitem[Bojanowski et~al.(2016)Bojanowski, Grave, Joulin, and
  Mikolov]{bojanowski2016fasttext}
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tom{\'{a}}s Mikolov.
\newblock Enriching word vectors with subword information.
\newblock \emph{CoRR}, abs/1607.04606, 2016.
\newblock URL \url{http://arxiv.org/abs/1607.04606}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin,
  Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such,
  Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino,
  Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam,
  Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew,
  Amodei, McCandlish, Sutskever, and Zaremba]{chen2021Evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy
  Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
  Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens
  Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias
  Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,
  William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
  Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
  Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
  Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
  Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
  Wojciech Zaremba.
\newblock Evaluating large language models trained on code, 2021.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Choi et~al.(2018)Choi, He, Iyyer, Yatskar, Yih, Choi, Liang, and
  Zettlemoyer]{choi2018quac}
Eunsol Choi, He~He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy
  Liang, and Luke Zettlemoyer.
\newblock Quac: Question answering in context.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2174--2184, 2018.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{palm1}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano-etal-2017-deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Valter, Narang,
  Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, hsin Chi, Dean, Devlin, Roberts,
  Zhou, Le, and Wei]{Chung2022ScalingIL}
Hyung~Won Chung, Le~Hou, S.~Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric
  Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
  Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
  Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams~Wei Yu, Vincent
  Zhao, Yanping Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~Huai hsin
  Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason
  Wei.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Clark et~al.(2021)Clark, August, Serrano, Haduong, Gururangan, and
  Smith]{clark-etal-2021-thats}
Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan,
  and Noah~A. Smith.
\newblock All that{'}s {`}human{'} is not gold: Evaluating human evaluation of
  generated text.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 7282--7296,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.565}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.565}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Deng et~al.(2023)Deng, Sun, Zhang, Cheng, and Huang]{deng2023recent}
Jiawen Deng, Hao Sun, Zhexin Zhang, Jiale Cheng, and Minlie Huang.
\newblock Recent advances towards safe, responsible, and moral dialogue
  systems: A survey.
\newblock \emph{arXiv preprint arXiv:2302.09270}, 2023.

\bibitem[Deng et~al.(2019)Deng, Bakhtin, Ott, Szlam, and
  Ranzato]{deng2019residual}
Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato.
\newblock Residual energy-based models for text generation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Dhamala et~al.(2021)Dhamala, Sun, Kumar, Krishna, Pruksachatkun,
  Chang, and Gupta]{dhamala2021bold}
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun,
  Kai-Wei Chang, and Rahul Gupta.
\newblock {BOLD}: Dataset and metrics for measuring biases in open-ended
  language generation.
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, pages 862--872, 2021.

\bibitem[Dinan et~al.(2021)Dinan, Abercrombie, Bergman, Spruit, Hovy, Boureau,
  and Rieser]{dinan2021anticipating}
Emily Dinan, Gavin Abercrombie, A~Stevie Bergman, Shannon Spruit, Dirk Hovy,
  Y-Lan Boureau, and Verena Rieser.
\newblock Anticipating safety issues in e2e conversational ai: Framework and
  tooling.
\newblock \emph{arXiv preprint arXiv:2107.03451}, 2021.

\bibitem[Dodge et~al.(2021)Dodge, Sap, Marasovi{\'c}, Agnew, Ilharco,
  Groeneveld, Mitchell, and Gardner]{dodge-etal-2021-documenting}
Jesse Dodge, Maarten Sap, Ana Marasovi{\'c}, William Agnew, Gabriel Ilharco,
  Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
\newblock Documenting large webtext corpora: A case study on the colossal clean
  crawled corpus.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 1286--1305, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.98}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.98}.

\bibitem[Dodge et~al.(2022)Dodge, Prewitt, Combes, Odmark, Schwartz, Strubell,
  Luccioni, Smith, DeCario, and Buchanan]{dodge2022measuring}
Jesse Dodge, Taylor Prewitt, Remi Tachet~Des Combes, Erika Odmark, Roy
  Schwartz, Emma Strubell, Alexandra~Sasha Luccioni, Noah~A Smith, Nicole
  DeCario, and Will Buchanan.
\newblock Measuring the carbon intensity of ai in cloud instances.
\newblock \emph{arXiv preprint arXiv:2206.05229}, 2022.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson,
  Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
  Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus,
  Maarten~P Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie
  Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun
  Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock {GL}a{M}: Efficient scaling of language models with
  mixture-of-experts.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 5547--5569. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/du22c.html}.

\bibitem[Ethayarajh et~al.(2022)Ethayarajh, Choi, and Swayamdipta]{SHP}
Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta.
\newblock Understanding dataset difficulty with $\mathcal{V}$-usable
  information.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 5988--6008. PMLR,
  17--23 Jul 2022.

\bibitem[Ganesh et~al.(2023)Ganesh, Chang, Strobel, and
  Shokri]{ganesh2023impact}
Prakhar Ganesh, Hongyan Chang, Martin Strobel, and Reza Shokri.
\newblock On the impact of machine learning randomness on group fairness.
\newblock In \emph{Proceedings of the 2023 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 1789--1800, 2023.

\bibitem[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath,
  Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red}
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav
  Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al.
\newblock Red teaming language models to reduce harms: Methods, scaling
  behaviors, and lessons learned.
\newblock \emph{arXiv preprint arXiv:2209.07858}, 2022.

\bibitem[Ganguli et~al.(2023)Ganguli, Askell, Schiefer, Liao,
  Luko{\v{s}}i{\=u}t{\.e}, Chen, Goldie, Mirhoseini, Olsson, Hernandez,
  et~al.]{ganguli2023capacity}
Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil{\.e}
  Luko{\v{s}}i{\=u}t{\.e}, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine
  Olsson, Danny Hernandez, et~al.
\newblock The capacity for moral self-correction in large language models.
\newblock \emph{arXiv preprint arXiv:2302.07459}, 2023.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and
  Zou]{llm-eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
  and Andy Zou.
\newblock A framework for few-shot language model evaluation, September 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Gehrmann et~al.(2023)Gehrmann, Clark, and
  Sellam]{gehrmann-etal-2023-repairing}
Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam.
\newblock Repairing the cracked foundation: A survey of obstacles in evaluation
  practices for generated text.
\newblock \emph{Journal of Artificial Intelligence Research}, 77:\penalty0
  103--166, 2023.

\bibitem[Gilardi et~al.(2023)Gilardi, Alizadeh, and Kubli]{gilardi2023chatgpt}
Fabrizio Gilardi, Meysam Alizadeh, and Ma{\"e}l Kubli.
\newblock Chatgpt outperforms crowd-workers for text-annotation tasks.
\newblock \emph{arXiv preprint arXiv:2303.15056}, 2023.

\bibitem[Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel,
  Levine, and Song]{gudibande2023false}
Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter
  Abbeel, Sergey Levine, and Dawn Song.
\newblock The false promise of imitating proprietary llms.
\newblock \emph{arXiv preprint arXiv:2305.15717}, 2023.

\bibitem[Gupta et~al.(2022{\natexlab{a}})Gupta, Elgamal, Hills, Wei, Lee,
  Brooks, and Wu]{gupta2022act}
Udit Gupta, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin~S Lee, David
  Brooks, and Carole-Jean Wu.
\newblock Act: designing sustainable computer systems with an architectural
  carbon modeling tool.
\newblock In \emph{Proceedings of the 49th Annual International Symposium on
  Computer Architecture}, pages 784--799, 2022{\natexlab{a}}.

\bibitem[Gupta et~al.(2022{\natexlab{b}})Gupta, Kim, Lee, Tse, Lee, Wei,
  Brooks, and Wu]{gupta2022chasing}
Udit Gupta, Young~Guen Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin~Sean Lee,
  Gu-Yeon Wei, David Brooks, and Carole-Jean Wu.
\newblock Chasing carbon: The elusive environmental footprint of computing.
\newblock \emph{IEEE Micro}, 2022{\natexlab{b}}.

\bibitem[Gwet(2014)]{gwet-2014-handbook}
Kilem~L. Gwet.
\newblock \emph{Handbook of inter-rater reliability: The definitive guide to
  measuring the extent of agreement among raters}.
\newblock Advanced Analytics, LLC, 2014.

\bibitem[Gwet(2008)]{gwet-2008-computing}
Kilem~Li Gwet.
\newblock Computing inter-rater reliability and its variance in the presence of
  high agreement.
\newblock \emph{British Journal of Mathematical and Statistical Psychology},
  61\penalty0 (1):\penalty0 29--48, 2008.

\bibitem[Hartvigsen et~al.(2022)Hartvigsen, Gabriel, Palangi, Sap, Ray, and
  Kamar]{hartvigsen2022toxigen}
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
  and Ece Kamar.
\newblock Toxigen: A large-scale machine-generated dataset for adversarial and
  implicit hate speech detection.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 3309--3326,
  2022.

\bibitem[Havrilla()]{synthetic_gptj}
Alex Havrilla.
\newblock synthetic-instruct-gptj-pairwise.
\newblock
  \url{https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise}.

\bibitem[He et~al.(2020)He, Liu, Gao, and Chen]{deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock \emph{arXiv preprint arXiv:2006.03654}, 2020.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{Hendrycks2020MeasuringMM}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
  Dawn~Xiaodong Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang,
  Song, and Steinhardt]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
  Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{arXiv preprint arXiv:2103.03874}, 2021.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{nucleus_sampling}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[Honovich et~al.(2022)Honovich, Scialom, Levy, and
  Schick]{honovich2022unnatural}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
\newblock Unnatural instructions: Tuning language models with (almost) no human
  labor.
\newblock \emph{arXiv preprint arXiv:2212.09689}, 2022.

\bibitem[Hosseini et~al.(2023)Hosseini, Palangi, and
  Awadallah]{hosseini2023empirical}
Saghar Hosseini, Hamid Palangi, and Ahmed~Hassan Awadallah.
\newblock An empirical study of metrics to measure representational harms in
  pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2301.09211}, 2023.

\bibitem[Huang et~al.(2023)Huang, Kwak, and An]{huang2023chatgpt}
Fan Huang, Haewoon Kwak, and Jisun An.
\newblock Is chatgpt better than human annotators? potential and limitations of
  chatgpt in explaining implicit hate speech.
\newblock \emph{arXiv preprint arXiv:2302.07736}, 2023.

\bibitem[Hutto and Gilbert(2014)]{hutto2014vader}
Clayton Hutto and Eric Gilbert.
\newblock Vader: A parsimonious rule-based model for sentiment analysis of
  social media text.
\newblock In \emph{Proceedings of the international AAAI conference on web and
  social media}, volume~8, pages 216--225, 2014.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock \emph{arXiv preprint arXiv:1705.03551}, 2017.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick-etal-2017-overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[K{\"o}pf et~al.(2023)K{\"o}pf, Kilcher, von R{\"u}tte, Anagnostidis,
  Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi, et~al.]{kopf2023openassistant}
Andreas K{\"o}pf, Yannic Kilcher, Dimitri von R{\"u}tte, Sotiris Anagnostidis,
  Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver
  Stanley, Rich{\'a}rd Nagyfi, et~al.
\newblock Openassistant conversations--democratizing large language model
  alignment.
\newblock \emph{arXiv preprint arXiv:2304.07327}, 2023.

\bibitem[Korbak et~al.(2023)Korbak, Shi, Chen, Bhalerao, Buckley, Phang,
  Bowman, and Perez]{korbak2023pretraining}
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher~L
  Buckley, Jason Phang, Samuel~R Bowman, and Ethan Perez.
\newblock Pretraining language models with human preferences.
\newblock \emph{arXiv preprint arXiv:2302.08582}, 2023.

\bibitem[Kudo and Richardson(2018)]{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing, 2018.

\bibitem[Kumar et~al.(2022)Kumar, Balachandran, Njoo, Anastasopoulos, and
  Tsvetkov]{kumar2022language}
Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, and
  Yulia Tsvetkov.
\newblock Language generation models can cause harm: So what can we do about
  it? an actionable survey.
\newblock \emph{arXiv preprint arXiv:2210.07700}, 2022.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins,
  Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee,
  et~al.]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
  Kenton Lee, et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 453--466, 2019.

\bibitem[Lambert et~al.(2023)Lambert, Tunstall, Rajani, and
  Thrush]{h4stackexchange}
Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush.
\newblock Huggingface h4 stack exchange preference dataset.
\newblock 2023.
\newblock URL
  \url{https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences}.

\bibitem[Lee et~al.(2022)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch,
  and Carlini]{suffixarrays}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
  Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics}. Association for Computational Linguistics,
  2022.

\bibitem[Lee and Sengupta(2022)]{lee2022rsc}
Kevin Lee and Shubho Sengupta.
\newblock Introducing the ai research supercluster — meta’s cutting-edge ai
  supercomputer for ai research, 2022.
\newblock URL \url{https://ai.facebook.com/blog/ai-rsc/}.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei, et~al.]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}, 2023.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe,
  Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2023self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}, 2023.

\bibitem[Mialon et~al.(2023)Mialon, Dess{\`\i}, Lomeli, Nalmpantis, Pasunuru,
  Raileanu, Rozi{\`e}re, Schick, Dwivedi-Yu, Celikyilmaz,
  et~al.]{mialon2023augmented}
Gr{\'e}goire Mialon, Roberto Dess{\`\i}, Maria Lomeli, Christoforos Nalmpantis,
  Ram Pasunuru, Roberta Raileanu, Baptiste Rozi{\`e}re, Timo Schick, Jane
  Dwivedi-Yu, Asli Celikyilmaz, et~al.
\newblock Augmented language models: a survey.
\newblock \emph{arXiv preprint arXiv:2302.07842}, 2023.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[Mitchell et~al.(2018)Mitchell, Wu, Zaldivar, Barnes, Vasserman,
  Hutchinson, Spitzer, Raji, and Gebru]{mitchellModelCards}
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
  Ben Hutchinson, Elena Spitzer, Inioluwa~Deborah Raji, and Timnit Gebru.
\newblock Model cards for model reporting.
\newblock \emph{CoRR}, abs/1810.03993, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.03993}.

\bibitem[{MosaicML NLP Team} et~al.(2023)]{MosaicML2023Introducing}
{MosaicML NLP Team} et~al.
\newblock Introducing mpt-7b: A new standard for open-source, commercially
  usable llms, 2023.

\bibitem[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyanbrown, Kim, Hesse,
  Jain, Kosaraju, Saunders, Jiang, Cobbe, Eloundou, Krueger, Button, Knight,
  Chess, and Schulman]{nakano2021webgpt}
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Lonbrown Ouyanbrown,
  Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William
  Saunders, Xu~Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
  Button, Matthew Knight, Benjamin Chess, and John Schulman.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock In \emph{arXiv}, 2021.

\bibitem[Nguyen et~al.(2019)Nguyen, Achille, Lam, Hassner, Mahadevan, and
  Soatto]{nguyen-etal-2019-toward}
Cuong~V. Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan,
  and Stefano Soatto.
\newblock Toward understanding catastrophic forgetting in continual learning.
\newblock \emph{arXiv preprint arXiv:1908.01091}, 2019.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023.
\newblock \doi{10.48550/arXiv.2303.08774}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.08774}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Patterson et~al.(2021)Patterson, Gonzalez, Le, Liang, Munguia,
  Rothchild, So, Texier, and Dean]{patterson2021carbon}
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
  Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock \emph{arXiv preprint arXiv:2104.10350}, 2021.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli,
  Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
  Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
  and Julien Launay.
\newblock The refinedweb dataset for falcon llm: Outperforming curated corpora
  with web data, and web data only, 2023.

\bibitem[Pope et~al.(2022)Pope, Douglas, Chowdhery, Devlin, Bradbury, Levskaya,
  Heek, Xiao, Agrawal, and Dean]{palminf}
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
  Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.
\newblock Efficiently scaling transformer inference, 2022.

\bibitem[Rae et~al.(2022)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri,
  Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy,
  Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart,
  Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis,
  Kavukcuoglu, and Irving]{rae2022scaling}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
  Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
  Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
  de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
  Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris Jones, James
  Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel,
  William Isaac, Ed~Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
  Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray
  Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher, 2022.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{rajpurkar2018know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock \emph{arXiv preprint arXiv:1806.03822}, 2018.

\bibitem[Ramasesh et~al.(2021)Ramasesh, Lewkowycz, and
  Dyer]{ramasesh-etal-2021-effect}
Vinay~Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer.
\newblock Effect of scale on catastrophic forgetting in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Roller et~al.(2020)Roller, Boureau, Weston, Bordes, Dinan, Fan,
  Gunning, Ju, Li, Poff, et~al.]{roller2020open}
Stephen Roller, Y-Lan Boureau, Jason Weston, Antoine Bordes, Emily Dinan,
  Angela Fan, David Gunning, Da~Ju, Margaret Li, Spencer Poff, et~al.
\newblock Open-domain conversational agents: Current progress, open problems,
  and future directions.
\newblock \emph{arXiv preprint arXiv:2006.12442}, 2020.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and
  Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106,
  2021.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and
  Choi]{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock \emph{arXiv preprint arXiv:1904.09728}, 2019.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow,
  Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli,
  Zettlemoyer, Cancedda, and Scialom]{schick2023toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria
  Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{arXiv preprint arXiv:2302.04761}, 2023.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Scialom et~al.(2020{\natexlab{a}})Scialom, Dray, Lamprier, Piwowarski,
  and Staiano]{pmlr-v119-scialom20a}
Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and
  Jacopo Staiano.
\newblock Discriminative adversarial search for abstractive summarization.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 8555--8564. PMLR,
  13--18 Jul 2020{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v119/scialom20a.html}.

\bibitem[Scialom et~al.(2020{\natexlab{b}})Scialom, Dray, Lamprier, Piwowarski,
  and Staiano]{scialom2020coldgans}
Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and
  Jacopo Staiano.
\newblock Coldgans: Taming language gans with cautious sampling strategies.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18978--18989, 2020{\natexlab{b}}.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{sennrich2016neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units, 2016.

\bibitem[Shaham et~al.(2022)Shaham, Segal, Ivgi, Efrat, Yoran, Haviv, Gupta,
  Xiong, Geva, Berant, and Levy]{shaham-etal-2022-scrolls}
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit
  Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy.
\newblock {SCROLLS}: Standardized {C}ompa{R}ison over long language sequences.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 12007--12021, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.823}.

\bibitem[Shazeer(2019)]{shazeer2019mq}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need, 2019.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer, 2020.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{tp2019}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism, 2019.

\bibitem[Shumailov et~al.(2023)Shumailov, Shumaylov, Zhao, Gal, Papernot, and
  Anderson]{shumailov2023curse}
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and
  Ross Anderson.
\newblock The curse of recursion: Training on generated data makes models
  forget.
\newblock \emph{arXiv preprint arxiv:2305.17493}, 2023.

\bibitem[Smith and Williams(2021)]{smith2021hi}
Eric~Michael Smith and Adina Williams.
\newblock Hi, my name is martha: Using names to measure and mitigate bias in
  generative dialogue models.
\newblock \emph{arXiv preprint arXiv:2109.03300}, 2021.

\bibitem[Smith et~al.(2022)Smith, Hall, Kambadur, Presani, and
  Williams]{smith2022m}
Eric~Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina
  Williams.
\newblock “i’m sorry to hear that”: Finding new biases in language models
  with a holistic descriptor dataset.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 9180--9211, 2022.

\bibitem[Solaiman et~al.(2023)Solaiman, Talat, Agnew, Ahmad, Baker, Blodgett,
  Daum{\'e}~III, Dodge, Evans, Hooker, et~al.]{solaiman2023evaluating}
Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su~Lin
  Blodgett, Hal Daum{\'e}~III, Jesse Dodge, Ellie Evans, Sara Hooker, et~al.
\newblock Evaluating the social impact of generative ai systems in systems and
  society.
\newblock \emph{arXiv preprint arXiv:2306.05949}, 2023.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stienon2020learning}
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M. Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul Christiano.
\newblock Learning to summarize from human feedback.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Su et~al.(2022)Su, Lu, Pan, Murtadha, Wen, and Liu]{su2022roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Ahmed Murtadha, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding, 2022.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung,
  Chowdhery, Le, Chi, Zhou, et~al.]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve
  them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.

\bibitem[Synnaeve et~al.(2019)Synnaeve, Gehring, Lin, Haziza, Usunier,
  Rothermel, Mella, Ju, Carion, Gustafson, et~al.]{synnaeve2019growing}
Gabriel Synnaeve, Jonas Gehring, Zeming Lin, Daniel Haziza, Nicolas Usunier,
  Danielle Rothermel, Vegard Mella, Da~Ju, Nicolas Carion, Laura Gustafson,
  et~al.
\newblock Growing up together: Structured exploration for large action spaces.
\newblock 2019.

\bibitem[Tal et~al.(2022)Tal, Magar, and Schwartz]{tal-etal-2022-fewer}
Yarden Tal, Inbal Magar, and Roy Schwartz.
\newblock Fewer errors, but more stereotypes? the effect of model size on
  gender bias.
\newblock In \emph{Proceedings of the 4th Workshop on Gender Bias in Natural
  Language Processing (GeBNLP)}, pages 112--120, Seattle, Washington, July
  2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.gebnlp-1.13}.
\newblock URL \url{https://aclanthology.org/2022.gebnlp-1.13}.

\bibitem[Talmor et~al.(2018)Talmor, Herzig, Lourie, and
  Berant]{talmor2018commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense
  knowledge.
\newblock \emph{arXiv preprint arXiv:1811.00937}, 2018.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn,
  Saravia, Poulton, Kerkez, and Stojnic]{taylor2022galactica}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
  Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
\newblock Galactica: A large language model for science.
\newblock \emph{arXiv preprint arXiv:2211.09085}, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{Touvron2023LLaMAOA}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2017.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and
  Hajishirzi]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated
  instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022.

\bibitem[Webb(2019)]{webb2019impact}
Michael Webb.
\newblock The impact of artificial intelligence on the labor market.
\newblock \emph{Available at SSRN 3482150}, 2019.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{weifinetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du,
  Dai, and Le]{flan}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M. Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Xia, Chi,
  Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24824--24837, 2022{\natexlab{b}}.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang,
  Cheng, Glaese, Balle, Kasirzadeh, et~al.]{weidinger2021ethical}
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato,
  Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et~al.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{arXiv preprint arXiv:2112.04359}, 2021.

\bibitem[Welbl et~al.(2021)Welbl, Glaese, Uesato, Dathathri, Mellor, Hendricks,
  Anderson, Kohli, Coppin, and Huang]{welbl2021challenges}
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,
  Lisa~Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen
  Huang.
\newblock Challenges in detoxifying language models, 2021.

\bibitem[Wu et~al.(2022)Wu, Raghavendra, Gupta, Acun, Ardalani, Maeng, Chang,
  Aga, Huang, Bai, et~al.]{wu2022sustainable}
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
  Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et~al.
\newblock Sustainable ai: Environmental implications, challenges and
  opportunities.
\newblock \emph{Proceedings of Machine Learning and Systems}, 4:\penalty0
  795--813, 2022.

\bibitem[Xu et~al.(2021)Xu, Ju, Li, Boureau, Weston, and Dinan]{xu2021recipes}
Jing Xu, Da~Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan.
\newblock Recipes for safety in open-domain chatbots, 2021.

\bibitem[Zellers et~al.(2019{\natexlab{a}})Zellers, Holtzman, Bisk, Farhadi,
  and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019{\natexlab{a}}.

\bibitem[Zellers et~al.(2019{\natexlab{b}})Zellers, Holtzman, Rashkin, Bisk,
  Farhadi, Roesner, and Choi]{zellers2019defending}
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
  Franziska Roesner, and Yejin Choi.
\newblock Defending against neural fake news.
\newblock \emph{Advances in neural information processing systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Zhang and Sennrich(2019)]{zhang2019root}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Gu, Varma, Luo, Huang, Xu, Wright, Shojanazeri,
  Ott, Shleifer, Desmaison, Balioglu, Nguyen, Chauhan, Hao, and
  Li]{zhao2023fsdp}
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less
  Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can
  Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li.
\newblock Pytorch fsdp: Experiences on scaling fully sharded data parallel,
  2023.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and
  Duan]{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin
  Saied, Weizhu Chen, and Nan Duan.
\newblock Agieval: A human-centric benchmark for evaluating foundation models.
\newblock \emph{arXiv preprint arXiv:2304.06364}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu,
  Zhang, Ghosh, Lewis, Zettlemoyer, and Levy]{zhou-etal-2023-lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke
  Zettlemoyer, and Omer Levy.
\newblock Lima: Less is more for alignment.
\newblock \emph{arXiv preprint arXiv:2305.11206}, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and
  Ba]{zhou2022large}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,
  Harris Chan, and Jimmy Ba.
\newblock Large language models are human-level prompt engineers.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Zhuo et~al.(2023)Zhuo, Huang, Chen, and Xing]{zhuo2023exploring}
Terry~Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing.
\newblock Exploring ai ethics of chatgpt: A diagnostic analysis.
\newblock \emph{arXiv preprint arXiv:2301.12867}, 2023.

\end{thebibliography}
