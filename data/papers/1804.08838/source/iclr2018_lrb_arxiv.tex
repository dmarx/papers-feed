\documentclass{article} % For LaTeX2e
\usepackage[usenames,dvipsnames]{xcolor}
% \usepackage{iclr2018_conference_arxiv,times}
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{courier}
\usepackage{ownstyles}

\include{subtex/mlVecMat}

\graphicspath{{figs/}}

%\usepackage{sidecap}

% % Commenting highlights
\newif\ifcomments

%Uncomment one of the two lines below to turn todos on/off
\commentsfalse
%\commentstrue

\ifcomments
\newcommand{\comments}[1]{#1}
\else
\newcommand{\comments}[1]{}
\fi
% % Commenting highlights

% \cl now in ownstyles
%\newcommand{\cl}[1]{{\color{magenta}{\bf\sf [CL: #1]}}}



% Notation
\newcommand{\thetad}{\theta^{(d)}}
\newcommand{\thetado}{\theta^{(d)}_0}
\newcommand{\thetads}{\theta^{(d)}_*}
\newcommand{\thetaD}{\theta^{(D)}}
\newcommand{\thetaDo}{\theta^{(D)}_0}
\newcommand{\thetaDs}{\theta^{(D)}_*}
\newcommand{\dint}{d_{\mathrm{int}}}
\newcommand{\dintn}{d_{\mathrm{int90}}}
\newcommand{\dinto}{d_{\mathrm{int100}}}


% Used for SI title
\newcommand{\titl}{Measuring the Intrinsic Dimension of Objective Landscapes}
\title{Measuring the Intrinsic Dimension \\ of Objective Landscapes}



% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Chunyuan Li \thanks{Work performed as an intern at Uber AI Labs.} \\
Duke University\\
\texttt{cl319@duke.edu} \\
\And
Heerad Farkhoor, Rosanne Liu, and Jason Yosinski \\
Uber AI Labs\\
\texttt{\{heerad,rosanne,yosinski\}@uber.com} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}

Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the \emph{intrinsic dimension} of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.

\end{abstract}



\section{Introduction}

%In this paper we define and investigate the \emph{intrinsic dimension} of a neural network optimization problem. To introduce this concept, we first briefly review neural network training from an optimization perspective.
%
Training a neural network to model a given dataset entails several steps. First, the network designer chooses a loss function and a network architecture for a given dataset. The architecture is then initialized by populating its weights with random values drawn from some distribution. Finally, the network is trained by adjusting its weights to produce a loss as low as possible. We can think of the training procedure as traversing some path along an \emph{objective landscape}.
Note that as soon as a dataset and network architecture are specified, the landscape in its entirety is completely determined. It is instantiated and frozen; all subsequent parameter initialization, forward and backward propagation, and gradient steps taken by an optimizer are just details of how the frozen space is explored.

% practical computationally convenient details of poking about this space>
% Concretely, if the network has D parameters, the landscape is a mapping L from R^D to R. Each point in D-dimensions <....>

Consider a network parameterized by $D$ weights. We can picture its associated objective landscape as a set of ``hills and valleys'' in $D$ dimensions, where each point in $\mathbb{R}^D$ corresponds to a value of the loss, \ie the elevation of the landscape. If $D = 2$, the map from two coordinates to one scalar loss can be easily imagined and intuitively understood by those living in a three-dimensional world with similar hills.
%
However, in higher dimensions, our intuitions may not be so faithful, and generally we must be careful, as extrapolating low-dimensional intuitions to higher dimensions can lead to unreliable conclusions.
The difficulty of understanding high-dimensional landscapes notwithstanding, it is the lot of neural network researchers to spend their efforts leading (or following?) networks over these multi-dimensional surfaces. Therefore, any interpreted geography of these landscapes is valuable.
%In light of this, we investigate and define \emph{intrinsic dimension}, as a measure of the difficulty in exploring and traversing objective landscapes.

Several papers have shed valuable light on this landscape, particularly by pointing out flaws in common extrapolation from low-dimensional reasoning.
%\begin{itemize}
%\item
\cite{dauphin2014identifying-and-attacking-the-saddle} showed that, in contrast to conventional thinking about getting stuck in local optima (as one might be stuck in a valley in our familiar $D=2$), local critical points in high dimension are almost never valleys but are instead saddlepoints: structures which are ``valleys'' along a multitude of dimensions with ``exits'' in a multitude of other dimensions. The striking conclusion is that one has less to fear becoming hemmed in on all sides by higher loss but more to fear being waylaid nearly indefinitely by nearly flat regions.
\cite{goodfellow-2015-ICLR-qualitatively-characterizing-neural} showed another property: that paths directly from the initial point to the final point of optimization are often monotonically decreasing. Though dimension is high, the space is in some sense simpler than we thought: rather than winding around hills and through long twisting corridors, the walk could just as well have taken a straight line without encountering any obstacles, if only the direction of the line could have been determined at the outset.
%  \item(maybe include?) Small batch \vs large batch
%  \item (maybe) In a general sense, every paper reporting better results is reporting first a method of creating a landscape that happens to contain lower valleys and second, if we assume the authors were not extraordinarily lucky, proof that following simple optimization methods from random starting positions can reliably find one of those optima.
%\end{itemize}
%Both showed the landscape is simpler than we might have thought.

In this paper we seek further understanding of the structure of the
objective landscape by restricting training to random slices through
it, allowing optimization to proceed in randomly generated subspaces
of the full parameter space.
Whereas
standard neural network training involves computing a gradient and
taking a step in the full parameter space ($\mathbb{R}^D$ above), we
instead choose a random $d$-dimensional subspace of $\mathbb{R}^D$,
where generally $d < D$, and optimize directly in this subspace. By
performing experiments with gradually larger values of $d$, we can
find the subspace dimension at which solutions first appear, which we call the
measured \emph{intrinsic dimension} of a particular problem. Examining
intrinsic dimensions across a variety of problems leads to a few new
intuitions about the optimization problems that arise from neural network
models.

We begin in \secref{approach} by defining more precisely the notion of intrinsic dimension as a measure of the difficulty of objective landscapes. In \secref{results} we measure intrinsic dimension over a variety
of network types and datasets, including MNIST, CIFAR-10, ImageNet, and several RL tasks. Based
on these measurements, we draw a few insights on network behavior, and we conclude in \secref{conclusion}.
 %\todo{summarize other sections}
 
% REINSTATE
%\jby{redo this part vvv}
%
%\begin{itemize}
%  \item We describe the basic approach of random projections (\secref{approach}).
%  \item We show that ........
%\end{itemize}
%
%find that solutions are
%relatively everywhere and highly redundant, fairly conserved across
%different parameterizations,
%``Solutions are easy to find because they are everywhere.''
%
%\jby{redo this part \^{}\^{}\^{}}




\section{Defining and Estimating Intrinsic Dimension}
% REMOVED: via random subspace training
\seclabel{approach}

We introduce the intrinsic dimension of an objective landscape with an illustrative toy problem.
Let $\thetaD \in \mathbb{R}^D$ be a parameter vector in a parameter space of dimension $D$, let $\thetaDo$ be a randomly chosen initial parameter vector, and
let $\thetaDs$ be the final parameter vector arrived at via optimization.

Consider a toy optimization problem where $D=1000$ %$\thetaDo$ is drawn from a Gaussian distribution, 
and where $\thetaD$ optimized to minimize a squared error cost function that requires the first 100 elements to sum to 1, the second 100 elements to sum to 2, and so on until the vector has been divided into 10 groups with their requisite 10 sums.
%$\thetaD$ optimized to minimize 
%he objective $ J(\theta) = \sum_{r=1}^{10} \| 1- \sum_{i=100r-99}^{100r} \theta_i \|^2$, where $\theta_i$ is the $i$th element of $\thetaD$.
% a squared error cost function that requires the first 100 elements to sum to 1, the second 100 elements to sum to 2, and so on until the vector has been divided into 10 groups with their requisite 10 sums.
%
We may start from a $\thetaDo$ that is drawn from a Gaussian distribution and optimize in $\mathbb{R}^D$ to find a $\thetaDs$ that solves the problem with cost arbitrarily close to zero.

Solutions to this problem are highly redundant. With a little algebra, one can find that the manifold of solutions is a 990 dimensional hyperplane: from any point that has zero cost, there are 990 orthogonal directions one can move and remain at zero cost. Denoting as $s$ the dimensionality of the solution set, we define the intrinsic dimensionality $\dint$ of a solution as the codimension of the solution set inside of $\mathbb{R}^D$:
\beq
D = \dint + s
\eeq

%Thus is illustrated a simple relationship between the dimension of the parameter space, $D$ (here 1000), the intrinsic dimension, $\dint$ (here 10), and the dimension of the solution manifold, which we will denote $s$ (here 990):
%Here, the intrinsic dimension of the problem is 10 (1000 = 10 + 990).

Here the intrinsic dimension $\dint$ is 10 (1000 = 10 + 990), with 10 corresponding intuitively to the number of constraints placed on the parameter vector. Though the space is large ($D = 1000$), the number of things one needs to get right is small ($\dint = 10$).


\subsection{Measuring Intrinsic Dimension via Random Subspace Training}

The above example had a simple enough form that we obtained $\dint = 10$ by calculation. But in general we desire a method to measure or approximate $\dint$ for more complicated problems, including problems with data-dependent objective functions, e.g. neural network training.
Random subspace optimization provides such a method.

Standard optimization, which we will refer to hereafter as the \emph{direct} method of training, entails evaluating the gradient of a loss with respect to $\thetaD$ and taking steps directly in the space of $\thetaD$. To train in a random subspace, we instead define $\thetaD$ in the following way:
%
\beq
\thetaD = \thetaDo + P\thetad~
\eeq
%
where $P$ is a randomly generated $D \times d$ projection matrix\footnote{This projection matrix can take a variety of forms each with different computational considerations. In later sections we consider dense, sparse, and implicit $P$ matrices.} and $\thetad$ is a parameter vector in a generally smaller space $\mathbb{R}^d$. $\thetaDo$ and $P$ are randomly generated and frozen (not trained), so the system has only $d$ degrees of freedom. We initialize $\thetad$ to a vector of all zeros, so initially $\thetaD = \thetaDo$. This convention serves an important purpose for neural network training:
it allows the network to benefit from beginning in a region of parameter space
designed by any number of good initialization schemes
\citep{glorot2010understanding,he2015delving}
to be well-conditioned, such that
gradient descent via commonly used optimizers will tend to work well.\footnote{A second, more subtle reason to start away from the origin and with a randomly oriented subspace is that this puts the subspace used for training and the solution set in \emph{general position} with respect to each other. Intuitively, this avoids pathological cases where both the solution set and the random subspace contain structure oriented around the origin or along axes, which could bias toward non-intersection or toward intersection, depending on the problem.}
%; in the case of neural network training, we may sample 
%$\thetaDo$ from some cleverly designed
%distribution

%It allows decoupling of the initialization of the network from the random subspace optimization process itself.
%Note that setting $\thetaDo$ to an
%all-zero vector would have been an exceptionally poor choice for neural networks: for
%reasons of symmetry, the gradient can end up being zero in many
%directions, hindering effective optimization.

Training proceeds by computing gradients with respect to $\thetad$ and taking steps in that space. Columns of $P$ are normalized to unit length, so steps of unit length in $\thetad$ chart out unit length motions of $\thetaD$. Columns of $P$ may also be orthogonalized if desired, but in our experiments we relied simply on the approximate orthogonality of high dimensional random vectors. By this construction $P$ forms an approximately orthonormal basis for a randomly oriented $d$ dimensional subspace of $\mathbb{R}^D$, with the origin of the new coordinate system at $\thetaDo$. \figref{subspace_drawing_crop.pdf} (left and middle) shows an illustration of the related vectors.

%, dl/dtheta at theta\_0 and using this to inform an optimizer, which generally takes steps along the negative gradient to produce a lower expected loss the next step. This gradient is the slope along each of the D dimensions of theta.
%Let us now define an alternate optimization procedure. As before, we start at theta0, but rather than allowing motion along every direction, we restrict motion to d dimensions, where d is generally much less than D.

%\figp{subspace_drawing_crop.pdf}{.5}{Illustration of parameter vectors for the $D=3, d=2$ case.}
%\figp{toy_example_crop.pdf}{.5}{Illustration of toy example. \later{JBY: Provide actual plot of the results of this toy experiment.} }
%\figgp[h]{subspace_drawing_crop.pdf}{.45}{toy_example_crop.pdf}{.5}{(left) Illustration of parameter vectors for the $D=3, d=2$ case. (right) Illustration of toy example. \later{JBY: Provide actual plot of the results of this toy experiment.} }

\begin{figure}
\begin{center}

\includegraphics[width=.44\linewidth]{subspace_drawing_crop.pdf}
\hspace{.04\linewidth}
\includegraphics[width=.50\linewidth]{toy_example_crop.pdf}

\caption{
  \textbf{(left)} Illustration of parameter vectors for direct optimization in the $D=3$ case.
  \textbf{(middle)} Illustration of parameter vectors and a possible random subspace for the $D=3, d=2$ case.
  \textbf{(right)} Plot of performance \vs subspace dimension for the toy example of toy example of \secref{approach}. The problem becomes both 90\% solvable and 100\% solvable at random subspace dimension 10, so $d_{\mathrm{int90}}$ and $d_{\mathrm{int100}}$ are 10.
}
\figlabel{subspace_drawing_crop.pdf}
\end{center}
\end{figure}

Consider a few properties of this training approach.
%Our subspace training scheme shares interesting connections with the direct trainining approach.
% Consider the optimization problem this formulation creates. 
If $d=D$ and $P$ is a large identity matrix, we recover exactly the direct optimization problem. If $d=D$ but $P$ is instead a random orthonormal basis for all of $\mathbb{R}^D$ (just a random rotation matrix), we recover a rotated version of the direct problem.
Note that for some ``rotation-invariant'' optimizers, such as SGD and SGD with momentum, rotating the basis will not change the steps taken nor the solution found, but for optimizers with axis-aligned assumptions, such as RMSProp \citep{tieleman-2012-lecture-6.5-rmsprop:-divide} and Adam \citep{kingma-2014-arXiv-adam:-a-method-for-stochastic}, the path taken through $\thetaD$ space by an optimizer will depend on the rotation chosen.
Finally, in the general case where $d<D$ and solutions exist in $D$, solutions will \emph{almost surely} (with probability 1) not be found if $d$ is less than the codimension of the solution. \muchlater{cite for this}
On the other hand, when $d \geq D - s$,
if the solution set is a hyperplane, the solution will almost surely intersect the subspace, but for solution sets of arbitrary topology, intersection is not guaranteed.
Nonetheless, by iteratively increasing $d$, re-running optimization, and checking for solutions, we obtain one estimate of $\dint$. We try this sweep of $d$ for our toy problem laid out in the beginning of this section, measuring (by convention as described in the next section) the positive performance (higher is better) instead of loss.\footnote{For this toy problem we define $\mathrm{performance} = \exp({-\mathrm{loss}})$, bounding performance between 0 and 1, with 1 being a perfect solution.}
As expected, the solutions are first found at $d = 10$ (see \figref{subspace_drawing_crop.pdf}, right), confirming our intuition that for this problem, $\dint = 10$.



\subsection{Details and conventions}
\seclabel{conventions}

%\jby{vvvv}

%--- and a randomly generated 10 dimensional hyperplane arising from $P$ will almost surely intersect this solution manifold.

%[we might consider instead volume of solutions, but this behaves weirdly in high dim.]

%If we search for solutions with $d$ for any value smaller than 10 we will \emph{almost surely}\footnote{With probability 1.} not find a solution, but for $d \geq 10$, solutions will almost surely be found.

%\jby{-----}

%\subsection{Optimization in random subspaces}

%.collected into a vector $\theta^{(D)} \in \mathbb{R}^D$.
%Let us denote the initial, random parameter vector $\theta^{(D)}_0$ and the final parameter vector at convergence $\theta^{(D)}_*$
%\jby{starting at theta0 and proceeding to some final theta*.}

%distribution not because it is expected that samples will produce low loss -- such a thing would require exponentially much luck -- but it is chosen via any number of cleverly scaled distributions

%solutions will be different. In our scenario where $d < D$, 
%but allow motion in only a small subset of the D possible directions, say d possible directions,  We could effect this search procedure by choosing a random subset of the dimensions of D, allowing some parameters to change but freezing others. But in general we may choose the random subset poorly, so it 
%Consider d random unit-length vectors originating at theta0 and pointing away in different (orthogonal or nearly orthogonal) directions. In general, d is much less than D. If these d vectors are chosen 

%\subsection{Intrinsic dimension as minimum subspace containing approximate solutions}

%The main question we ask in this paper is: what is the lowest dimension $d$ at which a given problem can reliably be solved? We call this lowest dimension the problem's \emph{intrinsic dimension}.

%\later{JBY: justify this somehow? Or say that it's just an intuitive notion? Mention that it fails in rare cases, like getting a very lucky subspace?}

In the rest of this paper, we measure intrinsic dimensions for particular neural network problems and draw conclusions about the associated objective landscapes and solution sets. Because modeling real data is more complex than the above toy example, and losses are generally never exactly zero, we first choose a heuristic for classifying points on the objective landscape as solutions \vs non-solutions.
The heuristic we choose is to threshold network performance at some level relative to a baseline model, where generally we take as baseline the best directly trained model.
In supervised classification settings, validation accuracy is used as the measure of performance, and in reinforcement learning scenarios, the total reward (shifted up or down such that the minimum reward is 0) is used.
Accuracy and reward are preferred to loss to ensure results are grounded to real-world performance and to allow comparison across models with differing scales of loss and different amounts of regularization included in the loss.

%When a maximum level of performance is defined by the problem and generally taken as achievable (e.g. a perfect score in Pong) we take it as baseline, else we take as baseline the performance of the best directly trained model.

We define $\dinto$ as the intrinsic dimension of the ``100\%'' solution: solutions whose performance is statistically indistinguishable from baseline solutions. However, when attempting to measure $\dinto$, we observed it to vary widely, for a few confounding reasons: $\dinto$ can be very high --- nearly as high as $D$ --- when the task requires matching a very well-tuned baseline model, but can drop significantly when the regularization effect of restricting parameters to a subspace boosts performance by tiny amounts.
%
While these are interesting effects, we primarily set out to measure the basic difficulty of problems and the degrees of freedom needed to solve (or approximately solve) them rather than these subtler effects.


%Thus, we found it more useful define a second, lower threshold for solution \vs non-solution and measure the dimension at which that threshold is crossed.

%We use as a heuristic a threshold in network performance
%We classify as solutions or non-solutions by thresholding network performance at some baseline level, using
%validation accuracy as the performance metric for supervised classification problems.

%In reinforcement learning scenarios, we take as baseline either a predefined level of reward (e.g. a perfect score in Pong) 

%Taking a positive performance metric also allows some comparison across supervised and reinf

%--- ignoring components of the loss like ---  the performance metric for supervised classification problems. Taking positive performance allows for 

%To do this for a particular modeling problem, we first define baseline performance to be the performance of the best directly parameterized model in $D$ dimensions.

%If $d$ is chosen to be too small, performance will be poor, but as $d$ approaches $D$, performance should improve until it reaches performance of the direct model; in the limiting case and with use of rotation invariant optimizers, as mentioned above, convergence is guaranteed.

Thus, we found it more practical and useful to define and measure $\dintn$ as the intrinsic dimension of the ``90\%'' solution: solutions with performance at least 90\% of the baseline.
%

We chose 90\% after looking at a number of dimension \vs performance plots (e.g. \figref{dim_mnist_mlp_2_200_std}) as a reasonable trade off between wanting to guarantee solutions are as good as possible, but also wanting measured $\dint$ values to be robust to small noise in measured performance. If too high a threshold is used, then the dimension at which performance crosses the threshold changes a lot for only tiny changes in accuracy, and we always observe tiny changes in accuracy due to training noise.
%The 90\% threshold was empirically chosen based on the observation over a variety of dimension \vs performance plots: it is a reasonable trade off 
%between maitaining good solutions (and a robust measurement) for individual problems and crafting a sensible measurement across multiple problems. 

% between wanting to guarantee the solution found is as good as possible, but also not wanting measured $d$ values to be very sensitive to small noise in measured performance. 
%
%When the threshold is high, the dimension of crossing the threshold changes significantly for only tiny changes of accuracy, which is usually due to training noise.
%
%
% JBY: removed this point for now
%Further, \cite{giryes2016deep} showed that the main structure of the data manifold is preserved via random weights of neural networks, and the role of training is to refine and encode the details of decision boundary. In our case,  $\dintn$ is designed to reflect the macro structures of the objective landscape.
%Therefore, if too high a threshold was used, we expect most of results in the rest of the paper to remain qualitatively unchanged, with $d$ saturated in measuring the training noise and landspace details.
%
%\later{JBY is skeptical of or at least confused by above point}
%
If a somewhat different (higher or lower) threshold were chosen, we expect most of conclusions in the rest of the paper to remain qualitatively unchanged. %albeit with $\dint$ values scaled up (or down) by some factor and more noisily (or less noisily) measured. 
In the future, researchers may find it useful to measure $\dint$ using higher or lower thresholds.

%\todo{above: remove or merge}


% If a higher (or lower) threshold were chosen, we expect most of results in the rest of the paper to remain qualitatively unchanged, albeit with $d$ scaled up (or down) by some factor and more noisily (or less noisily) measured.

%This measure is much more robust to noise in the training procedure.

%Due to noise in the training procedure, we define a somewhat arbitrary threshold after which performance of the subspace model is considered to be good enough, after which the problem is deemed ``approximately solved.'' In this paper we set this threshold to 90\% of the performance of the direct model. For classification tasks, threshold performance is taken to be 90\% of validation accuracy,
%\footnote{\secref{FILL_IN_RANDOM_LABEL_SECTION} uses a slightly different definition}   % REINSTATE
%and for reinforcement learning (RL) tasks, episode reward is first shifted so that minimum reward is at zero and then 90\% of the shifted reward must be achieved.


%\later{JBY or CL?? Take intrinsic dim all the way up to D with enough runs per value of d to be statistically significant. Consider using a statistical test to establish crossing.}
%\todo{JBY: explain results of the MNIST Equality experiment in draft (dim never quite gets there!)}


%\vspace{-5mm}
\section{Results and Discussion}
\seclabel{results}

\subsection{MNIST}

We begin by analyzing a fully connected (FC) classifier trained on MNIST. We choose a network with layer sizes \mbox{784--200--200--10}, i.e. a network with two hidden layers of width 200; this results in a total number of parameters $D=199,210$. A series of experiments with gradually increasing subspace dimension $d$ produce monotonically increasing performances, as shown in \figref{dim_mnist_mlp_2_200_std}~(left). By checking the subspace dimension at which performance crosses the 90\% mark, we measure this network's intrinsic dimension $\dintn$ at about 750.
%\todo{CL:  750 for global, 525 for individual, the former is put here, as I saw you chose the the global for 20 FC's.}


%\figp{placeholder}{.7}{Performance (validation accuracy) \vs subspace dimension $d$ for a 784-200-200-200-10 fully-connected network ($D =$ 199,210) trained on MNIST. The solid black line shows performance of a well-trained direct model, and the dashed black line shows the 90\% threshold after which we consider the problem ``approximately solved.''}

% \vspace{-5mm}
\figgp{dim_mnist_mlp_2_200_std}{.48}
{dim_mnist_lenet5_std}{.48}{
%\figgp{mnist_mlp_dim_linear_global}{.48}
%{mnist_lenet_dim_linear}{.48}{	
% {dim_mnist_lenet_log}{.48}{
  Performance (validation accuracy) \vs subspace dimension $d$ for two networks trained on MNIST: 
  \textbf{(left)} a 784--200--200--10 fully-connected (FC) network ($D =$ 199,210) and \textbf{(right)} a convolutional network, LeNet ($D =$ 44,426). The solid line shows performance of a well-trained direct (FC or conv) model, and the dashed line shows the 90\% threshold we use to define $\dintn$. The standard derivation of validation accuracy and measured  $\dintn$ are visualized as the blue vertical and red horizontal error bars.
  We oversample the region around the threshold to estimate the dimension of crossing more exactly. We use one-run measurements for $\dintn$ of 750 and 290, respectively.
}
% \vspace{-5mm}



\paragraph{Some networks are very compressible.}
A salient initial conclusion is that 750 is quite low. At that subspace dimension, only 750 degrees of freedom (0.4\%) are being used and 198,460 (99.6\%) unused to obtain 90\% of the performance of the direct baseline model.
A compelling corollary of this result is a simple, new way of creating and training compressed networks, particularly networks for applications in which the absolute best performance is not critical. To store this network, one need only store a tuple of three items:
$(\RN{1})$ the random seed to generate the frozen $\thetaDo$,
$(\RN{2})$ the random seed to generate $P$ and $(\RN{3})$ the 750 floating point numbers in $\thetads$. It leads to compression (assuming 32-bit floats) by a factor of 260$\times$ from 793kB to only 3.2kB, or 0.4\% of the full parameter size. Such compression could be very useful for scenarios where storage or bandwidth are limited, e.g. including neural networks in downloaded mobile apps or on web pages.

This compression approach differs from other neural network compression methods in the following aspects.
$(\RN{1})$
While it has previously been appreciated that large networks waste parameters \citep{dauphin-2013-arXiv-big-neural-networks-waste} and weights contain redundancy \citep{denil-2013-NIPS-predicting-parameters-in-deep} that can be exploited for post-hoc compression~\citep{wen2016learning}, this paper's method constitutes a much simpler approach to compression, where training happens once, end-to-end, and where any parameterized model is an allowable base model.
\muchlater{compare the compressing rates with deep compression~\cite{han2015deep}, they report 40 times compression rate for LeNet without loss of accuracy.}
$(\RN{2})$
Unlike layerwise compression models~\citep{denil-2013-NIPS-predicting-parameters-in-deep,wen2016learning}, we operate in the entire parameter space, which could work better or worse, depending on the network.
$(\RN{3})$
Compared to methods like that of \cite{louizos-2017-arXiv-bayesian-compression-for-deep}, who take a Bayesian perspective and consider redundancy on the level of groups of parameters (input weights to a single neuron) by using group-sparsity-inducing hierarchical priors on the weights, our approach is simpler but not likely to lead to compression as high as the levels they attain.
% They achieve a similar compression rate as in our example, albeit at the cost of extra hyperparameters to encode the prior distributions.
$(\RN{4})$
Our approach only reduces the number of degrees of freedom, not the number of bits required to store each degree of freedom, e.g. as could be accomplished by quantizing weights~\citep{han2015deep}. Both approaches could be combined.
$(\RN{5})$
There is a beautiful array of papers on compressing networks such that they also achieve computational savings during the forward pass \citep{wen2016learning,han2015deep,yang-2015-CVPR-deep-fried-convnets}; subspace training does not speed up execution time during inference.
%purpose is for understanding and interpretation of neural networks.
$(\RN{6})$
Finally, note the relationships between weight pruning, weight tying, and subspace training: weight pruning is equivalent to finding, post-hoc, a subspace that is orthogonal to certain axes of the full parameter space and that intersects those axes at the origin. Weight tying, e.g. by random hashing of weights into buckets~\citep{chen-2015-arXiv-compressing-neural-networks}, is equivalent to subspace training where the subspace is restricted to lie along the equidistant ``diagonals'' between any axes that are tied together.



\paragraph{Robustness of intrinsic dimension.}
Next, we investigate how intrinsic dimension varies across FC networks with a varying number of layers and varying layer width.\footnote{Note that here we used a global baseline of 100\% accuracy to compare simply and fairly across all models. See \secref{si_mnist} for similar results obtained using instead 20 separate baselines for each of the 20 models.}
We perform a grid sweep of networks with number of hidden layers $L$ chosen from \{1, 2, 3, 4, 5\} and width $W$ chosen from \{50, 100, 200, 400\}.
\figref{fnn_mnist_all_configs_btsp.pdf} in the Supplementary Information shows performance \vs subspace dimension plots in the style of \figref{dim_mnist_mlp_2_200_std} for all 20 networks, and
\figref{fnn_mnist_dim_global_crop.pdf} shows each network's $\dintn$
plotted against its native dimension $D$. As one can see, $D$ changes by a factor of
%24.13
$24.1$
between the smallest and largest networks, but $\dintn$ changes over this range by a factor of only $1.33$, with much of this possibly due to noise.

Thus it turns out that the intrinsic dimension changes little even as models grown in width or depth!
The striking conclusion is that every extra parameter added to the network --- every extra dimension added to $D$ --- just ends up adding one dimension to the redundancy of the solution, $s$.

Often the most accurate directly trained models for a problem have far more parameters
than needed \citep{zhang2016understanding}; this may be because they are just
easier to train, and our observation suggests a reason why:
with larger models, solutions have greater redundancy and in a sense ``cover'' more of the space.\footnote{To be precise, we may not conclude 
  ``greater coverage'' in terms of the volume of the solution set --- volumes are not comparable across spaces of different dimension, and our measurements have only estimated the dimension of the solution set, not its volume. A conclusion we \emph{may} make is that as extra parameters are added, the ratio of solution dimension to total dimension, $s/D$, increases, approaching 1. Further research could address other notions of coverage.}
%
To our knowledge, this is the first time this phenomenon has been directly measured.
We should also be careful not to claim that all FC nets on MNIST will have an intrinsic dimension of around 750; instead, we should just consider that we have found for this architecture/dataset combination a wide plateau of hyperparamter space over which intrinsic dimension is approximately constant.
%\todo{CL: check the 1.33 factor. It looks like 900 / 650 = 1.38 to me.... It is 900 / 675 = 1.33}
\muchlater{CL: run multiple runs here to reduce the amount of noise}


\figp[t]{fnn_mnist_dim_global_crop.pdf}{1}{Measured intrinsic dimension $\dintn$ vs number of parameters $D$ for 20 FC models of varying width (from 50 to 400) and depth (number of hidden layers from 1 to 5) trained on MNIST.
The red interval is the standard derivation of the measurement of $\dintn$.
Though the number of native parameters $D$ varies by a factor of $24.1$, $\dintn$ varies by only $1.33$, with much of that factor possibly due to noise, showing that $\dintn$ is a fairly robust measure across a model family and that each extra parameter ends up adding an extra dimension directly to the redundancy of the solution. Standard
deviation was estimated via bootstrap; see \secref{sweep_and_variance}.}



\figgp[t]{mnist_cmp_mlp_log_crop.pdf}{.49}{mnist_cmp_lenet_log_crop.pdf}{.49}{Performance \vs number of trainable parameters for \textbf{(left)} FC networks and \textbf{(right)} convolutional networks trained on MNIST. Randomly generated direct networks are shown (gray circles) alongside all random subspace training results (blue circles) from the sweep shown in \figref{fnn_mnist_all_configs_btsp.pdf}. FC networks show a persistent gap in dimension, suggesting general parameter inefficiency of FC models. 
	The parameter efficiency of convolutional networks varies, as the gray points can be significantly to the right  of or close to the blue manifold.}

% Convolutional networks, on the other hand, may be parameter inefficient (gray points significantly to the right of blue manifold) or parameter efficient when well tuned (gray points near blue manifold).}

\paragraph{Are random subspaces really more parameter-efficient for FC nets?}
One might wonder to what extent claiming 750 parameters is meaningful
given that performance achieved (90\%) is far worse than a state of the
art network trained on MNIST. With such a low bar for performance,
could a directly trained network with a comparable number of trainable
parameters be found that achieves the same performance?  We generated
1000 small networks (depth randomly chosen from \{1, 2, 3, 4, 5\}, layer width randomly from \{2, 3, 5, 8, 10, 15, 20, 25\}, seed set randomly) in an attempt to
find high-performing, small FC networks, but as
\figref{mnist_cmp_mlp_log_crop.pdf}~(left) shows, a gap
still exists between the subspace dimension and the smallest direct FC
network giving the same performance at most levels of performance.


\paragraph{Measuring $\dintn$ on a convolutional network.}
Next we measure $\dintn$ of a convolutional network, LeNet (D=44,426).
\figref{dim_mnist_mlp_2_200_std}~(right) shows validation accuracy \vs subspace dimension $d$, and
we find $\dintn=290$, or a compression rate of about 150$\times$ for this network.
As with the FC case above, we also do a sweep of random networks,

but notice that the performance gap of convnets between direct and subspace training methods becomes closer for fixed budgets, \ie the number of trainable parameters. Further, the performance of direct training varies significantly, depending on the extrinsic design of convet architectures.
We interpret these results in terms of the Minimum Description Length below.
\later{How robust is intrinsic dim here? Why don't we have a measurement of this?!?}


\paragraph{Relationship between Intrinsic Dimension and Minimum Description Length (MDL).}
As discussed earlier, the random subspace training method leads naturally to a compressed representation of a network, where only $d$ floating point numbers need to be stored.
%By further assuming that all trainable parameters require the same bits, we can consider
We can consider this $d$ as an upper bound on the MDL of the problem solution.\footnote{We consider MDL in terms of number of degrees of freedom instead of bits. For degrees of freedom stored with constant fidelity (e.g. \code{float32}), these quantities are related by a constant factor (e.g. 32).} We cannot yet conclude the extent to which this bound is loose or tight, and tightness may vary by problem.
However, to the extent that it is tighter than previous bounds (\eg just the number of parameters $D$) and to the extent that it is correlated with the actual MDL,
we can use this interpretation to judge which solutions are more well-suited
\later{what should ``well-suited'' mean?}
to the problem in a principled way.
As developed by \cite{rissanen-1978-automatica-modeling-by-shortest-data} and further by \cite{hinton-1993-COLT-keeping-the-neural-networks}, holding accuracy constant, the best model is the one with the shortest MDL.

Thus, there is some rigor behind our intuitive assumption that LeNet is a \emph{better} model than an FC network for MNIST image classification, because its intrinsic dimension is lower ($\dintn$ of 290 \vs 750).
In this particular case we are lead to a predictable conclusion, but as models
become larger, more complex, and more heterogeneous, conclusions of this type will often not be
obvious. Having a simple method of approximating MDL may prove extremely useful for guiding model exploration, for example, for the countless datasets less well-studied than MNIST and for 
models consisting of separate sub-models that may be individually designed and evaluated \citep{ren-2015-faster-r-cnn:-towards,kaiser-2017-arXiv-one-model-to-learn-them}. In this latter case,
considering the MDL for a sub-model could provide a more detailed view of that sub-model's properties
than would be available by just analyzing the system's overall validation performance.

Finally, note that although our approach is related to a rich body of work on estimating the ``intrinsic dimension of a dataset'' \citep{camastra-2002-TPAMI-estimating-the-intrinsic-dimension,kegl-2003-NIPS-intrinsic-dimension-estimation,fukunaga-1971-ITC-an-algorithm-for-finding-intrinsic,levina-2005-NIPS-maximum-likelihood-estimation,tenenbaum-2000-Science-a-global-geometric-framework}, it differs in a few respects. Here we do not measure the number of degrees of freedom necessary to represent a dataset (which requires representation of a global $p(X)$ and per-example properties and thus grows with the size of the dataset), but those required to represent a model for part of the dataset (here $p(y|X)$, which intuitively might saturate at some complexity even as a dataset grows very large). That said, in the following section
we do show measurements for a corner case where the model must memorize per-example properties.

%one must be careful to distinguish between estimating the number of principal dimensions of variability in a dataset, and the number of bits necessary to represent a dataset. This essentially amounts to representing the data distribution, $p(X)$ \vs the number of bits necessary to represent the conditional $p(y|X)$ (as we measure here for supervised models).



\paragraph{Are convnets \emph{always} better on MNIST? Measuring $\dintn$ on shuffled data.}
\cite{zhang2016understanding} provocatively showed that large networks
normally thought to generalize well can nearly as easily be trained to
memorize entire training sets with randomly assigned labels or with
input pixels provided in random order.
% 
Consider two identically sized networks: one trained on a real, non-shuffled dataset and another
trained with shuffled pixels or labels. As noted by \cite{zhang2016understanding}, externally the networks are
very similar, and the training loss may even be identical at the final
epoch. However, the intrinsic dimension of each may be measured to
expose the differences in problem difficulty.  
%
When training on a dataset with \emph{shuffled pixels} --- pixels for each example in the dataset subject to a random permutation, chosen once for the entire dataset --- 
the intrinsic dimension of an FC
network remains the same at 750, because FC networks are invariant to input permutation. But the intrinsic
dimension of a convnet increases from 290 to 1400, even higher than
an FC network. Thus while convnets are better suited to
classifying digits given images with local structure, when
this structure is removed, violating convolutional assumptions, our measure can clearly reveal that many more
degrees of freedom are now required to model the underlying distribution.
%
When training on MNIST with \emph{shuffled labels} --- the label for each example is randomly chosen ---
we redefine our
measure of $\dintn$ relative to training accuracy (validation
accuracy is always at chance). We find that memorizing random
labels on the 50,000 example MNIST training set requires a very high dimension, $\dintn =
190,000$, or 3.8 floats per memorized
label. \secref{si_mnist_shuffled} gives a few further results, in
particular that the more labels are memorized, the more efficient
memorization is in terms of floats per label. Thus, while
the network obviously does not generalize to an unseen validation set,
it would seem ``generalization'' \emph{within} a training set may
be occurring as the network builds a shared infrastructure that makes
it possible to more efficiently memorize labels.

                
%We run our experiments on MNIST with the following modifications of the dataset:
%$(\RN{1})$
%{\bf Standard}: the original dataset without modification;
%$(\RN{2})$
%{\bf Shuffled labels}: a random permutation of the labels is applied to both training and test set;
%$(\RN{3})$
%{\bf Shuffled pixels}: a random permutation of the pixels is applied to all the images in both training and test set.

\subsection{CIFAR-10 and ImageNet}

We scale to larger supervised classification problems by considering CIFAR-10 \citep{krizhevsky2009learning} and ImageNet \citep{russakovsky2015imagenet}. When scaling beyond MNIST-sized networks with $D$ on the order of 200k and $d$ on the order of 1k, we find it necessary to use more efficient methods of generating and projecting from random subspaces. This is particularly true in the case of ImageNet, where the direct network can easily require millions of parameters.
In \secref{scaling}, we describe and characterize scaling properties of three methods of projection: dense matrix projection, sparse matrix projection~\citep{li2006verysparse}, and the remarkable Fastfood transform \citep{le2013fastfood}. We generally use the sparse projection method to train networks on CIFAR-10 and the Fastfood transform for ImageNet.

Measured $\dintn$ values for CIFAR-10 and are ImageNet given in \tabref{dim-table}, next to all previous MNIST results and RL results to come. For CIFAR-10 we find qualitatively similar results to MNIST, but with generally higher dimension (9k \vs 750 for FC and 2.9k \vs 290 for LeNet). It is also interesting to observe the difference of $\dintn$ across network architectures. For example, to achieve a global $>\!\!50\%$ validation accuracy on CIFAR-10, FC, LeNet and ResNet approximately requires $\dintn=9$k, $2.9$k and $1$k, respectively, showing that ResNets are more efficient.
Full results and experiment details are given in \secref{si_cifar} and \secref{si_imagenet}.
%
Due to limited time and memory issues, training on ImageNet has not yet given a reliable estimate for $\dintn$ except that it is over 500k.


%Extending the aforementioned conjecture of the intrinsic dimension revealing that LeNet is a better network structure than FC on MNIST, we venture to proffer $\dintn$ as a quantitative metric to study the fitness of different architectures for a given problem.





\begin{table}[t]
\small
\renewcommand{\arraystretch}{1.3}
\caption{Measured $\dintn$ on various supervised and reinforcement learning problems.}
\tablabel{dim-table}
\vspace{-4mm}
\begin{center}
\begin{tabular}{|c |c | c | c | c | c  | c |  }\hline
\multicolumn{1}{|c| }{\bf Dataset}
&\multicolumn{2}{c|}{\bf MNIST}
& \multicolumn{2}{c|}{\bf MNIST (Shuf Pixels)}
& \multicolumn{1}{c|}{\bf MNIST (Shuf Labels)}

\\ \hline
\multicolumn{1}{|c|}{\bf Network Type}
&\multicolumn{1}{c|}{\bf FC} &  \multicolumn{1}{c|}{\bf LeNet}
&\multicolumn{1}{c|}{\bf FC} &  \multicolumn{1}{c|}{\bf LeNet}& 
\multicolumn{1}{|c|}{\bf FC}

\\ \hline
Parameter Dim. $D$  & 199,210 &  44,426 &  199,210 &  44,426 & 959,610  \\  \hline
Intrinsic Dim. $\dintn$  & 750 &  290 & 750 & 1,400 & 190,000\\  \hline
\end{tabular}

\vspace{2mm}
\begin{tabular}{|c |c | c | c || c | c | c | c | c |}\hline
\multicolumn{1}{|c| }{...}
& \multicolumn{2}{c|}{\bf CIFAR-10}
& \multicolumn{1}{c||}{\bf ImageNet}
& \multicolumn{1}{c|}{\bf Inverted Pendulum}
& \multicolumn{1}{c|}{\bf Humanoid}
& \multicolumn{1}{c|}{\bf Atari Pong}

\\ \hline
\multicolumn{1}{|c|}{...}
&\multicolumn{1}{c|}{\bf FC} &  \multicolumn{1}{c|}{\bf LeNet}
& \multicolumn{1}{|c||}{\bf SqueezeNet}
& \multicolumn{1}{|c|}{\bf FC}
& \multicolumn{1}{|c|}{\bf FC}
& \multicolumn{1}{|c|}{\bf ConvNet}

\\ \hline
... & 656,810  &  62,006  & 1,248,424 & 562 & 166,673 & 1,005,974   \\  \hline
... & 9,000 & 2,900 & $>$ 500k & 4 & 700 & 6,000   \\  \hline
\end{tabular}
\end{center}


\end{table}




\subsection{Reinforcement Learning environments}

%\jby{HERE}

Measuring intrinsic dimension allows us to perform some comparison across the divide between supervised learning and reinforcement learning.
%Are there an abundance of value function or policy parameterizations that lead to equally-performant behavior?
%Are value-based or policy-based algorithms more efficient?
In this section we measure the intrinsic dimension of three control tasks of varying difficulties using both value-based
and policy-based algorithms. The value-based algorithm we evaluate is the Deep Q-Network (DQN) \citep{mnih2013playing-atari-with}, and the policy-based algorithm is Evolutionary Strategies (ES) \citep{salimans-2017-arXiv-evolution-strategies-as-a-scalable}. Training details are given in \secref{si_rl_training}.
%
For all tasks, performance is defined as the maximum-attained (over training iterations) mean evaluation reward (averaged over 30 evaluations for a given parameter setting). In \figref{es_results}, we show results of ES on three tasks:
$\mathtt{InvertedPendulum\!-\!v1}$, $\mathtt{Humanoid\!-\!v1}$ in MuJoCo~\citep{todorov2012mujoco}, and $\mathtt{Pong\!-\!v0}$ in Atari.
Dots in each plot correspond to the (noisy) median of observed performance values across many runs for each given $d$, and the vertical uncertainty bar shows the maximum and minimum observed performance values. The dotted horizontal line corresponds to the usual 90\% baseline derived from the best directly-trained network (the solid horizontal line). A dot is darkened signifying the first $d$ that allows a satisfactory performance. We find that the inverted pendulum task is surprisingly easy, with $\dinto=\dintn=4$, meaning that only four parameters are needed to perfectly solve the problem (see \cite{stanley-2002-EvoComp-evolving-neural-networks} for a similarly small solution found via evolution).
The walking humanoid task is more difficult: solutions are found reliably by dimension 700, a similar complexity to that required to model MNIST with an FC network, and far less than modeling CIFAR-10 with a convnet. Finally, to play Pong on Atari (directly from pixels) requires a network trained in a 6k dimensional subspace, making it on the same order of modeling CIFAR-10. For an easy side-by-side comparison we list all intrinsic dimension values found for all problems in \tabref{dim-table}.
For more complete ES results see \secref{si_rl_es}, and \secref{si_rl_dqn} for DQN results.


\begin{figure}
  \begin{center}

  \includegraphics[width=.30\linewidth]{InvertedPendulum-v1_crop.pdf}~~~
  \includegraphics[width=.30\linewidth]{Humanoid-v1_crop.pdf}~~~
  \includegraphics[width=.30\linewidth]{AtariPong_crop.pdf} \\ \vspace{.2cm}~~
  \includegraphics[width=.29\linewidth]{big_inverted_pendulum.png}~~~~~~
  \includegraphics[width=.28\linewidth]{humanoid.jpg}~~~~~~
  \includegraphics[width=.28\linewidth]{pong_rgb.png}

  \caption{Results using the policy-based ES algorithm to train agents on \textbf{(left column)} $\mathtt{InvertedPendulum\!-\!v1}$, \textbf{(middle column)} $\mathtt{Humanoid\!-\!v1}$, and \textbf{(right column)} $\mathtt{Pong\!-\!v0}$. The intrinsic dimensions found are $4$, $700$, and 6k. This places the walking humanoid task on a similar level of difficulty as modeling MNIST with a FC network (far less than modeling CIFAR-10 with a convnet), and Pong on the same order of modeling CIFAR-10.}
  \figlabel{es_results}
  \end{center}
\vspace{-5mm}
\end{figure}



\vspace*{-.1cm}

\section{Conclusions and Future Directions}
\seclabel{conclusion}
\vspace*{-.2cm}

In this paper, we have defined the intrinsic dimension of objective landscapes and shown  
a simple method --- random subspace training --- of approximating it for neural network modeling problems.
We use this approach to compare problem difficulty within and across domains.
We find in some cases the intrinsic dimension is much lower than the direct parameter dimension, and hence enable network compression, 
and in other cases the intrinsic dimension is similar to that of the best tuned models, and suggesting those models are better suited to the problem.

% It shows some interesting properties, including strong regularization and robust optimization.


%matter; one can imagine the random subspace approach being useful as
%it gives directly a small set of those that do.
%Though the free
%dimensions within the solution space may well not be orthogonal to the
%random subspace intersecting it, 
Further work could also identify better ways of creating subspaces for
reparameterization: here we chose random linear subspaces, but one
might carefully construct other linear or non-linear subspaces to be
even more likely to contain solutions.
Finally, as the field departs from
single stack-of-layers image classification models toward
larger and more heterogeneous networks \citep{ren-2015-faster-r-cnn:-towards,kaiser-2017-arXiv-one-model-to-learn-them}
often composed of many modules and trained by many losses,
methods like measuring intrinsic dimension that allow some automatic assessment of model components might provide much-needed greater understanding
of individual black-box module properties.

\todo{add layerwise projection}

\todo{add what we don't understand yet: why training is harder in some cases. Due to projection across dims, ruining adam? And/or slight non-orthogonality?}

\subsubsection*{Acknowledgments}

The authors gratefully acknowledge Zoubin Ghahramani, Peter Dayan, Sam Greydanus, Jeff Clune, and Ken Stanley for insightful discussions, 
Joel Lehman for initial idea validation,
Felipe Such, Edoardo Conti and Xingwen Zhang for helping scale the ES experiments to the cluster,
Vashisht Madhavan for insights on training Pong,
Shrivastava Anshumali for conversations about random projections, and
Ozan Sener for discussion of second order methods. 
We are also grateful to Paul Mikesell, Leon Rosenshein, Alex Sergeev and the entire OpusStack Team inside Uber for providing our computing platform and for technical support.

%Authors acknowledge
%useful conversations with
%Ozan Sener,                 % second order methods.
%Ken Stanley, Jeff Clune,    % ES paper
%Zoubin Ghahramani,          % general ideas
%Sam Greydanus,              % general ideas
%Peter Dayan,                % 50+eps vs 100-eps
%CUDA implementation of Hadamard transform by Felipe Such.
%an initial experiment by Joel Lehman
%ES code for cluster by Felipe Such, Edoardo Conti and Xingwen Zhang
%Vash for Pong network
% Opus / etc help from Paul Mikesell, Leon Rosenshein, Alex Sergeev
%Shrivastava Anshumali for chats about random projections


\bibliography{subtex/references,subtex/jby_refs}
\bibliographystyle{iclr2018_conference}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% SUPPLEMENTARY INFORMATION
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\clearpage

% Supplementary Information hacks from http://jshodges.com/index.php?qs=kb_001
% For section headers starting with S
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

% Issue: Resetting counters breaks the refs to the sections/figures in SI.
\newcommand{\beginsupplementary}{%
%		\setcounter{table}{0}
	\renewcommand{\thetable}{S\arabic{table}}%
%		\setcounter{figure}{0}
	\renewcommand{\thefigure}{S\arabic{figure}}%
%		\setcounter{section}{0}
}

\beginsupplementary

%\clearpage
%
\noindent\makebox[\linewidth]{\rule{\linewidth}{3.5pt}}
\begin{center}
	{\LARGE\sc  Supplementary Information for:\\ \titl\par}
\end{center}

\noindent\makebox[\linewidth]{\rule{\linewidth}{1pt}}
%\appendix          % uncomment for sections like "A.1"

% \todo{make sure we refer to supplementary info in the main text}



\section{Additional MNIST results and insights}
\seclabel{si_mnist}

%\subsection{Results for all 20 FC networks}
%\figp{fnn_mnist_all_configs_btsp.pdf}{1}{MNIST FC network: validation accuracy vs subspace dimension for a sweep over network depth and width.}

\subsection{Sweeping depths and widths; multiple runs to estimate variance}
\seclabel{sweep_and_variance}


In the main paper, we attempted to find $\dintn$ across 20 FC networks with various depths and widths.
A grid sweep of number of hidden layers from \{1,2,3,4,5\} and width of each hidden layer from \{50,100,200,400\} is performed, and all 20 plots are shown in \figref{fnn_mnist_all_configs_btsp.pdf}. For each $d$ we take 3 runs and plot the mean and variance with blue dots and blue error bars. $\dintn$ is indicated in plots (darkened blue dots) by the dimension at which the median of the 3 runs passes 90\% performance threshold. The variance of $\dintn$ is estimated  using 50 bootstrap samples.
%, and
%\todo{RL / CL: what does the remainder of this sentence mean?} \rl{I thought that's what you meant. You bootstrap, then take only ones that passed the threshold, then calculate variance? Feel free to change it.}
%gathering all the points that first passed the threshold.
Note that the variance of both accuracy and measured $\dintn$ for a given hyper-parameter setting are generally small, and the mean of performance monotonically increases (very similar to the single-run result) as $d$ increases. This
illustrates that the difference between lucky \vs unlucky random projections have little
impact on the quality of solutions, while the subspace dimensionality has a great impact.
We hypothesize that the variance due to different $P$ matrices will be smaller than the variance due to
different random initial parameter vectors $\thetaDo$ because there are $dD$ i.i.d. samples used to create $P$ (at least in the dense case) but only $D$ samples used to create $\thetaDo$, and aspects of the network depending on smaller numbers of random samples will exhibit greater variance.
Hence, in some other experiments we rely on single runs to estimate the intrinsic dimension, though slightly more accurate estimates could be obtained via multiple runs.

In similar manner to the above, in \figref{fnn_mnist_dim_local_crop.pdf} we show the relationship between $\dintn$ and $D$ across 20 networks but using a per-model, directly trained baseline. Most baselines are slightly below 100\% accuracy. This is in contrast to \figref{fnn_mnist_dim_global_crop.pdf}, which used a simpler global baseline of 100\% across all models.
%\todo{CL: 100\%, right?}
Results are qualitatively similar but with slightly lower intrinsic dimension due to slightly lower thresholds.

%The results on 3 runs are considered, and bootstrap at each $d$ up to 50 samples.
%The means are plot as the dots, and standard derivations of the accuracy are visualized as the vertical blue error bars. The standard derivation of the measured intrinsic dimension $\dintn$ is visualized as the horizontal red error bars. 

% We conjecture the reason is that the loss surfaces of FC networks have exponentially many critical points, and SGD can converge to the band of low critical points, which are local minima of similar high quality measured by the test error~\citep{choromanska2015loss}.


\figp{fnn_mnist_all_configs_btsp.pdf}{1}{A sweep of FC networks on MNIST. Each column contains networks of the same depth, and each row those of the same number of hidden nodes in each of its layers. Mean and variance at each $d$ is shown by blue dots and blue bars. $\dintn$ is found by dark blue dots, and the variance of it is indicated by red bars spanning in the $d$ axis.}

\figp{fnn_mnist_dim_local_crop.pdf}{1}{Measured intrinsic dimension $\dintn$ vs number of parameters $D$ for 20 models of varying width (from 50 to 400) and depth (number of hidden layers from 1 to 5) trained on MNIST. The vertical red interval is the standard derivation of measured $\dintn$.
As opposed to \figref{fnn_mnist_dim_global_crop.pdf}, which used a global, shared baseline across all models, here a per-model baseline is used. The number of native parameters varies by a factor of 24.1, but $\dintn$ varies by only 1.42. The per-model baseline results in higher measured $\dintn$ for larger models because they have a higher baseline performance than the shallower models.}


\subsection{Additional details on shuffled MNIST datasets}
\seclabel{si_mnist_shuffled}

Two kinds of shuffled MNIST datasets are considered:

%\begin{minipage}{1.00\linewidth}
\begin{itemize}
  %\vspace{1mm}
\item 
  {\bf The shuffled pixel dataset}: the label for each example remains the same as the normal dataset, but a random permutation of pixels is chosen once and then applied to all images in the training and test sets. FC networks solve the shuffled pixel datasets exactly as easily as the base dataset, because there is no privileged ordering of input dimension in FC networks; all orderings are equivalent.
\item 
  {\bf The shuffled label dataset}: the images remain the same as the normal dataset, but labels are randomly shuffled for the entire training set. Here, as in~\citep{zhang2016understanding}, we only evaluate training accuracy, as test set accuracy remains forever at chance level (the training set $X$ and $y$ convey no information about test set $p(y|X)$, because the shuffled relationship in test is independent of that of training).		
\end{itemize}
%\end{minipage}


On the full shuffled label MNIST dataset (50k images), we trained an FC network ($L=5, W=400$,  which had $\dintn=750$ on standard MNIST), it yields $\dintn = 190$k.
We can interpret this as requiring 3.8 floats to memorize each random label (at 90\% accuracy).
%
Wondering how this scales with dataset size, we estimated $\dintn$ on shuffled label versions of MNIST at different scales and found curious results, shown in \tabref{mnist_shuffled_labels} and \figref{mnist_fnn_permulated_label}.
As the dataset memorized becomes smaller, the number of floats required to memorize each label becomes larger.
Put another way, as dataset size increases, the intrinsic dimension also increases, but not as fast as linearly.
%As 10\%, 50\%, 100\% of the shuffled label MNIST dataset  was used, and $\dintn$ increased from 90k, 130k to 190k (see \figref{mnist_fnn_permulated_label}). This is consistent with our intuition that the problem becomes harder, as more ``random pairs'' are required to memorize via the neural networks. 
%
The best interpretation is not yet clear, but one possible interpretation is that networks required to memorize large training sets make use of shared machinery for memorization. In other words, though performance does not generalize to a validation set, generalization \emph{within} a training set is non-negligible even though labels are random.

\begin{table}[t]
\caption{$\dintn$ required to memorize shuffled MNIST labels. As dataset size grows, memorization becomes more efficient, suggesting a form of ``generalization'' from one part of the training set to another, even though labels are random.}
\tablabel{mnist_shuffled_labels}
\begin{center}
\begin{tabular}{|r|c|c|}
  \hline
Fraction of MNIST training set & $\dintn$            & Floats per label \\
\hline
100\%    & 190k                & 3.8 \\
50\%     & 130k                & 5.2 \\
10\%     & 90k                 & 18.0 \\
\hline
\end{tabular}
\end{center}
\vspace*{-.7em}
\end{table}

\figp[t]{mnist_fnn_permulated_label}{.7}{Training accuracy \vs subspace dimension $d$ for a FC networks
  ($W\!\!=\!\!400$, $L\!\!=\!\!5$) trained on a shuffled label version of MNIST containing
  100\%, 50\%, and 10\% of the dataset.}



%
\subsection{Training stability}

An interesting tangential observation is that random subspace training can in some cases make optimization more stable. First, it helps in the case of deeper networks. \figref{fnn_mnist_depth} shows training results for FC networks with up to 10 layers. SGD with step 0.1,  and ReLUs with He initialization is used.
% \later{Chunyuan: confirm if Adam was used here. How much effort was put into getting direct models to converge? ?}
Multiple networks failed at depths 4, and all failed at depths higher than 4, 
% despite using Adam --- normally fairly lenient with ill-scaled gradients --- with a low learning rate \later{Chunyuan: confirm details} and 
despite the activation function and initialization designed to make learning stable \citep{he2015delving}. Second, for MNIST with shuffled labels, we noticed that
it is difficult to reach high training accuracy using the direct training method with SGD, though both
subspace training with SGD and either type of training with Adam reliably reach 100\% memorization as $d$ increases (see \figref{mnist_fnn_permulated_label}).

Because each random basis vector projects across all $D$ direct parameters, the optimization problem may be far better conditioned in the subspace case than in the direct case. A related potential downside is that projecting across $D$ parameters which may have widely varying scale could result in ignoring parameter dimensions with tiny gradients. This situation is similar to that faced by methods like SGD, but ameliorated by \mbox{RMSProp}, Adam, and other methods that rescale per-dimension step sizes to account for individual parameter scales. Though convergence of the subspace approach seems robust, further work may be needed to improve network amenability to subspace training: for example by ensuring direct parameters are similarly scaled by clever initialization or by inserting a pre-scaling layer between the projected subspace and the direct parameters themselves.


\begin{figure}[t] \centering
  \begin{tabular}{cc}
    \hspace{-0mm}
    \includegraphics[width=6.4cm]{acc_fnn_mnist_depth}
    &        \hspace{-4mm}
    \includegraphics[width=6.4cm]{nll_fnn_mnist_depth}
    \\
    \small{(a) Accuracy} &
    \small{(b) Negative Log Likelihood (NLL)}
  \end{tabular} \vspace{-2mm}
  \caption{Results of subspace training versus the number of layers in a fully connected network trained on MNIST. The direct method always fail to converge when $L>5$, while subspace training yields stable performance across all depths.}
  \figlabel{fnn_mnist_depth}
  \vspace{-3mm}
\end{figure}

% \paragraph{Dense \vs Sparse}We compare the two schemes to implement projection matrices in Figure~\ref{fig:pro_mat}. For each $d$, we report the mean and standard derivation of the running time in (a), testing accuracy in (b) and testing negative log likelihood (NLL) in (c).



%\later{Consier to merge this into training stability.}


\subsection{The role of optimizers}
%\jby{HERE2}

Another finding through our experiments with MNIST FC networks has to do with the role of optimizers. The same set of experiments are run with both SGD (learning rate 0.1) and ADAM (learning rate 0.001), allowing us to investigate the impact of stochastic optimizers on the intrinsic dimension achieved. 

The intrinsic dimension $\dintn$ are reported in~\figref{fnn_mnist_optimizer} (a)(b). In addition to two optimizers we also use two baselines: Global baseline that is set up as 90\% of best performance achieved across all models, and individual baseline that is with regards to the performance of the same model in direct training.

%
\begin{figure}[t!] \centering
	\begin{tabular}{c}
		\hspace{-0mm}
		\includegraphics[width=14.4cm]{optimizer_cmp/fnn_mnist_global_optimizer_cmp}
		\\       
		(a) Global baseline \\
		\hspace{-0mm}
		\includegraphics[width=14.4cm]{optimizer_cmp/fnn_mnist_local_optimizer_cmp}
		\\          
		(b) Individual baseline
	\end{tabular} \vspace{-2mm}
	\caption{The role of optimizers on MNIST FC networks. The transparent dots indicate SGD results, and opaque dots indicate Adam results. Adam generally yields higher intrinsic dimensions because higher baselines are achieved, especially when individual baselines in (b) are used.
Note that the Adam points are slightly different between
\figref{fnn_mnist_dim_global_crop.pdf} and \figref{fnn_mnist_dim_local_crop.pdf}, 
because in the former we average over three runs, and in the latter we show one run each for all optimization methods.}	

	\label{fig:fnn_mnist_optimizer}
	\vspace{-0mm}
\end{figure}







\section{Additional Reinforcement Learning results and details}
\seclabel{si_rl}

\subsection{DQN experiments}
\seclabel{si_rl_dqn}
%
%
\begin{figure}[h!] \centering
	\begin{tabular}{ccc}
		\hspace{-5mm}
		\includegraphics[width=5.0cm]{dqn_cp/CP_v0_reward.pdf}
		&
		\hspace{-6mm}
		\includegraphics[width=5.0cm]{dqn_cp/CP_v2_reward.pdf}
		&
		\hspace{-6mm}
		\includegraphics[width=5.0cm]{dqn_cp/CP_v3_reward.pdf}
		\\          
		(a) CartPole  ($\dintn = 25$)&	
		(b) Pole ($\dintn = 23$)&	
		(c) Cart ($\dintn = 7$)
	\end{tabular} \vspace{-2mm}
	\caption{Subspace training of DQN on CartPole game. Shown as dots are rewards collected through a game run averaged over the last 100 episodes, 	under each subspace training of DQN, and each game environment. The line connects mean rewards across different $d$s.}
	\label{fig:dqn_cartpole}
	\vspace{-0mm}
\end{figure}


\paragraph{DQN on Cartpole} We start with a simple classic control game $\mathtt{CartPole\!-\!v0}$ in OpenAI Gym~\citep{openai_gym}. A pendulum starts upright, and the goal is to prevent it from falling over. The system is controlled by applying a force of LEFT or RIGHT to the cart. 
The full game ends when one of two failure conditions is satisfied: the cart moves more than 2.4 units from the center (where it started), or the pole is more than 15 degrees from vertical (where it started).
A reward of +1 is provided for every time step as long as the game is going. 
We further created two easier environments $\mathtt{Pole}$ and $\mathtt{Cart}$, each confined by one of the failure modes only.

A DQN is used, where the value network is parameterized by an FC ($L=2, W=400$). For each subspace $d$ at least 5 runs are conducted, the mean of which is used to computed $\dintn$, and the baseline is set as 195.0\footnote{$\mathtt{CartPole\!-\!v0}$ is considered as ``solved'' when the average reward over the last 100 episodes is 195}. The results are shown in~\figref{dqn_cartpole}. The solid line connects mean rewards within a run over the last 100 episodes, across different $d$s. Due to the noise-sensitiveness of RL games the course is not monotonic any more. The intrinsic dimension for $\mathtt{CartPole}$, $\mathtt{Pole}$ and $\mathtt{Cart}$ is  $\dintn = 25$, $23$ and $7$, respectively. This reveals that the difficulty of optimization landscape of these games is remarkably low, as well as interesting insights such as driving a cart is much easier than keeping a pole straight, the latter being the major cause of difficulty when trying to do both.



\subsection{Evolutionary Strategies (ES) complete results}
\seclabel{si_rl_es}
%\paragraph{ES Training Details} 
We carry out with ES 3 RL tasks: $\mathtt{InvertedPendulum\!-\!v1}$, $\mathtt{Humanoid\!-\!v1}$, $\mathtt{Pong\!-\!v0}$. The hyperparameter settings for training are in Table \ref{tab:rl-hyperparams}.\seclabel{si_rl_training}

\begin{table}[h]
\centering
\begin{tabular}{c|ccccc}
         & $\ell_2$ penalty & Adam LR & ES $\sigma$ & Iterations \\
\hline
$\mathtt{InvertedPendulum\!-\!v1}$ & $1 \times 10^{-8}$ & $3 \times 10^{-1}$ & $2 \times 10^{-2}$ & 1000 \\
 $\mathtt{Humanoid\!-\!v1}$  & $5 \times 10^{-3}$ & $3 \times 10^{-2}$ & $2 \times 10^{-2}$ & 2000 \\
$\mathtt{Pong\!-\!v0}$ & $5 \times 10^{-3}$ & $3 \times 10^{-2}$ & $2 \times 10^{-2}$ & 500 
\end{tabular}
\caption{Hyperparameters used in training RL tasks using ES. $\sigma$ refers to the parameter perturbation noise used in ES. Default Adam parameters of $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1 \times 10^{-7}$ were used.}
\label{tab:rl-hyperparams}
\end{table}



\paragraph{Inverted pendulum}

The $\mathtt{InvertedPendulum\!-\!v1}$ environment uses the MuJoCo physics simulator~\citep{todorov2012mujoco} to instantiate the same problem as $\mathtt{CartPole\!-\!v0}$ in a realistic setting. We expect that even with richer environment dynamics, as well as a different RL algorithm -- ES -- the intrinsic dimensionality should be similar. As seen in \figref{es_results}, the measured intrinsic dimensionality $\dintn = 4$ is of the same order of magnitude, but smaller. Interestingly, although the environment dynamics are more complex than in $\mathtt{CartPole\!-\!v0}$, using ES rather than DQN seems to induce a simpler objective landscape.

%\figgp{InvertedPendulum-v1_crop.pdf}{.4}{big_inverted_pendulum.png}{.45}{\textbf{Left:} \texttt{InvertedPendulum-v1} trained using ES, on a 2-layer network with 20 hidden units per layer. \textbf{Right:} Screenshot of the environment.}

\paragraph{Learning to walk}

A more challenging problem is $\mathtt{Humanoid\!-\!v1}$ in MuJoCo simulator. Intuitively, one might believe that learning to walk is a more complex task than classifying images. Our results show the contrary -- that the learned intrinsic dimensionality of $\dintn = 700$ is similar to that of MNIST on a fully-connected network ($\dintn = 650$) but significantly less than that of even a convnet trained on CIFAR-10 ($\dintn = 2,500$). \figref{es_results} shows the full results. Interestingly, we begin to see training runs reach the threshold as early as $d = 400$, with the median performance steadily increasing with $d$.

%\figgp{Humanoid-v1_crop.pdf}{.4}{humanoid.jpg}{.35}{\textbf{Left:} \texttt{Humanoid-v1} trained using ES, on a 2-layer network with 256 hidden units per layer. \textbf{Right:} Screenshot of the environment.}

\paragraph{Atari Pong}

Finally, using a base convnet of approximately $D = 1M$ in the $\mathtt{Pong\!-\!v0}$  pixels-to-actions environment (using 4-frame stacking). The agent receives an image frame (size of  $210\times160\times3$) and the action is to move the paddle UP or DOWN. We were able to determine $\dintn = 6,000$.

%\figgp{AtariPong_crop.pdf}{.4}{pong.png}{.45}{\textbf{Left:} \texttt{Pong-v0} trained using ES, on a CONV-BN-CONV-BN-FC-BN-FC network. \textbf{Right:} Screenshot of the environment.}



%\begin{figure}[t!] \centering
%        \begin{tabular}{c c c}
%                \hspace{-0mm}
%                \includegraphics[width=4.4cm]{fnn_mnist_time_sparse_dense}
%                &        \hspace{-4mm}
%                \includegraphics[width=4.4cm]{fnn_mnist_acc_sparse_dense}
%                & \hspace{-4mm}
%                \includegraphics[width=4.4cm]{fnn_mnist_nll_sparse_dense}
%                \\
%                \hspace{-7mm}
%                \small{(a) Running time} &
%                \small{(b) Testing Accuracy}	&
%                \small{(c) Testing NLL}
%        \end{tabular} \vspace{-2mm}
%        \caption{{\small A comparison of projection matrices: dense {\it \vs} sparse. }}
%        \label{fig:pro_mat}
%        \vspace{-3mm}
%\end{figure}


%\begin{figure}[t!] \centering
%        \begin{tabular}{c c c}
%                \hspace{-0mm}
%                \includegraphics[width=4.4cm]{fnn_dim_global_standard}
%                &        \hspace{-4mm}
%                \includegraphics[width=4.4cm]{fnn_dim_global_ni}
%                & \hspace{-4mm}
%                \includegraphics[width=4.4cm]{fnn_dim_global_nl}
%                \\
%                \hspace{-7mm}
%                \small{(a) Standard} &
%                \small{(b) Perturbed pixels}	&
%                \small{(c) Perturbed labels}
%        \end{tabular} \vspace{-2mm}
%        \caption{A comparison of different version of MNIST using FC. }
%        \label{fig:fnn_mnist_ni}
%        \vspace{-3mm}
%\end{figure}


%\begin{figure}[t!] \centering
%        \begin{tabular}{c c}
%                \hspace{-0mm}
%                \includegraphics[width=4.4cm]{lenet_mnist_l2_ni_lr_01_train_test_acc}
%                &        \hspace{-4mm}
%                \includegraphics[width=4.4cm]{lenet_mnist_l2_ni_lr_01_train_test_nll}
%                \\
%                \hspace{-7mm}
%                \small{(a) Accuracy} &
%                \small{(b) NLL}
%        \end{tabular} \vspace{-2mm}
%        \caption{{\small A comparison of shuffled pixel version of MNIST using LeNet (Weight decay). }}
%        \label{fig:lenet_mnist_l2_ni_lr_01_train_test}
%        \vspace{-3mm}
%\end{figure}










%\begin{figure}[t!] \centering
%        \begin{tabular}{c c}
%                \hspace{-0mm}
%                \includegraphics[width=5.9cm]{lenet_cifar_l2_train_test_acc}
%                &        \hspace{4mm}
%                \includegraphics[width=5.9cm]{lenet_cifar_l2_train_test_nll}
%                \\
%                \hspace{-7mm}
%                \small{(a) Accuracy} &
%                \small{(b) NLL}\\
%                \includegraphics[width=5.9cm]{lenet_cifar_l2_dp05_train_test_acc}
%                &        \hspace{4mm}
%                \includegraphics[width=5.9cm]{lenet_cifar_l2_dp05_train_test_nll}
%                \\
%                \hspace{-7mm}
%                \small{(c) Accuracy (Dropout: 0.5)} &
%                \small{(d) NLL (Dropout: 0.5)}\\
%        \end{tabular} \vspace{-2mm}
%        \caption{{\small Results on CIFAR-10 using LeNet. }}
%        \label{fig:lenet_cifar}
%        \vspace{-6mm}
%\end{figure}


%\begin{figure}[t!] \centering
%        \begin{tabular}{c c}
%                \hspace{-0mm}
%                \includegraphics[width=5.9cm]{exp_PL/PL_train_acc_base}
%                &        \hspace{4mm}
%                \includegraphics[width=5.4cm]{exp_PL/PL_test_acc_base}
%                \\
%                \hspace{-7mm}
%                \small{(a) train acc: base method} &
%                \small{(b) test acc: base method}\\
%                \includegraphics[width=5.9cm]{exp_PL/PL_train_acc_1500}
%                &        \hspace{4mm}
%                \includegraphics[width=5.4cm]{exp_PL/PL_model_params}
%                \\
%                \hspace{-7mm}
%                \small{(c) train acc: $p=1500$} &
%                \small{(d) model size}\\
%        \end{tabular} \vspace{-2mm}
%        \caption{{\small Results of shuffled label version of MNIST using FC (500 epoch and lr=0.1) }}
%        \label{fig:lenet_mnist_pl}
%        \vspace{-6mm}
%\end{figure}



\rl{HERE}

\section{Three Methods of random projection}
\seclabel{scaling}

\newcommand{\bigO}{\mathcal{O}}

Scaling the random subspace training procedure to large problems requires an efficient way to map from $\R^d$ into a random $d$-dimensional subspace of $\R^D$ that does not necessarily include the origin. Algebraically, we need to left-multiply a vector of parameters $v \in \R^d$ by a random matrix $M \in \R^{D \times d}$, whose columns are orthonormal, then add an offset vector $\theta_0 \in \R^D$. If the low-dimensional parameter vector in $\R^d$ is initialized to zero, then specifying an offset vector is equivalent to choosing an initialization point in the original model parameter space $\R^D$.

A \naive approach to generating the random matrix $M$ is to use a dense $D \times d$ matrix of independent standard normal entries, then scale each column to be of length 1. The columns will be approximately orthogonal if $D$ is large because of the independence of the entries. Although this approach is sufficient for low-rank training of models with few parameters, we quickly run into scaling limits because both matrix-vector multiply time and storage of the matrix scale according to $\bigO(Dd)$. We were able to successfully determine the intrinsic dimensionality of MNIST ($d$=225) using a LeNet ($D$=44,426), but were unable to increase $d$ beyond 1,000 when applying a LeNet ($D$=62,006) to CIFAR-10, which did not meet the performance criterion to be considered the problems intrinsic dimensionality.

Random matrices need not be dense for their columns to be approximately orthonormal. In fact, a method exists for ``very sparse'' random projections~\citep{li2006verysparse}, which achieves a density of $\frac{1}{\sqrt{D}}$. To construct the $D \times d$ matrix, each entry is chosen to be nonzero with probability $\frac{1}{\sqrt{D}}$. \later{from Zoubin: ``related to Rademacher vectors?''} If chosen, then with equal probability, the entry is either positive or negative with the same magnitude in either case. The density of $\frac{1}{\sqrt{D}}$ implies $\sqrt{D}d$ nonzero entries, or $\bigO(\sqrt{D}d)$ time and space complexity. Implementing this procedure allowed us to find the intrinsic dimension of $d$=2,500 for CIFAR-10 using a LeNet mentioned above. Unfortunately, when using Tensorflow's \texttt{SparseTensor} implementation we did not achieve the theoretical $\sqrt{D}$-factor improvement in time complexity (closer to a constant 10x). Nonzero elements also have a significant memory footprint of 24 bytes, so we could not scale to larger problems with millions of model parameters and large intrinsic dimensionalities.

We need not explicitly form and store the transformation matrix. The Fastfood transform~\citep{le2013fastfood} was initially developed as an efficient way to compute a nonlinear, high-dimensional feature map $\phi(x)$ for a vector $x$. A portion of the procedure involves implicitly generating a $D \times d$ matrix with approximately uncorrelated standard normal entries, using only $\bigO(D)$ space, which can be multiplied by $v$ in $\bigO(D\log{d})$ time using a specialized method. The method relies on the fact that Hadamard matrices multiplied by Gaussian vectors behave like dense Gaussian matrices. In detail, to implicitly multiply $v$ by a random \textit{square} Gaussian matrix $M$ with side-lengths equal to a power of two, the matrix is factorized into multiple simple matrices: $M = H G \Pi H B$, where $B$ is a random diagonal matrix with entries +-1 with equal probability, $H$ is a Hadamard matrix, $\Pi$ is a random permutation matrix, and $G$ is a random diagonal matrix with independent standard normal entries. Multiplication by a Hadamard matrix can be done via the Fast Walsh-Hadamard Transform in $\bigO(d\log{d})$ time and takes no additional space. The other matrices have linear time and space complexities. When $D > d$, multiple independent samples of $M$ can be stacked to increase the output dimensionality. When $d$ is not a power of two, we can zero-pad $v$ appropriately. Stacking $\frac{D}{d}$ samples of $M$ results in an overall time complexity of $\bigO(\frac{D}{d}d\log{d})$ = $\bigO(D\log{d})$, and a space complexity of $\bigO(\frac{D}{d}d)$ = $\bigO(D)$. In practice, the reduction in space footprint allowed us to scale to much larger problems, including the Pong RL task using a 1M parameter convolutional network for the policy function.

Table \ref{tab:matrix-comparison} summarizes the performance of each of the three methods theoretically and empirically.

\begin{table}[h]
\centering
\begin{tabular}{c|ccccc}
         & Time complexity & Space complexity & $D$ = 100k & $D$ = 1M & $D$ = 60M \\
\hline
Dense    & $\bigO(Dd)$         & $\bigO(Dd)$          & 0.0169 s & 1.0742 s* & 4399.1 s* \\
Sparse   & $\bigO(\sqrt{D}d)$  & $\bigO(\sqrt{D}d)$   & 0.0002 s & 0.0019 s & 0.5307 s* \\
Fastfood & $\bigO(D\log{d})$   & $\bigO(D)$           & 0.0181 s & 0.0195 s & 0.7949 s

% Forward
% Dense    & $\bigO(Dd)$         & $\bigO(Dd)$          & 0.0085 s & 0.5362 s & 2196.0 s \\
% Sparse   & $\bigO(\sqrt{D}d)$  & $\bigO(\sqrt{D}d)$   & 0.0001 s & 0.0010 s & 0.0530 s \\
% Fastfood & $\bigO(D\log{d})$   & $\bigO(D)$           & 0.0031 s & 0.0059 s & 0.3893 s
% Backward
% Dense    & $\bigO(Dd)$         & $\bigO(Dd)$          & 0.0084 s & 0.5380 s & 2203.1 s \\
% Sparse   & $\bigO(\sqrt{D}d)$  & $\bigO(\sqrt{D}d)$   & 0.0001 s & 0.0009 s & 0.4777 s \\
% Fastfood & $\bigO(D\log{d})$   & $\bigO(D)$           & 0.0150 s & 0.0136 s & 0.4056 s

\end{tabular}
\caption{Comparison of theoretical complexity and average duration of a forward+backward pass through $M$ (in seconds). $d$ was fixed to 1\% of $D$ in each measurement. $D$ = 100k is approximately the size of an MNIST fully-connected network, and $D$ = 60M is approximately the size of AlexNet. The Fastfood timings are based on a Tensorflow implementation of the Fast Walsh-Hadamard Transform, and could be drastically reduced with an efficient CUDA implementation. Asterisks mean that we encountered an out-of-memory error, and the values are extrapolated from the largest successful run (a few powers of two smaller). For example, we expect sparse to outperform Fastfood if it didn't run into memory issues.}
\label{tab:matrix-comparison}
\end{table}


Figure \ref{tab:matrix-comparison} compares the computational time for direct and subspace training (various projections) methods for each update. Our subspace training is more computational expensive, because the subspace training method has to propagate the signals through two modules: the layers of neural networks, and the projection between two spaces. The direct training only propagates signals in the layers of neural networks. We have made efforts to reduce the extra computational cost. For example, the sparse projection less than doubles the time cost for a large range of subspace dimensions. 

 \figgp{mnist_100k_crop.pdf}{.49}{mnist_1m_crop.pdf}{.49}{MNIST compute time for direct \vs various projection methods for 100k parameters (left) and 1M parameters (right). }







\vspace{-0mm}
\section{Additional CIFAR-10 Results}
\seclabel{si_cifar}

\paragraph{FC networks}
We consider the CIFAR-10 dataset and test the same set of FC and LeNet architectures as on MNIST. 
For FC networks,  $\dintn$ values for all 20 networks are shown in \figref{fnn_cifar_dim} (a) plotted against the native dimension $D$ of each network; $D$ changes by a factor of 12.16 between the smallest and largest networks, but $\dintn$ changes over this range by a factor of 5.0.
However, much of this change is due to change of baseline performance.
In \figref{fnn_cifar_dim} (b), we instead compute the intrinsic dimension with respect to a global baseline: 50\% validation accuracy. $\dintn$ changes over this range by a factor of 1.52. This indicates that various FC networks share similar intrinsic dimension ($\dintn=5000 \sim 8000$) to achieve the same level of task performance. For LeNet ($D=62,006$), the validation accuracy \vs subspace dimension $d$ is shown in~\figref{conv_cifar} (b), the corresponding $\dintn=2900$. It yields a compression rate of 5\%, which is 10 times larger than LeNet on MNIST. It shows that CIFAR-10 images are significantly more difficult to be correctly classified than MNIST. In another word, CIFAR-10 is a harder problem than MNIST, especially given the fact that the notion of ``problem solved'' (baseline performance) is defined as 99\% accuracy on MNIST and 58\% accuracy on CIFAR-10. On the CIFAR-10 dataset, as $d$ increases, subspace training tends to overfitting; we study the role of subspace training as a regularizer below.

%
\begin{figure}[t!] \centering
	\vspace{-5mm}
	\begin{tabular}{c}
		\hspace{-0mm}
		\includegraphics[width=13.4cm]{fnn_cifar_dim_local}
		\\          
		(a) Individual baseline		\\
		\hspace{-0mm}
		\includegraphics[width=13.4cm]{fnn_cifar_dim_global}
		\\       
		(b) Global baseline 
	\end{tabular} \vspace{-2mm}
	\caption{Intrinsic dimension of FC networks with various width and depth on the CIFAR-10 dataset. In (b), we use a simple 50\% baseline globally.}
	\label{fig:fnn_cifar_dim}
	\vspace{-3mm}
\end{figure}


\paragraph{ResNet \vs LeNet}
We test ResNets, compare to LeNet, and find they make efficient use of parameters.
We adopt the smallest 20-layer structure of ResNet with 280k parameters, and find out in~\figref{conv_cifar} (b) that it reaches LeNet baseline with $\dintn=1000 \sim 2000$ (lower than the $\dintn$ of LeNet), while takes a larger $\dintn$ ($20,000 \sim 50,000$) to reach reach its own, much higher baseline.
%
\begin{figure}[h!] \centering
	\begin{tabular}{c}
		\hspace{-3mm}
		\includegraphics[width=8.0cm]{dim_cifar_fnn_w2_std_xlim} \\
		(a) FC ($W=200$, $L=2$)
		\\		
		\includegraphics[width=8.0cm]{dim_cifar_lenet5_std_xlim}\\
		(b) LeNet 
		\\
		\includegraphics[width=8.0cm]{cifar_resnet_crop} \\
		(c) ResNet		\\          
	\end{tabular} \vspace{-2mm}
	\caption{Validation accuracy of an FC network, LeNet and ResNet on CIFAR with different subspace dimension $d$. In (a)(b), the variance of validation accuracy and measured  $\dintn$ are visualized as the blue vertical and red horizontal error bars, respectively. Subspace method surpasses the 90\% baseline on LeNet at $d$ between 1000 and 2000,  90\% of ResNet baseline between 20k and 50k.}
	\label{fig:conv_cifar}
	\vspace{-2mm}
\end{figure}
%


\paragraph{The role of regularizers} Our subspace training can be considered as a regularization scheme, as it restricts the solution set. We study and compare its effects with two traditional regularizers with an FC network ($L\!\!=\!\!2$, $W\!\!=\!\!200$) on CIFAR-10 dataset, including $\ell_2$ penalty on the weights (\ie weight decay) and Dropout.

%\begin{minipage}{1.00\linewidth}
	\begin{itemize}
		\vspace{1mm}
		\item {\bf $\ell_2$  penalty}~~
		Various amount of $\ell_2$  penalty from $\{10^{-2}, 10^{-3}, 5 \!\times\! 10^{-4}, 10^{-4}, 10^{-5}, 0\}$ are considered. The accuracy and negative log-likelihood (NLL) are reported in~\figref{fnn_cifar_l2} (a) (b), respectively. As expected, larger amount of weight decay reduces the gap between training and testing performance for both direct and subspace training methods, and eventually closes the gap (\ie $\ell_2$ penalty~=~0.01). Subspace training itself exhibits strong regularization ability, especially when $d$ is small, at which the performance gap between training and testing is smaller. 
		\item  {\bf Dropout}~~
		Various dropout rates from $\{0.5, 0.4, 0.3, 0.2, 0.1, 0\}$ are considered. The accuracy and NLL are reported in~\figref{fnn_cifar_dropout}.  Larger dropout rates reduce the gap between training and testing performance for both direct and subspace training methods. 
		When observing testing NLL, subspace training tends to overfit the training dataset less.		
		\item  {\bf Subspace training as implicit regularization} Subspace training method performs implicit regularization, as it restricts the solution set. We visualized the testing NLL in~\figref{fnn_cifar_reg}.
		 Subspace training method outperforms direct method when $d$ is properly chosen (when $\ell_2$ penalty$<5 \!\times\! 10^{-4}$, or dropout rate $<0.1$ ), suggesting the potential of this method as a better alternative to traditional regularizers. When $d$ is large, the method also overfits the training dataset. Note that the these methods perform regularization in different ways: weight decay enforces the learned weights concentrating around zeros, while subspace training directly reduces the number of dimensions of the solution space.
		
		%LetNet:~\vspace{-0mm}
		%\item 
		%AlexNet: 
	\end{itemize}
%\end{minipage}

\begin{figure}
  \centering
	\vspace{-2mm}
	\begin{tabular}{c}
		\hspace{-5mm}
		\includegraphics[width=14.4cm]{fnn_cifar_l2_reg/fnn_cifar_l2_reg_acc_all}
		\\
		(a) Accuracy \\
		\hspace{-5mm}
		\includegraphics[width=14.4cm]{fnn_cifar_l2_reg/fnn_cifar_l2_reg_nll_all}
		\\
		(b) NLL   % \\
		% JBY: Removed this plot for now for lack of interpretation.
		%\includegraphics[width=5.0cm]{fnn_cifar_l2_reg/fnn_cifar_l2_reg_intrinsic_dim}
		%\\          
		%(c) Intrinsic dim $d$
	\end{tabular} \vspace{-2mm}
	\caption{Comparing regularization induced by $\ell_2$ penalty and subspace training. Weight decay interacts with $\dintn$ since it changes the objective landscapes through various loss functions. }
	\label{fig:fnn_cifar_l2}
	\vspace{-0mm}
\end{figure}

\begin{figure}
  \centering
	\vspace{-0mm}
	\begin{tabular}{c}
		\hspace{-5mm}
		\includegraphics[width=14.4cm]{fnn_cifar_dropout_reg/fnn_cifar_dropout_reg_acc_all}
		\\
		(a) Accuracy \\
		\hspace{-5mm}
		\includegraphics[width=14.4cm]{fnn_cifar_dropout_reg/fnn_cifar_dropout_reg_nll_all}
		\\
		(b) NLL   % \\
		% JBY: Removed this plot for now for lack of interpretation.
		%\includegraphics[width=5.0cm]{fnn_cifar_l2_reg/fnn_cifar_l2_reg_intrinsic_dim}
		%\\          
		%(c) Intrinsic dim $d$
	\end{tabular} \vspace{-2mm}
	\caption{Comparing regularization induced by Dropout and subspace training. Dropout interacts with $\dintn$ since it changes the objective landscapes through randomly removing hidden units of the extrinsic neural networks. }
	\label{fig:fnn_cifar_dropout}
	\vspace{-3mm}
\end{figure}

\begin{figure}
  \centering
	\vspace{-0mm}
	\begin{tabular}{c c}
		\hspace{-9mm}
		\includegraphics[width=7.4cm]{fnn_cifar_l2_reg/fnn_cifar_l2_reg_ability} &
		\hspace{-7mm}
		\includegraphics[width=7.4cm]{fnn_cifar_dropout_reg/fnn_cifar_dropout_reg_ability}
		\\
		(a) $\ell_2$ penalty & 	(b) Dropout   % \\
		% JBY: Removed this plot for now for lack of interpretation.
		%\includegraphics[width=5.0cm]{fnn_cifar_l2_reg/fnn_cifar_l2_reg_intrinsic_dim}
		%\\          
		%(c) Intrinsic dim $d$
	\end{tabular} \vspace{-2mm}
	\caption{Comparing regularization induced by $\ell_2$ penalty, Dropout and subspace training. The gray surface and black line indicate direct training.}
	\label{fig:fnn_cifar_reg}
	\vspace{-3mm}
\end{figure}



\section{ImageNet}
\seclabel{si_imagenet}

To investigate even larger problems, we attempted to measure $\dintn$ for an ImageNet classification network. We
use a relatively smaller network, SqueezeNet by \cite{iandola2016squeezenet}, with 1.24M parameters. Larger networks suffered from memory issues.
A direct training produces Top-1 accuracy of 55.5\%. We vary intrinsic dimension from {50k, 100k, 200k, 500k, 800k}, and record the validation accuracies as shown in \figref{imagenet_sqz.pdf}. The training of each intrinsic dimension takes about 6 to 7 days, distributed across 4 GPUs.
Due to limited time, training on ImageNet has not yet produced a reliable estimate for $\dintn$ except that it is over 500k.

\figp{imagenet_sqz.pdf}{.7}{Validation accuracy of SqueezeNet on ImageNet with different $d$. At $d=500k$ the accuracy reaches 34.34\%, which is not yet past the threshold required to estimate $\dintn$.}


\section{Investigation of Convolutional Networks}
\seclabel{sec_conv}

Since the learned $\dintn$ can be used as a robust measure to study the fitness of neural network architectures for specific tasks, we further apply it to understand the contribution of each component in convolutional networks for image classification task. The convolutional network is a special case of FC network in two aspects: local receptive fields and weight-tying. 
Local receptive fields force each filter to ``look'' only at a small, localized region of the image or layer below. Weight-tying enforces that each filter shares the same weights, which reduces the number of learnable parameters.
We performed control experiments to investigate the degree to which each component contributes.
Four variants of LeNet are considered:
\begin{minipage}{1.00\linewidth}
	\begin{itemize}
		\vspace{1mm}
		\item {\bf Standard LeNet}~~
		6 kernels ($5\times5$) --  max-pooling ($2\times2$) -- 16 kernels ($5\times5$)  -- max-pooling ($2\times2$) -- 120 FC  -- 84 FC -- 10 FC
		\item {\bf Untied-LeNet}~~ The same architecture with the standard LeNet is employed, 
		except that weights are unshared, \ie a different set of filters is applied at each different patch of the input. For example in Keras, the $\mathtt{LocallyConnected2D}$ layer is used to replace the $\mathtt{Conv2D}$ layer.
		\item {\bf FCTied-LeNet}~~The same set of filters is applied at each different patch of the input. we break local connections by applying filters to global patches of the input. Assume the image size is $H \times H$, the architecture is 6 kernels ($(2H\!-\!1)\times(2H\!-\!1)$) --  max-pooling ($2\times2$) -- 16 kernels ($(H\!-\!1)\times(H\!-\!1)$)  -- max-pooling ($2\times2$) -- 120 FC  -- 84 FC -- 10 FC. The padding type is  $\mathtt{same}$.
		\item {\bf FC-LeNet}~~ Neither local connections or tied weights is employed, we mimic LeNet with its FC implementation. The same number of hidden units as the standard LeNet are used at each layer.
	\end{itemize}
\end{minipage}

The results  are shown in Fig.~\ref{fig:lenet_variants} (a)(b). We set a crossing-line accuracy (\ie threshold) for each task, and investigate $\dintn$ needed to achieve it. For MNIST and CIFAR-10, the threshold is 90\% and 45\%, respectively. For the above LeNet variants, $\dintn=290, 600, 425, 2000$ on MNIST, and $\dintn=1000, 2750, 2500, 35000$ on CIFAR-10. Experiments show both tied-weights and local connections are important to the model. That tied-weights should matter seems sensible. However, models with maximal convolutions (convolutions covering the whole image) may have had the same intrinsic dimension as smaller convolutions, but this turns out not to be the case.

\begin{figure}[h!] \centering
	\begin{tabular}{cc}
		\hspace{-3mm}
		\includegraphics[width=7.0cm]{mnist_lenet_variants}
		&
		\hspace{-3mm}
		\includegraphics[width=7.0cm]{cifar_lenet_variants}
		\\          
		(a) MNIST &	
		(b) CIFAR-10
	\end{tabular} \vspace{-2mm}
	\caption{Validation accuracy of LeNet variants with different subspace dimension $d$. The conclusion is that convnets are more efficient than FC nets both due to local connectivity and due to weight tying.}
	\label{fig:lenet_variants}
	\vspace{-2mm}
\end{figure}
%



% \newpage
\section{Summarization of $\dintn$}
We summarize $\dintn$ of the objective landscape on all different problems and neural network architectures in Table~\ref{tab:summarized_d} and Fig.~\ref{fig:summarized_d}.  ``SP'' indicates shuffled pixel, and ``SL'' for shuffled label, and ``FC-5'' for a 5-layer FC. %By fixing the architecture as FC, we see that different intrinsic dimensions are obtained on various problems in different domains. Though the scaling factor of $D$ for FC is small (as a result that we fixed FC), the obtained $\dintn$ can change signicantly on different problems. 
$\dintn$ indicates the minimum number of dimensions of trainable parameters required to properly solve the problem, and thus reflects the difficulty level of problems. %Similar conclusions can be reached when fixing the architecture as LeNet.
%
%Further work could also identify better ways of creating subspaces for
%reparameterization: here we chose random linear subspaces, but one
%might carefully construct linear or non-linear subspaces to be even
%more efficient, or pre-scale or otherwise transform the direct
%parameters before projecting to or from the subspace to make
%projections more likely to intersect solutions.  Finally, as the field departs from
%single stack-of-layers image classification models toward
%larger and more heterogeneous networks \citep{ren-2015-faster-r-cnn:-towards,kaiser-2017-arXiv-one-model-to-learn-them}
%often composed of many modules and trained by many losses,
%methods --- like measuring intrinsic dimension --- that allow some automatic assessment of model components might provide much-needed greater understanding
%of individual black-box module properties.

\begin{table*}[h!]
	\centering
	\caption{Intrinsic dimension of different objective landscapes, determined by dataset and network.}
	\vskip 0.00in
	\hskip -0.02in
	% \scriptsize
	% \begin{tabular}{c|cccc|cccc}
	\begin{adjustbox}{scale=1.00,tabular=c|c|c|c,center}
		\hline
		Dataset  & Network & $D $ & $\dintn$ \\
		\hline\hline
MNIST & FC & 199210 & 750 \\
MNIST & LeNet & 44426 & 275 \\ 
CIFAR-10 & FC & 1055610 & 9000 \\ 
CIFAR-10 & LeNet & 62006 & 2900 \\  \hline
MNIST-SP & FC & 199210 & 750 \\ 
MNIST-SP & LeNet & 44426 & 650 \\ 
MNIST-SL-100\% & FC-5 & 959610 & 190000 \\ 
MNIST-SL-50\% & FC-5 & 959610 & 130000 \\ 
MNIST-SL-10\% & FC-5 & 959610 & 90000 \\  \hline
%MNIST & Untied-LeNet & 286334 & 600 \\ 
%MNIST & FC-LeNet & 3640574 & 2000 \\ 
%Cifar & Untied-LeNet & 658238 & 9000 \\ 
%Cifar & FC-LeNet & 16397726 & $>$100000 \\  \hline
ImageNet & SqueezeNet & 1248424 & $>$500000 \\ \hline
CartPole & FC & 199210 & 25 \\ 
Pole & FC & 199210 & 23 \\ 
Cart & FC & 199210 & 7 \\ \hline
Inverted Pendulum & FC & 562 & 4 \\ 
Humanoid & FC & 166673 & 700 \\ 
Atari Pong & ConvNet & 1005974 & 6000 \\ 

		\hline
		% \end{tabular}
	\end{adjustbox}
	\label{tab:summarized_d}
\end{table*}

\begin{figure}[h!] \centering
	\hspace{-3mm}
	\includegraphics[width=14.0cm]{d_int_road_map}
	\caption{Intrinsic dimension of the objective landscapes created by all combinations of \emph{dataset} and \emph{network} we tried in this paper.}
	\label{fig:summarized_d}
	\vspace{-2mm}
\end{figure}
%



%\clearpage
%\section{EXTRA INFORMATION / SCRATCH: INCLUDE MAYBE?}
%
%\jby{note from Zoubin: ``not needed''}
%
%\subsubsection{Preliminaries on Deep Neural Networks}
%
%Assume we are given data $\Dcal_{\mathtt{train}} = \{\dv_1, \cdots, \dv_N\}$, where $\dv_n \triangleq  (\xv_n, \yv_n) $. The input object/feature is $\xv_n \in \R^D $ and output can be
%$(\RN{1})$ discrete label $y_i \in \Ycal$ for classification problem, with $\Ycal$ being the output label space, or
%$(\RN{2})$ $y_i \in \R^O$ for regression problem, with $O$ being the dimension of output space.
%
%Deep neural networks (DNNs) characterize the relationship from $\xv$ to $\yv$ with parameters $\thetav \in \R^P$, where $P$ the dimension of parameter space. The parameters $\thetav$ are assigned a prior distribution $p(\thetav)$.  The corresponding data likelihood is $p(\Dcal_{\mathtt{train}} | \thetav) = \prod_{i=1}^N p(\dv_i | \thetav)$.
%%
%DNNs parameterize the form of relationship,
%as a composition of a set of nonlinear functions. Specifically, an
%$L$-layer DNN
%%
%\begin{align} \label{eq:transformations}
%p( \yv | \xv, \thetav) = \phi \big ( g_{\thetav_L}  \circ  \cdots  \circ  g_{\thetav_0} (\xv  )   \big )~,
%\end{align}
%%
%where $\circ$ denotes function composition, $g_{\thetav_{\ell}}$ is the non-linear function in the $\ell$-th layer, parameterized by $\thetav_{\ell}$. Thus, the entire parameter of interest is $\thetav = [\thetav_0, \cdots, \thetav_L]$.
%%
%The last layer $\phi$ can be
%$(\RN{1})$ a softmax function $\text{softmax}(\xv) \triangleq e^{\xv}/(\sum_{i}e^{x_i})$ for multi-class classifcation.
%or
%$(\RN{2})$ a Gaussian distribution in regression.
%
%
%
%Following Bayes rule, the posterior is $p(\thetav | \Dcal_{\mathtt{train}}) \propto p(\thetav) p(\Dcal_{\mathtt{train}} | \thetav)$.
%For testing dataset $D_{\mathtt{test}}$, given a test input  $\tilde{\xv}$ (with missing $\tilde{\yv}$),
%the uncertainty  learned in training is transferred to prediction, yielding the posterior predictive
%distribution in \eqref{eq:ppp}.
%%
%\vspace{-0mm}
%\begin{align}
%\hspace{-4mm}p(\tilde{\yv}|\tilde{\xv},\Dcal_{\mathtt{train}} )
%\!=\mathbb{E}_{p(\thetav| \Dcal_{\mathtt{train}} )}[p(\tilde{\yv}|\tilde{\xv},\thetav)]
%\!=\int_{\thetav} p( \tilde{\yv} | \tilde{\xv}, \thetav) p(\thetav | \Dcal_{\mathtt{train}} )~. %\mathrm{d} \thetav
%\label{eq:ppp}\hspace{-2mm}
%\end{align}
%
%Usually, $\thetav_{\mathtt{MAP}} = \arg \max_{\thetav} p(\tilde{\yv}|\tilde{\xv},\Dcal_{\mathtt{train}} )$ is used as a simple and efficient approximation to $p(\thetav | \Dcal_{\mathtt{train}})$ in stochastic optimzation.
%In complex real-world modeling,  simple parametric models are often
%not expressive enough for capable {\it representation} and robust {\it generalization}.
%
%\subsubsection{Low-Rank Basis Training}
%
%\begin{align}
%\hspace{-4mm} \thetav = \Mmat \vv ~. % \thetav
%\label{eq:lrb}
%\end{align}
%%
%where $\vv \in \R^d (d\le P)$ is the intrinsic representation of the model parameters $\thetav$, and  $ \Mmat \in \R^{P \times d}$ is the projection matrix.
%
%\jby{note from Zoubin: ``this part could be nice though.''}
%
%\paragraph{Formal Definitions}
%
%One definition of the intrinsic dimesion $d^*$ for training dataset $D_{\mathtt{train}}$ is:
%\begin{align}
%d^* = \arg\min_{d} p(\yv | \xv, \thetav _0 + \Mmat \Delta \vv ) \ge  \alpha p(\yv | \xv, \thetav_{\mathtt{MAP}}),~~
%d \in \{d: d\le P, d \in \mathbb{Z} \}
%\label{eq:def1}
%\end{align}
%%
%where $\alpha \in (0, 1]$ is precision hyper-parameter of our low-rank approximation to the performance of original optimal parameter $\thetav_{\mathtt{MAP}}$.
%
%\cl{\bf Discussion on the definitions of Intrinsic dimension}
%
%\cl{Q1: Where to define the intrinsic dimension?}
%
%A: Consider to define intrinsic dimensions for training $d_{\mathtt{train}}$ and testing  $d_{\mathtt{test}}$ seperately, and define the difference $d_{\mathtt{diff}}=d_{\mathtt{test}} - d_{\mathtt{train}}$ to characterize generalization. Note that $d_{ \mathtt{diff} }$ can be an indicator of various regularization methods, $d_{ \mathtt{train} }$ characterize the representation capability. $d_{ \mathtt{test} }$ characterizes all aspects.
%
%However, this may raise a potential issue in RL, where no training/testing is not used.
%
%\cl{Q2: What does the intrinsic dimension really characterize?}
%
%A: Note that $d( \thetav, \Dcal)$. For parameter, $\thetav \in \Thetamat \triangleq \{ \Thetamat_{\Amat} \}$ vary in two aspects: archictures (functional forms) $\Thetamat_{\Amat}$ (\eg $\Amat$ = citep, CNN, RNN \etc) and weight values in the specific $\Thetamat_{\Amat}$. For dataset/problem, $\Dcal = \{ \Dcal_{ \mathtt{train} }, \Dcal_{\mathtt{test} } \}$. Therefore, $d( \thetav, \Dcal)$ can be defined on different level by choosing specific $\thetav$ and $\Dcal$, to characterize different properties.
%
%\subsubsection{Bayesian Interpretation}
%
%
%\section{PLACEHOLDER AND MISC TEXT}
%
%
%
%
%\paragraph{Efficent Implemetation} Let's denote $\vv = \vv_0 +  \Delta \vv $ and $\thetav_0 = \Mmat \vv_0$, we have
%\begin{align}
%\thetav = \Mmat \vv = \Mmat  (\vv_0 +  \Delta \vv ) =  \Mmat \vv_0 + \Mmat  \Delta \vv   = \thetav _0 + \Mmat \Delta \vv
%\label{eq:lrb2}
%\end{align}
%%
%Eq. \eqref{eq:lrb2} allows us to leverage the existing effective weights initialization schemes for $\thetav_0$, \eg He~\citep{he2015delving}/Xavier~\citep{glorot2010understanding}.
%
%
%
%
%
%\paragraph{On the basic properties of neural networks}
%Multilayer feedforward networks are universal approximators~\citep{hornik1989multilayer}.





\later{SECTION: MISC NOTES}

\later{mention EWC, eigenvalues, how EWC requires approx to find number of meaningful parameters. Mention extension to EWC domain to solve catastrophic forgetting, but orthogonality is tricky.}

\later{add point: lower dim does not mean easier to train -- actually it's harder to train in many cases, and one can imagine very high dim scenarios that are easy (regression to single point), \vs low dim that are hard. Lower dim is about compressibility, about MDL, about complexity, but not in the cases accessible theoretically; it's about cases accessible practically, using current optimization methods, etc. May also be connections to VC dimension.}

\later{possibly add point: not even possible to really get best baseline perf. What about adversarially created black box scenarios with a single much better point?}

\later{work in somewhere: other ways of finding even lower dimensions and/or lower numbers of bits. Nonlinear projections. Search + mask for lucky/unlucky dims. Try measuring dimension per section or module of network (for big nets / complicated heterogeneous tasks).}


\end{document}


