 
\chapter{M-Estimation and MLE}
\label{chapter::m-mle} 

 
A wide range of statistics estimation problems can be formulated as an estimating equation:
$$
\bar{m}(W,b) = n^{-1}\sumn m(w_{i},b) = 0
$$
where $m(\cdot, \cdot)$ is a vector function with the same dimension as $b$, and $W = \left\{ w_{i}\right\} _{i=1}^{n}$ are the observed data. 
Let $\hat{\beta} $ denote the solution which is an estimator of $\beta$. Under mild regularity conditions,  $\hat{\beta} $ is consistent and asymptotically Normal \footnote{There are counterexamples in which $\hat{\beta} $ is inconsistent; see \citet{freedman1982inconsistent}. The examples in this book are all regular.}. 
This is the classical theory of M-estimation. I will review it below. See \citet{stefanski2002calculus} for a reader-friendly introduction that contains many interesting and important examples. The proofs below are not rigorous. 
See \citet{newey1994large} for the rigorous ones. 


\section{M-estimation}



I start with the simple case with IID data. 


\begin{theorem}
\label{theorem:sandwich-theorem-cov-iid}Assume that $W = \left\{ w_{i}\right\} _{i=1}^{n}$ are IID with the same distribution as $w$.
The true parameter $\beta\in\mathbb{R}^{p}$ is the unique solution of  
\[
E\left\{ m(w,b)\right\} =0,
\]
and the estimator $\hat{\beta}\in\mathbb{R}^{p}$ is the solution of
\[
\bar{m}(W,b)  =0 . 
\]
Under some regularity conditions, 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\rightarrow\N(0,B^{-1}MB^{-\T})
\]
in distribution, where 
$$
B   = -  \frac{\partial E\left\{ m(w,\beta) \right\} }{\partial b^{\T}},\quad 
%=   \left(\begin{array}{ccc}
% \frac{\partial E\left\{ m(w,\beta) \right\} }{\partial b_1}& \cdots & \frac{\partial E\left\{  m(w,\beta) \right\} }{\partial b_p}
%\end{array}\right) 
M = E\{  m(w,\beta) m(w,\beta)^{\T}\}  .
$$
\end{theorem}







\begin{myproof}{Theorem}{\ref{theorem:sandwich-theorem-cov-iid}}
I give a ``physics'' proof.  When I use approximations, I mean the
error terms are of higher orders under some regularity conditions. The
consistency follows from swapping the order of ``solving equation''
and ``taking the limit based on the law of large numbers'':
\begin{align*}
\lim_{n\rightarrow\infty}\hat{\beta} & =\lim_{n\rightarrow\infty}\left\{ \text{solve } \bar{m}(W,b)=0\right\} \\
 & =\text{solve } \left\{ \lim_{n\rightarrow\infty} \bar{m}(W,b) =0 \right\} \\
 & =\text{solve } \left[ E\left\{ m(w,b)\right\} =0 \right] \\
 &=\beta.
\end{align*}

The asymptotic Normality follows from three steps. First, from the Taylor expansion
$$
0  = \bar{m}(W,\hat{\beta}) \cong \bar{m}(W,\beta)
+ \frac{\partial \bar{m}(W,\beta)}{\partial b^{\T}} (\hat{\beta}-\beta)
$$
we obtain 
$$
 \sqrt{n}\left(\hat{\beta}-\beta\right) \cong  \left\{  -   \frac{\partial \bar{m}(W,\beta)}{\partial b^{\T}} \right\} ^{-1}\left\{ \frac{1}{\text{\ensuremath{\sqrt{n}}}}\sumn m(w_{i},\beta)\right\} .
$$
Second, the law of large numbers ensures that 
\[
- \frac{\partial \bar{m}(W,\beta)}{\partial b^{\T}}  
\rightarrow - \frac{\partial E\left\{ m(w,\beta) \right\} }{\partial b^{\T}} =B
\]
in probability, and the CLT ensures that
$$
n^{-1/2}\sumn m(w_{i},\beta)\rightarrow\ \N(0,M)
$$
in distribution. 
Finally, Slutsky's theorem implies the result. 
\end{myproof}


The above result also holds with independent but non-IID data. 


\begin{theorem}
\label{theorem:sandwich-theorem-cov-ind}Assume that $\left\{ w_{i}\right\} _{i=1}^{n}$ are independent observations.
The true parameter $\beta\in\mathbb{R}^{p}$ is the unique solution to  
\[
E\left\{\bar{m}(W,b)\right\} =0,
\]
 and the estimator $\hat{\beta}\in\mathbb{R}^{p}$ is the solution to
\[
\bar{m}(W,b)  =0,
\]
Under some regularity conditions, 
\[
\sqrt{n}\left(\hat{\beta}-\beta\right)\rightarrow\N(0,B^{-1}MB^{-\T})
\]
in distribution, where 
$$
B  = - \lim_{n\rightarrow \infty } n^{-1} \sumn  
 \frac{\partial E\left\{ m(w_i,\beta) \right\} }{\partial b^{\T}} ,\quad 
%= - n^{-1} \sumn  \left(\begin{array}{ccc}
% \frac{\partial E\left\{ m(w_i,\beta) \right\} }{\partial b_1}& \cdots & \frac{\partial E\left\{  m(w_i,\beta) \right\} }{\partial b_p}
%\end{array}\right) 
M =   \lim_{n\rightarrow \infty } n^{-1} \sumn \cov\{ m(w_{i},\beta) \}  .
$$
\end{theorem}


For both cases above, we can further construct the following sandwich covariance estimator:
$$
%\hat{\cov}(\hat{\beta}) =  
\left( \sumn  
 \frac{\partial E \{ m(w_i, \hat\beta) \} }{\partial b^{\T}}  \right)^{-1} 
 \left( \sumn  m(w_{i},  \hat\beta) m(w_{i},  \hat\beta)^{\T}  \right)
  \left( \sumn  
 \frac{\partial E\{ m(w_i,  \hat\beta) \} }{\partial b}  \right)^{-1} 
$$
For non-IID data, the above covariance estimator can be conservative unless $ E\{ m(w_i, \beta) \}  = 0$ for all $i=1, \ldots, n$. 



\begin{example}
\label{eg::sample-variance}
Assume that $x_1,\ldots, x_n \iidsim x$ with mean $\mu$. The sample mean $\bar{x}$ solves the estimating equation
$$
n^{-1} \sumn (x_i -  \mu) = 0.
$$
Apply Theorem \ref{theorem:sandwich-theorem-cov-iid} to obtain $B = -1$ and $M = \sigma^2$, which imply
$$
\sqrt{n} (\bar{x}  -\mu   ) \rightarrow  \N( 0, \sigma^2)
$$
in distribution, the standard CLT for the sample mean. Moreover, the sandwich covariance estimator is
$$
\hat{V}  = 
n^{-2} \sumn ( x_i -  \bar{x}  )^2 ,
$$
which equals the sample variance of $x$, multiplied by $(n-1)/n^2 \approx 1/n$. This is a standard result. 

If we only assume that $x_1,\ldots, x_n$ are independent with the same mean $\mu$ but possibly different variances $\sigma_i^2\ (i=1,\ldots, n)$, the sample mean $\bar{x}$ is still a reasonable estimator for $\mu$ which solves the same estimating equation above. Moreover, the sandwich covariance estimator $\hat{V} $ is still a consistent estimator for the true variance of $\bar{x}$. This is less standard. 


If we assume that $x_i \sim [\mu_i, \sigma_i^2]$ are independent, we can still use $\bar{x}$ to estimate $\mu = n^{-1} \sumn \mu_i $. The estimating equation remains the same as above. The sandwich covariance estimator $\hat{V}$ becomes conservative since $E(x_i - \mu) \neq 0$ in general. 

Problem \ref{hw-math4::mean} gives more details.
\end{example}



\begin{example}
\label{eg::sample-meanandvariance}
Assume that $x_1,\ldots, x_n \iidsim x$ with mean $\mu$ and variance $\sigma^2$. The sample mean and variance $(\bar{x}, \hat{\sigma}^2)$ jointly solves the estimating equation with
$$
m(x, \mu, \sigma^2) = \begin{pmatrix}
x - \mu \\
(x-\mu)^2 - \sigma^2
\end{pmatrix}
$$
ignoring the difference between $n$ and $n-1$ in the definition of the sample variance. Apply Theorem \ref{theorem:sandwich-theorem-cov-iid} to obtain 
$$
B = \begin{pmatrix}
-1 & 0 \\ 
0 & -1
\end{pmatrix},\quad
M = \begin{pmatrix}
\sigma^2 & \mu_3 \\
\mu_3 & \mu_4 - \sigma^4
\end{pmatrix}
$$
where $\mu_k = E\{ (x-\mu)^k\}$, which imply 
$$
\sqrt{n} \begin{pmatrix}
\bar{x} - \mu \\
\hat{\sigma}^2 - \sigma^2
\end{pmatrix} \rightarrow 
\N(0, M)
$$
in distribution. 
\end{example}




\begin{example}
Assume that $(x_i, y_i)_{i=1}^n$ are IID draws from $(x,y)$ with mean $(\mu_x, \mu_y)$. Use $\bar{x} / \bar{y}$ to estimate $\gamma = \mu_x / \mu_y$. It satisfies the estimating equation with 
$$
m(x,y,\gamma) = x - \gamma y .
$$
Apply Theorem \ref{theorem:sandwich-theorem-cov-iid} to obtain $B = -\mu_y$ and $M = \var(x-\gamma y)$, which imply 
$$
\sqrt{n} \left(   \frac{ \bar{x} }{ \bar{y} }  - \frac{ \mu_x }{   \mu_y }  \right)
\rightarrow
\N\left(0,   \frac{  \var(x-\gamma y) }{  \mu_y^2  }   \right)
$$
if $\mu_y \neq 0$. 
\end{example}


 
\section{Maximum likelihood estimator}\label{sec::mle}

As an important application of Theorem \ref{theorem:sandwich-theorem-cov-iid}, we can derive the asymptotic properties of the maximum likelihood estimator (MLE) $\hat{\theta}$ under \textup{IID} sampling from a parametric model 
$$
 y_1,\ldots, y_n \iidsim  f(y\mid \theta).
 $$
 The MLE satisfies the following estimating equation:
\begin{eqnarray}
E\left\{  \frac{\partial \log f(y\mid \theta)}{ \partial \theta}   \right\} = 0,
\label{eq::bartlett-vector1}
\end{eqnarray}
which is Bartlett's first identity. 
Under regularity conditions, $\sqrt{n} (\hat{\theta} - \theta)$ converges in distribution to Normal with mean zero and covariance $B^{-1} M B^{-1}$, where
\begin{eqnarray*}
B 
&=& - \frac{\partial }{\partial \theta^{\T}} E\left\{  \frac{\partial \log f(y\mid \theta)}{ \partial \theta}   \right\} \\
&=&
E\left\{ -  \frac{\partial^2 \log f(y\mid \theta)}{ \partial \theta  \partial \theta^{\T} }   \right\} 
\end{eqnarray*}
is called the Fisher information matrix, denoted by $I(\theta)$,
and
$$
M = E\left\{  \frac{\partial \log f(y\mid \theta)}{ \partial \theta}  \frac{\partial \log f(y\mid \theta)}{ \partial \theta^{\T}}   \right\}
$$
is sometimes also called the Fisher information matrix, denoted by  $J(\theta)$.


If the model is correct,
Bartlett's second identity ensures that 
\begin{eqnarray}
I(\theta)=J(\theta),
\label{eq::bartlett-vector2}
\end{eqnarray}
and therefore  $\sqrt{n} (\hat{\theta} - \theta)$ converges in distribution to Normal with mean zero and covariance $I(\theta)^{-1} = J(\theta)^{-1}$. So a covariance matrix estimator for the MLE is $I_n(\hat{\theta})^{-1}$ or $J_n(\hat{\theta}) ^{-1}$, where 
$$
I_n(\hat{\theta}) =  - \sumn   \frac{\partial^2 \log f(y_i\mid \hat{\theta} )}{ \partial \theta \partial \theta^{\T} }  
$$   
and
$$
J_n(\hat{\theta}) = \sumn \frac{\partial \log f(y_i \mid \hat{\theta})}{ \partial \theta}  \frac{\partial \log f(y_i \mid \hat{\theta})}{ \partial \theta^{\T}}. 
$$
\citet{fisher1925theory} pioneered the asymptotic theory of the MLE under correctly specified models. 

If the model is incorrect, $I(\theta)$ can be different from $J(\theta)$ but the sandwich covariance $B^{-1} M B^{-1}$ still holds. So a covariance matrix estimator for the MLE under misspecification is 
$$
I_n(\hat{\theta})^{-1} J_n(\hat{\theta}) I_n(\hat{\theta})^{-1}.
$$
\citet{huber::1967} studied the asymptotic theory of the MLE under correctly specified models. He focused on the case with IID observations and pioneered
 the sandwich covariance formula. 
Perhaps a more important question is what is the parameter if the model is misspecified.  The population analog of the MLE is the minimizer of
$$
- E \{  \log f(y\mid \theta) \} ,
$$
where the expectation is over true but unknown distribution $y \sim g(y)$.  We can rewrite the population objective function as
$$
 -\int g(y) \log f(y\mid \theta)  \diff y
= \int g(y) \log  \frac{ g(y) }{ f(y\mid \theta)  }  \diff y
-\int g(y) \log  g(y)  \diff y .
$$
The first term is called the Kullback--Leibler divergence or relative entropy of $g(y)$ and $f(y\mid \theta)$, whereas the second term is called the entropy of $g(y)$. The first term depends on $\theta$ whereas the second term does not. Therefore, the targeted parameter of the MLE is the minimizer of the Kullback--Leibler divergence. By Gibbs' inequality, the  Kullback--Leibler divergence is non-negative in general and is $0$ if $g(y) = f(y\mid \theta)$. Therefore, if the model is correct, then the true $\theta$ indeed minimizes the Kullback--Leibler divergence with minimum value $0$ 
 

\begin{example}
Assume that $y_1,\ldots, y_n \iidsim  \N(\mu ,1 )$. The log-likelihood contributed by unit $i$ is 
$$
\log f(y_i \mid \mu) = -\frac{1}{2} \log (2\pi) - \frac{1}{2}  (y_i -  \mu)^2,
$$
so
$$
\frac{ \partial \log f(y_i \mid \mu) }{ \partial \mu } =  y_i -  \mu ,\quad
\frac{ \partial^2 \log f(y_i \mid \mu) }{ \partial \mu^2 } =  - 1. 
$$
The MLE is $\hat{\mu} = \bar{y}$. 
If the model is correctly specified, we can use 
$$
I_n(\hat{\mu})^{-1} = n^{-1} \text{ or }
J_n(\hat{\mu})^{-1} = 1 / \sumn  (y_i -  \hat\mu)^2
$$
to estimate the variance of $\hat{\mu}$. If the model is misspecified, we can use
$$
I_n(\hat{\mu})^{-1} J_n(\hat{\mu})I_n(\hat{\mu})^{-1} =  \sumn  (y_i -  \hat\mu)^2/n^2
$$
to estimate the variance of $\hat{\mu}$. 

The sandwich variance estimator seems the best overall. The Normal model can be totally wrong but it is still meaningful to estimate the mean parameter $\mu = E(y)$. The MLE is just the sample moment estimator which has variance var$(y)/n$. Since the sample variance $s^2 = \sumn  (y_i -  \hat\mu)^2/(n-1)$ is unbiased for var$(y)$, a natural unbiased estimator for var$(\hat{\mu})$ is $s^2/n$, which is close to the sandwich variance estimator. 
\end{example}



The above discussion extends to the case with independent but non-IID data. The covariance estimators still apply by replacing each $f$ by $f_i$ within the summation. Note that the sandwich covariance estimator is conservative in general. \citet{white1982maximum} pioneered the asymptotic analysis of the MLE with misspecified models in econometrics but made a mistake for the $M$ term. \citet{chow1984maximum} corrected his error, and \citet{abadie2014inference} developed a more general theory. 


A leading application is the MLE under a misspecified Normal linear model. The EHW robust covariance arises naturally in this case. 


\begin{example}
\label{eg::huber-EHW}
The Normal linear model has individual log-likelihood: 
$$
l_i  = -\frac{1}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (y_i - x_i^{\T} \beta)^2 ,\quad (i=1, \ldots, n)
$$
with the simplification $l_i = \log  f(y_i \mid x_i, \beta, \sigma^2).$
So the first-order derivatives are
$$
\frac{\partial l_i   }{ \partial \beta } = \frac{1}{\sigma^2} x_i (y_i - x_i^{\T} \beta), \quad 
\quad 
\frac{\partial l_i  }{ \partial \sigma^2} = -\frac{1}{2 \sigma^2} +  \frac{1}{2(\sigma^2)^2}  (y_i - x_i^{\T} \beta)^2 ;
$$
the second derivative is
$$
\frac{\partial^2 l_i   }{ \partial \beta^2 } =
-  \frac{1}{\sigma^2} x_i x_i^{\T},\quad
\frac{\partial^2 l_i   }{ \partial (\sigma^2)^2 } =
\frac{1}{2(\sigma^2)^2} - \frac{1}{(\sigma^2)^3} (y_i - x_i^{\T} \beta)^2
$$
and
$$
\frac{\partial^2 l_i  }{ \partial \beta \partial \sigma^2} =
-\frac{1}{(\sigma^2)^2}  x_i (y_i - x_i^{\T} \beta).
$$
The MLE of $\beta$ is the OLS estimator $\hat{\beta}$ and the MLE of $\sigma^2$ is $\tilde{\sigma}^2 = \sumn \hat{\varepsilon}_i^2 /n$, where $ \hat{\varepsilon}_i = y_i - x_i^{\T} \hat{\beta}$  is the residual.  

We have
$$
I_n(\hat\beta, \tilde\sigma^2 ) = \textup{diag}\left( \frac{1}{  \tilde{\sigma}^2}  \sumn x_i x_i^{\T} , \frac{n}{  2 (\tilde{\sigma}^2)^2 } \right), 
$$
and
$$
J_n(\hat\beta, \tilde\sigma^2 ) = \begin{pmatrix}
\frac{1}{  (\tilde{\sigma}^2)^2 }  \sumn  \hat{\varepsilon}_i^2 x_i x_i^{\T}  & *\\
*&*
\end{pmatrix},
$$
where the $*$ terms do not matter for the later calculations. If the Normal linear model is correctly specified, we can use the $(1,1)$th block of $I_n(\hat\beta, \tilde\sigma^2 )^{-1}$ as the covariance estimator for $\hat{\beta}$, which equals
$$
 \tilde{\sigma}^2  \left( \sumn x_i x_i^{\T} \right)^{-1}.
$$
If the Normal linear model is  misspecified, we can use the $(1,1)$th block of $I_n(\hat\beta, \tilde\sigma^2 )^{-1} J_n(\hat\beta, \tilde\sigma^2 ) I_n(\hat\beta, \tilde\sigma^2 )^{-1}$ as the covariance estimator for $\hat{\beta}$, which equals
$$
\left( \sumn x_i x_i^{\T} \right)^{-1}
\left(   \sumn  \hat{\varepsilon}_i^2 x_i x_i^{\T}  \right) 
\left( \sumn x_i x_i^{\T} \right)^{-1},
$$
the EHW robust covariance estimator introduced in Chapter \ref{chapter::EHW}. 
\end{example}





%\section{More on likelihood-based inference}
%
%
%Wald, LRT, score
%
%Wilks
%
%AIC, BIC




\section{Homework problems}


\paragraph{Estimating the mean}\label{hw-math4::mean}

Example \ref{eg::sample-variance} concerns the asymptotic properties. This problem supplements it with more finite-sample results. 


Slightly modify the sandwich covariance estimator to 
$$
\tilde{V} = \frac{1}{n(n-1)} \sumn (x_i - \bar{x})^2.
$$
Show that $E(\tilde{V} ) = \var(\bar{x})$ when $x_1,\ldots, x_n$ are independent with the same mean $\mu$, and $E(\tilde{V} ) \geq  \var(\bar{x})$ when $x_i \sim [\mu_i, \sigma_i^2]$ are independent. 


\paragraph{Sample Pearson correlation coefficient}\label{hw-math4::pearson-cor-asym}

Assume that $(x_i, y_i)_{i=1}^n$ are IID draws from $(x,y)$ with mean $(\mu_x, \mu_y)$ and fourth moments. Derive the asymptotic distribution of the sample Pearson correlation coefficient. Express the asymptotic variance in terms of
$$
\mu_{kl} = E\{  (x-\mu_x)^k (y-\mu_y)^l \},
$$
for example, 
$$
\var(x) = \mu_{20}, \quad  \var(y) = \mu_{02},   \quad \cov(x,y) = \mu_{11},  \quad \rho = \mu_{11} / \sqrt{ \mu_{20}   \mu_{02} } .
$$

Hint: Use the fact that $\beta = (\mu_x, \mu_y, \mu_{20}, \mu_{02}, \rho)^{\T}$ satisfies the estimating equation with
$$
m(x,y,\beta) = \begin{pmatrix}
x - \mu_x \\
y - \mu_y \\
(x - \mu_x)^2 - \mu_{20} \\
(y - \mu_y)^2 - \mu_{02} \\
(x - \mu_x)(y - \mu_y) - \rho \sqrt{ \mu_{20}  \mu_{02} }
\end{pmatrix}
$$
and the sample moments and Pearson correlation coefficient are the corresponding estimators. 
You may also find the formula \eqref{eq::3X3-lower-inverse} useful. 


 

%\paragraph{Function of the MLE}
%Use the delta method to derive the asymptotic distribution of $g(\hat{\theta})$ where $\hat{\theta}$ is the MLE in Example \ref{eg::mle-can}. 

\paragraph{A misspecified Exponential model}\label{hw00math3::sandwich-exponential}
Assume that $y_1,\ldots, y_n \iidsim $ Exponential distribution with  mean $\mu  $. Find the MLE of $\mu$ and its asymptotic variance estimators under correctly specified and incorrectly specified models.

 

