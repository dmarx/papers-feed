 
\chapter{Linear Algebra}\label{chapter::linear-algebra}
 

All vectors are column vectors in this book. This is coherent with \ri{R}. 


\section{Basics of vectors and matrices}
 \label{sec::basics-vectors-matrices}

\paragraph*{Euclidean space}

The $n$-dimensional Euclidean space $\mathbb{R}^{n}$ is a set of
all $n$-dimensional vectors equipped with an inner product: 
$$
\langle x,y \rangle =x^{\T}y=\sumn x_{i}y_{i}
$$
where $x=(x_{1},\ldots,x_{n})^{\T}$ and $y=(y_{1},\ldots,y_{n})^{\T}$
are two $n$-dimensional vectors. The length of a vector $x$
is defined as 
$$
\|x\|=\sqrt{\langle x,x\rangle }=\sqrt{x^{\T}x}.
$$ 
The Cauchy--Schwarz
inequality states that
\[
|\langle x,y \rangle |\leq\|x\|\cdot\|y\|,
\]
or, more transparently,
$$
\left(  \sumn x_i y_i  \right)^2  \leq  \left(  \sumn x_i^2  \right) \left(  \sumn y_i^2  \right)  . 
$$
The equality holds if and only if $y_i = a+bx_i$ for some $a$ and $b$, for all $i=1,\ldots, n.$ We can use the Cauchy--Schwarz inequality to prove the triangle inequality
$$
\| x + y \| \leq \| x\| + \| y\| .  
$$


We say that $x$ and $y$ are {\it orthogonal}, denoted by $x\perp y$, if $\langle x,y \rangle=0$. We call a set of vectors $v_1, \ldots, v_m \in \mathbb{R}^{n}$ {\it orthonormal} if they all have unit length and are mutually orthogonal. 




Geometrically, we can define the cosine of the angle between two vectors $x, y \in \mathbb{R}^n$ as
$$
\cos \angle (x, y) = \frac{ \langle x, y \rangle    }{   \| x \| \| y \|  }
= \frac{  \sumn x_i y_i  }{    \sqrt{  \sumn x_i^2 \sumn y_i^2  }  }.
$$
For unit vectors, it reduces to the inner product. 
When both $x$ and $y$ are orthogonal to $1_n$, that is, $\bar{x} = n^{-1} \sumn x_i =0$ and $\bar{y} = n^{-1} \sumn y_i =0$, the formula of the cosine of the angle is identical to the sample Pearson correlation coefficient
$$
\hat{\rho}_{xy} =  \frac{  \sumn (x_i - \bar{x})  (y_i - \bar{y} )  }{  \sqrt{   \sumn (x_i - \bar{x} ) ^2 \sumn (y_i - \bar{y})^2  }  } .
$$
Sometimes, we simply say that the cosine of the angle of two vectors measures their correlation even when they are not orthogonal to $1_n$. 


\paragraph*{Column space of a matrix}




Given an $n\times m$ matrix $A$, we can view it in terms of all elements 
$$
A = (a_{ij})
= \begin{pmatrix}
a_{11} & \cdots & a_{1m} \\
\vdots &  & \vdots \\
a_{n1} & \cdots & a_{nm}
\end{pmatrix},
$$ 
or row vectors
$$
A = \begin{pmatrix}
a_1^{\T} \\
\vdots \\
a_n^{\T}
\end{pmatrix},
$$
where $a_i \in \mathbb{R}^m$ $(i=1, \ldots, n)$,  or column vectors
$$
A=(A_{1},\ldots,A_{m}),
$$
where $A_j \in \mathbb{R}^n$ $j=1,\ldots, m$. 
In statistics, the rows are corresponding to the units, so the $i$th row vector is the vector observations for unit $i$. Moreover, viewing $A$ in terms of its column vectors can give more insights. Define the  column space of $A$ as 
\[
\mathcal{C}(A)=\left\{ \alpha_{1}A_{1}+\cdots+\alpha_{m}A_{m}: \alpha_{1},\ldots,\alpha_{m}\in\mathbb{R}\right\} ,
\]
which is the set of all linear combinations of the column vectors  $A_{1},\ldots,A_{m}$.  The column space is important because  we can write $A\alpha$, with $\alpha = (\alpha_1, \ldots, \alpha_m)^{\T}$, as
$$
A\alpha = (A_{1},\ldots,A_{m}) \begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_m
\end{pmatrix}
 = \alpha_{1}A_{1}+\cdots+\alpha_{m}A_{m}
 \in  \mathcal{C}(A).
$$

We define the row space of $A$ as the column space of $A^{\T}$. 



\paragraph{Matrix product}

Given an $n\times m$ matrix $A = (a_{ij})$ and an $m\times r$ matrix $B = (b_{ij})$, we can define their product as $C = AB$ where the $n\times r$ matrix $C = (c_{ij})$ has the $(i, j)$th element
$$
c_{ij} = \sum_{k=1}^m a_{ik} b_{kj} .
$$
In terms of the row vectors or column vectors of $A$ and $B$, we have
$$
c_{ij} = a_i^{\T} B_j ,
$$
that is, $c_{ij}$ equals the inner product of the $i$th row vector of $A$ and the $j$th column vector of $B$. Moreover, the matrix product satisfies 
\begin{equation}\label{eq::matrix-product-1}
AB = A (B_1, \ldots, B_r) = (AB_1, \ldots, AB_r)
\end{equation}
so the column vectors of $AB$ belongs to the column space of $A$; it also satisfies 
\begin{equation}\label{eq::matrix-product-2}
AB = \begin{pmatrix}
a_1^{\T} \\
\vdots \\
a_n^{\T}
\end{pmatrix} B
= \begin{pmatrix}
a_1^{\T} B \\
\vdots \\
a_n^{\T} B
\end{pmatrix}
\end{equation}
so the row vectors of $AB$ belong to the column space of $B^{\T}$, or equivalently, the row space of $B$. 


\paragraph{Linearly independent vectors and rank}

We call a set of vectors $A_1, \ldots, A_m \in \mathbb{R}^{n}$ {\it linearly independent} if 
$$
x_1 A_1 + \cdots + x_m A_m = 0 
$$
must imply $x_1= \cdots = x_m = 0$. 
We call $A_{j_1}, \ldots, A_{j_k}$ maximally linearly independent if adding another vector makes them linearly dependent. Define $k$ as the rank of of $\{A_1, \ldots, A_m\}$ and also define $k$ as the rank of the matrix $A = (A_1, \ldots, A_m)$. 


A set of vectors may have different subsets of vectors that are maximally linearly independent. But the rank $k$ is unique. We can also define the rank of a matrix in terms of its row vectors. A remarkable theorem in linear algebra is that it does not matter whether we define the rank of a matrix in terms of its column vectors or row vectors. 


From the matrix product formulas \eqref{eq::matrix-product-1} and \eqref{eq::matrix-product-2}, we have the following result.
%he column vectors of $AB$ can be linearly represented by the column vectors of $A$, and the row vectors of $AB$ can be linearly represented by the row vectors of $B$. So

\begin{proposition}\label{eq::matrix-product-inequality}
$
\textup{rank}(AB) \leq  \min\{  \textup{rank}(A),  \textup{rank}(B) \} . 
$
\end{proposition}


The rank decomposition of a matrix decomposes $A$ into the product of two matrices of full ranks. 

\begin{proposition}\label{eq::rank-decomposition}
If an $n\times m$ matrix has rank $k$, then $A = BC$ for some $n\times k$ matrix $B$ and $k\times m$ matrix $C$. 
\end{proposition}

\begin{myproof}{Proposition}{\ref{eq::rank-decomposition}}
Let $A_{j_1}, \ldots, A_{j_k}$ be the maximally linearly independent column vectors of $A$. Stack them into an $n\times k$ matrix $B = (A_{j_1}, \ldots, A_{j_k}) $. They can linearly represent all column vectors of $A$:
$$
A = (c_{11} A_{j_1} + \cdots + c_{k1} A_{j_k}, \ldots, c_{1m} A_{j_1} + \cdots + c_{km} A_{j_k} )
=  (BC_1, \ldots, B C_m) = BC,
$$
where $C =  ( C_1, \ldots,  C_m)$ is an $k\times m$ matrix with column vectors
$$
C_1 = \begin{pmatrix}
c_{11}  \\
\vdots\\
 c_{k1}
\end{pmatrix},\quad
C_m = \begin{pmatrix}
 c_{1m} \\
 \vdots \\
 c_{km}
\end{pmatrix}.
$$
\end{myproof}


Proposition \ref{eq::matrix-product-inequality} ensures that rank$(B) \geq k$ and rank$(C) \geq k$ so they must both have rank $k$. The decomposition in Proposition \ref{eq::rank-decomposition} is not unique since the choice of the maximally linearly independent column vectors of $A$ is not unique. 


\paragraph*{Some special matrices}

An $n\times n$ matrix $A$ is symmetric if $A^{\T}=A$. 
An $n\times n$ diagonal matrix $A$ has zero off-diagonal elements,
denoted by $A=\text{diag}\{a_{11},\ldots,a_{nn}\}.$
Diagonal matrices are symmetric. 

An $n\times n$
matrix is orthogonal if $A^{\T}A=AA^{\T}=I_{n}$. The column vectors of an orthogonal matrix are orthonormal; so are its row vectors. If $A$ is orthogonal, then 
$$
\| A x \|  = \|x\|
$$
for any vector $x\in \mathbb{R}^n$. That is, multiplying a vector by an orthogonal matrix does not change the length of the vector. Geometrically, an orthogonal matrix corresponds to rotation. 




An $n\times n$ matrix $A$ is upper triangular if $a_{ij} = 0$ for $i > j$ and lower triangular if $a_{ij} = 0$ for $i < j$. An $n\times n$ matrix $A$ can be factorized as
$$
A = LU
$$
where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix. This is called the LU decomposition of a matrix. 



\paragraph*{Determinant}

The original definition of the determinant of a square matrix $A = (a_{ij})$ has a very complex form, which will not be used in this book. 

%I will introduce a simpler definition via the LU decomposition. For a lower or upper triangular matrix, define its determinant as the product of the diagonal elements. For a general square matrix $A$, define
%$$
%\text{det}(A) = \text{det}(L) \text{det}(U)
%$$
%based on the LU decomposition $A = LU$. 


The determinant of a $2\times 2$ matrix has a simple form:
\begin{equation}\label{eq::det-2X2}
\textup{det}\begin{pmatrix}
a & b \\
c& d
\end{pmatrix}
= ad - bc.
\end{equation}
The determinant of the Vandermonde matrix has the following formula:
\begin{equation}
\label{eq::det-Vandermonde}
\textup{det}\begin{pmatrix}
1& x_1 & x_1^2 & \cdots & x_1^{n-1} \\
1& x_2 & x_2^2 & \cdots & x_2^{n-1} \\
\vdots & \vdots & \vdots & & \vdots \\
1&x_n & x_n^2 & \cdots & x_n^{n-1}
\end{pmatrix} 
= \prod_{1\leq i, j \leq n} (x_j - x_i).
\end{equation}



The properties of the determinant are more useful. I will review two.

\begin{proposition}
For two square matrices $A$ and $B$, we have
$$
\textup{det}(AB) = \textup{det}(A) \textup{det}(B)  = \textup{det}(BA). 
$$
\end{proposition}



\begin{proposition}
For two square matrices $A \in \mathbb{R}^{m\times m}$ and $B \in \mathbb{R}^{n \times n}$, we have
$$
\textup{det}
\begin{pmatrix}
A & 0 \\
C& B
\end{pmatrix}
= 
\textup{det}
\begin{pmatrix}
A & D \\
0& B
\end{pmatrix}
=
\textup{det}(A) \textup{det}(B) . 
$$
\end{proposition}




\paragraph*{Inverse of a matrix}

Let $I_{n}$ be the $n\times n$ identity matrix. An $n\times n$
matrix $A$ is invertible/nonsingular if there exists an $n\times n$ matrix
$B$ such that $AB=BA=I_{n}.$ We call $B$ the inverse of $A$, denoted
by $A^{-1}.$  If $A$ is an orthogonal matrix, then $A^{\T} =A^{-1}.$



A square matrix is invertible if and only if det$(A) \neq 0.$


The inverse of a $2\times 2$ matrix is
\begin{equation}
\label{eq::2X2-inverse}
\begin{pmatrix}
a & b \\
c& d
\end{pmatrix}^{-1}
= \frac{1}{ad - bc} \begin{pmatrix}
d & -b \\
-c& a
\end{pmatrix}.
\end{equation}
The inverse of a $3\times 3$ lower triangular matrix is
\begin{equation}
\label{eq::3X3-lower-inverse}
\left(\begin{array}{lll}
a & 0 & 0 \\
b & c & 0 \\
d & e & f
\end{array}\right)^{-1}=\frac{1}{a c f}\left(\begin{array}{ccc}
c f & 0 & 0 \\
-b f & a f & 0 \\
b e-c d & -a e & a c
\end{array}\right). 
\end{equation}



A useful identity is
$$
(AB)^{-1}  = B^{-1} A^{-1}
$$
if both $A$ and $B$ are invertible. 



\paragraph*{Eigenvalues and eigenvectors}

For an $n\times n$ matrix $A$, if there exists a pair of $n$-dimensional
vector $x$ and a scalar $\lambda$ such that 
$$
Ax=\lambda x,
$$ 
then we call $\lambda$ an eigenvalue and $x$ the associated eigenvector of $A$. From
the definition, eigenvalue and eigenvector always come in pairs. The
following eigen-decomposition theorem is important for real symmetric
matrices.

\begin{theorem}\label{thm::eigendecomposition}
If $A$ is an $n\times n$ symmetric matrix, then there exists
an orthogonal matrix $P$ such that 
\[
P^{\T}AP=\textup{diag}\{\lambda_{1},\ldots,\lambda_{n}\},
\]
where the $\lambda$'s are the $n$ eigenvalues of $A$, and the column vectors of $P=(\gamma_{1},\cdots,\gamma_{n})$ are the corresponding eigenvectors. 
\end{theorem}

If we write the eigendecomposition as 
$$
AP=P\text{diag}\{\lambda_{1},\ldots,\lambda_{n}\}
$$
or, equivalently,
$$
A(\gamma_{1},\cdots,\gamma_{n}) = (\lambda_1 \gamma_{1},\cdots, \lambda_{n}\gamma_{n}),
$$
then $(\lambda_i,\gamma_i)$ must be a pair of eigenvalue and eigenvector. Moreover, the eigendecomposition in Theorem \ref{thm::eigendecomposition} is unique up to the permutation of the columns of $P$ and the corresponding $\lambda_i$'s. 

\begin{corollary}
If $P^{\T}AP=\textup{diag}\{\lambda_{1},\ldots,\lambda_{n}\}$, then 
$$
A=P\textup{diag}\{\lambda_{1},\ldots,\lambda_{n}\}P^{\T},\quad 
A^{k}=A\cdot A\cdots A=P\textup{diag}\{\lambda_{1}^{k},\ldots,\lambda_{n}^{k}\}P^{\T};
$$
if the eigenvalues of $A$ are nonzero, then 
$$A^{-1}=P\textup{diag}\{1/\lambda_{1},\ldots,1/\lambda_{n}\}P^{\T}.$$
\end{corollary}


The eigen-decomposition is also useful for defining the square root
of an $n\times n$ symmetric matrix. In particular, if the eigenvalues
of $A$ are nonnegative, then we can define
\[
A^{1/2}=P\text{diag}\{\sqrt{\lambda_{1}},\ldots,\sqrt{\lambda_{n}}\}P^{\T}
\]
By definition, $A^{1/2}$ is a symmetric matrix satisfying $A^{1/2}A^{1/2}=A.$
There are other definitions of the square root of a symmetric matrix,
but we adopt this form in this book.

From Theorem \ref{thm::eigendecomposition}, we can write $A$ as 
\begin{align*}
A & =P\text{diag}\{\lambda_{1},\ldots,\lambda_{n}\}P^{\T}\\
 & =(\gamma_{1},\cdots,\gamma_{n})\text{diag}\{\lambda_{1},\ldots,\lambda_{n}\}\left(\begin{array}{c}
\gamma_{1}^{\T}\\
\vdots\\
\gamma_{n}^{\T}
\end{array}\right)\\
 & =\sumn\lambda_{i}\gamma_{i}\gamma_{i}^{\T}. 
\end{align*}
 

For an $n\times n$ symmetric matrix $A$, its rank equals the number of non-zero eigenvalues
and its determinant equals the product of all eigenvalues. The matrix
$A$ is of full rank if all its eigenvalues are non-zero, which implies
that its rank equals $n$ and its determinant is non-zero.



\paragraph*{Quadratic form}

For an $n\times n$ symmetric matrix $A=(a_{ij})$ and an $n$-dimensional
vector $x$, we can define the quadratic form as

\[
x^{\T}Ax=\langle x,Ax \rangle =\sumn\sum_{j=1}^{n}a_{ij}x_{i}x_{j}.
\]

We always consider a symmetric matrix in the quadratic form without
loss of generality. Otherwise, we can  symmetrize $A$ as $ \tilde{A} =  (A+A^{\T})/2$
without changing the value of the quadratic form because
\[
x^{\T}Ax=x^{\T}  \tilde{A}  x.
\]
 

We call $A$ positive semi-definite, denoted by $A\succeq 0,$ if $x^{\T}Ax\ge0$
for all $x$; we call $A$ positive definite, denoted by $A\succ 0,$
if $x^{\T}Ax>0$ for all nonzero $x.$


We can also define the partial order between matrices. We call $A\succeq B$
if and only if $A-B\succeq0$, and we call $A\succ B$ if and only
if $A-B\succ0$. This is important in statistics because we often
compare the efficiency of estimators based on their variances or covariance matrices. Given two unbiased estimators $\hat\theta_1$ and $\hat\theta_2$ for a scalar parameter $\theta$, we say that $\hat\theta_1$ is more efficient than $\hat\theta_2$ if var$(\hat\theta_2) \geq $ var$(\hat\theta_1)$. In the vector case, we say that $\hat\theta_1$ is more efficient than $\hat\theta_2$  if cov$(\hat\theta_2) \succeq $ cov$(\hat\theta_1)$, which is equivalent to var$(\ell^{\T} \hat\theta_2) \geq $ var$(\ell^{\T}\hat\theta_1)$ for any linear combination of the estimators. 



The eigenvalues of a symmetric matrix determine whether it is positive semi-definite or positive definite. 


\begin{theorem}
For a symmetric matrix $A$, it is positive semi-definite
if and only if all its eigenvalues are nonnegative, and it is positive
definite if and only if all its eigenvalues are positive.
\end{theorem} 





An important result is the relationship between the eigenvalues and the extreme values of the quadratic form. Assume that the eigenvalues are rearranged in decreasing order such that $\lambda_1 \geq \cdots \geq \lambda_n$. 
For a unit vector $x$, we have that
$$
x^{\T} A x = x^{\T} \sumn\lambda_{i}\gamma_{i}\gamma_{i}^{\T} x 
= \sumn\lambda_{i}  \alpha_{i}^2  
$$
where 
$$
\alpha 
= \begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_n
\end{pmatrix}
= \begin{pmatrix}
\gamma_1^{\T} x \\
\vdots \\
\gamma_{n}^{\T} x
\end{pmatrix}
= P^{\T} x
$$
has length $ \|\alpha \|^2  = \| x\|^2 = 1$.
Then the maximum value of $x^{\T} A x $ is $\lambda_1$ which is achieved at $\alpha_1 = 1$ and $\alpha_2 = \cdots = \alpha_n = 0$ (for example, if $x = \gamma_1$, then $\alpha_1 = 1$ and $\alpha_2 = \cdots = \alpha_n = 0$). For a unit vector $x$ that is orthogonal to $\gamma_1$, we have that
$$
x^{\T} A x  = \sum_{i=2}^n \lambda_{i}  \alpha_{i}^2 
$$
where $\alpha = P^{\T} x$ has unit length with $\alpha_1 = 0$. The maximum value of $x^{\T} A x $ is $\lambda_2$ which is achieved at $\alpha_2 = 1$ and $\alpha_1 = \alpha_3 = \cdots = \alpha_n = 0$, for example, $x = \gamma_2$. By induction, we have the following theorem.

\begin{theorem}
\label{thm::eigven-max-q}
Suppose that an $n\times n$ symmetric matrix has eigen-decomposition $\sumn\lambda_{i}\gamma_{i}\gamma_{i}^{\T}$ where $\lambda_1 \geq \cdots \geq \lambda_n$. 
\begin{enumerate}
\item
The optimization problem
$$
\max_x x^{\T} A x \text{ such that } \|x\| = 1
$$
has maximum $\lambda_1$ which can be achieved by $\gamma_1$.
\item
The optimization problem
$$
\max_x x^{\T} A x \text{ such that } \|x\| = 1,  x \perp \gamma_1 
$$
has maximum $\lambda_2$ which can be achieved  by $\gamma_2$.
\item
The optimization problem
$$
\max_x x^{\T} A x \text{ such that } \|x\| = 1,  x \perp \gamma_1 ,\ldots, x\perp \gamma_k
$$
has maximum $\lambda_{k+1}$ which can be achieved by $\gamma_{k+1}$ $(k=1, \ldots, n-1)$. 
\end{enumerate}
\end{theorem}

Theorem \ref{thm::eigven-max-q} implies the following theorem on the  Rayleigh quotient
$$
r(x)=x^{\T}Ax/x^{\T}x \qquad  (x\in \mathbb{R}^n).
$$


\begin{theorem}
(Rayleigh quotient and eigenvalues) \label{theorem::rayleigh}
%For an $n\times n$ symmetric matrix $A$,
%let $r(x)=x^{\T}Ax/x^{\T}x$ be the Rayleigh quotient of $x \in \mathbb{R}^n$. 
The maximum and minimum eigenvalues of  an $n\times n$ symmetric matrix $A$ equals
\[
\lambda_{\max}(A)=\max_{x\neq0}r(x),\qquad\lambda_{\min}(A)=\min_{x\neq0}r(x)
\]
with the maximizer and minimizer being the eigenvectors corresponding to the maximum and minimum eigenvalues, respectively. 
\end{theorem}

An immediate consequence of Theorem \ref{theorem::rayleigh} is that the diagonal elements of $A$ are bounded by the smallest and largest eigenvalues of $A$. This follows by taking $x = (0, \ldots, 1, \ldots, 0)^{\T}$ where only the $i$th element equals $1$. 

\paragraph*{Trace}

The trace of an $n\times n$ matrix $A=(a_{ij})$ is the sum of all
its diagonal elements, denoted by 
$$
\text{trace}(A)=\sumn a_{ii}.
$$

The trace operator has two important properties that can sometimes
help to simplify calculations.

\begin{proposition}\label{prop::trace-property1}
$\textup{trace}(AB)=\textup{trace}(BA)$ as long as $AB$ and $BA$
are both square matrices.
\end{proposition}

We can verify
Proposition \ref{prop::trace-property1} by definition. It states that $AB$ and $BA$ have the same trace although $AB$ differs from $BA$ in general. In fact, it is particularly useful if the dimension of $BA$
is much lower than the dimension of $AB$. For example, if both $A=(a_{1},\ldots,a_{n})^{\T}$
and $B=(b_{1},\ldots,b_{n})$ are vectors, then $\text{trace}(AB)=\text{trace}(BA)=\langle B^{\T},A \rangle =\sumn a_{i}b_{i}.$ 

\begin{proposition}\label{proposition::trace-eigenvalues}
The trace of an $n\times n$ symmetric matrix $A$ equals the sum
of its eigenvalues: $\textup{trace}(A)=\sumn\lambda_{i}$. 
\end{proposition}

\begin{myproof}{Proposition}{\ref{proposition::trace-eigenvalues}}
It follows from the eigen-decomposition and Proposition \ref{prop::trace-property1}. Let $\Lambda=\text{diag}\{\lambda_{1},\ldots,\lambda_{n}\}$,
and we have
$$
\text{trace}(A)  =\text{trace}(P\Lambda P^{\T})
  =\text{trace}(\Lambda P^{\T}P)
  =\text{trace}(\Lambda)
  =\sumn\lambda_{i}.
$$
\end{myproof}


\paragraph*{Projection matrix}

An $n\times n$ matrix $H$ is a projection matrix if it is symmetric and $H^{2}=H.$ The eigenvalues of $H$ must be either $1$ or $0$. To see this, we assume that $H x = \lambda x$ for some nonzero vector $x$, and use two ways to calculate $H^2x$:
$$
H^2x = Hx = \lambda x,\quad H^2x = H(Hx) = H(\lambda x ) = \lambda Hx =  \lambda^2 x.
$$
So $ (\lambda - \lambda^2) x = 0$ which implies that $\lambda - \lambda^2=0$, i.e., $\lambda = 0$ or $1$. 
%Based on the eigen-decomposition $H=\sumn\lambda_{i}\gamma_{i}\gamma_{i}^{\T}$,
%we have
%
%\begin{align*}
%H^{2} =H &\Longrightarrow\sumn\lambda_{i}^{2}\gamma_{i}\gamma_{i}^{\T}=\sumn \lambda_{i}\gamma_{i}\gamma_{i}^{\T}\\
% & \Longrightarrow\sumn(\lambda_{i}^{2}-\lambda_{i})\gamma_{i}\gamma_{i}^{\T}=0\\
% & \Longrightarrow\lambda_{i}^{2}-\lambda_{i}=0, \qquad(i=1,\ldots,n)
%\end{align*}
%which implies that the eigenvalues of $H$ must be either $1$ or $0$. 
So the trace of
$H$ equals its rank:
\[
\text{trace}(H)=\text{rank}(H).
\]

Why is this a reasonable definition of a ``projection matrix''? Or, why must a projection matrix
satisfy $H^{\T}=H$ and $H^{2}=H$? First, it is reasonable
to require that $Hx_{1}=x_{1}$ for any $x_{1} \in \mathcal{C}(H)$,
the column space of $H.$ Since $x_{1}=H\alpha$ for some $\alpha$,
we indeed have $Hx_{1}=H(H\alpha)=H^{2}\alpha=H\alpha=x_{1}$ because
of the property $H^{2}=H$. Second, it is reasonable to require that $x_{1}\perp x_{2}$
for any vector $x_{1}=H\alpha \in\mathcal{C}(H)$ and $x_{2}$ such that $Hx_{2}=0$.
So we need $\alpha^{\T}H^{\T}x_{2}=0$ which is true if $H=H^{\T}.$
Therefore, the two conditions are natural for the definition of a
projection matrix. 


More interestingly, a project matrix has a more explicit form as stated below. 


\begin{theorem}\label{thm::projection-matrix-form}
If an $n\times p$ matrix $X$ has $p$ linearly independent columns, then $H = X  (X^{\T} X)^{-1} X^{\T}$ is a projection matrix. Conversely, if an $n\times n$ matrix $H$ is a projection matrix with rank $p$, then $H = X  (X^{\T} X)^{-1} X^{\T}$ for some $n\times p$ matrix $X$ with linearly independent columns.
\end{theorem}

It is relatively easy to verify the first part of Theorem \ref{thm::projection-matrix-form}; see Chapter \ref{chapter::ols-vector}. The second part of Theorem \ref{thm::projection-matrix-form} follows from the eigen-decomposition of $H$, with the first $p$ eigen-vectors being the column vectors of $X$. 




\paragraph*{Cholesky decomposition}

An $n\times n$ positive semi-definite matrix $A$ can be decomposed
as $A=LL^{\T}$ where $L$ is an $n\times n$ lower triangular matrix
with non-negative diagonal elements. 
If $A$ is positive definite,  the decomposition is unique. In general, it is not.
Take an arbitrary orthogonal
matrix $Q$, we have $A=LQQ^{\T}L^{\T}=CC^{\T}$ where $C=LQ$. So
we can decompose a positive semi-definite matrix $A$ as $A=CC^{\T}$, but
this decomposition is not unique. 



\paragraph*{Singular value decomposition (SVD)}

Any $n\times m$ matrix $A$ can be decomposed as
$$
A = UDV^{\T}
$$
where $U$ is $n\times n$ orthogonal matrix, $V$ is $m\times m$ orthogonal matrix, and $D$ is $n\times m$ matrix with all zeros for the non-diagonal elements. 
For a tall matrix with $n \geq m$, the diagonal matrix $D$ has many zeros, so we can also write
$$
A = UDV^{\T}
$$
where $U$ is $n\times m$ matrix with orthonormal columns ($U^{\T} U = I_m$), $V$ is $m\times m$ orthogonal matrix, and $D$ is $m\times m$ diagonal matrix. Similarly, for a wide matrix with $n\leq m$, we can write
$$
A = UDV^{\T}
$$
where $U$ is $n\times n$ orthogonal matrix, $V$ is $m\times n$ matrix with orthonormal columns ($V^{\T} V = I_n$), and $D$ is $n\times n$ diagonal matrix. 

If $D$ has only $r \leq \min(m,n)$ nonzero elements, then we can further simplify the decomposition as
$$
A = U D  V^{\T}
$$
where $U$ is $n\times r$ matrix with orthonormal columns ($U^{\T} U = I_r$), $V$ is $m \times r$ matrix with orthonormal columns ($V^{\T} V = I_r$), and $D$ is $r\times r$ diagonal matrix. With more explicit forms of 
$$
U = (U_{1}, \ldots, U_{r}), \quad D = \text{diag}(d_1, \ldots, d_r),\quad V =  (V_{1}, \ldots, V_{r}),
$$
we can write $A$ as
$$
A = (U_{1}, \ldots, U_{r}) \begin{pmatrix}
d_1 & &  \\
 &  \ddots & \\
 & & d_r
\end{pmatrix}
\begin{pmatrix}
V_1^{\T} \\
\vdots \\
V_r^{\T}
\end{pmatrix}
= \sum_{k=1}^r  d_k U_k V_k^{\T}. 
$$

The SVD implies that
$$
AA^{\T} = UDD^{\T} U^{\T},\quad
A^{\T} A = V D^{\T}  D V^{\T} ,
$$
which are the eigen decompositions of $AA^{\T}$ and $A^{\T} A $. This ensures that $AA^{\T} $ and $A^{\T} A $ have the same non-zero eigenvalues. 


An application of the SVD is to define the pseudoinverse of any matrix. Define $D^{+}$ as the pseudoinverse of $D$ with the non-zero elements inverted but the zero elements intact at zero. Define
$$
A^{+} = V D^{+} U^{\T} =  \sum_{k=1}^r d_k^{-1} V_k U_k^{\T} 
$$
as the pseudoinverse of $A$. The definition holds even if $A$ is not a square matrix. We can verify that 
$$
AA^{+} A = A, \quad 
A^{+}AA^{+} = A^{+}.
$$
If $A$ is a square nondegenerate matrix, then $A^{+} = A^{-1}$ equals the standard definition of the inverse. 
In the special case with a symmetric $A$, its SVD is identical to its eigen decomposition. If $A$ is not invertible, its pseudoinverse equals
$$
A^{+} = P\text{diag}(\lambda_1^{-1}, \ldots, \lambda_k^{-1}, 0, \ldots, 0) P^{\T} 
$$
if rank$(A) = k < n$ and $\lambda_1,\lambda_1, \ldots, \lambda_k$ are the nonzero eigen-values. 




Another application of the SVD is the  {\it polar decomposition} for any square matrix $A$. Since $A = UDV^{\T} = UDU^{\T} UV^{\T}$ with orthogonal $U$ and $V$, we have
$$
A = (AA^{\T})^{1/2}\Gamma,
$$
where $ (AA^{\T})^{1/2}\ = UDU^{\T} $ and $\Gamma =  UV^{\T}$ is an orthogonal matrix. 


 


\section{Vector calculus}

If $f(x)$ is a function from $\mathbb{R}^p$ to $\mathbb{R}$, then we use the notation 
$$
\frac{\partial f(x)}{\partial  x} \equiv  \begin{pmatrix}
\frac{\partial f(x)}{\partial x_1} \\
\vdots \\
 \frac{\partial f(x)}{\partial  x_p}  
\end{pmatrix}
$$
for the component-wise partial derivative, which must have the same dimension as $x$. It is often called the {\it gradient} of $f.$
For example, for a linear function $f(x) = x^{\T} a = a^{\T} x$ with $a,x\in \mathbb{R}^p$, we have
\begin{eqnarray}\label{eq::diff-linear}
\frac{\partial a^{\T} x  }{\partial  x} = \begin{pmatrix}
\frac{\partial a^{\T} x}{\partial x_1} \\
\vdots \\
 \frac{\partial a^{\T} x}{\partial  x_p}  
\end{pmatrix}
= \begin{pmatrix}
 \frac{\partial \sum_{j=1}^p  a_j x_j }{\partial x_1} \\
\vdots \\
 \frac{\partial \sum_{j=1}^p  a_j x_j }{\partial  x_p}  
\end{pmatrix}
= \begin{pmatrix}
a_1 \\
\vdots \\
a_p
\end{pmatrix}
= a;
\end{eqnarray}
for a quadratic function $f(x) = x^{\T} A x $ with a symmetric $A\in \mathbb{R}^{p\times p}$  and $x \in \mathbb{R}^p$, we have 
$$
\frac{ \partial  x^{\T} A x }{ \partial  x} = \begin{pmatrix}
\frac{\partial x^{\T} A x}{\partial x_1} \\
\vdots \\
 \frac{\partial x^{\T} A x}{\partial  x_p}  
\end{pmatrix}
= \begin{pmatrix}
\frac{\partial   \sum_{i=1}^p \sum_{j=1}^p a_{ij} x_i x_j }{\partial x_1} \\
\vdots \\
 \frac{\partial  \sum_{i=1}^p \sum_{j=1}^p a_{ij} x_i x_j   }{\partial  x_p}  
\end{pmatrix} 
= \begin{pmatrix}
2 a_{11} x_1 + \cdots + 2a_{1p} x_p \\
\vdots \\
2 a_{p1} x_1 + \cdots + 2a_{pp} x_p
\end{pmatrix} 
=2 Ax .
$$
These are two important rules of vector calculus used in this book, summarized below.

\begin{proposition}\label{prop::vector-calculus}
We have
\begin{eqnarray*}
\frac{\partial a^{\T} x  }{\partial  x}  &=& a, \\
\frac{ \partial  x^{\T} A x }{ \partial  x} &=& 2 Ax . 
\end{eqnarray*}
\end{proposition}

We can also extend the definition to vector functions. If $f(x) = (f_1(x), \ldots, f_q(x))^{\T}$ is a function from $\mathbb{R}^p$ to $\mathbb{R}^q$, then we use the notation
\begin{eqnarray}
\label{eq::diff-vector-veector}
\frac{\partial f(x)}{\partial  x}  \equiv  \left( \frac{\partial f_1(x)}{\partial  x}  ,\cdots, \frac{\partial f_q(x)}{\partial  x}  \right)
=\begin{pmatrix}
 \frac{\partial f_1(x)}{\partial  x_1} & \cdots &  \frac{\partial f_q(x)}{\partial  x_1} \\
 \vdots & & \vdots \\
  \frac{\partial f_1(x)}{\partial  x_p} & \cdots &  \frac{\partial f_q(x)}{\partial  x_p} 
\end{pmatrix},
\end{eqnarray} 
which is a $p\times q$ matrix with rows corresponding to the elements of $x$ and the columns corresponding to the elements of $f(x)$. 
We can easily extend the first result of Proposition \ref{prop::vector-calculus}.

\begin{proposition}\label{prop::vector-calculus-matrix}
For $B\in \mathbb{R}^{p \times q}$ and $x\in \mathbb{R}^p$, we have 
$$
\frac{\partial B^{\T} x  }{\partial  x}  = B. 
$$
\end{proposition}

 \begin{myproof}{Proposition}{\ref{prop::vector-calculus-matrix}}
 Partition $B= (B_1, \ldots, B_q)$ in terms of its column vectors. 
The $j$th element of $B^{\T}x$ is $B_j^{\T}x$ so the $j$-th column of $ \partial B^{\T}x / \partial x$ is $B_j$ based on Proposition \ref{prop::vector-calculus}. This verifies that $ \partial B^{\T}x / \partial x$ equals $B$. 
 \end{myproof}
 
 
 
 

Some authors define $\partial f(x) / \partial  x$ as the transpose of \eqref{eq::diff-vector-veector}. I adopt this form for its natural  connection with 
\eqref{eq::diff-linear} when $q=1$. 
Sometimes, it is indeed more convenient to work with the transpose of $\partial f(x) / \partial  x$. Then I will use the notation
$$
\frac{\partial f(x)}{\partial  x^{\T}}  = \left( \frac{\partial f(x)}{\partial  x}   \right)^{\T} 
=  \left( \frac{\partial f(x)}{\partial  x_1}  ,\cdots, \frac{\partial f(x)}{\partial  x_p}  \right)
$$
which puts the transpose notation on $x$. 


The above formulas become more powerful in conjunction with the chain rule. For example, for any differentiable function $h(z)$ mapping from $\mathbb{R} $ to $ \mathbb{R}$ with derivative $h'(z)$, we have
\begin{eqnarray*}
\frac{ \partial h (a^{\T} x)  }{\partial  x}  &=& h'(a^{\T} x)  a,\\ 
\frac{ \partial  h( x^{\T} A x) }{ \partial  x} &=& 2h'( x^{\T} A x) Ax.
\end{eqnarray*}
For any differentiable function $h(z)$ mapping from $ \mathbb{R}^q $ to $ \mathbb{R}$ with gradient 
$\partial h(z) / \partial z$, we have
\begin{eqnarray*}
\frac{\partial h(B^{\T} x)  }{\partial  x} 
&=& \frac{\partial h(B_1^{\T} x, \ldots, B_q^{\T}x)  }{\partial  x} \\
&=& \sum_{j=1}^q \frac{\partial h(B_1^{\T} x, \ldots, B_q^{\T}x)  }{\partial  z_j} B_j \\
&=&  B \frac{ \partial h(B^{\T} x) }{  \partial z } . 
\end{eqnarray*}

Moreover, we can also define the Hessian matrix of a function $f(x)$ mapping from $\mathbb{R}^p$ to $\mathbb{R}$:
$$
\frac{  \partial^2 f(x) }{ \partial x \partial x^{\T} } = \left( \frac{  \partial^2 f(x) }{ \partial x_i \partial x_j} \right)_{1\leq i, j \leq p}
= \frac{ \partial }{ \partial x^{\T} } \left(  \frac{  \partial f(x) }{ \partial x }  \right). 
$$


\section{Homework problems}


\paragraph{Triangle inequality of the inner product}\label{hwmath1::triangle-inner-product}

With three unit vectors $u, v, w  \in \mathbb{R}^n$, prove that
$$
\sqrt{ 1 -  \langle  u, w  \rangle } \leq \sqrt{  1 -   \langle  u, v  \rangle } + \sqrt{  1 -   \langle   v, w  \rangle }.
$$


Remark: The result is a direct consequence of the standard triangle inequality but it has an interesting implication. If $ \langle  u, v  \rangle  \geq 1-\epsilon$ and  $  \langle   v, w  \rangle \geq 1 - \epsilon$, then $\langle  u, w  \rangle \geq 1-4\epsilon$. This implied inequality is mostly interesting when $\epsilon $ is small. It states that when $u$ and $v$ are highly corrected and $v$ and $w$ are highly correlated, then $u$ and $w$ must also be highly correlated. Note that we can find counterexamples for the following relationship:
$$
\langle  u, v  \rangle > 0, \quad  \langle   v, w  \rangle  > 0 \quad   \text{ but }\quad   \langle  u, w  \rangle = 0. 
$$



\paragraph{Van der Corput inequality}
\label{hwmath1::vandercorputineq}

Assume that $v, u_1, \ldots, u_m \in \mathbb{R}^n$ have unit length. Prove  that 
$$
\left(  \sum_{i=1}^m   \langle v, u_i \rangle    \right)^2 \leq \sum_{i=1}^m \sum_{j=1}^m \langle u_i, u_j \rangle. 
$$


Remark: This result is not too difficult to prove but it says something fundamentally interesting. If $v$ is correlated with many vectors $u_1, \ldots, u_m$, then at least some vectors in  $u_1, \ldots, u_m$ must be also correlated. 



\paragraph{Inverse of a block matrix}\label{hwmath1::inverse-block-matrix}

Prove that 
\begin{align*}
&\left(\begin{array}{cc}
A & B\\
C & D
\end{array}\right)^{-1} \\
& =\left(\begin{array}{cc}
A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1}\\
-(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}
\end{array}\right)\\
 & =\left(\begin{array}{cc}
(A-BD^{-1}C)^{-1} & -(A-BD^{-1}C)^{-1}BD^{-1}\\
-D^{-1}C (A-BD^{-1}C)^{-1} & D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}
\end{array}\right),
\end{align*}
provided that all the inverses of the matrices exist. The two forms of the inverse imply the Woodbury formula:
\[
(A-BD^{-1}C)^{-1}=A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1},
\]
which further implies the Sherman--Morrison formula:
\[
(A+uv^{\T})^{-1}=A^{-1}-(1+v^{\T}A^{-1}u)^{-1}A^{-1}uv^{\T}A^{-1},
\]
where $A$ is an invertible square matrix,  and $u$ and $v$ are two column vectors. 



\paragraph{Matrix determinant lemma}\label{hwmath1::matrix-determinat-matrix}

Prove that  given the identity matrix $I_n$ and two $n$-vectors $u$ and $v$, we have
$$
\text{det}(I_n + uv^{\T}) = 1 + v^{\T} u. 
$$
Further show that if $I_n$ is replaced by an $n\times n$ invertible matrix $A$, we have
$$
\text{det}(A + uv^{\T}) = (1 +  v^{\T} A^{-1} u)  \cdot  \text{det}(A). 
$$



\paragraph{Decomposition of a positive semi-definite matrix}\label{hwmath::decompose-psd}

Show that if $A$ is positive semi-definite, then there exists a matrix $C$ such that $A = CC^{\T}$. 



\paragraph{Trace of the product of two matrices}
\label{hwmath1::trace-product}
Prove that $A$ and $B$ are two $n\times n$ positive semi-definite matrices, then trace$(AB) \geq 0.$

Hint: Use the eigen-decomposition of $A = \sum_{i=1}^n \lambda_i \gamma_i \gamma_i^{\T}$.

Remark: In fact, a stronger result holds. If two $n\times n$ symmetric matrices $A$ and $B$ have eigenvalues
$$
\lambda_1 \geq \cdots \geq \lambda_n, \quad
\mu_1 \geq \cdots \geq \mu_n
$$
respectively, then 
$$
\sumn \lambda_i \mu_{n+1-i} \leq 
\textup{trace}(AB) \leq 
\sumn \lambda_i \mu_i. 
$$
The result is
due to \citet{von1937some} and \citet{ruhe1970perturbation}. See also \citet[][Lemma 4.12]{chen2019model}.


\paragraph{Vector calculus}\label{hwmath1::vector-calculus-asymmetric}
What is the formula for $ \partial  x^{\T} A x / \partial  x$ if $A$ is not symmetric in Proposition 
\ref{prop::vector-calculus}?  

 

