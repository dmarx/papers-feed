 
\chapter{From Generalized Linear Models to Restricted Mean Models: the Sandwich Covariance Matrix}\label{chapter::sandwich}
   \chaptermark{GLM and Sandwich Covariance Matrix}


This chapter discusses the consequence of misspecified GLMs, extending the EHW covariance estimator to its analogs under the GLMs. It serves as a stepping stone to the next chapter on the generalized estimating equation. 



 


\section{Restricted mean model}
\label{sec::rmm-estimation-equation}

The logistic, Poisson, and Negative-Binomial models are extensions of
the Normal linear model. All of them are fully parametric models.
However, we have also discussed OLS as a restricted mean model
\[
E(y_{i}\mid x_{i})=x_{i}^{\T}\beta
\]
without imposing any additional assumptions (e.g., the variance) on
the conditional distribution. The restricted
mean model is a semiparametric model. Then a natural question is: what
are the analogs of the restricted mean model for the binary and count models? 

Binary outcome is too special because the conditional mean determines
the distribution. So if we assume that the conditional mean is $\mu_{i}=e^{x_{i}^{\T}\beta}/(1+e^{x_{i}^{\T}\beta})$,
then conditional distribution must be Bernoulli$(\mu_{i})$. Consequently, misspecification of the conditional mean function implies misspecification of the whole conditional distribution. 

For other outcomes, the conditional mean cannot determine the conditional
distribution. If we assume 
\[
E(y_{i}\mid x_{i})=\mu(x_{i}^{\T}\beta),
\]
we can verify that 
\[
E\left\{ \sumn\frac{y_{i}-\mu(x_{i}^{\T}\beta)}{\tilde{\sigma}^{2} (x_i, \beta)}\frac{\partial\mu(x_{i}^{\T}\beta)}{\partial\beta}\right\} =E\left[E\left\{ \sumn\frac{y_{i}-\mu(x_{i}^{\T}\beta)}{ \tilde{\sigma}^{2} (x_i, \beta) }\frac{\partial\mu(x_{i}^{\T}\beta)}{\partial\beta}\mid x_i \right\} \right]=0
\]
for any $\tilde{\sigma}^{2}$ that can be a function of $x_{i}$, the true parameter $\beta$, and maybe $\phi$. So we can estimate $\beta$ by solving the estimating equation:
\begin{equation}
\sumn\frac{y_{i}-\mu(x_{i}^{\T}\beta)}{ \tilde{\sigma}^{2} (x_i, \beta)  }\frac{\partial\mu(x_{i}^{\T}\beta)}{\partial\beta}=0. \label{eq:estimating-equation-conditional-mean}
\end{equation}
If $\tilde{\sigma}^{2} (x_i, \beta) = \sigma^2(x_i) =\var(y_{i}\mid x_{i})$,
then the above estimating equation is the score equation derived from
the GLM of an exponential family. If not, \eqref{eq:estimating-equation-conditional-mean} is not a score function but it is still a valid estimating equation.
In the latter case, $ \tilde{\sigma}^{2} (x_i, \beta)  $ is a ``working''
variance. This has important implications for the practical data analysis.
First, we can interpret the MLE from a GLM more broadly: it is also
valid under a restricted mean model even if the conditional distribution
is misspecified. Second, we can construct more general estimators
beyond the MLEs from GLMs. However, we must address the issue of variance
estimation since the inference based on the Fisher information matrix no longer
works in general. 



\section{Sandwich covariance matrix}
\label{sec::rmm-sandwich-covariance}
 
 To simplify the notation, we assume $(x_i,y_i)_{i=1}^n$ are IID draws although we usually view the covariates as fixed. This additional assumption is innocuous as the final inferential procedures are identical. 
 
 
 \begin{theorem}
 \label{theorem::glm-sandwich}
Assume $(x_i,y_i)_{i=1}^n$ are IID with $E(y_i \mid x_i) = \mu(x_{i}^{\T}\beta)$. We have
$\sqrt{n}\left(\hat{\beta}-\beta\right)\rightarrow\N(0,B^{-1}MB^{-1})$
with
\begin{align}
B & =E\left\{ \frac{1}{\tilde{\sigma}^{2}(x, \beta)}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta^{\T}}\right\} , \label{eq::sandwich-var-b}\\
M & =E\left[\frac{\sigma^{2}(x)}{\left\{ \tilde{\sigma}^{2}(x,  \beta)\right\} ^{2}}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta^{\T}}\right]. \label{eq::sandwich-var-m}
\end{align}
\end{theorem}  
 
 
 \begin{myproof}{Theorem}{\ref{theorem::glm-sandwich}}
 Applying Theorem \ref{theorem:sandwich-theorem-cov-iid} to 
\[
w=(x,y),\quad m(w,\beta)=\frac{y-\mu(x^{\T} b)}{\tilde{\sigma}^{2}(x, \beta)}\frac{\partial\mu(x^{\T} \beta)}{\partial \beta },
\]
we can derive the asymptotic distribution of the $\hat{\beta}$. 

The bread matrix equals
\begin{align}
B & =-E\left\{ \frac{\partial m(w,\beta)}{\partial\beta^{\T}}\right\} \nonumber \\
 & =-E\left[\frac{\partial}{\partial\beta^{\T}}\left\{ \frac{y-\mu(x^{\T}\beta)}{\tilde{\sigma}^{2}(x,\beta)}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\right\} \right]\nonumber \\
 & =-E\left[\frac{y-\mu(x^{\T}\beta)}{\tilde{\sigma}^{2}(x,\beta)}\frac{\partial^{2}\mu(x^{\T}\beta)}{\partial\beta\partial\beta^{\T}}+\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial}{\partial\beta^{\T}}\left\{ \frac{y-\mu(x^{\T}\beta)}{\tilde{\sigma}^{2}(x,\beta)}\right\} \right]\label{eq:first-term-zero}\\
 & =-E\left[\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial}{\partial\beta^{\T}}\left\{ \frac{y-\mu(x^{\T}\beta)}{\tilde{\sigma}^{2}(x,\beta)}\right\} \right]\nonumber \\
 & =E\left[\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta^{\T}}/\tilde{\sigma}^{2}(x,\beta)+\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial\tilde{\sigma}^{2}(x,\beta)}{\partial\beta^{\T}}\frac{y-\mu(x^{\T}\beta)}{\left\{ \tilde{\sigma}^{2}(x,\beta)\right\} ^{2}}\right]\label{eq:second-term-zero}\\
 & =E\left\{ \frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta^{\T}}/\tilde{\sigma}^{2}(x,\beta)\right\} ,\nonumber 
\end{align}
where the first term of (\ref{eq:first-term-zero}) and the second
term of (\ref{eq:second-term-zero}) are both zero under the restricted
mean model. The meat matrix equals
\begin{align*}
M & =E\left\{ m(w,\beta)m(w,\beta)^{\T}\right\} \\
 & =E\left[\left\{ \frac{y-\mu(x^{\T}\beta)}{\tilde{\sigma}^{2}(x)}\right\} ^{2}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta^{\T}}\right]\\
 & =E\left[\frac{\sigma^{2}(x)}{\left\{ \tilde{\sigma}^{2}(x)\right\} ^{2}}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta}\frac{\partial\mu(x^{\T}\beta)}{\partial\beta^{\T}}\right].
\end{align*}
 \end{myproof}
 
 


We can estimate the asymptotic variance by replacing $B$ and $M$
by their sample analogs.
With $\hat\beta$ and the residual $\hat{\varepsilon}_i = y_{i}-\mu(x_{i}^{\T}\hat{\beta} ) $, we can conduct statistical inference based on the following Normal approximation:
\[
\hat{\beta}\asim\N(\beta,\hat{V}),
\]
with $\hat{V} \equiv n^{-1}\hat{B}^{-1}\hat{M}\hat{B}^{-1}$, 
where 
\begin{align*}
\hat{B} & =n^{-1}\sumn\frac{1}{\tilde{\sigma}^{2}(x_{i}, \hat{\beta})}\frac{\partial\mu(x_{i}^{\T}\hat{\beta})}{\partial\beta}\frac{\partial\mu(x_{i}^{\T}\hat{\beta})}{\partial\beta^{\T}},\\
\hat{M} & =n^{-1}\sumn \frac{ \hat{\varepsilon}_i^2    }{\tilde{\sigma}^{4}(x_{i}, \hat{\beta})} \frac{\partial\mu(x_{i}^{\T}\hat{\beta})}{\partial\beta}\frac{\partial\mu(x_{i}^{\T}\hat{\beta})}{\partial\beta^{\T}}.
\end{align*}
As a special case, when the GLM is correctly specified with $\sigma^2(x) = \tilde{\sigma}^2(x, \beta)$, then $B=M$ and the asymptotic variance reduces to the inverse of the Fisher information matrix discussed in Section \ref{sec::mle-glm-fisher}. 



\setcounter{example}{0}
\begin{example}[continued]\label{eg::gaussianlinearmodel-continue2}
In a working Normal linear model,   $\tilde{\sigma}^2(x_i, \beta)  = \tilde{\sigma}^2$ is constant and 
$  \partial\mu(x_{i}^{\T} \beta ) / \partial\beta  = x_i$. So
$$
\hat{B} = n^{-1}\sumn\frac{1}{\tilde{\sigma}^{2} } x_i x_i^{\T},\quad
\hat{M} = n^{-1}\sumn  \frac{   \hat{\varepsilon}_i^2 }{  (\tilde{\sigma}^{2})^2 }  x_i x_i^{\T}
$$
with residual $ \hat{\varepsilon}_i  = y_i - x_i^{\T} \hat{\beta}$, recovering the EHW variance estimator
$$
\hat{V} = \left( \sumn   x_i x_i^{\T} \right)^{-1}
\left(\sumn \hat{\varepsilon}_i^2 x_i x_i^{\T} \right)
 \left( \sumn x_i x_i^{\T} \right)^{-1} . 
$$
\end{example}



\begin{example}[continued]\label{eg::binarylogisticmodel-continue2}
In a working binary logistic model, $\tilde{\sigma}^2(x_i, \beta)  = \pi(x_i, \beta) \{1-\pi(x_i, \beta) \} $ and $ \partial\mu(x_{i}^{\T} \beta ) / \partial\beta  = \pi(x_i, \beta) \{1-\pi(x_i, \beta)\} x_i$, where $\pi(x_i, \beta)  = \mu(x_i^{\T} \beta) = e^{x_i^{\T} \beta}  / (1 +e^{x_i^{\T} \beta} )$. So
$$
\hat{B} = n^{-1}\sumn \hat{\pi}_i (1-\hat{\pi}_i )  x_i x_i^{\T},\quad
\hat{M} = n^{-1}\sumn  \hat{\varepsilon}_i^2   x_i x_i^{\T}
$$
with fitted mean $\hat{\pi}_i = e^{x_i^{\T}  \hat{\beta}}  / (1 +e^{x_i^{\T}  \hat{\beta}} )  $ and residual $ \hat{\varepsilon}_i  = y_i - \hat{\pi}_i$, yielding a new covariance estimator
$$
\hat{V} = \left( \sumn \hat{\pi}_i (1-\hat{\pi}_i )   x_i x_i^{\T} \right)^{-1}
\left(\sumn \hat{\varepsilon}_i^2 x_i x_i^{\T} \right)
 \left( \sumn  \hat{\pi}_i (1-\hat{\pi}_i )   x_i x_i^{\T} \right)^{-1} . 
$$
\end{example}


\begin{example}[continued]\label{eg::countpoissonmodel-continue2}
In a working Poisson model, $\tilde{\sigma}^2(x_i, \beta)  = \lambda(x_i, \beta)   $ and $ \partial\mu(x_{i}^{\T} \beta ) / \partial\beta  = \lambda(x_i, \beta)  x_i$, where $\lambda(x_i, \beta)  = \mu(x_i^{\T} \beta) = e^{x_i^{\T} \beta}  $. So
$$
\hat{B} = n^{-1}\sumn \hat{\lambda}_i  x_i x_i^{\T},\quad
\hat{M} = n^{-1}\sumn  \hat{\varepsilon}_i^2   x_i x_i^{\T}
$$
with fitted mean $ \hat{\lambda}_i =  e^{x_i^{\T} \hat{\beta} }  $ and residual $ \hat{\varepsilon}_i  = y_i -  \hat{\lambda}_i $, yielding a new covariance estimator
$$
\hat{V} = \left( \sumn  \hat{\lambda}_i   x_i x_i^{\T} \right)^{-1}
\left(\sumn \hat{\varepsilon}_i^2 x_i x_i^{\T} \right)
 \left( \sumn \hat{\lambda}_i x_i x_i^{\T} \right)^{-1} . 
$$
\end{example}

 

Again, I relegate the derivation of the formulas for the Negative-Binomial regression as a homework problem. 
The \ri{R} package \ri{sandwich} implements the above covariance matrices \citep{zeileis2006object}. 



\section{Applications of the sandwich standard errors}

\subsection{Linear regression}


In \ri{R}, several functions can compute the EHW standard error: the \ri{hccm} function in the \ri{car} package, and the \ri{vcovHC} and \ri{sandwich} functions in the \ri{sandwich} package. The first two are special functions for OLS, and the third one works for general models. Below, we use these functions to compute various types of standard errors. 

\begin{lstlisting}
> library("car")
> library("lmtest")
> library("sandwich")
> library("mlbench")
> 
> ## linear regression
> data("BostonHousing")
> lm.boston = lm(medv ~ ., data = BostonHousing)
> hccm0     = hccm(lm.boston, type = "hc0")
> sandwich0 = sandwich(lm.boston, adjust = FALSE)
> vcovHC0   = vcovHC(lm.boston, type = "HC0")
> 
> hccm1     = hccm(lm.boston, type = "hc1") 
> sandwich1 = sandwich(lm.boston, adjust = TRUE) 
> vcovHC1   = vcovHC(lm.boston, type = "HC1")
> 
> hccm3     = hccm(lm.boston, type = "hc3") 
> vcovHC3   = vcovHC(lm.boston, type = "HC3")
> 
> dat.reg = data.frame(hccm0     = diag(hccm0)^(0.5),
+                      sandwich0 = diag(sandwich0)^(0.5),
+                      vcovHC0   = diag(vcovHC0)^(0.5),
+                      
+                      hccm1     = diag(hccm1)^(0.5),
+                      sandwich1 = diag(sandwich1)^(0.5),
+                      vcovHC1   = diag(vcovHC1)^(0.5),
+                      
+                      hccm3     = diag(hccm3)^(0.5),
+                      vcovHC3   = diag(vcovHC3)^(0.5))
> round(dat.reg[-1, ], 2)
        hccm0 sandwich0 vcovHC0 hccm1 sandwich1 vcovHC1 hccm3 vcovHC3
crim     0.03      0.03    0.03  0.03      0.03    0.03  0.03    0.03
zn       0.01      0.01    0.01  0.01      0.01    0.01  0.01    0.01
indus    0.05      0.05    0.05  0.05      0.05    0.05  0.05    0.05
chas1    1.28      1.28    1.28  1.29      1.29    1.29  1.35    1.35
nox      3.73      3.73    3.73  3.79      3.79    3.79  3.92    3.92
rm       0.83      0.83    0.83  0.84      0.84    0.84  0.89    0.89
age      0.02      0.02    0.02  0.02      0.02    0.02  0.02    0.02
dis      0.21      0.21    0.21  0.21      0.21    0.21  0.22    0.22
rad      0.06      0.06    0.06  0.06      0.06    0.06  0.06    0.06
tax      0.00      0.00    0.00  0.00      0.00    0.00  0.00    0.00
ptratio  0.12      0.12    0.12  0.12      0.12    0.12  0.12    0.12
b        0.00      0.00    0.00  0.00      0.00    0.00  0.00    0.00
lstat    0.10      0.10    0.10  0.10      0.10    0.10  0.10    0.10
\end{lstlisting}


The \ri{sandwich} function can compute HC0 and HC1, corresponding to adjusting for the degrees of freedom or not; \ri{hccm} and \ri{vcovHC} can compute other HC standard errors. 


\subsection{Logistic regression}

\subsubsection{An application}
In the flu shot example, two types of standard errors are rather similar. The simple logistic model does not seem to suffer from severe misspecification. 

\begin{lstlisting}
> flu = read.table("fludata.txt", header = TRUE)
> flu = within(flu, rm(receive))
> assign.logit = glm(outcome ~ ., 
+                    family  = binomial(link = logit), 
+                    data    = flu)
> summary(assign.logit)
Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -2.199815   0.408684  -5.383 7.34e-08 ***
assign      -0.197528   0.136235  -1.450  0.14709    
age         -0.007986   0.005569  -1.434  0.15154    
copd         0.337037   0.153939   2.189  0.02857 *  
dm           0.454342   0.143593   3.164  0.00156 ** 
heartd       0.676190   0.153384   4.408 1.04e-05 ***
race        -0.242949   0.143013  -1.699  0.08936 .  
renal        1.519505   0.365973   4.152 3.30e-05 ***
sex         -0.212095   0.144477  -1.468  0.14210    
liverd       0.098957   1.084644   0.091  0.92731    

> coeftest(assign.logit, vcov = sandwich)

z test of coefficients:

              Estimate Std. Error z value  Pr(>|z|)    
(Intercept) -2.1998145  0.4059386 -5.4191 5.991e-08 ***
assign      -0.1975283  0.1371785 -1.4399  0.149885    
age         -0.0079859  0.0057053 -1.3997  0.161590    
copd         0.3370371  0.1556781  2.1650  0.030391 *  
dm           0.4543416  0.1394709  3.2576  0.001124 ** 
heartd       0.6761895  0.1521105  4.4454 8.774e-06 ***
race        -0.2429488  0.1430957 -1.6978  0.089544 .  
renal        1.5195049  0.3659238  4.1525 3.288e-05 ***
sex         -0.2120954  0.1489435 -1.4240  0.154447    
liverd       0.0989572  1.1411133  0.0867  0.930894    
\end{lstlisting} 



\subsubsection{A misspecified logistic regression}

\citet{freedman2006so} discussed the following misspecified logistic regression. The discrepancy between the two types of standard errors is a warning of the misspecification of the conditional mean function because it determines the whole conditional distribution. In this case, it is not meaningful to interpret the coefficients. 

\begin{lstlisting}
> n = 100
> x = runif(n, 0, 10)
> prob.x = 1/(1 + exp(3*x - 0.5*x^2))
> y = rbinom(n, 1, prob.x)
> freedman.logit = glm(y ~ x, family = binomial(link = logit))
> summary(freedman.logit)
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -6.6764     1.3254  -5.037 4.72e-07 ***
x             1.1083     0.2209   5.017 5.25e-07 ***

> coeftest(freedman.logit, vcov = sandwich)

z test of coefficients:

            Estimate Std. Error z value Pr(>|z|)   
(Intercept) -6.67641    2.46035 -2.7136 0.006656 **
x            1.10832    0.39672  2.7937 0.005211 **
\end{lstlisting}



\subsection{Poisson regression}

\subsubsection{A correctly specified Poisson regression}

I first generate data from a correctly specified Poisson regression. The two types of standard errors are very close.

\begin{lstlisting}
> n = 1000
> x = rnorm(n)
> lambda.x = exp(x/5)
> y = rpois(n, lambda.x)
> pois.pois = glm(y ~ x, family = poisson(link = log))
> summary(pois.pois)
Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.004386   0.032117  -0.137    0.891    
x            0.189069   0.031110   6.077 1.22e-09 ***

> coeftest(pois.pois, vcov = sandwich)

z test of coefficients:

              Estimate Std. Error z value  Pr(>|z|)    
(Intercept) -0.0043862  0.0311957 -0.1406    0.8882    
x            0.1890691  0.0299124  6.3208 2.603e-10 ***
\end{lstlisting}


\subsubsection{A Negative-Binomial regression model}


I then generate data from a Negative-Binomial regression model. The conditional mean function is still $E(y_i\mid x_i) = e^{x_i^{\T} \beta}$, so we can still use Poisson regression as a working model. The robust standard error doubles the classical standard error. 

\begin{lstlisting}
> library(MASS)
> theta = 0.2
> y = rnegbin(n, mu = lambda.x, theta = theta)
> nb.pois = glm(y ~ x, family = poisson(link = log))
> summary(nb.pois)
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.07747    0.03315  -2.337   0.0194 *  
x            0.13847    0.03241   4.272 1.94e-05 ***

> coeftest(nb.pois, vcov = sandwich)

z test of coefficients:

             Estimate Std. Error z value Pr(>|z|)  
(Intercept) -0.077475   0.079431 -0.9754  0.32937  
x            0.138467   0.061460  2.2530  0.02426 *
\end{lstlisting}


Because the true model is the Negative-Binomial regression, we can use the correct model to fit the data. Theoretically, the MLE is the most efficient estimator. However, in this particular dataset, the robust standard error from Poisson regression is no larger than the one from Negative-Binomial regression. Moreover, the robust standard errors from the Poisson and Negative-Binomial regressions are very close. 

\begin{lstlisting}
> nb.nb = glm.nb(y ~ x)
> summary(nb.nb)
Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept) -0.08047    0.07336  -1.097   0.2727  
x            0.16487    0.07276   2.266   0.0234 *

> coeftest(nb.nb, vcov = sandwich)

z test of coefficients:

             Estimate Std. Error z value Pr(>|z|)   
(Intercept) -0.080467   0.079510  -1.012 0.311517   
x            0.164869   0.063902   2.580 0.009879 **
\end{lstlisting}


 

\subsubsection{Misspecification of the conditional mean}


When the conditional mean function is misspecified, the Poisson and Negative-Binomial regressions give different point estimates, and it is unclear how to compare the standard errors. 

\begin{lstlisting}
> lambda.x = x^2
> y = rpois(n, lambda.x)
> wr.pois = glm(y ~ x, family = poisson(link = log))
> summary(wr.pois)
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.03760    0.03245  -1.159 0.246457    
x            0.11933    0.03182   3.751 0.000176 ***

> coeftest(wr.pois, vcov = sandwich)

z test of coefficients:

             Estimate Std. Error z value Pr(>|z|)
(Intercept) -0.037604   0.053033 -0.7091   0.4783
x            0.119331   0.101126  1.1800   0.2380

> 
> wr.nb = glm.nb(y ~ x)
There were 26 warnings (use warnings() to see them)
> summary(wr.nb)
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  0.15984    0.05802   2.755  0.00587 ** 
x           -0.34622    0.05789  -5.981 2.22e-09 ***

> coeftest(wr.nb, vcov = sandwich)

z test of coefficients:

             Estimate Std. Error z value Pr(>|z|)   
(Intercept)  0.159837   0.061564  2.5963 0.009424 **
x           -0.346223   0.238124 -1.4540 0.145957   
\end{lstlisting}


Overall, for count outcome regression, it seems that Poisson regression suffices as long as we use the robust standard error. The Negative-Binomial is unlikely to offer more if only the conditional mean is of interest. 




\subsection{Poisson regression for binary outcomes}


\citet{zou2004modified} proposed to use Poisson regression to analyze binary outcomes. This can be reasonable if the parameter of interest is the risk ratio instead of the odds ratio. Importantly, since the Poisson model is a wrong model, we must use the sandwich covariance estimator. 

 


\subsection{How robust are the robust standard errors?}


Section \ref{sec::rmm-estimation-equation} discusses the restricted mean model as an extension of the GLM, allowing for misspecification of the GLM while still preserving the conditional mean. We can extend the discussion to other parametric models. 
\citet{huber::1967} started the literature on the statistical properties of the MLE in a misspecified model, and
\citet{white1982maximum} addressed detailed inferential problems. 
\citet{buja2019models2} reviewed this topic recently. 



The discussion in Section \ref{sec::rmm-estimation-equation} is useful when the conditional mean is correctly specified. However, if we think the GLM is severely misspecified with a wrong conditional mean, then the robust sandwich standard errors are unlikely to be helpful because the MLE converges to a wrong parameter in the first place \citep{freedman2006so}.  

 




\section{Homework problems}

 

\paragraph{MLE in GLMs with binary regressors}\label{hw20::mle-binary-z-misspecified}


Continue with Problem \ref{hw20::mle-binary-z}. Find the variance estimators of $\hat\beta$ without assuming the models are correct. 





 
\paragraph{Negative-Binomial covariance matrices}\label{hw20::nb-sandwich-misspecified}


Continue with Problem \ref{hw20::nb-sandwich}. Derive the estimated asymptotic covariance matrices of the MLE without assuming the Negative-Binomial model is correct. 

 


\paragraph{Robust standard errors in the Karolinska data}\label{hw20::robust-se-karolinska}

Report the robust standard errors in the case study of Section \ref{sec::multinomial-case-study} in Chapter \ref{chapter::logit-categorical}. For some models, the function \ri{coeftest(*, vcov = sandwich)} does work. Alternatively, you can use the nonparametric bootstrap to obtain the robust standard errors. 



\paragraph{Robust standard errors in the gym data}

Report the robust standard errors in the case study of Section \ref{sec::count-case-study} in Chapter \ref{chapter::count}. 



 
