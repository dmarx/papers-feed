 
\chapter{The Gauss--Markov Model and Theorem}\label{chapter::gauss-markov}
 

\section{Gauss--Markov model}

Without any stochastic assumptions, the OLS in Chapter \ref{chapter::ols-vector} 
is purely algebraic. From now on, we want to discuss the statistical
properties of $\hat{\beta}$ and associated quantities, so we need
to invoke some statistical modeling assumptions. A simple starting point is the
following Gauss--Markov model with a fixed design matrix $X$ and unknown
parameters $(\beta,\sigma^{2})$. 

\begin{assumption}[Gauss--Markov model]\label{assume::gm-model}
We have
\[
Y=X\beta+\varepsilon
\]
where the design matrix $X$ is fixed with linearly independent column vectors, and the random error term $\varepsilon$ has the first two moments
\begin{eqnarray*}
E(\varepsilon)  &=&  0,\\ 
\cov(\varepsilon) &=& \sigma^{2}I_{n} . 
\end{eqnarray*}
The unknown parameters are $ (\beta,   \sigma^{2} )$.
\end{assumption}

The Gauss--Markov model assumes that $Y$ has mean $X\beta$ and covariance
matrix $\sigma^{2}I_{n}.$ At the individual level, we can also write
it as
\[
y_{i}=x_{i}^{\T}\beta+\varepsilon_{i},\qquad(i=1,\ldots,n)
\]
where the error terms are uncorrelated with mean $0$ and variance
$\sigma^{2}$. 


The assumption that $X$ is fixed is not essential, because we can condition on $X$ even if we think $X$ is random. The mean of each $y_i$ is linear in $x_i$ with the same $\beta$ coefficient, which is a rather strong assumption. So is the {\it homoskedasticity}\footnote{In this book, I do not spell it as {\it homoscedasticity} since ``k'' better indicates the meaning of variance. \citet{mcculloch1985miscellanea} gave a convincing argument. See also \citet{paloyo2014did}.
} assumption that the error terms have the same variance $\sigma^2$. The critiques on the assumptions aside, I will  derive the properties of $\hat{\beta}$
under the Gauss--Markov model. 

\section{Properties of the OLS estimator}

I first derive the mean and covariance of $\hat{\beta}=(X^{\T}X)^{-1}X^{\T}Y$.
\begin{theorem}
\label{thm:varOLS}Under Assumption  \ref{assume::gm-model}, we have 
\begin{eqnarray*}
E(\hat{\beta}) &=& \beta,\\ 
\cov(\hat{\beta}) &=&  \sigma^{2}(X^{\T}X)^{-1}.
\end{eqnarray*}
\end{theorem}

\begin{myproof}{Theorem}{\ref{thm:varOLS}}
Because $E(Y)=X\beta,$ we have
\begin{eqnarray*}
E(\hat{\beta}) &=& E\left\{ (X^{\T}X)^{-1}X^{\T}Y\right\} \\
 &=& (X^{\T}X)^{-1}X^{\T}E(Y) \\
 &=&(X^{\T}X)^{-1}X^{\T}X\beta \\
 &=&\beta.
\end{eqnarray*}
 Because $\cov(Y)=\sigma^{2}I_{n}$, we have 
\begin{align*}
\cov(\hat{\beta}) & =\cov\left\{ (X^{\T}X)^{-1}X^{\T}Y\right\}  \\
&=(X^{\T}X)^{-1}X^{\T}\cov(Y)X(X^{\T}X)^{-1}\\
 & =\sigma^{2}(X^{\T}X)^{-1}X^{\T}X(X^{\T}X)^{-1} \\
 & =\sigma^{2}(X^{\T}X)^{-1}.
\end{align*}
\end{myproof}


We can decompose the response vector as 
\[
Y=\hat{Y}+\hat{\varepsilon},
\]
 where the fitted vector is $\hat{Y}=X\hat{\beta}=HY$ and the
residual vector is $\hat{\varepsilon}=Y-\hat{Y}=(I_{n}-H)Y.$ The two
matrices $H$ and $I_{n}-H$ are the keys, which have the following
properties. 



\begin{lemma}
\label{lem:projectionms}
Both $H$ and $I_{n}-H$ are projection matrices. In particular, 
$$
HX=X,\quad (I_{n}-H)X=0 , 
$$ 
and they are orthogonal: 
$$
H(I_{n}-H)=(I_{n}-H)H=0.
$$
\end{lemma}

These follow from simple linear algebra, and I leave the proof as Problem \ref{hw::gauss-markov-projectionm}. It states that $H$ and $I_{n}-H$ are projection matrices onto the column space of $X$ and its complement. 
 Algebraically, $\hat{Y}$ and $\hat{\varepsilon}$ are orthogonal
by the OLS projection because Lemma \ref{lem:projectionms} implies
\begin{eqnarray*}
\hat{Y}^{\T}\hat{\varepsilon} &=& Y^{\T}H^{\T}(I_{n}-H)Y  \\
&=&  Y^{\T}H(I_{n}-H)Y \\
&=& 0.
\end{eqnarray*}
This is also coherent with the geometry in Figure \ref{fig::olsgeometry}. 
 

Moreover, we can derive the mean and covariance matrix of $\hat{Y}$
and $\hat{\varepsilon}$.
\begin{theorem}
\label{thm:GMcov}Under Assumption \ref{assume::gm-model}, we have 
\[
E\left(\begin{array}{c}
\hat{Y}\\
\hat{\varepsilon}
\end{array}\right)=\left(\begin{array}{c}
X\beta\\
0
\end{array}\right)
\]
and 
\[
\cov\left(\begin{array}{c}
\hat{Y}\\
\hat{\varepsilon}
\end{array}\right)=\sigma^{2}\left(\begin{array}{cc}
H & 0 \\
0 & I_{n}-H
\end{array}\right).
\]
So $\hat{Y}$ and $\hat{\varepsilon}$
are uncorrelated. 
\end{theorem}


Please do not be confused with the two statements above. First, $\hat{Y}$ and $\hat{\varepsilon}$ are orthogonal. Second, $\hat{Y}$ and $\hat{\varepsilon}$
are uncorrelated. They have different meanings. The first statement is an algebraic fact of the OLS procedure. It is about a relationship between two vectors $\hat{Y}$ and $\hat{\varepsilon}$ which holds without assuming the Gauss--Markov model. The second statement is stochastic. It is about a relationship between two random vectors $\hat{Y}$ and $\hat{\varepsilon}$ which requires the Gauss--Markov model assumption. 


\begin{myproof}{Theorem}{\ref{thm:GMcov}}
The conclusion follows from the simple fact that 
\[
\left(\begin{array}{c}
\hat{Y}\\
\hat{\varepsilon}
\end{array}\right)=\left(\begin{array}{c}
HY\\
(I_{n}-H)Y
\end{array}\right)=\left(\begin{array}{c}
H\\
I_{n}-H
\end{array}\right)Y
\]
 is a linear transformation of $Y$. It has mean
\begin{eqnarray*}
E\left(\begin{array}{c}
\hat{Y}\\
\hat{\varepsilon}
\end{array}\right)
&=& \left(\begin{array}{c}
H\\
I_{n}-H
\end{array}\right)E(Y) \\
&=&\left(\begin{array}{c}
H\\
I_{n}-H
\end{array}\right)X\beta \\
&=&\left(\begin{array}{c}
HX\beta\\
\left(I_{n}-H\right)X\beta
\end{array}\right) \\
&=&\left(\begin{array}{c}
X\beta\\
0
\end{array}\right),
\end{eqnarray*}
and covariance matrix
\begin{align*}
\cov\left(\begin{array}{c}
\hat{Y}\\
\hat{\varepsilon}
\end{array}\right) & =\left(\begin{array}{c}
H\\
I_{n}-H
\end{array}\right)\cov(Y)\left(\begin{array}{cc}
H^{\T} & (I_{n}-H)^{\T}\end{array}\right)\\
 & =\sigma^{2}\left(\begin{array}{c}
H\\
I_{n}-H
\end{array}\right)\left(\begin{array}{cc}
H & I_{n}-H\end{array}\right)\\
 & =\sigma^{2}\left(\begin{array}{cc}
H^{2} & H(I_{n}-H)\\
(I_{n}-H)H & (I_{n}-H)^{2}
\end{array}\right)\\
 & =\sigma^{2}\left(\begin{array}{cc}
H & 0\\
0 & I_{n}-H
\end{array}\right),
\end{align*}
where the last step follows from Lemma \ref{lem:projectionms}. 
\end{myproof}

Assume the Gauss--Markov model. 
Although the original responses and error terms are uncorrelated between
units with $\cov(\varepsilon_i, \varepsilon_j)=0$ for $i\neq j$, the fitted values and the residuals are correlated with 
$$
\cov(\hat{y}_i, \hat{y}_j) = \sigma^2 h_{ij},\quad 
\cov(\hat{\varepsilon}_i, \hat{\varepsilon}_j) =  \sigma^2 (1 -h_{ij} ) 
$$ 
for $i\neq j$ based on Theorem
\ref{thm:GMcov}. 

\section{Variance estimation}

Theorem \ref{thm:varOLS} quantifies the uncertainty of $\hat{\beta}$
by its covariance matrix. However, it is not directly useful because
$\sigma^{2}$ is still unknown. Our next task is to estimate $\sigma^{2}$ based on
the observed data. It is the variance of each $\varepsilon_{i}$,
but the $\varepsilon_{i}$'s are not observable either. Their empirical
analogues are the residuals $\hat{\varepsilon}_{i}=y_{i}-x_{i}^{\T}\hat{\beta}$.
It seems intuitive to estimate $\sigma^{2}$ by 
\[
\tilde{\sigma}^{2}=\textsc{rss}/n
\]
where
\[
\textsc{rss}=\sumn\hat{\varepsilon}_{i}^{2}
\]
is the residual sum of squares. However, Theorem \ref{thm:GMcov}
shows that $\hat{\varepsilon}_{i}$ has mean zero and variance $\sigma^{2}(1-h_{ii}),$
which is not the same as the variance of original $\varepsilon_{i}.$ Consequently,  $\textsc{rss}$ has mean 
\begin{eqnarray*}
E(\textsc{rss}) 
&=& 
\sumn\sigma^{2}(1-h_{ii}) \\
&=& \sigma^{2}\left\{ n-\text{trace}(H)\right\} \\
&=& \sigma^{2}(n-p),
\end{eqnarray*}
which implies the following theorem. 


\begin{theorem}
\label{thm:varianceestOLS}Define 
$$
\hat{\sigma}^{2}=\textsc{rss}/(n-p)=\sumn\hat{\varepsilon}_{i}^{2}/(n-p).
$$
Then $E(\hat{\sigma}^{2})=\sigma^{2}$ under Assumption \ref{assume::gm-model}. 
\end{theorem}


Theorem \ref{thm:varianceestOLS} implies that $\tilde{\sigma}^{2}$
is a biased estimator for $\sigma^{2}$ because $E(\tilde{\sigma}^{2})=\sigma^{2}(n-p)/n.$
It underestimates $\sigma^{2}$ but with a large sample size $n$, the bias is
small. 

\section{Gauss--Markov Theorem}

So far, we have focused on the OLS estimator. It is intuitive, but
we have not answered the fundamental question yet. Why should we focus
on it? Are there any other better estimators? Under the Gauss--Markov
model, the answer is definite: we focus on the OLS estimator because
it is optimal in the sense of having the smallest covariance matrix
among all linear unbiased estimators. The following famous Gauss--Markov
theorem quantifies this claim, which was named after Carl Friedrich Gauss and Andrey Markov\footnote{\citet{david1938extensions} used the name Markoff theorem. \citet{lehmann1951general} appeared to first use the name Gauss--Markov theorem.}. It is for this reason that I call the
corresponding model the Gauss--Markov model. The textbook by \citet{monahan2008primer} also uses this name. 







\begin{theorem}
\label{thm:GMtheorem}Under Assumption \ref{assume::gm-model}, the OLS estimator
$\hat{\beta}$ for $\beta$ is the best linear unbiased estimator
(BLUE) in the sense that\footnote{We write $M_1 \succ M_2$  is $M_1 - M_2$ is positive semi-definite. See Chapter \ref{chapter::linear-algebra} for a review.} 
$$
\cov(\tilde{\beta})  \succeq 
\cov(\hat{\beta})
$$
for any estimator $\tilde{\beta}$ satisfying 
\begin{enumerate}
[(C1)]
\item\label{item::gm-linear} $\tilde{\beta}=AY$ for some $A\in\mathbb{R}^{p\times n}$ not depending
on $Y$;
\item\label{item::gm-unbiased} $ E (\tilde{\beta} ) = \beta$   for any
$\beta$.
\end{enumerate}
\end{theorem}

Before proving Theorem \ref{thm:GMtheorem}, we need to understand its meaning and immediate implications. We do not compare the OLS estimator with any arbitrary estimators. In fact, we restrict to the estimators that are linear and unbiased.  Condition (C\ref{item::gm-linear}) requires that  $\tilde{\beta}$ is a linear estimator. More precisely, it is a linear transformation of the
response vector $Y$, where $A$ can be any complex and possibly nonlinear function of $X$. Condition (C\ref{item::gm-unbiased}) requires that $\tilde{\beta}$  is an unbiased estimator for $\beta$, no matter what true value $\beta$ takes. 



Why do we restrict the estimator to be linear? The class of linear estimator is actually quite large because $A$ can be any nonlinear function of $X$, and the only requirement is that the estimator is linear in $Y$. The unbiasedness is a natural requirement for many problems. However, in many modern applications with many covariates, some biased estimators can perform better than unbiased estimators if they have smaller variances. We will discuss these estimators in Part \ref{part::modelselection} of this book.  


We compare the estimators based on their covariances, which are natural extensions of variances for scalar random variables. The conclusion $\cov(\tilde{\beta}) \succeq  \cov(\hat{\beta})$ implies that for any vector $c  \in \mathbb{R}^p$, we have
$$
c ^{\T}\cov(\tilde{\beta}) c 
\succeq
c ^{\T}  \cov(\hat{\beta})  c  
$$
which is equivalent to
$$
\var(  c ^{\T} \tilde{\beta} ) \geq   \var(c ^{\T}  \hat{\beta}) ,
$$
So any linear transformation of the OLS estimator has a variance smaller than or equal to the same linear transformation of any other estimator. In particular, if $c  = (0,\ldots, 1,\ldots, 0)^{\T}$ with only the $j$th coordinate being $1$, then the above inequality implies that 
$$
\var(\tilde{\beta}_j) \geq 
\var(\hat{\beta}_j)   ,\quad (j=1,\ldots, p).
$$
So the OLS estimator has a smaller variance than other estimators for all coordinates. 

Now we prove the theorem. 

\begin{myproof}{Theorem}{\ref{thm:GMtheorem}}
We must verify that the OLS estimator itself satisfies (C\ref{item::gm-linear}) and (C\ref{item::gm-unbiased}). We have $\hat{\beta}=\hat{A}Y$ with  $\hat{A}=(X^{\T}X)^{-1}X^{\T}$, and it is unbiased by Theorem \ref{thm:varOLS}. 

First, the unbiasedness requirement implies that
\begin{eqnarray*}
E(\tilde{\beta}) =\beta & \Longrightarrow & E(AY)=AE(Y)=AX\beta=\beta \\
&\Longrightarrow & AX\beta=\beta
\end{eqnarray*}
 for any value of $\beta$. So 
\begin{eqnarray}\label{eq::gaussmarkov-unbiasedcondition}
 AX=I_{p}
\end{eqnarray}
 must hold. In particular,
the OLS estimator satisfies $\hat{A}X = (X^{\T}X)^{-1}X^{\T} X =I_{p}.$

Second, we can decompose the covariance of $\tilde{\beta}$ as
\begin{eqnarray*}
\cov(\tilde{\beta}) 
&=& \cov(\hat{\beta} + \tilde{\beta} - \hat{\beta} ) \\
&=&  \cov(\hat{\beta} )  + \cov ( \tilde{\beta} - \hat{\beta} ) 
+ \cov(\hat{\beta} ,  \tilde{\beta} - \hat{\beta})   +  \cov(   \tilde{\beta} - \hat{\beta}, \hat{\beta}) .
%& =&\cov(AY)\\
% & =&\cov(AY-\hat{A}Y+\hat{A}Y)\\
% & =&\cov\left\{ (A-\hat{A})Y\right\} +\cov(\hat{A}Y) \\
% &&+\cov\left\{ (A-\hat{A})Y,\hat{A}Y\right\} +\cov\left\{ \hat{A}Y,(A-\hat{A})Y\right\} .
\end{eqnarray*}
The last two terms are in fact zero. By symmetry, we only need to
show that the third term is zero:
\begin{eqnarray*}
  \cov(\hat{\beta} ,  \tilde{\beta} - \hat{\beta})   
& = &\cov\left\{ \hat{A}Y,(A-\hat{A})Y\right\} \\
 & =&\hat{A}\cov(Y)(A-\hat{A})^{\T}\\
 & =&\sigma^{2}\hat{A}(A-\hat{A})^{\T}\\
 & =&\sigma^{2}(\hat{A}A^{\T}-\hat{A}\hat{A}^{\T})\\
 & =&\sigma^{2}\left\{ (X^{\T}X)^{-1}X^{\T}A^{\T}-(X^{\T}X)^{-1}X^{\T}X(X^{\T}X)^{-1}\right\} \\
 & =&\sigma^{2}\left\{ (X^{\T}X)^{-1}I_{p}-(X^{\T}X)^{-1}\right\}  \qquad (\text{by } \eqref{eq::gaussmarkov-unbiasedcondition}) \\
 & =&0.
\end{eqnarray*}
The above covariance decomposition simplifies to
\[
\cov(\tilde{\beta})= \cov(\hat{\beta} )  + \cov ( \tilde{\beta} - \hat{\beta} )  , 
\]
which implies 
\[
\cov(\tilde{\beta})-\cov(\hat{\beta}) = \cov(\tilde{\beta} - \hat{\beta}) \succeq 0. 
\]
% which implies that $\cov(\hat{\beta})\preceq\cov(\tilde{\beta})$. 
\end{myproof}




In the process of the proof, we have shown two stronger results
$$
\cov(\tilde{\beta} - \hat{\beta}, \hat{\beta}) = 0
$$
and
$$
\cov(\tilde{\beta} - \hat{\beta})  =  \cov(\tilde{\beta})-\cov(\hat{\beta}).
$$
They hold only when $\hat{\beta}$ is BLUE. They do not hold when comparing two general estimators. 


Theorem \ref{thm:GMtheorem} is elegant but abstract. It says that
in some sense, we can just focus on the OLS estimator because it is
the best one in terms of the covariance among all linear unbiased estimators. Then
we do not need to consider other estimators. However, we have not
mentioned any other estimators for $\beta$ yet, which makes Theorem
\ref{thm:GMtheorem} not concrete enough. From the proof above, a
linear unbiased estimator $\tilde{\beta}=AY$ only needs to satisfy
$AX=I_{p}$, which imposes $p^{2}$ constraints on the $p\times n$
matrix $A$. Therefore, we have $p(n-p)$ free parameters to choose
from and have infinitely many linear unbiased estimators in general. A
class of linear unbiased estimators discussed more thoroughly in Chapter \ref{chapter::WLS}, are the weighted least squares estimators
\[
\tilde{\beta}=(X^{\T}\Sigma^{-1}X)^{-1}X^{\T}\Sigma^{-1}Y,
\]
 where $\Sigma$ is a positive definite matrix not depending on $Y$
such that $\Sigma$ and $X^{\T}\Sigma^{-1}X$ are invertible. It is linear, and we
can show that it is unbiased for $\beta$:
\begin{eqnarray*}
E(\tilde{\beta}) 
&=& E\left\{ (X^{\T}\Sigma^{-1}X)^{-1}X^{\T}\Sigma^{-1}Y\right\} \\
&=& (X^{\T}\Sigma^{-1}X)^{-1}X^{\T}\Sigma^{-1}X\beta \\
&=&\beta.
\end{eqnarray*}
Different choices of $\Sigma$ give different $\tilde{\beta},$ but
Theorem \ref{thm:GMtheorem} states that the OLS estimator with $\Sigma=I_{n}$
has the smallest covariance matrix under the Gauss--Markov model. 

I will give an extension and some applications of the Gauss--Markov Theorem as homework problems. 



 

\section{Homework problems}

\paragraph{Projection matrices}\label{hw::gauss-markov-projectionm}

Prove Lemma \ref{lem:projectionms}. 



\paragraph{Univariate OLS and the optimal design}
\label{hw::gaussmarkov-1d}

Assume the Gauss--Markov model $y_i = \alpha + \beta x_i + \varepsilon_i$ $(i=1,\ldots, n)$ with a scalar $x_i$. Show that the variance of the OLS coefficient for $x_i$ equals
$$
\var(  \hat{\beta} ) = \sigma^2 \Big / \sumn (x_i - \bar{x})^2 .
$$

Assume $x_i$ must be in the interval $[0, 1]$. We want to choose their values to minimize $\var(  \hat{\beta} )$. Assume that $n$ is an even number. Find the minimizers $x_i$'s. 

Hint: You may find the following probability result useful. For a random variable $\xi$ in the interval $[0, 1]$, we have the following inequality
\begin{eqnarray*}
\var(\xi) &=& E(\xi^2) - \{ E(\xi) \}^2 \\
 &\leq & E(\xi) - \{ E(\xi) \}^2  \\
 &=& E(\xi)\{ 1 - E(\xi) \} \\
 &\leq & 1/4. 
\end{eqnarray*}
The first inequality becomes an equality if and only if $\xi=0$ or $1$; the second inequality becomes an equality if and only if $E(\xi) = 1/2.$



\paragraph{BLUE estimator for the mean}\label{hw04::blue-mean}

Assume that $y_i$ has mean $\mu$ and variance $\sigma^2$, and $y_i\ (i=1,\ldots, n)$ are uncorrelated. A linear estimator of the mean $\mu$ has the form $\hat{\mu} = \sumn a_i y_i$, which is unbiased as long as $ \sumn a_i = 1$. So there are infinitely many linear unbiased estimators for $\mu$. 

Find the BLUE for $\mu$ and prove why it is BLUE. 




\paragraph{Consequence of useless regressors}\label{hw4::useless-regressor}

Partition the covariate matrix and parameter into 
$$
X=(X_1, X_2),\quad \beta = \begin{pmatrix}
\beta_1\\
\beta_2
\end{pmatrix},
$$
where $X_1 \in \mathbb{R}^{n\times k}, X_2 \in \mathbb{R}^{n\times l}, \beta_1 \in \mathbb{R}^k$ and $\beta_2 \in \mathbb{R}^l$ with $k+l=p$. Assume the Gauss--Markov model with $\beta_2 = 0$. Let $\hat{\beta}_1$ be the first $k$ coordinates of $\hat{\beta} = (X^{\T}X)^{-1} X^{\T} Y$ and $\tilde{\beta}_1 = (X_1^{\T}X_1)^{-1} X_1^{\T} Y$ be the coefficient based on the partial OLS fit of $Y$ on $X_1$ only. Show that 
$$
\cov(\hat{\beta}_1) \succeq \cov(\tilde{\beta}_1).
$$


 
 
 \paragraph{Simple average of subsample OLS coefficients}\label{hw4::average-subsample-ols}
 
 Inherit the setting of Problem \ref{hw3::full-subsample-ols}. Define the simple average of the subsample OLS coefficients as $\bar{\beta} = K^{-1}\sum_{k=1}^{K} \hat{\beta}_{(k)}$. Assume the Gauss--Markov model. Show that 
 $$
\cov(\bar{\beta}) \succeq \cov(\hat{\beta}).
$$


 
 \paragraph{Gauss--Markov theorem for prediction}\label{hw4::gaussmarkov-prediction}

Under Assumption \ref{assume::gm-model}, the OLS predictor 
$\hat{Y}=X\hat{\beta}$ for the mean $X\beta$ is the best linear unbiased predictor in the sense that $\cov(\tilde{Y}) \succeq \cov( \hat{Y} )$
for any predictor $\tilde{Y}$ satisfying 
\begin{enumerate}
[(C1)]
\item\label{item::gm-linear-pred} $\tilde{Y}=\tilde{H}Y$ for some $ \tilde{H} \in\mathbb{R}^{n \times n}$ not depending
on $Y$;
\item\label{item::gm-unbiased-pred} $E ( \tilde{Y} ) = X \beta$   for any $\beta$. 
\end{enumerate}
Prove this theorem. 




\paragraph{Nonlinear unbiased estimator under the Gauss--Markov model}
\label{hw4::nonlinear-unbiased-underGM}

Under Assumption \ref{assume::gm-model}, prove that if 
$$
X^{\T} Q_j X = 0, \quad \text{trace}(Q_j) = 0, \quad (j=1,\ldots, p)
$$
then
$$
\tilde\beta = \hat\beta + \begin{pmatrix}
Y^{\T} Q_1 Y \\
\vdots \\
Y^{\T} Q_p Y 
\end{pmatrix}
$$
is unbiased for $\beta$. 


Remark: The above estimator $\tilde\beta $ is a quadratic function of $Y$. It is a nonlinear unbiased estimator for $\beta$. It is not difficult to show the unbiasedness. More remarkably, \citet[][Theorem 4.3]{koopmann1982parameterschatzung} showed that under Assumption \ref{assume::gm-model}, any unbiased estimator for $\beta$ must have the form of $\tilde\beta $.



