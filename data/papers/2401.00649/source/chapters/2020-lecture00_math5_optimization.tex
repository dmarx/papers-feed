 
\chapter{Basics of Optimization}
\label{chapter::optimization} 





\section{Unconstrained optimization}


Define an unconstrained optimization problem as
$$
\min_{x\in \mathbb{R}^p} f(x).
$$



\section{Constrained optimization}



  
  We can apply the Lagranger multipliers to give another proof of the Gauss--Markov theorem based on optimization. To show that $\hat{\beta}$ is BLUE, we only need to show that $\ell^{\T} \hat\beta = \ell^{T}(X^{\T}X)^{-1}X^{\T}Y$ is BLUE for $\ell^{\T}\beta$ for any $\ell$. That is, among all linear unbiased estimators $c^{\T}Y$, the one with the minimum variance corresponds to $\hat{c} = \ell^{T}(X^{\T}X)^{-1}X^{\T}. $

Under the Gauss--Markov model, $\var(c^{\T}Y) = \sigma^2 c^{\T} c$. The unbiasedness requires
$
E( c^{\T}Y ) = c^{\T} X\beta  = \ell^{\T} \beta
$
for all $\beta$, so 
$$
 X^{\T} c = \ell.
$$
So the proof reduces to solving the following constrained optimization problem:
$$
\min_c   \frac{1}{2} c^{\T} c \text{ such that }  X^{\T} c = \ell.
$$
The corresponding Lagrange multiplier is
$$
  \frac{1}{2}  c^{\T}c-\lambda^{\T}(X^{\T}c-\ell) .
$$
From the first order condition, we have 
$$
c-X\lambda=0 \Longrightarrow   c=X\lambda ,
$$
which, coupled with the constraint, gives
$$
X^{\T}X\lambda=\ell  \Longrightarrow   \lambda = (X^{\T}X)^{-1}\ell \Longrightarrow   c=X(X^{\T}X)^{-1}\ell.
$$
By convexity, $\hat{c} = \ell^{T}(X^{\T}X)^{-1}X^{\T}$ must be the minimizer, resulting in the OLS estimator $ \hat{c}^{\T} Y =  \ell^{T}(X^{\T}X)^{-1}X^{\T}Y=\ell^{\T}\hat{\beta}$ for $\ell^{\T}\beta$. 



Reference: Simplifying General Least Squares Kevin Hayes \& John Haslett



\section{Homework problems}


\paragraph{???}\label{hw-math5::??}

 