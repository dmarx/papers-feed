 
\chapter{Motivations for Statistical Models}\label{chapter::motivation}
 

\section{Data and statistical models}
A wide range of problems in statistics and machine learning have the data structure as below: 

 
\begin{tabular}{|c|c|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}
Unit & outcome/response & \multicolumn{4}{c|}{covariates/features/predictors}\tabularnewline
\hline 
\hline 
$i$ & $Y$ & $X_{1}$ & $X_{2}$ & $\cdots$ & $X_{p}$\tabularnewline
\hline 
$1$ & $y_{1}$ & $x_{11}$ & $x_{12}$ & $\cdots$ & $x_{1p}$\tabularnewline
\hline 
$2$ & $y_{2}$ & $x_{21}$ & $x_{22}$ & $\cdots$ & $x_{2p}$\tabularnewline
\hline 
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &  & $\vdots$\tabularnewline
\hline 
$n$ & $y_{n}$ & $x_{n1}$ & $x_{n2}$ & $\cdots$ & $x_{np}$\tabularnewline
\end{tabular}
 
 
For each unit $i$, we observe the outcome/response of interest, $y_i$, as well as $p$  covariates/features/predictors. 
We often use 
\[
Y=\left(\begin{array}{c}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right)
\]
to denote the $n$-dimensional outcome/response vector, and 
\[
X=\left(\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1p}\\
x_{21} & x_{22} & \cdots & x_{2p}\\
\vdots & \vdots &  & \vdots\\
x_{n1} & x_{n2} & \cdots & x_{np}
\end{array}\right)
\]
to denote the $n\times p$ covariate/feature/predictor matrix, also called the {\it design matrix}. In
most cases, the first column of $X$ contains constants $1$s. 

Based on the data $(X,Y)$ , we can ask the following questions:
\begin{enumerate}[(Q1)]
\item Describe the relationship between $X$ and $Y$, i.e., their association
or correlation. For example, how is the patients' average height related
to the children's average height? How is one's height related to one's
weight? How are one's education and working experience related to one's income?

\item Predict $Y^{*}$ based on new data $X^{*}$. In particular,
we want to use the current data $(X,Y)$ to train a predictor, and
then use it to predict future $Y^*$ based on future $X^*$. This is called
{\it supervised learning} in the field of machine learning. For example,
how do we predict whether an email is spam or not based on the frequencies of the most commonly occurring words and punctuation marks in the email? How do we predict cancer patients' survival time based on their clinical measures?


\item Estimate the causal effect of some components in $X$ on $Y$. What if we change
some components of $X$? How do we measure the impact of the hypothetical
intervention of some components of $X$ on $Y$? This is a much harder
question because most statistical tools are designed to infer association, not causation. For example, the U.S. Food and Drug Administration (FDA) approves drugs based on randomized
controlled trials (RCT) because RCTs are most credible to infer causal
effects of drugs on health outcomes. Economists are interested in evaluating the effect of a
job training program on employment and wages. However, this is a notoriously difficult problem with only observational data. 
\end{enumerate}




The above descriptions are about generic $X$ and $Y$, which can be
many different types. We often use different statistical models to
capture the features of different types of data. I give a brief overview
of models that will appear in later parts of this book.
\begin{enumerate}[(T1)]
\item $X$ and $Y$ are univariate and continuous. In Francis Galton's\footnote{Who was Francis Galton? He was Charles Darwin's nephew and was famous
for his pioneer work in statistics and for devising a method for classifying
fingerprints that proved useful in forensic science. He also invented the term {\it eugenics}, a field that causes a lot of controversies nowadays.} classic example, $X$ is the parents' average height and $Y$ is the children's
average height \citep{galton1886regression}. Galton derived the following formula:
\[
y = \bar{y} + \hat\rho \frac{\hat{\sigma}_{y}}{\hat{\sigma}_{x}} (x-\bar{x})
\]
which is equivalent to
\begin{equation}
\label{eq::galtonian-formula-1}
\frac{y-\bar{y}}{\hat{\sigma}_{y}}
= \hat\rho \frac{x-\bar{x}}{\hat{\sigma}_{x}},
\end{equation}
where 
$$
\bar{x} = n^{-1} \sumn x_i,\qquad \bar{y} = n^{-1} \sumn y_i
$$ 
are the sample means, 
$$
\hat{\sigma}_{x}^2 = (n-1)^{-1} \sumn (x_i - \bar{x})^2,\qquad \hat{\sigma}_{y}^2 =  (n-1)^{-1} \sumn (y_i - \bar{y})^2
$$ 
are the sample variances, and $ \hat\rho =  \hat{\sigma}_{xy} / ( \hat{\sigma}_{x} \hat{\sigma}_{xy} ) $ is the
sample Pearson correlation coefficient with the sample covariance 
$$
\hat{\sigma}_{xy}  =  (n-1)^{-1} \sumn(x_{i}-\bar{x})(y_{i}-\bar{y}).
$$
%\[
%
% \hat\rho=\frac{\sumn(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sqrt{\sumn(x_{i}-\bar{x})^{2}}\sqrt{\sumn(y_{i}-\bar{y})^{2}}}.
%\]
 This is the famous formula of ``regression towards mediocrity'' or ``regression towards the mean''. Galton first introduced the terminology ``regression.'' Galton called regression because the relative deviation of the children's average height is smaller than that of the parents' average height if $ | \hat\rho  | < 1$. We will derive the above Galtonian formula in Chapter \ref{chapter::ols-1d}. 
The name ``regression'' is widely used in statistics now. For instance, we sometimes use ``linear regression'' interchangeably with ``linear model''; we also extend the name to ``logistic regression'' or ``Cox regression'' which will be discussed in later chapters of this book. 
 
 
 
\item $Y$ univariate and continuous, and $X$ multivariate of mixed types.
In the \ri{R} package \ri{ElemStatLearn}, the dataset \ri{prostate}
has an outcome of interest as the log of the prostate-specific antigen\ri{lpsa} and some potential predictors
including the log cancer volume \ri{lcavol}, the log prostate weight \ri{lweight}, age \ri{age}, etc. 

 


\item $Y$ binary or indicator of two classes, and $X$ multivariate of
mixed types. For example, in the \ri{R} package \ri{wooldridge}, the dataset \ri{mroz} contains
an outcome of interest being the binary indicator for whether a woman
was in the labor force in 1975, and some useful covariates are

\begin{center}
\begin{tabular}{|c|c|}
\hline 
covariate name & covariate meaning\tabularnewline
\hline 
\hline 
kidslt6 & number of kids younger than six years old\tabularnewline
\hline 
kidsge6 & number of kids between six and eighteen years old\tabularnewline
\hline 
age & age\tabularnewline
\hline 
educ & years of education\tabularnewline
\hline 
husage & husband's age\tabularnewline
\hline 
huseduc & husband's years of education\tabularnewline
\hline 
\end{tabular} 
\end{center}


\item $Y$ categorical without ordering. For example, the choice of housing type, single-family house, townhouse, or condominium, is a categorical variable. 

\item $Y$ categorical and ordered. For example, the final course evaluation at UC Berkeley can take value in $\{ 1,2,3,4,5,6,7 \}$. These numbers have clear ordering but they are not the usual real numbers. 

\item $Y$ counts. For example, the number of times one went to the gym last week is a non-negative integer representing counts. 

\item $Y$ time-to-event outcome. For example, in medical trials, a major outcome of interest is the survival time; in labor economics, a major outcome of interest is the time to find the next job. The former is called {\it survival analysis} in biostatistics and the latter is called {\it duration analysis} in econometrics.


\item $Y$ multivariate and correlated. In medical trials, the data are often longitudinal, meaning that the patient's outcomes are measured repeatedly over time. So each patient has a multivariate outcome. In field experiments of public health and development economics, the randomized interventions are often at the village level but the outcome data are collected at the household level. So within villages, the outcomes are correlated. 


\end{enumerate}


\section{Why linear models?}

Why do we study linear models if most real problems may have nonlinear structures? There are important reasons. 


\begin{enumerate}[(R1)]
\item Linear models are simple but non-trivial starting points for learning.
\item Linear models can provide insights because we can derive explicit
formulas based on elegant algebra and geometry.
\item Linear models can handle nonlinearity by incorporating nonlinear terms,
for example, $X$ can contain the polynomials or nonlinear transformations
of the original covariates. In statistics, ``linear'' often means linear in parameters, not necessarily in covariates. 
\item Linear models can be good approximations to nonlinear data-generating
processes.
\item Linear models are simpler than nonlinear models, but they do not necessarily
perform worse than more complicated nonlinear models. We have finite data so we cannot fit arbitrarily complicated models. 


\end{enumerate}

If you are interested in nonlinear models, you can take another machine learning course.

 
