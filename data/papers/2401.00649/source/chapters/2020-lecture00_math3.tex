 
\chapter{Limiting Theorems and Basic Asymptotics}
\label{chapter::limiting-theorems} 


This chapter reviews the basics of limiting theorems and asymptotic analyses that are useful for this book. See \citet{newey1994large} and \citet{van2000asymptotic} for in-depth discussions. 

\section{Convergence in probability}



\begin{definition}
Random vectors $Z_{n}\in\mathbb{R}^{K}$ converge to $Z$ in probability,
denoted by $Z_{n}\rightarrow Z$ in probability, if 
\[
\pr\left\{ \|Z_{n}-Z\|>c\right\} \rightarrow0,\quad n\rightarrow\infty 
\]
for all $c>0$. 
\end{definition}


This definition incorporates the classic definition of convergence
of non-random vectors:
\begin{proposition}
If non-random vectors $Z_{n}\rightarrow Z$, the convergence also
holds in probability.
\end{proposition}
%
Convergence in probability for random vectors is equivalent to element-wise
convergence because of the following result:
\begin{proposition}
If $Z_{n}\rightarrow Z$ and $W_{n}\rightarrow W$ in probability,
then $(Z_{n},W_{n})\rightarrow(Z,W)$ in probability. 
\end{proposition}


The above proposition does not require any conditions on the joint distribution of $(Z_n, W_n)$. 


For an IID sequence of random vectors, we have the following weak law of
large numbers:
\begin{proposition}[Khintchine's weak law of large numbers]
If $Z_{1},\ldots,Z_{n}$ are \textup{IID} with mean $\mu\in\mathbb{R}^{K},$
then $  n^{-1}\sumn Z_{i}\rightarrow\mu$ in probability. 
\end{proposition}

A more elementary tool is Markov's inequality:
\begin{equation}
\pr\left\{ \|Z_{n}-Z\|>c\right\} \leq E\left\{ \|Z_{n}-Z\|\right\} /c\label{eq:1stmoment}
\end{equation}
or 
\begin{equation}
\pr\left\{ \|Z_{n}-Z\|>c\right\} \leq E\left\{ \|Z_{n}-Z\|^{2}\right\} /c^{2}.\label{eq:2ndmoment}
\end{equation}

Inequality (\ref{eq:1stmoment}) is useful if $E\left\{ \|Z_{n}-Z\|\right\} $
converges to zero, and inequality (\ref{eq:2ndmoment}) is useful
if $E\left\{ \|Z_{n}-Z\|^{2}\right\} $ converges to zero. The latter
gives a standard tool for establishing convergence in probability
by showing that the covariance matrix converges to zero.

\begin{proposition}\label{prop::markov-lln}
If random vectors $Z_{n}\in\mathbb{R}^{K}$ have mean zero and covariance
$\cov(Z_{n})=a_{n}C_{n}$ where $a_{n}\rightarrow0$ and $C_{n}\rightarrow C<\infty$,
then $Z_{n}\rightarrow0$ in probability. 
\end{proposition}


\begin{myproof}{Proposition}{\ref{prop::markov-lln}}
Using (\ref{eq:2ndmoment}), we have
\begin{align*}
\pr\left\{ \|Z_{n}\|>c\right\}  & \leq c^{-2}E\left\{ \|Z_{n}\|^{2}\right\} \\
 & =c^{-2}E\left(Z_{n}^{\T}Z_{n}\right)\\
 & =c^{-2}\text{trace}\left\{ E\left(Z_{n}Z_{n}^{\T}\right)\right\} \\
 & =c^{-2}\text{trace}\left\{ \cov(Z_{n})\right\} \\
 & =c^{-2}a_{n}\text{trace}(C_{n})\rightarrow0,
\end{align*}
which implies that $Z_{n}\rightarrow0$ in probability. 
\end{myproof}

For example, we usually use Proposition \ref{prop::markov-lln} to show the weak law of large numbers for the sample mean of independent random variables $\bar{Z}_n = n^{-1} \sumn Z_i$. If we can show that
\begin{eqnarray}
\cov( \bar{Z}_n   ) = n^{-2} \sumn \cov(Z_i) \rightarrow 0,
\label{eq::covariance-of-average}
\end{eqnarray}
then we can conclude that $\bar{Z}_n - n^{-1} \sumn E(Z_i) \rightarrow 0$ in probability. The condition in \eqref{eq::covariance-of-average} holds if $n^{-1} \sumn \cov(Z_i) $ converges to a constant matrix. 



Note that convergence in probability does not imply convergence of moments in general. The following theorem gives a sufficient condition. 


\begin{proposition}[dominant convergence theorem]\label{prop::DCT}
If $Z_n \rightarrow Z$ in probability and $\| Z_n \| \leq \| Y \|$ with $E\| Y\| < \infty$, then $E(Z_n) \rightarrow E(Z)$. 
\end{proposition}


\section{Convergence in distribution}


\begin{definition}
Random vectors $Z_{n}\in\mathbb{R}^{K}$ converge to $Z$ in distribution,
if for all every continuous point $z$ of the function $t\rightarrow\pr(Z\leq t)$, we have 
\[
\pr(Z_{n}\leq z)\rightarrow\pr(Z\leq z),\quad n\rightarrow\infty.
\]
\end{definition}

When the limit is a constant, we have an equivalence of convergences
in probability and distribution:

\begin{proposition}
If $c$ is a non-random vector, then $Z_{n}\rightarrow c$ in probability
is equivalent to $Z_{n}\rightarrow c$ in distribution.
\end{proposition}


For IID sequences of random vectors, we have the Lindeberg--L\'{e}vy central limit theorem
(CLT):
\begin{proposition}[Lindeberg--L\'{e}vy CLT]
If random vectors $Z_{1},\ldots,Z_{n}$ are \textup{IID} with mean $\mu$
and covariance $\Sigma$, then $n^{1/2}(\bar{Z}_{n}-\mu)=n^{-1/2}\sumn(Z_{i}-\mu)\rightarrow\N(0,\Sigma)$
in distribution. 
\end{proposition}


The more general Lindeberg--Feller CLT holds for independent sequences
of random vectors:
\begin{proposition}\label{prop::lf-clt}
For each $n$, let $Z_{n1},\ldots,Z_{n,k_{n}}$ be independent random
vectors with finite variances such that
\begin{enumerate}
[(LF1)]
\item\label{enum::LF1} $\sum_{i=1}^{k_{n}}E\left[\|Z_{ni}\|^{2}1\left\{ \|Z_{ni}\|>c\right\} \right]\rightarrow0$
for every $c>0$;
\item\label{enum::LF2} $\sum_{i=1}^{k_{n}}\cov(Z_{ni})\rightarrow\Sigma.$ 
\end{enumerate}
Then $\sum_{i=1}^{k_{n}}\left\{ Z_{ni}-E(Z_{ni})\right\} \rightarrow\N(0,\Sigma)$
in distribution.
\end{proposition}


Condition (LF\ref{enum::LF2}) often holds by proper standardization, and the key is to verify Condition (LF\ref{enum::LF1}). 
Condition (LF\ref{enum::LF1}) is general but it looks cumbersome. In many cases, we impose a stronger moment condition that is easier to verify:
\begin{enumerate}
[(LF1')]
\item\label{enum::LF'1} $\sum_{i=1}^{k_{n}}E \|Z_{ni}\|^{2+\delta} \rightarrow 0$ for some $\delta >0$. 
\end{enumerate}

We can show that (LF\ref{enum::LF1}') implies that (LF\ref{enum::LF1}):
\begin{eqnarray*}
\sum_{i=1}^{k_{n}}E\left[\|Z_{ni}\|^{2}1\left\{ \|Z_{ni}\|>c\right\} \right]
&=& \sum_{i=1}^{k_{n}}E\left[\|Z_{ni}\|^{2+\delta}  \|Z_{ni}\|^{-\delta} 1\left\{ \|Z_{ni}\|^{\delta}>c^{\delta} \right\} \right] \\
&\leq & \sum_{i=1}^{k_{n}}E \|Z_{ni}\|^{2+\delta} c^{- \delta} \rightarrow 0.
\end{eqnarray*}

Condition (LF\ref{enum::LF'1}') is called the Lyapunov condition.




A beautiful application of the Lindeberg--Feller CLT is the proof of \citet{huber1973robust}'s theorem on OLS mentioned in Chapter \ref{chapter::leave-one-out}. I first review the theorem and then give a proof. 


\begin{theorem}
\label{thm::huber-ols}
Assume $Y= X \beta + \varepsilon $ where the covariates are fixed and  error terms $\varepsilon = (\varepsilon_1, \ldots, \varepsilon_n)^{\T}$ are IID non-Normal with mean zero and finite variance $\sigma^2$. Recall the OLS estimator $\hat{\beta} = (X^{\T} X)^{-1} X^{\T} Y$. Any linear combination of $\hat{\beta}$ is asymptotically Normal  if and only if
$$
\max_{1\leq i \leq n} h_{ii} \rightarrow 0 ,
$$
where $h_{ii}$ is the $i$th diagonal element of the hat matrix $H  =  X (X^{\T} X)^{-1} X^{\T}$. 
\end{theorem}


In the main text, $h_{ii}$ is called the {\it leverage score} of unit $i$. The maximum leverage score 
$$
\kappa = \max_{1\leq i \leq n} h_{ii}
$$ 
plays an important role in analyzing the properties of OLS.


Theorem \ref{thm::huber-ols} assumes that the errors are not Normal because the asymptotic Normality under Normal errors is a trivial result (See Chapter \ref{chapter::normal-linear-model}). 
It is slightly different from the asymptotic analysis in Chapter \ref{chapter::EHW}. Theorem \ref{thm::huber-ols} only concerns any linear combination of the OLS estimator alone, but the results in  Chapter \ref{chapter::EHW} allow for the joint inference of several linear combinations of the OLS estimator. An implicit assumption of  Chapter \ref{chapter::EHW} is that the dimension $p$ of the covariate matrix is fixed, but Theorem \ref{thm::huber-ols} allows for a diverging $p$. The leverage score condition implicitly restricts the dimension and moments of the covariates. Another interesting feature of Theorem \ref{thm::huber-ols} is that the statement is coordinate-free, that is, it holds up to a non-singular transformation of the covariates (See also Problems \ref{hw3::invariance-ols} and \ref{hw03::invariance-of-H}). The proof of sufficiency follows \citet{huber1973robust} closely, and the proof of necessity was suggested by Professor Peter Bickel. 


\begin{myproof}{Theorem}{\ref{thm::huber-ols}}
I first simplify the notation without essential loss of generality. 
By the invariance of the OLS estimator and the hat matrix in Problems \ref{hw3::invariance-ols} and \ref{hw03::invariance-of-H}, we can also assume $X^{\T} X = I_p$. So
$$
\hat{\beta} - \beta = (X^{\T} X)^{-1} X^{\T} \varepsilon = X^{\T} \varepsilon
$$
and the hat matrix 
$$
H = X (X^{\T} X)^{-1} X^{\T} = X  X^{\T}
$$
has diagonal elements $h_{ii} = x_i^{\T} x_i = \|x_i \|^2$ and non-diagonal elements $h_{ij} =x_i^{\T} x_j  $. We can also assume $\sigma^2 = 1$. 

Consider a fixed vector $a \in \mathbb{R}^p$ and assume $\|a\|^2 = 1$. 
We have
$$
a^{\T}  \hat{\beta} - a^{\T}  \beta = a^{\T}  X^{\T} \varepsilon \equiv s^{\T} \varepsilon, 
$$
where 
$$
s = X a  \Longleftrightarrow   s_i = x_i^{\T} a \quad (i=1,\ldots, n)
$$ 
satisfies 
$$
\| s \|^2 = a^{\T}  X^{\T}  X a = \|a\|^2 = 1
$$ 
and 
$$
s_i^2 = (x_i^{\T} a)^2 \leq \| x_i\|^2 \| a \|^2 = \| x_i\|^2 = h_{ii}
$$
by the Cauchy--Schwarz inequality. 


I first prove the sufficiency. 
The key term $a^{\T}  \hat{\beta} - a^{\T}  \beta$ is a linear combination of the IID errors, and it has mean $0$ and variance 
$
\var( s^{\T} \varepsilon ) = \| s \|^2 =1.
$
We only need to verify Condition (LF\ref{enum::LF1}) to establish the CLT. It holds because for any fixed $c>0$, we have 
\begin{eqnarray} 
 \sumn E\left[   s_i^2 \varepsilon_i^2 1\left\{    |s_i  \varepsilon_i |  > c \right\}  \right] 
&\leq &  \sumn  s_i^2 \max_{1\leq i \leq  n } E\left[   \varepsilon_i^2 1\left\{    |s_i  \varepsilon_i |  > c \right\}  \right]   \label{eq::huber-max} \\
&=&   \max_{1\leq i \leq  n } E\left[   \varepsilon_i^2 1\left\{    |s_i  \varepsilon_i |  > c \right\}  \right]  \label{eq::huber-sum1} \\
&\leq &   E\left[   \varepsilon_i^2 1\left\{  \kappa^{1/2}    |  \varepsilon_i |  > c \right\}  \right]  \label{eq::huber-upper} \\
&\rightarrow & 0, \label{eq::huber-convergence}
\end{eqnarray}  
where \eqref{eq::huber-max} follows from the property of $\max$, \eqref{eq::huber-sum1} follows from the fact $\| s \|^2 =1$, \eqref{eq::huber-upper} follows from the fact that $ |s_i| \leq | h_{ii} |  \leq \kappa^{1/2}$, and \eqref{eq::huber-convergence} follows from $\kappa \rightarrow 0$ and the dominant convergence theorem in Proposition \ref{prop::DCT}.


I then prove the necessity. 
Pick one $i^*$ from $ \arg\max_{1\leq i\leq n}  h_{ii} $. 
Consider a special linear combination of the OLS estimator: $\hat{y}_{i^*} = x_{i^*}^{\T} \hat{\beta}$, which is the fitted value of the $i^*$th observation and has the form
\begin{eqnarray*}
\hat{y}_{i^*}   
&=&  x_{i^*}^{\T} \hat \beta  \\
&=&  x_{i^*}^{\T} X^{\T} \varepsilon \\
&=& \sum_{j=1}^n x_{i^*}^{\T} x_j \varepsilon_j \\
&=&  h_{i^*i^*} \varepsilon_{i^*} + \sum_{j\neq i^*} h_{i^*j} \varepsilon_j . 
\end{eqnarray*}
If $\hat{y}_{i^*} $ is asymptotically Normal, then both $ h_{i^*i^*} \varepsilon_{i^*} $ and $\sum_{j\neq i^*} h_{i^*j} \varepsilon_j$ must have Normal limiting distributions by Theorem \ref{thm::levy-cramer}. Therefore, $ h_{i^*i^*} $ must converge to zero because $\varepsilon_{i^*} $ has a non-Normal distribution. So $\max_{1\leq i \leq n} h_{ii}$ must converge to zero. 
\end{myproof}



\section{Tools for proving convergence in probability and distribution}

The first tool is the continuous mapping theorem:
\begin{proposition}
Let $f:\mathbb{R}^{K}\rightarrow\mathbb{R}^{L}$ be continuous except
on a set $O$ with $\pr(Z\in O)=0$. Then $Z_{n}\rightarrow Z$ implies
$f(Z_{n})\rightarrow f(Z)$ in probability (and in distribution).
\end{proposition}


The second tool is Slutsky's Theorem:
\begin{proposition}
Let $Z_{n}$ and $W_{n}$ be random vectors. If $Z_{n}\rightarrow Z$
in distribution, and $W_{n}\rightarrow c$ in probability (or in distribution)
for a constant $c$, then
\begin{enumerate}
\item $Z_{n}+W_{n}\rightarrow Z+c$ in distribution;
\item $W_{n}Z_{n}\rightarrow cZ$ in distribution;
\item $W_{n}^{-1} Z_{n}\rightarrow c^{-1}Z$ in distribution if $c\neq0$.
\end{enumerate}
\end{proposition}


The third tool is the delta method. I will present a special case below for asymptotically Normal random vectors. Heuristically, it states that if $T_n$ is asymptotically Normal, then any function of $T_n$ is also asymptotically Normal. This is true because any function is a locally linear function by the first-order Taylor expansion. 

\begin{proposition}\label{prop::delta-method}
Let $f(z)$ be a function from $\mathbb{R}^p$ to $\mathbb{R}^q$, and $\partial f(z) / \partial z \in \mathbb{R}^{p\times q}$ be the partial derivative matrix. 
If $\sqrt{n} (Z_n - \theta) \rightarrow \N(\mu, \Sigma)$ in distribution, then 
$$
\sqrt{n} \{ f(Z_n) - f(\theta) \} \rightarrow \N\left( 
\frac{ \partial f(\theta) }{  \partial z^{\T} } \mu, \frac{ \partial f(\theta) }{  \partial z^{\T} } \Sigma \frac{ \partial f(\theta) }{  \partial z}
\right)
$$
in distribution. 
\end{proposition}

\begin{myproof}{Proposition}{\ref{prop::delta-method}}
I will give an informal proof. Using Taylor expansion, we have
$$
\sqrt{n} \{ f(Z_n)   - f(\theta) \}\cong  \frac{ \partial f(\theta) }{  \partial z^{\T} } \sqrt{n} (Z_n - \theta),
$$
which is a linear transformation of $\sqrt{n} (Z_n - \theta)$. Because $\sqrt{n} (Z_n - \theta) \rightarrow \N(\mu, \Sigma)$ in distribution, we have
\begin{eqnarray*}
\sqrt{n} \{ f(Z_n)   - f(\theta) \} 
&\rightarrow &  \frac{ \partial f(\theta) }{  \partial z^{\T} }\N(\mu, \Sigma) \\
&=& \N\left( 
\frac{ \partial f(\theta) }{  \partial z^{\T} } \mu, \frac{ \partial f(\theta) }{  \partial z^{\T} } \Sigma \frac{ \partial f(\theta) }{  \partial z}
\right)
\end{eqnarray*}
in distribution. 
\end{myproof}



Proposition \ref{prop::delta-method} above is more useful when $\partial f(\theta) /   \partial z \neq 0$. Otherwise, we need to invoke higher-order Taylor expansion to obtain a more accurate asymptotic approximation. 
 
 

