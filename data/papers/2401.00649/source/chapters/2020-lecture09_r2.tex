 
\chapter{Multiple Correlation Coefficient}
 
This chapter will introduce the $R^2$,  the {\it multiple correlation coefficient}, also called the {\it coefficient of determination} \citep{sewall1921correlation}. It can achieve two goals: 
first, it extends the sample Pearson correlation
coefficient between two scalars to a measure of correlation between a
scalar outcome and a vector covariate;
second, it measures how well multiple covariates
can linearly represent an outcome.
 

\section{Equivalent definitions of $R^{2}$}

I start with the standard definition of $R^{2}$ between $Y$ and $X$.  
Slightly different from other chapters, $X$ excludes the column of $1$'s, so now $X$ is an $n\times (p-1)$ matrix. 
Based on the OLS of $Y$ on $(1_n, X)$, we define 
\[
R^{2}=\frac{\sumn(\hat{y}_{i}-\bar{y})^{2}}{\sumn(y_{i}-\bar{y})^{2}}.
\]
We have discussed before that including $1_{n}$ in the OLS ensures
\begin{eqnarray*}
&&n^{-1}\sumn\hat{\varepsilon}_{i}=0 , \\
&\Longrightarrow & n^{-1}\sumn y_{i}=n^{-1}\sumn\hat{y}_{i} , \\
&\Longrightarrow & \bar{y}=\bar{\hat{y}},
\end{eqnarray*}
i.e., the average of the fitted values equals the average of the original observed
outcomes. So I use $\bar{y}$ for both the means of outcomes
and the fitted values. With scaling factor $(n-1)^{-1}$, the denominator
of $R^{2}$ is the sample variance of the outcomes, and the numerator
of $R^{2}$ is the sample variance of the fitted values. We can verify
the following decomposition:
\begin{lemma} 
\label{lemma:The-total-sum}We have the following variance decomposition: 
\[
\sumn(y_{i}-\bar{y})^{2}=\sumn(\hat{y}_{i}-\bar{y})^{2}+\sumn(y_{i}-\hat{y}_{i})^{2}.
\]
\end{lemma}

I leave the proof of Lemma \ref{lemma:The-total-sum} as Problem \ref{hw09::var-decompose}. 
Lemma \ref{lemma:The-total-sum} states that the total sum of squares $\sumn(y_{i}-\bar{y})^{2}$
equals  the regression sum of squares $\sumn(\hat{y}_{i}-\bar{y})^{2}$
plus the residual sum of squares $\sumn(y_{i}-\hat{y}_{i})^{2}$. 
From Lemma
\ref{lemma:The-total-sum}, $R^{2}$ must be lie within the interval
$[0,1]$ which measures the proportion of the regression sum of squares
in the total sum of squares. An immediate consequence of Lemma \ref{lemma:The-total-sum} is that
$$
\textsc{rss} = (1-R^2) \sumn(y_{i}-\bar{y})^{2}.
$$

We can also verify that $R^{2}$ is the squared sample Pearson correlation
coefficient between $Y$ and $\hat{Y}$. 
\begin{theorem}\label{theorem::r2=rho2}
We have $R^{2}= \hat{\rho}_{y\hat{y}}^2$ where
\begin{equation}
\hat{\rho}_{y\hat{y}}
= \frac{\sumn(y_{i}-\bar{y})(\hat{y}_{i}-\bar{y})}{\sqrt{\sumn(y_{i}-\bar{y})^{2}}\sqrt{\sumn(\hat{y}_{i}-\bar{y})^{2}}} .
\label{eq:r2PCC}
\end{equation}
\end{theorem}

I leave the proof of Theorem \ref{theorem::r2=rho2} as Problem \ref{hw9::r2-pcc}.  It states that the multiple correlation coefficient equals the squared Pearson correlation coefficient between $y_i$ and $\hat{y}_i$. 
Although the sample Pearson correlation coefficient can be positive or negative, $R^2$ is always non-negative.  
Geometrically, $R^2$ equals the squared cosine of the angle between the centered vectors $Y-\bar{y} 1_n$ and $\hat{Y} - \bar{y} 1_n$; see Chapter \ref{sec::basics-vectors-matrices}. 


 

In terms of long and short regressions, we can partition the design matrix into   $ 1_{n}$ and $X$, then the OLS fit of the long regression is
\begin{equation}
Y=1_{n}\hat{\beta}_0+X \hat{\beta} +\hat{\varepsilon},\label{eq:longregR2}
\end{equation}
 and the OLS fit of the short regression is
\begin{equation}
Y=1_{n}\tilde{\beta}_0 +\tilde{\varepsilon},\label{eq:shortregR2}
\end{equation}
with $\tilde{\beta}_0 =\bar{y}$.
The total sum of squares is the residual sum of squares from the short
regression so by Lemma \ref{lemma:The-total-sum}, $R^{2}$ also equals 
\begin{equation}
R^{2}  =  \frac{\textsc{rss}_{\text{short}}-\textsc{rss}_{\text{long}}}{\textsc{rss}_{\text{short}}} . \label{eq::residuals-R2} 
\end{equation}





\section{$R^{2}$ and the $F$ statistic}

Under the Normal linear model 
\begin{equation}
Y=1_{n}\beta_0+X \beta +\varepsilon,\qquad\varepsilon\sim\N(0,\sigma^{2}I_{n}),\label{eq:gaussianolsR2}
\end{equation}
we can use the $F$ statistic to test whether $\beta =0$. This
$F$ statistic is a monotone function of $R^{2}$. Most standard software
packages report both $F$ and $R^2$. I first give a numeric result without assuming that model \eqref{eq:gaussianolsR2} is correct. 

\begin{theorem}
\label{theorem:F-R2}We have 
\[
F=\frac{n-p}{p-1}\times\frac{R^{2}}{1-R^{2}}.
\]
\end{theorem}

\begin{myproof}{Theorem}{\ref{theorem:F-R2}}
Based on the long regression (\ref{eq:longregR2}) and the short regression
(\ref{eq:shortregR2}), we have \eqref{eq::residuals-R2} and 
\[
F=\frac{(\textsc{rss}_{\text{short}}-\textsc{rss}_{\text{long}})/(p-1)}{\textsc{rss}_{\text{long}}/(n-p)} . 
\]
So the conclusion follows. 
\end{myproof}

I then give the exact distribution of $R^2$ under the Normal linear model. 

\begin{corollary}\label{coro::dist-r2}
Under the Normal linear model (\ref{eq:gaussianolsR2}), if $\beta =0$,
then 
\[
R^{2}\sim\textup{Beta}\left(\frac{p-1}{2},\frac{n-p}{2}\right).
\]
\end{corollary}
%
\begin{myproof}{Corollary}{\ref{coro::dist-r2}}
By definition, the $F$ statistic can be represented as 
\[
F=\frac{\chi_{p-1}^{2}/(p-1)}{\chi_{n-p}^{2}/(n-p)}
\]
where $\chi_{p-1}^{2}$ and $\chi_{n-p}^{2}$ denote independent $\chi_{p-1}^{2}$ and $\chi_{n-p}^{2}$ random variables, respectively, with a little abuse of notation. Using Theorem \ref{theorem:F-R2},
we have
\[
\frac{R^{2}}{1-R^{2}}=F\times\frac{p-1}{n-p}=\frac{\chi_{p-1}^{2}}{\chi_{n-p}^{2}}
\]
which implies
\[ 
R^{2}=\frac{\chi_{p-1}^{2}}{\chi_{p-1}^{2}+\chi_{n-p}^{2}}\text{.}
\]
Because $\chi_{p-1}^{2}\sim\textup{Gamma}\left(\frac{p-1}{2},\frac{1}{2}\right)$
and $\chi_{n-p}^{2}\sim\textup{Gamma}\left(\frac{n-p}{2},\frac{1}{2}\right)$ by Proposition \ref{prop::chi2-gamma},
we have
$$
R^{2} 
=\frac{\textup{Gamma}\left(\frac{p-1}{2},\frac{1}{2}\right)}{\textup{Gamma}\left(\frac{p-1}{2},\frac{1}{2}\right)+\textup{Gamma}\left(\frac{n-p}{2},\frac{1}{2}\right)} 
$$
where $\textup{Gamma}\left(\frac{p-1}{2},\frac{1}{2}\right)$ and $\textup{Gamma}\left(\frac{n-p}{2},\frac{1}{2}\right)$ denote independent Gamma random variables, with a little abuse of notation. 
%Therefore,
%$$
%R^{2}  \sim  \textup{Beta}\left(\frac{p-1}{2},\frac{n-p}{2}\right),
%$$
%which 
The $R^2$ follows the Beta distribution by the Beta--Gamma duality in Theorem \ref{thm:beta-gamma-duality}. 
\end{myproof}
%


\section{Numerical examples}

Below I first use the LaLonde data to verify Theorems \ref{theorem::r2=rho2} and \ref{theorem:F-R2} numerically. 
\begin{lstlisting}
> library("Matching")
> data(lalonde)
> ols.fit = lm(re78 ~ ., y = TRUE, data = lalonde)
> ols.summary = summary(ols.fit)
> r2 = ols.summary$r.squared
> all.equal(r2, (cor(ols.fit$y, ols.fit$fitted.values))^2,
+           check.names = FALSE)
[1] TRUE
> 
> fstat = ols.summary$fstatistic
> all.equal(fstat[1], fstat[3]/fstat[2]*r2/(1-r2),
+           check.names = FALSE)
[1] TRUE
\end{lstlisting}    


I then use the data from \citet{king2015robust} to verify Theorems \ref{theorem::r2=rho2} and \ref{theorem:F-R2} numerically. 
\begin{lstlisting}
> library(foreign)
> dat = read.dta("isq.dta")
> dat = na.omit(dat[,c("multish", "lnpop", "lnpopsq", 
+                      "lngdp", "lncolony", "lndist", 
+                      "freedom", "militexp", "arms", 
+                      "year83", "year86", "year89", "year92")])
> 
> ols.fit = lm(log(multish + 1) ~ lnpop + lnpopsq + lngdp +  lncolony 
+              + lndist + freedom + militexp + arms 
+              + year83 + year86 + year89 + year92, 
+              y = TRUE, data=dat)
> ols.summary = summary(ols.fit)
> r2 = ols.summary$r.squared
> all.equal(r2, (cor(ols.fit$y, ols.fit$fitted.values))^2,
+           check.names = FALSE)
[1] TRUE
> 
> fstat = ols.summary$fstatistic
> all.equal(fstat[1], fstat[3]/fstat[2]*r2/(1-r2),
+           check.names = FALSE)
[1] TRUE
\end{lstlisting}    
    
The \ri{R} code is in \ri{code10.3.R}. 
    

\section{Homework problems}

\paragraph{Variance decomposition}\label{hw09::var-decompose}
Prove Lemma \ref{lemma:The-total-sum}. 


\paragraph{$R^2$ and the sample Pearson correlation coefficient}\label{hw9::r2-pcc}
Prove Theorem \ref{theorem::r2=rho2}. 


\paragraph{Exact distribution of $\hat{\rho}$}\label{hw9::exact-d-rho}

Assume the Normal linear model $y_i = \alpha + \beta x_i + \varepsilon_i$ with
 a univariate $x_i$ with $\beta = 0$ and $\varepsilon_i$'s IID $\N(0,  \sigma^2 )$. 
 Find the exact distribution of $\hat{\rho}_{xy}$. 

 

\paragraph{Partial $R^2$}\label{hw09::partial-R2}


The form \eqref{eq::residuals-R2} of $R^2$ is well defined in more general long and short regressions:
$$
\hat{Y} = 1_n \hat{\beta}_0 + X \hat{\beta} + W \hat{\gamma} + \hat{\varepsilon}_Y
$$
and
$$
\hat{Y} = 1_n \tilde{\beta}_0   + W \tilde{\gamma} + \tilde{\varepsilon}_Y
$$
where $X$ is an $n\times k$ matrix and $W$ is an $n\times l$ matrix. Define the partial $R^2$ between $Y$ and $X$ given $W$ as
$$
R^2_{Y.X|W} = \frac{ \textsc{rss}(Y\sim 1_n+W) -   \textsc{rss}(Y\sim 1_n+X+W) }{ \textsc{rss}(Y\sim 1_n+W) }
$$
which spells out the formulas of the long and short regressions. This is an intuitive measure of the multiple correlation between $Y$ and $X$ after controlling for $W$. The following properties make this intuition more explicit. 


\begin{enumerate}
\item
The partial $R^2$ equals 
$$
R^2_{Y.X|W} = \frac{  R^2_{Y.XW}  - R^2_{Y.W}  }{ 1- R^2_{Y.W}}
$$
where $R^2_{Y.XW} $ is the multiple correlation between $Y$ and $(X,W)$, and $R^2_{Y.W}$ is the multiple correlation between $Y$ and $W$.

\item
The partial $R^2$ equals the $R^2$ between $ \tilde{\varepsilon}_Y $ and $\tilde{\epsilon}_X$:
$$
R^2_{Y.X|W} =  R^2_{  \tilde{\varepsilon}_Y . \tilde{\varepsilon}_X }
$$
where $\tilde{\varepsilon}_X$ is the residual matrix from the OLS fit of $X$ on $(1_n, W)$. 
\end{enumerate}

Prove the above two results. 

Do the following two results hold? 
\begin{eqnarray*}
R^2_{Y.XW} &=& R^2_{Y.W} +  R^2_{Y.X|W} ,\\
R^2_{Y.XW} &=& R^2_{Y.W|X} +  R^2_{Y.X|W}.
\end{eqnarray*}
For each result, give a proof if it is correct, or give a counterexample if it is incorrect in general. 




\paragraph{Omitted-variable bias in terms of the partial $R^2$}\label{hw09::cochran-partial-R2}

Revisit Section \ref{sec::ovb} on the following three OLS fits. The first one involves only the observed variables:
$$
y_i  =  \tilde{\beta}_0 + \tilde{\beta}_1 z_i + \tilde{\beta}^{\T}_2 x_i + \tilde{\varepsilon}_i,
$$
and the second and third ones involve the unobserved $u$:
\begin{eqnarray*}
y_i &=& \hat{\beta}_0 + \hat{\beta}_1 z_i + \hat{\beta}^{\T}_2 x_i  + \hat{\beta}_3 u_i + \hat{\varepsilon}_i ,\\
u_i &=&  \hat{\delta}_0 +  \hat{\delta}_1 z_i  + \hat{\delta}_2^{\T} x_i + \hat{v}_i.
\end{eqnarray*}
The omitted-variable bias formula states that $\tilde{\beta} - \hat{\beta}_1 =  \hat{\beta}_3 \hat{\delta}_1$. This formula is simple but may be difficult to interpret since $u$ is unobserved and its scale is unclear to researchers. 

Prove that the formula has an alternative form:
$$
|\tilde{\beta}_1 - \hat{\beta}_1|^2 = R^2_{Y.U\mid ZX} \times \frac{R^2_{Z.U\mid X}}{1 - R^2_{Z.U\mid X}}  
\times   \frac{ \rss(Y\sim 1_n+Z+X)  }{  \rss(Z\sim 1_n+X) }  . 
$$
 

Remark: \citet{cinelli2020making} suggested the partial $R^2$ parametrization for the omitted-variable bias formula. 
The formula has three factors: the first two factors depend on the unknown {\it sensitivity parameters} $R^2_{Y.U\mid ZX} $ and $R^2_{Z.U\mid X}$, and the third factor equals the ratio of the two residual sums of squares based on the observed data. 




