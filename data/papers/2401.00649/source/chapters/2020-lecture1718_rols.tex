 
\chapter{Restricted OLS}
 \label{chapter::rols}
  



Assume that in the standard linear model $Y=X\beta  + \varepsilon$, the parameter has restriction
\begin{equation}
\label{eq::rOLS-restriction}
C \beta = r
\end{equation}
where $C $ is an $l\times p$ matrix and $r$ is a $l$ dimensional vector. Assume that $C $ has linearly independent row vectors; otherwise, some restrictions are redundant. 
We can use the  restricted OLS:  
$$
\hat\beta_\textup{r} = 
\arg \min_{b\in \mathbb{R}^p }  \|  Y - Xb \|^2
$$
under the restriction 
$$
C  b = r . 
$$

I first give some examples of linear models with restricted parameters, then derive the algebraic properties of the restricted OLS estimator $\hat\beta_\textup{r} $, and finally discuss statistical inference with restricted OLS. 




\section{Examples}


\begin{example}[Short regression]\label{eg::short-regression-rols}
Partition $X$ into $X_1$ and $X_2$ with $k$ and $l$ columns, respectively, with $p = k+l$. The short regression of $Y$ on $X_1$ yields OLS coefficient $\hat\beta_1$. So $(\hat\beta_1^{\T}, 0_l^{\T}) = \hat\beta_\textup{r} $ with 
$$
C  = (0_{l\times k}, I_{l\times l}),\quad r=0_l. 
$$
\end{example}




\begin{example}[Testing linear hypothesis]\label{eg::test-linear-hypothesis-rols}
Consider testing the linear hypothesis $C \beta = r$ in the linear model. We have discussed in Chapter \ref{chapter::normal-linear-model} the Wald test based on the OLS estimator and its estimated covariance matrix under the Normal linear model. An alternative strategy is to test the hypothesis based on comparing the residual sum of squares under the OLS and restricted OLS. Therefore, we need to compute both $\hat\beta$ and $\hat\beta_\textup{r} $. 
\end{example}





\begin{example}[One-way analysis of variance]\label{eg::one-way-anova-rols}
If $x_i  $ contains the intercept and $Q_1$ dummy variables of a discrete regressor of $Q_1$ levels, $(f_{i1}, \ldots, f_{iQ_1})^{\T}$, then we must impose a restriction on the parameter  in the linear model
$$
y_i = \alpha +\sum_{j=1}^{Q_1}   \beta_j f_{ij}   + \varepsilon_i.
$$
A canonical choice is $\beta_{Q_1} = 0$, which is equivalent to dropping the last dummy variable due to its redundancy. Another canonical choice is $\sum_{j=1}^{Q_1}   \beta_j  = 0$. This restriction keeps the symmetry of the regressors in the linear model and changes the interpretation of $\beta_j$ as the deviation from the ``effect'' of level $j$ with respect to the average ``effect.'' Both are special cases of restricted OLS.   
\end{example}



\begin{example}[Two-way analysis of variance]\label{eg::two-way-anova-rols}
With two factors of levels $Q_1$ and $Q_2$, respectively, the regressor $x_i$ contains the $Q_1$ dummy variables of the first factor, $(f_{i1}, \ldots, f_{iQ_1})^{\T}$, the $Q_2$ dummies of the second factor, $(g_{i1}, \ldots, g_{iQ_2})^{\T}$, and  the $Q_1Q_2$ dummy variables of the interaction terms, $(f_{i1}g_{i1}, \ldots, f_{iQ_1} g_{iQ_2})^{\T}$. We must impose restrictions on the parameters in the linear model
$$
y_i = \alpha + \sum_{j=1}^{Q_1}   \beta_j f_{ij} + \sum_{k=1}^{Q_2}  \gamma_k g_{ik} 
+ \sum_{j=1}^{Q_1}    \sum_{k=1}^{Q_2}  \delta_{jk} f_{ij} g_{ik}   + \varepsilon_i .
$$
Similar to the discussion in Example \ref{eg::one-way-anova-rols}, two canonical choices of restrictions are 
$$
\beta_{Q_1} = 0, \quad 
\gamma_{Q_2} = 0,\quad 
\delta_{Q_1, k} = \delta_{j, Q_2} = 0 ,\quad (j=1,\ldots, Q_1; k = 1, \ldots, Q_2) .
$$
and
$$
 \sum_{j=1}^{Q_1}   \beta_j = 0,\quad
 \sum_{k=1}^{Q_2}  \gamma_k = 0,\quad
  \sum_{j=1}^{Q_1}    \delta_{jk}  =  \sum_{k=1}^{Q_2}  \delta_{jk} =0,\quad (j=1,\ldots, Q_1; k = 1, \ldots, Q_2) .
$$
\end{example}


\section{Algebraic properties}




I first give an explicit formula of the restricted OLS \citep{theil, rao}. For simplicity, the following theorem assumes that $ X^{\T} X$ is invertible. This condition may not hold in general; see Examples \ref{eg::one-way-anova-rols} and \ref{eg::two-way-anova-rols}. \citet{greene} discussed the results without this assumption; see Problem \ref{para::rols-degenerate-greene} for more details.  

\begin{theorem}
\label{thm::formula-rols}
If $ X^{\T} X$ is invertible, then
$$
\hat{\beta}_\textup{r} = \hat{\beta} - (X^{\T} X)^{-1} C ^{\T} \{  C  (X^{\T} X)^{-1} C ^{\T} \}^{-1} (C  \hat{\beta} - r), 
$$
where $\hat{\beta}  $ is the unrestricted OLS coefficient. 
\end{theorem}


\begin{myproof}{Theorem}{\ref{thm::formula-rols}}
The Lagrangian for the restricted optimization problem is
$$
(Y - Xb)^{\T} (Y - Xb) - 2\lambda^{\T} (C b - r).
$$
So the first order condition is
$$
2X^{\T} (Y - Xb) - 2 C^{\T} \lambda = 0 
$$
which implies
\begin{equation}\label{eq::rls-linearsystem}
X^{\T}  X b = X^{\T} Y  -  C^{\T} \lambda .
\end{equation}
Solve the linear system in \eqref{eq::rls-linearsystem} to obtain
$$
b = (X^{\T}  X)^{-1}  (X^{\T} Y  -  C^{\T} \lambda).
$$
Using the linear restriction $C b = r$, we have
$$
C (X^{\T}  X)^{-1}  (X^{\T} Y  -  C^{\T} \lambda) = r
$$
which implies that
$$
\lambda =  
\{  C(X^{\T} X)^{-1} C^{\T} \}^{-1} (C  \hat{\beta} - r ).
$$
So the restricted OLS coefficient is
\begin{eqnarray*}
\hat{\beta}_\textup{r} 
&=& (X^{\T}  X)^{-1}  (X^{\T} Y  -  C^{\T} \lambda)  \\
&=&   \hat{\beta}  -  (X^{\T}  X)^{-1} C^{\T} \lambda  \\
&=&\hat{\beta} - (X^{\T} X)^{-1} C ^{\T} \{  C  (X^{\T} X)^{-1} C ^{\T} \}^{-1} (C  \hat{\beta} - r).
\end{eqnarray*}
Since the objective function is convex and the restrictions are linear, the solution from the first-order condition is indeed the minimizer. 
\end{myproof}
 
 
 
In the special case with $r=0$, Theorem \ref{thm::formula-rols} has a simpler form.

 \begin{corollary}
 \label{corollary::formula-rols-homo}
 Under the restriction \eqref{eq::rOLS-restriction} with $r=0$, we have 
 $$
\hat{\beta}_\textup{r}  =  M_\textup{r}  \hat{\beta} ,
 $$
 where 
  $$
 M_\textup{r} = I_p - (X^{\T} X)^{-1}  C ^{\T} \{  C  (X^{\T} X)^{-1} C ^{\T} \}^{-1}  C .
 $$ 
 Moreover, $ M_\textup{r} $ satisfies the following properties
 $$
 M_\textup{r}  (X^{\T} X)^{-1}  C ^{\T} = 0,
 \quad
 C  M_\textup{r}  = 0,
 \quad 
 \{ I_p -  C^{\T} (C C^{\T})^{-1} C \}  M_\textup{r} =  M_\textup{r} .
 $$
 \end{corollary}

The $M_\textup{r} $ matrix plays central roles below.





 
 
 The following result is also an immediate corollary of Theorem \ref{thm::formula-rols}. 
 
 \begin{corollary}
 \label{corollary::formula-rols}
 Under the restriction \eqref{eq::rOLS-restriction}, we have 
 $$
 \hat{\beta}_\textup{r} - \beta = M_\textup{r} ( \hat{\beta} - \beta  ) . 
 $$
 \end{corollary}
 
 
 I leave the proofs of Corollaries \ref{corollary::formula-rols-homo} and \ref{corollary::formula-rols} as Problem \ref{para::algebra-rols}. 
 
 
 \section{Statistical inference}\label{sec::stat-inf-rols}

I first focus on the Gauss--Markov model with the restriction \eqref{eq::rOLS-restriction}.  As direct consequences of Corollary \ref{corollary::formula-rols}, we can show that the restricted OLS estimator is unbiased for $\beta$, and obtain its covariance matrix below. 



\begin{corollary}
\label{corollary::moments-rols}
Assume the Gauss--Markov model with the restriction \eqref{eq::rOLS-restriction}. We have 
\begin{eqnarray*}
E (\hat{\beta}_\textup{r}  )  & = & \beta    ,\\ 
\cov(  \hat{\beta}_\textup{r}  ) & = &
\sigma^2 M_\textup{r} (X^{\T}X)^{-1} M_\textup{r}^{\T}.
\end{eqnarray*}
\end{corollary}



Moreover, under the Normal linear model with the restriction \eqref{eq::rOLS-restriction}, we can derive the exact distribution of the restricted OLS estimator and propose an unbiased estimator for $\sigma^2$. 


\begin{theorem}
\label{thm::distribution-normal-rols}
Assume the Normal linear model with the restriction \eqref{eq::rOLS-restriction}. We have 
$$
\hat{\beta}_\textup{r}  \sim \N (  \beta ,  \sigma^2 M_\textup{r} (X^{\T}X)^{-1} M_\textup{r}^{\T}). 
$$
An unbiased estimator for $\sigma^2$ is
$$
\hat{\sigma}^2_\textup{r} = \hat{\varepsilon}_\textup{r} ^{\T} \hat{\varepsilon}_\textup{r}  / (n-p+l),
$$
where $\hat{\varepsilon}_\textup{r}  =  Y - X \hat{\beta}_\textup{r} $. Moreover, 
$$
\hat{\beta}_\textup{r}   \ind \hat{\sigma}^2_\textup{r} .
$$
\end{theorem}


Based on the results in Theorem \ref{thm::distribution-normal-rols}, we can derive the $t$ and $F$ statistics for finite-sample inference of $\beta$ based on the estimator $\hat{\beta}_\textup{r} $ and the estimated covariance matrix
$$
\hat{\sigma}^2_\textup{r}  M_\textup{r} (X^{\T}X)^{-1} M_\textup{r}^{\T} . 
$$

Corollary \ref{corollary::moments-rols} and Theorem \ref{thm::distribution-normal-rols} extend the results for the OLS estimator. I leave their proofs as Problem  \ref{para::moments-distribution-rols}. 

 





 I then discuss statistical inference under the heteroskedastic linear model with the restriction \eqref{eq::rOLS-restriction}. Corollary \ref{corollary::formula-rols} implies that
 $$
 \cov(  \hat{\beta}_\textup{r}   )
=
 M_\textup{r} (X^{\T}X)^{-1}  X^{\T} \text{diag}\{\sigma_1^2, \ldots, \sigma_n^2 \}  X     (X^{\T}X)^{-1} M_\textup{r}^{\T}.
 $$
 Therefore, the EHW-type estimated covariance matrix is
 $$
 \hat{V}_{\textsc{ehw}, \textup{r}}
=
M_\textup{r} (X^{\T}X)^{-1}  
X^{\T} \text{diag}\{   \hat{\varepsilon}_{i, \textup{r}}^2 , \ldots, \hat{\varepsilon}_{n, \textup{r}}^2 \}  X    
 (X^{\T}X)^{-1} M_\textup{r}^{\T}.
 $$ 
 where the $\hat{\varepsilon}_{i, \textup{r}}$'s are the residuals from the restricted OLS. 


 


\section{Final remarks}


This chapter follows \citet{theil} and \cite{rao}. \citet{tarpey2000note} contains additional algebraic results on restricted OLS. 
  




\section{Homework problems}

\paragraph{Algebraic details of restricted OLS}
\label{para::algebra-rols}

Prove Corollaries \ref{corollary::formula-rols-homo} and \ref{corollary::formula-rols}. 




\paragraph{Invariance of restricted OLS}\label{para::invariance-rls}

Consider an $N \times 1$ vector $ Y$  and two $N\times p$ matrices, $X$ and $X'$, that satisfy $X' = X\Gamma$ for some nonsingular $p\times p$ matrix $\Gamma$. 
The restricted OLS fits of 
$$
\begin{array}{ll}
Y =  X\hat{\beta}_\textup{r} + \hat{\epsilon}_\textup{r}  &\quad\text{subject to} \ \ C \hat{\beta}_\textup{r}  = r, 
 \\
Y =  \tilde X \tilde{\beta}_\textup{r}  +\tilde{\epsilon}_\textup{r}  &\quad \text{subject to} \ \ \tilde C  \tilde{\beta}_\textup{r}    = r ,
\end{array}
$$
with $\tilde X = X\Gamma$ and $\tilde C = C\Gamma$
yield $(\hat{\beta}_\textup{r} ,  \hat{\epsilon}_\textup{r},  \hat{V}_{\textsc{ehw}, \textup{r}})$ and $(\tilde{\beta}_\textup{r}  ,  \tilde{\epsilon}_\textup{r}  ,  \tilde{V}_{\textsc{ehw}, \textup{r}}  )$ as the coefficient vectors, residuals, and robust covariances. 


Prove that they must satisfy 
$$
\hat{\beta}_\textup{r} = \Gamma \tilde{\beta}_\textup{r} ,\qquad
\hat{\epsilon}_\textup{r} = \tilde{\epsilon}_\textup{r}  ,\qquad
\hat{V}_{\textsc{ehw}, \textup{r}}= \Gamma \tilde{V}_{\textsc{ehw}, \textup{r}}   \Gamma^{\T}.
$$ 


\paragraph{Moments and distribution of restricted OLS}
\label{para::moments-distribution-rols}

Prove Corollary \ref{corollary::moments-rols} and Theorem \ref{thm::distribution-normal-rols}. 


\paragraph{Gauss--Markov theorem for restricted OLS}
\label{para::gauss-markov-rols}

The Gauss--Markov theorem for $\hat{\beta}_\textup{r} $ holds, as an extension of Theorem \ref{thm:GMtheorem} for  $\hat{\beta} $. 

\begin{theorem}
\label{thm::gauss-markov-rols}
Under the  Gauss--Markov model with the restrictions \eqref{eq::rOLS-restriction}, $\hat{\beta}_\textup{r} $ is the best linear unbiased estimator in the sense that $ \cov(\tilde{\beta}_\textup{r}) -  \cov( \hat{\beta}_\textup{r}  ) \succeq 0$ for any linear estimator $\tilde{\beta}_\textup{r} = \tilde{c} + \tilde{A}_\textup{r} Y$, with $\tilde{c} \in \mathbb{R}^p$ and $\tilde{A}_\textup{r} \in \mathbb{R}^{p\times n}$, that satisfies $E(\tilde{\beta}_\textup{r}) = \beta$ for all $\beta$ under constraint \eqref{eq::rOLS-restriction}. 
\end{theorem}

Prove Theorem \ref{thm::gauss-markov-rols}. 

Remark: As a corollary of Theorem \ref{thm::gauss-markov-rols}, we have
$$
(X^{\T}X)^{-1} \succeq
M_\textup{r} (X^{\T}X)^{-1} M_\textup{r}^{\T} 
$$
because the restricted OLS estimator is BLUE whereas the unrestricted OLS is not, under the Gauss--Markov theorem with the restriction \eqref{eq::rOLS-restriction}. 


\paragraph{Short regression as restricted OLS}
\label{para::short-regression-rols}

The short regression is a  special case of the formula of $\hat{\beta}_\textup{r} $. Show that 
$$
\hat{\beta}_\textup{r}  = 
\begin{pmatrix}
(X_1^{\T} X_1)^{-1} X_1^{\T} Y  \\
0_l 
\end{pmatrix}
$$ 
with
$$
C  = (0_{l\times k}, I_{l\times l}),\quad r=0_l. 
$$
In this special case, $p=k+l$. 

From the short regression, we can obtain the EHW estimated covariance matrix $\hat{V}_{\textsc{ehw}, 1}$. We can also obtain the EHW estimated covariance matrix $\hat{V}_{\textsc{ehw}, \textup{r}} $ from the restricted OLS. Show that
$$
\hat{V}_{\textsc{ehw}, \textup{r}}  = \begin{pmatrix}
\hat{V}_{\textsc{ehw}, 1} & 0 \\
0 & 0 
\end{pmatrix} .
$$




\paragraph{Reducing restricted OLS to OLS}\label{para::rols-to-ols}




Consider the restricted OLS fit 
\begin{equation}
\label{eq::hw-rols}
Y = X  \hat{\beta}_\textup{r} + \hat{\varepsilon}_\textup{r}   \qquad \text{subject to} \ \  C \hat{\beta}_\textup{r} = 0,
\end{equation}
where $X \in \mathbb R^{n\times p}$ and $C \in \mathbb R^{l \times p}$. 

Let $C_\perp \in \mathbb R^{(p-l) \times p}$ be an orthogonal complement of $C$ in the sense that $(C_\perp^{\T}, C^{\T})$ is nonsingular with $ C_\perp C^{\T} = 0$. Define 
$$
X_{\perp} = X  C_\perp^{\T} (C_\perp C_\perp^{\T})^{-1} .
$$

Consider the corresponding unrestricted OLS fit
\begin{equation}
\label{eq::hw-ols-withoutres}
Y =X_{\perp} \hat\beta_\perp  +  \hat{\varepsilon}_\perp , 
\end{equation}

First, prove that the coefficient and residual vectors must satisfy
$$
\hat\beta_\perp = C_\perp  \hat{\beta}_\textup{r}  , \qquad 
\hat{\varepsilon}_\perp = \hat{\varepsilon}_\textup{r}  . 
$$


Second, prove  
$$
\hat{V}_{\textsc{ehw}, \perp} =  C_\perp \hat{V}_{\textsc{ehw}, \textup{r}}  C_\perp^{\T} , 
$$
where $\hat{V}_{\textsc{ehw}, \perp} $ is the EHW robust covariance matrix from \eqref{eq::hw-ols-withoutres} and  $\hat{V}_{\textsc{ehw}, \textup{r}} $ is the EHW robust covariance matrix from \eqref{eq::hw-rols}. 




 


\paragraph{Minimum normal estimator as restricted OLS}
\label{para::minimum-norm-rols}


An application of the formula of $\hat{\beta}_\textup{r} $ is the minimum norm estimator for under-determined linear equations. When $X$ has more columns than rows, $Y=X\beta$ can have infinitely many solutions, but we may only be interested in the solution with the minimum norm. Assume $p\geq n$ and the rows of $X$ are linearly independent. Show that the solution to
$$
\min_b \| b\|^2 \text{ such that } Y = Xb
$$
is
$$
\hat{\beta}_\text{m} = X^{\T} (X X^{\T})^{-1} Y.
$$
 




\paragraph{Restricted OLS with degenerate design matrix}
\label{para::rols-degenerate-greene}


\citet{greene} pointed out that restricted OLS does not require that $X^{\T} X$ be invertible, although the proof of Theorem \ref{thm::formula-rols} does. Modify the proof to show that the restricted OLS and the Lagrange multiplier satisfy 
$$
\begin{pmatrix}
\hat{\beta}_\textup{r}  \\
\lambda 
\end{pmatrix}
= 
W^{-1}
\begin{pmatrix}
X^{\T} Y \\
r
\end{pmatrix}
$$
as long as 
$$
W = 
\begin{pmatrix}
X^{\T} X &  C^{\T} \\
C & 0 
\end{pmatrix}
$$
is invertible. 



Derive the statistical results in parallel with Section \ref{sec::stat-inf-rols}. 


Remark:
If $X$ has full column rank $p$, then $W$ must be invertible. Even if $X$ does not have full column rank, $W$ can still be invertible. See Problem \ref{para::rols-degenerate-condition} below for more details. 



\paragraph{Restricted OLS with degenerate design matrix: more algebra}
\label{para::rols-degenerate-condition}


This problem provides more algebraic details for Problem \ref{para::rols-degenerate-greene}. Prove Lemma \ref{lemma::invertible-W-rls} below.  


\begin{lemma}
\label{lemma::invertible-W-rls}
Consider 
$$
W = 
\begin{pmatrix}
X^{\T} X &  C^{\T} \\
C & 0 
\end{pmatrix}
$$
where $X^{\T} X$ may not be invertible and $C$ has full row rank. 

The matrix $W$ is invertible if and only if
$
\begin{pmatrix}
X\\
C
\end{pmatrix}
$
has full column rank $p$. 
\end{lemma}



Remark: When $X$ has full column rank $p$, then 
$
\begin{pmatrix}
X\\
C
\end{pmatrix}
$
must have full column rank $p$, which ensures that $W$ is invertible by Lemma \ref{lemma::invertible-W-rls}. I made the comment in Problem \ref{para::rols-degenerate-greene}. 

The invertibility of $W$ plays an important role in other applications. See \citet{benzi2005numerical} and \citet{bai2013nonsingularity} for more general results. 
 
 
