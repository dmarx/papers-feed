 
\chapter{The Frisch--Waugh--Lovell Theorem}
 \label{chapter::FWL-theorem}

\section{Long and short regressions}

If we partition $X$ and $\beta$ into
\[
X=\left(\begin{array}{cc}
X_{1} & X_{2}\end{array}\right),\qquad\beta=\left(\begin{array}{c}
\beta_{1}\\
\beta_{2}
\end{array}\right),
\]
where $X_{1}\in\mathbb{R}^{n\times k},X_{2}\in\mathbb{R}^{n\times l},\beta_{1}\in\mathbb{R}^{k}$
and $\beta_2 \in  \mathbb{R}^{l},$ then we can consider the {\it long regression} 
\begin{eqnarray*}
Y &=& X\hat{\beta}+\hat{\varepsilon}  \\
&=&\left(\begin{array}{cc}
X_{1} & X_{2}\end{array}\right)\left(\begin{array}{c}
\hat{\beta}_{1}\\
\hat{\beta}_{2}
\end{array}\right)+\hat{\varepsilon} \\
&=&X_{1}\hat{\beta}_{1}+X_{2}\hat{\beta}_{2}+\hat{\varepsilon},
\end{eqnarray*}
 and the {\it short regression} 
\[
Y=X_{2}\tilde{\beta}_{2}+\tilde{\varepsilon},
\]
where $\hat{\beta}=\left(\begin{array}{c}
\hat{\beta}_{1}\\
\hat{\beta}_{2}
\end{array}\right)$ and $\tilde{\beta}_{2}$ are the OLS coefficients, and $\hat{\varepsilon}$
and $\tilde{\varepsilon}$ are the residual vectors from the long
and short regressions, respectively. These two regressions are of great interest
in practice. For example, we can ask the following questions: 
\begin{enumerate}
[(Q1)]
\item if the true $\beta_{1}$ is zero, then what is the consequence of including $X_1$ in the long regression?
\item if the true $\beta_{1}$ is not zero, then what is the consequence
of omitting $X_1$ in the short regression?
\item what is the difference between $\hat{\beta}_{2}$ and $\tilde{\beta}_{2}$?
Both of them are measures of the ``impact'' of $X_{2}$ on $Y$. Then
why are they different? Does their difference give us any information
about $\beta_{1}$?
\end{enumerate}

Many problems in statistics are related to the long and short regressions. We will discuss some applications in Chapter \ref{chapter::FWL-application} and give a related result in Chapter \ref{chapter::cochran-ovb}. 

 

\section{FWL theorem for the regression coefficients}

The following theorem helps to answer these questions. 
\begin{theorem}
\label{thm:fwl}The OLS estimator for $\beta_2$ in the short regression is $\tilde{\beta}_{2}=(X_{2}^{\T}X_{2})^{-1}X_{2}^{\T}Y$, and the OLS estimator for $\beta_{2}$ in the long regression has
the following equivalent forms
\begin{align}
\hat{\beta}_{2} & =\left[(X^{\T}X)^{-1}X^{\T}Y\right]_{\textup{last }l\textup{ elements}}\label{eq:fwl1}\\
 & =\left\{ X_{2}^{\T}(I_{n}-H_{1})X_{2}\right\} ^{-1}X_{2}^{\T}(I_{n}-H_{1})Y\quad\text{ where }H_{1}=X_{1}(X_{1}^{\T}X_{1})^{-1}X_{1}^{\T}  \label{eq:fwl2}\\
 & =(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}\tilde{X}_{2}^{\T}Y\quad\text{ where }\tilde{X}_{2}=(I_{n}-H_{1})X_{2}\label{eq:fwl3}\\
 & =(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}\tilde{X}_{2}^{\T}\tilde{Y}\quad\text{ where }\tilde{Y}=(I_{n}-H_{1})Y.\label{eq:fwl4}
\end{align}
\end{theorem}


This result is often called the  Frisch--Waugh--Lovell (FWL) Theorem in econometrics \citep{frisch1933partial, lovell1963seasonal}, although its equivalent forms were also known in classic statistics\footnote{Professor Alan Agresti gave me the reference of \citet{yule1907theory}.}. 
%A similar form is called Cochran's formula, which will appear as a homework problem.   
%fisher1925statistical, cochran1938omission  

Before proving Theorem \ref{thm:fwl}, I will first discuss its meanings
and interpretations. Equation (\ref{eq:fwl1}) follows from the definition
of the OLS coefficient. The matrix $I_{n}-H_{1}$ in equation (\ref{eq:fwl2})
is the projection matrix onto the space orthogonal to the column
space of $X_{1}$. Equation (\ref{eq:fwl3}) states that $\hat{\beta}_{2}$
equals the OLS coefficient of $Y$ on $\tilde{X}_{2}=(I_{n}-H_{1})X_{2}$,
which is the residual matrix from the column-wise OLS fit of $X_{2}$
on $X_{1}$\footnote{See Problem \ref{hw03::multiple-responses} for more details. }. So $\hat{\beta}_{2}$ measures the ``impact'' of $X_{2}$
on $Y$ after ``adjusting'' for the impact of $X_{1}$, that is, it
measures the partial or pure ``impact'' of $X_{2}$ on $Y$. Equation
(\ref{eq:fwl4}) is a slight modification of Equation (\ref{eq:fwl3}),
stating that $\hat{\beta}_{2}$ equals the OLS coefficient of $\tilde{Y}$
on $\tilde{X}_{2}$, where $\tilde{Y}=(I_{n}-H_{1})Y$ is the residual
vector from the OLS fit of $Y$ on $X_{1}$. From (\ref{eq:fwl3}) and
(\ref{eq:fwl4}), it is not crucial to residualize $Y$, but
it is crucial to residualize $X_{2}$. 

The forms (\ref{eq:fwl3}) and (\ref{eq:fwl4}) suggest the interpretation of
$\hat{\beta}_{2}$ as the ``impact'' of $X_{2}$ on $Y$ holding $X_{1}$
constant, or in an econometric term, the ``impact'' of $X_{2}$ on
$Y$ \emph{ceteris paribus}. \citet{marshall1890principles} used the Latin phrase \emph{ceteris paribus}. Its English meaning is ``with other conditions remaining the same.'' 
However, the algebraic meaning of the FWL theorem is that the OLS coefficient of a variable equals the {\it partial regression} coefficient based on the residuals. 
Therefore, taking the Latin phase too
seriously may be problematic because Theorem \ref{thm:fwl} is a pure algebraic
result without any distributional assumptions. We cannot hold $X_{1}$
constant using pure linear algebra. Sometimes, we can manipulate the value
of $X_{1}$ in an experimental setting, but this relies on the assumption of the data-collecting process. 

There are many ways to prove Theorem \ref{thm:fwl}. Below I first take
a detour to give an unnecessarily complicated proof because some intermediate
steps will be useful for later parts of the book. I will then give a
simpler proof which requires a deep understanding of OLS as a linear projection. 


The first proof relies on the following lemma. 

\begin{lemma}
\label{lem:blockinverse}The inverse of $X^{\T}X$ is
\[
(X^{\T}X)^{-1}=\left(\begin{array}{cc}
 S_{11}  &  S_{12} \\
 S_{21}  &  S_{22} 
\end{array}\right),
\]
 where 
\begin{align*}
 S_{11}  & =(X_{1}^{\T}X_{1})^{-1}+(X_{1}^{\T}X_{1})^{-1}X_{1}^{\T}X_{2}(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}X_{2}^{\T}X_{1}(X_{1}^{\T}X_{1})^{-1},\\
 S_{12}  & =-(X_{1}^{\T}X_{1})^{-1}X_{1}^{\T}X_{2}(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1},\\
 S_{21}  & = S_{12}^{\T},\\
 S_{22}  & =(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}.
\end{align*}
\end{lemma}




I leave the proof of Lemma \ref{lem:blockinverse} as Problem \ref{hw6::inverse-block-gram}. With Lemma \ref{lem:blockinverse}, we can easily prove Theorem \ref{thm:fwl}.


\begin{myproof}{Theorem}{\ref{thm:fwl}} (Version 1) 
The OLS coefficient is
\[
\left(\begin{array}{c}
\hat{\beta}_{1}\\
\hat{\beta}_{2}
\end{array}\right)=(X^{\T}X)^{-1}X^{\T}Y=\left(\begin{array}{cc}
 S_{11}  &  S_{12} \\
 S_{21}  &  S_{22} 
\end{array}\right)\left(\begin{array}{c}
X_{1}^{\T}Y\\
X_{2}^{\T}Y
\end{array}\right).
\]
Then using Lemma \ref{lem:blockinverse}, we can simplify $\hat{\beta}_{2}$
as 
\begin{align}
\hat{\beta}_{2} & = S_{21} X_{1}^{\T}Y+ S_{22} X_{2}^{\T}Y\nonumber \\
 & =-(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}X_{2}^{\T}X_{1}(X_{1}^{\T}X_{1})^{-1}X_{1}^{\T}Y+(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}X_{2}^{\T}Y\nonumber \\
 & =-(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}X_{2}^{\T}H_{1}Y+(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}X_{2}^{\T}Y\nonumber \\
 & =(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}X_{2}^{\T}(I_{n}-H_{1})Y\label{eq:form2}\\
 & =(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}\tilde{X}_{2}^{\T}Y.\label{eq:form3}
\end{align}
Equation (\ref{eq:form2}) is the form (\ref{eq:fwl2}), and Equation
(\ref{eq:form3}) is the form (\ref{eq:fwl3}). Because we also have
$X_{2}^{\T}(I_{n}-H_{1})Y=X_{2}^{\T}(I_{n}-H_{1})^{2}Y=\tilde{X}_{2}^{\T}\tilde{Y}$,
we can write $\hat{\beta}_{2}$ as $\hat{\beta}_{2}=(\tilde{X}_{2}^{\T}\tilde{X}_{2})^{-1}\tilde{X}_{2}^{\T}\tilde{Y}$,
giving the form (\ref{eq:fwl4}). 
\end{myproof}




The second proof does not invert the block matrix of $X^{\T} X$ directly. 


\begin{myproof}{Theorem}{\ref{thm:fwl}} (Version 2)
The OLS decomposition $Y =X_{1}\hat{\beta}_{1}+X_{2}\hat{\beta}_{2}+\hat{\varepsilon}$
satisfies 
\[
X^{\T}\hat{\varepsilon}=0\Longrightarrow\left(\begin{array}{cc}
X_{1} & X_{2}\end{array}\right)^{\T}\hat{\varepsilon}=0\Longrightarrow X_{1}^{\T}\hat{\varepsilon}=0,X_{2}^{\T}\hat{\varepsilon}=0.
\]
Multiplying $I_{n}-H_{1}$ on both sides of the OLS decomposition,
we have
\[
(I_{n}-H_{1}) Y =(I_{n}-H_{1})X_{1}  \hat{\beta}_1 +(I_{n}-H_{1})X_{2} \hat{\beta}_2 +(I_{n}-H_{1})\hat{\varepsilon},
\]
which reduces to 
$$
(I_{n}-H_{1})Y=(I_{n}-H_{1})X_{2}\hat{\beta}_{2}+\hat{\varepsilon}
$$
because $(I_{n}-H_{1})X_{1}=0$ and 
$
(I_{n}-H_{1})\hat{\varepsilon}=\hat{\varepsilon}-H_{1}\hat{\varepsilon}=\hat{\varepsilon}-X_{1}(X_{1}^{\T}X_{1})^{-1}X_{1}^{\T}\hat{\varepsilon}=\hat{\varepsilon}.
$
Further multiplying $X_2^{\T}$ on both sides of the identity, we have
$$
X_2^{\T}(I_{n}-H_{1})Y=X_2^{\T}(I_{n}-H_{1})X_{2}\hat{\beta}_{2}
$$
because $X_{2}^{\T}\hat{\varepsilon}=0$. 
The FWL theorem follows immediately. 

A subtle issue in this proof is to verify that $X_{2}^{\T}(I_{n}-H_{1})X_{2}$ is invertible. It is easy to show that matrix $X_{2}^{\T}(I_{n}-H_{1})X_{2}$ is positive semi-definite. To show it has rank $l$, we only need to show that 
$$
u_{2}^{\T}X_{2}^{\T}(I_{n}-H_{1})X_{2}u_{2}  =0 \Longrightarrow u_2 = 0.
$$
We have $u_{2}^{\T}X_{2}^{\T}(I_{n}-H_{1})X_{2}u_{2}   = \|(I_{n}-H_{1})X_{2}u_{2} \|^2=0$, so $(I_{n}-H_{1})X_{2}u_{2} = 0$, which further implies $X_{2}u_{2}\in\mathcal{C}(X_{1})$ by Proposition \ref{prop::projectionmatrix-geometry}. That is, $ X_{2}u_{2} = X_1 u_1$ for some $u_1$. So $X_1 u_1 - X_2 u_2 = 0$. Since the columns of $X$ are linearly independent, we must have $u_1 = 0$ and $u_2 = 0$. 
\end{myproof}

 


I will end this section with two byproducts of the FWL theorem. First, $\tilde{X}_{2}$ is the residual matrix from the OLS fit of $X_2$ on $X_1$. It is an $n\times l$ matrix with linearly independent columns as shown in the proof of Theorem \ref{thm:fwl} (Version 2)  and induces a projection matrix
$$
\tilde{H}_2 = \tilde{X}_{2} (\tilde{X}_{2}^{\T} \tilde{X}_{2})^{-1} \tilde{X}_{2}^{\T}. 
$$
This projection matrix is closely related to the projection matrices induced by $X$ and $X_1$ as shown in the following lemma. 

\begin{lemma}
\label{lemma::decompose-projection-matrices}
We have
$$
H_1 \tilde{H}_2  = \tilde{H}_2  H_1 = 0,\quad
H = H_1 +  \tilde{H}_2. 
$$
\end{lemma}

Lemma \ref{lemma::decompose-projection-matrices} is purely algebraic. I leave the proof as Problem \ref{hw08::decompose-projection-m}. The first two identities imply that the column space of $\tilde{X}_{2}$ is orthogonal to the column space of $X_1$.  The last identity $H = H_1 +  \tilde{H}_2$ has a clear geometric interpretation. For any vector $v \in \mathbb{R}^n$, we have $Hv = H_1v +  \tilde{H}_2v$, so the projection of $v$ onto the column space of $X$ equals the summation of the projection of $v$ onto the column space of $X_1$ and the projection of $v$ onto the column space of $\tilde{X}_{2}$. Importantly, $H \neq H_1 + H_2$ in general. 



Second, we can obtain $\hat{\beta}_2$ from \eqref{eq:fwl3} or \eqref{eq:fwl4}, which corresponds to the partial regression of $Y$ on $\tilde{X}_{2}$ or the partial regression of $\tilde{Y}$ on $\tilde{X}_{2}$. We can verify that the residual vector from the second partial regression equals the residual vector from the full regression. 

\begin{corollary}
\label{corollary::fwl-residuals}
We have $\hat{\varepsilon} = \hat{e}$, where $\hat{\varepsilon}$ is the residual vector from the OLS fit of $Y$ on $X$ and $\hat{e}$ is the residual vector from the OLS fit of $\tilde{Y}$ on $\tilde{X}_{2}$, respectively. 
\end{corollary} 

It is important to note that this conclusion is only true if both $Y$ and $X_2$ are residualized. The conclusion does not hold if we only residualize $X_2$. See Problem \ref{hw06-fwl-residual}. 



\begin{myproof}{Corollary}{\ref{corollary::fwl-residuals}}
We have $\hat{ \varepsilon} = (I-H)Y$ and 
$$
\hat{e}= (I-\tilde{H}_2) \tilde{Y} = (I- \tilde{H}_2)(I-H_1) Y .
$$ 
It suffices to show that $I-H = (I- \tilde{H}_2)(I-H_1)$, or, equivalently, $ I-H =  I - H_1 - \tilde{H}_2 + \tilde{H}_2 H_1$. This holds due to Lemma \ref{lemma::decompose-projection-matrices}. 
\end{myproof}

%\section{A numerical example}
%
%We first generate data and compute the coefficients from the long regression.
%\begin{lstlisting}
%> n   = 100
%> X1  = rnorm(n)
%> X2  = 0.5*X1 + rnorm(n)
%> Y   = 1 + X1 + X2 + rnorm(n)
%> lm(Y ~ X1 + X2)$coef
%(Intercept)          X1          X2 
%  1.0097883   0.8813489   1.0133137 
%\end{lstlisting}
%
%We can see that the coefficient of $X_2$ equals those from two partial regressions below:
%\begin{lstlisting} 
%> reg2.1 = lm(X2 ~ X1)
%> regY.1 = lm(Y ~ X1)
%> lm(Y ~ 0 + I(reg2.1$residual))$coef
%I(reg2.1$residual) 
%          1.013314 
%> lm(I(regY.1$residual) ~ 0 + I(reg2.1$residual))$coef
%I(reg2.1$residual) 
%          1.013314 
%\end{lstlisting}
%
%Moreover, it does matter whether we include the intercept or not in the partial regressions because the residuals are centered. 
%\begin{lstlisting} 
%> lm(Y ~ I(reg2.1$residual))$coef
%       (Intercept) I(reg2.1$residual) 
%         0.8217285          1.0133137 
%> lm(I(regY.1$residual) ~ I(reg2.1$residual))$coef
%       (Intercept) I(reg2.1$residual) 
%      5.536334e-17       1.013314e+00 
%\end{lstlisting}



\section{FWL theorem for the standard errors}

Based on the OLS fit of $Y$ on $X$, we have two estimated covariances for the second component $ \hat{\beta}_2$: $\hat{V}$ assuming homoskedasticity and $\hat{V}_{\textsc{ehw}}$
allowing for heteroskedasticity.

The FWL theorem demonstrates that we can also obtain $ \hat{\beta}_2$ from the OLS fit of $\tilde{Y}$ on $\tilde{X}_2$. Then based on this partial regression, we have two estimated covariances for $ \hat{\beta}_2$: 
$
\tilde{V}  
$ 
assuming homoskedasticity and 
$
\tilde{V}_{\textsc{ehw}}  
$
 allowing for heteroskedasticity. 
 
 The following theorem establishes their equivalence. 
 
 
 \begin{theorem}
 \label{thm::fwl-se}
 $(n-k-l)\hat{V} = (n-l)  \tilde{V} $ and $\hat{V}_{\textsc{ehw}} = \tilde{V}_{\textsc{ehw}}.$
 \end{theorem}
 
 
Theorem \ref{thm:fwl} is well known for a long time but Theorem \ref{thm::fwl-se} is less well known. \citet{lovell1963seasonal} hinted at the first identity in Theorem \ref{thm::fwl-se}, and 
\citet{ding2021frisch} proved Theorem \ref{thm::fwl-se}. 



\begin{myproof}{Theorem}{\ref{thm::fwl-se}}
%thm::fwl-seWe first recall the formulas of the estimators of the covariance matrix of $ \hat{\beta}_2$. 
By Corollary \ref{corollary::fwl-residuals}, the full regression and partial regression have the same residual vector, denoted by $\hat{ \varepsilon}$. Therefore, 
$\hat{\Omega}_\textsc{ehw} = \tilde{\Omega}_\textsc{ehw} = \text{diag}\{ \hat{\varepsilon}^2\}$ in the EHW covariance estimator. 

Based on the full regression, define $\hat{\sigma}^2 = \| \hat{ \varepsilon} \|_2^2/(n-k-l)$.  Then $\hat{V}$ equals the $(2,2)$th block of
$
  \hat{\sigma}^2 (X^{\T} X)^{-1} ,
$
and $\hat{V}_{\textsc{ehw}}$ equals the $(2,2)$th block of
$
  (X^{\T} X)^{-1} X^{\T} \hat{\Omega}_\textsc{ehw} X (X^{\T}X)^{-1}.
$ 
Based on the partial regression, define $\tilde{\sigma}^2 = \| \hat{\varepsilon}\|_2^2/(n-l)$.  Then $
\tilde{V} =   \tilde{\sigma}^2 ( \tilde{X}_2 ^{\T} \tilde{X}_2)^{-1} 
$  and $
\tilde{V}_{\textsc{ehw}} = ( \tilde{X}_2 ^{\T} \tilde{X}_2)^{-1}  \tilde{X}_2^{\T}  \tilde{\Omega}_\textsc{ehw} \tilde{X}_2  ( \tilde{X}_2 ^{\T} \tilde{X}_2)^{-1}. 
$

 



Let $\hat{\sigma}^2 = \| \hat{ \varepsilon} \|^2/(n-k-l)$ and $\tilde{\sigma}^2 = \|  \hat{ \varepsilon} \|^2/(n-l)$ be the common variance estimators. They are identical up to the degrees of freedom correction. 
Under homoskedasticity, the covariance estimator for $\hat{\beta}_2$ is the $(2,2)$th block of $\hat{\sigma}^2 (X ^{\T} X)^{-1}$, that is, $ \hat{\sigma}^2   S_{22} = \hat{\sigma}^2  ( \tilde{X}_2 ^{\T} \tilde{X}_2 )^{-1}$ by Lemma \ref{lem:blockinverse}, which is identical to the covariance estimator for $\tilde{\beta}_2$ up to the degrees of freedom correction. 


The EHW covariance estimator from the full regression is the $(2,2)$ block of
$
 \hat{A}\hat{\Omega}_\textsc{ehw} \hat{A}^{\T},
$
where  
$$
\hat{A} = (X ^{\T} X)^{-1} X ^{\T} = \begin{pmatrix}
*\\
 -  ( \tilde{X}_2 ^{\T} \tilde{X}_2 )^{-1} X_2 ^{\T} H_1 + ( \tilde{X}_2 ^{\T}  \tilde{X}_2 )^{-1}X_2 ^{\T}
\end{pmatrix}
=\begin{pmatrix}
*\\
 ( \tilde{X}_2 ^{\T} \tilde{X}_2 )^{-1}\tilde{X}_2 ^{\T}
\end{pmatrix}
$$
by Lemma \ref{lem:blockinverse}. I omit the  $*$ term because it does not affect the final calculation.
Define $\tilde{A}_2 =   ( \tilde{X}_2 ^{\T} \tilde{X}_2 )^{-1}\tilde{X}_2^{\T}$, and then
$$
\hat{V}_{\textsc{ehw}} 
= \tilde{A}_2 \hat{\Omega}_\textsc{ehw} \tilde{A}_2^{\T}
=  \tilde{A}_2 \tilde{\Omega}_\textsc{ehw}  \tilde{A}_2 ^{\T},
$$
which equals the EHW covariance estimator $\tilde{V}_{\textsc{ehw}}$ from the partial regression. 
\end{myproof}




\section{Gram--Schmidt orthogonalization, QR decomposition, and the computation of OLS}



 When the regressors are orthogonal, the coefficients from the long and short regressions are identical, which simplifies the calculation and theoretical discussion. 

\begin{corollary}
\label{cor:orthogonal}If $X_{1}^{\T}X_{2}=0$, i.e., the columns
of $X_{1}$ and $X_{2}$ are orthogonal, then $\tilde{X}_{2}=X_{2}$
and $\hat{\beta}_{2}=\tilde{\beta}_{2}.$ 
\end{corollary}


\begin{myproof}{Corollary}{\ref{cor:orthogonal}}
We can directly prove Corollary \ref{cor:orthogonal} by verifying that $X^{\T} X$ is block diagonal. 

Alternatively, 
Corollary \ref{cor:orthogonal} follows directly from
\[
\tilde{X}_{2}=(I_{n}-H_{1})X_{2}=X_{2}-X_{1}(X_{1}^{\T}X_{1})^{-1}X_{1}^{\T}X_{2}=X_{2},
\]
 and Theorem \ref{thm:fwl}.
\end{myproof}
 

This simple fact motivates us to orthogonalize the columns of the covariate matrix, which in turn gives the famous QR decomposition in linear algebra. Interestingly, the \ri{lm} function in \ri{R} uses the QR decomposition to compute the OLS estimator. To facilitate the discussion, I will use the notation
$$
 \hat{\beta}_{V_2|V_1} V_1
$$
as the linear projection of the vector $V_2 \in \mathbb{R}^n$ onto the vector $V_1 \in \mathbb{R}^n$, where $\hat{\beta}_{V_2|V_1}  =  V_2^{\T} V_1  /   V_1^{\T} V_1 $.
 This is from the univariate OLS of $V_2$ on $ V_1$. 



With a slight abuse of notation, partition the covariate matrix into column vectors $X = (X_1, \ldots, X_p)$. The goal is to find orthogonal vectors $(U_1, \ldots, U_p)$ that generate the same column space as $X$.  
Start with 
$$
X_1 = U_1.
$$ 
Regress $X_2$ on $U_1$ to obtain the fitted and residual vector  
$$
X_2 = \hat{\beta}_{X_2|U_1} U_1  + U_2; 
$$
by OLS, $U_1$ and $U_2$ must be orthogonal. 
Regress $X_3$ on $(U_1, U_2)$ to obtain the fitted and residual vector  
 $$
X_3 = \hat{\beta}_{X_3|U_1} U_1 +   \hat{\beta}_{X_3|U_2} U_2 + U_3; 
$$
by Corollary \ref{cor:orthogonal}, the OLS reduces to two univariate OLS and ensures that $U_3$ is orthogonal to both $U_1$ and $U_2$.  This justifies the notation $\hat{\beta}_{X_3|U_1} $ and $\hat{\beta}_{X_3|U_2}$. Continue this procedure to the last column vector:
$$
X_p = \sum_{j=1}^{p-1} \hat{\beta}_{X_p|U_j} U_j + U_p;
$$
by OLS, $U_p$ is orthogonal to all $U_j$ $(j=1,\ldots, p-1)$. We further normalize the $U$ vectors to have unit length:
$$
Q_j = U_j / \|  U_j \|,\quad (j= 1, \ldots, p).
$$
The whole process is called the Gram--Schmidt orthogonalization, which is essentially the sequential OLS fits. This process generates an $n\times p$ matrix with orthonormal column vectors 
$$
Q = (Q_1, \ldots, Q_p). 
$$
More interestingly, the column vectors of $X$ and $Q$ can linearly represent each other because 
\begin{eqnarray*}
X &=&  (X_1, \ldots, X_p) \\
&=& (U_1, \ldots, U_p) \begin{pmatrix}
1 & \hat{\beta}_{X_2|U_1} & \hat{\beta}_{X_3|U_1} &\cdots & \hat{\beta}_{X_p|U_1} \\
0& 1& \hat{\beta}_{X_3|U_2} &\cdots & \hat{\beta}_{X_p|U_2} \\
\vdots & \vdots & \vdots & \cdots & \vdots \\
0&0&0& \cdots & 1
\end{pmatrix} \\
&=& Q
\text{diag}\{ \|  U_j \| \}_{j=1}^p
\begin{pmatrix}
1 & \hat{\beta}_{X_2|U_1} & \hat{\beta}_{X_3|U_1} &\cdots & \hat{\beta}_{X_p|U_1} \\
0& 1& \hat{\beta}_{X_3|U_2} &\cdots & \hat{\beta}_{X_p|U_2} \\
\vdots & \vdots & \vdots & \cdots & \vdots \\
0&0&0& \cdots & 1
\end{pmatrix}  . 
\end{eqnarray*}
We can verify that the product of the second and the third matrix is an upper triangular matrix, denoted by $R$. By definition, the $j$th diagonal element of $R$ equals $\|  U_j \|$, and the $(j, j')$th element of $R$ equals $\|  U_j \|  \hat{\beta}_{X_{j'}|U_j} $ for $j' > j.$
Therefore, we can decompose $X$ as
$$
X = QR
$$
where $Q$ is an $n\times p$ matrix with orthonormal columns and $R$ is a $p\times p$ upper triangular matrix. This is called the QR decomposition of $X$. 



Most software packages, for example, \texttt{R}, do not calculate the inverse of $X^{\T} X$ directly. Instead, they first find the QR decomposition of $X = QR$. Since the Normal equation simplifies to 
\begin{align*}
X^{\T}X\hat{\beta} & =X^{\T}Y,\\
R^{\T}Q^{\T}QR\hat{\beta} & =R^{\T}Q^{\T}Y,\\
R\hat{\beta} & =Q^{\T}Y,
\end{align*}
they then backsolve the linear equation since $R$ is upper triangular. 



In \ri{R}, the \ri{qr} function returns the QR decomposition of a matrix. 

\begin{lstlisting}
> X   = matrix(rnorm(7*3), 7, 3)
> X
            [,1]       [,2]       [,3]
[1,] -0.57231223  0.1196325  0.8087505
[2,] -1.76090225  1.0627631  1.8170361
[3,] -0.04144281 -0.2904749 -1.8372247
[4,] -0.37627821  0.4476932 -0.9629320
[5,] -1.40848027  0.2735408 -0.8047917
[6,]  1.84878518  0.7290005  1.2688929
[7,]  0.06432856  0.2256284  0.3972229
> qrX = qr(X)
> qr.Q(qrX)
            [,1]        [,2]        [,3]
[1,] -0.19100878 -0.03460617  0.30340481
[2,] -0.58769981 -0.60442928  0.23753900
[3,] -0.01383151  0.21191991 -0.55839928
[4,] -0.12558257 -0.28728403 -0.62864750
[5,] -0.47007924 -0.07020076 -0.36640938
[6,]  0.61703067 -0.68778411 -0.09999859
[7,]  0.02146961 -0.16748246  0.01605493
> qr.R(qrX)
         [,1]       [,2]       [,3]
[1,] 2.996261 -0.3735673  0.0937788
[2,] 0.000000 -1.3950642 -2.1217223
[3,] 0.000000  0.0000000  2.4826186
\end{lstlisting}


If we specify \ri{qr = TRUE} in the \ri{lm} function, it will also return the QR decomposition of the covariate matrix. 

\begin{lstlisting}
> Y   = rnorm(7)
> lmfit = lm(Y ~ 0 + X, qr = TRUE)
> qr.Q(lmfit$qr)
            [,1]        [,2]        [,3]
[1,] -0.43535054 -0.25679823 -0.65480400
[2,] -0.47091275 -0.13639459  0.14746444
[3,]  0.66494532  0.07725435 -0.39436265
[4,]  0.21136347 -0.78814737  0.34611820
[5,] -0.04493356 -0.40413829 -0.01156273
[6,] -0.28046504  0.10655755 -0.16193251
[7,]  0.14561808 -0.33708219 -0.49780561
> qr.R(lmfit$qr)
        X1         X2         X3
1 3.190035 -0.6964269  1.8693260
2 0.000000  2.0719787  1.9210212
3 0.000000  0.0000000 -0.9261921
\end{lstlisting}

The replicating \ri{R} code is in \ri{code7.4.R}\footnote{The ``\texttt{0 +}'' in the above code forces the OLS to exclude the constant term.}.




\section{Homework problems}

\paragraph{Inverse of a block matrix}\label{hw6::inverse-block-gram}

Prove Lemma \ref{lem:blockinverse} and the following alternative form: 
$$
(X^{\T} X)^{-1} = \begin{pmatrix}
Q_{11} & Q_{12} \\
Q_{21} & Q_{22}
\end{pmatrix},
$$
where $H_2 = X_2(X_2^{\T} X_2)^{-1} X_2 ^{\T} $, $\tilde{X}_1 = (I_n - H_2) X_1$, and
\begin{eqnarray*}
Q_{11} &=& (\tilde{X}_1^{\T} \tilde{X}_1)^{-1} ,\\
Q_{12} &=& - (\tilde{X}_1^{\T} \tilde{X}_1)^{-1} X_1^{\T} X_2 (X_2^{\T} X_2)^{-1} ,\\
Q_{21} &=& Q_{12}^{\T},\\
Q_{22} &=& (X_2^{\T} X_2)^{-1}  + (X_2^{\T} X_2)^{-1} X_2^{\T} X_1 (\tilde{X}_1^{\T} \tilde{X}_1)^{-1} X_1^{\T} X_2 (X_2^{\T} X_2)^{-1}.
\end{eqnarray*}

Hint: Use the formula in Problem \ref{hwmath1::inverse-block-matrix}. 


 

\paragraph{Residuals in the FWL theorem}\label{hw06-fwl-residual}

Give an example in which the residual vector from the partial regression of $Y$ on $\tilde{X}_{2}$ does not equal to the residual vector from the full regression. 

 

%\paragraph{Multivariate regression via univariate regressions}
%
%The FWL Theorem states that the OLS coefficient in the long regression can be obtained from several short regressions. Consider the most extreme case, if you only know how to compute univariate regressions, how can you compute $\hat{\beta}_j$, the $j$-th coordinate in the long regression?

%Hint: The coefficient in the OLS fit of a vector $a$ on a vector $b$ equals $a^{\T} b/ b^{\T} b$. 
%\paragraph{The sample version of Cochran's formula}\label{hw6::sample-cochran-formula}
%
%Consider an $n\times 1$ vector $Y$, an $n\times k$ matrix $X_1$, and an $n\times l$ matrix $X_2$. Similar to the FWL Theorem, we do not assume any randomness. 
%We can fit the following OLS:
%\begin{eqnarray*}
%Y &=& X_1 \hat{\beta}_1 + X_2 \hat{\beta}_2+ \hat{\varepsilon},\\
%Y &=& X_2 \tilde{\beta}_2 + \tilde{\varepsilon} ,\\
%X_1 &=& X_2 \hat{\delta} + \hat{U},
%\end{eqnarray*}
%where $\hat{\varepsilon}, \tilde{\varepsilon} , \hat{U}$ are the residuals. 
%The last OLS fit means the OLS fit of each column of $X_1$ on $X_2$, and therefore the corresponding residual $\hat{U}$ is an $n\times k$ matrix.
%Show that 
%$$
%\tilde{\beta}_2 = \hat{\beta}_2 +  \hat{\delta} \hat{\beta}_1.
%$$
%Note that this is a pure linear algebra fact similar to the FWL Theorem. 

 
 
 
 
\paragraph{Projection matrices}\label{hw08::decompose-projection-m}

 Prove Lemma \ref{lemma::decompose-projection-matrices}. 
 
 
 Hint: Use Lemma \ref{lem:blockinverse}. 



\paragraph{FWL theorem and leverage scores}\label{hw7::fwl-hat-matrix}

Consider the partitioned regression $Y = X_1 \hat{\beta}_1 + X_2 \hat{\beta}_2 + \hat{\varepsilon}$.
To obtain the coefficient $\hat{\beta}_2$, we can run two OLS fits: 
\begin{enumerate}
[(R1)]
\item\label{reg1-leverage}
regress $X_2$ on $X_1$ to obtain the residual $\tilde{X}_2$;
\item\label{reg2-leverage}
regress $Y$ on $\tilde{X}_2$ to obtain the coefficient, which equals $\hat{\beta}_2$ by the FWL Theorem. 
\end{enumerate}

Although partial regression (R\ref{reg2-leverage}) can recover the OLS coefficient, the leverage scores from  (R\ref{reg2-leverage}) are not the same as those from the long regression. Show that the summation of the corresponding leverage scores from (R\ref{reg1-leverage}) and (R\ref{reg2-leverage}) equals the leverage scores from the long regression. 


Remark: The leverage scores are the diagonal elements of the hat matrix from OLS fits. Chapter \ref{chapter::EHW} before mentioned them and Chapter \ref{chapter::leave-one-out} later will discuss them in more detail. 



\paragraph{Another invariance property of the OLS coefficient}\label{hw7::invariance-ols-fwl}


Partition the covariate matrix as $X = (X_1, X_2)$ where $X_1 \in \mathbb{R}^{n\times k}$ and $X_2 \in \mathbb{R}^{n\times l}$. Given any $A\in \mathbb{R}^{k\times l}$, define $\tilde{X}_2 = X_2 - X_1A$. Fit two OLS:
$$
Y = \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \hat\varepsilon
$$
and
$$
Y = \tilde{\beta}_1 X_1 + \tilde{\beta}_2 \tilde{X}_2 + \tilde\varepsilon .
$$
Show that
$$
\hat{\beta}_2 = \tilde{\beta}_2, \quad \hat\varepsilon = \tilde\varepsilon.
$$

Hint: Use the result in Problem \ref{hw3::invariance-ols}. 

Remark: Choose $A = (X_1^{\T}X_1)^{-1} X_1^{\T} X_2$ to be the coefficient matrix of the OLS fit of $X_2$ on $X_1$. The above result ensures that $\hat{\beta}_2$ equals $\tilde{\beta}_2$ from the OLS fit of $Y$ on  $X_1$ and $(I_n-H_1)X_2$, which is coherent with the FWL theorem since $X_1^{\T} (I_n-H_1)X_2 = 0$.




\paragraph{Alternative formula for the EHW standard error}\label{hw7::alternative-ehw-se}


Consider the partition regression $Y = X_1 \hat\beta_1 + X_2 \hat\beta_2  + \hat{\varepsilon}$ with $X_1$ is an $n\times (p-1)$ matrix and  $X_2 $ is an $n$ dimensional vector. So $ \hat\beta_2$ is a scalar, and the $(p,p)$th element of $\hat{V}_\textsc{ehw}$ equals $\hat{\text{se}}_{\textsc{ehw}, 2}^2$, the squared EHW standard error for $ \hat\beta_2$. 

Define 
$$
\tilde{X}_2 = (I_n - H_1) X_2 = \begin{pmatrix}
\tilde{x}_{12}\\
\vdots \\
\tilde{x}_{n2}
\end{pmatrix} . 
$$
Prove that under Assumption \ref{assume::heteroskedasticity-lm},  we have 
\begin{eqnarray*}
\var( \hat\beta_2  ) &=& \sumn  w_i \sigma_i^2,  \\
\hat{\text{se}}_{\textsc{ehw}, 2}^2  &=&  \sumn  w_i   \hat\varepsilon_i^2
\end{eqnarray*}
where 
$$
w_i = \frac{  \tilde{x}_{i2}^2  }{   ( \sumn    \tilde{x}_{i2}^2 )^2  } .
$$ 




Remark: You can use Theorems \ref{thm:fwl} and \ref{thm::fwl-se} to prove the result. 
The original formula of the EHW covariance matrix has a complex form. However, using the FWL theorems, we can simplify each of the squared EHW standard errors as a weighted average of the squared residuals, or, equivalently, a simple quadratic form of the residual vector. 



\paragraph{A counterexample to the Gauss--Markov theorem}\label{hw3::counterexample-gauss-markov-heteroskedasticity}


The Gauss--Markov theorem does not hold under the heteroskedastic linear model. This problem gives a counterexample in a simple linear model.


Assume $y_i = \beta x_i + \varepsilon_i$ without the intercept and with potentially different $\var(\varepsilon_i) = \sigma_i^2$ across $i=1,\ldots, n$. 
Consider two OLS estimators: the first OLS estimator does not contain the intercept $\hat{\beta} = \sumn x_i y_i / \sumn x_i^2$; the second OLS estimator contains the intercept $\tilde{\beta} =\sumn (x_i - \bar{x}) y_i / \sumn (x_i - \bar{x})^2 $ even though the true linear model does not contain the intercept. 


The Gauss-Markov theorem ensures that if $\sigma_i^2=\sigma^2$ for all $i$'s, then the variance of $\hat{\beta}$ is smaller than or equal to the variance of $\tilde{\beta} $. However, it does not hold when $\sigma_i^2$'s vary. 

Give a counterexample in which the variance of $\hat{\beta}$ is larger than the variance of $\tilde{\beta} $. 





 \paragraph{QR decomposition of $X$ and the computation of OLS}\label{hw3::ols-qr-decomposition}
 
Verify that the $R$ matrix equals 
 $$
R = 
\begin{pmatrix}
Q_1^{\T} X_1 & Q_1^{\T} X_2 & \cdots &  Q_1^{\T} X_p  \\
0 & Q_2^{\T} X_2& \cdots &  Q_2^{\T} X_p  \\
\vdots & \vdots &  \cdots &  \vdots \\
0&0& \cdots & Q_p^{\T} X_p
\end{pmatrix}.
$$

Based on the QR decomposition of $X$, show that 
$$
H = QQ^{\T},
$$ 
and $h_{ii}$ equals the squared length of the $i$-th row of $Q$. 




 \paragraph{Uniqueness of the QR decomposition}\label{hw6::uniqueQR}
 
 Show that if $X$ has linearly independent column vectors, the QR decomposition must be unique. That is, if $X = QR = Q_1 R_1$ where $Q$ and $Q_1$ have orthonormal columns and $R$ and $R_1$ are upper triangular, then we must have
 $$
 Q = Q_1,\quad  R = R_1.
 $$
 

