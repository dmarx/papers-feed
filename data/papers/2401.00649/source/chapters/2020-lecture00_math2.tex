 
\chapter{Random Variables}\label{chapter:appendix-rvs}
 
Let ``IID'' denote ``independent and identically distributed'', ``$\iidsim$'' denote a sequence of random variables that are IID with some common distribution, and ``$\ind$'' denote independence between random variables. 
Define Euler's Gamma function as 
$$
\Gamma(z)=\int_{0}^{\infty}x^{z-1}e^{-x}\d x ,\qquad (z>0),
$$
which is a natural extension of the factorial since $\Gamma(n) = (n-1)!$. Further define
the digamma function  as  $\psi(z) = \diff \log  \Gamma (z) / \diff z$ and the trigamma function as $\psi'(z)$. 
In \ri{R}, we can use 
\begin{rc}
gamma(z)
lgamma(z)
digamma(z)
trigamma(z)
 \end{rc}
 to compute $\Gamma(z)$, $\log \Gamma(z)$, $\psi(z)$, and $\psi'(z)$. 
 
 

\section{Some important univariate random variables}

\subsection{Normal, $\chi^{2}$, $t$ and $F$}
The standard Normal random variable $Z\sim\N(0,1)$ has density 
\[
f(z)=(2\pi)^{-1/2}\exp\left(-z^{2}/2\right).
\]
A Normal random variable $X$ has mean $\mu$ and variance $\sigma^{2}$, denoted by $\N(\mu,\sigma^2)$, 
if $X=\mu+\sigma Z$. We can show that $X$ has density 
\[
f(x)=(2\pi)^{1/2}\exp\left\{ -(x-\mu)^{2}/(2\sigma^{2})\right\} .
\]

A chi-squared random variable with degrees of freedom $n,$ denoted
by $Q_{n}\sim\chi_{n}^{2},$ can be represented as 
$$
Q_{n}=\sumn Z_{i}^{2} , 
$$
where $Z_{i} \iidsim \N(0,1)$. We can show that its density is
\begin{equation}
f_{n}(q)=q^{n/2}\exp(-q/2)\Big/\left\{ 2^{n/2}\Gamma(n/2)\right\} ,\qquad(q>0).\label{eq:chisqPDF}
\end{equation}
We can verify that the above density \eqref{eq:chisqPDF} is well-defined even if we change
the integer $n$ to be an arbitrary positive real number $\nu$, and
call the corresponding random variable $Q_{\nu}$ a chi-squared random
variable with degrees of freedom $\nu$, denoted by $Q_\nu\sim \chi^2_\nu$.


A $t$ random variable with degrees of freedom $\nu$ can be represented
as 
$$
t_{\nu}=\frac{ Z}{ \sqrt{Q_{\nu}  / \nu }    } 
$$ 
where $Z\sim\N(0,1),Q_{\nu}\sim\chi_{\nu}^{2}$, 
and $Z\ind Q_{\nu}.$ 

An $F$ random variable with degrees of freedom $(r,s)$ can be represented
as
\[
F=\frac{Q_{r}/r}{Q_{s}/s}
\]
where $Q_{r}\sim\chi_{r}^{2},Q_{s}\sim\chi_{s}^{2}$, and $Q_{r}\ind Q_{s}$. 

\subsection{Beta--Gamma duality}

The $\text{Gamma}(\alpha,\beta)$ random variable with parameters
$\alpha,\beta>0$ has density
\begin{equation}
f(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x},\quad(x>0).\label{eq:gammaPDF}
\end{equation}
The $\text{Beta}(\alpha,\beta)$ random variable with parameters $\alpha,\beta>0$
has density
\[
f(x)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1},\quad(0<x<1).
\]
These two random variables are closely related as shown in the following
theorem.
\begin{theorem}
\label{thm:beta-gamma-duality}If $X\sim\textup{Gamma}(\alpha,\theta),Y\sim\textup{Gamma}(\beta,\theta)$
and $X\ind Y$, then
\begin{enumerate}
\item $X+Y\sim\textup{Gamma}(\alpha+\beta,\theta),$
\item $X/(X+Y)\sim\textup{Beta}(\alpha,\beta),$
\item $X+Y\ind X/(X+Y).$
\end{enumerate}
\end{theorem}



Another simple but useful fact is that $\chi^{2}$ is a special Gamma
random variable. Comparing the densities in (\ref{eq:chisqPDF}) and
(\ref{eq:gammaPDF}), we obtain the following result.
\begin{proposition}\label{prop::chi2-gamma}
$\chi_{n}^{2}\sim\textup{Gamma}(n/2,1/2).$
\end{proposition}
We can also calculate the moments of the Gamma and Beta distributions.
\begin{proposition}
\label{thm:gammamoments}If $X\sim\textup{Gamma}(\alpha,\beta)$,
then
\begin{eqnarray*}
E(X) &=&\frac{\alpha}{\beta},\\ 
\var(X) &=&\frac{\alpha}{\beta^{2}}.
\end{eqnarray*}
\end{proposition}


\begin{proposition}
\label{thm:gammamoments-log}If $X\sim\textup{Gamma}(\alpha,\beta)$,
then
\begin{eqnarray*}
E(\log X)  &=&  \psi(\alpha) - \log \beta ,\\ 
\var(\log X) &=& \psi'(\alpha). 
\end{eqnarray*}
\end{proposition}


\begin{proposition}
\label{thm:beta-moments}If $X\sim\textup{Beta}(\alpha,\beta),$ then
\begin{eqnarray*}
E(X)  &=&  \frac{\alpha}{\alpha+\beta},\\
\var(X) &=& \frac{\alpha\beta}{(\alpha+\beta)(\alpha+\beta+1)}.
\end{eqnarray*}
\end{proposition}

\begin{proposition}
\label{thm:beta-moments-log}If $X\sim\textup{Beta}(\alpha,\beta),$ then
\begin{eqnarray*}
E(\log X)  &=& \psi(\alpha) - \psi(\alpha + \beta) ,\\ 
\var(\log X) &=& \psi ' (\alpha) - \psi ' (\alpha + \beta) . 
\end{eqnarray*}
\end{proposition}


%
I leave the proofs of the above propositions as Problem \ref{hwmath2::beta-gamma-moments}. 



\subsection{Exponential, Laplace, and Gumbel distributions}
\label{subsec::expo-gumbel}

An Exponential$(\lambda)$ random variable $X \geq 0$ has density $f(x) = \lambda e^{-\lambda x}$, mean $1/\lambda$, median $\log 2/\lambda $ and variance $1/\lambda^2$.  The standard Exponential random variable $X_0$ has $\lambda = 1$, and $X_0 / \lambda$ generates Exponential$(\lambda)$. 



An important feature of Exponential$(\lambda)$ is the memoryless property.

\begin{proposition}
\label{prop::memoryless}
If $X\sim $ Exponential$(\lambda)$, then 
$$
\pr(X \geq x+c \mid X\geq c) =\pr(X \geq x).
$$
\end{proposition}

If $X$ represents the survival time, then the probability of surviving another $x$ time is always the same no matter how long the existing survival time is. 

\begin{myproof}{Proposition}{\ref{prop::memoryless}}
Because $\pr(X>x) = e^{-\lambda x}$, we have 
\begin{eqnarray*}
\pr(X \geq x+c \mid X\geq c) 
&=& \frac{  \pr(X \geq x+c) }{ \pr(X \geq c) }   \\
&=& \frac{e^{-\lambda (x + c)}}{ e^{-\lambda c} } \\
&=& e^{-\lambda x} \\
&=& \pr(X \geq x).
\end{eqnarray*}
\end{myproof}



The minimum of independent exponential random variables also follows an exponential distribution. 

\begin{proposition}
\label{prop::minimum-expo}
Assume that $X_{i}\sim\text{Exponential}(\lambda_{i})$ are independent $(i=1,\ldots,n)$. Then
$$
\underline{X} = \min(X_{1},\ldots,X_{n} ) \sim  \textup{Exponential}(  \lambda_{1}+\cdots+\lambda_{n}  )
$$
and
\[
\pr (X_{i}=\underline{X}  ) =\frac{\lambda_{i}}{\lambda_{1}+\cdots+\lambda_{n}}.
\]
\end{proposition}


\begin{myproof}{Proposition}{\ref{prop::minimum-expo}}
First, 
\begin{eqnarray*}
\pr(\underline{X} > x) 
&=& \pr(X_i > x, i=1,\ldots, n) \\ 
&=& \prod_{i=1}^n \pr(X_i > x) \\
&=&  \prod_{i=1}^n e^{-\lambda_i x} \\
&=& e^{-\sumn \lambda_i x} 
\end{eqnarray*}
so $\underline{X}$ is Exponential$(\sumn \lambda_i)$.

Second, we have 
\begin{align*}
\pr(  X_{i}=\underline{X}  )   & =\pr(X_{i}<X_{j}\text{ for all }j\neq i)\\
 & =\int_{0}^{\infty}\prod_{j\neq i}\pr(X_{j}>x)\lambda_{i}e^{-\lambda_{i}x}\d x\\
 & =\int_{0}^{\infty}\prod_{j\neq i}e^{-\lambda_{j}x}\lambda_{i}e^{-\lambda_{i}x}\d x\\
 & =\lambda_{i}\int_{0}^{\infty}\prod_{i=1}^n e^{-\lambda_{j}x}\d x\\
 & =\lambda_{i}\int_{0}^{\infty}e^{-\sum_{j=1}^n\lambda_{j}x}\d x\\
 & =\lambda_{i}/\sum_{j=1}^n\lambda_{j}.
\end{align*}
\end{myproof}


The difference between two IID exponential random variables follows the Laplace distribution 

\begin{proposition}
\label{prop::expo-laplace}
If $y_1$ and $y_2$ are two IID Exponential$(\lambda)$, then $y = y_1 - y_2$ has  density 
$$
 \frac{\lambda}{2}  \exp( - \lambda | c| ),\quad -\infty < c<\infty
$$ 
which is the density of a Laplace distribution with mean $0$ and variance $2/\lambda^2$. 
\end{proposition}

\begin{myproof}{Proposition}{\ref{prop::expo-laplace}}
Both $y_{1}$ and $y_{2}$ have density $f(c)=\lambda e^{-\lambda c}$
and CDF $F(c)=1-e^{-\lambda c}$. The CDF of $y=y_{1}-y_{2}$ at $c\geq0$
is
\begin{align*}
 \pr(y_{1}-y_{2}  \leq c)&=\int_{0}^{\infty}\pr(y_{2}\geq z-c)\lambda e^{-\lambda z}\text{d}z\\
 & =\int_{0}^{\infty}e^{-\lambda(z-c)}\lambda e^{-\lambda z}\text{d}z\\
 & =\lambda e^{\lambda c}\int_{0}^{\infty}e^{-2\lambda z}\text{d}z\\
 & =\lambda e^{\lambda c}/(2\lambda)\\
 & =e^{\lambda c}/2.
\end{align*}
By symmetry, $y_{1}-y_{2}\sim y_{2}-y_{1}$, so the CDF at $c\leq0$
is
\begin{align*}
\pr(y_{1}-y_{2}  \leq c) & =1-\pr(y_{1}-y_{2}\leq-c)\\
 & =1-e^{-\lambda c}/2.
\end{align*}
Therefore, the density of $y$ at $c\geq0$ is 
$
\frac{\lambda}{2}e^{\lambda c},
$
and the density of $y$ at $c\leq0$ is
$
\frac{\lambda}{2}e^{-\lambda c},
$
which can be unified as 
$
\frac{\lambda}{2}e^{\lambda|c|}.
$
\end{myproof}

If $X_0 $ is the standard exponential random variable, then we define the Gumbel$(\mu, \beta)$ random variable as 
$$
Y = \mu - \beta \log X_0.
$$
The standard Gumbel distribution has $\mu =0$ and $\beta  =1$, with
CDF
\[
F(y)=\exp(-e^{-y}),\quad y\in\mathbb{R}
\]
and density
\[
f(y)=\exp(-e^{-y})e^{-y},\quad y\in\mathbb{R}.
\]


By definition and Proposition \ref{prop::minimum-expo}, we can verify that the maximum of IID Gumbels is also Gumbel. 

\begin{proposition}
\label{prop::max-gumbel}
If $Y_1, \ldots, Y_n$ are IID Gumbel$(\mu,\beta)$, then
$$
\max_{1\leq i \leq n}  Y_i \sim \textup{Gumbel}(\mu + \beta \log n, \beta ).
$$
If $Y_1, \ldots, Y_n$ are independent Gumbel$(\mu_i, 1)$, then
$$
\max_{1\leq i \leq n}  Y_i \sim \textup{Gumbel}\left(\log \sumn e^{\mu_i}, 1 \right). 
$$
\end{proposition}


I leave the proof as Problem \ref{hwmath2::max-gumbel}. 




\section{Multivariate distributions}

A random vector $(X_{1},\ldots,X_{n})^{\T}$ is a vector consisting of $n$ random
variables. If all components are continuous, we can define its joint
density $f_{X_{1}\cdots X_{n}}(x_{1},\ldots,x_{n}).$ 

For a random vector $\binom{X}{Y}$ with $X$ and $Y$ possibly being
vectors, if it has joint density $f_{XY}(x,y)$, then we can obtain
the marginal distribution of $X$   
$$
f_{X}(x)=\int f_{XY}(x,y)\d y
$$
and define the conditional density  
\[
f_{Y|X}(y\mid x)=\frac{f_{XY}(x,y)}{f_{X}(x)}\qquad \text{if }  f_{X}(x)\neq 0. 
\]
Based on the conditional density, we can define the conditional expectation
of any function of $Y$  as 
\[
E\left\{ g(Y)\mid X=x\right\} =\int g(y)f_{Y|X}(y\mid x)\d y
\]
and the conditional variance as 
\[
\var\left\{ g(Y)\mid X=x\right\} =E\left[\left\{ g(Y)\right\} ^{2}\mid X=x\right]-\left[E\left\{ g(Y)\mid X=x\right\} \right]^{2}.
\]
 In the above definitions, the conditional mean and variance are both deterministic
functions of $x$. We can replace $x$ by the random variable $X$ to define 
$E\left\{ g(Y)\mid X\right\} $
and $\var\left\{ g(Y)\mid X\right\} $, which are functions of the
random variable $X$ and are thus random variables. 

Below are two important laws of conditional expectation and variance. 

\begin{theorem}
[Law of total expectation] We have 
\[
E( Y ) =E\left\{  E ( Y \mid X ) \right\}  . 
\]
\end{theorem}


\begin{theorem}
[Law of total variance or analysis of variance] 
\label{theorem::law-total-var}
We have 
\[
\var (Y) =E\left\{  \var( Y \mid X) \right\}  +\var\left\{  E(Y \mid X) \right\}  . 
\]
\end{theorem}
 

\paragraph*{Independence}

Random variables $(X_{1},\ldots,X_{n})$ are mutually independent
if 
$$
f_{X_{1}\cdots X_{n}}(x_{1},\ldots,x_{n})=f_{X_{1}}(x_{1})\cdots f_{X_{n}}(x_{n}).
$$
Note that in this definition, each of $(X_{1},\ldots,X_{n})$ can
be vectors. We have the following rules under independence.


\begin{proposition}
 If $X\ind Y$, then $h(X)\ind g(Y)$ for any functions
$h(\cdot)$ and $g(\cdot)$.
\end{proposition}

\begin{proposition}
If $X\ind Y$, then
\begin{align*}
f_{XY}(x,y) & =f_{X}(x)f_{Y}(y),\\
f_{Y\mid X}(y\mid x) & =f_{Y}(y),\\
E\left\{ g(Y)\mid X\right\}  & =E\left\{ g(Y)\right\} ,\\
E\left\{ g(Y)h(X)\right\}  & =E\left\{ g(Y)\right\} E\left\{ h(X)\right\} .
\end{align*}
\end{proposition}



\paragraph*{Expectations of random vectors or random matrices}

For a random matrix $W=(W_{ij})$, we define $E(W)=(E(W_{ij}))$.
For constant matrices $A$ and $C$, we can verify that
\begin{align*}
E(AW+C) & =AE(W)+C,\\
E(AWC) & =AE(W)C.
\end{align*}


\paragraph*{Covariance between two random vectors}

If $W\in\mathbb{R}^{r}$ and $Y\in\mathbb{R}^{s},$ then their covariance 
$$
\cov(W,Y)=E\left[\left\{ W-E(W)\right\} \left\{ Y-E(Y)\right\} ^{\T}\right]
$$
is an $r\times s$ matrix. 
As a special case, 
$$
\cov(Y)=\cov(Y,Y)=E\left[\left\{ Y-E(Y)\right\} \left\{ Y-E(Y)\right\} ^{\T}\right]
= E(YY^{\T}) - E(Y) E(Y)^{\T}. 
$$
For a scalar random variable, $\cov(Y)=\var(Y).$

\begin{proposition}\label{proposition::covariance-lineartrans}
For $A\in \mathbb{R}^{r\times n},Y\in\mathbb{R}^{n}$ and
$C\in\mathbb{R}^{r}$, we have $\cov(AY+C)=A\cov(Y)A^{\T}$.
\end{proposition}

Using Proposition \ref{proposition::covariance-lineartrans},
we can verify that for any $n$-dimensional random vector, $\cov(Y)\succeq0$
because for all $x\in\mathbb{R}^{n}$, we have 
\[
x^{\T}\cov(Y)x=\cov(x^{\T}Y)=\var(x^{\T}Y)\geq0.
\]

\begin{proposition}
For two random vectors $W$ and $Y$, we have
\[
\cov(AW+C,BY+D)=A\cov(W,Y)B^{\T}
\]
and 
\[
\cov(AW+BY)=A\cov(W)A^{\T}+B\cov(Y)B^{\T}+A\cov(W,Y)B^{\T}+B\cov(Y,W)A^{\T}.
\]
\end{proposition}


Similar to Theorem \ref{theorem::law-total-var}, we have the following decomposition of the covariance. 

\begin{theorem}
[Law of total covariance] 
\label{theorem::law-total-cov}
We have 
\[
\cov\left(   Y, W    \right)  =E\left\{  \cov\left( Y ,W\mid X\right) \right\} +\cov\left\{ E(Y\mid X) , E(W\mid X) \right\}. 
\]
\end{theorem}

\section{Multivariate Normal and its properties}

I use a generative definition of the multivariate Normal random vector.
First, $Z$ is a standard Normal random vector if $Z=(Z_{1},\ldots,Z_{n})^{\T}$
has components $Z_{i}\iidsim \N(0,1)$. Given a mean vector $\mu$ and a
positive semi-definite covariance matrix $\Sigma$, define a Normal
random vector $Y\sim\text{N(\ensuremath{\mu,\Sigma})}$ with mean
$\mu$ and covariance $\Sigma$ if $Y$ can be represented as  
\begin{eqnarray}
Y=\mu+AZ, \label{eq::mvn-definitions}
\end{eqnarray}
where $A$ satisfies $\Sigma=AA^{\T}$. We can verify that $\cov(Y)=\Sigma$,
so indeed $\Sigma$ is its covariance matrix. If $\Sigma\succ0$,
then we can verify that $Y$ has density
\begin{equation}\label{equation::normal-density}
f_{Y}(y)=(2\pi)^{-n/2}\left\{ \det(\Sigma)\right\} ^{-1/2}\exp\left\{ -(y-\mu)^{\T}\Sigma^{-1}(y-\mu)/2\right\} .
\end{equation}

We can easily verify the following result by calculating the density.

\begin{proposition}\label{prop::rotation-mvn}
If $Z\sim\textup{N}(0,I_{n})$ and $\Gamma$ is an orthogonal matrix, then
$\Gamma Z\sim \N(0,I_{n}).$
\end{proposition}


I do not define multivariate Normal based on the density \eqref{equation::normal-density} because it is only well defined with a positive definite $\Sigma$. I do not define multivariate Normal based on the characteristic function because it is more advanced than the level of this book.
Definition \eqref{eq::mvn-definitions} does not require $\Sigma$ to be positive definite and is more elementary. 
However, it has a subtle issue of uniqueness. Although
the decomposition $\Sigma=AA^{\T}$ is not unique, the resulting distribution
$Y=\mu+AZ$ is. We can verify this using the Polar decomposition. 
Because $A=\Sigma^{1/2}\Gamma$ where $\Gamma$ is an orthogonal matrix,
we have $Y=\mu+\Sigma^{1/2}\Gamma Z=\mu+\Sigma^{1/2}\tilde{Z}$ where
$\tilde{Z}=\Gamma Z$ is a standard Normal random vector by Proposition \ref{prop::rotation-mvn}.
Importantly, although the definition \eqref{eq::mvn-definitions} can be general, we usually use the following representation
$$
Y=\mu+ \Sigma^{1/2} Z.
$$



\begin{theorem}
\label{thm:Normal-ind-uncorr}Assume that
\[
\left(\begin{array}{c}
Y_{1}\\
Y_{2}
\end{array}\right)\sim\textup{N}\left(\left(\begin{array}{c}
\mu_{1}\\
\mu_{2}
\end{array}\right),\left(\begin{array}{cc}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)\right).
\]
Then $Y_{1}\ind Y_{2}$ if and only if $\Sigma_{12}=0.$
\end{theorem}
%
I leave the proof of Theorem \ref{thm:Normal-ind-uncorr} as Problem \ref{hwmath2::independence-uncorr-normal}.



\begin{proposition}\label{prop::linear-normal}
If $Y\sim\textup{N(\ensuremath{\mu,}\ensuremath{\Sigma})},$ then $BY+C\sim\textup{N\ensuremath{(B\mu+C,B\Sigma B^{\T}})}$,
that is, any linear transformation of a Normal random vector is also
a Normal random vector.
\end{proposition}
%
\begin{myproof}{Proposition}{\ref{prop::linear-normal}}
By definition, $Y=\mu+\Sigma^{1/2}Z$ where $Z$ is the standard Normal
random vector, we have 
\begin{align*}
BY+c & =B(\mu+\Sigma^{1/2}Z)+C\\
 & =B\mu+C+B\Sigma^{1/2}Z\\
 & \sim\text{N}(B\mu+C,B\Sigma^{1/2}\Sigma^{1/2\T}B^{\T})\\
 & \sim\text{N}(B\mu+C,B\Sigma B^{\T}).
\end{align*}
\end{myproof}



An obvious corollary of Proposition \ref{prop::linear-normal} is that if $X_1 \sim \N(\mu_1, \sigma_1^2)$ and $X_2 \sim \N(\mu_2, \sigma_2^2)$ are independent, then $X_1+X_2 \sim \N(  \mu_1 + \mu_2,  \sigma_1^2 + \sigma_2^2 )$. So the summation of two independent Normals is also Normal. Remarkably, the reverse of the result is also true. 

\begin{theorem}[Levy--Cramer]
\label{thm::levy-cramer}
 If $X_1 \ind X_2$ and $X_1 + X_2$ is Normal, then both $X_1$ and $X_2$ must be Normal.
\end{theorem}


The statement of Theorem \ref{thm::levy-cramer} is extremely simple. But its proof is non-trivial and beyond the scope of this book. See \citet{benhamou2018three} for a proof. 



\begin{theorem}
Assume 
\[
\left(\begin{array}{c}
Y_{1}\\
Y_{2}
\end{array}\right)\sim\textup{N}\left(\left(\begin{array}{c}
\mu_{1}\\
\mu_{2}
\end{array}\right),\left(\begin{array}{cc}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)\right).
\]
\begin{enumerate}
\item The marginal distributions are Normal:
\begin{align*}
Y_{1} & \sim\textup{N}\left(\mu_{1},\Sigma_{11}\right),\\
Y_{2} & \sim\textup{N}\left(\mu_{2},\Sigma_{22}\right).
\end{align*}
\item If $\Sigma_{22}\succ0,$ then the conditional distribution is Normal:
\[
Y_{1}\mid Y_{2}=y_{2}\sim\textup{N}\left(\mu_{1}+\Sigma_{12}\Sigma_{22}^{-1}(y_{2}-\mu_{2}),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right);
\]
$Y_{2}$ is independent of the residual 
\[
Y_{1}-\Sigma_{12}\Sigma_{22}^{-1}(Y_{2}-\mu_{2})\sim\textup{N}\left(\mu_{1},\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right).
\]
\end{enumerate}
\end{theorem}



I review some other results of the multivariate Normal below. 
 
\begin{proposition}
Assume $Y\sim\textup{N}(\mu,\sigma^{2}I_{n})$. If $AB^{\T}=0$, then
$AY\ind BY$.
\end{proposition}
%
%
\begin{proposition}
Assume %that $(Y_{1},Y_{2})^{\T}$ follows a bivariate Normal distribution
\[
\left(\begin{array}{c}
Y_{1}\\
Y_{2}
\end{array}\right)\sim\textup{N}\left(\left(\begin{array}{c}
\mu_{1}\\
\mu_{2}
\end{array}\right),\left(\begin{array}{cc}
\sigma_{1}^{2} & \rho\sigma_{1}\sigma_{2}\\
\rho\sigma_{1}\sigma_{2} & \sigma_{2}^{2}
\end{array}\right)\right),
\]
where $\rho$ is the correlation coefficient defined as
\[
\rho=\frac{\cov(Y_{1},Y_{2})}{\sqrt{\var(Y_{1})\var(Y_{2})}}.
\]
Then the conditional distribution is
\[
Y_{1}\mid Y_{2}=y_{2}\sim\textup{N}\left(\mu_{1}+\rho\frac{\sigma_{1}}{\sigma_{2}}(y_{2}-\mu_{2}),\sigma_{1}^{2}(1-\rho^{2})\right).
\]
\end{proposition}
%


\section{Quadratic forms of random vectors}

Given a random vector $Y$ and a symmetric matrix $A$, we can define
the quadratic form $Y^{\T}AY$, which is a random variable playing
an important role in statistics. The first theorem is about its mean.

\begin{theorem}\label{thm::mean-quadratic-form}
If $Y$ has mean $\mu$ and covariance $\Sigma$, then 
\[
E(Y^{\T}AY)=\textup{trace}(A\Sigma)+\mu^{\T}A\mu.
\]
\end{theorem}

\begin{myproof}{Theorem}{\ref{thm::mean-quadratic-form}}
The proof relies on the following two basic facts.
\begin{itemize}
\item $E(YY^{\T})=\cov(Y)+E(Y)E(Y^{\T})=\Sigma+\mu\mu^{\T}.$
\item For an $n\times n$ symmetric random matrix $W=(w_{ij})$, we have $E\left\{ \text{trace}(W)\right\} =\text{trace}\left\{ E(W)\right\} $ because $E\left(\sumn w_{ii}\right)=\sumn E(w_{ii}) .$
\end{itemize}

The conclusion follows from
\begin{align*}
E(Y^{\T}AY) & =E\left\{ \text{trace}(Y^{\T}AY)\right\} \\
 & =E\left\{ \text{trace}(AYY^{\T})\right\} \\
 & =\text{trace}\left\{ E(AYY^{\T})\right\} \\
 & =\text{trace}\left\{ AE(YY^{T})\right\} \\
 & =\text{trace}\left\{ A(\Sigma+\mu\mu^{\T})\right\} \\
 & =\text{trace}\left(A\Sigma+A\mu\mu^{T}\right)\\
 & =\text{trace}(A\Sigma)+\text{trace}(\mu^{\T}A\mu)\\
 & =\text{trace}(A\Sigma)+\mu^{\T}A\mu.
\end{align*}
\end{myproof}


The variance of the quadratic form is much more complicated for a general
random vector. For the multivariate Normal random vector, we have the
following formula.
\begin{theorem}
\label{thm:varianceofquadraticforms}If $Y\sim\textup{N}(\mu,\Sigma)$,
then
\[
\var(Y^{\T}AY)=2\textup{trace}(A\Sigma A\Sigma)+4\mu^{\T}A\Sigma A\mu.
\]
\end{theorem}

I relegate the proof as Problem \ref{hw00math2::variance-quadratic}. 


From its definition, $\chi_n^2$ is the summation of the squares of $n$ IID standard Normal random variables. It is closely related to
quadratic forms of multivariate Normals. 

\begin{theorem}\label{thm::normal-chisq}
\begin{enumerate}
\item If $Y\sim\textup{N}(\mu,\Sigma)$ is an $n$-dimensional random vector
with $\Sigma\succ 0$, then 
\[
(Y-\mu)^{\T}\Sigma^{-1}(Y-\mu)\sim\chi_{n}^{2}.
\]
If rank$(\Sigma) = k \leq n$, then 
$$
(Y-\mu)^{\T}\Sigma^{+}(Y-\mu)\sim\chi_{k}^{2}.
$$

\item If $Y\sim\textup{N}(0,I_{n})$ and $H$ is a projection matrix of rank
$K$, then 
\[
Y^{\T}HY\sim\chi_{K}^{2}.
\]
\item If $Y\sim\textup{N}(0,H)$ where $H$ is a projection matrix of rank
$K$, then
\[
Y^{\T}Y\sim\chi_{K}^{2}.
\]
\end{enumerate}
\end{theorem}

\begin{myproof}{Theorem}{\ref{thm::normal-chisq}}
\begin{enumerate}
\item 
I only prove the general result with rank$(\Sigma) = k \leq n$.
By definition, $Y=\mu+\Sigma^{1/2}Z$ where $Z$ is a standard Normal
random vector, then
\begin{eqnarray*}
(Y-\mu)^{\T}\Sigma^{+}(Y-\mu)
&=&  Z^{\T}\Sigma^{1/2}\Sigma^{+}\Sigma^{1/2}Z \\
&=&  \sum_{i=1}^k Z_i^2  \sim  \chi_k^{2}.
\end{eqnarray*}
\item Using the eigendecomposition of the projection matrix
\[
H=P\text{diag}\left\{ 1,\ldots,1,0,\ldots,0\right\} P^{\T}
\]
with $K$ $1$'s in the diagonal matrix, we have
\begin{align*}
Y^{\T}HY & =Y^{\T}P\text{diag}\left\{ 1,\ldots,1,0,\ldots,0\right\} P^{\T}Y\\
 & =Z^{\T}\text{diag}\left\{ 1,\ldots,1,0,\ldots,0\right\} Z,
\end{align*}
where $Z=(Z_{1},\ldots,Z_{n})^{\T}=P^{\T}Y\sim\text{N}(0,P^{\T}P)=\text{N}(0,I_{n})$
is a standard Normal random vector. So
\[
Y^{\T}HY=\sum_{i=1}^{K}Z_{i}^{2}\sim\chi_{K}^{2}.
\]
\item Writing $Y=H^{1/2}Z$ where $Z$ is a standard Normal random vector,
we have 
\[
Y^{\T}Y=Z^{\T}H^{1/2}H^{1/2}Z=Z^{\T}HZ\sim\chi_{K}^{2}
\]
using the second result. 
\end{enumerate}
\end{myproof} 

\section{Homework problems}

\paragraph{Beta-Gamma duality}\label{hwmath2::beta-gamma-dual}

Prove Theorem \ref{thm:beta-gamma-duality}.

Hint: Calculate the joint density of $(X+Y, X/(X+Y))$. 

\paragraph{Gamma and Beta moments}\label{hwmath2::beta-gamma-moments}

Prove Propositions \ref{thm:gammamoments}--\ref{thm:beta-moments-log}.



\paragraph{Maximums of Gumbels}\label{hwmath2::max-gumbel}

Prove Proposition \ref{prop::max-gumbel}. 




\paragraph{Independence and uncorrelatedness in the multivariate Normal}\label{hwmath2::independence-uncorr-normal}

Prove Theorem \ref{thm:Normal-ind-uncorr}. 

\paragraph{Transformation of bivariate Normal}


Prove that if $(Y_{1},Y_{2})^{\T}$ follows
a bivariate Normal distribution
\[
\left(\begin{array}{c}
Y_{1}\\
Y_{2}
\end{array}\right)\sim\textup{N}\left(\left(\begin{array}{c}
0\\
0
\end{array}\right),\left(\begin{array}{cc}
1 & \rho\\
\rho & 1
\end{array}\right)\right),
\]
then 
$$
Y_{1}+Y_{2}\ind Y_{1}-Y_{2}.
$$



Remark: This result holds for arbitrary $\rho$. 


\paragraph{Normal conditional distributions}\label{hwmath2::normal-conditionals}


Suppose that $(X_1, X_2)$ has the joint distribution
$$
f_{X_1X_2}\left(x_{1}, x_{2}\right) \propto C_0 \exp \left\{  -\frac{1}{2}\left( A x_{1}^{2} x_{2}^{2}+x_{1}^{2}+x_{2}^{2} -2 B x_{1} x_{2}-2 C_{1} x_{1}-2 C_{2} x_{2} \right) \right\},
$$
where $C_0$ is the normalizing constant depending on $(A,B,C_1, C_2)$. To ensure that this is a well-defined density, we need $A \geq 0$, and if $A=0$ then $|B| < 1$. Prove that the conditional distributions are
\begin{eqnarray*}
X_{1} \mid X_2 =  x_{2}  &\sim& \N\left(\frac{B x_{2}+C_{1}}{A x_{2}^{2}+1}, \frac{1}{A x_{2}^{2}+1}\right) , \\
X_{2} \mid X_1 =  x_{1}  &\sim& \N\left(\frac{B x_{1}+C_{2}}{A x_{1}^{2}+1}, \frac{1}{A x_{1}^{2}+1}\right).
\end{eqnarray*}


Remark: 
For a bivariate Normal distribution, the two conditional distributions are both Normal. The converse of the statement is not true. That is, even if the two conditional distributions are both Normal, the joint distribution may not be bivariate Normal. 
\citet{gelman1991note} reported this interesting result. 



\paragraph{Inverse of covariance matrix and conditional independence in multivariate Normal}\label{hwmath2::inverse-cov-conind-normal}


Assume $X = (X_1, \ldots, X_p)^{\T} \sim \N(\mu, \Sigma)$. Denote the inverse of its covariance matrix by $\Sigma^{-1}  = (\sigma^{jk})_{1\leq j, k\leq p}$. Show that for any pair of $j \neq k$, we have 
$$
\sigma^{jk} = 0 \Longleftrightarrow  X_j \ind X_k \mid X_{\backslash (j,k)}
$$
where $X_{\backslash (j,k)}$ contains all the variables except $X_j$ and $X_k$. 


\paragraph{Independence of linear and quadratic functions of the multivariate Normal}\label{hw00math2::independence-linear-quadratic}

Assume $Y\sim\text{N}(\mu,\sigma^{2}I_{n})$. For an $n$ dimensional
vector $a$ and two $n\times n$ symmetric matrices $A$ and $B$, show that
\begin{enumerate}
\item if $ Aa =0$, then $a^{\T}Y\ind Y^{\T}AY$;
\item if $AB=BA=0$, then $Y^{\T}AY\ind Y^{\T}BY$.
\end{enumerate}


Hint: To simplify the proof, you can the pseudoinverse of $A$ which satisfies $AA^{+}A=A$.
In fact, a strong result holds. \citet{ogasawara1951independence} proved the following theorem; see also \citet[][Theorem 5]{styan1970notes}.

\begin{theorem}
Assume $Y\sim\text{N}(\mu,\Sigma )$. Define quadratic forms $Y^{\T}AY$ and $ Y^{\T}BY$ for two symmetric matrices $A$ and $B$. The $Y^{\T}AY$ and $ Y^{\T}BY$ are independent if and only if 
$$
\Sigma A \Sigma B \Sigma = 0,\quad
\Sigma A \Sigma B \mu = \Sigma B \Sigma A \mu = 0,\quad
\mu^{\T} A \Sigma B \mu = 0.
$$
\end{theorem}







\paragraph{Independence of the sample mean and variance of IID Normals}\label{hw00math2::independence-mean-variance}

If $X_{1},\ldots,X_{n} \iidsim \N(\mu,\sigma^{2}),$ then $\bar{X}\ind S^{2}$,
where $\bar{X}=n^{-1}\sumn X_{i}$ and $S^{2}=(n-1)^{-1}\sumn(X_{i}-\bar{X})^{2}$.



Remark: A remarkable result due to \citet{geary1936distribution} ensures the reverse of the above result. That is, if $X_{1},\ldots,X_{n} $ are IID and $\bar{X}\ind S^{2}$, then $X_{1},\ldots,X_{n} $ must be Normals. See \citet{lukacs1942characterization} and \citet{benhamou2018three} for proofs. 



\paragraph{Variance of the quadratic form of the multivariate Normal}\label{hw00math2::variance-quadratic}

Prove Theorem \ref{thm:varianceofquadraticforms}. Use it to further prove that if $Y\sim\textup{N}(\mu,\Sigma)$, then
$$
\cov(  Y^{\T}A_1Y, Y^{\T}A_2Y)
= 2\textup{trace}(A_1\Sigma A_2\Sigma)+4\mu^{\T}A_1\Sigma A_2\mu . 
$$


Hint: 
Write $Y = \mu + \Sigma^{1/2} Z$ and reduce the problem to calculating the moments of standard Normals. 





 

