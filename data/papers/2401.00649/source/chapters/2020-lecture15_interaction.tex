 
\chapter{Interaction}\label{chapter::interaction}
 
 Interaction is an important notion in applied statistics. It measures the interplay of two or more variables acting simultaneously on an outcome. Epidemiologists find that cigarette smoking and alcohol consumption both increase the risks of many cancers. Then they want to measure how cigarette smoking and alcohol consumption jointly increase the risks. That is, does cigarette smoking increase the risks of cancers more in the presence of alcohol consumption than in the absence of it? Political scientists are interested in measuring the interplay of different get-out-to-vote interventions on voting behavior. This chapter will review many aspects of interaction in the context of linear regression. \citet{Cox:1984tx} and \citet{berrington2007interpretation} reviewed interaction from a statistical perspective. \citet{Vanderweele::2015} offers a textbook discussion on interaction with a focus on applications in epidemiology. 
 
 
 
  
\section{Two binary covariates interact} 

Let's start with the simplest yet nontrivial example with two binary covariates $x_{i1}, x_{i2} \in \{0, 1\}$. We can fit the OLS:
\begin{equation}
\label{eq::2X2-ols}
y_i=\hat{\beta}_{0}+\hat{\beta}_{1} x_{i1}+\hat{\beta}_{2} x_{i2}+\hat{\beta}_{12} x_{i1} x_{i2}+\hat{\varepsilon}_i. 
\end{equation}
We can express the coefficients in terms of the means of the outcomes within four combinations of the covariates. The following proposition
is an algebraic result.


\begin{proposition}\label{proposition::2X2-ols}
From \eqref{eq::2X2-ols}, we have
\begin{eqnarray*} 
\hat{\beta}_{0} &=& \bar{y}_{00}, \\
\hat{\beta}_{1} &=& \bar{y}_{10} - \bar{y}_{00}, \\
\hat{\beta}_{2} &=& \bar{y}_{01}-\bar{y}_{00}, \\
\hat{\beta}_{12} &=& (\bar{y}_{11}-\bar{y}_{10})-(\bar{y}_{01}-\bar{y}_{00}),   
\end{eqnarray*} 
where $\bar{y}_{f_{1}f_{2}}$ is the average value of the $y_{i}$'s
with $x_{i1} = f_{1}$ and $x_{i2} = f_{2}$. 
\end{proposition}

The proof of Proposition \ref{proposition::2X2-ols} is pure algebraic which is relegated to Problem \ref{hw15interaction::2X2ols}. 
The proposition generalizes to OLS with more than two binary covariates. See \citet{zhao2021regression} for more details. 


Practitioners also interpret the coefficient of the product term of two continuous variables as an interaction. The coefficient $\hat{\beta}_{12}$ equals the difference between $\bar{y}_{11}-\bar{y}_{10}$, the effect of $x_{i2}$ on $y_i$ holding $x_{i1}$ at level $1$, and $\bar{y}_{01}-\bar{y}_{00}$, the effect of $x_{i2}$ on $y_i$ holding $x_{i1}$ at level $0$. It also equals 
$$
\hat{\beta}_{12}  =  (\bar{y}_{11}-\bar{y}_{01})-(\bar{y}_{10}-\bar{y}_{00}),
$$
that is, the difference between $\bar{y}_{11}-\bar{y}_{01} $, the effect of $x_{i1}$ on $y_i$ holding $x_{i2}$ at level $1$, and $\bar{y}_{10}-\bar{y}_{00}$, the effect of $x_{i1}$ on $y_i$ holding $x_{i2}$ at level $0$. The formula shows the symmetry of $x_{i1} $ and $x_{i2}$ in defining interaction. 




\section{A binary covariate interacts with a general covariate}


\subsection{Treatment effect heterogeneity}


In many studies, we are interested in the effect of a binary treatment $z_i$ on an outcome $y_i$, adjusting for some background covariates $x_i$. The covariates can play many roles in this problem. They may affect the treatment, enter the outcome model, and modify the effect of the treatment on the outcome. We can formulate the problem in terms of linear regression: 
\begin{eqnarray}
\label{eq::ols-interaction}
y_i =  \beta_0 + \beta_1 z_i + \beta_2^{\T} x_i + \beta_3^{\T} x_iz_i  + \varepsilon_i ,
\end{eqnarray}
where $E( \varepsilon_i \mid z_i, x_i ) = 0$. So
$$
E(y_i \mid z_i = 1, x_i) =  \beta_0 + \beta_1 + (\beta_2 + \beta_3)^{\T} x_i
$$
and
$$
E(y_i \mid z_i = 0, x_i) =  \beta_0   + \beta_2  ^{\T} x_i,
$$
which implies that
$$
E(y_i \mid z_i = 1, x_i)  - E(y_i \mid z_i = 0, x_i)  = \beta_1 + \beta_3^{\T} x_i.
$$
The conditional average treatment effect is thus a linear function of the covariates. As long as $\beta_3 \neq 0$, we have treatment effect heterogeneity, which is also called {\it effect modification}. A statistical test for $\beta_3 = 0$ is straightforward based on OLS and EHW standard error. 

Note that \eqref{eq::ols-interaction} includes the interaction of the treatment and all covariates. With prior knowledge, we may believe that the treatment effect varies with respect to a subset of covariates, or, equivalently, we may set some components of $\beta_3$ to be zero. 


 
 
 \subsection{Johnson--Neyman technique}

\citet{johnson1936tests} proposed a technique to identify the region of covariates in which the conditional average treatment $\beta_1 + \beta_3^{\T} x$ is zero. 
For a given $x$, we can test the null hypothesis that $\beta_1 + \beta_3^{\T} x = 0$, which is a linear combination of the regression coefficients of \eqref{eq::ols-interaction}. If we fail to reject the null hypothesis, then this $x$ belongs to the region of zero conditional average effect. 
See \citet{rogosa1981relationship} for more discussions. 



\subsection{Blinder--Oaxaca decomposition}

The linear regression \eqref{eq::ols-interaction} also applies to descriptive statistics when $z_i$ is a binary indicator for subgroups. For example, $z_i$ can be a binary indicator for age, racial, or gender groups, $y_i$ can be the log wage, and $x_i$ can be a vector of explanatory variables such as education, experience, industry, and occupation. 
Sometimes, it is more insightful to write \eqref{eq::ols-interaction} in terms of two possibly non-parallel linear regressions:
\begin{eqnarray}
\label{eq::ols-control}
y_i = \gamma_0 + \theta_0^{\T} x_i + \varepsilon_{i}, \quad 
E( \varepsilon_{i} \mid z_i =0, x_i ) = 0
\end{eqnarray}
for the group with $z_i = 0$, and
\begin{eqnarray}
\label{eq::ols-treatment}
y_i = \gamma_1 + \theta_1^{\T} x_i + \varepsilon_{i}, \quad
E( \varepsilon_{i} \mid z_i = 1, x_i ) = 0
\end{eqnarray}
for the group with $z_i = 1$. Regressions \eqref{eq::ols-control} and \eqref{eq::ols-treatment} are just a reparametrization of \eqref{eq::ols-interaction} with
$$
\gamma_0 = \beta_0,\quad
\theta_0 = \beta_2,\\
\gamma_1 =\beta_0 + \beta_1,\quad
\theta_1 = \beta_2 + \beta_3 . 
$$

Based on \eqref{eq::ols-control} and \eqref{eq::ols-treatment}, we can decompose the difference in the outcome means as
\begin{eqnarray*}
&& E(y_i \mid z_i = 1) - E(y_i \mid z_i = 0) \\
&=&  \{ \gamma_1 + \theta_1^{\T} E(x_i \mid z_i = 1)  \} - \{ \gamma_0 + \theta_0^{\T} E(x_i \mid z_i = 0 )  \} \\
&=& \theta_0^{\T}  \{ E(x_i \mid z_i = 1)  -   E(x_i \mid z_i = 0 ) \}  \\
&&  + (\theta_1 - \theta_0) ^{\T}   E(x_i \mid z_i = 0 )    +  \gamma_1 - \gamma_0  \\
&&  + (\theta_1 - \theta_0) ^{\T} \{ E(x_i \mid z_i = 1)  -   E(x_i \mid z_i = 0 ) \} .
\end{eqnarray*}
The decomposition has three components: the first component 
\begin{eqnarray*}
\mathcal E 
&=& 
\theta_0^{\T}  \{ E(x_i \mid z_i = 1)  -   E(x_i \mid z_i = 0 ) \}  \\
&=& \beta_2^{\T}  \{ E(x_i \mid z_i = 1)  -   E(x_i \mid z_i = 0 ) \} 
\end{eqnarray*}
measures the {\it endowment effect} since it is due to the difference in the covariates; the second component
\begin{eqnarray*}
\mathcal C 
&=& 
(\theta_1 - \theta_0) ^{\T}   E(x_i \mid z_i = 0 )    +  \gamma_1 - \gamma_0  \\
&=& \beta_3^{\T}   E(x_i \mid z_i = 0 ) + \beta_1
\end{eqnarray*}
measures the difference in coefficients; the third component 
\begin{eqnarray*}
\mathcal I 
&=& 
(\theta_1 - \theta_0) ^{\T} \{ E(x_i \mid z_i = 1)  -   E(x_i \mid z_i = 0 ) \} \\
&=& \beta_3^{\T}  \{ E(x_i \mid z_i = 1)  -   E(x_i \mid z_i = 0 ) \}
\end{eqnarray*}
measures the interaction between the endowment and coefficients. This is called the Blinder--Oaxaca decomposition. \citet{jann2008blinder} reviews other forms of the decomposition, extending the original forms in \citet{blinder1973wage} and \citet{oaxaca1973male}. 

Estimation and testing for $\mathcal E, \mathcal C$, and $\mathcal I$ are straightforward. Based on the OLS of \eqref{eq::ols-interaction} and the sample means $\bar{x}_1$ and $\bar{x}_0$ of the covariates, we have point estimators
\begin{eqnarray*}
\hat{\mathcal E} &=& \hat \beta_2^{\T} (\bar{x}_1 - \bar{x}_0),\\ 
\hat{\mathcal C} &=& \hat \beta_3^{\T}  \bar{x}_0 +  \hat \beta_1,\\ 
\hat{\mathcal I} &=& \hat \beta_3^{\T} (\bar{x}_1 - \bar{x}_0).
\end{eqnarray*}
Given the covariates, they are just linear transformations of the OLS coefficients. Statistical inference is thus straightforward. 


\subsection{Chow test}


\citet{chow1960tests} proposed to test whether the two regressions \eqref{eq::ols-control} and \eqref{eq::ols-treatment} are identical. Under the null hypothesis that $\gamma_0 = \gamma_1$ and $\theta_0 = \theta_1$, he proposed an $F$ test assuming homoskedasticity, which is called the {\it Chow test} in econometrics. In fact, this is just a special case of the standard $F$ test for the null hypothesis that $\beta_1=0$ and $\beta_3 = 0$ in \eqref{eq::ols-interaction}. Moreover, based on the OLS in \eqref{eq::ols-interaction}, we can also derive the robust test based on the EHW covariance estimator. \citet{chow1960tests} discussed a subtle case in which one group has a small size rending the OLS fit underdetermined. I relegate the details to Problem \ref{hw15interaction::chow-small}. Note that under this null hypothesis, $\mathcal C = \mathcal I  = 0,$ so the difference in the outcome means is purely due to the difference in the covariate means. 







 



 
 





\section{Difficulties of intereaction}



\subsection{Removable interaction}


The significance of the interaction term differs with $y$ and $\log(y)$. 

\begin{lstlisting}
> n  = 1000
> x1 = rnorm(n)
> x2 = rnorm(n)
> y  = exp(x1 + x2 + rnorm(n))
> ols.fit = lm(log(y) ~ x1*x2)
> summary(ols.fit)

Call:
lm(formula = log(y) ~ x1 * x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7373 -0.6822 -0.0111  0.7084  3.1039 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.003214   0.031286   0.103    0.918    
x1           1.056801   0.030649  34.480   <2e-16 ***
x2           1.009404   0.030778  32.797   <2e-16 ***
x1:x2       -0.017528   0.030526  -0.574    0.566    

> ols.fit = lm(y ~ x1*x2)
> summary(ols.fit)

Call:
lm(formula = y ~ x1 * x2)

Residuals:
   Min     1Q Median     3Q    Max 
-35.95  -5.17  -0.97   2.34 513.35 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   5.2842     0.6686   7.903 7.17e-15 ***
x1            6.7565     0.6550  10.315  < 2e-16 ***
x2            4.9548     0.6577   7.533 1.11e-13 ***
x1:x2         7.3810     0.6524  11.314  < 2e-16 ***
\end{lstlisting}


\subsection{Main effect in the presence of interaction}\label{sec::main-interaction-centering}

In the OLS fit below, we observe significant main effects.
\begin{lstlisting}
> ## data from "https://stats.idre.ucla.edu/stat/data/hsbdemo.dta"
> hsbdemo = read.table("hsbdemo.txt")
> ols.fit = lm(read ~ math + socst, data = hsbdemo)
> summary(ols.fit)

Call:
lm(formula = read ~ math + socst, data = hsbdemo)

Residuals:
     Min       1Q   Median       3Q      Max 
-18.8729  -4.8987  -0.6286   5.2380  23.6993 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.14654    3.04066   2.350   0.0197 *  
math         0.50384    0.06337   7.951 1.41e-13 ***
socst        0.35414    0.05530   6.404 1.08e-09 ***
\end{lstlisting}

Then we add the interaction term into the OLS, and suddenly we have
significant interaction but not significant main effects. 
\begin{lstlisting}
> ols.fit = lm(read ~ math*socst, data = hsbdemo)
> summary(ols.fit)

Call:
lm(formula = read ~ math * socst, data = hsbdemo)

Residuals:
     Min       1Q   Median       3Q      Max 
-18.6071  -4.9228  -0.7195   4.5912  21.8592 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)   
(Intercept) 37.842715  14.545210   2.602  0.00998 **
math        -0.110512   0.291634  -0.379  0.70514   
socst       -0.220044   0.271754  -0.810  0.41908   
math:socst   0.011281   0.005229   2.157  0.03221 * 
\end{lstlisting}

However, if we center the covariates, the main effects are significant again. 
\begin{lstlisting}
> hsbdemo$math.c = hsbdemo$math - mean(hsbdemo$math)
> hsbdemo$socst.c = hsbdemo$socst - mean(hsbdemo$socst)
> ols.fit = lm(read ~ math.c*socst.c, data = hsbdemo)
> summary(ols.fit)

Call:
lm(formula = read ~ math.c * socst.c, data = hsbdemo)

Residuals:
     Min       1Q   Median       3Q      Max 
-18.6071  -4.9228  -0.7195   4.5912  21.8592 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)    51.615327   0.568685  90.763  < 2e-16 ***
math.c          0.480654   0.063701   7.545 1.65e-12 ***
socst.c         0.373829   0.055546   6.730 1.82e-10 ***
math.c:socst.c  0.011281   0.005229   2.157   0.0322 *  
\end{lstlisting}


Based on the linear model with interaction
$$
E(y_i\mid x_{i1}, x_{i2}) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_{12} x_{i1} x_{i2},
$$
better definitions of the main effects are
$$
n^{-1} \sumn \frac{  \partial E(y_i\mid x_{i1}, x_{i2}) }{ \partial x_{i1} } = n^{-1} \sumn ( \beta_1 +  \beta_{12}   x_{i2}  )
= \beta_1 +  \beta_{12} \bar{x}_2
$$
and
$$
n^{-1} \sumn \frac{  \partial E(y_i\mid x_{i1}, x_{i2}) }{ \partial x_{i2} } = n^{-1} \sumn ( \beta_2 +  \beta_{12}   x_{i1}  )
= \beta_2 +  \beta_{12} \bar{x}_1 ,
$$
which are called the {\it average partial or marginal effects}. 
So when the covariates are centered, we can interpret $\beta_1$ and $\beta_2$ as the main effects. 
In contrast, the interpretation of the interaction term does not depend on the centering of the covariates because
$$
\frac{  \partial^2 E(y_i\mid x_{i1}, x_{i2}) }{ \partial x_{i1} \partial x_{i2}}  = \beta_{12}.
$$

The \ri{R} code in this section is in \ri{code17.3.R}. 


\subsection{Power}

Usually, statistical tests for interaction do not have enough power. Proposition \ref{proposition::2X2-ols} provides a simple explanation. The variance of the interaction equals
$$
\var(\hat{\beta}_{12}) = \frac{\sigma_{11}^2}{n_{11}} + \frac{\sigma_{10}^2}{n_{10}}  +\frac{\sigma_{01}^2}{n_{01}}  +\frac{\sigma_{00}^2}{n_{00}} ,
$$
where $\sigma^2_{f_1f_2} = \var(y_i \mid x_{i1}   = f_1, x_{i2} = f_2)$. 
Therefore, its variance is driven by the smallest value of $n_{11}, n_{10}, n_{01}, n_{00}$. Even when the total sample size is large, one of the subgroup sample sizes can be small, resulting in a large variance of the estimator of the interaction. 



\section{Homework problems}

\paragraph{Interaction and difference-in-difference}\label{hw15interaction::2X2ols}

Prove Proposition \ref{proposition::2X2-ols}. Moreover, simplify the HC0 and HC2 versions of the EHW standard errors of the coefficients in terms of $n_{f_1 f_2}$ and $\hat \sigma^2_{f_1f_2} ,$ where $n_{f_1 f_2}$ is the sample size and $\hat \sigma^2_{f_1f_2}$ is the sample variance of the outcomes for units with $x_{i1} = f_1$ and $x_{i2} = f_2$. 


Hint: You can prove the proposition by inverting the $4\times 4$ matrix $X^{\T} X$. However, this method is a little too tedious. Moreover, this proof does not generalize to OLS with $K > 2$ binary covariates. So it is better to find alternative proofs.
For the EHW standard errors, you can use the result in Problems \ref{hw8::anova-ols-hc02} and \ref{hw08::invariance-ehw01234}. 



\paragraph{Two OLS}\label{hw15interaction::2ols}
Given data $(x_i, z_i, y_i)_{i=1}^n$ where $x_i$ denotes the covariates, $z_i$ denotes the binary group indicator, and $y_i$ denotes the outcome. We can fit two separate OLS:
$$
\hat{y}_i =  \hat\gamma_1 + x_i^{\T} \hat \beta_1
$$
and
$$
\hat{y}_i =   \hat   \gamma_0 + x_i^{\T} \hat \beta_0
$$
with  data in group $1$ and group $0$, respectively. We can also fit a joint OLS using the pooled data:
$$
\hat{y}_i =   \hat \alpha_0 + \hat\alpha_z z_i + x_i^{\T} \hat\alpha_x  +  z_i x_i^{\T} \hat\alpha_{zx}.
$$

\begin{enumerate}
\item
Find $(\hat{\alpha}_0, \hat{\alpha}_z, \hat{\alpha}_x, \hat{\alpha}_{zx})$ in terms of $ (  \hat{\gamma}_1, \hat{\beta}_1, \hat{\gamma}_0, \hat{\beta}_0 )$.
\item
Show that the fitted values $\hat y_i$'s are the same from the separate and the pooled OLS for all units $i=1, \ldots, n$. 
\item
Show that the leverage scores $h_{ii}$'s are the same from the separate and the pooled OLS. 
\end{enumerate}
 




\paragraph{Chow test when one group size is too small}\label{hw15interaction::chow-small}

Assume \eqref{eq::ols-control} and \eqref{eq::ols-treatment} with homoskedastic Normal error terms. 
Let $n_1$ and $n_0$ denote the sample sizes of groups with $z_i = 1$ and $z_i = 0$. Consider the case with $n_0$ larger than the number of covariates but $n_1$ smaller than the number of covariates. So we can fit OLS and estimate the variance based on \eqref{eq::ols-control}, but we cannot do so based on \eqref{eq::ols-treatment}. The statistical test discussed in the main paper does not apply. \citet{chow1960tests} proposed the following test based on prediction.


Let $\hat{\gamma}_0$ and $\hat{\theta}_0$ be the coefficients, and $\sigma^2_0$ be the variance estimate based on OLS with units $z_i = 0$. Under the null hypothesis that   $\gamma_0 = \gamma_1$ and $\theta_0 = \theta_1$,  predict the outcomes of the units $z_i=1$:
$$
\hat{y}_i = \hat{\gamma}_0 + \hat{\theta}_0^{\T} x_i 
$$
with the prediction error
$$
d_i = y_i - \hat{y}_i
$$
following a multivariate Normal distribution. Propose an $F$ test based on $d_i$ with $z_i = 1$. 


Hint: It is more convenient to use the matrix form of OLS. 


\paragraph{Invariance of the interaction}\label{hw15interaction::2X2-EHW}

In Section \ref{sec::main-interaction-centering}, the point estimate and standard error of the coefficient of the interaction term remain the same no matter whether we center the covariates or not. This result holds in general. This problem quantifies this phenomenon.


With scalars $x_{i1}, x_{i2}, y_i$ $(i=1,\ldots, n)$, we can fit the OLS
$$
y_i = \hat{\beta}_0  + \hat{\beta}_1 x_{i1}  +  \hat{\beta}_2 x_{i2} + \hat{\beta}_{12} x_{i1}  x_{i2}  + \hat{\varepsilon}_i.
$$
Under any location transformations of the covariates $  x_{i1}' =x_{i1}  -c_1 , x_{i2} ' = x_{i2} - c_2$, we can fit the OLS
$$
y_i = \tilde{\beta}_0  + \tilde{\beta}_1 x_{i1}'  +  \tilde{\beta}_2 x_{i2}' + \tilde{\beta}_{12} x_{i1} ' x_{i2}'  + \tilde{\varepsilon}_i.
$$

\begin{enumerate}
\item
Express $\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_{12}$ in terms of $\tilde{\beta}_0 , \tilde{\beta}_1, \tilde{\beta}_2, \tilde{\beta}_{12}$. Verify that $\hat{\beta}_{12} = \tilde{\beta}_{12} .$

\item
Show that the EHW standard errors for $\hat{\beta}_{12}$ and $\tilde{\beta}_{12} $ are identical.
\end{enumerate}

 Hint: Use the results in Problems \ref{hw3::invariance-ols} and \ref{hw08::invariance-ehw01234}. 
 
 
 
 
