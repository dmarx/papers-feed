\section{Discussion}
% \subsection{Concurrent Work}
% In recent years it has become more popular to release ``uncensored'' language models, and with that there have been tools and techniques developed to detect undesirable outputs produced by these models. One such tool is Llama Guard \cite{inan2023llama}, a large language model designed to perform both prompt- and response-classification to maximise the safety of chat systems. Like DPH, Llama Guard can be used to detect and reject harmful outputs produced by a language model. However, unlike DPH, Llama Guard is an entirely separate model which must be run concurrently with chat model and therefor increases the compute and memory requirements of the combined system. % this especially contrasts with DPH being especially suited for small language models which inherently require a much smaller compute budget.

\subsection{Future Work}
As shown in the results section, DPH is capable of learning to assign higher rewards to preferred outputs and lower rewards to dispreferred outputs which implies the pooling function learns rich features with respect to prompt-completion pairs. We believe that it would be possible to also extract additional information from the output of the pooling function to detect finer grained signals such as helpfulness, humor, creativity, toxic content, etc. This can be achieved by training on a conversational dataset such Open Assistant \cite{k√∂pf2023openassistant} which contains a variety of human-curated labels in addition to machine-generated labels produced by Detoxify \cite{Detoxify}.

% It is also worth examining how the performance of DPH scales with larger language models, and if it can be used as a post-hoc technique to provide safety guardrails for uncensored language models such as Mistral 7B and observe how it performs compared to systems such as Llama Guard while maintaining much lower compute requirements.

\subsection{Limitations}
The main benefit of DPH being its ability to perform alignment without directly effecting the model's output distribution is also its main limitation: unlike other alignment techniques which can help prevent the model generating harmful outputs, DPH is only capable of \textit{detecting} harmful outputs. Although we do include DPO alignment in our experiments to reduce the likelihood of harmful outputs, DPH does not require such model alignment to function, which shifts the responsibility of rejecting harmful outputs to the end user or service provider.

\subsection{Conclusion}
In this paper we introduced Direct Preference Heads, a novel form of language model alignment which is performed at inference time to prune candidate completions for a given prompt. Unlike other alignment techniques which coerce the model into generating human preference aligned outputs, DPH instead produces reward scores for candidate outputs without affecting the actual generation process and therefor avoids the issue of RLHF leading to degraded performance when applied to smaller language models. We formulated two loss functions for DPH and find strong connections to Conservative DPO, implying that DPH is robust to label noise and can be tuned to a specific confidence margin. Finally, we evaluated our methods on a number of NLU, commonsense reasoning and reading Comprehension tasks and found that DPH is able to consistently outperform both our SFT baseline and multiple publicly available language model checkpoints of varying size and training volume.

\subsection*{Broader Impacts}
As with all language modeling systems we cannot guarantee all responses produced by our models are factually correct nor can we guarantee that they are safe and free from harmful content. Our work focuses on creating a system that helps filter out incorrect and harmful messages by scoring candidate outputs, but as with all alignment techniques our models may be susceptible to so-called `jailbreaks' which can coerce the model into incorrectly assigning a higher score to less desirable content. To maximise safety DPH should be implemented alongside other safety guardrails such as Llama Guard \cite{inan2023llama} when used for publicly facing chat systems, and we intend for our provided model checkpoints to be used for reproduction of results and further research in the field of alignment.