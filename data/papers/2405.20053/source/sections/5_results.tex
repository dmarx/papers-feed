\section{Results}
\subsection{Evaluation Methodology}
As described in Section~\ref{sec:methodology} we use NLU, commonsense reasoning and reading comprehension tasks to measure model capabilities, while the instruction following and auxiliary tasks are used to provide additional training signals. For the NLU tasks we evaluate on the test set of GLUE, providing average scores both with and without WNLI. For reading comprehension we evaluate on the RACE test set. For commonsense reasoning we follow the LM Evaluation Harness \cite{eval-harness} implementations of these tasks, evaluating on the test sets of ARC and OpenBookQA and the validation sets of HellaSwag, WinoGrande, BoolQ and PIQA, which brings our evaluations in line with other models.

For vocab extension and SFT checkpoints we obtain model predictions from the completions with the highest scoring log-probabilities. For the DPH checkpoints we report metrics for both log-probability predictions (Ours\textsubscript{DPO}) and predictions chosen from the DPH rewards (Ours\textsubscript{DPH}). We use the SwiGLU-based pooler with the separable objective function for all our experiments as we found this combination to perform best overall as shown in Section~\ref{sec:head-objective-ablations}.

%To obtain a baseline of the performance of the pre-trained model we run the vocab extension checkpoint through the evaluation suite. We use the vocab extension checkpoint instead of the pre-trained checkpoint so we can use identical prompts for all stage of evaluation, which requires the models to `understand' the \texttt{<|im\_end|>} and \texttt{<|im\_end|>} tokens.

\subsubsection{Natural Language Understanding}
Our results for NLU performance are included in Table~\ref{tab:glue-table}. Note that the results for GPT-1 \cite{Radford2018ImprovingLU} and BERT \cite{devlin2019bert} are from sub-task specific fine-tunes.

\begingroup
\vspace{-12pt}
\setlength{\tabcolsep}{3pt}
\setlength{\extrarowheight}{3pt}
\begin{table}[ht]
\caption{Comparison of GLUE performance. Dashes represent unpublished results. Note that the Spearman correlation for Ours\textsubscript{Vocab} is misleading and caused by predicting ``0'' for all test samples.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lrr|ccccccccc|cc}
\hline
\textbf{System} &
\textbf{Tokens} &
\textbf{Params} &
\begin{tabular}[c]{@{}c@{}}\textbf{MNLI}\\ \textsuperscript{\textbf{m/mm}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{QQP}\\ \textsuperscript{\textbf{F1/Acc}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{QNLI}\\ \textsuperscript{\textbf{Acc}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{SST-2}\\ \textsuperscript{\textbf{Acc}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{CoLA}\\ \textsuperscript{\textbf{M Corr}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{STS-B}\\ \textsuperscript{\textbf{P/S Corr}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{MRPC}\\ \textsuperscript{\textbf{F1/Acc}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{RTE}\\ \textsuperscript{\textbf{Acc}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{Score}\\ \textsuperscript{\textbf{w/o WNLI}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{WNLI}\\ \textsuperscript{\textbf{Acc}}\end{tabular} &
\begin{tabular}[c]{@{}c@{}}\textbf{Score}\\ \textsuperscript{\textbf{w/ WNLI}}\end{tabular} \\
\hline
Ours\textsubscript{Vocab} & 100B & 551M
& 34.1/34.7 & 28.2/42.9 & 50.2 & 58.0 & 0.9 & -0.9/99.2 & 69.4/57.4 & 50.9 & 42.8 & 34.9 & 41.9 \\

Ours\textsubscript{SFT} & 100B & 551M
& 73.6/75.0 & 59.1/82.8 & 81.4 & 90.8 & 22.7 & 80.6/92.4 & 80.6/75.2 & 71.4 & 72.0 & 38.4 & 68.2 \\

Ours\textsubscript{DPO} & 100B & 551M
& 78.8/80.2 & 65.6/85.6 & 87.0 & 93.3 & 36.5 & 83.7/94.4 & 83.9/79.1 & 73.9 & 77.0 & 37.7 & 72.7 \\

Ours\textsubscript{DPH} & 100B & +19M
& 80.0/80.6 & 65.8/85.3 & 87.5 & 94.0 & 43.8 & \textbf{85.3/93.0} & 85.5/80.2 & \textbf{75.3} & 78.6 & 46.6 & 75.0 \\
\hline
GPT-1 & 32B & 117M
& 82.1/81.4 & 70.3/  -  & 87.4 & 91.3 & 45.4 & 82.0/80.0 & 82.3/  -  & 56.0 & -    & -    & 72.8 \\

BERT\textsubscript{Base} & 128B & 110M
& 84.6/83.4 & 71.2/  -  & 90.5 & 93.5 & 52.1 & -  /85.8  & 88.9/  -  & 66.4 & -    & -    & 78.3 \\

BERT\textsubscript{Large} & 128B & 340M
& \textbf{86.7/85.9} & \textbf{72.1/89.3} & \textbf{92.7 }& \textbf{94.9} & \textbf{60.5} & 87.6/86.5 & \textbf{89.3/85.4} & 70.1 & \textbf{82.5} & \textbf{65.1} & \textbf{80.5} \\
\hline
\end{tabular}}
\label{tab:glue-table}
\vspace*{-0.5\baselineskip}
\end{table}
\endgroup

It is unsurprising that our model does not outperform BERT\textsubscript{Large} even though it has more parameters; this is likely due to BERT's task specific fine-tunes in comparison to our model which was jointly trained on several tasks. Despite this our instruction following DPH model achieves a 2.2\% higher average GLUE score compared to task-specific GPT-1 fine-tunes and manages to attain the highest overall accuracy and macro-average on RTE and STS-B respectively.

%\todo{An interesting observation is that our DPH model achieves an 8.2\% higher accuracy on WNLI compared to the SFT checkpoint while the score is 0.7\% lower for DPO, creating an 8.9\% gap between the two alignment methods for a task which was not included in the training data.}

\subsubsection{Commonsense Reasoning}
Our results for commonsense reasoning are summarized in Table~\ref{tab:gpt4all-table}. Note the Pythia \cite{biderman2023pythia} and TinyLlama \cite{zhang2024tinyllama} models were not fine-tuned for any specific task but received significantly more pre-training and have much higher parameter counts.

\begingroup
\vspace{-12pt}
\setlength{\tabcolsep}{3pt}
\setlength{\extrarowheight}{3pt}
\begin{table}[ht]
\caption{Comparison of accuracy on the GPT4All test suite.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lrr|ccccccc|c}
\hline
\textbf{System} &
\textbf{Tokens} &
\textbf{Params} &
\textbf{HellaSwag} &
\textbf{OpenBookQA} &
\textbf{WinoGrande} &
\textbf{ARC-Challenge} &
\textbf{ARC-Easy} &
\textbf{BoolQ} &
\textbf{PIQA} &
\textbf{Average} \\
\hline
Ours\textsubscript{Vocab} & 100B & 551M & 36.93 & 28.60 & 51.14 & 26.19 & 25.67 & 61.25 & 65.39 & 42.17 \\
Ours\textsubscript{SFT} & 100B & 551M & 42.59 & 45.20 & 55.01 & 35.84 & 47.01 & 76.24 & 69.37 & 53.04 \\
Ours\textsubscript{DPO} & 100B & 551M & 44.83 & 52.40 & 57.38 & 39.76 & 53.54 & \textbf{79.08} & 72.36 & 57.05 \\
Ours\textsubscript{DPH} & 100B & +19M & \textbf{59.36} & \textbf{57.40} & \textbf{59.12} & \textbf{41.21} & \textbf{56.82} & 78.81 & 68.77 & \textbf{60.21} \\
\hline
Pythia-1.0B & 300B & 1.1B & 47.16 & 31.40 & 53.43 & 27.05 & 48.99 & 60.83 & 69.21 & 48.30 \\
Pythia-1.4B & 300B & 1.5B & 52.01 & 33.20 & 57.38 & 28.50 & 54.00 & 63.27 & 70.95 & 51.33 \\
% TinyLlama & 103B & 1.1B & 43.50 & 29.80 & 53.28 & 24.32 & 44.91 & 59.66 & 67.30 & 46.11 \\
TinyLlama & 3T & 1.1B   & 59.20 & 36.00 & \textbf{59.12} & 30.12 & 55.25 & 57.83 & \textbf{73.29} & 52.99 \\
\hline
\end{tabular}}
\label{tab:gpt4all-table}
\vspace*{-0.5\baselineskip}
% \vspace{-6pt}
\end{table}
\endgroup

With SFT alone we are able to attain comparable performance to TinyLlama using half as many parameters, and when applying DPH alignment we achieve a 7.2\% increase over the TinyLlama average score and the highest accuracy in 5 of the 7 tasks. %\todo{Interestingly, the PIQA score increases for DPO but decreases for DPH when compared to the SFT baseline which suggests that the pooling function and reward head may benefit from further fine-tuning.}

\subsubsection{Reading Comprehension}
Our results for reading comprehension are included in Table~\ref{tab:race-table}. The results for GPT-1 were taken from a RACE specific fine-tune, and the results for LLaMA \cite{touvron2023llama} were zero-shot without fine-tuning.

\begingroup
\vspace{-12pt}
\setlength{\tabcolsep}{9pt}
\setlength{\extrarowheight}{3pt}
\begin{table}[ht]
\centering
\caption{Comparison of accuracy on the RACE test set.}
\resizebox{0.75\linewidth}{!}{%
\begin{tabular}{lrr|cc|c}
\hline
\textbf{System} &
\textbf{Tokens} &
\textbf{Params} &
\textbf{RACE-middle} &
\textbf{RACE-high} &
\textbf{Weighted Average} \\
\hline
Ours\textsubscript{Vocab} & 100B & 551M & 26.0 & 24.6 & 25.0 \\
Ours\textsubscript{SFT}   & 100B & 551M & 56.1 & 52.9 & 53.8 \\
Ours\textsubscript{DPO}   & 100B & 551M & 65.9 & 59.8 & 61.6 \\
Ours\textsubscript{DPH}   & 100B & +19M & \textbf{66.9} & \textbf{60.6} & \textbf{62.5} \\
\hline
GPT-1     & 32B & 117M & 62.9 & 57.4 & 59.0 \\
LLaMA 7B  & 1T  & 6.7B & 61.1 & 46.9 & 51.0 \\
LLaMA 13B & 1T  & 13B  & 61.6 & 47.2 & 51.4 \\
\hline
\end{tabular}}
\label{tab:race-table}
\vspace*{-0.5\baselineskip}
% \vspace{-6pt}
\end{table}
\endgroup

Our SFT baseline achieves a higher average accuracy on RACE compared with the non fine-tuned LLaMa models but cannot match the accuracy of the RACE specific GPT-1 fine-tune; however after alignment our model attains a 3.5\% higher average over GPT-1 while still maintaining excellent scores on other tasks using the same model weights. %, \todo{and a 0.9\% improvement over DPO.}

\subsection{Ablations}
%\todo{Unless otherwise stated, we make use of the validation set scores rather than test scores for GLUE in our ablations due to rate limiting imposed by the evaluation server.}

\subsubsection{Pooling Head Function and Objective Choice} \label{sec:head-objective-ablations}
We ablate over the three pooling head and two objective function choices. We perform alignment for 7680 steps and report the validation scores in Table~\ref{tab:pooling-object}.

\begingroup
\vspace{-12pt}
\setlength{\tabcolsep}{9pt}
\setlength{\extrarowheight}{3pt}
\begin{table}[ht]
\centering
\caption{Comparison of DPH validation scores for different objective and pooler combinations.}
\resizebox{1\linewidth}{!}{%
\begin{tabular}{c|lc|ccc|ccc}
\hline
\textbf{Objective} & \textbf{Pooling Function} & \textbf{Add. Params} & \textbf{GLUE} & \textbf{GPT4All} & \textbf{RACE} & \textbf{HellaSwag} & \textbf{WinoGrande} & \textbf{PIQA} \\
\hline
Separable   & Identity   & 1536 & 75.06 & 56.86 & 56.54 & 46.63 & 53.20 & 65.29 \\
Separable   & BERT Style & 2.4M & 75.13 & 55.86 & 56.62 & 45.84 & 52.17 & 64.69 \\
Separable   & SwiGLU FFN & 19M  & \textbf{75.19} & 57.14 & \textbf{57.60} & 48.72 & 53.35 & 64.96 \\
\hline
Contrastive & Identity   & 1536 & 74.99 & 57.66 & 54.09 & 50.93 & 53.83 & 66.87 \\
Contrastive & BERT Style & 2.4M & 73.91 & 57.07 & 55.89 & 49.98 & 54.62 & 67.30 \\
Contrastive & SwiGLU FFN & 19M  & 74.04 & \textbf{58.28} & 55.95 & \textbf{51.38} & \textbf{55.80} & \textbf{67.57} \\
\hline
\end{tabular}}
\label{tab:pooling-object}
\vspace*{-0.5\baselineskip}
% \vspace{-3pt}
\end{table}
\endgroup

For both separable and contrastive objectives the SwiGLU pooler performs best on the three benchmarks, and for both GLUE and RACE the separable objective performs best overall. However during these experiments we discovered that contrastive DPH was achieving higher scores than separable DPH for specifically the sentence completion style tasks like HellaSwag, WinoGrande and PIQA. We hypothesise this is caused by situations where multiple completions to a given prompt may be plausible even though there is only one `gold' answer, and as such the model benefits from maximising the relative reward margin with the contrastive objective rather than optimising absolute rewards with the separable objective. 

\subsubsection{Task Specific Heads} \label{sec:task-specific}
By taking the DPH checkpoint and freezing all backbone parameters it is possible to learn task specific heads and pooling functions for different downstream tasks at the cost of only 19M parameters per task. We train new heads for the three task groups and plot the confusion matrix of each head for each task average in Table~\ref{tab:dph-multi-head}. We further fine-tune for an additional 7680 steps on each task group using the same training setup as DPH alignment.

\begingroup
\vspace{-9pt}
\setlength{\tabcolsep}{12pt}
\setlength{\extrarowheight}{3pt}
\begin{table}[ht]
\centering
\caption{Confusion matrix comparing validation scores for alternate heads.}
\resizebox{0.75\linewidth}{!}{%
\begin{tabular}{l|cccc}
\hline
\multicolumn{1}{l|}{\textbf{Benchmark}} & \textbf{Baseline Head} & \textbf{GLUE Head} & \textbf{GPT4All Head} & \textbf{RACE Head} \\
\hline
GLUE         & 76.12 & \textbf{76.36} & 76.20 & 76.13 \\
GPT4All      & 60.19 & 60.13 & \textbf{60.29} & 60.24 \\
RACE         & 64.17 & 64.05 & \textbf{64.48} & 64.43 \\
\hline
\end{tabular}}
\label{tab:dph-multi-head}
\vspace*{-0.5\baselineskip}
% \vspace{-3pt}
\end{table}
\endgroup

Unsurprisingly the GLUE and GPT4All heads achieve the highest scores for GLUE and GPT4All benchmarks respectively, however the GPT4All head manages to outperform the RACE head on the RACE benchmark. We hypothesise this may be due to the inclusion of muliple choice QA and reading comprehension tasks in GPT4All which may prove better training signals than the RACE training data alone.

\subsubsection{Model Ablations}
Our final experiments involve exploring the behaviour of DPH when applied to frozen language models in an ad-hoc fashion. We experiment using the Qwen 1.5 model family \cite{qwen} and train only the pooler and reward head weights, reporting results in Table~\ref{tab:qwen-table}. We use an identical training setup to DPH alignment but disable dropout due to the low number of trainable parameters.

Because the model backbone and embeddings remain frozen during alignment the `Log' scores represent the model's pre-trained (or fine-tuned) capabilities. When observing the difference between the Log scores of the 0.5B Qwen models it is evident that the fine-tuning and alignment used to transform the pre-trained model into the ``chat'' model resulted in degraded performance across the 3 tasks. This phenomenon is less apparent for the 1.8B models, and actually results in higher GLUE scores for the ``chat'' variant of the model. This further confirms the hypothesis that alignment can harm the reasoning capabilities of smaller language models.

\begingroup
\vspace{-6pt}
\setlength{\tabcolsep}{9pt}
\setlength{\extrarowheight}{3pt}
\begin{table}[ht]
\centering
\caption{Comparison of validation scores calculated using the log probabilities from the vanilla model checkpoints and reward scores produced by the trained Direct Preference Heads.}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{System} &
\textbf{GLUE Log} &
\textbf{GPT4All Log} &
\textbf{RACE Log} &
\textbf{GLUE DPH} &
\textbf{GPT4All DPH} &
\textbf{RACE DPH} \\
\hline
Qwen1.5-0.5B      & 41.94 & 53.11 & 51.38 & 45.69 & 48.52 & 41.21 \\
Qwen1.5-0.5B-Chat & 39.82 & 49.70 & 50.32 & 48.99 & 49.72 & 46.90 \\
\hline
Qwen1.5-1.8B      & 47.03 & 62.53 & 68.14 & 59.18 & 51.61 & 46.56 \\
Qwen1.5-1.8B-Chat & 53.85 & 61.69 & 67.47 & 62.38 & 54.47 & 53.33 \\
\hline
\end{tabular}}
\label{tab:qwen-table}
\vspace*{-.5\baselineskip}
% \vspace{-6pt}
\end{table}
\endgroup

For all models DPH is consistently able to attain higher scores on the GLUE tasks compared to the log probabilities produced by the language modelling head, but the opposite is observed for RACE which suggests the hidden states produced by the frozen backbone do not contain rich enough features for long range modelling tasks such as reading comprehension. We also observe the ``chat'' variants produce higher task scores for DPH than the non-chat variants which we hypothesise is a result of the authors' fine-tuning with the Chat-ML format which lead to the models' greater understanding of message structure and therefor improved hidden state aggregation for the final end message token.

When we combine these findings with those presented in Section~\ref{sec:task-specific}, it becomes evident that the pooling function and reward head exhibit slower convergence when the model backbone is frozen. This observation further supports our hypothesis in Section~\ref{sec:regularization}, indicating that the hidden states generated by the models are are initially sub-optimal and that further fine-tuning is necessary to optimize these hidden states to achieve the best features for DPH.