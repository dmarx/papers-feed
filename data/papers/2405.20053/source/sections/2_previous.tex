\section{Prior Approaches}
Prior approaches to language model alignment involve directly optimizing the logits produced by the language modelling head to increase the likelihood of producing preferable responses while decreasing the likelihood of undesirable responses. %This is often realized through RLHF or contrastive methods which are outlined below.

\subsection{Reinforcement Learning from Human Feedback (RLHF)}
Reinforcement Learning from Human Feedback seeks to learn a reward model from human feedback on completions generated by a language model which can be used to align an LM with human preferences. A typical RLHF pipeline consists of 3 steps: (1) supervised fine-tuning, (2) preference sampling and reward modelling, and (3) RL fine-tuning.

\textbf{Supervised Fine-Tuning\ } The first step of a standard RLHF pipeline is fine-tuning a pre-trained LM on high quality data for downstream tasks to obtain a model $\pi^\text{SFT}$.

\textbf{Reward Modelling\ } Next, the SFT model is prompted with input tokens $x$ to produce completions $y$. These answers are then rated by human labelers which rate the answers based on one or more criteria. A reward model $r_\phi(x,y)$ is then trained to estimate the scores assigned by human labelers using maximum likelihood estimation.

\textbf{RL Fine-Tuning\ } During the RL phase the learned reward function is used to provide feedback to the language model using the following optimization problem
\begin{equation} \label{eq:rl-objective}
    \underset{\pi_\theta}{\max}\, \mathbb{E}_{x \sim \mathcal{D},y \sim \pi_\theta(y|x)}
    \left[ r_\phi(x,y) \right]
    - \beta D_{\text{KL}}\left[ \pi_\theta(y|x)||\pi_\text{ref}(y|x) \right]
\end{equation}
where $\beta$ controls the deviation from the base reference policy $\pi_{\text{ref}}$, which is typically initialized from $\pi^\text{SFT}$. Due to the non-differentiable nature of language generation this objective must be optimized using a reinforcement learning algorithm such as PPO \cite{schulman2017proximal}.

\subsection{Direct Preference Optimization (DPO)}
Direct Preference Optimization was introduced as a reparameterization of RLHF which eliminates both the sampling stage and the reward modelling stages and reformulates alignment procedure as a loss function which can be optimized directly on a dataset of pairs of preferred and dispreferred completions to given prompts. This allows DPO to stably and efficiently converge on an optimal policy using what is effectively a classification loss over positive and negative pairs.

Given a dataset $\{(x,y_w,y_l)\}$ where $x$ is the prompt and $y_w,y_l$ are the preferred and dispreferred completions, we introduce the following loss function:
%the following loss function is formulated:
\begin{equation}
    \mathcal{L}_\text{DPO}(x,y_w,y_l)=
    -\log\sigma
    \left(
        \beta\log \frac{\pi_\theta(y_w|x)}{\pi_\text{ref}(y_w|x)} -
        \beta\log \frac{\pi_\theta(y_l|x)}{\pi_\text{ref}(y_l|x)}
    \right)
    \label{eq:dpo}
\end{equation}
where $\pi_\theta(y_*|x)$ and $\pi_\text{ref}(y_*|x)$ are the probabilities of completions $y_*$ for prompt $x$ given by the policy model and reference models respectively, and the $\beta$ parameter controls the deviation from the reference policy.

There also exists an augmentation of DPO namely Conservative DPO (cDPO) \cite{cdpo} which is designed to be more robust to noisy labels through the introduction of label smoothing parameter $\epsilon$. The objective function for cDPO is given by:% the following formula:
\begin{equation}
    \mathcal{L}_\text{cDPO}(x,y_w,y_l)=
    (1-\epsilon) \mathcal{L}_\text{DPO}(x,y_w,y_l) +
    \epsilon\,\mathcal{L}_\text{DPO}(x,y_l,y_w)
    \label{eq:cdpo}
\end{equation}