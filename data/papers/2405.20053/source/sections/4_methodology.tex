\section{Experimental Setup and Data} \label{sec:methodology}

\subsection{Datasets}
We make use of a variety of datasets for fine-tuning and evaluation which are outlined below. The specific prompt templates used for fine-tuning and evaluation are described in Appendix~\ref{sec:datasets_cont}.

\textbf{Natural Language Understanding (NLU)\ }
For general NLU we make use of the standard \textbf{GLUE} benchmark \cite{wang2019glue}. The overall score for GLUE is computed by the macro-average of unweighted metric averages for all 9 tasks, however we also include a secondary score which does not included the `problematic' WNLI task following the evaluation used for BERT \cite{devlin2019bert}. We opted to omit WNLI during fine-tuning due to the low sample size. %, overlap of sentences in the training and validation splits, and different label distribution in the test set.

\textbf{Commonsense Reasoning\ }
In accordance with the \textbf{GPT4All} \cite{gpt4all} evaluation suite, we use the following datasets to evaluate commonsense reasoning abilities:
\textbf{HellaSwag} \cite{zellers2019hellaswag},
\textbf{OpenBookQA} \cite{mihaylov2018suit},
\textbf{WinoGrande} \cite{DBLP:journals/corr/abs-1907-10641},
\textbf{ARC} \cite{clark2018think},
\textbf{BoolQ} \cite{clark2019boolq},
and \textbf{PIQA} \cite{bisk2019piqa}.
% \textbf{HellaSwag} \cite{zellers2019hellaswag} -- an adversarially constructed sequence completion task,
% \textbf{OpenBookQA} \cite{mihaylov2018suit} -- a commonsense multiple-choice task,
% \textbf{WinoGrande} \cite{DBLP:journals/corr/abs-1907-10641} -- a fill-in-a-blank task with binary options,
% \textbf{ARC} \cite{clark2018think} -- a collection of multiple-choice grade-school science questions,
% \textbf{BoolQ} \cite{clark2019boolq} -- an open-book yes/no QA task, and
% \textbf{PIQA} \cite{bisk2019piqa} -- a QA task focused on reasoning over physical actions.

% \todo{These tasks are included in the fine-tuning and alignment mixes used to train our models. We use the macro-average of task scores as an indicator of average commonsense reasoning ability, where we use specific validation or test split used by the LM Evaluation Harness \cite{eval-harness} for consistency with other evaluations.}

\textbf{Reading Comprehension\ }
To evaluate reading comprehension abilities we use the \textbf{RACE} dataset \cite{lai2017race}, a multiple-choice task which requires reasoning over provided passages.

% \todo{We include both these tasks in our fine-tuning and alignment mixes, but opt to not include SQuAD as part of our model evaluation results; this is due to the need to reformulate SQuAD as a generative task for causal LMs which introduces a multitude of sampling and generation hyperparameters which must be ablated over. Never-the-less we do include results of a reformulated version of SQuAD V2 which compares the log-probabilities of ground-truth answers from the validation set with `distractor' spans extracted using SpaCy; this is included purely to evaluate the performance of DPH alignment and these results should not be compared with the scores of encoder-only models.}

\textbf{Instruction Following\ }
We include the \textbf{Alpaca} \cite{alpaca}, \textbf{OpenOrca} \cite{OpenOrca}, and \textbf{UltraFeedback} \cite{cui2023ultrafeedback} datasets to train our models for instruction following.
% \textbf{Alpaca} \cite{alpaca} -- a collection of 52,000 self-instruct question-answer pairs,
% \textbf{OpenOrca} \cite{OpenOrca} -- a large dataset of augmented question-answer pairs from the FLAN Collection \cite{longpre2023flan}, and
% \textbf{UltraFeedback} \cite{cui2023ultrafeedback} -- a large scale preference dataset generated from a variety of LLMs.
We make use of OpenOrca and a cleaned version of \href{https://huggingface.co/datasets/yahma/alpaca-cleaned}{Alpaca} for SFT, and binarized versions of \href{https://huggingface.co/datasets/Intel/orca_dpo_pairs}{OpenOrca} and \href{https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned}{UltraFeedback} for alignment.

\textbf{Auxiliary Datasets\ }
% We also make use of auxiliary train split from \textbf{MMLU} \cite{hendrycks2021measuring} to provide additional multiple-choice training data, but opt not to evaluate our models on this dataset due to requiring highly domain-specific knowledge. Additionally, we include \textbf{SQuAD V2} \cite{rajpurkar-etal-2018-know,rajpurkar-etal-2016-squad}, \textbf{Tiny Stories} \cite{eldan2023tinystories}, \textbf{CNN-Dailymail} \cite{nallapati2016abstractive} and \textbf{CoQA} \cite{reddy2019coqa} to provide signals for a wider range of tasks during SFT.
To provide additional training data for SFT we include the \textbf{MMLU} \cite{hendrycks2021measuring}, \textbf{SQuAD V2} \cite{rajpurkar-etal-2018-know,rajpurkar-etal-2016-squad}, \textbf{Tiny Stories} \cite{eldan2023tinystories}, \textbf{CNN-Dailymail} \cite{nallapati2016abstractive} and \textbf{CoQA} \cite{reddy2019coqa} training splits. For alignment we only include MMLU and SQuAD V2.

\subsection{Prompts and Sampling}
\textbf{Prompts\ } We make use of the ChatML prompt templating scheme \cite{chatml} with handcrafted \texttt{system}, \texttt{user} and \texttt{assistant} prompts specific to each task. During fine-tuning we mask out the loss for all tokens of the prompt and condition the model on the content of \texttt{assistant} messages including the final \texttt{<|im\_end|>} token. During evaluation we select the highest scoring answer using the average log-probabilities of the tokens in the final \texttt{assistant} message, or compute the reward scores on the final \texttt{<|im\_end|>} token when evaluating with DPH.

\textbf{SFT Sampling\ } \label{sec:sft-sampling} When sampling from the datasets for SFT we randomly shuffle each dataset and uniformly interleave samples from all tasks in the mix. To control the weighting of samples from each task we fill the context window with $n$ consecutive samples from the same task before sampling from a different task, where $n$ is chosen to be 5 in our experiments. To maximise compute utilisation and minimize unused portions of the context window we make us of Transformer-XL \cite{dai2019transformerxl} style training with a context window size of 2048 tokens and a recurrent memory size of 2048 tokens.

\textbf{DPH Sampling\ } \label{sec:dph-sampling} When sampling from datasets for DPH alignment we switch from the Transformer-XL style pipeline to typical SFT training, opting to only include single samples in the context window padded to a fixed maximum length. As some of the datasets we use for DPH are intended for SFT rather than alignment (namely GLUE, GPT4All, RACE, MMLU and SQuAD) we synthesise preference pairs where the `correct' answer is used as the preferred completion and we uniformly sample an `incorrect' answer from the available choices for the dispreferred completion. This is trivial for most datasets, however we use a special process for the SQuAD V2 dataset; for answerable questions we use ``unanswerable'' as the dispreferred completion, and for unanswerable questions we use SpaCy to randomly sample a noun span from the context to use as the dispreferred completion.

\subsection{Regularization} \label{sec:regularization}
The hidden states $h$ used to compute the reward scores are likely sub-optimal for computing rewards when initialising $\pi_\theta$ from $\pi^{\text{SFT}}$. As such, it may be desirable to fine-tune some or all parameters in the language model to learn better reward signals. This necessitates the use of regularization to prevent degradation of the models generative capabilities while learning to predict rewards.

\textbf{Prior Regularization\ }
Typical parameter regularization strategies such as weight decay make the assumption that parameters $\theta$ follow a zero-mean Normal distribution $p(\theta) \sim \mathcal{N}(0,\tfrac{1}{\beta}\text{I})$ leading to an auxiliary loss term $\tfrac{\beta}{2}||\theta||^2_2$. However, when performing transfer-learning or fine-tuning on a pre-trained model this assumption can be harmful and aid in catastrophic forgetting of the model's previously learnt abilities.

An alternative regularization scheme is Prior Regularization \cite{CHELBA2006382, daum√©2009frustratingly, grachten2019strategies} which instead makes the assumption that the fine-tuned parameters are normally distributed around the original parameters $\theta_{\text{ref}}$, that is $\theta \sim \mathcal{N}(\theta_{\text{ref}},\tfrac{1}{\beta}\text{I})$, leading to the auxiliary loss term $\tfrac{\beta}{2}||\theta-\theta_{\text{ref}}||^2_2$.

We employ Prior Regularization to limit the divergence of $\pi_\theta$ from $\pi^{\text{SFT}}$ while still facilitating the learning of improved hidden state representations for the Direct Preference Head. Pseudocode for optimizer based decoupled prior regularization is included in Appendix~\ref{sec:decoupled-pr}.

% \paragraph{KL Divergence Regularization}
% Utilising KL Divergence as means to prevent a policy from diverging too far from the initial parameters is popular form of regularization: In TRPO the KL term is used to constrain the optimized policy \cite{schulman2017trust}, while PPO uses the KL Divergence as a penalty in the objective function \cite{schulman2017proximal}. \todo{Following PPO, we can form an auxiliary loss penalty with the following formula
% \begin{equation}
%     L_{penalty}(x,y) =
%     \beta D_{\text{KL}}\left[ \pi_\theta(y|x)||\pi_\text{ref}(y|x) \right]
% \end{equation}
% where $\beta$ is the regularization penalty. As noted in the PPO paper, the $\beta$ parameter requires careful tuning, and varying this coefficient throughout training may be required.}

\textbf{cDPO Regularization\ }
Rather than directly employing a KL Divergence penalty similar to that used in \eqref{eq:rl-objective} we find that it is possible -- and even beneficial -- to use Conservative DPO as a means of (1) limiting the divergence of the policy model to a fixed delta from the reference model, and (2) `nudging' the model towards generating more preferable outputs which increases the chance of generating a better candidate completion at inference time with fewer sampling steps.

% \paragraph{Head Regularization}
% \todo{Although both flavors of the DPH objective include label smoothing which regularizes the confidence of reward scores, it is still possible for the a preference head optimized with Contrastive DPH loss to produce rewards in divergent manor. This is due to ConDPH loss operating on the \textit{margin} of preferred and dispreferred rewards which may not necesserily be centered around zero. This can be prevented in two ways, by constraining the head weights $w_{dph}$ using weight decay or by applying a penalty to the rewards as they diverge from zero.}

\subsection{Training Pipeline}
We progressively fine-tune the models in 3 stages: vocab extension, supervised fine-tuning, and DPH alignment. The details of the pre-trained model are included in Appendix~\ref{sec:pretrained-model}.

\textbf{Vocab Extension\ } Since our model was pre-trained without a chat structure it is necessary to train the embeddings for additional \texttt{<|im\_start|>} and \texttt{<|im\_end|>} tokens: we freeze all non-embedding parameters and use the same datasets as SFT. We fine-tune the embeddings for 4096 steps with a batch size of 128, a max LR of 6e-5 which warms up over 200 steps followed by cosine decay down to zero, and clip the global gradient norm to 1.

\textbf{Supervised Fine-Tuning\ } After vocab extension we move onto the SFT step which conditions the model for NLU tasks and instruction following using the sampling and loss masking method described in section~\ref{sec:sft-sampling}. We fine-tune the model for 6144 steps with a batch size of 128, a max LR of 3e-5 which warms up over 200 steps followed by cosine decay down to zero, prior-regularization applied to all non-embedding parameters with coefficient 0.5, and clip the global gradient norm to 1.

\textbf{DPH Alignment\ } Using the sampling method described in section~\ref{sec:dph-sampling} we jointly learn DPH rewards and perform cDPO alignment. The goal here is to gently push the model towards producing preferable outputs without compromising the model's reasoning abilities, and the priority is to attain the highest validation metrics from the DPH rewards. This requires balancing the two objectives, and as such we introduce weighting parameters $\alpha_1, \alpha_2$ to our final joint objective in \eqref{eq:alignment_objective} where $\mathcal{L}_\text{DPH}$ is either $\mathcal{L}_\text{sepDPH}$ or $\mathcal{L}_\text{conDPH}$. We find $\alpha_1,\alpha_2=1$ to be a good blance between DPO and DPH in our experiments.
\begin{equation} \label{eq:alignment_objective}
    \mathcal{L}_\text{joint}(x,y_w,y_l,r_w,r_l) =
    \alpha_1 \mathcal{L}_\text{cDPO}(x,y_w,y_l) +
    \alpha_2 \mathcal{L}_\text{DPH}(r_w,r_l)
\end{equation}
We align the model for 23040 steps with a batch size of 64 pairs, a max LR of 3e-6 which warms up over 200 steps followed by cosine decay down to 3e-7, prior-regularization applied to all parameters with coefficient 0.5, and clip the global gradient norm to 1. Following the optimal DPO parameters for OpenHermes-7b-2.5 \cite{pref-tuning} we use $\beta=0.6$ and chose cDPO $\epsilon=0.25$ and DPH $\epsilon=0.1$ for regularisation. Additionally, we apply dropout with $p=0.1$ to the outputs of the pooler.

\subsection{Compute Resources}
All fine-tuning was performed using an NVIDIA A100 SXM4 80GB GPU on a compute cluster, with jobs allocated 24 cores and 160GB of memory. Each checkpoint is saved in FP16 format which consumes about 1.1GB of storage, and the datasets require minimal storage space.

For vocab extension we train for 4096 steps with an average of 7.99 seconds of compute per step which translates to about 9 hours. For supervised fine-tuning we train for 6144 steps with an average of 9.26 seconds of compute per step which translates to about 16 hours. For DPH alignment we train for 23040 steps with an average of 7.21 seconds of compute per step which translates to about 46 hours. The DPH ablations with our models use about 140 hours of compute, and the Qwen ablations use about 60 hours of compute. In total, we used approximately 270 hours of A100 compute to train our models and collect the results included in our paper. We used additional compute for preliminary tests and fixing bugs for silently failing experiments although this wasn't tracked.