\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anand et~al.(2023)Anand, Nussbaum, Duderstadt, Schmidt, and Mulyar]{gpt4all}
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.
\newblock Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.
\newblock \url{https://github.com/nomic-ai/gpt4all}, 2023.

\bibitem[au2(2009)]{daumé2009frustratingly}
Hal Daumé~III au2.
\newblock Frustratingly easy domain adaptation, 2009.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, El-Showk, Elhage, Hatfield-Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.

\bibitem[Beeching et~al.(2023)Beeching, Fourrier, Habib, Han, Lambert, Rajani, Sanseviero, Tunstall, and Wolf]{open-llm-leaderboard}
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.
\newblock Open llm leaderboard.
\newblock \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}, 2023.

\bibitem[Bekbayev et~al.(2023)Bekbayev, Chun, Dulat, and Yamazaki]{bekbayev2023poison}
Aibek Bekbayev, Sungbae Chun, Yerzat Dulat, and James Yamazaki.
\newblock The poison of alignment, 2023.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der Wal.
\newblock Pythia: A suite for analyzing large language models across training and scaling, 2023.

\bibitem[Bisk et~al.(2019)Bisk, Zellers, Bras, Gao, and Choi]{bisk2019piqa}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language, 2019.

\bibitem[Chelba and Acero(2006)]{CHELBA2006382}
Ciprian Chelba and Alex Acero.
\newblock Adaptation of maximum entropy capitalizer: Little data can help a lot.
\newblock \emph{Computer Speech \& Language}, 20\penalty0 (4):\penalty0 382--399, 2006.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{DBLP:journals/corr/abs-2110-14168}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems, 2021.

\bibitem[Cui et~al.(2023)Cui, Yuan, Ding, Yao, Zhu, Ni, Xie, Liu, and Sun]{cui2023ultrafeedback}
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun.
\newblock Ultrafeedback: Boosting language models with high-quality feedback, 2023.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov]{dai2019transformerxl}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V. Le, and Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length context, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.

\bibitem[Eldan and Li(2023)]{eldan2023tinystories}
Ronen Eldan and Yuanzhi Li.
\newblock Tinystories: How small can language models be and still speak coherent english?, 2023.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding, Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 2021.

\bibitem[Grachten and Chacón(2019)]{grachten2019strategies}
Maarten Grachten and Carlos Eduardo~Cancino Chacón.
\newblock Strategies for conceptual change in convolutional neural networks, 2019.

\bibitem[Hanu and {Unitary team}(2020)]{Detoxify}
Laura Hanu and {Unitary team}.
\newblock Detoxify.
\newblock Github. \url{https://github.com/unitaryai/detoxify}, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding, 2021.

\bibitem[Inan et~al.(2023)Inan, Upasani, Chi, Rungta, Iyer, Mao, Tontchev, Hu, Fuller, Testuggine, and Khabsa]{inan2023llama}
Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa.
\newblock Llama guard: Llm-based input-output safeguard for human-ai conversations, 2023.

\bibitem[kashif et~al.(2024)kashif, edbeeching, lewtun, lvwerra, and osanseviero]{pref-tuning}
kashif, edbeeching, lewtun, lvwerra, and osanseviero.
\newblock Preference tuning llms with direct preference optimization methods.
\newblock Github. \url{https://github.com/huggingface/blog/blob/main/pref-tuning.md}, 2024.

\bibitem[Köpf et~al.(2023)Köpf, Kilcher, von Rütte, Anagnostidis, Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi, ES, Suri, Glushkov, Dantuluri, Maguire, Schuhmann, Nguyen, and Mattick]{köpf2023openassistant}
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick.
\newblock Openassistant conversations -- democratizing large language model alignment, 2023.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{lai2017race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock Race: Large-scale reading comprehension dataset from examinations, 2017.

\bibitem[Lian et~al.(2023)Lian, Goodson, Pentland, Cook, Vong, and "Teknium"]{OpenOrca}
Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium".
\newblock Openorca: An open dataset of gpt augmented flan reasoning traces.
\newblock \url{https://https://huggingface.co/Open-Orca/OpenOrca}, 2023.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin2022truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods, 2022.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018suit}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.

\bibitem[Mitchell(2023)]{cdpo}
Eric Mitchell.
\newblock A note on dpo with noisy preferences \& relationship to ipo.
\newblock \url{https://ericmitchell.ai/cdpo.pdf}, 2023.

\bibitem[Nallapati et~al.(2016)Nallapati, Zhou, dos santos, Gulcehre, and Xiang]{nallapati2016abstractive}
Ramesh Nallapati, Bowen Zhou, Cicero~Nogueira dos santos, Caglar Gulcehre, and Bing Xiang.
\newblock Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016.

\bibitem[OpenAI()]{chatml}
OpenAI.
\newblock Chat markup language.
\newblock \url{https://github.com/openai/openai-python/blob/f7ccce126325ea35b6e5224ab954652c97a74896/chatml.md}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback, 2022.

\bibitem[Radford and Narasimhan(2018)]{Radford2018ImprovingLU}
Alec Radford and Karthik Narasimhan.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D. Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model, 2023.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{rajpurkar-etal-2016-squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, pages 2383--2392, Austin, Texas, 2016. Association for Computational Linguistics.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{rajpurkar-etal-2018-know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don{'}t know: Unanswerable questions for {SQ}u{AD}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 784--789, Melbourne, Australia, 2018. Association for Computational Linguistics.

\bibitem[Reddy et~al.(2019)Reddy, Chen, and Manning]{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D. Manning.
\newblock Coqa: A conversational question answering challenge, 2019.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi]{DBLP:journals/corr/abs-1907-10641}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock {WINOGRANDE:} an adversarial winograd schema challenge at scale, 2019.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms, 2017.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer, 2020.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R. Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?, 2019.

\bibitem[Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
\newblock Tinyllama: An open-source small language model, 2024.

\bibitem[Ziyin et~al.(2021)Ziyin, Wang, and Ueda]{ziyin2021laprop}
Liu Ziyin, Zhikang~T. Wang, and Masahito Ueda.
\newblock Laprop: Separating momentum and adaptivity in adam, 2021.

\end{thebibliography}
