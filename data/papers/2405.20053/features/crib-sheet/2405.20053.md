- **Key Concept**: Direct Preference Heads (DPH) framework allows language models (LMs) to learn human preferences through an auxiliary reward head without altering the output distribution of the language modeling head.

- **Problem Addressed**: RLHF can degrade reasoning capabilities and introduce hallucinations in LMs. DPH mitigates these issues by optimizing reward signals directly.

- **Objective Functions**:
  - **Separable DPH Loss**:
    \[
    L_{SepDPH}(r_w, r_l) = -[(1 - \epsilon) \log \sigma(r_w) + \epsilon \log \sigma(-r_w)] - [\epsilon \log \sigma(r_l) + (1 - \epsilon) \log \sigma(-r_l)]
    \]
    - Convexity proven; optimizes preferred and dispreferred rewards.
  
  - **Contrastive DPH Loss**:
    \[
    L_{ConDPH}(r_w, r_l) = - (1 - \epsilon) \log \sigma(r_w - r_l) - \epsilon \log \sigma(r_l - r_w)
    \]
    - Optimizes the margin between preferred and dispreferred rewards.

- **Relation to cDPO**: DPH methods (Separable and Contrastive) show strong ties to Conservative Direct Preference Optimization (cDPO), enhancing robustness to label noise.

- **Reinforcement Learning from Human Feedback (RLHF) Pipeline**:
  1. **Supervised Fine-Tuning (SFT)**: Fine-tune LM on high-quality data.
  2. **Reward Modeling**: Train a reward model \( r_\phi(x, y) \) based on human feedback.
  3. **RL Fine-Tuning**: Optimize using:
    \[
    \max_{\pi_\theta} E_{x \sim D, y \sim \pi_\theta(y|x)} [r_\phi(x, y)] - \beta D_{KL}[\pi_\theta(y|x) || \pi_{ref}(y|x)]
    \]

- **Experimental Evaluation**: DPH evaluated on GLUE, RACE, and GPT4All, outperforming models fine-tuned with SFT or DPO alone.

- **Datasets Used**:
  - **NLU**: GLUE benchmark.
  - **Commonsense Reasoning**: HellaSwag, OpenBookQA, WinoGrande, ARC, BoolQ, PIQA.
  - **Reading Comprehension**: RACE dataset.
  - **Instruction Following**: Alpaca, OpenOrca, UltraFeedback.

- **Reward Calculation**:
  \[
  r = f(h) \cdot w_{dph}
  \]
  - Where \( h \) is the aggregated hidden state, \( f \) is a pooling function, and \( w_{dph} \) is a learnable vector.

- **Regularization**: Necessary to prevent degradation of generative capabilities while learning to predict rewards.

- **Implementation Note**: DPH can be combined with existing alignment techniques, making it versatile for various model sizes and architectures.