- **Key Concept**: Direct Preference Heads (DPH) framework allows language models (LMs) to learn human preferences through an auxiliary reward head without altering the output distribution of the language modeling head.

- **Problem Addressed**: RLHF can degrade reasoning capabilities and introduce hallucinations in LMs. DPH mitigates these issues by optimizing reward signals directly.

- **Objective Functions**:
  - **Separable DPH Loss**:
    \[
    L_{SepDPH}(r_w, r_l) = -[(1 - \epsilon) \log \sigma(r_w) + \epsilon \log \sigma(-r_w)] - [\epsilon \log \sigma(r_l) + (1 - \epsilon) \log \sigma(-r_l)]
    \]
    - Convexity ensures convergence of preferred rewards \( r_w \) and dispreferred rewards \( r_l \) to specific log values.
  
  - **Contrastive DPH Loss**:
    \[
    L_{ConDPH}(r_w, r_l) = -(1 - \epsilon) \log \sigma(r_w - r_l) - \epsilon \log \sigma(r_l - r_w)
    \]
    - Optimizes the margin between preferred and dispreferred rewards.

- **Reward Calculation**:
  \[
  r = f(h) \cdot w_{dph}
  \]
  - Where \( h \) is the aggregated hidden state, \( f \) is a pooling function, and \( w_{dph} \) is a learnable vector.

- **Comparison with Traditional RLHF**:
  - DPH does not require separate reward modeling or multiple models (reward, reference, policy). It uses a single model to generate responses and rewards.

- **Evaluation Metrics**: DPH models evaluated on GLUE, RACE, and GPT4All, showing superior performance compared to models fine-tuned with SFT or DPO alone.

- **Datasets Used**:
  - **NLU**: GLUE benchmark.
  - **Commonsense Reasoning**: HellaSwag, OpenBookQA, WinoGrande, ARC, BoolQ, PIQA.
  - **Reading Comprehension**: RACE dataset.
  - **Instruction Following**: Alpaca, OpenOrca, UltraFeedback.

- **Regularization**: Necessary to prevent degradation of generative capabilities while learning to predict rewards.

- **Theoretical Foundations**: Strong ties to Conservative Direct Preference Optimization (cDPO), enhancing robustness to noisy labels.

- **Implementation Note**: Code and model weights available on GitHub and Hugging Face, facilitating reproducibility and further research.