@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@article{Piao_2023,
   title={{Enabling Large Batch Size Training for DNN Models Beyond the Memory Limit While Maintaining Performance}},
   volume={11},
   ISSN={2169-3536},
   url={http://dx.doi.org/10.1109/ACCESS.2023.3312572},
   DOI={10.1109/access.2023.3312572},
   journal={IEEE Access},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Piao, Xinyu and Synn, Doangjoo and Park, Jooyoung and Kim, Jong-Kook},
   year={2023},
   pages={102981–102990} 
}

@article{li2020pytorch,
  title={{PyTorch Distributed: Experiences on Accelerating Data Parallel Training}},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={12},
  pages={3005-3018},
  year={2020},
  url={https://vldb.org/pvldb/vol13/p3005-li.pdf}
}

@article{robbins1951sgd,
  title={{A Stochastic Approximation Method}},
  author={Herbert Robbins and Sutton Monro},
  journal={The Annals of Mathematical Statistics},
  volume={22},
  number={3},
  pages={400-407},
  year={1951}
}

@inproceedings{yao2023gradientshapingmulticonstraintsafe,
  title={{Gradient Shaping for Multi-Constraint Safe Reinforcement Learning}}, 
  author={Yihang Yao and Zuxin Liu and Zhepeng Cen and Peide Huang and Tingnan Zhang and Wenhao Yu and Ding Zhao},
  year={2024},
  booktitle={6th Annual Learning for Dynamics \& Control Conference},
}

@article{yu2020gradientsurgerymultitasklearning,
  title={{Gradient Surgery for Multi-Task Learning}}, 
  author={Tianhe Yu and Saurabh Kumar and Abhishek Gupta and Sergey Levine and Karol Hausman and Chelsea Finn},
  year={2020},
  journal={Advances in Neural Information Processing Systems}
}

@inproceedings{wei2022learning,
  title={{Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations}},
  author={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang Niu and Yang Liu},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=TBWA6PLJZQm}
}

@inproceedings{baidu2017ringallreduce,
  title={{Large-Scale Distributed Deep Learning: Lessons Learned from 3,000,000 GPU Hours on TitanX}},
  author={Yanping Huang and others},
  booktitle={the 25th ACM Symposium on Operating Systems Principles},
  year={2017},
  pages={19-33}
}

@article{keskar2016large,
  title={{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
  author={Nitish Shirish Keskar and others},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@inproceedings{arpit2017closer,
  title={{A Closer Look at Memorization in Deep Networks}},
  author={Arpit, Devansh and others},
  booktitle={the 34th International Conference on Machine Learning},
  year={2017}
}

@inproceedings{he2016deep,
  title={{Deep Residual Learning for Image Recognition}},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{dosovitskiy2021an,
  title={{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and others},
  journal={International Conference on Learning Representations},
  year={2021}
}

@article{mccandlish2018empirical,
  title={{An Empirical Model of Large-Batch Training}},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and others},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}

@article{polyak1964some,
  title={{Some Methods of Speeding Up the Convergence of Iteration Methods}},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964}
}

@inproceedings{kingma2014adam,
  author       = {Diederik P. Kingma and Jimmy Ba},
  title={{Adam: A Method for Stochastic Optimization}},
  booktitle    = {International Conference on Learning Representations},
  year         = {2015}
}

@article{loshchilov2017decoupled,
  title={{Decoupled Weight Decay Regularization}},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{dean2012large,
  title={{Large Scale Distributed Deep Networks}},
  author={Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2012}
}

@article{tieleman2012lecture,
  title={{Lecture 6.5-RMSProp: Divide the Gradient by a Running Average of its Recent Magnitude}},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={Coursera: Neural Networks for Machine Learning},
  volume={4},
  number={2},
  year={2012}
}

@inproceedings{you2020large,
  title={{Large Batch Optimization for Deep Learning: Training BERT in 76 Minutes}},
  author={You, Yang and others},
  booktitle={International Conference on Learning Representations},
    year={2020},
}

@article{zhuang2020adabelief,
  title={{AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients}},
  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and others},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{srivastava2014dropout,
  title={{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
  author={Srivastava, Nitish and others},
  journal={Journal of Machine Learning Research},
  volume={15},
  pages={1929--1958},
  year={2014}
}

@inbook{prechelt1998early,
  title={{Early Stopping - But When?}},
  author={Prechelt, Lutz},
  booktitle={Neural Networks: Tricks of the Trade},
  pages={55--69},
  year={1998},
  publisher={Springer}
}

@inproceedings{li2020learning,
      title={{Learning from Noisy Labels with Distillation}}, 
      author={Yuncheng Li and Jianchao Yang and Yale Song and Liangliang Cao and Jiebo Luo and Li-Jia Li},
      year={2017},
      booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}
}

@inproceedings{han2018coteaching,
  title={{Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels}},
  author={Han, Bo and Yao, Quanming and Liu, Xingrui and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8527--8537},
  year={2018}
}

@inproceedings{liu2022self,
    title={{Robust Training under Label Noise by Over-parameterization}}, 
    author={Sheng Liu and Zhihui Zhu and Qing Qu and Chong You},
    year={2022},
    booktitle={International Conference on Machine Learning},
}

@inproceedings{li2020dividemix,
  title={{DivideMix: Learning with Noisy Labels as Semi-supervised Learning}},
  author={Li, Junnan and Socher, Richard and Hoi, Steven C.H.},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{shoeybi2019megatron,
  title={{Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raghavendra and others},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019},
  url={https://arxiv.org/abs/1909.08053}
}

@article{anil2021scalable,
  title={{Scalable Second Order Optimization for Deep Learning}},
  author={Anil, Rohan and others},
  journal={Machine Learning and Systems},
  year={2021}
}

@inproceedings{ren2018learning,
  title={{Learning to Reweight Examples for Robust Deep Learning}},
  author={Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  booktitle={International Conference on Machine Learning},
  pages={4334--4343},
  year={2018}
}

@inproceedings{xiao2023promix,
  title={{ProMix: Combating Label Noise via Maximizing Clean Sample Utility}},
  author={Xiao, Ruixuan and Dong, Yiwen and Wang, Haobo and Feng, Lei and Wu, Runze and Chen, Gang and Zhao, Junbo},
  booktitle={Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
  pages={4442--4450},
  year={2023}
}

@inproceedings{chen2023sample,
  title={{Sample Prior Guided Robust Model Learning to Suppress Noisy Labels}},
  author={Chen, Wenkai and Zhu, Chuang and Li, Mengting},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={3--19},
  year={2023},
  organization={Springer}
}

@inproceedings{foret2020sharpness,
  title={{Sharpness-Aware Minimization for Efficiently Improving Generalization}},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{hoffmann2022training,
  title={{Training Compute-Optimal Large Language Models}},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  booktitle={International Conference on Neural Information Processing Systems},
  pages={30016--30030},
  year={2022}
}

@mastersthesis{krizhevsky2009learning,
  title={{Learning Multiple Layers of Features from Tiny Images}},
  author={Krizhevsky, Alex},
  year={2009},
  school={University of Toronto},
}

@inproceedings{deng2009imagenet,
  title={{ImageNet: A Large-Scale Hierarchical Image Database}},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={248--255},
  year={2009}
}

@inproceedings{tishby2015deep,
  title={{Deep Learning and the Information Bottleneck Principle}},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={IEEE Information Theory Workshop},
  year={2015}
}

@article{goyal2018accuratelargeminibatchsgd,
      title={{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}}, 
      author={Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
      year={2017},
      journal={arXiv preprint arXiv:1706.02677},
}

@article{pascanu2013difficultytrainingrecurrentneural,
      title={{On the Difficulty of Training Recurrent Neural Networks}}, 
      author={Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
      year={2013},
      journal = {International Conference on Machine Learning}
}


@misc{neelakantan2015addinggradientnoiseimproves,
      title={{Adding Gradient Noise Improves Learning for Very Deep Networks}}, 
      author={Arvind Neelakantan and Luke Vilnis and Quoc V. Le and Ilya Sutskever and Lukasz Kaiser and Karol Kurach and James Martens},
      year={2015},
      eprint={1511.06807},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1511.06807}, 
}

@INPROCEEDINGS{5206848,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={{ImageNet: A large-scale hierarchical image database}}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}