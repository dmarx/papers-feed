\section{Discussion}
\label{sec:discussion}

We have proposed \mixup{}, a data-agnostic and straightforward data
augmentation principle.  We have shown that \mixup{} is a form of vicinal risk
minimization, which trains on virtual examples constructed as the linear
interpolation of two random examples from the training set and their labels.
Incorporating \mixup{} into existing training pipelines reduces to a few lines
of code, and introduces little or no computational overhead.  Throughout an
extensive evaluation, we have shown that \mixup{} improves the generalization
error of state-of-the-art models on ImageNet, CIFAR, speech, and tabular
datasets. Furthermore, \mixup{} helps to combat memorization of corrupt labels,
sensitivity to adversarial examples, and instability in adversarial training.

In our experiments, the following trend is consistent: with increasingly large
$\alpha$, the training error on real data increases, while the generalization
gap decreases. This sustains our hypothesis that \mixup{} implicitly controls
model complexity. However, we do not yet have a good theory for understanding
the `sweet spot' of this bias-variance trade-off. For example, in CIFAR-10
classification we can get very low training error on real data even when
$\alpha \to \infty$ (i.e., training \emph{only} on averages of pairs of real
examples), whereas in ImageNet classification, the training error on real data
increases significantly with $\alpha \to \infty$. Based on our ImageNet and
Google commands experiments with different model architectures, we conjecture
that increasing the model capacity would make training error less sensitive to
large $\alpha$, hence giving \mixup{} a more significant advantage. 

\mixup{} also opens up several possibilities for further exploration. First, is
it possible to make similar ideas work on other types of supervised learning
problems, such as regression and structured prediction? While generalizing
\mixup{} to regression problems is straightforward, its application to
structured prediction problems such as image segmentation remains less obvious.
Second, can similar methods prove helpful beyond supervised learning? The
interpolation principle seems like a reasonable inductive bias which might also
help in unsupervised, semi-supervised, and reinforcement learning. Can we
extend \mixup{} to feature-label extrapolation to guarantee a robust model
behavior far away from the training data? Although our discussion of these
directions is still speculative, we are excited about the possibilities
\mixup{} opens up, and hope that our observations will prove useful for future
development. 

\ificlrfinal
\section*{Acknowledgements}
We would like to thank Priya Goyal, Yossi Adi and the PyTorch team. We also thank the Anonymous Review 2 for proposing the \mixup{} + dropout experiments.
\fi
