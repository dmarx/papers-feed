\section{Related Work}
\label{sec:related}

Data augmentation lies at the heart of all successful applications of deep
learning, ranging from image classification~\citep{krizhevsky2012imagenet} to
speech recognition~\citep{graves2013speech, amodei2016deep}. In all cases,
substantial domain knowledge is leveraged to design suitable data
transformations leading to improved generalization. In image classification,
for example, one routinely uses rotation, translation, cropping, resizing,
flipping~\citep{lecun98, simonyan2014very}, and random
erasing~\citep{zhong2017random} to enforce visually plausible invariances in
the model through the training data. Similarly, in speech recognition, noise
injection is a prevalent practice to improve the robustness and accuracy of the
trained models~\citep{amodei2016deep}. 

More related to \mixup{}, \citet{chawla2002smote} propose to augment the rare class in an imbalanced dataset by interpolating the nearest neighbors; \citet{devries2017dataset} show that interpolation
and extrapolation the nearest neighbors of the same class in feature space can improve generalization. However, their
proposals only operate among the nearest neighbors within a certain class at the input / feature level, and hence does not account for changes in
the corresponding labels. Recent approaches have also proposed to regularize
the output distribution of a neural network by label
smoothing~\citep{szegedy2016rethinking}, or penalizing high-confidence softmax
distributions~\citep{pereyra2017regularizing}. These methods bear similarities
with \mixup{} in the sense that supervision depends on multiple smooth labels,
rather than on single hard labels as in traditional ERM. However, the label
smoothing in these works is applied or regularized independently from the
associated feature values.

\mixup{} enjoys several desirable aspects of previous data augmentation and
regularization schemes without suffering from their drawbacks. Like the method
of~\cite{devries2017dataset}, it does not require significant domain knowledge.
Like label smoothing, the supervision of every example is not overly dominated
by the ground-truth label. Unlike both of these approaches, the \mixup{}
transformation establishes a linear relationship between data augmentation and
the supervision signal. We believe that this leads to a strong regularizer that
improves generalization as demonstrated by our experiments. The linearity
constraint, through its effect on the derivatives of the function approximated,
also relates \mixup{} to other methods such as Sobolev training of neural
networks~\citep{czarnecki2017sobolev} or WGAN-GP~\citep{gulrajani2017improved}.
