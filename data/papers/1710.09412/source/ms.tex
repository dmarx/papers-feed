\pdfoutput=1

\documentclass{article}
\usepackage{iclr2018}
\usepackage{times}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{hyphenat}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\mixup}{\emph{mixup}}
\newcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\lstdefinestyle{mypython}{
  language=python,
  breaklines=true,
  basicstyle=\fontsize{8.5}{13}\selectfont\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
}

\iclrfinalcopy

\title{\mixup{}: Beyond Empirical Risk Minimization}

\author{%
Hongyi Zhang \\
MIT
\And
Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz\thanks{Alphabetical order.} \\
FAIR
}

\begin{document}
    \maketitle
    \begin{abstract}
        Large deep neural networks are powerful, but exhibit undesirable
        behaviors such as memorization and sensitivity to adversarial examples.
        In this work, we propose \mixup{}, a simple learning principle to
        alleviate these issues. In essence, \mixup{} trains a neural network on
        convex combinations of pairs of examples and their labels.  By doing
        so, \mixup{} regularizes the neural network to favor simple linear
        behavior in-between training examples.  Our experiments on the
        ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets
        show that \mixup{} improves the generalization of state-of-the-art
        neural network architectures.  We also find that \mixup{} reduces the
        memorization of corrupt labels, increases the robustness to adversarial
        examples, and stabilizes the training of generative adversarial
        networks.
    \end{abstract}
    \input{introduction}
    \input{method}
    \input{experiments}
	\input{related}
    \input{conclusion}
    {\small
        \bibliography{mixup}
        \bibliographystyle{iclr2018}}
\end{document}
