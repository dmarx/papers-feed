\section{Introduction}

Large deep neural networks have enabled breakthroughs in fields such as
computer vision \citep{krizhevsky2012imagenet}, speech
recognition~\citep{hinton2012deep}, and reinforcement
learning~\citep{silver2016mastering}.  In most successful applications, these
neural networks share two commonalities.  First, they are trained as to
minimize their average error over the training data, a learning rule also known
as the Empirical Risk Minimization (ERM) principle \citep{vapnik98}.  Second,
the size of these state-of-the-art neural networks scales linearly with the
number of training examples.  For instance, the network of
\citet{springenberg2014striving} used $10^6$ parameters to model the $5 \cdot
10^4$ images in the CIFAR-10 dataset, the network of \citep{simonyan2014very}
used $10^8$ parameters to model the $10^6$ images in the ImageNet-2012 dataset,
and the network of \citet{chelba2013one} used $2 \cdot 10^{10}$ parameters to
model the $10^9$ words in the One Billion Word dataset. 

Strikingly, a classical result in learning theory \citep{vapnik1971uniform}
tells us that the convergence of ERM is guaranteed as long as the size of the
learning machine (e.g., the neural network) does not increase with the number
of training data. Here, the size of a learning machine is measured in terms of
its number of parameters or, relatedly, its VC-complexity
\citep{harvey2017nearly}.

This contradiction challenges the suitability of ERM to train our current
neural network models, as highlighted in recent research.  On the one hand, ERM
allows large neural networks to \emph{memorize} (instead of \emph{generalize}
from) the training data even in the presence of strong regularization, or in
classification problems where the labels are assigned at random
\citep{2016arXiv161103530Z}. On the other hand, neural networks trained with
ERM change their predictions drastically when evaluated on examples just
outside the training distribution \citep{SzegedyZSBEGF13}, also known as
\emph{adversarial examples}.  This evidence suggests that ERM is unable to
explain or provide generalization on testing distributions that differ
\emph{only slightly} from the training data. However, what is the alternative
to ERM? 

The method of choice to train on similar but different examples to the training
data is known as \emph{data augmentation} \citep{simard1998transformation},
formalized by the Vicinal Risk Minimization (VRM) principle \citep{vicinal}. In
VRM, human knowledge is required to describe a \emph{vicinity} or neighborhood
around each example in the training data. Then, additional \emph{virtual}
examples can be drawn from the vicinity distribution of the training examples
to enlarge the support of the training distribution. For instance, when
performing image classification, it is common to define the vicinity of one
image as the set of its horizontal reflections, slight rotations, and mild
scalings.  While data augmentation consistently leads to improved
generalization \citep{simard1998transformation}, the procedure is
dataset-dependent, and thus requires the use of expert knowledge.  Furthermore,
data augmentation assumes that the examples in the vicinity share the same
class, and does not model the vicinity relation across examples of different
classes.

\paragraph{Contribution} Motivated by these issues, we introduce a simple and
data-agnostic data augmentation routine, termed \mixup{}
(Section~\ref{sec:mixup}).  In a nutshell, \mixup{} constructs virtual training
examples 
\begin{align*}
  \tilde{x} &= \lambda x_i + (1 - \lambda) x_j,\qquad \text{where~} x_i, x_j \text{~are~raw~input~vectors}\\
  \tilde{y} &= \lambda y_i + (1 - \lambda) y_j,\qquad \text{where~} y_i, y_j \text{~are~one-hot~label~encodings}
\end{align*}
$(x_i, y_i)$ and $(x_j, y_j)$ are two examples drawn at random from our
training data, and $\lambda \in [0,1]$. Therefore, \mixup{} extends the
training distribution by incorporating the prior knowledge that linear
interpolations of feature vectors should lead to linear interpolations of
the associated targets.  \mixup{} can be implemented in a few lines of code,
and introduces minimal computation overhead. 

Despite its simplicity, \mixup{} allows a new state-of-the-art performance in
the CIFAR-10, CIFAR-100, and ImageNet-2012 image classification datasets
(Sections~\ref{sec:imagenet} and \ref{sec:cifar}). Furthermore, \mixup{}
increases the robustness of neural networks when learning from corrupt labels
(Section~\ref{sec:corrupt}), or facing adversarial examples
(Section~\ref{sec:adversarial}).  Finally, \mixup{} improves generalization on
speech (Sections~\ref{sec:speech}) and tabular (Section~\ref{sec:uci}) data,
and can be used to stabilize the training of GANs (Section~\ref{sec:gans}). The
source-code necessary to replicate our CIFAR-10 experiments is available at:
\begin{center}
\url{https://github.com/facebookresearch/mixup-cifar10}.
\end{center}
To understand the effects of various design choices in \mixup{}, we conduct a thorough set of ablation study experiments (Section~\ref{sec:ablation}). The results suggest that \mixup{} performs significantly better than related methods in previous work, and each of the design choices contributes to the final performance. We conclude by exploring the connections to prior work
(Section~\ref{sec:related}), as well as offering some points for discussion
(Section~\ref{sec:discussion}).
