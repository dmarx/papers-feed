\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei et~al.(2016)Amodei, Ananthanarayanan, Anubhai, Bai, Battenberg,
  Case, Casper, Catanzaro, Cheng, Chen, et~al.]{amodei2016deep}
D.~Amodei, S.~Ananthanarayanan, R.~Anubhai, J.~Bai, E.~Battenberg, C.~Case,
  J.~Casper, B.~Catanzaro, Q.~Cheng, G.~Chen, et~al.
\newblock Deep speech 2: End-to-end speech recognition in {E}nglish and
  {M}andarin.
\newblock In \emph{ICML}, 2016.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
D.~Arpit, S.~Jastrzebski, N.~Ballas, D.~Krueger, E.~Bengio, M.~S. Kanwal,
  T.~Maharaj, A.~Fischer, A.~Courville, Y.~Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock \emph{ICML}, 2017.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
P.~Bartlett, D.~J. Foster, and M.~Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{NIPS}, 2017.

\bibitem[Chapelle et~al.(2000)Chapelle, Weston, Bottou, and Vapnik]{vicinal}
O.~Chapelle, J.~Weston, L.~Bottou, and V.~Vapnik.
\newblock Vicinal risk minimization.
\newblock \emph{NIPS}, 2000.

\bibitem[Chawla et~al.(2002)Chawla, Bowyer, Hall, and
  Kegelmeyer]{chawla2002smote}
N.~V. Chawla, K.~W. Bowyer, L.~O. Hall, and W.~P. Kegelmeyer.
\newblock {SMOTE}: synthetic minority over-sampling technique.
\newblock \emph{Journal of artificial intelligence research}, 16:\penalty0
  321--357, 2002.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{chelba2013one}
C.~Chelba, T.~Mikolov, M.~Schuster, Q.~Ge, T.~Brants, P.~Koehn, and
  T.~Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{arXiv}, 2013.

\bibitem[Cisse et~al.(2017)Cisse, Bojanowski, Grave, Dauphin, and
  Usunier]{cisse2017parseval}
M.~Cisse, P.~Bojanowski, E.~Grave, Y.~Dauphin, and N.~Usunier.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock \emph{ICML}, 2017.

\bibitem[Czarnecki et~al.(2017)Czarnecki, Osindero, Jaderberg, {\'S}wirszcz,
  and Pascanu]{czarnecki2017sobolev}
W.~M. Czarnecki, S.~Osindero, M.~Jaderberg, G.~{\'S}wirszcz, and R.~Pascanu.
\newblock Sobolev training for neural networks.
\newblock \emph{NIPS}, 2017.

\bibitem[DeVries \& Taylor(2017)DeVries and Taylor]{devries2017dataset}
T.~DeVries and G.~W. Taylor.
\newblock Dataset augmentation in feature space.
\newblock \emph{ICLR Workshops}, 2017.

\bibitem[Drucker \& Le~Cun(1992)Drucker and Le~Cun]{drucker1992improving}
H.~Drucker and Y.~Le~Cun.
\newblock Improving generalization performance using double backpropagation.
\newblock \emph{IEEE Transactions on Neural Networks}, 3\penalty0 (6):\penalty0
  991--997, 1992.

\bibitem[Goodfellow(2016)]{goodfellow2016nips}
I.~Goodfellow.
\newblock Tutorial: Generative adversarial networks.
\newblock \emph{NIPS}, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock \emph{NIPS}, 2014.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{ICLR}, 2015.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock Accurate, large minibatch {SGD}: Training {I}mage{Net} in 1 hour.
\newblock \emph{arXiv}, 2017.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{graves2013speech}
A.~Graves, A.-r. Mohamed, and G.~Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{ICASSP}. IEEE, 2013.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~Courville.
\newblock Improved training of {W}asserstein {GAN}s.
\newblock \emph{NIPS}, 2017.

\bibitem[Harvey et~al.(2017)Harvey, Liaw, and Mehrabian]{harvey2017nearly}
N.~Harvey, C.~Liaw, and A.~Mehrabian.
\newblock Nearly-tight {VC}-dimension bounds for piecewise linear neural
  networks.
\newblock \emph{JMLR}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock \emph{ECCV}, 2016.

\bibitem[Hein \& Andriushchenko(2017)Hein and Andriushchenko]{hein2017formal}
M.~Hein and M.~Andriushchenko.
\newblock Formal guarantees on the robustness of a classifier against
  adversarial manipulation.
\newblock \emph{NIPS}, 2017.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, et~al.]{hinton2012deep}
G.~Hinton, L.~Deng, D.~Yu, G.~E. Dahl, A.-r. Mohamed, N.~Jaitly, A.~Senior,
  V.~Vanhoucke, P.~Nguyen, T.~N. Sainath, et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{IEEE Signal Processing Magazine}, 2012.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{huang2017densely}
G.~Huang, Z.~Liu, L.~van~der Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock \emph{CVPR}, 2017.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Image{Net} classification with deep convolutional neural networks.
\newblock \emph{NIPS}, 2012.

\bibitem[Lecun et~al.(2001)Lecun, Bottou, Bengio, and Haffner]{lecun98}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of IEEE}, 2001.

\bibitem[Lichman(2013)]{uci}
M.~Lichman.
\newblock {UCI} machine learning repository, 2013.

\bibitem[Liu(2017)]{cifar-pytorch}
K.~Liu, 2017.
\newblock URL \url{https://github.com/kuangliu/pytorch-cifar}.

\bibitem[Pereyra et~al.(2017)Pereyra, Tucker, Chorowski, Kaiser, and
  Hinton]{pereyra2017regularizing}
G.~Pereyra, G.~Tucker, J.~Chorowski, {\L}.~Kaiser, and G.~Hinton.
\newblock Regularizing neural networks by penalizing confident output
  distributions.
\newblock \emph{ICLR Workshops}, 2017.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei.
\newblock Image{Net} large scale visual recognition challenge.
\newblock \emph{IJCV}, 2015.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock \emph{Nature}, 2016.

\bibitem[Simard et~al.(1998)Simard, LeCun, Denker, and
  Victorri]{simard1998transformation}
P.~Simard, Y.~LeCun, J.~Denker, and B.~Victorri.
\newblock Transformation invariance in pattern recognitionâ€”tangent distance
  and tangent propagation.
\newblock \emph{Neural networks: tricks of the trade}, 1998.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{ICLR}, 2015.

\bibitem[Springenberg et~al.(2015)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenberg2014striving}
J.~T. Springenberg, A.~Dosovitskiy, T.~Brox, and M.~Riedmiller.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{ICLR Workshops}, 2015.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~E. Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{SzegedyZSBEGF13}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~J. Goodfellow, and
  R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{ICLR}, 2014.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the {I}nception architecture for computer vision.
\newblock \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem[Vapnik(1998)]{vapnik98}
V.~N. Vapnik.
\newblock \emph{Statistical learning theory}.
\newblock J. Wiley, 1998.

\bibitem[Vapnik \& Chervonenkis(1971)Vapnik and
  Chervonenkis]{vapnik1971uniform}
V.~Vapnik and A.~Y. Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock \emph{Theory of Probability and its Applications}, 1971.

\bibitem[Veit(2017)]{dense-andreas}
A.~Veit, 2017.
\newblock URL \url{https://github.com/andreasveit}.

\bibitem[Warden(2017)]{commands}
P.~Warden, 2017.
\newblock URL
  \url{https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html}.

\bibitem[Xie et~al.(2016)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2016aggregated}
S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock \emph{CVPR}, 2016.

\bibitem[Zagoruyko \& Komodakis(2016{\natexlab{a}})Zagoruyko and
  Komodakis]{Zagoruyko2016WRN}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock \emph{BMVC}, 2016{\natexlab{a}}.

\bibitem[Zagoruyko \& Komodakis(2016{\natexlab{b}})Zagoruyko and
  Komodakis]{wide-sergey}
S.~Zagoruyko and N.~Komodakis, 2016{\natexlab{b}}.
\newblock URL \url{https://github.com/szagoruyko/wide-residual-networks}.

\bibitem[{Zhang} et~al.(2017){Zhang}, {Bengio}, {Hardt}, {Recht}, and
  {Vinyals}]{2016arXiv161103530Z}
C.~{Zhang}, S.~{Bengio}, M.~{Hardt}, B.~{Recht}, and O.~{Vinyals}.
\newblock {Understanding deep learning requires rethinking generalization}.
\newblock \emph{ICLR}, 2017.

\bibitem[Zhang(2017)]{random-labels}
C.~Zhang, 2017.
\newblock URL \url{https://github.com/pluskid/fitting-random-labels}.

\bibitem[Zhong et~al.(2017)Zhong, Zheng, Kang, Li, and Yang]{zhong2017random}
Z.~Zhong, L.~Zheng, G.~Kang, S.~Li, and Y.~Yang.
\newblock Random erasing data augmentation.
\newblock \emph{arXiv}, 2017.

\end{thebibliography}
