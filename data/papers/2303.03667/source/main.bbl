\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{ali2021xcit}
Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze,
  Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob
  Verbeek, et~al.
\newblock Xcit: Cross-covariance image transformers.
\newblock {\em Advances in neural information processing systems},
  34:20014--20027, 2021.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{cai2022efficientvit}
Han Cai, Chuang Gan, and Song Han.
\newblock Efficientvit: Enhanced linear attention for high-resolution
  low-computation visual recognition.
\newblock {\em arXiv preprint arXiv:2205.14756}, 2022.

\bibitem{chen2022tvconv}
Jierun Chen, Tianlang He, Weipeng Zhuo, Li Ma, Sangtae Ha, and S-H~Gary Chan.
\newblock Tvconv: Efficient translation variant convolution for layout-aware
  visual processing.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12548--12558, 2022.

\bibitem{chen2021cyclemlp}
Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo.
\newblock Cyclemlp: A mlp-like architecture for dense prediction.
\newblock {\em arXiv preprint arXiv:2107.10224}, 2021.

\bibitem{chen2022mobile}
Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan,
  and Zicheng Liu.
\newblock Mobile-former: Bridging mobilenet and transformer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5270--5279, 2022.

\bibitem{chen2019drop}
Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus
  Rohrbach, Shuicheng Yan, and Jiashi Feng.
\newblock Drop an octave: Reducing spatial redundancy in convolutional neural
  networks with octave convolution.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3435--3444, 2019.

\bibitem{chollet2017xception}
Fran{\c{c}}ois Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1251--1258, 2017.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition workshops}, pages 702--703, 2020.

\bibitem{dai2021coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V Le, and Mingxing Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock {\em Advances in Neural Information Processing Systems},
  34:3965--3977, 2021.

\bibitem{ding2022scaling_short}
Xiaohan Ding et~al.
\newblock Scaling up your kernels to 31x31: Revisiting large kernel design in
  cnns.
\newblock In {\em CVPR}, 2022.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{elsken2019neural}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Neural architecture search: A survey.
\newblock {\em The Journal of Machine Learning Research}, 20(1):1997--2017,
  2019.

\bibitem{gao2022convmae}
Peng Gao, Teli Ma, Hongsheng Li, Jifeng Dai, and Yu Qiao.
\newblock Convmae: Masked convolution meets masked autoencoders.
\newblock {\em arXiv preprint arXiv:2205.03892}, 2022.

\bibitem{graham2021levit}
Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin,
  Herv{\'e} J{\'e}gou, and Matthijs Douze.
\newblock Levit: a vision transformer in convnet's clothing for faster
  inference.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 12259--12269, 2021.

\bibitem{haase2020rethinking_short}
Daniel Haase et~al.
\newblock Rethinking depthwise separable convolutions: How intra-kernel
  correlations lead to improved mobilenets.
\newblock In {\em CVPR}, 2020.

\bibitem{han2020ghostnet}
Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu.
\newblock Ghostnet: More features from cheap operations.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 1580--1589, 2020.

\bibitem{han2020model}
Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, and Tong Zhang.
\newblock Model rubikâ€™s cube: Twisting resolution, depth and width for
  tinynets.
\newblock {\em Advances in Neural Information Processing Systems},
  33:19353--19364, 2020.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2961--2969, 2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{he2022tackling}
Tianlang He, Jiajie Tan, Weipeng Zhuo, Maximilian Printz, and S-H~Gary Chan.
\newblock Tackling multipath and biased training data for imu-assisted ble
  proximity detection.
\newblock In {\em IEEE INFOCOM 2022-IEEE Conference on Computer
  Communications}, pages 1259--1268. IEEE, 2022.

\bibitem{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{howard2019searching}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 1314--1324, 2019.

\bibitem{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{hu2019local}
Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin.
\newblock Local relation networks for image recognition.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3464--3473, 2019.

\bibitem{huang2018condensenet}
Gao Huang, Shichen Liu, Laurens Van~der Maaten, and Kilian~Q Weinberger.
\newblock Condensenet: An efficient densenet using learned group convolutions.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2752--2761, 2018.

\bibitem{huang2016deep}
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em European conference on computer vision}, pages 646--661.
  Springer, 2016.

\bibitem{huang2022lightvit}
Tao Huang, Lang Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu.
\newblock Lightvit: Towards light-weight convolution-free vision transformers.
\newblock {\em arXiv preprint arXiv:2207.05557}, 2022.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems}, 25, 2012.

\bibitem{krogh1991simple}
Anders Krogh and John Hertz.
\newblock A simple weight decay can improve generalization.
\newblock {\em Advances in neural information processing systems}, 4, 1991.

\bibitem{li2021micronet}
Yunsheng Li, Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Lu Yuan,
  Zicheng Liu, Lei Zhang, and Nuno Vasconcelos.
\newblock Micronet: Improving image recognition with extremely low flops.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 468--477, 2021.

\bibitem{li2022efficientformer}
Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov,
  Yanzhi Wang, and Jian Ren.
\newblock Efficientformer: Vision transformers at mobilenet speed.
\newblock {\em arXiv preprint arXiv:2206.01191}, 2022.

\bibitem{lian2021mlp}
Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao.
\newblock As-mlp: An axial shifted mlp architecture for vision.
\newblock {\em arXiv preprint arXiv:2107.08391}, 2021.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{liu2022partial}
Guilin Liu, Aysegul Dundar, Kevin~J Shih, Ting-Chun Wang, Fitsum~A Reda, Karan
  Sapra, Zhiding Yu, Xiaodong Yang, Andrew Tao, and Bryan Catanzaro.
\newblock Partial convolution for padding, inpainting, and image synthesis.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2022.

\bibitem{liu2018image}
Guilin Liu, Fitsum~A Reda, Kevin~J Shih, Ting-Chun Wang, Andrew Tao, and Bryan
  Catanzaro.
\newblock Image inpainting for irregular holes using partial convolutions.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 85--100, 2018.

\bibitem{liu2022we}
Ruiyang Liu, Yinghui Li, Linmi Tao, Dun Liang, and Hai-Tao Zheng.
\newblock Are we ready for a new paradigm shift? a survey on visual deep mlp.
\newblock {\em Patterns}, 3(7):100520, 2022.

\bibitem{liu2022swin}
Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li Dong, et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12009--12019, 2022.

\bibitem{liu2021swin}
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022, 2021.

\bibitem{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
  and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11976--11986, 2022.

\bibitem{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock {\em arXiv preprint arXiv:1608.03983}, 2016.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{lu2021soft}
Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao,
  Chunjing Xu, Tao Xiang, and Li Zhang.
\newblock Soft: softmax-free transformer with linear complexity.
\newblock {\em Advances in Neural Information Processing Systems},
  34:21297--21309, 2021.

\bibitem{ma2018shufflenet}
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
\newblock Shufflenet v2: Practical guidelines for efficient cnn architecture
  design.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 116--131, 2018.

\bibitem{maaz2022edgenext}
Muhammad Maaz, Abdelrahman Shaker, Hisham Cholakkal, Salman Khan, Syed~Waqas
  Zamir, Rao~Muhammad Anwer, and Fahad~Shahbaz Khan.
\newblock Edgenext: efficiently amalgamated cnn-transformer architecture for
  mobile vision applications.
\newblock {\em arXiv preprint arXiv:2206.10589}, 2022.

\bibitem{mehta2021mobilevit}
Sachin Mehta and Mohammad Rastegari.
\newblock Mobilevit: light-weight, general-purpose, and mobile-friendly vision
  transformer.
\newblock {\em arXiv preprint arXiv:2110.02178}, 2021.

\bibitem{mehta2022separable}
Sachin Mehta and Mohammad Rastegari.
\newblock Separable self-attention for mobile vision transformers.
\newblock {\em arXiv preprint arXiv:2206.02680}, 2022.

\bibitem{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock {\em arXiv preprint arXiv:1611.06440}, 2016.

\bibitem{nair2010rectified}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em Icml}, 2010.

\bibitem{pan2022edgevits}
Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz Dudziak, Hongsheng
  Li, Georgios Tzimiropoulos, and Brais Martinez.
\newblock Edgevits: Competing light-weight cnns on mobile devices with vision
  transformers.
\newblock {\em arXiv preprint arXiv:2205.03436}, pages 1--6, 2022.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem{sifre2014rigid}
Laurent Sifre and St{\'e}phane Mallat.
\newblock Rigid-motion scattering for texture classification.
\newblock {\em arXiv preprint arXiv:1403.1687}, 2014.

\bibitem{singh2019hetconv}
Pravendra Singh, Vinay~Kumar Verma, Piyush Rai, and Vinay~P Namboodiri.
\newblock Hetconv: Heterogeneous kernel-based convolutions for deep cnns.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4835--4844, 2019.

\bibitem{srinivas2021bottleneck}
Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel,
  and Ashish Vaswani.
\newblock Bottleneck transformers for visual recognition.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 16519--16529, 2021.

\bibitem{steiner2021train}
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob
  Uszkoreit, and Lucas Beyer.
\newblock How to train your vit? data, augmentation, and regularization in
  vision transformers.
\newblock {\em arXiv preprint arXiv:2106.10270}, 2021.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{tan2019mnasnet}
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2820--2828, 2019.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International conference on machine learning}, pages
  6105--6114. PMLR, 2019.

\bibitem{tan2021efficientnetv2}
Mingxing Tan and Quoc Le.
\newblock Efficientnetv2: Smaller models and faster training.
\newblock In {\em International Conference on Machine Learning}, pages
  10096--10106. PMLR, 2021.

\bibitem{tang2022quadtree}
Shitao Tang, Jiahui Zhang, Siyu Zhu, and Ping Tan.
\newblock Quadtree attention for vision transformers.
\newblock {\em arXiv preprint arXiv:2201.02767}, 2022.

\bibitem{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em Advances in Neural Information Processing Systems},
  34:24261--24272, 2021.

\bibitem{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  10347--10357. PMLR, 2021.

\bibitem{touvron2022deit}
Hugo Touvron, Matthieu Cord, and Herv{\'e} J{\'e}gou.
\newblock Deit iii: Revenge of the vit.
\newblock {\em arXiv preprint arXiv:2204.07118}, 2022.

\bibitem{ulyanov2016instance}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock {\em arXiv preprint arXiv:1607.08022}, 2016.

\bibitem{vaswani2021scaling}
Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake
  Hechtman, and Jonathon Shlens.
\newblock Scaling local self-attention for parameter efficient visual
  backbones.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12894--12904, 2021.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wadekar2022mobilevitv3}
Shakti~N Wadekar and Abhishek Chaurasia.
\newblock Mobilevitv3: Mobile-friendly vision transformer with simple and
  effective fusion of local, global and input features.
\newblock {\em arXiv preprint arXiv:2209.15159}, 2022.

\bibitem{wang2022shift}
Guangting Wang, Yucheng Zhao, Chuanxin Tang, Chong Luo, and Wenjun Zeng.
\newblock When shift operation meets vision transformer: An extremely simple
  alternative to attention mechanism.
\newblock {\em arXiv preprint arXiv:2201.10801}, 2022.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 568--578, 2021.

\bibitem{wen2022social}
Song Wen, Hao Wang, and Dimitris Metaxas.
\newblock Social ode: Multi-agent trajectory forecasting with neural ordinary
  differential equations.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXII}, pages 217--233.
  Springer, 2022.

\bibitem{wu2019fbnet}
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu,
  Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.
\newblock Fbnet: Hardware-aware efficient convnet design via differentiable
  neural architecture search.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10734--10742, 2019.

\bibitem{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 3--19, 2018.

\bibitem{xia2022trt_short}
Xin Xia et~al.
\newblock Trt-vit: Tensorrt-oriented vision transformer.
\newblock {\em arXiv preprint}, 2022.

\bibitem{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem{yang2021condensenet}
Le Yang, Haojun Jiang, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang, and Qi
  Tian.
\newblock Condensenet v2: Sparse feature reactivation for deep networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3569--3578, 2021.

\bibitem{yu2022metaformer}
Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi
  Feng, and Shuicheng Yan.
\newblock Metaformer is actually what you need for vision.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10819--10829, 2022.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032, 2019.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{zhang2020split}
Qiulin Zhang, Zhuqing Jiang, Qishuo Lu, Jia'nan Han, Zhengxin Zeng, Shang-Hua
  Gao, and Aidong Men.
\newblock Split to be slim: An overlooked redundancy in vanilla convolution.
\newblock {\em arXiv preprint arXiv:2006.12085}, 2020.

\bibitem{zhang2017interleaved}
Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang.
\newblock Interleaved group convolutions.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 4373--4382, 2017.

\bibitem{zhang2018shufflenet}
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6848--6856, 2018.

\bibitem{zhong2022tree}
Shuhan Zhong, Sizhe Song, Guanyao Li, and S-H~Gary Chan.
\newblock A tree-based structure-aware transformer decoder for image-to-markup
  generation.
\newblock In {\em Proceedings of the 30th ACM International Conference on
  Multimedia}, pages 5751--5760, 2022.

\bibitem{zhuo2022semi}
Weipeng Zhuo, Ka~Ho Chiu, Jierun Chen, Jiajie Tan, Edmund Sumpena, S-H~Gary
  Chan, Sangtae Ha, and Chul-Ho Lee.
\newblock Semi-supervised learning with network embedding on ambient rf signals
  for geofencing services.
\newblock {\em arXiv preprint arXiv:2210.07889}, 2022.

\bibitem{zoph2016neural}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.01578}, 2016.

\end{thebibliography}
