\section{Fused Communication Collectives}
In this section, we describe bla bla bla \aj{TODO}

All communication collectives provided by NCCL can be conceptually described using a series of three primitives: (i) \texttt{SEND} that sends data from one rank to other, (ii) \texttt{RECV} that receives data from another rank, and (iii) \texttt{REDUCE} that performs reduction based on a reduction operator.
For example, in \reduce each rank first \texttt{SEND}s data, then the destination rank \texttt{RECV}s data, and finally the destination rank performs \texttt{REDUCE} on the data received.
In distributed model training, it is common that pointwise computations are performed on the data received after communication collectives, or the input to communication collectives are output of a pointwise computations.
For example, in synchronous Adam the average of all gradients distributed among different ranks is obtained using \allreduce and several pointwise computations are performed to do parameter update.
Existing techniques writes the output of communication collectives to a memory location, loads from that location and performs the computation.
Hence, this requires one store for storing the output and one load for loading the output.

However, we can remove these extra stores and loads by fusing the pointwise computations with any of the primitives, thereby passing the results of communication collectives to the computation from registers instead.
For example, 

\begin{table*}
  \small
  % \begin{subtable}{\textwidth}
    \begin{tabular}[]{|l|l|l|l|l|}\hline
      \thead{Computation with\\ Communication Collective} & Series of Primitives & Fused Communication Collective\\ \hline
      $f_2($\reduce($f_1(t)$,redOp)) & $f_1(t)\rightarrow$ SEND $\rightarrow$ RECV $\rightarrow$ $f_2$(REDUCE(redOp)) & FusedReduce(t, $f_1$, redOp, $f_2$)\\ \hline
      $f_2($\broadcast($f_1(t)$)) & $f_1(t)\rightarrow$ SEND $\rightarrow$ $f_2$(RECV) & FusedBroadcast(t, $f_1$, $f_2$)\\ \hline
      $f_2($\allreduce($f_1(t)$,redOp)) & \thead{$f_1(t)\rightarrow$SEND $\rightarrow$ RECV $\rightarrow$ $f_2$(REDUCE(redOp)) \\$\rightarrow$ SEND $\rightarrow$ RECV} & FusedAllReduce(t, $f_1$, redOp, $f_2$)\\ \hline
      $f_2($\reducescatter($f_1(t)$,redOp)) &  $f_1(t)\rightarrow$ SEND $\rightarrow$ RECV $\rightarrow$ $f_2$(REDUCE(redOp)) & FusedReduceScatter(t, $f_1$, redOp, $f_2$) \\ \hline
      $f_2($\allgather($f_1(t)$)) & $f_1(t)\rightarrow$SEND $\rightarrow$ $f_1$(RECV) & FusedAllGather(t, $f_1$, $f_2$)\\ \hline

    \end{tabular}

  \caption{Computation over Communication Collectives can be described in a series of SEND, RECV, and REDUCE primitives. We formalize these primitives into Fused Communication Collectives.}
\end{table*}

