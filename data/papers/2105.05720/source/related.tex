\section{Related Work}
\label{sec:related}

\spara{Distributed Machine Learning Abstractions}
Existing machine learning frameworks~\cite{mxnet,tensorflow,jia2014caffe,pytorch,sergeev2018horovod} and DSLs~\cite{tvm18,distributed-halide} provide abstractions for writing distributed machine learning workloads.
Similar to \tool, in these abstractions, a distributed machine learning program takes input tensors, performs operations on tensors, and returns tensors as the output.
However, unlike these abstractions, \tool preserves the layout information for each tensor.
The layout information enables \tool to perform static type checking of each operation, and automatically perform transformations on the program, which is not possible with existing abstractions.

\spara{Distributed Neural Network Training} 
Several works have improved data-, model-, and pipeline-parallel techniques for both training and inference.
Mesh-Tensorflow \cite{meshtensorflow} and GShard~\cite{gshard} create \emph{shards} of weights and model state that can be split among ranks.
Horovod~\cite{sergeev2018horovod} introduced the \emph{Tensor Fusion} optimization that copies all gradients to a single buffer of 64MB, calls \allreduce on the buffer, and then copies the updated value to original gradients.
ZeRO~\cite{zero} splits weights and model state among ranks and uses \reducescatter and \allgather to distribute computation.
FlexFlow~\cite{flexflow} performs operator splitting as a way to represent both data-parallelism and model-parallelism, but does not optimize computation with communication.
\tool provides several optimizations over these works that are possible only by breaking the abstraction: (i) scattered tensors that remove extra storage and memory copy operations, (ii) fusion communication collectives, and (iii) novel communication and computation overlapping techniques.
PyTorch's DDP~\cite{pytorch-ddp} overlaps \allreduce of gradients with the forward and backward pass.
However, unlike \tool, PyTorch's DDP requires extra memory for overlapping, which can increase training time for very large models~\cite{megatronlm-github} and do not support slicing of optimizer parameter update that significantly decrease memory usage.
GPipe~\cite{gpipe}, Pipedream~\cite{pipedream}, and Narayanan et al.~\cite{narayanan2021efficient} proposed pipeline training to improve model parallelism, by dividing the forward and backward pass into several mini-batches, which are then pipelined across devices.
vPipe~\cite{vpipe} improves these works by providing higher GPU utilization.
\tool improves on these works by overlapping inter and intra-node communication operations.
BytePS~\cite{osdi20:byteps} utilizes CPU in heterogenous clusters to improve training, which is complementary to \tool.

\spara{Optimizing Stencil Computations}
Prior works have proposed several DSLs and optimizations for data-parallel stencil computations on CPUs, GPUs, and other accelerators.
Halide~\cite{halide} and Fireiron~\cite{fireiron} separate the algorithm and schedule, which describes the optimizations like fusion, and loop tiling.
TVM~\cite{tvm18} extends Halide for generating optimized compute kernels. 
\textsc{Lift}~\cite{lift-cgo18,lift-cgo17} and PolyMage~\cite{polymage-gpu} automatically optimize stencil computations for a single GPU.
Distributed-Halide~\cite{distributed-halide} extends Halide with scheduling primitives that allow distributing parallelizable dimensions of loops.
\tool extends these works to reason about and compose collective communication with computation, which is crucial for distributed machine learning scenarios.

\spara{Overlapping Computation and Communication}
State-of-the-art works on overlapping \cite{Barigou2017, KOZIRIS20031138,7336201,10.1145/1810085.1810091,10.1007/978-3-319-58667-0_18} use either pipelined execution to overlap communication and computation or non-blocking MPI operations.
Pencil~\cite{sc20:pencil} improves upon these works by performing pipelining within a process and supports computations in multiple connected iteration spaces.
Several techniques distribute tiles and automatically generate communication~\cite{10.1145/2503210.2503289, distributed-halide,8121995}.
Basu et. al.~\cite{6799131} uses overlapped tiling in each process to remove communication between processes.
Denis and Trahay~\cite{7573826} studied the efficiency of overlap.
dCUDA~\cite{dcuda} provides hardware supported overlap.
These works for MPI+OpenMP are valid for CPU based stencil computations that require sends and receives to share the halo regions.
However, unlike \tool, these works do not support overlapping between collectives communication and complex computations like convolutions and matrix multiplications.
% ACE~\cite{isca21saeed} is a novel DL collective communication accelerator to enable overlap of communication and computation.
\tool supports overlapping multiple computation and communication operations on GPUs without an accelerator.

% , ~\cite{6687341}, ~\cite{gpurdma}, ~\cite{gpunet}, ~\cite{10.1145/3126908.3126950}