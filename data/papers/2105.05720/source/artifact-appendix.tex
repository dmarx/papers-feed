
\appendix
\section{Artifact Appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Abstract}

This artifact appendix describes how to reproduce results for
standalone experiments in Figure~\ref{fig:bandwidth64GPUs},
~\ref{fig:matmul-overlap}, and ~\ref{fig:pipeline-overlap} and integration 
results in Section~\ref{sec:experiments:bert},~\ref{sec:experiments:model-parallel:integeration}, and ~\ref{sec:experiments:pipeline-parallel:integeration}.
This artifact includes the \tool{} DSL and compiler, and \tool{}'s generated code integrated 
with PyTorch, Megatron-LM, and NVIDIA Bert.
To reproduce the results, the experiments should be executed on a system similar to our experimental system.
However, all experiments can be executed on a system with more than one NVIDIA GPUs.

\subsection{Artifact Check-list (Meta-information)}

{\small
\begin{itemize}
  \item {\bf Program:} \tool{} DSL and compiler written in C++.
  \item {\bf Compilation:} A C++ compiler (\texttt{g++} or \texttt{clang}) to compile \tool{}. A C++ compiler with MPI support (\texttt{mpicxx}) and CUDA compiler (\texttt{nvcc}) to compile generated programs.
  \item {\bf Binary:} Each \tool{} program compiles to a binary that generates an MPI program containing CUDA kernels.
  \item {\bf Data set:} BERT, GPT-2, and GPT-3 training datasets for integration experiments. 
  % We provide instructions to download datasets.
  \item {\bf Run-time environment:} Ubuntu 20.04 with Python 3.7+, CUDA 11.0+, and OpenMPI 4.0+.
  \item {\bf Hardware:} We performed experiments on 16 NVIDIA DGX-2 nodes, i.e., a total of 256 NVIDIA Tesla V100 GPUs. 
  However, the experiments can be executed on any system with two or more GPUs.
  % The functionality of standalone experiments (Figure~\ref{fig:bandwidth64GPUs},~\ref{fig:matmul-overlap}, and ~\ref{fig:pipeline-overlap}) can be tested with as low as 2 GPUs, however, the integration experiments (Section~\ref{sec:experiments:bert},~\ref{sec:experiments:model-parallel:integeration}, and ~\ref{sec:experiments:pipeline-parallel:integeration}) requires atleast 16 DGX-2 nodes to run.
  \item {\bf Run-time state:} Python, MPI, and CUDA.
  \item {\bf Execution:} Use \texttt{mpirun} to run the experiments.
  \item {\bf Metrics:} Decrease in execution time of benchmarks.
  \item {\bf Output:} Execution time of each experiment and \tool{} speedup over baselines.
  \item {\bf Experiments:} Execution of standalone experiments and training and inference tasks of BERT, GPT-2, and GPT-3 models.
  \item {\bf How much disk space required (approximately)?:} 100 GB in total. 90\% of the space usage is required for storing dataset.
  \item {\bf How much time will be spent in preparing the workflow (approximately)?:} 1 hour.
  \item {\bf How much time is needed to complete experiments (approximately)?:} 5 hours.
  \item {\bf Publicly available?:} Yes.
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description}

\subsubsection{How to Access}

The \tool{} implementation and the benchmarking infrastructure used in our evaluation are publicly available as the artifact~\cite{coconet-artifact}. 
This artifact contains a zip file with two directories: (i) \texttt{coconet}, which is the implementation of \tool{}, and (ii) \texttt{coconet-experiments}, which is the benchmarking infrastructure.
Latest versions of these directories are available at \url{https://github.com/parasailteam/coconet} and \url{https://github.com/parasailteam/coconet-experiments}.

\subsubsection{Hardware Dependencies}
All benchmarks can be executed on a distributed system with two or more NVIDIA GPUs.
However, our results will be reproducible on the evaluation system described in Section~\ref{sec:experiments}.

\subsubsection{Software Dependencies}
Our experiments require a system running Ubuntu 20.04 with Python 3.8+ and CUDA 11.0+.
Prerequisites and their installation procedure is described in \texttt{README.md} files of \texttt{coconet} and \texttt{coconet-experiments} directories.

\subsubsection{Data Sets}
The standalone benchmarks (Figure~\ref{fig:bandwidth64GPUs},~\ref{fig:matmul-overlap}, and ~\ref{fig:pipeline-overlap}) do not require any dataset.
Datasets required for executing experiments in Section~\ref{sec:experiments:bert},~\ref{sec:experiments:model-parallel:integeration}, and ~\ref{sec:experiments:pipeline-parallel:integeration} can be obtained by following \textit{Dataset} section of \texttt{README.md} in \texttt{coconet-experiments}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Installation}
Following instructions have been tested with Ubuntu 20.04.

\paragraph{Standalone Experiments Dependencies} Install dependencies by following the \textit{Prerequisites} section in \texttt{README.md} file of \texttt{coconet} directory.

\paragraph{Integration Experiments Dependencies} Follow the \textit{Prerequisites} section in \texttt{README.md} file of \texttt{coconet-experiments} directory to build PyTorch and install all dependencies for Megatron-LM and NVIDIA Bert.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment Workflow}
\subsubsection{Standalone Experiments}
\label{appendix:sec-standalone}
This section describe how to execute standalone experiments of Section~\ref{sec:experiments} and produce results for Figure~\ref{fig:bandwidth64GPUs}, Figure~\ref{fig:matmul-overlap}, and Figure~\ref{fig:pipeline-overlap}.
All of these experiments will take 1 hour combined.
\begin{enumerate}
  \item Install all \tool prerequisites in \texttt{coconet/README.md}.
  \item The \texttt{experiments/} directory contains all scripts for standalone experiments.

{\footnotesize
\begin{lstlisting}[language=bash]
$ cd coconet/experiments/
\end{lstlisting}
}

  \item Since all our experiments uses MPI to run the executable on all GPUs,  set the environment variable \texttt{NPROC} to the number of GPUs in the system. In our experiments, we set \texttt{NPROC} to 256 as follows:
  
  {\footnotesize
\begin{lstlisting}[language=bash]
$ export NPROC=256
\end{lstlisting}
}
\textbf{Note}: Setting \texttt{NPROC} to a value more than the number of GPUs in a system can lead to failed experiments.

  \item If the experiments are performed on a system with multiple nodes then additional arguments to \texttt{mpirun} can be passed by setting the \texttt{MPI\_ARGS} environment variable.
\end{enumerate}

\paragraph{Data-Parallel Experiments}
\begin{enumerate}
  \item To execute standalone data parallel experiments execute \texttt{data-parallel-exp.py}. This script takes a directory to store the results as an argument.
  Additionally, the script requires \texttt{MASTER\_ADDR} and \texttt{MASTER\_PORT} to be passed as \texttt{MPI\_RUN\_ARGS}. If the experiments are done on a single system, then it is common to set \texttt{MASTER\_ADDR=127.0.0.1} and \texttt{MASTER\_PORT=10000}.

  {\footnotesize
\begin{lstlisting}[language=bash]
$ export MPI_ARGS="-x MASTER_ADDR=127.0.0.1"
$ export MPI_ARGS="$MPI_ARGS -x MASTER_PORT=10000"
$ python data-parallel-exp.py results/
\end{lstlisting}
}

  The above execution of script will execute all data parallel executables and store the results in the \texttt{results} directory.

  \item Generate both graphs of Figure~\ref{fig:bandwidth64GPUs} by executing the script \texttt{gen-data-parallel-graphs.py}. This script takes the directory with results generated in the previous step as an argument.
  
{\footnotesize
\begin{lstlisting}[language=bash]
$ python gen-data-parallel-graphs.py results/
\end{lstlisting}
}

Graphs are stored in two files of \texttt{experiments} directory: \texttt{results-adam-fp16.pdf} and \texttt{results-lamb-fp16.pdf}.
\end{enumerate}

\paragraph{Model-Parallel Experiments}
\begin{enumerate}
  \item To execute standalone model-parallel experiments execute \texttt{model-parallel-exp.py}. Similar to the previous script, this script also takes a directory to store results as its argument.
  
  {\footnotesize
\begin{lstlisting}[language=bash]
$ python model-parallel-exp.py results/
\end{lstlisting}
}

  The script will execute all model parallel executables and stores the results in the \texttt{results} directory.
  \item Generate Figure~\ref{fig:matmul-overlap} by executing following script. This script will take above results directory as its argument. 
  
{\footnotesize
\begin{lstlisting}[language=bash]
$ python gen-model-parallel-graphs.py results/
\end{lstlisting}
}

Graph is stored as \texttt{results-model-parallel.pdf}.
\end{enumerate}

\paragraph{Pipeline-Parallel Experiments}
\begin{enumerate}
  \item To execute standalone pipeline-parallel experiments execute \texttt{pipeline-parallel-exp.py}. This script also requires a directory to store results as its command line argument.

  {\footnotesize
\begin{lstlisting}[language=bash]
$ python pipeline-parallel-exp.py results/
\end{lstlisting}
}

  Above execution of the script will execute all pipeline parallel executables and store the results in \texttt{results} directory.
  \item To generate Figure~\ref{fig:pipeline-overlap} execute the script \\ \texttt{gen-pipeline-parallel-graphs.py}. This script takes the directory containing above results as its argument.
  
{\footnotesize
\begin{lstlisting}[language=bash]
$ python gen-pipeline-parallel-graphs.py results/
\end{lstlisting}
}

The graph is stored in \texttt{results-model-parallel.pdf}.
\end{enumerate}

\subsubsection{Integration Experiments}
\label{appendix:sec-integration}
In this section, we will execute the integration experiments of Section~\ref{sec:experiments:bert},~\ref{sec:experiments:model-parallel:integeration}, and~\ref{sec:experiments:pipeline-parallel:integeration}.

\paragraph{Prerequisites} Install prerequisites and obtain dataset by following the steps in \texttt{coconet-experiments/README.md}.

\paragraph{Data-Parallel Training}
Go to \texttt{Nvidia-Bert} directory and execute \texttt{coconet-experiments.py}.

{\footnotesize
\begin{lstlisting}[language=bash]
$ cd NV-BERT 
$ python coconet-experiments.py
\end{lstlisting}
}

This script will execute data parallel training experiments and then print Table~\ref{tab:bert-results}.
This experiment will take 1 hour to complete.
This script contains maximum batch sizes supported by each implementation for our evaluation system of 256 Tesla V100 GPUs.
It is possible that for a different system the maximum batch size will be different.
The batch size dictionary in \texttt{coconet-experiments.py} can be modified to find maximum batch size for underlying system.

\paragraph{Model-Parallel Inference} 
Go to \texttt{MegatronLM-Model-Parallel} directory and execute \texttt{coconet-experiments.py}.
{\footnotesize
\begin{lstlisting}[language=bash]
$ cd MegatronLM-Model-Parallel
$ python coconet-experiments.py
\end{lstlisting}
}
This script will execute model parallel inference experiments and then print the values in Section~\ref{sec:experiments:model-parallel:integeration}.
This experiment will take less than 30 minutes to complete.

\paragraph{Pipeline-Parallel Inference} 
Execute \texttt{coconet-experiments.py} in the directory
\texttt{MegatronLM-Pipeline-Parallel}.
{\footnotesize
\begin{lstlisting}[language=bash]
$ cd MegatronLM-Pipeline-Parallel
$ python coconet-experiments.py
\end{lstlisting}
}
This script will execute pipeline parallel inference experiments and then print the table in Section~\ref{sec:experiments:pipeline-parallel:integeration}.
This experiment will take 3 hour to complete.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation and Expected Results}

\paragraph{Standalone Experiments} The figures generated by the experiments of Section~\ref{appendix:sec-standalone} can be matched with the figures: \ref{fig:bandwidth64GPUs}, \ref{fig:matmul-overlap}, and \ref{fig:pipeline-overlap}.

\paragraph{Integration Experiments}
The results generated in experiments of Section~\ref{appendix:sec-integration} can be matched with the results in Section~\ref{sec:experiments:bert},~\ref{sec:experiments:model-parallel:integeration}, and~\ref{sec:experiments:pipeline-parallel:integeration}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Methodology}

% Submission, reviewing and badging methodology:

% \begin{itemize}
%   \item \url{https://www.acm.org/publications/policies/artifact-review-badging}
%   \item \url{http://cTuning.org/ae/submission-20201122.html}
%   \item \url{http://cTuning.org/ae/reviewing-20201122.html}
% \end{itemize}
