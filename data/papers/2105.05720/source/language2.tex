\section{The \tool DSL}
\label{sec:dsl}
The \tool DSL extends the data representation in existing machine learning frameworks and provides constructs to express both computation and communication. 
The \tool DSL is embedded in C++.
Unifying the expression of computation and communication for distributed machine learning in the same DSL is the foundation to enable optimizations across computation and communication.

In this paper, we follow the MPI~\cite{mpi} terminology: \texttt{RANK} is the process ID of a distributed process, \texttt{GROUP} is a set of concurrent distributed processes, and \WORLD is the \texttt{GROUP} that includes all processes.
\tool supports dividing consecutive ranks into one or more process groups.

\begin{figure}[t]
  \footnotesize
%  \begin{subfigure}{0.49\textwidth}
  \begin{lstlisting}[language=DSL]
Tensor w(FP16, [H,H], Sliced(0), WORLD, RANK); |\label{line:mp:inputensor-begin}||\label{line:mp:continuous-tensor}|
Tensor b(FP16, [H], Replicated, WORLD); |\label{line:mp:inputensor-end}| 
Tensor in(FP16, [B,S,H], Sliced(2), WORLD, RANK);
Tensor r(FP16, [B,S,H], Replicated, WORLD);

// layer(FP16, [B,S,H], Local, WORLD, RANK)
Var layer = MatMul(in, w); |\label{line:mp:matmul}|
// sum(FP16, [B,S,H], Replicated, WORLD)
Var sum = AllReduce("+", layer); |\label{line:mp:allreduce}|
// dropout(FP16, [B,S,H], Replicated, WORLD)
Var dropout = Dropout(sum + b, 0.1); |\label{line:mp:pointwise-start}|
// out(FP16, [B,S,H], Replicated, WORLD)
Var out = dropout + r;|\label{line:mp:pointwise-end}|

Execute self_attention({w,in,b,r}, {out});
\end{lstlisting}
	\caption{An example program in \tool. \newline
		(\texttt{B}: batch size, \texttt{S}: sequence length, \texttt{H}: hidden dimension size)
	}
  \vspace{-2em}
    \label{fig:traditional-mp}
\end{figure}

\subsection{Tensor Layout}
\tool extends the concept of a tensor in machine learning frameworks from a single device data into distributed forms.
Besides item datatype, like \texttt{FP32} and \texttt{FP16}, and shape,
a \tool tensor also includes a \emph{layout} that describes the distributed allocation of tensor's data across a set of ranks. 
There are three layouts for a tensor: \emph{sliced}, \emph{replicated}, and \emph{local}.
A \emph{sliced} tensor is equally distributed among all nodes in a group along a specified dimension with \texttt{RANK} identifying the slice for that process.
% (first dimension by default). 
For example, in Figure~\ref{fig:traditional-mp}, which describes the Megatron-LM~\cite{megatronlm} model parallel logic of Self-Attention layer in \tool, \texttt{w} is sliced among all ranks in \WORLD in the 
first dimension and \texttt{in} is sliced in the third dimension.
A tensor can also be \emph{replicated} across all ranks in a group where it has the same value on each rank and it does not have a rank identifier.
For example, the bias \texttt{b} and the residual connection \texttt{r} are replicated as shown in Figure~\ref{fig:traditional-mp}. 
A \emph{local} tensor has same shape on all ranks but different
values on all ranks. A local tensor requires \texttt{RANK} to 
identify the values. For example, in Figure~\ref{fig:traditional-mp}, \texttt{layer}
is a local tensor that represents the result of MatMul operation.
A \texttt{Scalar} is a zero-dimensional tensor that represents a variable available on all ranks.
We discuss the layout of 
intermediate tensors in the next section.

\subsection{\tool's Operations}
A \tool program inherits the concept of data-flow graph (DFG) from existing machine learning frameworks with operations as vertices and data dependencies as edges.
Operations in \tool can be classified as (i) local computations, such as pointwise computations, matrix multiplication, and convolution, and (ii) cross rank communication operations, such as \allreduce, \allgather, and P2P \send-\recv.
%\tool follows numpy broadcast semantics and operation includes a matrix and a vector~\cite{numpymvm} \sm{this didn't make sense}.
Table~\ref{tab:operations} shows all operations supported by \tool{}.

A \texttt{Var} represents the intermediate tensor obtained after performing an operation. 
In the example of Figure~\ref{fig:traditional-mp}, the linear layer's weight (\texttt{w}) and the input
(\texttt{in}) are sliced across all ranks while the bias (\texttt{b}) and residual
(\texttt{r}) are replicated on all ranks. A \texttt{Var}'s shape and distribution layout are 
inferred based on the operation and inputs to the operation. For example,
line~\ref{line:mp:matmul} performs a MatMul operation on the input (\texttt{in}) and weights (\texttt{w}).
Since MatMul between two sliced tensors produces a local tensor, \texttt{layer} represents the partial result with \textit{local} layout.
At line~\ref{line:mp:allreduce}, \allreduce computes the sum of \texttt{layer} of all ranks and returns a \textit{replicated} tensor with the same values on each rank.
The computations at lines~\ref{line:mp:pointwise-start}--\ref{line:mp:pointwise-end} add the bias, use dropout as an activation, and add the residual. 
%Note that \texttt{b} and \texttt{sum} have shapes \texttt{[H]} and \texttt{[B,S,H]}, respectively.
At line~\ref{line:mp:pointwise-start}, the addition of \texttt{sum} and \texttt{b} follows
PyTorch's broadcast semantics\footnote{https://pytorch.org/docs/stable/notes/broadcasting.html} by replicating \texttt{b} in all dimensions of \texttt{sum}. 
Thus, the shape and layout of output of these operations are same as \texttt{sum}.
Finally, \texttt{Execute} defines the name, inputs, and outputs of the program. 

\begin{table}
  \small
  \caption{Operations supported by \tool includes all common communication and computation operations. \label{tab:operations}}
  \begin{tabular}{|c|l|}
    \hline
    \textbf{Communication} & AllReduce, AllGather, ReduceScatter, \\
    \textbf{Operations}    & Reduce, Broadcast, P2P Send-Recv \\\hline
    \textbf{Layers} & Matrix Multiplication, Convolution\\ \hline
    \textbf{Activations} & Dropout, tanh, ReLU \\ \hline
    \textbf{Tensor} & $+$, $-$, $*$, $\div$, Norm, ReduceTensor,\\
    \textbf{Operations} & Sqrt, Pow, Update\\\hline 
  \end{tabular}
  \par \bigskip% Do not remove this. Without this there is no space between the caption and the table
\end{table}

% In {\tool}, an input {\tt Tensor} has its distributed property tagged as \Sliced or \Replicated by user. 
% A \texttt{Var} has its distributed property and shape inferred from the operation producing it.
% For example, in \emph{MatMul} operation in Figure~\ref{fig:traditional-mp}, \texttt{layer}'s shape is [B, S, H] and exists locally (without any distribution information) as the sliced dimension in input tensors has vanished after computation. 
% %\TODO{or in \emph{Partial} property?}
% And according to the definition of \allreduce communication, \texttt{sum} is \replicated as the output is replicated across ranks. 
% %\sm{what is a tag? this paragraph is rough}

\subsection{Fused Collective Communication Operations}
\label{sec:fuse-comm-coll}
\tool enables efficient computations on the output of communication by providing fused collective communication operations, such as FusedAllReduce.
% To enable efficient computations on the output of communication, \tool extends all communication collectives to  fused communication collectives, such as FusedAllReduce.
Consider the \allreduce in Figure~\ref{fig:traditional-mp} followed by a Dropout (lines ~\ref{line:mp:allreduce}--\ref{line:mp:pointwise-start}).
The abstraction in existing machine learning frameworks requires the output of \allreduce 
to be stored in memory and then re-loaded by Dropout.
FusedAllReduce avoids such stores and loads by directly passing the output of communication to following computations through registers.
In addition to the argument of \allreduce, a FusedAllReduce takes computations as extra arguments.
Section~\ref{sec:code-gen-fused} discusses the implementation of Fused Collective Communication Operations.

\subsection{Overlapping Operations}
\label{sec:overlap-comm-coll}
\tool supports overlapping multiple dependent computation and communication operations using the \texttt{Overlap} construct.
For example, consecutive MatMul and \allreduce in Figure~\ref{fig:traditional-mp} 
(lines~\ref{line:mp:matmul}--\ref{line:mp:allreduce}) can be overlapped to fully utilize both network and computation resources.
Section~\ref{sec:overlap-impl} discusses the implementation of this construct.

\subsection{Custom Operations}
In \tool, the implementation of an operator needs to define three key properties of the operator: (i) syntax, (ii) semantics, and (iii) code generation.
The syntax of an operator is defined using C++ constructors and the semantics are defined by implementing rules to describe the layout and size of the output tensor based on the input tensors.
% This implementation of semantics is also used to validate the transformations.
Finally, the code generation requires implementing a function to generate a call to existing libraries or generate fused GPU kernels.
The implementation of syntax and semantics can be achieved in a few lines of code, however, implementing the code generation for complex operations like Matrix Multiplication and Convolution can potentially take hundreds of lines of code.
Fortunately, in practice the code generation for complex operations can call an optimized implementation of existing libraries.


\iffalse
\subsection{Communication Collectives}
\TODO{may move to section implementation}
In addition to the \allreduce primitive mentioned above, communication libraries like NCCL~\cite{nccl}, 
support several collective communications based on the MPI standard~\cite{mpi}. 
%NVIDIA Collective Communication Library~\cite{nccl} (NCCL) is a communication library for NVIDIA GPUs.
%NCCL provides communication primitive such as send and recv, and communication collectives to communicate between NVIDIA GPUs.
%Moreover, these functions follows MPI standards~\cite{mpi} and terminology.
%In the rest of the paper, 
The communication collectives in NCCL 
%NCCL provides five communication collectives and each 
take an input buffer $b_i$ of size $N_i$ and writes to an output buffer $b_o$ of size $N_o$.
\emph{\allreduce} performs a reduction operation on $b_i$ and leaves identical copies of $b_o$ on all ranks.
\emph{\allgather} gathers all $N_i$ values of $b_i$ from all ranks to $b_o$, such that, $N_o = N_i \times |\WORLD|$.
\emph{\reducescatter} performs a reduction operation on $b_i$ and scatter the result among all ranks in $b_o$, such that, $N_o = N_i \div |\WORLD|$.
\emph{\reduce} takes a root rank $r$ and performs reduction on $b_i$ and only writes the result to $b_o$ of $r$.
\emph{\broadcast} takes a root rank $r$. It copies $N_i$ values of $b_i$ of rank $r$ and leaves identical copies in $b_o$ of all ranks.
\fi

\iffalse 

%\paragraph{Example Tranformations}
\subsection{Equivalent Programs using Transformations}
\sm{this subsection provides nothing, I say we remove it.}
The way to express the same algorithm in \tool DSL is not unique.
%Previous work~\cite{mariannmt,zero,gshard} has observed that distributing the computation among ranks leads to better performance and lower memory usage for large models.
%These works 
For example, as shown in Figure~\ref{fig:reducescatter}, existing works~\cite{zero,gshard} distributes the computation by first doing \reducescatter to divide the summed values among all rank, then perform computation on the divided tensor, and finally performs \allgather to share the output of computation.
%as shown in Figure~\ref{fig:reduce-scatter-sgd}.
%Figure~\ref{fig:reduce-scatter-sgd} shows this optimization applied to model parallel update.
%While a user can specify this directly, the goal of \tool is to enable the transformations from Figure~\ref{fig:traditional-mp} to Figure~\ref{fig:reduce-scatter-sgd} and vice-versa through the scheduling language described in Section~\ref{sec:schedule}. 
Next section describes several output-invariant transformation between different concrete programs.
\fi

% For example, in Figure~\ref{fig:reduce-scatter-sgd}
% the contents of \lstinline[language=DSL]{m_} contains the values of
% \lstinline[language=DSL]{m} after {\em (i)} executing its arguments and
% {\em (ii)} storing the result of that computation in \lstinline[language=DSL]{m}.

\iffalse
\tool performs operations on 1-dimensional arrays, we refer to as Tensors.
\tool supports two types of operations that takes one or more tensors as input and returns one tensor.
First type is computations. 
\tool supports pointwise computations, where $i^{th}$ element of the output depends on the $i^{th}$ element of inputs, and reductions over a tensor using \texttt{ReduceTensor} construct.
Moreover, \tool supports casting of one element type to another.
This casting is useful for several scenarios like mixed precision training.
Second type are communication collectives supported by NCCL~\cite{nccl}.
A program written in \tool forms a Directed Acyclic Graph (DAG) of stages, where each \emph{stage} performs one or more computations and stage represent the output of the computation.
The properties of a stage, such as the size in each dimension, element type, are determined using the semantics of the computation.

Figure~\ref{fig:traditional-sgd} implements  Adam in \tool.
Tensors input to the program, such as gradient and weight, are defined 
using the \texttt{Tensor} (lines~\ref{line:adam:inputensor-begin}--\ref{line:adam:inputensor-end}).

Each tensor has four properties associated with it:
\begin{enumerate}
        \item \textbf{Element Type} of the tensor, such as, integers and floats.
        \item \textbf{Size}, i.e., the number of elements in the tensor.
        %  Currently \tool only supports single dimension tensors. However, we have found that this restriction does not prevent \tool from supporting a wide range of applications.
        \item \textbf{Set of Ranks} that stores this tensor in their memory. 
        A special set \WORLD contains all ranks.
        \item \textbf{Layout} \tool supports three layouts of a tensor.
        A \emph{\complete{}} tensor of size $N$ is stored on one or more ranks in a memory location of $N$ elements.
        A \emph{\replicated} tensor is a complete tensor that is stored on all ranks in \WORLD with each rank containing same contents.
        Since a \replicated tensor is a special kind of \complete tensor, \tool allows conversion of \replicated tensor to a \complete tensor.
        A \emph{\sliced{}} tensor of size $N$ stored on all ranks in \WORLD, is equally distributed among all ranks, such that, $i^{th}$ rank stores the $i^{th}$ part of tensor.
\end{enumerate}


Single dimension tensors that are input to the program and are available on all ranks are represented using \texttt{Scalar}.
Line~\ref{line:adam:scalars} defines \texttt{lr}, \texttt{beta1}, \texttt{beta2} as variables.
Each communication collective returns a \texttt{Stage} object. 
Lines~\ref{line:adam:avg} uses \allreduce to do elementwise reduction by summing all $i^{th}$ elements of \texttt{g} stored on all ranks and uses \texttt{avg} to represent the mean of these values.
Pointwise computations involving arithmetic, comparison, and bitwise operators can be expressed in a natural manner.
Pointwise computations are valid only when either both tensors are of same size and have same layout because operation is done on corresponding elements of both tensors, or when one or more tensor is a variable, hence, operation is done on the variable and each element of the tensor.
Lines~\ref{line:adam:pointwise-begin}--\ref{line:adam:pointwise-end} expresses pointwise computations to do the parameter update.
Finally, a \texttt{Pipeline} construct is used to define all inputs to the program and outputs of the program, which \tool uses to create the DAG stages.
\texttt{Pipeline::genCode} method generates CUDA code for all stages and links to NCCL API.
With default schedule, \tool generated code for  Adam is shown in Figure~\ref{fig:FusedSGD}.
% \paragraph{Tensor Functions}
% \tool provides two functions to retrieve a part of a tensor. First, the \texttt{at} function can be used to get an element at a particular index. Second, the \texttt{loadRank(Tensor\& t, int rank)} function returns the tensor stored at \texttt{rank}.
% \aj{I think a summary figure with all the above functions and their declarations will help? Or a figure with the grammar?}


% Addressed: \mm{Need to define {\it Continuous} tensors. I personally do not like the term Continuous, but its ok if we dont change it. Should we distinguish between Continuous tensors and "Replicated" tensors which have the same content in all nodes. AllReduce takes a Continuous Tensor and produces a Replicated Tensor. A Replicated Tensor {\it is a} Continuous tensor. We also need an operator that takes a Continuous tensor and produces a Tensor of a particular rank.}

\paragraph{Sliced Computations}

Many applications includes operation where $i^{th}$ rank performs computation on the $i^{th}$ part of the input to produce $i^{th}$ part of the output and then all parts of output can be combined using \allgather.
Recent techniques, such as Zero~\cite{zero}, utilizes this approach to do parameter update.
In \tool, we can express this algorithm as shown in Figure~\ref{fig:reduce-scatter-sgd}.
Lines~\ref{line:sliced-adam:inputensor-sliced-begin}--\ref{line:sliced-adam:inputensor-sliced-end} declares momentum and velocity tensors  
with the sliced layout.
At line~\ref{line:sliced-adam:reducescatter}, \reducescatter performs element-wise summation of gradient across the ranks to return a sliced stage.
Since momentum, velocity, and gradients are of sliced layout, we can perform all the pointwise computations.
However, before doing parameter update, we need to slice the parameters and then perform the computations(line~\ref{line:sliced-adam:sliced-parameter}).
Finally, an \allgather is introduced to gather all slices of updated parameters and store them in a \replicated layout (line~\ref{line:sliced-adam:allgather}).
\fi

%\tm{Describe update rule and relate to TF}


\iffalse

\subsubsection{Mixed Precisions}
% Mixed Precision neural network training~\cite{} is widely used to improve the training time.
% This technique involves computing gradients in 16-bit Half Precision Float, while parameters and other intermediates are stored in 32-bit Single Precision Float. 
% Using 16-bit Floats decreases the storage requirement and improves speed due to special computation that accelerates computation on 16-bit Floats, like Tensor Cores~\cite{}.
% \aj{Move above lines to background?}

\tool supports mixed precision applications where tensors can be of different type using its \texttt{Cast} construct, which cast all elements of the tensor to other type.
For example, in a parameter update that takes parameters in 32-bit Floats but gradients in 16-bit Floats, a \texttt{Cast} computation on gradients will cast all gradients from 16-bit Floats to 32-bit Floats.

\subsubsection{Tensor Reduction}
Many applications requires a reduction over tensors, such as, calculating the norm. 
For example, LAMB Optimizer~\cite{} calculates the norm of parameters and uses this norm in the parameter update. 
\tool provides \texttt{ReduceTensor} construct to perform a reduction computation on a tensor.
\texttt{ReduceTensor} supports several reduction operators, such as, sum, max, min.
\texttt{ReduceTensor} supports performing reduction on both \complete and sliced tensors and outputs a \texttt{Stage} containing a single element.
\fi

% \subsection{Model Parallelism in \tool}
% We now present the implementation of two widely used neural network optimizer Adam and LAMB in \tool.
% \aj{I think we should also have model parallelism here.}



\iffalse
\subsection{\tool's Functional Primitives}
\aj{probably needs to be removed}
\label{sec:functional-prmitives}
\newcommand{\List}[1]{\textit{List}[#1]}
\newcommand{\RepList}[1]{\textit{RepList}[#1]} 
\newcommand{\SlicedList}[1]{\textit{SlicedList}[#1]}
\newcommand{\LocList}[2]{\textit{LocList}_{#2}[#1]}
\newcommand{\func}[2]{#1 \rightarrow #2}
\newcommand{\lift}{\textit{lift}}
\newcommand{\funcdef}[3]{#1(#2) \rightarrow #3}

Given a program in \tool's DSL described above, \tool translates to a set of functional primitives. This provides a unified abstraction for computation and communication allowing us to define rewrite rules for transformations, validating the program using type checking rules, and generating code. 

We start with standard functional primitives. Starting with the base definition of $\List{T}$ as a vector of some type $T$, we define a tensor of $n$ dimensions recursively as a list of tensors of $n-1$ dimensions. Thus, a matrix is of type $\List{\List{T}}$ and so on. 
The function $\funcdef{map}{f:\func{T}{S}, l:\List{T})}{\List{S}}$ applies the function $f$ elementwise to the input list $l$. 
The function $\funcdef{fold}{l:\List{T}, f:\func{S\times T}{S}, a:S}{S}$ starts with the initial value $a$ and iteratively updates $a$ with $f(a,i)$ for each element $i$ in $l$. When $f$ is associative, we use $reduce$ instead of $fold$ to explicitly capture its parallel semantics.
 Another useful function is $zip$ that converts a pair of lists into a list of pairs. Also, we assume a $transpose$ function that permutes the dimension of a given tensor.   
We represent $map(f:\func{T}{S})$ as the `curried' version of $map$ that applies $f$ to an input $\List{T}$ and returns a $\List{S}$. Similarly, we use $\lift(f:\func{T\times S}{U})$ as the `zipped' version of the function that takes in a $\List{T}$ and $\List{S}$ and applies $f$ elementwise to return a $\List{U}$. For example, $\lift(+)$ represents vector addition, which allows us to define vector reduction as $reduce(lv: \List{\List{T}}, \lift(+), 0)$.

A key contribution of this work is to extend these functional primitives to work seamlessly
with distributed tensors. A $\RepList{T}$ represents a replicated tensor and a $\SlicedList{T}$ represents a sliced tensor. We also allow a tensor to be located at a particular rank $r$ with $\LocList{T}{r}$, with the function $rank$ returning the rank of such a list. Now we define primitives on these distributed tensors. 

The function $slice$ distributes a tensor along its first dimension to all the ranks in the \WORLD resulting in a sliced tensor. The function $join$ is its inverse. Similarly, the function $repl$ replicates a tensor to all the ranks in the \WORLD resulting in a replicated tensor, while $drop$ is its inverse. 
Finally, the function $store$ places a tensor at a particular rank, while $load$ removes this association. 
\begin{center}
$
\begin{array}{l}
\funcdef{slice}{l: \List{T}}{\SlicedList{T}}\\
\funcdef{join}{sl: \SlicedList{T}}{\List{T}}\\
\funcdef{repl}{l: \List{T}}{\RepList{T}}\\  
\funcdef{drop}{sl: \RepList{T}}{\List{T}}\\
\funcdef{store}{l: \List{T}, r: Int}{\LocList{T}{r}} \\
\funcdef{load}{ll: \LocList{T}{r}}{\List{T}}
\end{array}
$
\end{center}

These primitives allow us to capture the tensor computations and communication that occur in a distributed ML program. The base functional primitives allow us to capture tensor computations. For example, pointwise computations can be captured with $map$, dot product of two vectors with pointwise multiplication followed by a $reduce$, matrix-vector multiplication as a sequence of dot products, and so on. 

Lastly, our extensions for distributed tensors capture common communication collectives as shown below. 
These use the $reduce$ function on at least two dimensional sliced tensor.
$$\funcdef{reduce}{sl: \SlicedList{T}, f : (T \times T \rightarrow T)}{\List{T}}$$

\begin{figure}[H]
$
\begin{array}{l@{}l}
\text{Reduce}(sl: \SlicedList{T}, f, r) &: store(reduce(sl, f), r)\\
\text{Broadcast}(ll: \LocList{T}{r}) &: repl(load(ll))\\
\text{AllReduce}(sl: \SlicedList{T}, f) &: repl(reduce(sl, f))\\
\text{ReduceScatter}(sl: \SlicedList{T}, f) &: slice(reduce(sl, f))\\
\text{AllGather}(sl: \SlicedList{T}) &: repl(join(sl))\\
\end{array}
$
\end{figure}

% \begin{table*}
%   \small
%   % \begin{subtable}{\textwidth}
%     \begin{tabular}[]{|l|l|l|l|l|}\hline
%       \reduce & $l : List[Vector[T]] \rightarrow (f: T \rightarrow T \rightarrow T) \rightarrow List[Vector[T]] $  & $store(reduce(l, f), r)$ \\ \hline
%       \broadcast & $l : List[Vector[T]] \rightarrow RepList[Vector[T]] $  & $repl(l)$ \\ \hline
%       \allreduce & $l : List[Vector[T]] \rightarrow (f: T \rightarrow T \rightarrow T) \rightarrow RepList[Vector[T]] $  & $repl(reduce(l, f))$ \\ \hline
%       \reducescatter & $l : List[Vector[T]] \rightarrow (f: T \rightarrow T \rightarrow T) \rightarrow SliceList[Vector[T]] $ & $slice(reduce(l, f))$ \\ \hline
%       \allgather&$l : SliceList[Vector[T]] \rightarrow RepList[Vector[T]] $ & $repl(join(l))$ \\ \hline
%       TensorReduce & $l : RepList[Vector[T]] \rightarrow (f: Vector[T] \rightarrow T) \rightarrow RepList[T] $  & $repl(fold(head(l), f))$ \\ \hline
%       SlicedTensorReduce & $l : SlicedList[Vector[T]] \rightarrow (f: Vector[T] \rightarrow T) \rightarrow RepList[T] $  & $repl(fold(join(l), f))$ \\ \hline
%       Pointwise Function & $l: List[Vector[T]] \rightarrow (f: T \rightarrow T)$ & $map(map(f), l)$ \\ \hline
%       SlicedPointwise Function & $l: SliceList[Vector[T]] \rightarrow (f: T \rightarrow T)$ & $map(map(f), l)$ \\ \hline
%     \end{tabular}
%   \caption{\tool's constructs can be expressed using functional primitives\label{tab:constructs-to-primitives}}
% \end{table*}

% \begin{table*}
%   \small
%   % \begin{subtable}{\textwidth}
%     \begin{tabular}[]{|l@{}l|l|l|l|}\hline
%       \reduce & $:(l: \mathbf{List}, f: \mathbf{Op_r}, r: \mathbf{Int}) \rightarrow \mathbf{List}$ & $store(reduce(l, f), r)$ \\ \hline
%       \broadcast& $:(l: \mathbf{List})\rightarrow \mathbf{List}$ & $repl(l)$ \\ \hline
%       \allreduce& $:(l: \mathbf{List}, f: \mathbf{Op_r}) \rightarrow \mathbf{RepList}$ & $repl(reduce(l, f))$ \\ \hline
%       \reducescatter& $:(l: \mathbf{List}, f: \mathbf{Op_r}) \rightarrow \mathbf{SliceList}$&$slice(reduce(l, f))$ \\ \hline
%       \allgather&  $:(l: \mathbf{SliceList}) \rightarrow \mathbf{RepList}$& $repl(join(l))$ \\ \hline
%       TensorReduce& $:(l: \mathbf{RepList}, f: \mathbf{Op_r}) \rightarrow \mathbf{RepList}$ & $repl(fold(head(l), f))$ \\ \hline
%       Sliced TensorReduce& $:(l: \mathbf{SliceList}, f: \mathbf{Op_r}) \rightarrow \mathbf{RepList}$ &  $repl(fold(join(l), f))$ \\ \hline
%       Pointwise & $:(l: \mathbf{List}, f: \mathbf{F})\rightarrow \mathbf{List}$ & $map(map(f), l)$ \\ \hline
%       Sliced Pointwise& $:(l: \mathbf{SliceList}, f: \mathbf{F}) \rightarrow \mathbf{SliceList}$ & $map(map(f), l)$ \\ \hline
%     \end{tabular}
%   \caption{\tool's constructs can be expressed using functional primitives.
%   $List, SliceList$ and $RepList$ represents $List[Tensor[T]], SliceList[Tensor[T]]$ and $RepList[Tensor[T]]$ respectively.
%   $Op_r$ is type of a reduction function, i.e., $T \to T \to T$ and $F$ is type of a pointwise function, i.e., $T \to T$.
%   \label{tab:constructs-to-primitives}}
% \end{table*}
\fi

% \aj{Redo. Include stuff for Matmul}

%\subsection{Computation Validation}
%\label{sec:type-checking}
%\aj{Redo. Include stuff for Matmul}
%\tool performs validity of algorithm using \textit{static} checks on the functional primitives and generates \textit{dynamic} checks for the functional primitives representation.
%After converting the algorithm to functional primitives, the static checks are performed using the types of each primitive. 
%For example, $join$ takes a $SlicedList[T]$ as an argument and hence throws a type error if it is performed on a $LocList_r[T]$.
%Dynamic checks are generated to ensure that the rank parameter for $reduce$ is always in \WORLD and $load$ operation on $LocList_r[T]$ is valid when rank $r$ is in \WORLD.
%\tool generates code for these checks that uses a bit-vector representation of \WORLD.


