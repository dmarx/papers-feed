\documentclass[sigconf,screen]{acmart} 


% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{multirow}
\usepackage{makecell}
% inlined bib file
% \usepackage{filecontents}
\usepackage{enumitem}

% -------------------------------------------------------------------------------
% PARASAIL TEAM PACKAGES
%-------------------------------------------------------------------------------
%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\captionsetup{compatibility=false}

\newcommand{\asplossubmissionnumber}{1601}

\usepackage[normalem]{ulem}
\usepackage{xspace}
\usepackage{color}
\usepackage{listings}
\usepackage{tikz}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{mathpartir}
\usepackage{float}
\usepackage{textcomp}
\usepackage{pythonhighlight}
\usepackage{algorithm}
\usepackage{tabularx}
\usepackage{algpseudocode}% http://ctan.org/pkg/al
\usepackage{pervasives}
\usepackage{balance}

%%% If you see 'ACMUNKNOWN' in the 'setcopyright' statement below,
%%% please first submit your publishing-rights agreement with ACM (follow link on submission page).
%%% Then please update our instructions page and copy-and-paste the NEW commands into your article.
%%% Please contact us in case of questions; allow up to 10 min for the system to propagate the information.
%%%
%%% The following is specific to ASPLOS '22 and the paper
%%% 'Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads'
%%% by Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Madan Musuvathi, Olli Saarikivi, Saeed Maleki, Todd Mytkowicz, and Youshan Miao.
%%%
% \setcopyright{ACMUNKNOWN}
%setcopyright before submission (https://www.conference-publishing.com/Instructions.php?Event=ASPLOS22MAIN&Paper=dc7505a434aecf501da1e25d14ec02d294b684e8)

%%% The following is specific to ASPLOS '22 and the paper
%%% 'Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads'
%%% by Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madan Musuvathi, Todd Mytkowicz, and Olli Saarikivi.
%%%
\setcopyright{acmcopyright}
\acmPrice{15.00}
\acmDOI{10.1145/3503222.3507778}
\acmYear{2022}
\copyrightyear{2022}
\acmSubmissionID{asplos22main-p1601-p}
\acmISBN{978-1-4503-9205-1/22/02}
\acmConference[ASPLOS '22]{Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems}{February 28 -- March 4, 2022}{Lausanne, Switzerland}
\acmBooktitle{Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '22), February 28 -- March 4, 2022, Lausanne, Switzerland}


%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
% \title{\tool: Co-Optimizing Computation and Communication for Distributed Machine Learning}
\title[Breaking the Computation and Communication Abstraction Barrier in Distributed ML Workloads]{Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads}
%\title{Bridging the Computation-Communication Abstraction \\ to Co-optimize Distributed Machine Learning}

\author{Abhinav Jangda}
\affiliation{
    \institution{University of Massachusetts Amherst}
    \country{United States}
}
\author{Jun Huang}
\affiliation{
    \institution{Ohio State University}
    \country{United States}
}
\author{Guodong Liu}
\affiliation{
    \institution{Chinese Academy of Sciences}
    \country{China}
}
\author{Amir Hossein Nodehi Sabet}
\affiliation{%
  \institution{University of California, Riverside}
  \country{United States}
}
\author{Saeed Maleki}
\affiliation{%
  \institution{Microsoft Research}
  \country{United States}
}
\author{Youshan Miao}
\affiliation{%
  \institution{Microsoft Research}
  \country{China}
}
\author{Madanlal Musuvathi}
\affiliation{%
  \institution{Microsoft Research}
  \country{United States}
}
\author{Todd Mytkowicz}
\affiliation{%
  \institution{Microsoft Research}
  \country{United States}
}
\author{Olli Saarikivi}
\affiliation{%
  \institution{Microsoft Research}
  \country{United States}
}

\renewcommand{\shortauthors}{A. Jangda, J. Huang, G. Liu, A. H. N. Sabet, S. Maleki, Y. Miao, M. Musuvathi, T. Mytkowicz, O. Sarikivi}


% % %for single author (just remove % characters)
% \author{
% {\rm Abhinav Jangda}\\
% University of Massachusetts Amherst
% \and
% {\rm Jun Huang}\\
% Ohio State University
% \and
% {\rm Guodong Liu}\\
% Chinese Academy of Sciences
% \and
% {\rm Amir Hossein Nodehi Sabet}\\
% University of California, Riverside
% \and
% {\rm Madan Musuvathi}\\
% Microsoft Research
% \and
% {\rm Olli Saarikivi}\\
% Microsoft Research
% \and
% {\rm Saeed Maleki}\\
% Microsoft Research
% \and
% {\rm Todd Mytkowicz}\\
% Microsoft Research
% \and
% {\rm Youshan Miao}\\
% Microsoft Research
% } % end author

%-------------------------------------------------------------------------------
\begin{abstract}
Recent trends towards large machine learning models require both training and inference tasks to be distributed.
Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance.
However, the current logical separation between computation and communication kernels in machine learning
frameworks misses optimization opportunities across this barrier.
Breaking this abstraction can provide many optimizations to improve the performance 
of distributed workloads.
However, manually applying these optimizations requires modifying the underlying computation and communication libraries for each scenario, which is both time consuming and error-prone.

Therefore, we present \tool{}, which 
contains (i) a domain specific language to express a distributed machine learning program in the form of computation and communication operations, (ii) a set of semantics preserving transformations to optimize the program, and (iii) a compiler to generate jointly optimized communication and computation GPU kernels.
Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and
apply powerful optimizations, such as fusion or overlapping of communication and computation.
\tool 
enabled us to optimize data-, model- and pipeline-parallel workloads in large language models with only a few lines of code. 
Our experiments show that \tool 
significantly outperforms state-of-the-art distributed machine learning implementations.
\end{abstract}
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011050.10011017</concept_id>
<concept_desc>Software and its engineering~Domain specific languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011041</concept_id>
<concept_desc>Software and its engineering~Compilers</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010169</concept_id>
<concept_desc>Computing methodologies~Parallel computing methodologies</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Domain specific languages}
\ccsdesc[500]{Software and its engineering~Compilers}
\ccsdesc[500]{Computing methodologies~Parallel computing methodologies}

\keywords{Distributed Machine Learning, Collective Communication, MPI, CUDA, Code Generation, Compiler Optimizations}

\maketitle


% \settopmatter{printfolios=true}
%\settopmatter{printacmref=false} % Removes citation information below abstract


% \pagestyle{plain} % removes running headers


\input{introduction}
%\input{background}
%\input{motivation}
% \input{fused-comm-coll}
% \input{overview} %Overview has been integrated in the introduction
\input{language2}
\input{sched}
\input{runtime}
\input{experiments}
\input{related}
\input{conclusion}
\section{Data Availability Statement}
The artifact for this paper~\cite{coconet-artifact} contains the source code of our implementation of \tool and the benchmarking infrastructure to reproduce all the results in Section~\ref{sec:experiments}.

\begin{acks}
We thank the reviewers and our shepherd, Tyler Sorensen, for their constructive feedback.
This work was partially supported by the National Science Foundation grant CCF-2052696.
\end{acks}

\input{artifact-appendix}
\balance
%-------------------------------------------------------------------------------
\bibliographystyle{ACM-Reference-Format}
\interlinepenalty=10000
\bibliography{paper}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks
