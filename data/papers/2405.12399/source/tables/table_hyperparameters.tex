\begin{table}[h!]
\caption{Hyperparameters for \textsc{diamond}.}
\label{tbl_atari_hypers}
\begin{center}
% \resizebox{0.4 \columnwidth}{!}{
\begin{tabular}{ l c }
\multicolumn{1}{c}{\textbf{Hyperparameter}}  & \multicolumn{1}{c}{\textbf{Value}} \\ 

\hline \\

\multicolumn{2}{l}{\textbf{Training loop}} \\
Number of epochs & 1000 \\
Training steps per epoch & 400 \\
Batch size & 32 \\
Environment steps per epoch & 100 \\
Epsilon (greedy) for collection & 0.01 \\

\\

\multicolumn{2}{l}{\textbf{RL hyperparameters}} \\
Imagination horizon ($H$) & 15 \\
Discount factor ($\gamma$) &  0.985 \\ 
Entropy weight ($\eta$) & 0.001 \\ 
$\lambda$-returns coefficient ($\lambda$) & 0.95 \\

\\

\multicolumn{2}{l}{\textbf{Sequence construction during training}} \\
For $\mathbf{D}_\theta$, number of conditioning observations and actions ($L$) & 4 \\
For $R_\psi$, burn-in length ($B_{R}$), set to $L$ in practice & 4 \\ 
For $R_\psi$, training sequence length ($B_{R} + H$) & 19 \\
For $\pi_\phi$ and $V_\phi$, burn-in length ($B_{\pi,V}$), set to $L$ in practice & 4 \\

\\
\multicolumn{2}{l}{\textbf{Optimization}} \\
Optimizer & AdamW \\
Learning rate & 1e-4 \\
Epsilon & 1e-8 \\
Weight decay ($\mathbf{D}_\theta$) & 1e-2 \\
Weight decay ($R_\psi$) & 1e-2 \\
Weight decay ($\pi_\phi$ and $V_\phi$)  & 0 \\

% \hline \\
\\
\multicolumn{2}{l}{\textbf{Diffusion Sampling}} \\
Method &  Euler \\
Number of steps & 3 \\

\\
\multicolumn{2}{l}{\textbf{Environment}} \\
Image observation dimensions & 64$\times$64$\times$3 \\
Action space & Discrete (up to 18 actions) \\
Frameskip & 4 \\
Max noop & 30 \\
Termination on life loss & True \\
Reward clipping & $\{-1, 0, 1\}$ \\

\end{tabular}
% }
\end{center}
\end{table}
