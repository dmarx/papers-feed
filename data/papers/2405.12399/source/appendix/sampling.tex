\section{Sampling observations in \textsc{diamond}}
\label{appendix:sampling}

We describe here how we sample an observation $\x_t^0$ from our diffusion world model. We initialize the procedure with a noisy observation $\x_t^\Tau \sim p^{prior}$, and iteratively solve the reverse SDE in Equation \ref{eq:reverse_process} from $\tau = \Tau$ to $\tau = 0$, using the learned score model $\mathbf{S}_\theta(\x_t^\tau, \tau, \x_{<t}^0, a_{<t})$ conditioned on past observations $\x_{<t}^0$ and actions $a_{<t}$. This procedure is illustrated in Figure \ref{fig:architecture}.

In fact, there are many possible sampling methods for a given learned score model $\mathbf{S}_\theta$ \citep{karras2022elucidating}. Notably, \citet{song_sde} introduce a corresponding ``probability flow" ordinary differential equation (ODE), with marginals equivalent to the stochastic process described in Section \ref{subsec:diffusion}. In that case, the solving procedure is deterministic, and the only randomness comes from sampling the initial condition. In practice, this means that for a given score model, we can resort to any ODE or SDE solver, from simple first order methods like Euler (deterministic) and Euler–Maruyama (stochastic) schemes, to higher-order methods like Heun's method \citep{ascher1998computer}. 

Regardless of the choice of solver, each step introduces truncation errors, resulting from the local score approximation and the discretization of the continuous process. Higher order samplers may reduce this truncation error, but come at the cost of additional Number of Function Evaluations (NFE) -- how many forward passes of the network are required to generate a sample. This local error generally scales superlinearly with respect to the step size (for instance Euler's method is $\mathcal{O}(h^2)$ for step size $h$), so increasing the number of denoising steps improves the visual quality of the generated next frame. Therefore, there is a trade-off between visual quality and NFE that directly determines the inference cost of the diffusion world model.


% with t+1 instead of t

% \section{Sampling next observations in \textsc{diamond}}
% \label{appendix:sampling}

% We describe here how we sample a next observation $\x_{t+1}$ from our diffusion world model. We initialize the procedure with a noisy next observation $\x_{t+1}^\Tau \sim p^{prior}$, and iteratively solve the reverse SDE in Equation \ref{eq:reverse_process} from $\tau = \Tau$ to $\tau = 0$, using the learned score model $\mathbf{S}_\theta(\x_{t+1}^\tau, \tau, \x_{\le t}^0, a_{\le t})$ conditioned on past observations $\x_{\le t}^0$ and actions $a_{\le t}$. This procedure is illustrated in Figure \ref{fig:architecture}.

% In fact, there are many possible sampling methods for a given learned score model $S_\theta$ \citep{karras2022elucidating}. Notably, \citet{song_sde} introduce a corresponding ``probability flow" ordinary differential equation (ODE), with equivalent marginals. In that case, the solving procedure is deterministic, and the only randomness comes from sampling the initial condition. In practice, this means that for a given score model, we can resort to any ODE or SDE solver, from simple first order methods like Euler (deterministic) and Euler–Maruyama (stochastic) schemes, to higher-order methods like Heun's method \citep{ascher1998computer}. 

% Regardless of the choice of solver, each step introduces truncation errors, resulting from the local score approximation and the discretization of the continuous process. Higher order samplers may reduce this truncation error, but come at the cost of additional Number of Function Evaluations (NFE) -- how many forward passes of the network are required to generate a sample. This local error generally scales superlinearly with respect to the step size (for instance Euler's method is $\mathcal{O}(h^2)$ for step size $h$), so increasing the number of denoising steps improves the visual quality of the generated next frame. Therefore, there is a trade-off between visual quality and NFE that directly determines the inference cost of the diffusion world model.


