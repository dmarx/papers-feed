\section{Introduction}
\label{sec:introduction}

Generative models of environments, or ``world models" \citep{ha2018world}, are becoming increasingly important as a component for generalist agents to plan and reason about their environment \citep{lecun2022path}. Reinforcement Learning (RL) has demonstrated a wide variety of successes in recent years \citep{alphago,degrave2022magnetic,ouyang2022training}, but is well-known to be sample inefficient, which limits real-world applications. World models have shown promise for training reinforcement learning agents across diverse environments \citep{hafner2023dreamerv3,muzero2020}, with greatly improved sample-efficiency \citep{ye2021efficientzero}, which can enable learning from experience in the real world \citep{wu2023daydreamer}.

\input{images/figure1_tikz}

Recent world modeling methods \citep{hafner2021mastering,iris2023,robine2023transformer,hafner2023dreamerv3,zhang2023storm} often model environment dynamics as a sequence of discrete latent variables. Discretization of the latent space helps to avoid compounding error over multi-step time horizons. However, this encoding may lose information, resulting in a loss of generality and reconstruction quality. This may be problematic for more real-world scenarios where the information required for the task is less well-defined, such as training autonomous vehicles \citep{hu2023gaia}. In this case, small details in the visual input, such as a traffic light or a pedestrian in the distance, may change the policy of an agent. Increasing the number of discrete latents can mitigate this lossy compression, but comes with an increased computational cost \citep{iris2023}. 

In the meantime, diffusion models \citep{sohl2015difforigin,ho2020DDPM,song_sde} have become a dominant paradigm for high-resolution image generation \citep{ldm_stable_diffusion,podell2023sdxl}. This class of methods, in which the model learns to reverse a noising process, challenges well-established approaches modeling discrete tokens \citep{esser2021taming,ramesh2021zero,muse2023}, and thereby offers a promising alternative to alleviate the need for discretization in world modeling. Additionally, diffusion models are known to be easily conditionable and to flexibly model complex multi-modal distributions without mode collapse. These properties are instrumental to world modeling, since adherence to conditioning should allow a world model to reflect an agent's actions more closely, resulting in more reliable credit assignment, and modeling multi-modal distributions should provide greater diversity of training scenarios for an agent.

Motivated by these characteristics, we propose \textsc{diamond} (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. 
Careful design choices are necessary to ensure our diffusion world model is efficient and stable over long-time horizons, and we provide a qualitative analysis to illustrate their importance.
\textsc{diamond} achieves a mean human normalized score of 1.46 on the well-established Atari 100k benchmark; a new state of the art for agents trained entirely within a world model.
Additionally, operating in image space has the benefit of enabling our diffusion world model to be a drop-in substitute for the environment, which provides greater insight into world model and agent behaviors. 
In particular, we find the improved performance in some games follows from better modeling of critical visual details. 
To further demonstrate the effectiveness of our world model in isolation, we train \textsc{diamond}'s diffusion world model on $87$ hours of static \textit{Counter-Strike: Global Offensive} (CSGO) gameplay \citep{pearce2022counter} to produce an interactive neural game engine for the popular in-game map, \textit{Dust II}.
We release our code, agents and playable world models at \wslink.
