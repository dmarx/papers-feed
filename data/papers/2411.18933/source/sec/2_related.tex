% \vspace{-2mm}
\section{Related Work}
% \vspace{-2mm}
\label{sec:formatting}
%We now review relevant works on segmentation, vision transformers, and efficient attention.

%-------------------------------------------------------------------------
% \subsection{Video Object Segmentation}
{\bf Video Object Segmentation (VOS)} is a fundamental task in computer vision, segments objects of interest from the background and tracks target objects in a video. 
%Many research works have been proposed in this community on video object segmentation. 
In the unsupervised setting~\citep{grundmann2010efficient,brox2010object,lee2011key,xu2012evaluation,fragkiadaki2012video,perazzi2012saliency,zhang2013video,li2013video,papazoglou2013fast,faktor2014video,wang2015saliency,taylor2015causal,perazzi2016benchmark}, VOS models segment salient objects without a reference mask. In the semi-supervised setting~\citep{pont20172017,xu2018youtube,oh2019video,bhat2020learning,robinson2020learning,li2022recurrent,yang2022decoupling,cheng2022xmem,zhang2023joint,wang2023look,wu2023scalable,cheng2024putting,yang2024scalable}, VOS requires tracking and segmenting objects based on a first-frame mask of target objects. For interactive video object segmentation (iVOS)~\citep{caelles20182018,heo2020interactive,cheng2021modular,homayounfar2021videoclick,yang2023track,cheng2023segment,rajivc2023segment,cheng2024putting,delatolas2024learning}, iVOS models perform object segmentation in videos (masklets) with user guidance, e.g., clicks, bounding boxes, scribbles. In SAM 2~\citep{ravi2024sam}. Semi-supervised VOS and iVOS have been extended to promptable visual segmentation (PVS), where the model can be interactively prompted with different types of inputs such as clicks, boxes, and masks on any frame in a video for segmenting and tracking a valid object.
%-------------------------------------------------------------------------

% \subsection{Vision Transformers}
\noindent {\bf Vision Transformers (ViTs)} have achieved huge success on various vision tasks including image classification~\citep{dosovitskiy2020image}, object detection~\citep{li2022exploring}, image segmentation~\cite{cheng2022masked,kirillov2023segment}, video classification~\citep{fan2021multiscale}, and video object segmentation~\citep{duke2021sstvos,yang2023track}. The original ViT family scales from the efficient ViT-Tiny up to ViT-Huge, with a plain, non-hierarchical architecture. There are also hierarchical vision transformers that combine transformers with hierarchical stage structure, such as Swin~\citep{liu2021swin}, MViT~\citep{fan2021multiscale,li2022mvitv2}, PViT~\citep{wang2021pyramid}, and Hiera~\citep{ryali2023hiera}. While being successful, hierarchical models are usually slower than their plain ViT counterparts for practical deployment~\citep{ryali2023hiera}. 
Combining ViT with convolutions~\citep{lecun1989backpropagation} has been explored for fast hybrid models such as MobileViT~\citep{mehta2021mobilevit}, LeViT~\citep{graham2021levit},  EfficientFormer\citep{li2022efficientformer}, Next-ViT\citep{li2022next}, Tiny-ViT\citep{wu2022tinyvit}, Castling-ViT\citep{you2023castling}, EfficientViT~\citep{liu2023efficientvit}, and MobileNetv4~\citep{qin2024mobilenetv4}. This line of progression towards building efficient ViTs is orthogonal to our
EfficientTAM work towards building efficient video object segmentation. Following SAM~\citep{kirillov2023segment} and EfficientSAMs~\citep{xiong2024efficientsam}, we are pursuing plain ViT backbones for efficient video object segmentation and track anything tasks.  
%The community has also shown increasing interest in efficient vision transformers; \citep{touvron2021training} presented smaller ViTs such as ViT-Small and ViT-Tiny for complementing ViT-Huge, ViT-Large, and ViT-Base in \citep{dosovitskiy2020image}. 


%-------------------------------------------------------------------------
% \subsection{Efficient Attention}
\noindent {\bf Efficient Attention.} The field has developed methods to reduce the quadratic cost of standard self-attention with respect to input sequence length~\cite{attention_is_all_you_need}. 
Local windowed attention has been applied in \cite{beltagy2020longformer,zaheer2020bigbird} for reducing the complexity of self-attention. In \cite{shen2018efficient,katharopoulos-et-al-2020}, a linear dot product approximation is proposed to linearize the softmax matrix in self-attention by heuristically separating keys and queries. In \cite{choromanski2020rethinking}, the Performer model uses random features to approximate self-attention, achieving linear time and memory cost. Nystr\"{o}mformer in \cite{xiong2021nystromformer} makes use of the Nystr\"{o}m method to approximate self-attention with a linear cost. Linformer \cite{wang2020linformer} shows that self-attention is low-rank, which can be approximated by learning linear projection matrices for the keys and values. The approach of~\citep{liu2023efficientvit,you2023castling} leverages the associative property of matrix multiplication for efficient attentions in vision transformers. This direction has shown success and has achieved decent performance on vision tasks. However, in preliminary experiments we found that these methods underperformed in a memory cross-attention module when adapted for efficiency improvement.


%-------------------------------------------------------------------------
% \subsection{Segment Anything Model}
\noindent {\bf Segment Anything Model.} SAM~\citep{kirillov2023segment} is a vision foundation model that can segment any object in an image using interactive prompts such as points and bounding boxes. SAM has demonstrated remarkable zero-shot transfer performance and high versatility for many vision tasks including a broad range of segmentation applications~\citep{chen2023semantic,cen2023sad,deng2023segment,chen2023sam}, in-painting~\citep{yu2023inpaint}, image restoration~\citep{jiang2023restore}, image editing~\citep{gao2023editanything}, image shadow removal~\citep{zhang2023deshadow}, medical image segmentation~\citep{ma2023segment}, camouflaged object detection~\citep{tang2023can}, transparent object detection~\citep{han2023segment}, concept-based explanation~\citep{sun2023explain}, semantic communication~\citep{tariq2023segment}, and object tracking~\citep{cheng2023segment,yang2023track}. The strong ability on image segmentation with flexible prompts motivates the extension of SAM for video object segmentation and track anything. Track Anything Model (TAM)~\citep{yang2023track} combines SAM and XMem~\cite{cheng2022xmem} for interactive video object tracking and segmentation with SAM for frame segmentation and XMem for tracking. SAM-Track~\citep{cheng2023segment} perform object tracking and segmentation in videos by combining SAM~\citep{kirillov2023segment}, DeAOT~\citep{yang2022decoupling}, and Grounding-Dino~\citep{liu2023grounding}. The latest SAM 2~\citep{ravi2024sam} extended SAM for video segmentation through a hierarchical image encoder for frame embeddings and a memory module that conditions current frame embeddings on past frames. Motivated by mobile app use-cases and computationally-constrained applications, recent works have reduced the computational cost of SAM, such as MobileSAM~\citep{zhang2023faster}, FastSAM~\citep{zhao2023fast}, and EfficientSAM~\citep{xiong2024efficientsam}.
The present paper focuses on improving the efficiency challenges of SAM 2 for practical deployment of video object segmentation and track anything.  