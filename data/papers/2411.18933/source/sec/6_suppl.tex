% \clearpage
% \setcounter{page}{1}
% \maketitlesupplementary
\section{Efficient Cross-Attention}
Assume $\Tilde{K}_s$ is a coarser representation of memory spatial keys, $K_s$, a good surrogate of $K_s \in \R^{n \times d}$ with the same size, $\Bar{K}_s \in \R^{n \times d}$ from $\Tilde{K}_s \in \R^{\Tilde{w}\Tilde{h} \times d}$ is constructed by stacking each $\Tilde{k}_i, i = 1, \dots, \Tilde{w}\Tilde{h}$, $l_w\times l_h$ times, 
\begin{align*}
    \Bar{K}_s = [\underbrace{\Tilde{k}_1; \dots; \Tilde{k}_1}_{l_w\times l_h}; \underbrace{\Tilde{k}_2; \dots; \Tilde{k}_2}_{l_w\times l_h}; \dots; \underbrace{\Tilde{k}_{\Tilde{w}\Tilde{h}}; \dots; \Tilde{k}_{\Tilde{w}\Tilde{h}}}_{l_w\times l_h}]
\end{align*}
Each $\Tilde{v}_i, i =1, \dots, \Tilde{w}\Tilde{h}$, is stacked $l_w\times l_h$ times to make $\Bar{V}_s  \in \R^{n \times d}$ as a good surrogate of values, $V_s \in \R^{n\times d}$, 
\begin{align*}
    \small 
    \Bar{V}_s= [\underbrace{\Tilde{v}_1; \dots; \Tilde{v}_1}_{l_w\times l_h}; \underbrace{\Tilde{v}_2; \dots; \Tilde{v}_2}_{l_w \times l_h}; \dots; \underbrace{\Tilde{v}_{\Tilde{w}\Tilde{h}}; \dots; \Tilde{v}_{\Tilde{w}\Tilde{h}}}_{l_w \times l_h}]
\end{align*}
 The concatenation of coarse spatial tokens with object pointer tokens is, $\Bar{K} = [\Bar{K}_s; K_p]\in \R^{(n+P)\times d}$ and $\Bar{V} = [\Bar{V}_s; V_p]\in \R^{(n+P)\times d}$. 
 \begin{lemma}\label{lem:equiv}
For the coarse memory tokens, $\bar{K}$ and $\bar{V}$,  queries $Q\in\R^{L\times d}$, we have,
\begin{align}\label{eq:replace}
    \text{softmax}\Lleft\frac{Q\Bar{K}^{T}}{\sqrt{d}}\Rright\Bar{V} = \text{softmax}\Lleft A \Rright\Tilde{V},
\end{align}
where $A = [\frac{Q\Tilde{K}_s^{T}}{\sqrt{d}} + \ln{(l_w\times l_h)}, \frac{QK_p^{T}}{\sqrt{d}}] \in \R^{L\times (\Tilde{w}\Tilde{h} + P)}$, $\Tilde{V} = [\Tilde{V}_s; V_p] \in \R^{(\Tilde{w}\Tilde{h}+P)\times d}$.
\end{lemma}
\begin{figure*}[t!]
\centering
% \vspace{5pt}
\begin{overpic}[width=0.85\linewidth]{figures/video_seg_track_3.png}
\put (-8.3,25) {\scriptsize{SAM 2}}
\put (-12,18) {\scriptsize{EfficientTAM}}
\put (-8.3,10) {\scriptsize{SAM 2}}
\put (-12,3) {\scriptsize{EfficientTAM}}
\end{overpic}
\caption{Visualization results on video segmentation and tracking with SAM 2, and our EfficientTAM model. We sampled a subset of frames for visualization. The segmented objects with occlusion are colored in green. }
\label{fig:visual_vost3}
\end{figure*}
\begin{proof}
Denote $Q = [q_1; \dots; q_L]$, where $q_i \in \R^{1 \times d}$. The cross-attention matrix, $\Bar{C} = \text{softmax}\Lleft\frac{Q\Bar{K}^{T}}{\sqrt{d}}\Rright\Bar{V} \in \R^{L\times d}$. The softmax matrix $\bar{S} = \text{softmax}\Lleft\frac{Q\Bar{K}^{T}}{\sqrt{d}}\Rright \in \R^{L\times (n+P)}$ can be formulated as, 
\begin{equation*}
\resizebox{0.7\linewidth}{!}{$
\bar{S} = D_{\mathcal{S}}\begin{bmatrix} 
    e(\frac{q_1}{\sqrt{d}}\Tilde{k}_1^T) & \dots & e(\frac{q_1}{\sqrt{d}}\Tilde{k}_1^T) &\dots & e(\frac{q_1}{\sqrt{d}}\Tilde{k}_{\Tilde{w}\Tilde{h}}^T) & \dots & e(\frac{q_1}{\sqrt{d}}K_p^T)\\
    \vdots & \dots & \vdots & \dots & \vdots &\dots & \dots \\
    e(\frac{q_L}{\sqrt{d}}\Tilde{k}_1^T) & \dots  & e(\frac{q_L}{\sqrt{d}}\Tilde{k}_1^T) &\dots & e(\frac{q_L}{\sqrt{d}}\Tilde{k}_{\Tilde{w}\Tilde{h}}^T) & \dots & e(\frac{q_L}{\sqrt{d}}K_p^T)
    \end{bmatrix}$}
\end{equation*}
where $D_{\mathcal{S}}$ is a $L\times L$ diagonal matrix, which normalizes each row of the $\bar{S}$ matrix such that the row entries sum up to 1, and $e(\cdot)$ denotes $\exp(\cdot)$. 
For each row of the cross-attention matrix, we have, 
\begin{align}\label{eq:entry}
    \Bar{C}_{ij} & = D_{{\mathcal{S}}_{ii}}(\underbrace{e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_1 + \dots e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_1}_{l_w\times l_h} 
     + \dots + \underbrace{e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_{\Tilde{w}\Tilde{h}}  + \dots e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_{\Tilde{w}\Tilde{h}}}_{l_w\times l_h}  + e(\frac{q_i}{\sqrt{d}}K_p^T)V_p) \nonumber \\ 
    &= D_{{\mathcal{S}}_{ii}}(l_w\times l_h\times (e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_1 + \dots +  e(\frac{q_i}{\sqrt{d}}\Tilde{k}_1^T)\Tilde{v}_{\Tilde{w}\Tilde{h}}) + e(\frac{q_i}{\sqrt{d}}K_p^T)V_p) \nonumber \\ 
    & = D_{{\mathcal{S}}_{ii}}(l_w\times l_h \times e(\frac{q_i}{\sqrt{d}}\Tilde{K}_s^T)\Tilde{V}_s^T + e(\frac{q_i}{\sqrt{d}}K_p^T)V_p) \nonumber \\ 
    & = D_{{\mathcal{S}}_{ii}}(e(\ln(l_w\times l_h) + \frac{q_i}{\sqrt{d}}\Tilde{K}_s^T)\Tilde{V}_s + e(\frac{q_i}{\sqrt{d}}K_p^T)V_p) \nonumber \\ 
    & = \text{softmax}[\frac{q_i\Tilde{K}_s^{T}}{\sqrt{d}} + \ln{(l_w\times l_h)}, \frac{q_i\Tilde{K}_p^{T}}{\sqrt{d}}][\Tilde{V}_s; V_p] 
\end{align}
where $D_{{\mathcal{S}}_{ii}}$ is the $i^{\text{th}}$ diagonal element of the matrix $D_{\mathcal{S}}$. Note that the right side of \cref{eq:entry} is the $i^{\text{th}}$ row of $\text{softmax}\Lleft A \Rright\Tilde{V}$.
It concludes the proof. 
\end{proof}

% \input{tables/vos}
% \section{Main Results}
% We notice one typo of Tab. 1 in the main paper. The LVOS val number of SAM 2 did not show up in the table. We fixed this typo in \cref{tab:vos}.
\section{Ablation Studies}

\input{tables/pointer}
\textbf{Impact of the object pointer tokens.} We study the effect of the object pointer tokens when performing cross-attention in the memory module. We ablate the cross-attention with or without the object pointer tokens. When performing cross-attention,  
we find that object pointers significantly improve the performance on SA-V test dataset, 74.5 vs 72.1 $\mathcal{J}$\&$\mathcal{F}$, shown in \cref{tab:pointer}. The observations are consistent with SAM 2~\citep{ravi2024sam}. 
This demonstrates that object pointer tokens need to be cross-attended with spatial tokens.

\input{tables/pooling}
\noindent \textbf{Structure of memory tokens.} We ablate the impact of memory tokens for efficient cross-attention in the memory module. In our efficient cross-attention, we leverage the locality of memory spatial tokens for a coarser representation, and we concatenate the coarser embedding with object pointer tokens. 
In \cref{tab:pooling}, we observe that naively pooling the entire memory tokens instead of only the spatial tokens yields a large performance drop, 2.3 $\mathcal{J}$\&$\mathcal{F}$ on SA-V test.

\input{tables/windowed}
\noindent \textbf{Local windowed cross-attention.} We adapt local windowed attention for efficient cross-attention by partitioning input tokens into 4 non-overlapping segments (windows), within which we conduct cross-attention. In \cref{tab:windowed}, we find that local windowed cross-attention underperforms our proposed efficient cross-attention using averaging pooling, 72.4 vs 74.0 $\mathcal{J}$\&$\mathcal{F}$ on SA-V test dataset. These results demonstrate the effectiveness of our efficient cross-attention by leveraging the strong locality of spatial memory tokens. 

\begin{figure}[t]
    \centering
    \begin{overpic}[width=0.5\linewidth]{figures/across_attention.png}
\end{overpic}
   \vspace{-5pt}
    \caption{Visualization of the difference between original cross-attention and efficient cross-attention of \cref{eq:acrossattn}.}
    \label{fig:across_attn}
\end{figure}

\noindent \textbf{Efficient cross-attention variant.} We observe that \cref{eq:acrossattn} in the main paper is close to original cross-attention, visualized in \cref{fig:across_attn}. This suggests that \cref{eq:acrossattn} can also serve as a surrogate of the original cross-attention. 

\section{Qualitative Evaluation}
We provide more qualitative results of EfficientTAMs for video and image instance segmentation. \cref{fig:visual_vost3} shows two challenging video examples with occluded objects. We compare EfficientTAM and SAM 2 with a mask in the first frame prompted. We find that our EfficientTAM can generate high-quality masklet for the target occluded object as SAM 2. For image segmentation, we also observe that our EfficientTAM can generate quality image segmentation results as SAM and SAM 2, shown in  \cref{fig:visual_seg}. We report the predicted masks with two types of prompts, point and box, and also segment everything results. These results suggest that our EfficientTAMs have similar abilities to SAM 2, while EfficientTAM is more efficient. 

\begin{figure}
\vspace{10pt}
    \centering
    \begin{overpic}[width=1.0\linewidth]{figures/img_seg.png}
\put (5,96) {\scriptsize{Input Image}}
\put (17,96) {\scriptsize{SAM\citep{kirillov2023segment}}}
\put (37,96) {\scriptsize{EfficientSAM\citep{xiong2024efficientsam}}}
\put (63,96) {\scriptsize{SAM 2\citep{ravi2024sam}}}
\put (85,96) {\scriptsize{EfficientTAM}}
\end{overpic}
    \caption{Visualization results on image segmentation with point-prompt, box-prompt, and segment everything for SAM, EfficientSAM, SAM 2, and our EfficientTAM model.}
    \label{fig:visual_seg}
\end{figure}
