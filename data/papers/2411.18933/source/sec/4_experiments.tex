\section{Experiments}
\subsection{Experimental Setting}
\noindent \textbf{Pretraining.} The SA-1B dataset  consists of 11M diverse, high resolution images with 1.1B high-quality segmentation masks. Similar to~\citep{ravi2024sam}, we pretrain our EfficientTAM without memory components on SA-1B dataset~\citep{kirillov2023segment} for 90k steps. Our ViT image encoder is initialized from pre-trained ViTs~\citep{xiong2024efficientsam}
. We use the AdamW optimizer ~\citep{loshchilov2017decoupled} with a momentum, ($\beta_1 = 0.9$, $\beta_2 = 0.999$), a global batch size of 256, and a initial learning rate of $4e-4$. The learning rate is decayed by a reciprocal square root learning rate schedule~\citep{zhai2022scaling} with 1k iterations linear warmup and 5k iterations linear cooldown. We set weight decay to 0.1. We do not apply drop path for our image encoder. Layer-wise decay~\citep{clark2020electra} is set to 0.8. We apply horizontal flip augmentation and resize the input image resolution to $1024\times 1024$. We restrict our training to $64$ masks per image. Our models are pre-trained on 256 A100 GPUs with 80GB GPU memory with a linear combination of focal and dice loss for mask prediction (e.g., a ratio of 20:1). Bfloat16 is used during the training.

\noindent \textbf{Full Training Datasets.} Following~\citep{ravi2024sam}, we train our EfficientTAM including memory components on SA-V dataset~\citep{ravi2024sam} and a 10\% subset of SA-1B~\citep{kirillov2023segment}. SA-V is a large-scale and diverse video segmentation dataset, including 51K videos captured across 47 countries and 600K mask annotations covering whole objects and parts. SA-V video resolution ranges from 240p to 4K and duration ranges from 4 seconds to 138 seconds. Unlike SAM 2, we do not use other open-source datasets or internal datasets during our training for a fair comparison with baselines. 

\noindent \textbf{Full Training Implementation Details.} Similar to  ~\citep{ravi2024sam}, we train our EfficientTAM for 300k steps after pretraining. We use the AdamW optimizer ~\citep{loshchilov2017decoupled} with a momentum, ($\beta_1 = 0.9$, $\beta_2 = 0.999$), a batch size of 256, and a initial learning rate of $6e-5$ for image encoder and $3e-4$ for other components of the model. The learning rate is decayed by a cosine schedule with 15k iterations linear warmup. We set weight decay to 0.1. We do not apply drop path for our image encoder. Layer-wise decay~\citep{clark2020electra} is set to 0.8. We apply horizontal flip image augmentation and resize the input image resolution to $1024\times 1024$. For video, we apply horizontal flip augmentation, affine transformation with degree $25$ and shear $20$, color jittering with brightness $0.1$, contrast $0.03$, saturation $0.03$, gray scale augmentation with a probability of $0.05$, We restrict our training to $64$ masks per image and $3$ masks per frame for video. Our models are trained on 256 A100-80G GPUs with a linear combination of focal and dice losses for mask prediction, mean-absolution-error loss for IoU prediction, and cross-entropy loss for object prediction. The ratio for the linear combination loss is 20:1:1:1. Bfloat16 is used for training.

\noindent \textbf{Downstream Tasks/Datasets/Models.} \underline{\textit{Tasks and Datasets.}} We consider zero-shot video tasks including promptable video segmentation and semi-supervised video object segmentation, and zero-shot image tasks to demonstrate the competing capabilities of EfficientTAM on image and video segmentation.
For zero-shot image tasks, we evaluate EfficientTAM on 37 datasets including 23 datasets of SA-23~\citep{kirillov2023segment} and 14 video datasets introduced in~\citep{ravi2024sam}. For zero-shot video tasks, we evaluate our EfficientTAM on 9 densely annotated datasets for promptable video segmentation. We use 17 video datasets to evaluate zero-shot accuracy under interactive semi-supervised VOS setting using different prompts. For the standard semi-supervised VOS setting where a ground-truth mask on the first frame is provided, MOSE~\citep{ding2023mose}, DAVIS2017~\citep{pont20172017}, LVOS~\citep{hong2024lvos}, SA-V~\citep{ravi2024sam}, and YTVOS~\citep{xu2018youtube} are used to measure the VOS accuracy. We refer readers to~\citep{kirillov2023segment,ravi2024sam} for the details of these datasets.
\underline{\textit{Models.}} We use our EfficientTAM for zero-shot image and video tasks.

\noindent\textbf{Baselines and Evaluation Metrics.}
\underline{\textit{Baselines.}} For the standard semi-supervised VOS task, where the first-frame mask is provided, we compare the performance of our EfficientTAM with SAM 2\citep{ravi2024sam}, Cutie-base\citep{cheng2024putting}, DEVA~\citep{cheng2023tracking}, XMem~\citep{cheng2022xmem}, etc.
For the zero-shot promptable video segmentation task and the interactive semi-supervised video object segmentation task using different prompts, we compare our method with SAM2~\citep{ravi2024sam}, SAM+XMem++~\citep{ravi2024sam}, and SAM+Cutie~\citep{ravi2024sam}. For zero-shot image segmentation task, we compare with SAM~\citep{kirillov2023segment} and SAM2~\citep{ravi2024sam}. Note that we use the opensource version of SAM 2 (without training on MOSE/LVOS/YTVOS) for comparison. We also acknowledge the very recent release of SAM 2.1 trained with long memory contexts. 
\underline{\textit{Evaluation Metrics.}} We evaluate our method and all baselines using the accuracy metrics of the combined $\mathcal{J}$(region similarity)\&$\mathcal{F}$(contour accuracy),  for zero-shot video segmentation tasks; mIoU (mean intersection over union) for zero-shot image segmentation tasks. For efficiency metrics, we compare the number of model parameters or inference throughput on GPU (e.g, A100) and latency on mobile devices (e.g., iPhone 15 Pro Max). We follow SAM 2~\citep{ravi2024sam} to report metrics. When providing main results on MOSE, LVOS and YTVOS, we submit to their benchmarking servers to evaluate on, \textit{MOSE val}, \textit{LVOS val}, and \textit{YTVOS2019 val}, for final performance. For ablation studies, we evaluate on a MOSE development set, \textit{MOSE dev} with 200 randomly-sampled videos from the MOSE training split~\citep{ravi2024sam}.

\subsection{Main Results}
\input{tables/vos}
\textbf{Standard Semi-Supervised Video Object Segmentation.} Semi-supervised video object segmentation is the process of object segmentation and tracking in a video based on a ground-truth mask on the first frame. We follow SAM 2~\citep{ravi2024sam} and report accuracy of our methods on this standard semi-supervised video object segmentation task. We also report latency on a single A100 GPU with a batch size of 1. We evaluate EfficientTAMs with different image encoders, ViT-Tiny and ViT-Small, and memory modules, original memory block and efficient memory block with a $2\times2$ window pooling for a trade-off between efficiency and accuracy. EfficientTAM-S denotes EfficientTAM using a ViT-Small image encoder and the original memory block, and EfficientTAM-S/2 denotes EfficientTAM with a ViT-Small image encoder and efficiency memory block with a $2\times 2$ window pooling. \cref{tab:vos} compares our EfficientTAM with VOS baselines including SAM 2~\citep{ravi2024sam}, Cutie-base~\citep{cheng2024putting}, and XMem~\citep{cheng2022xmem}. On SA-V test, our EfficientTAM-S achieves 74.5 $\mathcal{J}$\&$\mathcal{F}$, outperforming Cutie-base, Cutie-base+, and XMem by 12.2, 12.9, and 14.4, respectively. On long-term video object segmentation benchmark, LVOS, we can also see that Our EfficientTAM-S outperform Cutie-base and XMem by a large margin. Notice that our EfficientTAM-S only underperforms SAM 2 by $<2$ $\mathcal{J}$\&$\mathcal{F}$ or $\mathcal{G}$ across 5 video benchmarks with $\sim$2x speedup and $\sim$2.4x fewer parameters. Further, EfficientTAM with efficient memory attention performs slightly worse than the one with original memory attention, but with much speedup, especially on mobile devices, $>$2x reduced latency on iPhone 15. For example, EfficientSAM-S achieves 74.5 $\mathcal{J}$\&$\mathcal{F}$ on SA-V test with 1010.8 ms running time per frame on iPhone 15. EfficientSAM-S/2 with efficient cross-memory attention obtain 74.0 $\mathcal{J}$\&$\mathcal{F}$ with only 450 ms. These results show the extraordinary benefits of EfficientTAMs for semi-supervised video object segmentation and validate the advantages of our methods for practical deployment.

\begin{figure*}[t]
    \centering
    \begin{overpic}[width=0.4\linewidth]{figures/offline_full.pdf}
    \end{overpic}
    \begin{overpic}[width=0.4\linewidth]{figures/online_full.pdf}
    \end{overpic}
    \caption{Promptable video segmentation results across 9 video segmentation datasets under interactive offline (left) and online (right) evaluation settings. The average $\mathcal{J}$\&$\mathcal{F}$ over $1, \dots, 8$ interacted frames is reported.}
    \label{fig:pvs}
\end{figure*}

\begin{figure*}[h]
\centering
% \vspace{5pt}
\begin{overpic}[width=0.85\linewidth]{figures/video_seg_track_2.png}
\put (-8.3,25) {\scriptsize{SAM 2}}
\put (-12.0,18) {\scriptsize{EfficientTAM}}
\put (-8.3,10) {\scriptsize{SAM 2}}
\put (-12.0,3) {\scriptsize{EfficientTAM}}
\end{overpic}
\caption{Visualization results on video segmentation and tracking with SAM 2, and our EfficientTAM model. We sampled a subset of frames for visualization. The segmented objects, e.g., the goose and the camel, are colored in red. }
\label{fig:visual_vost}
\end{figure*}


\noindent \textbf{Promptable Video Segmentation.} Similar to SAM 2~\citep{ravi2024sam}, we evaluate promptable video segmentation using two settings, offline evaluation and online evaluation. For offline evaluation, we make multiple passes through a video to annotate frames  w.r.t. the largest model error. For online evaluation, we make a single pass through the video to annotate frames. 3 clicks per frame are used for the evaluations on 9 densely annotated video datasets including 
EndoVis, ESD, LVOSv2, LV-VIS, UVO, VOST, PUMaVOS, Virtual KITTI 2, and VIPSeg. Average $\mathcal{J}$\&$\mathcal{F}$ accuracy over $1, \dots, 8$ interacted frames is reported. \cref{fig:pvs} shows the comparison between our method and strong baselines including SAM 2, SAM + XMem++, and SAM + Cutie. EfficientTAM outperforms SAM + XMem++ and SAM + Cutie for both evaluation settings. EfficientTAM also reduces the gap between SAM 2 for offline and online settings. Specifically, with 8 annotated frames with 3-click, EfficientTAM-S and EfficientTAM-S/2 achieve $\sim$ 82 $\mathcal{J}$\&$\mathcal{F}$ in average for offline evaluation setting and $\sim$ 81 $\mathcal{J}$\&$\mathcal{F}$ in average for online evaluation, outperforming SAM + XMem++, and SAM + Cutie by $>$3 $\mathcal{J}$\&$\mathcal{F}$ and reducing the gap of SAM 2. This set of experiments further validate the effectiveness of our EfficientTAM on promptable video segmentation. 

\input{tables/interactive}
\noindent \textbf{Interactive Semi-Supervised Video Object Segmentation.} We also evaluate our method on the interactive semi-supervised video object segmentation task with click, box, or mask prompts provided only on the first frame by following SAM 2. In \cref{tab:interactive}, we report the average $\mathcal{J}$\&$\mathcal{F}$ accuracy over 17 video datasets for each type of prompt. We observe that EfficientTAM outperforms SAM + XMem++, and SAM + Cutie with different input prompts. We also notice the reduced gap between EfficientTAM and SAM 2. With 1 click, our EfficientTAM-S obtain 63 $\mathcal{J}$\&$\mathcal{F}$ accuracy, with a 6 $\mathcal{J}$\&$\mathcal{F}$ gain over SAM + XMem++ and SAM + Cutie and a slight loss, 1.3 $\mathcal{J}$\&$\mathcal{F}$ comparing to SAM 2. In summary, EfficientTAM performs favorably on the interactive semi-supervised VOS task using different prompts. 

\input{tables/sa23}
\noindent \textbf{Segment Anything on Images.} We now evaluate our model for the segment anything task on images. In Table \cref{tab:sa23}, we report 1-click and 5-click mIoU accuracy on both SA-23 benchmark, plus the new benchmark introduced in SAM 2~\citep{ravi2024sam} with 14 video datasets from video domain. We compare our EfficientTAMs with SAM (ViT-H) and HQ-SAM (ViT-H). Our EfficientTAM-S obtains a 2.6 mIoU improvement over SAM (ViT-H) and 1.6 mIoU improvement over HQ-SAM (ViT-H) on 1-click accuracy. For 5-click, we observe consistent improvement over SAM (ViT-H) and HQ-SAM (ViT-H). We also notice a significant improvement on the video benchmarks of SA-23 and the one with 14 new videos. This indicates our EfficientTAMs are strong for both image and video segmentation.


\noindent \textbf{Qualitative Evaluation.} 
\cref{fig:visual_vost} shows two video examples. We compare EfficientTAM and SAM 2 with a mask in the first frame prompted. We find that our EfficientTAM can generate high-quality masklet for the target object as SAM 2. More video examples are in the appendix. These results suggest that our EfficientTAMs have similar abilities to SAM 2, while EfficientTAM is more efficient. 

\vspace{-1mm}
\subsection{Ablation Studies}
\textbf{Impact of the object pointer tokens.} We study the effect of the object pointer tokens when performing cross-attention in the memory module. We ablate the cross-attention with or without the object pointer tokens. We find that object pointers significantly improve the performance on SA-V test dataset, 74.5 vs 72.1 $\mathcal{J}$\&$\mathcal{F}$, consistent with SAM 2~\citep{ravi2024sam}. This demonstrates that object pointer tokens need to be cross-attended with spatial tokens from the memory bank.

\noindent \textbf{Structure of memory tokens.} We ablate the impact of memory tokens for efficient cross-attention in the memory module. In our efficient cross-attention, we leverage the locality of memory spatial tokens for a coarser representation, and we concatenate the coarser embedding with object pointer tokens. We observe that naively pooling the entire memory tokens instead of only the spatial tokens yields a large performance drop, 2.3 $\mathcal{J}$\&$\mathcal{F}$ on SA-V test.

\noindent \textbf{Impact of window size.} We perform an averaging pooling for a good surrogate in \cref{eq:ecrossattn}. We experiment with window sizes $2\times 2$ and $4 \times 4$. We find increasing the window from $2\times 2$ to $4\times 4$ for efficient cross-attention will lead to $\sim$ 1 $\mathcal{J}$\&$\mathcal{F}$ accuracy drop with marginal speed improvement. Therefore, we use window size $2\times 2$ to achieve a trade-off between accuracy and efficiency. 


\noindent \textbf{Linear cross-attention.} We explore adapting one representative efficient attention method such as linear attention~\citep{choromanski2020rethinking,cai2023efficientvit,you2023castling} by leveraging the associative property of matrix multiplication. We find that linear attention using associative property of matrix multiplication leads to significant performance drop, $>10$ $\mathcal{J}$\&$\mathcal{F}$ accuracy on SA-V test, comparing to our proposed efficient cross-attention. Therefore, leveraging the underlying token structure for efficient cross-attention is more effective. 

\input{tables/cross}
\noindent \textbf{Efficient cross-attention variants.} We compare efficient cross-attention variants. We find that the Linformer variant underperforms the efficient cross-attention in \cref{eq:ecrossattn}, 73.4 vs 74 $\mathcal{J}$\&$\mathcal{F}$ on SA-V test. However, we find that \cref{eq:acrossattn}, can achieve comparable performance, shown in \cref{tab:cross}. 

\input{tables/resolution}
\noindent \textbf{Impact of input resolution.} We ablate the impact of input resolution for video object segmentation. By default, we used $1024\times 1024$. We experiment with different input resolution, e.g., $512\times 512$. \cref{tab:res} shows that decreasing the input resolution leads to some performance drop. But it improves the efficiency, especially on mobile device, 12.5x speedup on iPhone 15. This gives flexibility for practical deployments with different latency and quality needs.