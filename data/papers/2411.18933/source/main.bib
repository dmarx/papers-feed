@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@inproceedings{heo2020interactive,
  title={Interactive video object segmentation using global and local transfer modules},
  author={Heo, Yuk and Jun Koh, Yeong and Kim, Chang-Su},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVII 16},
  pages={297--313},
  year={2020},
  organization={Springer}
}

@inproceedings{cheng2021modular,
  title={Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion},
  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5559--5568},
  year={2021}
}

@inproceedings{delatolas2024learning,
  title={Learning the What and How of Annotation in Video Object Segmentation},
  author={Delatolas, Thanos and Kalogeiton, Vicky and Papadopoulos, Dim P},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={6951--6961},
  year={2024}
}

@article{caelles20182018,
  title={The 2018 davis challenge on video object segmentation},
  author={Caelles, Sergi and Montes, Alberto and Maninis, Kevis-Kokitsi and Chen, Yuhua and Van Gool, Luc and Perazzi, Federico and Pont-Tuset, Jordi},
  journal={arXiv preprint arXiv:1803.00557},
  year={2018}
}

@article{homayounfar2021videoclick,
  title={Videoclick: Video object segmentation with a single click},
  author={Homayounfar, Namdar and Liang, Justin and Ma, Wei-Chiu and Urtasun, Raquel},
  journal={arXiv preprint arXiv:2101.06545},
  year={2021}
}

@inproceedings{cheng2024putting,
  title={Putting the object back into video object segmentation},
  author={Cheng, Ho Kei and Oh, Seoung Wug and Price, Brian and Lee, Joon-Young and Schwing, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3151--3161},
  year={2024}
}

@article{yang2023track,
  title={Track anything: Segment anything meets videos},
  author={Yang, Jinyu and Gao, Mingqi and Li, Zhe and Gao, Shang and Wang, Fangjing and Zheng, Feng},
  journal={arXiv preprint arXiv:2304.11968},
  year={2023}
}

@article{cheng2023segment,
  title={Segment and track anything},
  author={Cheng, Yangming and Li, Liulei and Xu, Yuanyou and Li, Xiaodi and Yang, Zongxin and Wang, Wenguan and Yang, Yi},
  journal={arXiv preprint arXiv:2305.06558},
  year={2023}
}

@article{rajivc2023segment,
  title={Segment anything meets point tracking},
  author={Raji{\v{c}}, Frano and Ke, Lei and Tai, Yu-Wing and Tang, Chi-Keung and Danelljan, Martin and Yu, Fisher},
  journal={arXiv preprint arXiv:2307.01197},
  year={2023}
}

@article{pont20172017,
  title={The 2017 davis challenge on video object segmentation},
  author={Pont-Tuset, Jordi and Perazzi, Federico and Caelles, Sergi and Arbel{\'a}ez, Pablo and Sorkine-Hornung, Alex and Van Gool, Luc},
  journal={arXiv preprint arXiv:1704.00675},
  year={2017}
}

@article{zhang2023joint,
  title={Joint modeling of feature, correspondence, and a compressed memory for video object segmentation},
  author={Zhang, Jiaming and Cui, Yutao and Wu, Gangshan and Wang, Limin},
  journal={arXiv preprint arXiv:2308.13505},
  year={2023}
}

@inproceedings{wang2023look,
  title={Look before you match: Instance understanding matters in video object segmentation},
  author={Wang, Junke and Chen, Dongdong and Wu, Zuxuan and Luo, Chong and Tang, Chuanxin and Dai, Xiyang and Zhao, Yucheng and Xie, Yujia and Yuan, Lu and Jiang, Yu-Gang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2268--2278},
  year={2023}
}

@inproceedings{wu2023scalable,
  title={Scalable video object segmentation with simplified framework},
  author={Wu, Qiangqiang and Yang, Tianyu and Wu, Wei and Chan, Antoni B},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={13879--13889},
  year={2023}
}

@article{yang2022decoupling,
  title={Decoupling features in hierarchical propagation for video object segmentation},
  author={Yang, Zongxin and Yang, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36324--36336},
  year={2022}
}

@inproceedings{cheng2022xmem,
  title={Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model},
  author={Cheng, Ho Kei and Schwing, Alexander G},
  booktitle={European Conference on Computer Vision},
  pages={640--658},
  year={2022},
  organization={Springer}
}

@article{yang2024scalable,
  title={Scalable video object segmentation with identification mechanism},
  author={Yang, Zongxin and Miao, Jiaxu and Wei, Yunchao and Wang, Wenguan and Wang, Xiaohan and Yang, Yi},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@inproceedings{li2022recurrent,
  title={Recurrent dynamic embedding for video object segmentation},
  author={Li, Mingxing and Hu, Li and Xiong, Zhiwei and Zhang, Bang and Pan, Pan and Liu, Dong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1332--1341},
  year={2022}
}

@inproceedings{xu2018youtube,
  title={Youtube-vos: Sequence-to-sequence video object segmentation},
  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yang, Jianchao and Yue, Dingcheng and Liang, Yuchen and Price, Brian and Cohen, Scott and Huang, Thomas},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={585--601},
  year={2018}
}

@inproceedings{oh2019video,
  title={Video object segmentation using space-time memory networks},
  author={Oh, Seoung Wug and Lee, Joon-Young and Xu, Ning and Kim, Seon Joo},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9226--9235},
  year={2019}
}

@article{hong2024lvos,
  title={LVOS: A Benchmark for Large-scale Long-term Video Object Segmentation},
  author={Hong, Lingyi and Liu, Zhongying and Chen, Wenchao and Tan, Chenzhi and Feng, Yuang and Zhou, Xinyu and Guo, Pinxue and Li, Jinglun and Chen, Zhaoyu and Gao, Shuyong and others},
  journal={arXiv preprint arXiv:2404.19326},
  year={2024}
}

@inproceedings{robinson2020learning,
  title={Learning fast and robust target models for video object segmentation},
  author={Robinson, Andreas and Lawin, Felix Jaremo and Danelljan, Martin and Khan, Fahad Shahbaz and Felsberg, Michael},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7406--7415},
  year={2020}
}

@inproceedings{bhat2020learning,
  title={Learning what to learn for video object segmentation},
  author={Bhat, Goutam and Lawin, Felix J{\"a}remo and Danelljan, Martin and Robinson, Andreas and Felsberg, Michael and Van Gool, Luc and Timofte, Radu},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={777--794},
  year={2020},
  organization={Springer}
}

@article{ravi2024sam,
  title={Sam 2: Segment anything in images and videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and others},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

@inproceedings{perazzi2016benchmark,
  title={A benchmark dataset and evaluation methodology for video object segmentation},
  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={724--732},
  year={2016}
}

@inproceedings{grundmann2010efficient,
  title={Efficient hierarchical graph-based video segmentation},
  author={Grundmann, Matthias and Kwatra, Vivek and Han, Mei and Essa, Irfan},
  booktitle={2010 ieee computer society conference on computer vision and pattern recognition},
  pages={2141--2148},
  year={2010},
  organization={IEEE}
}

@inproceedings{xu2012evaluation,
  title={Evaluation of super-voxel methods for early video processing},
  author={Xu, Chenliang and Corso, Jason J},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={1202--1209},
  year={2012},
  organization={IEEE}
}

@inproceedings{brox2010object,
  title={Object segmentation by long term analysis of point trajectories},
  author={Brox, Thomas and Malik, Jitendra},
  booktitle={European conference on computer vision},
  pages={282--295},
  year={2010},
  organization={Springer}
}

@inproceedings{fragkiadaki2012video,
  title={Video segmentation by tracing discontinuities in a trajectory embedding},
  author={Fragkiadaki, Katerina and Zhang, Geng and Shi, Jianbo},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1846--1853},
  year={2012},
  organization={IEEE}
}

@inproceedings{faktor2014video,
  title={Video segmentation by non-local consensus voting.},
  author={Faktor, Alon and Irani, Michal},
  booktitle={BMVC},
  volume={2},
  number={7},
  pages={8},
  year={2014}
}

@inproceedings{li2013video,
  title={Video segmentation by tracking many figure-ground segments},
  author={Li, Fuxin and Kim, Taeyoung and Humayun, Ahmad and Tsai, David and Rehg, James M},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2192--2199},
  year={2013}
}

@inproceedings{papazoglou2013fast,
  title={Fast object segmentation in unconstrained video},
  author={Papazoglou, Anestis and Ferrari, Vittorio},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1777--1784},
  year={2013}
}

@inproceedings{wang2015saliency,
  title={Saliency-aware geodesic video object segmentation},
  author={Wang, Wenguan and Shen, Jianbing and Porikli, Fatih},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3395--3402},
  year={2015}
}

@inproceedings{taylor2015causal,
  title={Causal video object segmentation from persistence of occlusions},
  author={Taylor, Brian and Karasev, Vasiliy and Soatto, Stefano},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4268--4276},
  year={2015}
}

@inproceedings{zhang2013video,
  title={Video object segmentation through spatially accurate and temporally dense extraction of primary object regions},
  author={Zhang, Dong and Javed, Omar and Shah, Mubarak},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={628--635},
  year={2013}
}

@inproceedings{perazzi2012saliency,
  title={Saliency filters: Contrast based filtering for salient region detection},
  author={Perazzi, Federico and Kr{\"a}henb{\"u}hl, Philipp and Pritch, Yael and Hornung, Alexander},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={733--740},
  year={2012},
  organization={IEEE}
}

@inproceedings{lee2011key,
  title={Key-segments for video object segmentation},
  author={Lee, Yong Jae and Kim, Jaechul and Grauman, Kristen},
  booktitle={2011 International conference on computer vision},
  pages={1995--2002},
  year={2011},
  organization={IEEE}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{li2022exploring,
  title={Exploring plain vision transformer backbones for object detection},
  author={Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  booktitle={European conference on computer vision},
  pages={280--296},
  year={2022},
  organization={Springer}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4015--4026},
  year={2023}
}

@inproceedings{cheng2022masked,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Misra, Ishan and Schwing, Alexander G and Kirillov, Alexander and Girdhar, Rohit},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1290--1299},
  year={2022}
}

@inproceedings{fan2021multiscale,
  title={Multiscale vision transformers},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6824--6835},
  year={2021}
}

@inproceedings{duke2021sstvos,
  title={Sstvos: Sparse spatiotemporal transformers for video object segmentation},
  author={Duke, Brendan and Ahmed, Abdalla and Wolf, Christian and Aarabi, Parham and Taylor, Graham W},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5912--5921},
  year={2021}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{li2022mvitv2,
  title={Mvitv2: Improved multiscale vision transformers for classification and detection},
  author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4804--4814},
  year={2022}
}

@inproceedings{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={568--578},
  year={2021}
}

@inproceedings{ryali2023hiera,
  title={Hiera: A hierarchical vision transformer without the bells-and-whistles},
  author={Ryali, Chaitanya and Hu, Yuan-Ting and Bolya, Daniel and Wei, Chen and Fan, Haoqi and Huang, Po-Yao and Aggarwal, Vaibhav and Chowdhury, Arkabandhu and Poursaeed, Omid and Hoffman, Judy and others},
  booktitle={International Conference on Machine Learning},
  pages={29441--29454},
  year={2023},
  organization={PMLR}
}

@inproceedings{xiong2024efficientsam,
  title={Efficientsam: Leveraged masked image pretraining for efficient segment anything},
  author={Xiong, Yunyang and Varadarajan, Bala and Wu, Lemeng and Xiang, Xiaoyu and Xiao, Fanyi and Zhu, Chenchen and Dai, Xiaoliang and Wang, Dilin and Sun, Fei and Iandola, Forrest and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16111--16121},
  year={2024}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@article{qin2024mobilenetv4,
  title={MobileNetV4-Universal Models for the Mobile Ecosystem},
  author={Qin, Danfeng and Leichner, Chas and Delakis, Manolis and Fornoni, Marco and Luo, Shixin and Yang, Fan and Wang, Weijun and Banbury, Colby and Ye, Chengxi and Akin, Berkin and others},
  journal={arXiv preprint arXiv:2404.10518},
  year={2024}
}

@inproceedings{mehta2021mobilevit,
  title={MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},
  author={Mehta, Sachin and Rastegari, Mohammad},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{graham2021levit,
  title={Levit: a vision transformer in convnet's clothing for faster inference},
  author={Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J{\'e}gou, Herv{\'e} and Douze, Matthijs},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={12259--12269},
  year={2021}
}

@article{li2022efficientformer,
  title={Efficientformer: Vision transformers at mobilenet speed},
  author={Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Ju and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12934--12949},
  year={2022}
}

@article{li2022next,
  title={Next-vit: Next generation vision transformer for efficient deployment in realistic industrial scenarios},
  author={Li, Jiashi and Xia, Xin and Li, Wei and Li, Huixia and Wang, Xing and Xiao, Xuefeng and Wang, Rui and Zheng, Min and Pan, Xin},
  journal={arXiv preprint arXiv:2207.05501},
  year={2022}
}

@inproceedings{wu2022tinyvit,
  title={Tinyvit: Fast pretraining distillation for small vision transformers},
  author={Wu, Kan and Zhang, Jinnian and Peng, Houwen and Liu, Mengchen and Xiao, Bin and Fu, Jianlong and Yuan, Lu},
  booktitle={European Conference on Computer Vision},
  pages={68--85},
  year={2022},
  organization={Springer}
}

@inproceedings{you2023castling,
  title={Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference},
  author={You, Haoran and Xiong, Yunyang and Dai, Xiaoliang and Wu, Bichen and Zhang, Peizhao and Fan, Haoqi and Vajda, Peter and Lin, Yingyan Celine},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14431--14442},
  year={2023}
}

@inproceedings{liu2023efficientvit,
  title={EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention},
  author={Liu, Xinyu and Peng, Houwen and Zheng, Ningxin and Yang, Yuqing and Hu, Han and Yuan, Yixuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14420--14430},
  year={2023}
}

@inproceedings{kitaev2019reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@inproceedings{yoso,
    title = 	 {You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling},
    author =   {Zeng, Zhanpeng and Xiong, Yunyang and Ravi, Sathya and Acharya, Shailesh and Fung, Glenn M and Singh, Vikas},
    booktitle = 	{International Conference on Machine Learning (ICML)},
    year = 	 {2021}
}

@article{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={arXiv:2004.05150},
  year={2020},
}

@article{zaheer2020bigbird,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020}
}

@article{shen2018efficient,
  title={Efficient Attention: Attention with Linear Complexities},
  author={Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
  journal={arXiv preprint arXiv:1812.01243},
  year={2018}
}

@inproceedings{katharopoulos-et-al-2020,
  author    = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},
  title     = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2020}
}

@inproceedings{
bello2021lambdanetworks,
title={LambdaNetworks: Modeling long-range Interactions without Attention},
author={Irwan Bello},
booktitle={International Conference on Learning Representations},
year={2021}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nystr{\"o}m-based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={14138--14148},
  year={2021}
}

@article{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{liu2023grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  journal={arXiv preprint arXiv:2303.05499},
  year={2023}
}

@article{zhang2023faster,
  title={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},
  author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung-Ho and Lee, Seungkyu and Hong, Choong Seon},
  journal={arXiv preprint arXiv:2306.14289},
  year={2023}
}

@article{zhao2023fast,
  title={Fast Segment Anything},
  author={Zhao, Xu and Ding, Wenchao and An, Yongqi and Du, Yinglong and Yu, Tao and Li, Min and Tang, Ming and Wang, Jinqiao},
  journal={arXiv preprint arXiv:2306.12156},
  year={2023}
}

@misc{chen2023semantic,
    title = {Semantic Segment Anything},
    author = {Chen, Jiaqi and Yang, Zeyu and Zhang, Li},
    howpublished = {\url{https://github.com/fudan-zvg/Semantic-Segment-Anything}},
    year = {2023}
}

@article{cen2023sad,
  title={SAD: Segment Any RGBD},
  author={Cen, Jun and Wu, Yizheng and Wang, Kewei and Li, Xingyi and Yang, Jingkang and Pei, Yixuan and Kong, Lingdong and Liu, Ziwei and Chen, Qifeng},
  journal={arXiv preprint arXiv:2305.14207},
  year={2023}
}

@article{deng2023segment,
  title={Segment anything model (sam) for digital pathology: Assess zero-shot segmentation on whole slide imaging},
  author={Deng, Ruining and Cui, Can and Liu, Quan and Yao, Tianyuan and Remedios, Lucas W and Bao, Shunxing and Landman, Bennett A and Wheless, Lee E and Coburn, Lori A and Wilson, Keith T and others},
  journal={arXiv preprint arXiv:2304.04155},
  year={2023}
}

@inproceedings{chen2023sam,
  title={SAM-Adapter: Adapting Segment Anything in Underperformed Scenes},
  author={Chen, Tianrun and Zhu, Lanyun and Deng, Chaotao and Cao, Runlong and Wang, Yan and Zhang, Shangzhan and Li, Zejian and Sun, Lingyun and Zang, Ying and Mao, Papa},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3367--3375},
  year={2023}
}

@article{yu2023inpaint,
  title={Inpaint anything: Segment anything meets image inpainting},
  author={Yu, Tao and Feng, Runseng and Feng, Ruoyu and Liu, Jinming and Jin, Xin and Zeng, Wenjun and Chen, Zhibo},
  journal={arXiv preprint arXiv:2304.06790},
  year={2023}
}

@article{jiang2023restore,
  title={Restore Anything Pipeline: Segment Anything Meets Image Restoration},
  author={Jiang, Jiaxi and Holz, Christian},
  journal={arXiv preprint arXiv:2305.13093},
  year={2023}
}

@inproceedings{gao2023editanything,
  title={EditAnything: Empowering Unparalleled Flexibility in Image Editing and Generation},
  author={Gao, Shanghua and Lin, Zhijie and Xie, Xingyu and Zhou, Pan and Cheng, Ming-Ming and Yan, Shuicheng},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={9414--9416},
  year={2023}
}

@article{zhang2023deshadow,
  title={Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal},
  author={Zhang, Xiao Feng and Song, Tian Yi and Yao, Jia Wei},
  journal={arXiv preprint arXiv:2309.11715},
  year={2023}
}

@article{ma2023segment,
  title={Segment anything in medical images},
  author={Ma, Jun and Wang, Bo},
  journal={arXiv preprint arXiv:2304.12306},
  year={2023}
}

@article{tang2023can,
  title={Can sam segment anything? when sam meets camouflaged object detection},
  author={Tang, Lv and Xiao, Haoke and Li, Bo},
  journal={arXiv preprint arXiv:2304.04709},
  year={2023}
}

@article{han2023segment,
  title={Segment anything model (sam) meets glass: Mirror and transparent objects cannot be easily detected},
  author={Han, Dongsheng and Zhang, Chaoning and Qiao, Yu and Qamar, Maryam and Jung, Yuna and Lee, SeungKyu and Bae, Sung-Ho and Hong, Choong Seon},
  journal={arXiv preprint arXiv:2305.00278},
  year={2023}
}

@article{sun2023explain,
  title={Explain Any Concept: Segment Anything Meets Concept-Based Explanation},
  author={Sun, Ao and Ma, Pingchuan and Yuan, Yuanyuan and Wang, Shuai},
  journal={arXiv preprint arXiv:2305.10289},
  year={2023}
}

@article{tariq2023segment,
  title={Segment anything meets semantic communication},
  author={Tariq, Shehbaz and Arfeto, Brian Estadimas and Zhang, Chaoning and Shin, Hyundong},
  journal={arXiv preprint arXiv:2306.02094},
  year={2023}
}

@inproceedings{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Ilya Loshchilov and Frank Hutter},
  journal={ICLR},
  year={2019}
}

@inproceedings{clark2020electra,
  title = {{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {ICLR},
  year = {2020},
  url = {https://openreview.net/pdf?id=r1xMH1BtvB}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12104--12113},
  year={2022}
}

@inproceedings{gupta2019lvis,
      title={LVIS: A Dataset for Large Vocabulary Instance Segmentation}, 
      author={Agrim Gupta and Piotr Dollár and Ross Girshick},
      year={2019},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}

@inproceedings{tokmakov2023breaking,
  title={Breaking the" Object" in Video Object Segmentation},
  author={Tokmakov, Pavel and Li, Jie and Gaidon, Adrien},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22836--22845},
  year={2023}
}

@inproceedings{cheng2023tracking,
  title={Tracking anything with decoupled video segmentation},
  author={Cheng, Ho Kei and Oh, Seoung Wug and Price, Brian and Schwing, Alexander and Lee, Joon-Young},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1316--1326},
  year={2023}
}

@inproceedings{cai2023efficientvit,
  title={Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction},
  author={Cai, Han and Li, Junyan and Hu, Muyan and Gan, Chuang and Han, Song},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17302--17313},
  year={2023}
}

@inproceedings{attention_is_all_you_need,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
    title = {Attention is all you need},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6000–6010},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}

@article{yang2021associating,
  title={Associating objects with transformers for video object segmentation},
  author={Yang, Zongxin and Wei, Yunchao and Yang, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2491--2502},
  year={2021}
}

@article{cheng2021rethinking,
  title={Rethinking space-time networks with improved memory coverage for efficient video object segmentation},
  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11781--11794},
  year={2021}
}
@article{zhang2024evf,
  title={Evf-sam: Early vision-language fusion for text-prompted segment anything model},
  author={Zhang, Yuxuan and Cheng, Tianheng and Hu, Rui and Liu, Lei and Liu, Heng and Ran, Longjin and Chen, Xiaoxin and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2406.20076},
  year={2024}
}
@article{xiong2024sam2,
  title={SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation},
  author={Xiong, Xinyu and Wu, Zihuang and Tan, Shuangyi and Li, Wenxue and Tang, Feilong and Chen, Ying and Li, Siying and Ma, Jie and Li, Guanbin},
  journal={arXiv preprint arXiv:2408.08870},
  year={2024}
}
@article{shen2024performance,
  title={Performance and non-adversarial robustness of the segment anything model 2 in surgical video segmentation},
  author={Shen, Yiqing and Ding, Hao and Shao, Xinyuan and Unberath, Mathias},
  journal={arXiv preprint arXiv:2408.04098},
  year={2024}
}
@article{zhang2024sam2,
  title={SAM2-PATH: A better segment anything model for semantic segmentation in digital pathology},
  author={Zhang, Mingya and Wang, Liang and Gu, Limei and Li, Zhao and Wang, Yaohui and Ling, Tingshen and Tao, Xianping},
  journal={arXiv preprint arXiv:2408.03651},
  year={2024}
}

@article{ding2024sam2long,
  title={SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree},
  author={Ding, Shuangrui and Qian, Rui and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Guo, Yuwei and Lin, Dahua and Wang, Jiaqi},
  journal={arXiv preprint arXiv:2410.16268},
  year={2024}
}

@article{qiu2024ded,
  title={DED-SAM: Adapting Segment Anything Model 2 for Dual Encoder-Decoder Change Detection},
  author={Qiu, Junlong and Liu, Wei and Li, Erzhu and Zhang, Lianpeng and Li, Xing},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  year={2024},
  publisher={IEEE}
}

@article{tang2024segment,
  title={Segment Any Mesh: Zero-shot Mesh Part Segmentation via Lifting Segment Anything 2 to 3D},
  author={Tang, George and Zhao, William and Ford, Logan and Benhaim, David and Zhang, Paul},
  journal={arXiv preprint arXiv:2408.13679},
  year={2024}
}

@article{zhou2024sam2,
  title={When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation},
  author={Zhou, Yuli and Sun, Guolei and Li, Yawei and Benini, Luca and Konukoglu, Ender},
  journal={arXiv preprint arXiv:2409.18653},
  year={2024}
}

@inproceedings{ding2023mose,
  title={MOSE: A new dataset for video object segmentation in complex scenes},
  author={Ding, Henghui and Liu, Chang and He, Shuting and Jiang, Xudong and Torr, Philip HS and Bai, Song},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20224--20234},
  year={2023}
}