---
abstract: |
  <span style="color: black"> Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one’s output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator’s cache. </span> While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present <span class="smallcaps">Medusa</span>, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a *tree-based attention mechanism*, <span class="smallcaps">Medusa</span> constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, <span class="smallcaps">Medusa</span> substantially <span style="color: black">reduces</span> the number of decoding steps required. We present two levels of fine-tuning procedures for <span class="smallcaps">Medusa</span> to meet the needs of different use cases: **<span class="smallcaps">Medusa</span>-1**: <span class="smallcaps">Medusa</span> is directly fine-tuned on top of a *frozen* backbone LLM, enabling lossless inference acceleration. **<span class="smallcaps">Medusa</span>-2**: <span class="smallcaps">Medusa</span> is fine-tuned together with the backbone LLM, enabling better prediction accuracy of <span class="smallcaps">Medusa</span> heads and higher speedup but needing a special training recipe that preserves the model’s capabilities. Moreover, we propose several extensions that improve or expand the utility of <span class="smallcaps">Medusa</span>, including a *self-distillation* to handle situations where no training data is available and a *typical acceptance scheme* to boost the acceptance rate while maintaining generation quality. We evaluate <span class="smallcaps">Medusa</span> on models of various sizes and training procedures. Our experiments demonstrate that <span class="smallcaps">Medusa</span>-1 can achieve over 2.2$\times$ speedup without compromising generation quality, while <span class="smallcaps">Medusa</span>-2 further improves the speedup to 2.3-2.8$\times$.
bibliography:
- icml/medusa_icml.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
---




# Experiment Settings

## Common Terms

We clarify three commonly used terms: a) Acceleration rate: This refers to the average number of tokens decoded per decoding step. In a standard auto-regressive model, this rate is 1.0. b) Overhead: This is used to characterize the per decoding step overhead compared to classic decoding, and is calculated by dividing the average per step latency of the <span class="smallcaps">Medusa</span> models by that of the vanilla model. c) Speedup: This refers to the wall-time acceleration rate. Following these definitions, we have the relation: Speedup = Acceleration rate / Overhead.

## Shared Settings

For all the experiments, we use the Axolotl  framework for training. We use a cosine learning rate scheduler with warmup and use 8-bit AdamW  optimizer. We train $5$ <span class="smallcaps">Medusa</span> heads with $1$ layer and set $\lambda_k$ in Eq.  to be $0.8^k$. For <span class="smallcaps">Medusa</span>-2, we use either LoRA  or QLoRA  for fine-tuning and set the learning rate of <span class="smallcaps">Medusa</span> heads to be $4$ times larger than the backbone model. LoRA is applied to all the linear layers of the backbone model, including the language model head. The rank of LoRA adapter is set to $32$, and $\alpha$ is set to $16$. A dropout of $0.05$ is added to the LoRA adapter.

## <span class="smallcaps">Medusa</span>-1 v.s. <span class="smallcaps">Medusa</span>-2 on Vicuna 7B and 13B

We use a global batch size of $64$ and a peak learning rate of $5e^{-4}$ for the backbone and $2e^{-3}$ for <span class="smallcaps">Medusa</span> heads and warmup for $40$ steps. We use $4$-bit quantized backbone models for both models. We first train the models with <span class="smallcaps">Medusa</span>-1 and use these trained models as initialization to train <span class="smallcaps">Medusa</span>-2. We employ QLoRA for <span class="smallcaps">Medusa</span>-2 and the $\lambda_0$ in Eq.  is set to be $0.2$.

##  Training with Self-Distillation on Vicuna-33B and Zephyr-7B

We use <span class="smallcaps">Medusa</span>-2 for both models instead of using a two-stage training procedure. We use a sine schedule for the $\theta_0$ to gradually increase the value to its peak at the end of the training. We find this approach is equally effective. We set the peak learning rate of the backbone LoRA adapter to be $1e^{-4}$ and the warmup steps to be $20$ since the self-distillation loss is relatively small. We set the $\lambda_0$ in Eq.  to be $0.01$.

# Visualization of optimized tree attention

Fig.  illustrates the structure of a sparsely constructed tree for the <span class="smallcaps">Medusa</span>-2 Vicuna-7B model. This tree structure extends four levels deep, indicating the engagement of four <span class="smallcaps">Medusa</span> heads in the computation. The tree is initially formed through a Cartesian product approach and subsequently refined by pruning based on the statistical expectations of the top-k predictions from each <span class="smallcaps">Medusa</span> head measured on the Alpaca-eval dataset . The tree’s lean towards the left visually represents the algorithm’s preference for nodes with higher probabilities on each head.

# Results of Speculative Decoding

In this study, speculative decoding was applied to Vicuna models  with varying sizes, specifically 7B, 13B, and 33B. The preliminary framework utilized open-source models such as Llama-68M and 160M , alongside Tiny-Llama  and Tiny-Vicuna , fine-tuned from Tiny-Llama with the Vicuna-style instructional tuning strategy. Due to the proprietary nature of speculative decoding methods , open-source alternatives[^1] were deployed for evaluation. Additionally, we utilize `torch.compile()` to accelerate the inference speed of draft models.

Our results shown in Fig. , reveal that the optimal settings of the draft model vary with the Vicuna model sizes. Specifically, the Llama-68M, with a setting of the draft token number $\gamma=4$, yielded the best performance for Vicuna-7B, while the same draft model with $\gamma=3$ was most effective for Vicuna-13B. For the larger Vicuna-33B, the Tiny-Vicuna <span style="color: black">(Vicuna-1B)</span>, with $\gamma=3$, provided the greatest acceleration. These results suggest that the choice and setting of the drafting model should be tailored to the size of the LLMs, presenting an area for further exploration in the field.

# Additional Results for All Models

We show speedup on various models in Fig. .

<figure id="fig:speedup_model_wild">
<span class="image placeholder" data-original-image-src="speedup_model_wild_wide.pdf" data-original-image-title="" width="45%"></span>
<figcaption>Speedup of various models with <span class="smallcaps">Medusa</span>-2. <span class="smallcaps">Medusa</span>-2 shows significant speed improvement over all the models, while models trained with self-distillation <span style="color: black">(Zephyr-7B, Vicuna-13/33B)</span> have weaker speedup due to the trade-off between preserving quality and boosting speed.</figcaption>
</figure>

# Additional Results on AlpacalEval Dataset

We conduct further experiments on the AlpacaEval  dataset. <span class="smallcaps">Medusa</span>-2 achieves consistent speedup similar to the results on MT-Bench.

<div id="tab:alpaca_eval_speedup">

|     | Model      | Base speed (tokens/s) | <span class="smallcaps">Medusa</span> speed (tokens/s) | Acc. rate | Speedup |
|:----|:-----------|----------------------:|-------------------------------------------------------:|----------:|--------:|
|     | Vicuna-7b  |                 37.07 |                                                 106.76 |      3.23 |    2.88 |
|     | Vicuna-13b |                 29.01 |                                                  91.54 |      3.28 |    3.16 |
|     | Vicuna-33b |                 17.87 |                                                  40.43 |      2.85 |    2.26 |
|     | Zephyr-7b  |                 34.21 |                                                  99.50 |      3.08 |    2.91 |

Speedup results on AlpacaEval  dataset.

</div>

# Exploration and Modeling of Hardware Constraints and <span class="smallcaps">Medusa</span>

 

We explore the hardware constraints, specifically memory-bandwidth bound, and their impact on <span class="smallcaps">Medusa</span>-style parallel decoding by incorporating a simplified <span style="color: black">Llama-series</span> model. First, we <span style="color: black">identify</span> that the operators involving matrix multiplications, such as linear layers and attention matrix multiplications, are the primary sources of overhead. We profile the performance of FLOP/s vs. Operational Intensity <span style="color: black">which is the ratio of FLOP/s to bandwidth (bytes/s)</span>, across various GPUs, including the A100-80GB-PCIe, A40, and A6000. Next, we examine the changes in FLOP/s vs. Operational Intensity when using <span class="smallcaps">Medusa</span> for different operators. Finally, we apply a straightforward analytical model to calculate acceleration rates and combine it with hardware benchmarks. This provides insights into the effects under different model sizes, sequence lengths, and batch sizes.

## Roofline Model of Operators

We present an analysis of the roofline model for various operators in large language models (LLMs), specifically focusing on Llama-7B, Llama-13B, and Llama-33B . These models were benchmarked on different GPUs, including the A100-80GB-PCIe, A40, and A6000. We looked into the three categories of matrix multiplication operators since they represent the primary sources of computational overhead in these models. Our study follows the report  which investigates the effectiveness of batch size but ours focuses more on decoding and parallel decoding.

Table  details the computation and space complexity for each operator during the prefill, decoding, and <span class="smallcaps">Medusa</span> decoding phases. The operators include the linear layers for query, key, and value matrices ($XW_{Q}$, $XW_{K}$, $XW_{V}$), the attention matrix multiplications ($QK^T$, $PV$), and the up/gate/down linear layers ($XW_{u}$, $XW_{g}$, $XW_{d}$). $b$ stands for the batch size, $s$ stands for the sequence length, $h$ stands for the hidden dimension, $i$ stands for the intermediate dimension, $n$ stands for the number of attention heads, $d$ stands for the head dimension and $q$ stands for the candidate length for <span class="smallcaps">Medusa</span>. For more details of these operators please refer to the articles .

| **Operator**                 |       **Input Shape**        | **Output Shape** | **Comp. Complexity** | **Space Complexity**  |
|:-----------------------------|:----------------------------:|:----------------:|:--------------------:|:---------------------:|
| **Prefill**                  |                              |                  |                      |                       |
| $XW_{Q}$, $XW_{K}$, $XW_{V}$ |         $(b, s, h)$          |   $(b, s, h)$    |      $O(bsh^2)$      |    $O(2bsh + h^2)$    |
| $QK^T$                       | $(b, n, s, d),(b, n, s, d)$  |  $(b, n, s, s)$  |     $O(bs^2nd)$      |  $O(2bsnd + bs^2n)$   |
| $PV$                         | $(b, n, s, s),(b, n, s, d)$  |  $(b, n, s, d)$  |                      |                       |
| $XW_{u}$, $XW_{g}$           |         $(b, s, h)$          |   $(b, s, i)$    |      $O(bshi)$       |  $O(bs(h + i) + hi)$  |
| $XW_{d}$                     |         $(b, s, i)$          |   $(b, s, h)$    |                      |                       |
| **Decoding**                 |                              |                  |                      |                       |
| $XW_{Q}$, $XW_{K}$, $XW_{V}$ |         $(b, 1, h)$          |   $(b, 1, h)$    |      $O(bh^2)$       |    $O(2bh + h^2)$     |
| $QK^T$                       | $(b, n, 1, d), (b, n, s, d)$ |  $(b, n, s, 1)$  |      $O(bsnd)$       | $O(bsn + bsnd + bnd)$ |
| $PV$                         | $(b, n, s, 1), (b, n, 1, d)$ |  $(b, n, 1, d)$  |                      |                       |
| $XW_{u}$, $XW_{g}$           |         $(b, 1, h)$          |   $(b, 1, i)$    |       $O(bhi)$       |  $O(b(h + i) + hi)$   |
| $XW_{d}$                     |         $(b, 1, i)$          |   $(b, 1, h)$    |                      |                       |
| **Parallel decoding**        |                              |                  |                      |                       |
| $XW_{Q}$, $XW_{K}$, $XW_{V}$ |         $(b, q, h)$          |   $(b, q, h)$    |      $O(bqh^2)$      |    $O(2bqh + h^2)$    |
| $QK^T$                       | $(b, n, q, d), (b, n, s, d)$ |  $(b, n, s, q)$  |      $O(bsqnd)$      | $O(bsqn + b(s+q)nd)$  |
| $PV$                         | $(b, n, s, q), (b, n, q, d)$ |  $(b, n, q, d)$  |                      |                       |
| $XW_{u}$, $XW_{g}$           |         $(b, q, h)$          |   $(b, q, i)$    |      $O(bqhi)$       |  $O(bq(h + i) + hi)$  |
| $XW_{d}$                     |         $(b, q, i)$          |   $(b, q, h)$    |                      |                       |

Computational and space complexity of the main operators in different phases. <span style="color: black">The table is based on Table 2 in the report .</span>

Figures - show the benchmark of three categories of operators on different models (7/13/33B) under various settings. To evaluate each operator’s performance and throughput, we chose the combination of settings including batch sizes from 1 to 64 in powers of 2 and sequence lengths from 128 to 8192 in powers of 2 <span style="color: black">(49 settings for each operator)</span>. From all the figures, we observe that the datapoints of each operator in the prefill and decoding stages cluster at very similar positions across all GPUs and for various model sizes.

During the prefill phase, increasing the batch size changes the FLOP/s of the attention matrix multiplications (see `‘qk/pv init‘`) but does not affect the Operational Intensity (refer to the vertical dashed arrow in Fig. 9). In contrast, increasing the sequence length impacts both FLOP/s and Operational Intensity in the prefill phase (refer to the diagonal dashed arrow in Fig. 9). During the decoding phase, the attention matrix multiplications are significantly limited by memory bandwidth. Despite an increase in FLOP/s with changes in batch size and sequence length, the Operational Intensity remains nearly unchanged (see `‘qk/pv ar‘`). This indicates suboptimal resource utilization in the self-attention mechanism.

The linear layers in the prefill phase are mostly compute-bound (see `‘qkv mlp init‘` and `‘up/gate/down init‘`). During the decoding phase, the datapoints of the linear layer form a line with the same slope as the GPU’s memory bandwidth (see `‘qkv mlp ar‘` and `‘up/gate/down ar‘`). This indicates the linear layers in the decoding stage are also bounded by memory bandwidth. Increasing the batch size improves the achieved FLOP/s and Operational Intensity under memory bandwidth constraints through better parallelism. Note that linear layers only process the new token and are independent of sequence length (See ‘Decoding‘ section in Table ).

<figure id="fig:llama7b-roofline-a100">
<span class="image placeholder" data-original-image-src="llama7b-roofline-a100.pdf" data-original-image-title="" width="80%"></span>
<figcaption>The figure shows the relationship between FLOP/s and Operational Intensity for all benchmarked datapoints of Llama-7B operators on A100-80GB-PCIe. The dashed lines represent the HBM bandwidth limit (1,935GB/s) and the peak performance limit (312 TFLOP/s) <span class="citation" data-cites="nvidia_a100_datasheet"></span>. ‘<code>qkv mlp</code>’ stands for the linear layers projecting hidden features to query/key/value features. ‘<code>up/gate/down</code>’ stands for the linear layers following the attention block. ‘<code>qk/pv</code>’ stands for the two steps of attention matrix multiplications. ‘<code>ar</code>’ stands for the decoding (autoregressive) and ‘<code>init</code>’ stands for the prefill phase.</figcaption>
</figure>

<figure id="fig:llama13b-roofline-a100">
<span class="image placeholder" data-original-image-src="llama13b-roofline-a100.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Llama-13B operators on A100-80GB-PCIe.</figcaption>
</figure>

<figure id="fig:llama33b-roofline-a100">
<span class="image placeholder" data-original-image-src="llama33b-roofline-a100.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Llama-33B operators on A100-80GB-PCIe.</figcaption>
</figure>

<figure id="fig:llama7b-roofline-a40">
<span class="image placeholder" data-original-image-src="llama7b-roofline-a40.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Llama-7B operators on A40.</figcaption>
</figure>

<figure id="fig:llama13b-roofline-a40">
<span class="image placeholder" data-original-image-src="llama13b-roofline-a40.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Llama-13B operators on A40.</figcaption>
</figure>

<figure id="fig:llama33b-roofline-a40">
<span class="image placeholder" data-original-image-src="llama33b-roofline-a40.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Llama-33B operators on A40.</figcaption>
</figure>

<figure id="fig:llama7b-roofline-a6000">
<span class="image placeholder" data-original-image-src="llama7b-roofline-a6000.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Llama-7B operators on A6000.</figcaption>
</figure>

<figure id="fig:llama13b-roofline-a6000">
<span class="image placeholder" data-original-image-src="llama13b-roofline-a6000.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Llama-13B operators on A6000.</figcaption>
</figure>

<figure id="fig:llama33b-roofline-a6000">
<span class="image placeholder" data-original-image-src="llama33b-roofline-a6000.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Llama-33B operators on A6000.</figcaption>
</figure>

## FLOP/s vs. Operational Intensity Variations in <span class="smallcaps">Medusa</span>

We investigate how Medusa can change Operational Intensity and elevate the FLOP/s. We choose Llama 33B on A100-80GB-PCIe as the setting.

First, we examine the attention matrix multiplication. Fig.  and Table  illustrate the effects of <span class="smallcaps">Medusa</span> while keeping the batch size fixed at 16. We observe increased FLOP/s and Operational Intensity as more candidate tokens are added (original decoding results are plotted as grey dots). This indicates that <span class="smallcaps">Medusa</span> can leverage additional candidate tokens to improve computational throughput. Compared to regular decoding, <span class="smallcaps">Medusa</span> achieves 44$\times$ FLOP/s and 41$\times$ Operational Intensity under the setting of batch size 16 and sequence length 1024 with 64 candidate tokens. Fig.  and Table  illustrate the effects of <span class="smallcaps">Medusa</span> decoding while keeping the sequence length fixed at 1024. Increasing the batch size does not improve Operational Intensity in this scenario.

Next, we examine the linear layer, focusing on the up/gate/down linear layers. The results are shown in Fig.  and Table . <span style="color: black">Since the linear layers in the decoding phase only process the future tokens while the past tokens are cached, they are independent of the sequence length.</span> We vary the batch size to observe the effects. As <span class="smallcaps">Medusa</span> increases the number of candidate tokens with the increasing batch size, we observe a shift from a memory-bandwidth-bound region to a computation-bound region. This shift demonstrates how <span class="smallcaps">Medusa</span> can transition the performance characteristics of the linear layers from being limited by memory bandwidth to being limited by computational capacity.

<figure id="fig:llama33b-spec-bs16">
<span class="image placeholder" data-original-image-src="llama33b-spec-bs16.pdf" data-original-image-title="" width="80%"></span>
<figcaption>FLOP/s vs. Operational Intensity of attention matrix multiplication with batch size 16.</figcaption>
</figure>

<figure id="fig:llama33b-spec-seq1024">
<span class="image placeholder" data-original-image-src="llama33b-spec-seq1024.pdf" data-original-image-title="" width="80%"></span>
<figcaption>FLOP/s vs. Operational Intensity of attention matrix multiplication with sequence length 1024.</figcaption>
</figure>

<figure id="fig:llama33b-spec--mlp-bsall">
<span class="image placeholder" data-original-image-src="llama33b-spec-mlp-bsall.pdf" data-original-image-title="" width="80%"></span>
<figcaption>FLOP/s vs. Operational Intensity of Linear layers.</figcaption>
</figure>

<div id="tab:llama33b-spec-bs16">

| Seq. Length | Number of Candidate Tokens |               |               |               |               |               |               |               |
|:------------|:--------------------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|
|             |             1              |      16       |      32       |      48       |      64       |      80       |      96       |      112      |
| 128         |        0.54 & 0.98         |  7.87 & 12.8  | 14.73 & 21.33 | 19.78 & 27.43 | 25.25 & 32.0  | 28.63 & 35.56 | 32.58 & 38.4  | 36.57 & 40.73 |
| 256         |        0.75 & 0.99         | 11.2 & 13.47  | 21.29 & 23.27 | 28.69 & 30.72 | 36.59 & 36.57 | 41.2 & 41.29  | 45.99 & 45.18 | 52.33 & 48.43 |
| 512         |        1.02 & 0.99         | 14.69 & 13.84 | 27.47 & 24.38 | 37.35 & 32.68 | 47.09 & 39.38 | 52.24 & 44.91 | 59.55 & 49.55 | 66.35 & 53.49 |
| 1024        |        1.24 & 0.99         | 17.42 & 14.03 | 32.15 & 24.98 | 43.89 & 33.76 | 54.8 & 40.96  | 60.19 & 46.97 | 68.28 & 52.07 | 75.45 & 56.44 |
| 2048        |        1.39 & 0.99         | 19.03 & 14.12 | 35.05 & 25.28 | 48.03 & 34.32 | 59.66 & 41.8  | 63.91 & 48.08 | 72.83 & 53.43 | 80.05 & 58.04 |
| 4096        |        1.48 & 0.99         | 19.8 & 14.17  | 36.59 & 25.44 | 50.4 & 34.61  | 62.29 & 42.23 | 65.84 & 48.65 | 74.86 & 54.13 | 82.06 & 58.87 |
| 8192        |        1.53 & 0.99         | 20.08 & 14.2  | 36.89 & 25.52 | 50.44 & 34.76 | 62.11 & 42.45 | 67.5 & 48.94  | 76.97 & 54.49 |  84.5 & 59.3  |

TFLOP/s & Operational Intensity of attention matrix multiplication with batch size 16 for Llama 33B on an A100 80GB PCIe.

</div>

<div id="tab:llama33b-spec-seq1024">

| Batch Size | Number of Candidate Tokens |               |               |               |               |               |               |               |
|:-----------|:--------------------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|
|            |             1              |      16       |      32       |      48       |      64       |      80       |      96       |      112      |
| 1          |        0.37 & 0.99         | 5.22 & 14.03  | 10.15 & 24.98 | 15.02 & 33.76 | 19.79 & 40.96 | 21.52 & 46.97 | 25.65 & 52.07 | 29.4 & 56.44  |
| 2          |        0.54 & 0.99         | 8.25 & 14.03  | 16.0 & 24.98  | 21.62 & 33.76 | 28.24 & 40.96 | 31.84 & 46.97 | 37.49 & 52.07 | 43.04 & 56.44 |
| 4          |        0.75 & 0.99         | 11.41 & 14.03 | 21.97 & 24.98 | 30.02 & 33.76 | 38.71 & 40.96 | 43.41 & 46.97 | 50.06 & 52.07 | 56.77 & 56.44 |
| 8          |        1.02 & 0.99         | 14.78 & 14.03 | 27.78 & 24.98 | 38.09 & 33.76 | 47.99 & 40.96 | 53.32 & 46.97 | 61.0 & 52.07  | 68.11 & 56.44 |
| 16         |        1.24 & 0.99         | 17.42 & 14.03 | 32.15 & 24.98 | 43.89 & 33.76 | 54.8 & 40.96  | 60.19 & 46.97 | 68.28 & 52.07 | 75.45 & 56.44 |
| 32         |        1.39 & 0.99         | 18.89 & 14.03 | 34.67 & 24.98 | 47.57 & 33.76 | 58.89 & 40.96 | 63.61 & 46.97 | 72.17 & 52.07 | 79.21 & 56.44 |
| 64         |        1.48 & 0.99         | 19.58 & 14.03 | 35.87 & 24.98 | 49.45 & 33.76 | 61.13 & 40.96 | 64.84 & 46.97 | 73.73 & 52.07 | 81.02 & 56.44 |

TFLOP/s & Operational Intensity of attention matrix multiplication with sequence length 1024 for Llama 33B on an A100 80GB PCIe.

</div>

<div id="tab:llama33b-spec--mlp-bsall">

| Batch Size | Number of Candidate Tokens |                 |                 |                  |                  |                  |                  |                  |
|:-----------|:--------------------------:|:---------------:|:---------------:|:----------------:|:----------------:|:----------------:|:----------------:|:----------------:|
|            |             1              |       16        |       32        |        48        |        64        |        80        |        96        |       112        |
| 1          |         1.26 & 1.0         |  19.95 & 15.95  |  39.69 & 31.79  |   58.4 & 47.53   |  76.57 & 63.17   |   94.4 & 78.7    |  111.91 & 94.14  | 128.64 & 109.47  |
| 2          |         2.51 & 2.0         |  39.66 & 31.79  |  76.53 & 63.17  |  112.05 & 94.14  | 145.73 & 124.71  | 130.67 & 154.89  |  129.1 & 184.69  | 148.56 & 214.12  |
| 4          |         5.03 & 4.0         |  76.44 & 63.17  | 145.8 & 124.71  | 128.85 & 184.69  | 167.85 & 243.17  | 201.19 & 300.21  | 236.93 & 355.85  | 195.91 & 410.14  |
| 8          |        10.06 & 7.99        | 145.72 & 124.71 | 168.26 & 243.17 | 236.83 & 355.85  | 221.11 & 463.14  | 207.79 & 565.44  | 236.95 & 663.07  |  227.8 & 756.36  |
| 16         |       19.96 & 15.95        | 168.35 & 243.17 | 221.41 & 463.14 |  237.5 & 663.07  | 224.71 & 845.59  | 232.49 & 1012.87 | 241.12 & 1166.74 | 229.25 & 1308.76 |
| 32         |       39.69 & 31.79        | 221.74 & 463.14 | 224.88 & 845.59 | 241.33 & 1166.74 | 239.02 & 1440.25 | 245.83 & 1675.97 | 243.55 & 1881.24 | 240.33 & 2061.59 |
| 64         |       76.57 & 63.17        | 225.19 & 845.59 | 239.2 & 1440.25 | 243.26 & 1881.24 | 246.16 & 2221.31 | 246.91 & 2491.55 | 244.52 & 2711.46 | 246.14 & 2893.91 |

TFLOP/s & Operational Intensity of linear layers (up/gate/down) for Llama 33B on an A100 80GB PCIe.

</div>

## Predicting <span class="smallcaps">Medusa</span> Performance

We further employ a straightforward analytical model <span style="color: black">for</span> the acceleration rate. The ablation study results in Sec.  indicate that the acceleration rate can be approximated by a simple logarithmic function. Using the results from Fig. , we model the curve as $\texttt{acc\_rate} = 0.477 \log(\texttt{num\_candidate})$. We simulate the latency of one simplified block of the Llama-7B model (sequentially processing $XW_Q$, $XW_K$, $XW_V$, $QK^T$, $PV$, $XW_u$, $XW_g$, $XW_d$) by first fixing the batch size at 1 and the sequence length at 1024. <span style="color: black"> The candidate tokens are processed parallelly by constructing the tree attention described in Section . We omit the latency of the post-processing steps including verification and acceptance for <span class="smallcaps">Medusa</span> since they introduce marginal overhead. </span> Fig.  illustrates the simulated acceleration rate and speedup for different numbers of candidate tokens under these settings. As the number of candidate tokens increases, both the acceleration rate and speedup initially show improvements. However, beyond 64, the speedup starts to decline, indicating diminishing returns with further increases in candidate length. This aligns with the experimental results in Fig.  and suggests that there is an optimal range for the numbers of candidate tokens where <span class="smallcaps">Medusa</span> provides the most significant performance gains.

We plot the simulated speedup under different batch size settings with a fixed sequence length of 1024 in Fig. . The results indicate that when the batch size exceeds 32, the speedup decreases and may even have a negative effect. This occurs because the linear layers shift from being memory-bandwidth-bound to computationally bound.

We conduct another experiment using a batch size of 4 and different sequence lengths. As shown in Fig. , the optimal number of candidate tokens remains relatively consistent across different sequence lengths. However, as the sequence length increases, the overall performance decreases. This performance drop is primarily due to the overhead from attention matrix multiplication, while the linear layer computation remains constant <span style="color: black">since the computation of linear layers is independent of the sequence length.</span>

Our simulations show that the optimal number of candidate tokens is key for model scaling with <span class="smallcaps">Medusa</span>, as benefits decrease beyond a certain range. Initially, increasing batch size improves performance through parallelism, but too large a batch size shifts linear layers from memory-bandwidth-bound to compute-bound, reducing speedup. Longer sequences increase attention matrix multiplication overhead, lowering performance, and emphasizing the need to optimize attention mechanisms. Effective model scaling requires balancing the number of candidate tokens, adjusting batch sizes to avoid compute-bound transitions, and enhancing attention mechanisms for longer sequences. These strategies ensure better resource utilization and higher performance, demonstrating the value of simulations in predicting performance and guiding acceleration strategy design.

<figure id="fig:llama7b-sim-bs1-seq1024">
<span class="image placeholder" data-original-image-src="llama7b-sim-bs1-seq1024.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Simulated acceleration rate, speedup, and normalized latency ablation using different numbers of candidate tokens under the setting of batch size 1 and sequence length 1024 for Llama-7B on an A100 80GB PCIe.</figcaption>
</figure>

<figure id="fig:llama7b-sim-bs1-allbs">
<span class="image placeholder" data-original-image-src="llama7b-sim-allbs.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Simulated speedup with sequence length 1024 for Llama-7B.</figcaption>
</figure>

<figure id="fig:llama7b-sim-allseq">
<span class="image placeholder" data-original-image-src="llama7b-sim-allseq.pdf" data-original-image-title="" width="80%"></span>
<figcaption>Simulated speedup with batch size 4 for Llama-7B.</figcaption>
</figure>

[^1]: <https://github.com/feifeibear/LLMSpeculativeSampling>
