
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2024}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\input{math_commands}
\newcommand{\ours}
{\textsc{Medusa}\xspace}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{\ours: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads}

\begin{document}

\twocolumn[
\icmltitle{\ours: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Tianle Cai}{equal,princeton,together}
\icmlauthor{Yuhong Li}{equal,uiuc}
\icmlauthor{Zhengyang Geng}{cmu}
\icmlauthor{Hongwu Peng}{uconn}
\icmlauthor{Jason D. Lee}{princeton}
\icmlauthor{Deming Chen}{uiuc}
\icmlauthor{Tri Dao}{princeton,together}
\end{icmlauthorlist}

\icmlaffiliation{princeton}{Princeton University}
\icmlaffiliation{together}{Together AI}
\icmlaffiliation{uiuc}{University of Illinois Urbana-Champaign}
\icmlaffiliation{cmu}{Carnegie Mellon University}
\icmlaffiliation{uconn}{University of Connecticut}

\icmlcorrespondingauthor{Tianle Cai}{tianle.cai@princeton.edu}
\icmlcorrespondingauthor{Yuhong Li}{leeyh@illinois.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
\textcolor{black}{
Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache.
}
While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. 
In this paper, we present \ours, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a \emph{tree-based attention mechanism}, \ours constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, 
\ours substantially \textcolor{black}{reduces} the number of decoding steps required.
We present two levels of fine-tuning procedures for \ours to meet the needs of different use cases: \textbf{\ours-1}: \ours is directly fine-tuned on top of a \emph{frozen} backbone LLM, enabling lossless inference acceleration. \textbf{\ours-2}: \ours is fine-tuned together with the backbone LLM, enabling better prediction accuracy of \ours heads and higher speedup but needing a special training recipe that preserves the model's capabilities. 
Moreover, we propose several extensions that improve or expand the utility of \ours, including a \emph{self-distillation} to handle situations where no training data is available and a \emph{typical acceptance scheme} to boost the acceptance rate while maintaining generation quality.
We evaluate \ours on models of various sizes and training procedures. Our experiments demonstrate that \ours-1 can achieve over 2.2$\times$ speedup without compromising generation quality, while \ours-2 further improves the speedup to 2.3-2.8$\times$.
\end{abstract}

\input{icml/intro}
\input{icml/method}
\input{icml/experiments}
\bibliography{icml/medusa_icml}
\bibliographystyle{icml2024}


\newpage
\appendix
\onecolumn
\input{icml/related_work}

\section{Experiment Settings}\label{appendix:experiment_settings}
\subsection{Common Terms}
We clarify three commonly used terms:
a) Acceleration rate: This refers to the average number of tokens decoded per decoding step. In a standard auto-regressive model, this rate is 1.0.
b) Overhead: This is used to characterize the per decoding step overhead compared to classic decoding, and is calculated by dividing the average per step latency of the \ours models by that of the vanilla model.
c) Speedup: This refers to the wall-time acceleration rate. 
Following these definitions, we have the relation: Speedup = Acceleration rate / Overhead.
\subsection{Shared Settings} 
For all the experiments, we use the Axolotl~\citep{axolotl2023} framework for training. We use a cosine learning rate scheduler with warmup and use 8-bit AdamW~\citep{dettmers20218bit} optimizer. We train $5$ \ours heads with $1$ layer and set $\lambda_k$ in Eq.~\eqref{eq:loss_medusa_1} to be $0.8^k$. For \ours-2, we use either LoRA~\citep{hu2021lora} or QLoRA~\citep{dettmers2023qlora} for fine-tuning and set the learning rate of \ours heads to be $4$ times larger than the backbone model. LoRA is applied to all the linear layers of the backbone model, including the language model head. The rank of LoRA adapter is set to $32$, and $\alpha$ is set to $16$. A dropout of $0.05$ is added to the LoRA adapter. 

\subsection{\ours-1 v.s. \ours-2 on Vicuna 7B and 13B} 
We use a global batch size of $64$ and a peak learning rate of $5e^{-4}$ for the backbone and $2e^{-3}$ for \ours heads and warmup for $40$ steps. We use $4$-bit quantized backbone models for both models. We first train the models with \ours-1 and use these trained models as initialization to train \ours-2. We employ QLoRA for \ours-2 and the  $\lambda_0$ in Eq.~\eqref{eq:loss_medusa_2} is set to be $0.2$.
\subsection{ Training with Self-Distillation on Vicuna-33B and Zephyr-7B} 
We use \ours-2 for both models instead of using a two-stage training procedure. We use a sine schedule for the $\theta_0$ to gradually increase the value to its peak at the end of the training. We find this approach is equally effective. We set the peak learning rate of the backbone LoRA adapter to be $1e^{-4}$ and the warmup steps to be $20$ since the self-distillation loss is relatively small. We set the $\lambda_0$ in Eq.~\eqref{eq:loss_medusa_2} to be $0.01$.
\section{Visualization of optimized tree attention}\label{appendix:sparse_tree}
Fig.~\ref{fig:sparse_tree} illustrates the structure of a sparsely constructed tree for the \ours-2 Vicuna-7B model. This tree structure extends four levels deep, indicating the engagement of four \ours heads in the computation. The tree is initially formed through a Cartesian product approach and subsequently refined by pruning based on the statistical expectations of the top-k predictions from each \ours head measured on the Alpaca-eval dataset~\cite{dubois2023alpacafarm}. The tree's lean towards the left visually represents the algorithm's preference for nodes with higher probabilities on each head.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{sparse_tree.pdf}
    \caption{Visualization of a sparse tree setting for \ours-2 Vicuna-7B. The tree has \textcolor{black}{64 nodes representing candidate tokens} and a depth of 4 which indicates 4 \ours heads involved in calculation. Each node indicates a token from a top-k prediction of a \ours head, and the edges show the connections between them. The red lines highlight the path that correctly predicts the future tokens.}
    \label{fig:sparse_tree}
\end{figure*}

\section{Results of Speculative Decoding}\label{appendix:spec}

In this study, speculative decoding was applied to Vicuna models~\citep{vicuna2023} with varying sizes, specifically 7B, 13B, and 33B. The preliminary framework utilized open-source models such as Llama-68M and 160M~\citep{miao2023specinfer}, alongside Tiny-Llama~\citep{zhang2024tinyllama} and Tiny-Vicuna~\citep{tiny_vicuna_1b}, fine-tuned from Tiny-Llama with the Vicuna-style instructional tuning strategy. Due to the proprietary nature of speculative decoding methods~\citep{chen2023accelerating, leviathan2022fast}, open-source alternatives\footnote{\href{https://github.com/feifeibear/LLMSpeculativeSampling}{https://github.com/feifeibear/LLMSpeculativeSampling}} were deployed for evaluation. Additionally, we utilize \verb|torch.compile()| to accelerate the inference speed of draft models.

Our results shown in Fig.~\ref{fig:speculative_decoding}, reveal that the optimal settings of the draft model vary with the Vicuna model sizes. Specifically, the Llama-68M, with a setting of the draft token number $\gamma=4$, yielded the best performance for Vicuna-7B, while the same draft model with $\gamma=3$ was most effective for Vicuna-13B. For the larger Vicuna-33B, the Tiny-Vicuna \textcolor{black}{(Vicuna-1B)}, with $\gamma=3$, provided the greatest acceleration. These results suggest that the choice and setting of the drafting model should be tailored to the size of the LLMs, presenting an area for further exploration in the field.



\begin{figure*}[h]
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{spec_7b.pdf}
         \caption{Vicuna-7B}
         \label{fig:spec7b}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{spec_13b.pdf}
         \caption{Vicuna-13B}
         \label{fig:spec13b}
     \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{spec_33b.pdf}
         \caption{Vicuna-33B}
         \label{fig:spec33b}
     \end{subfigure}
        \caption{Inference speed of various models using speculative decoding on MT-Bench. Baseline model speeds are presented by grey dotted lines for comparison. $\gamma$ denotes the draft token number.}
        \label{fig:speculative_decoding}
\end{figure*}

\section{Additional Results for All Models}\label{appendix:add_results}
We show speedup on various models in Fig.~\ref{fig:speedup_model_wild}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{speedup_model_wild_wide.pdf}
    \caption{Speedup of various models with \ours-2. \ours-2 shows significant speed improvement over all the models, while models trained with self-distillation \textcolor{black}{(Zephyr-7B, Vicuna-13/33B)} have weaker speedup due to the trade-off between preserving quality and boosting speed.}
    \label{fig:speedup_model_wild}
\end{figure}


\section{Additional Results on AlpacalEval Dataset}
We conduct further experiments on the AlpacaEval~\citep{alpaca_eval} dataset. \ours-2 achieves consistent speedup similar to the results on MT-Bench.
\begin{table}[h]
    \centering
    \begin{tabular}{llrrrr}
    \toprule
     & Model & Base speed (tokens/s) & \ours speed (tokens/s) & Acc. rate & Speedup \\
    \midrule
     & Vicuna-7b & 37.07 & 106.76 & 3.23 & 2.88 \\
     & Vicuna-13b & 29.01 & 91.54 & 3.28 & 3.16 \\
     & Vicuna-33b & 17.87 & 40.43 & 2.85 & 2.26 \\
     & Zephyr-7b & 34.21 & 99.50 & 3.08 & 2.91 \\
    \bottomrule
    \end{tabular}
    \caption{Speedup results on AlpacaEval~\citep{alpaca_eval} dataset.}
    \label{tab:alpaca_eval_speedup}
\end{table}

\section{Exploration and Modeling of Hardware Constraints and \ours}~\label{sec:roofline}


We explore the hardware constraints, specifically memory-bandwidth bound, and their impact on \ours-style parallel decoding by incorporating a simplified \textcolor{black}{Llama-series} model.
First, we \textcolor{black}{identify} that the operators involving matrix multiplications, such as linear layers and attention matrix multiplications, are the primary sources of overhead. We profile the performance of FLOP/s vs. Operational Intensity \textcolor{black}{which is the ratio of FLOP/s to bandwidth (bytes/s)}, across various GPUs, including the A100-80GB-PCIe, A40, and A6000.
Next, we examine the changes in FLOP/s vs. Operational Intensity when using \ours for different operators.
Finally, we apply a straightforward analytical model to calculate acceleration rates and combine it with hardware benchmarks. This provides insights into the effects under different model sizes, sequence lengths, and batch sizes.

\subsection{Roofline Model of Operators}
We present an analysis of the roofline model for various operators in large language models (LLMs), specifically focusing on Llama-7B, Llama-13B, and Llama-33B~\cite{touvron2023llama}. These models were benchmarked on different GPUs, including the A100-80GB-PCIe, A40, and A6000. We looked into the three categories of matrix multiplication operators since they represent the primary sources of computational overhead in these models. Our study follows the report~\cite{chen2023transformer} which investigates the effectiveness of batch size but ours focuses more on decoding and parallel decoding.

Table~\ref{tab:complexity} details the computation and space complexity for each operator during the prefill, decoding, and \ours decoding phases. The operators include the linear layers for query, key, and value matrices ($XW_{Q}$, $XW_{K}$, $XW_{V}$), the attention matrix multiplications ($QK^T$, $PV$), and the up/gate/down linear layers ($XW_{u}$, $XW_{g}$, $XW_{d}$).
$b$ stands for the batch size, $s$ stands for the sequence length, $h$ stands for the hidden dimension, $i$ stands for the intermediate dimension, $n$ stands for the number of attention heads, $d$ stands for the head dimension and $q$ stands for the candidate length for \ours.
For more details of these operators please refer to the articles~\cite{touvron2023llama, chen2023transformer}.


\begin{table}[h]
\centering
\caption{Computational and space complexity of the main operators in different phases. \textcolor{black}{The table is based on Table 2 in the report~\cite{chen2023transformer}.}}

\scriptsize
\begin{tabular}{lcccc}
\toprule
 \textbf{Operator} & \textbf{Input Shape} & \textbf{Output Shape} & \textbf{Comp. Complexity} & \textbf{Space Complexity} \\ \midrule
 \textbf{Prefill} \\ \midrule
 $XW_{Q}$, $XW_{K}$, $XW_{V}$ & $(b, s, h)$ & $(b, s, h)$ & $O(bsh^2)$ & $O(2bsh + h^2)$ \\ \midrule
  $QK^T$ & $(b, n, s, d),(b, n, s, d)$ & $(b, n, s, s)$ & $O(bs^2nd)$ & $O(2bsnd + bs^2n)$ \\ 
  $PV$ &$(b, n, s, s),(b, n, s, d)$&$(b, n, s, d)$&& \\ \midrule
  $XW_{u}$, $XW_{g}$ & $(b, s, h)$ & $(b, s, i)$ & $O(bshi)$ & $O(bs(h + i) + hi)$ \\ 
  $XW_{d}$&$(b, s, i)$&$(b, s, h)$&&\\ \midrule
   \textbf{Decoding} \\ \midrule
$XW_{Q}$, $XW_{K}$, $XW_{V}$ & $(b, 1, h)$ & $(b, 1, h)$ & $O(bh^2)$ & $O(2bh + h^2)$ \\ \midrule
  $QK^T$ & $(b, n, 1, d), (b, n, s, d)$ & $(b, n, s, 1)$ & $O(bsnd)$ & $O(bsn + bsnd + bnd)$ \\ 
  $PV$ & $(b, n, s, 1), (b, n, 1, d)$ & $(b, n, 1, d)$ & &  \\ \midrule
  $XW_{u}$, $XW_{g}$ & $(b, 1, h)$ & $(b, 1, i)$ & $O(bhi)$ & $O(b(h + i) + hi)$ \\
  $XW_{d}$ & $(b, 1, i)$ & $(b, 1, h)$ &  & \\\midrule
   \textbf{Parallel decoding} \\ \midrule
 $XW_{Q}$, $XW_{K}$, $XW_{V}$ & $(b, q, h)$ & $(b, q, h)$ & $O(bqh^2)$ & $O(2bqh + h^2)$ \\ \midrule
  $QK^T$ & $(b, n, q, d), (b, n, s, d)$ & $(b, n, s, q)$ & $O(bsqnd)$ & $O(bsqn + b(s+q)nd)$ \\ 
    $PV$ & $(b, n, s, q), (b, n, q, d)$ & $(b, n, q, d)$ & &  \\ \midrule
  $XW_{u}$, $XW_{g}$ & $(b, q, h)$ & $(b, q, i)$ & $O(bqhi)$ & $O(bq(h + i) + hi)$ \\
  $XW_{d}$ & $(b, q, i)$ & $(b, q, h)$ &  \\ \bottomrule
\end{tabular}
\label{tab:complexity}
\end{table}

Figures~\ref{fig:llama7b-roofline-a100}-\ref{fig:llama33b-roofline-a6000} show the benchmark of three categories of operators on different models (7/13/33B) under various settings. To evaluate each operator's performance and throughput, we chose the combination of settings including batch sizes from 1 to 64 in powers of 2 and sequence lengths from 128 to 8192 in powers of 2 \textcolor{black}{(49 settings for each operator)}. 
From all the figures, we observe that the datapoints of each operator in the prefill and decoding stages cluster at very similar positions across all GPUs and for various model sizes. 



During the prefill phase, increasing the batch size changes the FLOP/s of the attention matrix multiplications (see \texttt{`qk/pv init`}) but does not affect the Operational Intensity (refer to the vertical dashed arrow in Fig. 9). 
In contrast, increasing the sequence length impacts both FLOP/s and Operational Intensity in the prefill phase (refer to the diagonal dashed arrow in Fig. 9).
During the decoding phase, the attention matrix multiplications are significantly limited by memory bandwidth. Despite an increase in FLOP/s with changes in batch size and sequence length, the Operational Intensity remains nearly unchanged (see \texttt{`qk/pv ar`}). This indicates suboptimal resource utilization in the self-attention mechanism.

The linear layers in the prefill phase are mostly compute-bound (see \texttt{`qkv mlp init`} and \texttt{`up/gate/down init`}). During the decoding phase, the datapoints of the linear layer form a line with the same slope as the GPUâ€™s memory bandwidth (see \texttt{`qkv mlp ar`} and \texttt{`up/gate/down ar`}). This indicates the linear layers in the decoding stage are also bounded by memory bandwidth. Increasing the batch size improves the achieved FLOP/s and Operational Intensity under memory bandwidth constraints through better parallelism. Note that linear layers only process the new token and are independent of sequence length (See `Decoding` section in Table~\ref{tab:complexity}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama7b-roofline-a100.pdf}
    \caption{The figure shows the relationship between FLOP/s and Operational Intensity for all benchmarked datapoints of Llama-7B operators on A100-80GB-PCIe. The dashed lines represent the HBM bandwidth limit (1,935GB/s) and the peak performance limit (312 TFLOP/s)~\cite{nvidia_a100_datasheet}. `\texttt{qkv mlp}' stands for the linear layers projecting hidden features to query/key/value features. `\texttt{up/gate/down}' stands for the linear layers following the attention block. `\texttt{qk/pv}' stands for the two steps of attention matrix multiplications. `\texttt{ar}' stands for the decoding (autoregressive) and `\texttt{init}' stands for the prefill phase.}
    \label{fig:llama7b-roofline-a100}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama13b-roofline-a100.pdf}
    \caption{Llama-13B operators on A100-80GB-PCIe.}
    \label{fig:llama13b-roofline-a100}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama33b-roofline-a100.pdf}
    \caption{Llama-33B operators on A100-80GB-PCIe.}
    \label{fig:llama33b-roofline-a100}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama7b-roofline-a40.pdf}
    \caption{Llama-7B operators on A40.}
    \label{fig:llama7b-roofline-a40}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama13b-roofline-a40.pdf}
    \caption{Llama-13B operators on A40.}
    \label{fig:llama13b-roofline-a40}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama33b-roofline-a40.pdf}
    \caption{Llama-33B operators on A40.}
    \label{fig:llama33b-roofline-a40}
\end{figure}



\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama7b-roofline-a6000.pdf}
    \caption{Llama-7B operators on A6000.}
    \label{fig:llama7b-roofline-a6000}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama13b-roofline-a6000.pdf}
    \caption{Llama-13B operators on A6000.}
    \label{fig:llama13b-roofline-a6000}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama33b-roofline-a6000.pdf}
    \caption{Llama-33B operators on A6000.}
    \label{fig:llama33b-roofline-a6000}
\end{figure}

\clearpage



\subsection{FLOP/s vs. Operational Intensity Variations in \ours}

We investigate how Medusa can change Operational Intensity and elevate the FLOP/s.
We choose Llama 33B on A100-80GB-PCIe as the setting. 

First, we examine the attention matrix multiplication. Fig.~\ref{fig:llama33b-spec-bs16} and Table~\ref{tab:llama33b-spec-bs16} illustrate the effects of \ours while keeping the batch size fixed at 16. We observe increased FLOP/s and Operational Intensity as more candidate tokens are added (original decoding results are plotted as grey dots). This indicates that \ours can leverage additional candidate tokens to improve computational throughput. Compared to regular decoding, \ours achieves 44$\times$ FLOP/s and 41$\times$ Operational Intensity under the setting of batch size 16 and sequence length 1024 with 64 candidate tokens.
 Fig.~\ref{fig:llama33b-spec-seq1024} and Table~\ref{tab:llama33b-spec-seq1024} illustrate the effects of \ours decoding while keeping the sequence length fixed at 1024. Increasing the batch size does not improve Operational Intensity in this scenario. 

Next, we examine the linear layer, focusing on the up/gate/down linear layers. The results are shown in Fig.~\ref{fig:llama33b-spec--mlp-bsall} and Table~\ref{tab:llama33b-spec--mlp-bsall}. \textcolor{black}{Since the linear layers in the decoding phase only process the future tokens while the past tokens are cached, they are independent of the sequence length.} We vary the batch size to observe the effects. As \ours increases the number of candidate tokens with the increasing batch size, we observe a shift from a memory-bandwidth-bound region to a computation-bound region. This shift demonstrates how \ours can transition the performance characteristics of the linear layers from being limited by memory bandwidth to being limited by computational capacity.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama33b-spec-bs16.pdf}
    \caption{FLOP/s vs. Operational Intensity of attention matrix multiplication with batch size 16.}
    \label{fig:llama33b-spec-bs16}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama33b-spec-seq1024.pdf}
    \caption{FLOP/s vs. Operational Intensity of attention matrix multiplication with sequence length 1024.}
    \label{fig:llama33b-spec-seq1024}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama33b-spec-mlp-bsall.pdf}
    \caption{FLOP/s vs. Operational Intensity of Linear layers.}
    \label{fig:llama33b-spec--mlp-bsall}
\end{figure}

\clearpage


\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{lcccccccc}

\toprule
Seq. Length & \multicolumn{8}{c}{Number of Candidate Tokens} \\
\midrule
 & 1 & 16 & 32 & 48 & 64 & 80 & 96 & 112 \\
\midrule
128  & 0.54 \& 0.98 & 7.87 \& 12.8 & 14.73 \& 21.33 & 19.78 \& 27.43 & 25.25 \& 32.0 & 28.63 \& 35.56 & 32.58 \& 38.4 & 36.57 \& 40.73 \\
256  & 0.75 \& 0.99 & 11.2 \& 13.47 & 21.29 \& 23.27 & 28.69 \& 30.72 & 36.59 \& 36.57 & 41.2 \& 41.29 & 45.99 \& 45.18 & 52.33 \& 48.43 \\
512  & 1.02 \& 0.99 & 14.69 \& 13.84 & 27.47 \& 24.38 & 37.35 \& 32.68 & 47.09 \& 39.38 & 52.24 \& 44.91 & 59.55 \& 49.55 & 66.35 \& 53.49 \\
1024  & 1.24 \& 0.99 & 17.42 \& 14.03 & 32.15 \& 24.98 & 43.89 \& 33.76 & 54.8 \& 40.96 & 60.19 \& 46.97 & 68.28 \& 52.07 & 75.45 \& 56.44 \\
2048 & 1.39 \& 0.99 & 19.03 \& 14.12 & 35.05 \& 25.28 & 48.03 \& 34.32 & 59.66 \& 41.8 & 63.91 \& 48.08 & 72.83 \& 53.43 & 80.05 \& 58.04 \\
4096 & 1.48 \& 0.99 & 19.8 \& 14.17 & 36.59 \& 25.44 & 50.4 \& 34.61 & 62.29 \& 42.23 & 65.84 \& 48.65 & 74.86 \& 54.13 & 82.06 \& 58.87 \\
8192 & 1.53 \& 0.99 & 20.08 \& 14.2 & 36.89 \& 25.52 & 50.44 \& 34.76 & 62.11 \& 42.45 & 67.5 \& 48.94 & 76.97 \& 54.49 & 84.5 \& 59.3 \\
\bottomrule
\end{tabular}
\caption{
TFLOP/s \& Operational Intensity of attention matrix multiplication with batch size 16 for Llama 33B on an A100 80GB PCIe.}
\label{tab:llama33b-spec-bs16}
\end{table}


\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{lcccccccc}
\toprule
Batch Size & \multicolumn{8}{c}{Number of Candidate Tokens} \\
\midrule
 & 1 & 16 & 32 & 48 & 64 & 80 & 96 & 112 \\
\midrule
1  & 0.37 \& 0.99 & 5.22 \& 14.03 & 10.15 \& 24.98 & 15.02 \& 33.76 & 19.79 \& 40.96 & 21.52 \& 46.97 & 25.65 \& 52.07 & 29.4 \& 56.44 \\
2  & 0.54 \& 0.99 & 8.25 \& 14.03 & 16.0 \& 24.98 & 21.62 \& 33.76 & 28.24 \& 40.96 & 31.84 \& 46.97 & 37.49 \& 52.07 & 43.04 \& 56.44 \\
4  & 0.75 \& 0.99 & 11.41 \& 14.03 & 21.97 \& 24.98 & 30.02 \& 33.76 & 38.71 \& 40.96 & 43.41 \& 46.97 & 50.06 \& 52.07 & 56.77 \& 56.44 \\
8  & 1.02 \& 0.99 & 14.78 \& 14.03 & 27.78 \& 24.98 & 38.09 \& 33.76 & 47.99 \& 40.96 & 53.32 \& 46.97 & 61.0 \& 52.07 & 68.11 \& 56.44 \\
16 & 1.24 \& 0.99 & 17.42 \& 14.03 & 32.15 \& 24.98 & 43.89 \& 33.76 & 54.8 \& 40.96 & 60.19 \& 46.97 & 68.28 \& 52.07 & 75.45 \& 56.44 \\
32 & 1.39 \& 0.99 & 18.89 \& 14.03 & 34.67 \& 24.98 & 47.57 \& 33.76 & 58.89 \& 40.96 & 63.61 \& 46.97 & 72.17 \& 52.07 & 79.21 \& 56.44 \\
64 & 1.48 \& 0.99 & 19.58 \& 14.03 & 35.87 \& 24.98 & 49.45 \& 33.76 & 61.13 \& 40.96 & 64.84 \& 46.97 & 73.73 \& 52.07 & 81.02 \& 56.44 \\

\bottomrule
\end{tabular}
\caption{
TFLOP/s \& Operational Intensity of attention matrix multiplication with sequence length 1024 for Llama 33B on an A100 80GB PCIe.}
\label{tab:llama33b-spec-seq1024}
\end{table}

\begin{table}[h]
\centering
\tiny
\begin{tabular}{lcccccccc}
\toprule
Batch Size & \multicolumn{8}{c}{Number of Candidate Tokens} \\
\midrule
 & 1 & 16 & 32 & 48 & 64 & 80 & 96 & 112 \\
\midrule
1  & 1.26 \& 1.0 & 19.95 \& 15.95 & 39.69 \& 31.79 & 58.4 \& 47.53 & 76.57 \& 63.17 & 94.4 \& 78.7 & 111.91 \& 94.14 & 128.64 \& 109.47 \\
2  & 2.51 \& 2.0 & 39.66 \& 31.79 & 76.53 \& 63.17 & 112.05 \& 94.14 & 145.73 \& 124.71 & 130.67 \& 154.89 & 129.1 \& 184.69 & 148.56 \& 214.12 \\
4  & 5.03 \& 4.0 & 76.44 \& 63.17 & 145.8 \& 124.71 & 128.85 \& 184.69 & 167.85 \& 243.17 & 201.19 \& 300.21 & 236.93 \& 355.85 & 195.91 \& 410.14 \\
8  & 10.06 \& 7.99 & 145.72 \& 124.71 & 168.26 \& 243.17 & 236.83 \& 355.85 & 221.11 \& 463.14 & 207.79 \& 565.44 & 236.95 \& 663.07 & 227.8 \& 756.36 \\
16 & 19.96 \& 15.95 & 168.35 \& 243.17 & 221.41 \& 463.14 & 237.5 \& 663.07 & 224.71 \& 845.59 & 232.49 \& 1012.87 & 241.12 \& 1166.74 & 229.25 \& 1308.76 \\
32 & 39.69 \& 31.79 & 221.74 \& 463.14 & 224.88 \& 845.59 & 241.33 \& 1166.74 & 239.02 \& 1440.25 & 245.83 \& 1675.97 & 243.55 \& 1881.24 & 240.33 \& 2061.59 \\
64 & 76.57 \& 63.17 & 225.19 \& 845.59 & 239.2 \& 1440.25 & 243.26 \& 1881.24 & 246.16 \& 2221.31 & 246.91 \& 2491.55 & 244.52 \& 2711.46 & 246.14 \& 2893.91 \\
\bottomrule
\end{tabular}
\caption{
TFLOP/s \& Operational Intensity of linear layers (up/gate/down) for Llama 33B on an A100 80GB PCIe.
}\label{tab:llama33b-spec--mlp-bsall}
\end{table}

\subsection{Predicting \ours Performance}

We further employ a straightforward analytical model \textcolor{black}{for} the acceleration rate. The ablation study results in Sec.~\ref{section:config of tree} indicate that the acceleration rate can be approximated by a simple logarithmic function. Using the results from Fig.~\ref{fig:sparse_acc}, we model the curve as $\texttt{acc\_rate} = 0.477 \log(\texttt{num\_candidate})$. We simulate the latency of one simplified block of the Llama-7B model (sequentially processing $XW_Q$, $XW_K$, $XW_V$, $QK^T$, $PV$, $XW_u$, $XW_g$, $XW_d$) by first fixing the batch size at 1 and the sequence length at 1024.
\textcolor{black}{
The candidate tokens are processed parallelly by constructing the tree attention described in Section~\ref{sec:tree_attention}. We omit the latency of the post-processing steps including verification and acceptance for \ours since they introduce marginal overhead.
}
Fig.~\ref{fig:llama7b-sim-bs1-seq1024} illustrates the simulated acceleration rate and speedup for different numbers of candidate tokens under these settings. As the number of candidate tokens increases, both the acceleration rate and speedup initially show improvements. However, beyond 64, the speedup starts to decline, indicating diminishing returns with further increases in candidate length. This aligns with the experimental results in Fig.~\ref{fig:sparse_speed} and suggests that there is an optimal range for the numbers of candidate tokens where \ours provides the most significant performance gains.

We plot the simulated speedup under different batch size settings with a fixed sequence length of 1024 in Fig.~\ref{fig:llama7b-sim-bs1-allbs}. The results indicate that when the batch size exceeds 32, the speedup decreases and may even have a negative effect. This occurs because the linear layers shift from being memory-bandwidth-bound to computationally bound.

We conduct another experiment using a batch size of 4 and different sequence lengths. As shown in Fig.~\ref{fig:llama7b-sim-allseq}, the optimal number of candidate tokens remains relatively consistent across different sequence lengths. However, as the sequence length increases, the overall performance decreases. This performance drop is primarily due to the overhead from attention matrix multiplication, while the linear layer computation remains constant \textcolor{black}{since the computation of linear layers is independent of the sequence length.}

Our simulations show that the optimal number of candidate tokens is key for model scaling with \ours, as benefits decrease beyond a certain range. Initially, increasing batch size improves performance through parallelism, but too large a batch size shifts linear layers from memory-bandwidth-bound to compute-bound, reducing speedup. Longer sequences increase attention matrix multiplication overhead, lowering performance, and emphasizing the need to optimize attention mechanisms. Effective model scaling requires balancing the number of candidate tokens, adjusting batch sizes to avoid compute-bound transitions, and enhancing attention mechanisms for longer sequences. These strategies ensure better resource utilization and higher performance, demonstrating the value of simulations in predicting performance and guiding acceleration strategy design.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama7b-sim-bs1-seq1024.pdf}
    \caption{Simulated acceleration rate, speedup, and normalized latency ablation using different numbers of candidate tokens under the setting of batch size 1 and sequence length 1024 for Llama-7B on an A100 80GB PCIe.}
    \label{fig:llama7b-sim-bs1-seq1024}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama7b-sim-allbs.pdf}
    \caption{Simulated speedup with sequence length 1024 for Llama-7B.}
    \label{fig:llama7b-sim-bs1-allbs}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{llama7b-sim-allseq.pdf}
    \caption{Simulated speedup with batch size 4 for Llama-7B.}
    \label{fig:llama7b-sim-allseq}
\end{figure}
\end{document}