\section{Related Work}\label{sec:related_work}
\subsection{LLM Inference Acceleration}
The inefficiency of Large Language Model (LLM) inference is primarily attributed to the memory-bandwidth-bound nature of the auto-regressive decoding process. Several methods have been proposed to alleviate this issue, improving inference latency and throughput. Traditionally, batch inference has been employed as a straightforward method to enhance arithmetic intensity and escape memory-bandwidth-bound limitations. However, with LLMs, both model parameters and the Key-Value (KV) cache consume substantial accelerator memory, hindering the utilization of large batch sizes. Existing methods to tackle this problem can be conceptually divided into two main categories: (1) Reducing memory consumption, thereby minimizing memory transfer overhead and enabling larger batch sizes, and (2) Minimizing the number of decoding steps to decrease latency directly.

\paragraph{Reducing KV Cache.} Methods such as Multi-query attention~\citep{shazeer2019fast} and Grouped-query attention~\citep{ainslie2023gqa} adopt a direct approach to diminish the KV cache. By utilizing fewer key and value heads in the attention modules relative to query heads, these strategies substantially cut the KV's memory consumption, thereby facilitating larger batch sizes and enhanced accelerator utilization~\citep{pope2022efficiently}. Additionally, \citet{zhang2023h} proposes to selectively retain the most critical KV tokens, further reducing the KV cache. From a system perspective, \citet{kwon2023efficient} introduces a paged memory management scheme for reducing fragmentation of the KV cache.

\paragraph{Quantization.} Quantization techniques are extensively used to shrink LLMs' memory consumption. \citet{xiao2023smoothquant} apply rescaling between activations and parameters to eliminate outliers and simplify the quantization process. \citet{dettmers2022llm} breaks down matrix multiplications into predominantly 8-bit and a minority of 16-bit operations. \citet{frantar2022gptq} iteratively round weight columns into 3/4 bits, while \citet{lin2023awq} present an activation-aware quantization scheme to protect salient weights and compress LLMs to 3/4 bits. \citet{kim2023squeezellm} introduce a sparse plus low-precision pattern to handle a minor portion of vital weights, among other techniques.

\paragraph{Speculative Decoding.} As an approach orthogonal to the aforementioned methods, speculative decoding~\citep{leviathan2022fast,chen2023accelerating} aims to execute several decoding steps in parallel, thus reducing the total number of steps required. This parallelization is realized by employing a smaller draft model to conjecture several subsequent words, which the LLMs then collectively evaluate and accept as appropriate. While resonating with non-autoregressive generation literature~\citep{xiao2023survey}, this method is specifically tailored for LLMs to address the aforementioned inefficiency. Unlike previous works, we propose leveraging the original model to make predictions rather than introducing an additional draft model. This approach is more straightforward and seamlessly integrates into existing systems without the complexities of managing two models. Independently, \citet{miao2023specinfer, spector2023accelerating} propose the use of tree-structured attention to generate multiple candidates in parallel, where \citet{miao2023specinfer} suggest employing an ensemble of models to propose candidates, and \citet{spector2023accelerating} advocate adding another hierarchy for the draft model. 
\textcolor{black}{However, draft models require specialized pretraining and alignment with the target models. While employing multiple draft models can be cumbersome and involves the complexity of managing parallelism, our approach, which relies solely on decoding heads, offers a simpler alternative. \citet{miao2023specinfer} employ multiple draft models to generate tokens and merge them using tree attention, while \citet{spector2023accelerating} utilize a small draft model to process each level of the tree in batches. In contrast, our method directly uses the top predicted tokens from each of \ours heads to create a static sparse tree without autoregression or adjusting the tree structure. This approach simplifies the process and improves efficiency. Additionally, we demonstrate through a detailed ablation study how the nodes of the tree can affect decoding speed.}


\subsection{Sampling Scheme}
The manner in which text is sampled from Large Language Models (LLMs) can significantly influence the quality of the generated output. Recent studies have revealed that direct sampling from a language model may lead to incoherent or nonsensical results~\citep{pillutla2021mauve,holtzman2020curious}. In response to this challenge, \emph{truncation sampling} schemes have been introduced~\citep{fan2018hierarchical,basu2021mirostat,meister2022probability,hewitt2022truncation,meister2023locally}. These approaches aim to produce high-quality and diverse samples by performing sampling on a truncated distribution over a specific \emph{allowed set} at each decoding step.

Different strategies define this allowed set in various ways. For example, top-$k$ sampling~\citep{fan2018hierarchical} retains the $k$ most likely words, whereas top-$p$ sampling~\citep{holtzman2020curious} incorporates the minimal set of words that account for $p$ percent of the probability. Another method, known as typical decoding~\citep{meister2023locally}, employs the entropy of the predicted distribution to establish the threshold for inclusion. \citet{hewitt2022truncation} offers a unified framework to understand truncation sampling techniques comprehensively.

Drawing inspiration from these methods, our typical acceptance scheme aligns with the concept of defining an allowed set to exclude improbable candidates from the sampling process. However, we diverge because we do not insist on an exact correspondence between the output and language model distribution. This deviation allows us to facilitate more diverse yet high-quality outputs, achieving greater efficiency without compromising the integrity of the generated text.