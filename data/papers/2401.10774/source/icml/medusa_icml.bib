@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@misc{ding2023enhancing,
      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, 
      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
      year={2023},
      eprint={2305.14233},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{dettmers20218bit,
  title     = {8-bit Optimizers via Block-wise Quantization},
  author    = {Tim Dettmers and M. Lewis and Sam Shleifer and Luke Zettlemoyer},
  journal   = {International Conference on Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/11fe37ab6faf6bf85ad2f5746c154dec5412bd04}
}

@article{hu2021lora,
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  author    = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal   = {ICLR},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/a8ca46b171467ceb2d7652fbfb67fe701ad86092}
}

@article{kim2016sequencelevel,
  title   = {Sequence-Level Knowledge Distillation},
  author  = {Yoon Kim and Alexander M. Rush},
  year    = {2016},
  journal = {EMNLP}
}

@misc{tunstall2023zephyr,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
      eprint={2310.16944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kumar2022finetuning,
  title     = {Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  author    = {Ananya Kumar and Aditi Raghunathan and Robbie Jones and Tengyu Ma and Percy Liang},
  journal   = {International Conference on Learning Representations},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@article{zhou2023distillspec,
  title   = {DistillSpec: Improving Speculative Decoding via Knowledge Distillation},
  author  = {Yongchao Zhou and Kaifeng Lyu and Ankit Singh Rawat and Aditya Krishna Menon and Afshin Rostamizadeh and Sanjiv Kumar and Jean-François Kagy and Rishabh Agarwal},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2310.08461}
}

@article{liu2023online,
  title   = {Online Speculative Decoding},
  author  = {Xiaoxuan Liu and Lanxiang Hu and Peter Bailis and Ion Stoica and Zhijie Deng and Alvin Cheung and Hao Zhang},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2310.07177}
}

@article{su2023synergy,
  title={The Synergy of Speculative Decoding and Batching in Serving Large Language Models},
  author={Su, Qidong and Giannoula, Christina and Pekhimenko, Gennady},
  journal={arXiv preprint arXiv:2310.18813},
  year={2023}
}

@article{he2023rest,
  title   = {REST: Retrieval-Based Speculative Decoding},
  author  = {Zhenyu He and Zexuan Zhong and Tianle Cai and Jason D Lee and Di He},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2311.08252}
}

@misc{fu2023lookahead,
    title = {Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding},
    url = {https://lmsys.org/blog/2023-11-21-lookahead-decoding/},
    author = {Yichao Fu and Peter Bailis and Ion Stoica and Hao Zhang},
    month = {November},
    year = {2023}
}

@misc{dubois2023alpacafarm,
  title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}, 
  author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  year={2023},
  eprint={2305.14387},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{sharegpt2023,
    author = {ShareGPT},
    title = {{ShareGPT}},
    howpublished = {\url{https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered}},
    year = {2023},
}

@misc{axolotl2023,
    author = {Axolotl},
    title = {{Axolotl}},
    howpublished = {\url{https://github.com/OpenAccess-AI-Collective/axolotl}},
    year = {2023},
}

@article{ying2021transformers,
  title={Do transformers really perform badly for graph representation?},
  author={Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28877--28888},
  year={2021}
}

@inproceedings{
pillutla2021mauve,
title={{MAUVE}: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers},
author={Krishna Pillutla and Swabha Swayamdipta and Rowan Zellers and John Thickstun and Sean Welleck and Yejin Choi and Zaid Harchaoui},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Tqx7nJp7PR}
}

@inproceedings{
Holtzman2020The,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}

@inproceedings{meister2021language,
  title={Language Model Evaluation Beyond Perplexity},
  author={Meister, Clara and Cotterell, Ryan},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5328--5339},
  year={2021}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{xiao2023survey,
  title={A survey on non-autoregressive generation for neural machine translation and beyond},
  author={Xiao, Yisheng and Wu, Lijun and Guo, Junliang and Li, Juntao and Zhang, Min and Qin, Tao and Liu, Tie-yan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{chee2023quip,
  title={QuIP: 2-Bit Quantization of Large Language Models With Guarantees},
  author={Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher},
  journal={arXiv preprint arXiv:2307.13304},
  year={2023}
}

@article{zhang2023h,
  title={H $ \_2 $ O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={arXiv preprint arXiv:2306.14048},
  year={2023}
}

@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{kim2023squeezellm,
  title={SqueezeLLM: Dense-and-Sparse Quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2306.07629},
  year={2023}
}

@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{spector2023accelerating,
  title={Accelerating LLM Inference with Staged Speculative Decoding},
  author={Spector, Benjamin and Re, Chris},
  journal={arXiv preprint arXiv:2308.04623},
  year={2023}
}

@article{miao2023specinfer,
  title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Wong, Rae Ying Yee and Chen, Zhuoming and Arfeen, Daiyaan and Abhyankar, Reyna and Jia, Zhihao},
  journal={arXiv preprint arXiv:2305.09781},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{gptcache2023,
  title={GPTCache},
  author={Zilliz},
  year={2023},
  publisher={GitHub},
  howpublished={\url{https://github.com/zilliztech/GPTCache}},
}

@article{kim2023language,
  title     = {Language Models can Solve Computer Tasks},
  author    = {Geunwoo Kim and P. Baldi and S. McAleer},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2303.17491},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/f0880936cc1b6dcc6f5d27b55931f3b87948131d}
}

@article{wang2023voyager,
  title={Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2305.16291},
  year={2023}
}

@misc{chen2023transformer,
  author = {Lequn Chen},
  title = {Dissecting Batching Effects in GPT Inference},
  year = {2023},
  howpublished = {\url{https://le.qun.ch/en/blog/2023/05/13/transformer-batching/}},
  note = {Blog}
}

@article{qin2023tool,
  title={Tool learning with foundation models},
  author={Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize and Ding, Ning and Cui, Ganqu and Zeng, Zheni and Huang, Yufei and Xiao, Chaojun and Han, Chi and others},
  journal={arXiv preprint arXiv:2304.08354},
  year={2023}
}
@misc{nvidia_a100_datasheet,
  author       = {NVIDIA},
  title        = {NVIDIA A100 Tensor Core GPU},
}

@article{qian2023creator,
  title={CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation},
  author={Qian, Cheng and Han, Chi and Fung, Yi R and Qin, Yujia and Liu, Zhiyuan and Ji, Heng},
  journal={arXiv preprint arXiv:2305.14318},
  year={2023}
}

@inproceedings{wu2022ai,
  title={Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts},
  author={Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--22},
  year={2022}
}

@article{amini2019mathqa,
  title={Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  author={Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1905.13319},
  year={2019}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.03057},
  year={2022}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{ling-etal-2017-program,
    title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
    author = "Ling, Wang  and
      Yogatama, Dani  and
      Dyer, Chris  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1015",
    doi = "10.18653/v1/P17-1015",
    pages = "158--167",
}

@article{velivckovic2021neural,
  title={Neural algorithmic reasoning},
  author={Veli{\v{c}}kovi{\'c}, Petar and Blundell, Charles},
  journal={Patterns},
  volume={2},
  number={7},
  pages={100273},
  year={2021},
  publisher={Elsevier}
}

@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@article{liang2023taskmatrix,
  title={Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis},
  author={Liang, Yaobo and Wu, Chenfei and Song, Ting and Wu, Wenshan and Xia, Yan and Liu, Yu and Ou, Yang and Lu, Shuai and Ji, Lei and Mao, Shaoguang and others},
  journal={arXiv preprint arXiv:2303.16434},
  year={2023}
}

@article{jojic2023gpt,
  title={GPT is becoming a Turing machine: Here are some ways to program it},
  author={Jojic, Ana and Wang, Zhen and Jojic, Nebojsa},
  journal={arXiv preprint arXiv:2303.14310},
  year={2023}
}

@article{paranjape2023art,
  title={ART: Automatic multi-step reasoning and tool-use for large language models},
  author={Paranjape, Bhargavi and Lundberg, Scott and Singh, Sameer and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Ribeiro, Marco Tulio},
  journal={arXiv preprint arXiv:2303.09014},
  year={2023}
}

@article{yang2023mm,
  title={Mm-react: Prompting chatgpt for multimodal reasoning and action},
  author={Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
  journal={arXiv preprint arXiv:2303.11381},
  year={2023}
}

@article{suzgun2022challenging,
  title={Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{kosinski2023theory,
  title={Theory of mind may have spontaneously emerged in large language models},
  author={Kosinski, Michal},
  journal={arXiv preprint arXiv:2302.02083},
  year={2023}
}

@article{shen2023hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={arXiv preprint arXiv:2303.17580},
  year={2023}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{lu2023chameleon,
  title={Chameleon: Plug-and-play compositional reasoning with large language models},
  author={Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.09842},
  year={2023}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{chen2023teaching,
  title     = {Teaching Large Language Models to Self-Debug},
  author    = {Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2304.05128},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/eb55442d940accc67d6790e66d3196e1941fc7a4}
}

@book{halbert1984programming,
  title={Programming by example},
  author={Halbert, Daniel Conrad},
  year={1984},
  publisher={University of California, Berkeley}
}

@misc{schick2023toolformer,
      title={Toolformer: Language Models Can Teach Themselves to Use Tools}, 
      author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
      year={2023},
      eprint={2302.04761},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@misc{parisi2022talm,
      title={TALM: Tool Augmented Language Models}, 
      author={Aaron Parisi and Yao Zhao and Noah Fiedel},
      year={2022},
      eprint={2205.12255},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gao2023pal,
      title={PAL: Program-aided Language Models}, 
      author={Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
      year={2023},
      eprint={2211.10435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2022program,
      title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks}, 
      author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
      year={2022},
      eprint={2211.12588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{
xia2023speculative,
title={Speculative Decoding: Lossless Speedup of Autoregressive Translation},
author={Heming Xia and Tao Ge and Si-Qing Chen and Furu Wei and Zhifang Sui},
year={2023},
url={https://openreview.net/forum?id=H-VlwsYvVi}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{google2023palm2 ,
      title={PaLM 2 Technical Report}, 
      author={Google},
      year={2023},
      url={https://ai.google/static/documents/palm2techreport.pdf},
}

@inproceedings{
yao2023react,
title={ReAct: Synergizing Reasoning and Acting in Language Models},
author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WE_vluYUL-X}
}

@inproceedings{
liu2023minds,
title={Mind's Eye: Grounded Language Model Reasoning through Simulation},
author={Ruibo Liu and Jason Wei and Shixiang Shane Gu and Te-Yen Wu and Soroush Vosoughi and Claire Cui and Denny Zhou and Andrew M. Dai},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=4rXMRuoJlai}
}

@misc{dohan2022language,
      title={Language Model Cascades}, 
      author={David Dohan and Winnie Xu and Aitor Lewkowycz and Jacob Austin and David Bieber and Raphael Gontijo Lopes and Yuhuai Wu and Henryk Michalewski and Rif A. Saurous and Jascha Sohl-dickstein and Kevin Murphy and Charles Sutton},
      year={2022},
      eprint={2207.10342},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2023frugalgpt,
      title={FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance}, 
      author={Lingjiao Chen and Matei Zaharia and James Zou},
      year={2023},
      eprint={2305.05176},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@misc{arora2023language,
      title={Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes}, 
      author={Simran Arora and Brandon Yang and Sabri Eyuboglu and Avanika Narayan and Andrew Hojel and Immanuel Trummer and Christopher Ré},
      year={2023},
      eprint={2304.09433},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@Article{wang2022towards,
  author  = {Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  journal = {arXiv preprint arXiv:2212.10001},
  title   = {Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
  year    = {2022},
}

@Article{brown2020language,
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  title   = {Language models are few-shot learners},
  year    = {2020},
  pages   = {1877--1901},
  volume  = {33},
}

@InProceedings{ye2021crossfit,
  author    = {Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  title     = {CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP},
  year      = {2021},
  pages     = {7163--7189},
}

@InProceedings{khashabi2020unifiedqa,
  author    = {Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  title     = {UNIFIEDQA: Crossing Format Boundaries with a Single QA System},
  year      = {2020},
  pages     = {1896--1907},
}

@InProceedings{izacard2021leveraging,
  author    = {Izacard, Gautier and Grave, {\'E}douard},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  title     = {Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  year      = {2021},
  pages     = {874--880},
}


@InProceedings{lu2022fantastically,
  author    = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  year      = {2022},
  address   = {Dublin, Ireland},
  month     = may,
  pages     = {8086--8098},
  publisher = {Association for Computational Linguistics},
  abstract  = {When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are {``}fantastic{''} and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13{\%} relative improvement for GPT-family models across eleven different established text classification tasks.},
  doi       = {10.18653/v1/2022.acl-long.556},
  url       = {https://aclanthology.org/2022.acl-long.556},
}


@Article{min2022rethinking,
  author  = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:2202.12837},
  title   = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  year    = {2022},
}

@Article{raffel2020exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal = {Journal of Machine Learning Research},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year    = {2020},
  number  = {140},
  pages   = {1-67},
  volume  = {21},
  url     = {http://jmlr.org/papers/v21/20-074.html},
}

@Article{radford2019language,
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  title   = {Language models are unsupervised multitask learners},
  year    = {2019},
  number  = {8},
  pages   = {9},
  volume  = {1},
}

@Article{min2021metaicl,
  author  = {Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal = {arXiv preprint arXiv:2110.15943},
  title   = {Metaicl: Learning to learn in context},
  year    = {2021},
}

@Article{patel2022bidirectional,
  author  = {Patel, Ajay and Li, Bryan and Rasooli, Mohammad Sadegh and Constant, Noah and Raffel, Colin and Callison-Burch, Chris},
  journal = {arXiv preprint arXiv:2209.14500},
  title   = {Bidirectional Language Models Are Also Few-shot Learners},
  year    = {2022},
}


@InProceedings{min2022noisy,
  author    = {Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Noisy Channel Language Model Prompting for Few-Shot Text Classification},
  year      = {2022},
  address   = {Dublin, Ireland},
  month     = may,
  pages     = {5316--5330},
  publisher = {Association for Computational Linguistics},
  abstract  = {We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.},
  doi       = {10.18653/v1/2022.acl-long.365},
  url       = {https://aclanthology.org/2022.acl-long.365},
}

@InProceedings{zhao2021calibrate,
  author       = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle    = {International Conference on Machine Learning},
  title        = {Calibrate before use: Improving few-shot performance of language models},
  year         = {2021},
  organization = {PMLR},
  pages        = {12697--12706},
}

@InProceedings{liu2022what,
  author    = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, William B and Carin, Lawrence and Chen, Weizhu},
  booktitle = {Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
  title     = {What Makes Good In-Context Examples for GPT-3?},
  year      = {2022},
  pages     = {100--114},
}

@InProceedings{wei2021finetuned,
  author    = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle = {International Conference on Learning Representations},
  title     = {Finetuned Language Models are Zero-Shot Learners},
  year      = {2021},
}

@InProceedings{xie2021explanation,
  author    = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle = {International Conference on Learning Representations},
  title     = {An Explanation of In-context Learning as Implicit Bayesian Inference},
  year      = {2021},
}

@Article{rubin2021learning,
  author  = {Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  journal = {arXiv preprint arXiv:2112.08633},
  title   = {Learning to retrieve prompts for in-context learning},
  year    = {2021},
}

@Article{ouyang2022training,
  author  = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {arXiv preprint arXiv:2203.02155},
  title   = {Training language models to follow instructions with human feedback},
  year    = {2022},
}

@InProceedings{garg2022what,
  author    = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  year      = {2022},
}

@Article{beltagy2020longformer,
  author  = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal = {arXiv preprint arXiv:2004.05150},
  title   = {Longformer: The long-document transformer},
  year    = {2020},
}

@Article{chan2022data,
  author  = {Chan, Stephanie CY and Santoro, Adam and Lampinen, Andrew K and Wang, Jane X and Singh, Aaditya and Richemond, Pierre H and McClelland, Jay and Hill, Felix},
  journal = {arXiv preprint arXiv:2205.05055},
  title   = {Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers},
  year    = {2022},
}

@InProceedings{vaswani2017attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  title     = {Attention is all you need},
  year      = {2017},
  pages     = {5998--6008},
}

@Article{child2019generating,
  author  = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:1904.10509},
  title   = {Generating long sequences with sparse transformers},
  year    = {2019},
}

@Misc{wang2020linformer,
  author        = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
  title         = {Linformer: Self-Attention with Linear Complexity},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.04768},
  primaryclass  = {cs.LG},
}

@InProceedings{kitaev2019reformer,
  author    = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle = {International Conference on Learning Representations},
  title     = {Reformer: The Efficient Transformer},
  year      = {2019},
}


@Article{daras2020smyrf,
  author  = {Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G},
  journal = {Advances in Neural Information Processing Systems},
  title   = {SMYRF-Efficient Attention using Asymmetric Clustering},
  year    = {2020},
  volume  = {33},
}

@Misc{ho2019axial,
  author        = {Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},
  title         = {Axial Attention in Multidimensional Transformers},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1912.12180},
  primaryclass  = {cs.CV},
}

@InProceedings{qiu2020blockwise,
  author    = {Qiu, Jiezhong and Ma, Hao and Levy, Omer and Yih, Wen-tau and Wang, Sinong and Tang, Jie},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  title     = {Blockwise Self-Attention for Long Document Understanding},
  year      = {2020},
  pages     = {2555--2565},
}


@Article{tay2020synthesizer,
  author  = {Tay, Y and Bahri, D and Metzler, D and Juan, D and Zhao, Z and Zheng, C},
  journal = {arXiv preprint arXiv:2005.00743},
  title   = {Synthesizer: Rethinking self-attention in transformer models. arXiv 2020},
  year    = {2020},
  volume  = {2},
}

@Article{zaheer2020big,
  author  = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Big bird: Transformers for longer sequences},
  year    = {2020},
  pages   = {17283--17297},
  volume  = {33},
}

@InProceedings{peng2021random,
  author    = {Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
  booktitle = {International Conference on Learning Representations},
  title     = {Random Feature Attention},
  year      = {2021},
  url       = {https://openreview.net/forum?id=QtTKTdVrFBB},
}

@InProceedings{katharopoulos2020transformers,
  author       = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle    = {International Conference on Machine Learning},
  title        = {Transformers are rnns: Fast autoregressive transformers with linear attention},
  year         = {2020},
  organization = {PMLR},
  pages        = {5156--5165},
}

@InProceedings{qin2021cosformer,
  author    = {Qin, Zhen and Sun, Weixuan and Deng, Hui and Li, Dongxu and Wei, Yunshen and Lv, Baohong and Yan, Junjie and Kong, Lingpeng and Zhong, Yiran},
  booktitle = {International Conference on Learning Representations},
  title     = {cosFormer: Rethinking Softmax In Attention},
  year      = {2021},
}

@Article{akyuerek2022what,
  author  = {Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal = {arXiv preprint arXiv:2211.15661},
  title   = {What learning algorithm is in-context learning? Investigations with linear models},
  year    = {2022},
}

@Article{chung2022scaling,
  author  = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal = {arXiv preprint arXiv:2210.11416},
  title   = {Scaling instruction-finetuned language models},
  year    = {2022},
}

@Article{tay2022unifying,
  author  = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal = {arXiv preprint arXiv:2205.05131},
  title   = {Unifying Language Learning Paradigms},
  year    = {2022},
}

@InProceedings{sanh2021multitask,
  author    = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and others},
  booktitle = {International Conference on Learning Representations},
  title     = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  year      = {2021},
}

@InProceedings{shazeer2018adafactor,
  author       = {Shazeer, Noam and Stern, Mitchell},
  booktitle    = {International Conference on Machine Learning},
  title        = {Adafactor: Adaptive learning rates with sublinear memory cost},
  year         = {2018},
  organization = {PMLR},
  pages        = {4596--4604},
}

@Article{ye2022guess,
  author  = {Seonghyeon Ye and Doyoung Kim and Joel Jang and Joongbo Shin and Minjoon Seo},
  journal = {arXiv preprint arXiv: Arxiv-2210.02969},
  title   = {Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners},
  year    = {2022},
}

@Unpublished{ye2022investigating,
  author  = {Ye, Qinyuan and Beltagy, Iz and Peters, Matthew E. and Ren, Xiang and Hajishirzi, Hannaneh},
  note    = {preprint under review},
  title   = {Investigating Fusion Methods for In-Context Learning},
  year    = {2022},
  journal = {OpenReview Preprint},
}

@Article{ratner2022parallel,
  author  = {Ratner, Nir and Levine, Yoav and Belinkov, Yonatan and Ram, Ori and Abend, Omri and Karpas, Ehud and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal = {arXiv preprint arXiv:2212.10947},
  title   = {Parallel Context Windows Improve In-Context Learning of Large Language Models},
  year    = {2022},
}

@Article{hao2022structured,
  author  = {Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},
  journal = {arXiv preprint arXiv:2212.06713},
  title   = {Structured Prompting: Scaling In-Context Learning to 1,000 Examples},
  year    = {2022},
}

@Article{oswald2022transformers,
  author  = {von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  journal = {arXiv preprint arXiv:2212.07677},
  title   = {Transformers learn in-context by gradient descent},
  year    = {2022},
}

@Article{dai2022why,
  author  = {Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  journal = {arXiv preprint arXiv:2212.10559},
  title   = {Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers},
  year    = {2022},
}

@Article{wei2022chain,
  author  = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal = {arXiv preprint arXiv:2201.11903},
  title   = {Chain of thought prompting elicits reasoning in large language models},
  year    = {2022},
}

@Article{kojima2022large,
  author  = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal = {arXiv preprint arXiv:2205.11916},
  title   = {Large Language Models are Zero-Shot Reasoners},
  year    = {2022},
}

@Article{wang2022self,
  author  = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal = {arXiv preprint arXiv:2203.11171},
  title   = {Self-consistency improves chain of thought reasoning in language models},
  year    = {2022},
}

@InProceedings{zaken2022bitfit,
  author    = {Zaken, Elad Ben and Goldberg, Yoav and Ravfogel, Shauli},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  title     = {BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  year      = {2022},
  pages     = {1--9},
}


@InProceedings{houlsby2019parameter,
  author       = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle    = {International Conference on Machine Learning},
  title        = {Parameter-efficient transfer learning for NLP},
  year         = {2019},
  organization = {PMLR},
  pages        = {2790--2799},
}

@InProceedings{henderson2021compacter,
  author    = {Henderson, James and Ruder, Sebastian and others},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
  year      = {2021},
}

@Article{liu2022few,
  author  = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  journal = {arXiv preprint arXiv:2205.05638},
  title   = {Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  year    = {2022},
}

@InProceedings{lester2021power,
  author    = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  title     = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  year      = {2021},
  pages     = {3045--3059},
}

@Article{sung2021training,
  author  = {Sung, Yi-Lin and Nair, Varun and Raffel, Colin A},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Training neural networks with fixed sparse masks},
  year    = {2021},
  pages   = {24193--24205},
  volume  = {34},
}

@InProceedings{li2021prefix,
  author    = {Li, Xiang Lisa and Liang, Percy},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  title     = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  year      = {2021},
  pages     = {4582--4597},
}

@Article{rebuffi2017learning,
  author  = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  journal = {Advances in neural information processing systems},
  title   = {Learning multiple visual domains with residual adapters},
  year    = {2017},
  volume  = {30},
}

@InProceedings{bapna2019simple,
  author    = {Bapna, Ankur and Firat, Orhan},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {Simple, Scalable Adaptation for Neural Machine Translation},
  year      = {2019},
  pages     = {1538--1548},
}

@InProceedings{guo2021parameter,
  author    = {Guo, Demi and Rush, Alexander M and Kim, Yoon},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  title     = {Parameter-Efficient Transfer Learning with Diff Pruning},
  year      = {2021},
  pages     = {4884--4896},
}
@Article{liu2021p,
  author  = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal = {arXiv preprint arXiv:2110.07602},
  title   = {P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  year    = {2021},
}

@Article{an2022input,
  author  = {An, Shengnan and Li, Yifei and Lin, Zeqi and Liu, Qian and Chen, Bei and Fu, Qiang and Chen, Weizhu and Zheng, Nanning and Lou, Jian-Guang},
  journal = {arXiv preprint arXiv:2203.03131},
  title   = {Input-tuning: Adapting unfamiliar inputs to frozen pretrained models},
  year    = {2022},
}

@Article{chen2022adaprompt,
  author  = {Chen, Yulong and Liu, Yang and Dong, Li and Wang, Shuohang and Zhu, Chenguang and Zeng, Michael and Zhang, Yue},
  journal = {arXiv preprint arXiv:2202.04824},
  title   = {AdaPrompt: Adaptive Model Training for Prompt-based NLP},
  year    = {2022},
}

@InProceedings{gao2021making,
  author    = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  title     = {Making Pre-trained Language Models Better Few-shot Learners},
  year      = {2021},
  pages     = {3816--3830},
}

@InProceedings{chen2022meta,
  author    = {Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Meta-learning via Language Model In-context Tuning},
  year      = {2022},
  address   = {Dublin, Ireland},
  month     = may,
  pages     = {719--730},
  publisher = {Association for Computational Linguistics},
  abstract  = {The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose $\textit{in-context tuning}$ (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6{\%} average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10{\%}, and reduces the variance due to example ordering by 6x and example choices by 2x.},
  doi       = {10.18653/v1/2022.acl-long.53},
  url       = {https://aclanthology.org/2022.acl-long.53},
}

@InProceedings{chen2022improving,
  author    = {Chen, Mingda and Du, Jingfei and Pasunuru, Ramakanth and Mihaylov, Todor and Iyer, Srini and Stoyanov, Veselin and Kozareva, Zornitsa},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {Improving In-Context Few-Shot Learning via Self-Supervised Training},
  year      = {2022},
  address   = {Seattle, United States},
  month     = jul,
  pages     = {3558--3573},
  publisher = {Association for Computational Linguistics},
  abstract  = {Self-supervised pretraining has made few-shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in-context few-shot learning. In this paper, we propose to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning. We propose and evaluate four self-supervised objectives on two benchmarks. We find that the intermediate self-supervision stage produces models that outperform strong baselines. Ablation study shows that several factors affect the downstream performance, such as the amount of training data and the diversity of the self-supervised objectives. Human-annotated cross-task supervision and self-supervision are complementary. Qualitative analysis suggests that the self-supervised-trained models are better at following task requirements.},
  doi       = {10.18653/v1/2022.naacl-main.260},
  url       = {https://aclanthology.org/2022.naacl-main.260},
}

@Article{lyu2022z,
  author  = {Lyu, Xinxi and Min, Sewon and Beltagy, Iz and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal = {arXiv preprint arXiv:2212.09865},
  title   = {Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations},
  year    = {2022},
}

@Article{qiao2022reasoning,
  author  = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  journal = {arXiv preprint arXiv:2212.09597},
  title   = {Reasoning with Language Model Prompting: A Survey},
  year    = {2022},
}

@Article{li2023transformers,
  author  = {Li, Yingcong and Ildiz, M Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  journal = {arXiv preprint arXiv:2301.07067},
  title   = {Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning},
  year    = {2023},
}

@Article{tay2022efficient,
  author    = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal   = {ACM Computing Surveys},
  title     = {Efficient transformers: A survey},
  year      = {2022},
  number    = {6},
  pages     = {1--28},
  volume    = {55},
  publisher = {ACM New York, NY},
}

@InProceedings{zhou2021informer,
  author    = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Informer: Beyond efficient transformer for long sequence time-series forecasting},
  year      = {2021},
  number    = {12},
  pages     = {11106--11115},
  volume    = {35},
}

@Article{chowdhery2022palm,
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal = {arXiv preprint arXiv:2204.02311},
  title   = {Palm: Scaling language modeling with pathways},
  year    = {2022},
}

@Article{zhang2022opt,
  author  = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal = {arXiv preprint arXiv:2205.01068},
  title   = {Opt: Open pre-trained transformer language models},
  year    = {2022},
}

@Article{hoffmann2022training,
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  title   = {Training Compute-Optimal Large Language Models},
  year    = {2022},
}

@InProceedings{holtzman2021surface,
  author    = {Holtzman, Ari and West, Peter and Shwartz, Vered and Choi, Yejin and Zettlemoyer, Luke},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  title     = {Surface Form Competition: Why the Highest Probability Answer Isn{'}t Always Right},
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  month     = nov,
  pages     = {7038--7051},
  publisher = {Association for Computational Linguistics},
  abstract  = {Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition{---}wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. {``}computer{''} and {``}PC.{''} Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated and uncalibrated scoring functions on all GPT-2 and GPT-3 models on a variety of multiple choice datasets.},
  doi       = {10.18653/v1/2021.emnlp-main.564},
  url       = {https://aclanthology.org/2021.emnlp-main.564},
}

@Article{rao2021global,
  author  = {Rao, Yongming and Zhao, Wenliang and Zhu, Zheng and Lu, Jiwen and Zhou, Jie},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Global filter networks for image classification},
  year    = {2021},
  pages   = {980--993},
  volume  = {34},
}

@Article{vandenoord2016conditional,
  author  = {Van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and Vinyals, Oriol and Graves, Alex and others},
  journal = {Advances in neural information processing systems},
  title   = {Conditional image generation with pixelcnn decoders},
  year    = {2016},
  volume  = {29},
}

@Article{oord2016wavenet,
  author  = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal = {arXiv preprint arXiv:1609.03499},
  title   = {Wavenet: A generative model for raw audio},
  year    = {2016},
}

@InProceedings{romero2021ckconv,
  author    = {Romero, David W and Kuzina, Anna and Bekkers, Erik J and Tomczak, Jakub Mikolaj and Hoogendoorn, Mark},
  booktitle = {International Conference on Learning Representations},
  title     = {CKConv: Continuous Kernel Convolution For Sequential Data},
  year      = {2021},
}

@InProceedings{romero2021flexconv,
  author    = {Romero, David W and Bruintjes, Robert-Jan and Tomczak, Jakub Mikolaj and Bekkers, Erik J and Hoogendoorn, Mark and van Gemert, Jan},
  booktitle = {International Conference on Learning Representations},
  title     = {FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes},
  year      = {2021},
}

@Article{romero2022towards,
  author  = {Romero, David W and Knigge, David M and Gu, Albert and Bekkers, Erik J and Gavves, Efstratios and Tomczak, Jakub M and Hoogendoorn, Mark},
  journal = {arXiv preprint arXiv:2206.03398},
  title   = {Towards a General Purpose CNN for Long Range Dependencies in ND},
  year    = {2022},
}

@Article{gupta2022diagonal,
  author  = {Gupta, Ankit},
  journal = {arXiv preprint arXiv:2203.14343},
  title   = {Diagonal State Spaces are as Effective as Structured State Spaces},
  year    = {2022},
}

@Article{leethorp2021fnet,
  author  = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal = {arXiv preprint arXiv:2105.03824},
  title   = {Fnet: Mixing tokens with fourier transforms},
  year    = {2021},
}

@Article{hasani2022liquid,
  author  = {Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal = {arXiv preprint arXiv:2209.12951},
  title   = {Liquid structural state-space models},
  year    = {2022},
}

@Article{smith2022simplified,
  author  = {Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  journal = {arXiv preprint arXiv:2208.04933},
  title   = {Simplified state space layers for sequence modeling},
  year    = {2022},
}

@InProceedings{guibas2021efficient,
  author    = {Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  booktitle = {International Conference on Learning Representations},
  title     = {Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators},
  year      = {2021},
}

@Article{dai2021coatnet,
  author  = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V and Tan, Mingxing},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Coatnet: Marrying convolution and attention for all data sizes},
  year    = {2021},
  pages   = {3965--3977},
  volume  = {34},
}

@Article{luo2021stable,
  author  = {Luo, Shengjie and Li, Shanda and Cai, Tianle and He, Di and Peng, Dinglan and Zheng, Shuxin and Ke, Guolin and Wang, Liwei and Liu, Tie-Yan},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Stable, fast and accurate: Kernelized attention with relative positional encoding},
  year    = {2021},
  pages   = {22795--22807},
  volume  = {34},
}

@InProceedings{wu2021rethinking,
  author    = {Wu, Kan and Peng, Houwen and Chen, Minghao and Fu, Jianlong and Chao, Hongyang},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title     = {Rethinking and improving relative position encoding for vision transformer},
  year      = {2021},
  pages     = {10033--10041},
}

@Misc{tay2020efficient,
  author        = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
  title         = {Efficient Transformers: A Survey},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2009.06732},
  primaryclass  = {cs.LG},
}

@Misc{nagel2016cc,
  author = {Nagel, Sebastian},
  title  = {Cc-news},
  year   = {2016},
}

@Article{trinh2018simple,
  author  = {Trinh, Trieu H and Le, Quoc V},
  journal = {arXiv preprint arXiv:1806.02847},
  title   = {A simple method for commonsense reasoning},
  year    = {2018},
}

@InProceedings{dosovitskiy2021image,
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {International Conference on Learning Representations},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year      = {2021},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
}


@Article{gulati2020conformer,
  author  = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal = {Proc. Interspeech 2020},
  title   = {Conformer: Convolution-augmented Transformer for Speech Recognition},
  year    = {2020},
  pages   = {5036--5040},
}

@Article{linsley2018learning,
  author  = {Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Windolf, Charles and Serre, Thomas},
  journal = {Advances in neural information processing systems},
  title   = {Learning long-range spatial dependencies with horizontal gated recurrent units},
  year    = {2018},
  volume  = {31},
}

@InProceedings{bao2020unilmv2,
  author       = {Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Piao, Songhao and Zhou, Ming and others},
  booktitle    = {International Conference on Machine Learning},
  title        = {Unilmv2: Pseudo-masked language models for unified language model pre-training},
  year         = {2020},
  organization = {PMLR},
  pages        = {642--652},
}

@Book{gray2006toeplitz,
  author    = {Gray, Robert M},
  publisher = {now publishers inc},
  title     = {Toeplitz and circulant matrices: A review},
  year      = {2006},
}

@Article{narang2021do,
  author  = {Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, William and Fevry, Thibault and Matena, Michael and Malkan, Karishma and Fiedel, Noah and Shazeer, Noam and Lan, Zhenzhong and others},
  journal = {arXiv preprint arXiv:2102.11972},
  title   = {Do Transformer Modifications Transfer Across Implementations and Applications?},
  year    = {2021},
}

@Misc{ziegler2020fine,
  author        = {Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
  title         = {Fine-Tuning Language Models from Human Preferences},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1909.08593},
  primaryclass  = {cs.CL},
}


@Misc{raffel2020exploringa,
  author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1910.10683},
  primaryclass  = {cs.LG},
}

@Misc{parisotto2019stabilizing,
  author        = {Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},
  title         = {Stabilizing Transformers for Reinforcement Learning},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1910.06764},
  primaryclass  = {cs.LG},
}

@InProceedings{carion2020end,
  author       = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle    = {European Conference on Computer Vision},
  title        = {End-to-end object detection with transformers},
  year         = {2020},
  organization = {Springer},
  pages        = {213--229},
}

@Article{kingma2013auto,
  author  = {Kingma, Diederik P and Welling, Max},
  journal = {arXiv preprint arXiv:1312.6114},
  title   = {Auto-encoding variational bayes},
  year    = {2013},
}

@Article{salimans2016weight,
  author  = {Salimans, Tim and Kingma, Durk P},
  journal = {Advances in neural information processing systems},
  title   = {Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  year    = {2016},
  volume  = {29},
}

@Article{zagoruyko2017diracnets,
  author  = {Zagoruyko, Sergey and Komodakis, Nikos},
  journal = {arXiv preprint arXiv:1706.00388},
  title   = {Diracnets: Training very deep neural networks without skip-connections},
  year    = {2017},
}

@InProceedings{ding2021repvgg,
  author    = {Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Repvgg: Making vgg-style convnets great again},
  year      = {2021},
  pages     = {13733--13742},
}

@Misc{shaham2022scrolls,
  author        = {Uri Shaham and Elad Segal and Maor Ivgi and Avia Efrat and Ori Yoran and Adi Haviv and Ankit Gupta and Wenhan Xiong and Mor Geva and Jonathan Berant and Omer Levy},
  title         = {SCROLLS: Standardized CompaRison Over Long Language Sequences},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2201.03533},
  primaryclass  = {cs.CL},
}

@Article{devlin2018bert,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  year    = {2018},
}

@Article{guo2020expandnets,
  author  = {Guo, Shuxuan and Alvarez, Jose M and Salzmann, Mathieu},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Expandnets: Linear over-parameterization to train compact convolutional networks},
  year    = {2020},
  pages   = {1298--1310},
  volume  = {33},
}

@Article{cao2022do,
  author    = {Cao, Jinming and Li, Yangyan and Sun, Mingchao and Chen, Ying and Lischinski, Dani and Cohen-Or, Daniel and Chen, Baoquan and Tu, Changhe},
  journal   = {IEEE Transactions on Image Processing},
  title     = {Do-conv: Depthwise over-parameterized convolutional layer},
  year      = {2022},
  publisher = {IEEE},
}

@InProceedings{ding2019acnet,
  author    = {Ding, Xiaohan and Guo, Yuchen and Ding, Guiguang and Han, Jungong},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
  title     = {Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks},
  year      = {2019},
  pages     = {1911--1920},
}

@InProceedings{ioffe2015batch,
  author       = {Ioffe, Sergey and Szegedy, Christian},
  booktitle    = {International conference on machine learning},
  title        = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  year         = {2015},
  organization = {PMLR},
  pages        = {448--456},
}

@InProceedings{xiong2021nystromformer,
  author    = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Nystr{\"o}mformer: A Nystr{\"o}m-based Algorithm for Approximating Self-Attention},
  year      = {2021},
}

@InProceedings{rahimi2008random,
  author    = {Rahimi, Ali and Recht, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Random Features for Large-Scale Kernel Machines},
  year      = {2008},
  editor    = {J. Platt and D. Koller and Y. Singer and S. Roweis},
  publisher = {Curran Associates, Inc.},
  volume    = {20},
  url       = {https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
}

@Misc{tay2020sparse,
  author        = {Yi Tay and Dara Bahri and Liu Yang and Donald Metzler and Da-Cheng Juan},
  title         = {Sparse Sinkhorn Attention},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2002.11296},
  primaryclass  = {cs.LG},
}

@Misc{roy2020efficient,
  author        = {Aurko Roy and Mohammad Saffar and Ashish Vaswani and David Grangier},
  title         = {Efficient Content-Based Sparse Attention with Routing Transformers},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2003.05997},
  primaryclass  = {cs.LG},
}

@Misc{kasai2021finetuning,
  author        = {Jungo Kasai and Hao Peng and Yizhe Zhang and Dani Yogatama and Gabriel Ilharco and Nikolaos Pappas and Yi Mao and Weizhu Chen and Noah A. Smith},
  title         = {Finetuning Pretrained Transformers into RNNs},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2103.13076},
  primaryclass  = {cs.CL},
}

@InProceedings{he2021deberta,
  author    = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  booktitle = {International Conference on Learning Representations},
  title     = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  year      = {2021},
  url       = {https://openreview.net/forum?id=XPZIaotutsD},
}

@InProceedings{ding2022scaling,
  author    = {Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Scaling up your kernels to 31x31: Revisiting large kernel design in cnns},
  year      = {2022},
  pages     = {11963--11975},
}

@InProceedings{kim2019disentangling,
  author    = {Kim, Junkyung and Linsley, Drew and Thakkar, Kalpit and Serre, Thomas},
  booktitle = {International Conference on Learning Representations},
  title     = {Disentangling neural mechanisms for perceptual grouping},
  year      = {2019},
}

@Article{gu2020hippo,
  author  = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Hippo: Recurrent memory with optimal polynomial projections},
  year    = {2020},
  pages   = {1474--1487},
  volume  = {33},
}

@InProceedings{choromanski2021rethinking,
  author    = {Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
  booktitle = {International Conference on Learning Representations},
  title     = {Rethinking Attention with Performers},
  year      = {2021},
  url       = {https://openreview.net/forum?id=Ua6zuk0WRH},
}

@InProceedings{liu2022convnet,
  author    = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {A convnet for the 2020s},
  year      = {2022},
  pages     = {11976--11986},
}

@Misc{yu2016orthogonal,
  author        = {Felix X. Yu and Ananda Theertha Suresh and Krzysztof Choromanski and Daniel Holtmann-Rice and Sanjiv Kumar},
  title         = {Orthogonal Random Features},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1610.09072},
  primaryclass  = {cs.LG},
}

@InProceedings{parmar2018image,
  author       = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle    = {International Conference on Machine Learning},
  title        = {Image transformer},
  year         = {2018},
  organization = {PMLR},
  pages        = {4055--4064},
}

@Article{nguyen2019transformers,
  author  = {Nguyen, Toan Q and Salazar, Julian},
  journal = {arXiv preprint arXiv:1910.05895},
  title   = {Transformers without tears: Improving the normalization of self-attention},
  year    = {2019},
}

@InProceedings{xiong2020layer,
  author       = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle    = {International Conference on Machine Learning},
  title        = {On layer normalization in the transformer architecture},
  year         = {2020},
  organization = {PMLR},
  pages        = {10524--10533},
}

@Article{baevski2018adaptive,
  author  = {Baevski, Alexei and Auli, Michael},
  journal = {arXiv preprint arXiv:1809.10853},
  title   = {Adaptive input representations for neural language modeling},
  year    = {2018},
}

@Article{ma2022mega,
  author  = {Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:2209.10655},
  title   = {Mega: Moving Average Equipped Gated Attention},
  year    = {2022},
}

@Article{lee2019wide,
  author  = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal = {arXiv preprint arXiv:1902.06720},
  title   = {Wide neural networks of any depth evolve as linear models under gradient descent},
  year    = {2019},
}

@Article{lee2017deep,
  author  = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal = {arXiv preprint arXiv:1711.00165},
  title   = {Deep neural networks as gaussian processes},
  year    = {2017},
}

@InProceedings{xiao2018dynamical,
  author    = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey},
  booktitle = {International Conference on Machine Learning},
  title     = {Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks},
  year      = {2018},
  pages     = {5389--5398},
}

@Article{gu2022how,
  author  = {Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and R{\'e}, Christopher},
  journal = {arXiv preprint arXiv:2206.12037},
  title   = {How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections},
  year    = {2022},
}

@Article{yang2019mean,
  author  = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  journal = {arXiv preprint arXiv:1902.08129},
  title   = {A mean field theory of batch normalization},
  year    = {2019},
}

@Article{yang2018deep,
  author = {Yang, Greg and Schoenholz, Sam S},
  title  = {Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion},
  year   = {2018},
}

@InProceedings{yang2017mean,
  author    = {Yang, Ge and Schoenholz, Samuel},
  booktitle = {Advances in neural information processing systems},
  title     = {Mean field residual networks: On the edge of chaos},
  year      = {2017},
  pages     = {7103--7114},
}

@InProceedings{tan2019efficientnet,
  author       = {Tan, Mingxing and Le, Quoc},
  booktitle    = {International conference on machine learning},
  title        = {Efficientnet: Rethinking model scaling for convolutional neural networks},
  year         = {2019},
  organization = {PMLR},
  pages        = {6105--6114},
}

@InProceedings{liu2021swin,
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title     = {Swin transformer: Hierarchical vision transformer using shifted windows},
  year      = {2021},
  pages     = {10012--10022},
}

@InProceedings{johnson2017clevr,
  author    = {Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  year      = {2017},
  pages     = {2901--2910},
}

@InProceedings{zhu2020vision,
  author    = {Zhu, Fengda and Zhu, Yi and Chang, Xiaojun and Liang, Xiaodan},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Vision-language navigation with self-supervised auxiliary reasoning tasks},
  year      = {2020},
  pages     = {10012--10022},
}

@InProceedings{yu2022metaformer,
  author    = {Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Metaformer is actually what you need for vision},
  year      = {2022},
  pages     = {10819--10829},
}

@InProceedings{touvron2021going,
  author    = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title     = {Going deeper with image transformers},
  year      = {2021},
  pages     = {32--42},
}

@InProceedings{touvron2021training,
  author       = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle    = {International Conference on Machine Learning},
  title        = {Training data-efficient image transformers \& distillation through attention},
  year         = {2021},
  organization = {PMLR},
  pages        = {10347--10357},
}

@Article{dosovitskiy2020image,
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal = {arXiv preprint arXiv:2010.11929},
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  year    = {2020},
}

@Article{yang2019scaling,
  author  = {Yang, Greg},
  journal = {arXiv preprint arXiv:1902.04760},
  title   = {Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  year    = {2019},
}

@Online{foundationwikimedia,
  author = {Wikimedia Foundation},
  title  = {Wikimedia Downloads},
  url    = {https://dumps.wikimedia.org},
}

@Article{dai2019transformer,
  author  = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Cohen, William W and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal = {arXiv preprint arXiv:1901.02860},
  title   = {Transformer-xl: Attentive language models beyond a fixed-length context},
  year    = {2019},
}

@Book{geiser2009decomposition,
  author    = {Geiser, Juergen},
  publisher = {CRC Press},
  title     = {Decomposition methods for differential equations: theory and applications},
  year      = {2009},
}


@Article{dehghani2018universal,
  author  = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal = {arXiv preprint arXiv:1807.03819},
  title   = {Universal transformers},
  year    = {2018},
}

@Article{wu2019depth,
  author        = {Lijun Wu and Yiren Wang and Yingce Xia and Fei Tian and Fei Gao and Tao Qin and Jianhuang Lai and Tie{-}Yan Liu},
  journal       = {CoRR},
  title         = {Depth Growing for Neural Machine Translation},
  year          = {2019},
  volume        = {abs/1907.01968},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1907-01968},
  eprint        = {1907.01968},
  timestamp     = {Mon, 08 Jul 2019 14:12:33 +0200},
  url           = {http://arxiv.org/abs/1907.01968},
}

@Article{peters2018deep,
  author  = {Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:1802.05365},
  title   = {Deep contextualized word representations},
  year    = {2018},
}


@Article{radford2019languagea,
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title  = {Language Models are Unsupervised Multitask Learners},
  year   = {2019},
}

@InProceedings{devlin2019bert,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year      = {2019},
  pages     = {4171--4186},
}


@Article{edunov2018understanding,
  author  = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David},
  journal = {arXiv preprint arXiv:1808.09381},
  title   = {Understanding back-translation at scale},
  year    = {2018},
}


@Article{alrfou2018character,
  author  = {Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
  journal = {arXiv preprint arXiv:1808.04444},
  title   = {Character-level language modeling with deeper self-attention},
  year    = {2018},
}

@Article{thorpe2018deep,
  author  = {Thorpe, Matthew and van Gennip, Yves},
  journal = {arXiv preprint arXiv:1810.11741},
  title   = {Deep Limits of Residual Neural Networks},
  year    = {2018},
}

@Article{lecun2015deep,
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal   = {nature},
  title     = {Deep learning},
  year      = {2015},
  number    = {7553},
  pages     = {436},
  volume    = {521},
  publisher = {Nature Publishing Group},
}

@Article{ma2018fluid,
  author    = {Ma, Pingchuan and Tian, Yunsheng and Pan, Zherong and Ren, Bo and Manocha, Dinesh},
  journal   = {ACM Transactions on Graphics (TOG)},
  title     = {Fluid directed rigid body control using deep reinforcement learning},
  year      = {2018},
  number    = {4},
  pages     = {96},
  volume    = {37},
  publisher = {ACM},
}

@Article{weinan2019mean,
  author    = {Weinan, E and Han, Jiequn and Li, Qianxiao},
  journal   = {Research in the Mathematical Sciences},
  title     = {A mean-field optimal control formulation of deep learning},
  year      = {2019},
  number    = {1},
  pages     = {10},
  volume    = {6},
  publisher = {Springer},
}

@Article{evans2005introduction,
  author  = {Evans, Lawrence C},
  journal = {Lecture Notes, University of California, Department of Mathematics, Berkeley},
  title   = {An introduction to mathematical optimal control theory},
  year    = {2005},
}




@Article{wald1939contributions,
  author    = {Wald, Abraham},
  journal   = {The Annals of Mathematical Statistics},
  title     = {Contributions to the theory of statistical estimation and testing hypotheses},
  year      = {1939},
  number    = {4},
  pages     = {299--326},
  volume    = {10},
  publisher = {JSTOR},
}

@Book{moulton2012introduction,
  author    = {Moulton, Forest Ray},
  publisher = {Courier Corporation},
  title     = {An introduction to celestial mechanics},
  year      = {2012},
}


@Article{halkin1966maximum,
  author    = {Halkin, Hubert},
  journal   = {SIAM Journal on control},
  title     = {A maximum principle of the Pontryagin type for systems described by nonlinear difference equations},
  year      = {1966},
  number    = {1},
  pages     = {90--111},
  volume    = {4},
  publisher = {SIAM},
}

@Book{Ogata1995,
  author    = {Ogata, Katsuhiko and others},
  publisher = {Prentice Hall Englewood Cliffs, NJ},
  title     = {Discrete-time control systems},
  year      = {1995},
  volume    = {2},
}

@TechReport{Boltyanskii1960,
  author = {Boltyanskii, Vladimir Grigor'evich and Gamkrelidze, Revaz Valer'yanovich and Pontryagin, Lev Semenovich},
  title  = {The theory of optimal processes. I. The maximum principle},
  year   = {1960},
  school = {TRW SPACE TECHNOLOGY LABS LOS ANGELES CALIF},
}

@Book{Pontryagin1987,
  author    = {Pontryagin, Lev Semenovich},
  publisher = {CRC},
  title     = {Mathematical theory of optimal processes},
  year      = {1987},
}

@InProceedings{LeCun1988,
  author       = {LeCun, Yann and Touresky, D and Hinton, G and Sejnowski, T},
  booktitle    = {Proceedings of the 1988 connectionist models summer school},
  title        = {A theoretical framework for back-propagation},
  year         = {1988},
  organization = {CMU, Pittsburgh, Pa: Morgan Kaufmann},
  pages        = {21--28},
  volume       = {1},
}

@Article{Kurakin2016,
  author  = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal = {arXiv preprint arXiv:1611.01236},
  title   = {Adversarial machine learning at scale},
  year    = {2016},
}

@InProceedings{Cao2019,
  author    = {Shengcao Cao and Xiaofang Wang and Kris M. Kitani},
  booktitle = {International Conference on Learning Representations},
  title     = {Learnable Embedding Space for Efficient Neural Architecture Compression},
  year      = {2019},
  url       = {https://openreview.net/forum?id=S1xLN3C9YX},
}

@InProceedings{Ye2018,
  author    = {Ye, Nanyang and Zhu, Zhanxing},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Bayesian Adversarial Learning},
  year      = {2018},
  pages     = {6892--6901},
}

@InProceedings{Zhang2019,
  author    = {Wenxin Zhang},
  booktitle = {Iris Long},
  title     = {The Bonfire of Time},
  year      = {2019},
  editor    = {Tragoidia},
}

@InProceedings{Zhang2019a,
  author    = {Xiaoshuai Zhang and Yiping Lu and Jiaying Liu and Bin Dong},
  booktitle = {International Conference on Learning Representations},
  title     = {Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration},
  year      = {2019},
  url       = {https://openreview.net/forum?id=SJfZKiC5FX},
}

@InProceedings{Li2018,
  author    = {Li, Qianxiao and Hao, Shuji},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks},
  year      = {2018},
  address   = {Stockholmsmässan, Stockholm Sweden},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {2985--2994},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {Deep learning is formulated as a discrete-time optimal control problem. This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin’s maximum principle, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set. We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices.},
  pdf       = {http://proceedings.mlr.press/v80/li18b/li18b.pdf},
  url       = {http://proceedings.mlr.press/v80/li18b.html},
}

@Article{Zhang2019b,
  author  = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric P and Ghaoui, Laurent El and Jordan, Michael I},
  journal = {arXiv preprint arXiv:1901.08573},
  title   = {Theoretically Principled Trade-off between Robustness and Accuracy},
  year    = {2019},
}

@InProceedings{MoosaviDezfooli2016,
  author    = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Deepfool: a simple and accurate method to fool deep neural networks},
  year      = {2016},
  pages     = {2574--2582},
}

@Article{Szegedy2013,
  author  = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal = {arXiv preprint arXiv:1312.6199},
  title   = {Intriguing properties of neural networks},
  year    = {2013},
}

@Article{Ma2019,
  author  = {Ma, Yanjun and Yu, Dianhai and Wu, Tian and Wang, Haifeng},
  journal = {Frontiers of Data and Domputing},
  title   = {PaddlePaddle: An open-source deep learning platform from industrial practice},
  year    = {2019},
  number  = {1},
  pages   = {105--115},
  volume  = {1},
}

@Book{Chen2021,
  author    = {Chen, Lei},
  publisher = {Springer Nature},
  title     = {Deep Learning and Practice with MindSpore},
  year      = {2021},
}

@Article{Chen2015,
  author  = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal = {arXiv preprint arXiv:1512.01274},
  title   = {Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  year    = {2015},
}

@Article{Abadi2016,
  author  = {Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
  journal = {arXiv preprint arXiv:1603.04467},
  title   = {Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
  year    = {2016},
}

@Article{Gu2022a,
  author  = {Gu, Albert and Gupta, Ankit and Goel, Karan and R{\'e}, Christopher},
  journal = {arXiv preprint arXiv:2206.11893},
  title   = {On the Parameterization and Initialization of Diagonal State Space Models},
  year    = {2022},
}

@InProceedings{He2019,
  author    = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Bag of tricks for image classification with convolutional neural networks},
  year      = {2019},
  pages     = {558--567},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Going deeper with convolutions},
  year      = {2015},
  pages     = {1--9},
}

@Article{Donahue2018,
  author  = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
  journal = {arXiv preprint arXiv:1802.04208},
  title   = {Adversarial audio synthesis},
  year    = {2018},
}

@InProceedings{Morrill2021,
  author       = {Morrill, James and Salvi, Cristopher and Kidger, Patrick and Foster, James},
  booktitle    = {International Conference on Machine Learning},
  title        = {Neural rough differential equations for long time series},
  year         = {2021},
  organization = {PMLR},
  pages        = {7829--7838},
}

@InProceedings{Madry2018,
  author    = {Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
  booktitle = {International Conference on Learning Representations},
  title     = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  year      = {2018},
  url       = {https://openreview.net/forum?id=rJzIBfZAb},
}

@Article{Weinan2017,
  author    = {Weinan, E},
  journal   = {Communications in Mathematics and Statistics},
  title     = {A proposal on machine learning via dynamical systems},
  year      = {2017},
  number    = {1},
  pages     = {1--11},
  volume    = {5},
  publisher = {Springer},
}

@Article{Lu2017,
  author  = {Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong, Bin},
  journal = {arXiv preprint arXiv:1710.10121},
  title   = {Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations},
  year    = {2017},
}


@Article{Haber2017,
  author    = {Haber, Eldad and Ruthotto, Lars},
  journal   = {Inverse Problems},
  title     = {Stable architectures for deep neural networks},
  year      = {2017},
  number    = {1},
  pages     = {014004},
  volume    = {34},
  publisher = {IOP Publishing},
}

@Article{Chicone2007,
  author  = {Chicone, Carmen},
  journal = {Siam Review},
  title   = {Ordinary Differential Equations by Vladimir I. Arnold},
  year    = {2007},
  number  = {2},
  pages   = {335-336},
  volume  = {49},
}


@InProceedings{Chen2018,
  author    = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Neural ordinary differential equations},
  year      = {2018},
  pages     = {6572--6583},
}

@InProceedings{He2016,
  author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle    = {European conference on computer vision},
  title        = {Identity mappings in deep residual networks},
  year         = {2016},
  organization = {Springer},
  pages        = {630--645},
}

@Book{Ascher1998,
  author    = {Ascher, Uri M and Petzold, Linda R},
  publisher = {Siam},
  title     = {Computer methods for ordinary differential equations and differential-algebraic equations},
  year      = {1998},
  volume    = {61},
}

@Article{Li2017,
  author    = {Li, Qianxiao and Chen, Long and Tai, Cheng and Weinan, E},
  journal   = {The Journal of Machine Learning Research},
  title     = {Maximum principle based algorithms for deep learning},
  year      = {2017},
  number    = {1},
  pages     = {5998--6026},
  volume    = {18},
  publisher = {JMLR. org},
}


@Article{Zhu2018,
  author  = {Zhu, Mai and Chang, Bo and Fu, Chong},
  journal = {arXiv preprint arXiv:1802.08831},
  title   = {Convolutional Neural Networks combined with Runge-Kutta Methods},
  year    = {2018},
}

@Book{Bejan2013,
  author    = {Bejan, Adrian},
  publisher = {John wiley \& sons},
  title     = {Convection heat transfer},
  year      = {2013},
}

@InProceedings{He2016a,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Deep residual learning for image recognition},
  year      = {2016},
  pages     = {770--778},
}

@Article{McLachlan2002,
  author    = {McLachlan, Robert I and Quispel, G Reinout W},
  journal   = {Acta Numerica},
  title     = {Splitting methods},
  year      = {2002},
  pages     = {341--434},
  volume    = {11},
  publisher = {Cambridge University Press},
}

@Article{Zhang2019c,
  author  = {Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
  journal = {arXiv preprint arXiv:1901.09321},
  title   = {Fixup initialization: Residual learning without normalization},
  year    = {2019},
}

@InProceedings{Pennington2017,
  author    = {Pennington, Jeffrey and Worah, Pratik},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Nonlinear random matrix theory for deep learning},
  year      = {2017},
  pages     = {2637--2646},
}

@Article{Strang1968,
  author    = {Strang, Gilbert},
  journal   = {SIAM Journal on Numerical Analysis},
  title     = {On the construction and comparison of difference schemes},
  year      = {1968},
  number    = {3},
  pages     = {506--517},
  volume    = {5},
  publisher = {SIAM},
}

@Article{Sonoda2019,
  author    = {Sonoda, Sho and Murata, Noboru},
  journal   = {The Journal of Machine Learning Research},
  title     = {Transport analysis of infinitely deep neural network},
  year      = {2019},
  number    = {1},
  pages     = {31--82},
  volume    = {20},
  publisher = {JMLR. org},
}

@InProceedings{Glorot2010,
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  year      = {2010},
  pages     = {249--256},
}

@Article{Radford2015,
  author  = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal = {arXiv preprint arXiv:1511.06434},
  title   = {Unsupervised representation learning with deep convolutional generative adversarial networks},
  year    = {2015},
}

@InProceedings{Xu2015,
  author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle = {International conference on machine learning},
  title     = {Show, attend and tell: Neural image caption generation with visual attention},
  year      = {2015},
  pages     = {2048--2057},
}

@InProceedings{Isola2017,
  author    = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Image-to-image translation with conditional adversarial networks},
  year      = {2017},
  pages     = {1125--1134},
}

@Article{Arjovsky2017,
  author  = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal = {arXiv preprint arXiv:1701.07875},
  title   = {Wasserstein gan},
  year    = {2017},
}

@Article{Lu2019,
  author  = {Lu, Yiping and Li, Zhuohan and He, Di and Sun, Zhiqing and Dong, Bin and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal = {arXiv preprint arXiv:1906.02762},
  title   = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},
  year    = {2019},
}

@InProceedings{Kingma2015,
  author    = {Kingma, Diederik P and Ba, Jimmy},
  booktitle = {ICLR (Poster)},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2015},
}

@Article{Zeiler2012,
  author  = {Zeiler, Matthew D},
  journal = {arXiv preprint arXiv:1212.5701},
  title   = {ADADELTA: an adaptive learning rate method},
  year    = {2012},
}

@Article{Tieleman2012,
  author  = {Tieleman, Tijmen and Hinton, Geoffrey},
  journal = {University of Toronto, Technical Report},
  title   = {Lecture 6.5-rmsprop, coursera: Neural networks for machine learning},
  year    = {2012},
}

@Article{Duchi2011,
  author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {Journal of Machine Learning Research},
  title   = {Adaptive subgradient methods for online learning and stochastic optimization},
  year    = {2011},
  number  = {Jul},
  pages   = {2121--2159},
  volume  = {12},
}

@Article{Bobylev2001,
  author    = {Bobylev, Alexander V and Ohwada, Taku},
  journal   = {Applied Mathematics Letters},
  title     = {The error of the splitting scheme for solving evolutionary equations},
  year      = {2001},
  number    = {1},
  pages     = {45--48},
  volume    = {14},
  publisher = {Elsevier},
}

@Book{Glowinski2017,
  author    = {Glowinski, Roland and Osher, Stanley J and Yin, Wotao},
  publisher = {Springer},
  title     = {Splitting methods in communication, imaging, science, and engineering},
  year      = {2017},
}



@Article{Liao2016,
  author  = {Liao, Qianli and Poggio, Tomaso},
  journal = {arXiv preprint arXiv:1604.03640},
  title   = {Bridging the gaps between residual learning, recurrent neural networks and visual cortex},
  year    = {2016},
}

@Article{Chang2019,
  author  = {Chang, Bo and Chen, Minmin and Haber, Eldad and Chi, Ed H},
  journal = {arXiv preprint arXiv:1902.09689},
  title   = {AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks},
  year    = {2019},
}

@InProceedings{Tao2018,
  author    = {Tao, Yunzhe and Sun, Qi and Du, Qiang and Liu, Wei},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling},
  year      = {2018},
  pages     = {496--506},
}

@InProceedings{McCann2017,
  author    = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learned in translation: Contextualized word vectors},
  year      = {2017},
  pages     = {6297--6308},
}

@InProceedings{Pennington2014,
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  title     = {Glove: Global vectors for word representation},
  year      = {2014},
  pages     = {1532--1543},
}

@InProceedings{Mikolov2010,
  author    = {Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle = {Eleventh Annual Conference of the International Speech Communication Association},
  title     = {Recurrent neural network based language model},
  year      = {2010},
}

@Article{Lebret2013,
  author  = {Lebret, R{\'e}mi and Collobert, Ronan},
  journal = {arXiv preprint arXiv:1312.5542},
  title   = {Word emdeddings through hellinger PCA},
  year    = {2013},
}

@Article{Vaswani2018,
  author  = {Ashish Vaswani and Samy Bengio and Eugene Brevdo and Francois Chollet and Aidan N. Gomez and Stephan Gouws and Llion Jones and \L{}ukasz Kaiser and Nal Kalchbrenner and Niki Parmar and Ryan Sepassi and Noam Shazeer and Jakob Uszkoreit},
  journal = {CoRR},
  title   = {Tensor2Tensor for Neural Machine Translation},
  year    = {2018},
  volume  = {abs/1803.07416},
  url     = {http://arxiv.org/abs/1803.07416},
}

@Article{Mu2017,
  author  = {Mu, Jiaqi and Bhat, Suma and Viswanath, Pramod},
  journal = {arXiv preprint arXiv:1702.01417},
  title   = {All-but-the-top: simple and effective postprocessing for word representations},
  year    = {2017},
}

@InProceedings{Mikolov2013,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in neural information processing systems},
  title     = {Distributed representations of words and phrases and their compositionality},
  year      = {2013},
  pages     = {3111--3119},
}

@InProceedings{Shang2015,
  author    = {Shang, Lifeng and Lu, Zhengdong and Li, Hang},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  title     = {Neural Responding Machine for Short-Text Conversation},
  year      = {2015},
  pages     = {1577--1586},
  volume    = {1},
}

@InProceedings{Dauphin2017,
  author    = {Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle = {International Conference on Machine Learning},
  title     = {Language Modeling with Gated Convolutional Networks},
  year      = {2017},
  pages     = {933--941},
}

@InProceedings{Liu2016,
  author    = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Yang, Meng},
  booktitle = {ICML},
  title     = {Large-Margin Softmax Loss for Convolutional Neural Networks.},
  year      = {2016},
  pages     = {507--516},
}

@InProceedings{Koehn2006,
  author    = {Koehn, Philipp and Federico, Marcello and Shen, Wade and Bertoldi, Nicola and Bojar, Ondrej and Callison-Burch, Chris and Cowan, Brooke and Dyer, Chris and Hoang, Hieu and Zens, Richard and others},
  booktitle = {Final Report of the 2006 JHU Summer Workshop},
  title     = {Open source toolkit for statistical machine translation: Factored translation models and confusion network decoding},
  year      = {2006},
}

@Article{Grave2016,
  author  = {Grave, Edouard and Joulin, Armand and Usunier, Nicolas},
  journal = {arXiv preprint arXiv:1612.04426},
  title   = {Improving neural language models with a continuous cache},
  year    = {2016},
}

@InProceedings{Wen2016,
  author       = {Wen, Yandong and Zhang, Kaipeng and Li, Zhifeng and Qiao, Yu},
  booktitle    = {European Conference on Computer Vision},
  title        = {A discriminative feature learning approach for deep face recognition},
  year         = {2016},
  organization = {Springer},
  pages        = {499--515},
}

@Misc{Yue2018,
  author = {Yue,Zhao and Deli, Zhao and Shaohua, Wan and Bo,Zhang},
  title  = {Softmax Supervision with Isotropic Normalization},
  year   = {2018},
  url    = {https://openreview.net/forum?id=SyXNErg0W},
}

@Article{Yang2017a,
  author  = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W},
  journal = {arXiv preprint arXiv:1711.03953},
  title   = {Breaking the softmax bottleneck: a high-rank RNN language model},
  year    = {2017},
}

@InCollection{Bengio2007,
  author    = {Bengio, Yoshua and LeCun, Yann},
  booktitle = {Large Scale Kernel Machines},
  publisher = {MIT Press},
  title     = {Scaling Learning Algorithms Towards {AI}},
  year      = {2007},
}

@Article{Hinton2006,
  author  = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
  journal = {Neural Computation},
  title   = {A Fast Learning Algorithm for Deep Belief Nets},
  year    = {2006},
  pages   = {1527--1554},
  volume  = {18},
}

@Article{Vyas2020,
  author  = {Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Fast transformers with clustered attention},
  year    = {2020},
  volume  = {33},
}

@Article{Merikoski1984,
  author    = {Merikoski, Jorma Kaarlo},
  journal   = {Linear algebra and its applications},
  title     = {On the trace and the sum of elements of a matrix},
  year      = {1984},
  pages     = {177--185},
  volume    = {60},
  publisher = {Elsevier},
}

@InProceedings{Li2017a,
  author    = {Li, Jiwei and Monroe, Will and Shi, Tianlin and Jean, S{\.e}bastien and Ritter, Alan and Jurafsky, Dan},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  title     = {Adversarial Learning for Neural Dialogue Generation},
  year      = {2017},
  pages     = {2157--2169},
}

@Article{Gers1999,
  author    = {Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  title     = {Learning to forget: Continual prediction with LSTM},
  year      = {1999},
  publisher = {IET},
}

@Article{Hochreiter1997,
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal   = {Neural computation},
  title     = {Long short-term memory},
  year      = {1997},
  number    = {8},
  pages     = {1735--1780},
  volume    = {9},
  publisher = {MIT Press},
}

@Article{Jozefowicz2016,
  author  = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  journal = {arXiv preprint arXiv:1602.02410},
  title   = {Exploring the limits of language modeling},
  year    = {2016},
}

@InProceedings{Zhang2016,
  author       = {Zhang, Yu and Chen, Guoguo and Yu, Dong and Yaco, Kaisheng and Khudanpur, Sanjeev and Glass, James},
  booktitle    = {Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on},
  title        = {Highway long short-term memory rnns for distant speech recognition},
  year         = {2016},
  organization = {IEEE},
  pages        = {5755--5759},
}

@Article{Hochreiter1998,
  author    = {Hochreiter, Sepp},
  journal   = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  title     = {The vanishing gradient problem during learning recurrent neural nets and problem solutions},
  year      = {1998},
  number    = {02},
  pages     = {107--116},
  volume    = {6},
  publisher = {World Scientific},
}

@InProceedings{Xingjian2015,
  author    = {Xingjian, Shi and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  booktitle = {Advances in neural information processing systems},
  title     = {Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
  year      = {2015},
  pages     = {802--810},
}

@Article{Mishra2017,
  author  = {Mishra, Nikhil and Abbeel, Pieter and Mordatch, Igor},
  journal = {arXiv preprint arXiv:1703.04070},
  title   = {Prediction and Control with Temporal Segment Models},
  year    = {2017},
}

@Article{Villegas2017,
  author  = {Villegas, Ruben and Yang, Jimei and Zou, Yuliang and Sohn, Sungryull and Lin, Xunyu and Lee, Honglak},
  journal = {arXiv preprint arXiv:1704.05831},
  title   = {Learning to Generate Long-term Future via Hierarchical Prediction},
  year    = {2017},
}

@Article{Wu2016,
  author  = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal = {arXiv preprint arXiv:1609.08144},
  title   = {Google's neural machine translation system: Bridging the gap between human and machine translation},
  year    = {2016},
}

@Article{Britz2017,
  author  = {Britz, Denny and Goldie, Anna and Luong, Thang and Le, Quoc},
  journal = {arXiv preprint arXiv:1703.03906},
  title   = {Massive exploration of neural machine translation architectures},
  year    = {2017},
}

@InProceedings{Vinyals2015,
  author    = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Show and tell: A neural image caption generator},
  year      = {2015},
  pages     = {3156--3164},
}


@Article{Krueger2016,
  author = {Krueger, David and Maharaj, Tegan and Kramar, Janos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Christopher},
  title  = {Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations},
  year   = {2016},
}

@Article{Zaremba2014,
  author  = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  journal = {arXiv preprint arXiv:1409.2329},
  title   = {Recurrent neural network regularization},
  year    = {2014},
}

@Article{Semeniuta2016,
  author  = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
  journal = {arXiv preprint arXiv:1603.05118},
  title   = {Recurrent dropout without memory loss},
  year    = {2016},
}

@Article{Chaudhari2016,
  author  = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann},
  journal = {arXiv preprint arXiv:1611.01838},
  title   = {Entropy-sgd: Biasing gradient descent into wide valleys},
  year    = {2016},
}

@Article{Keskar2016,
  author  = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal = {arXiv preprint arXiv:1609.04836},
  title   = {On large-batch training for deep learning: Generalization gap and sharp minima},
  year    = {2016},
}

@Article{Haussler1997,
  author    = {Haussler, David and Opper, Manfred and others},
  journal   = {The Annals of Statistics},
  title     = {Mutual information, metric entropy and cumulative relative entropy risk},
  year      = {1997},
  number    = {6},
  pages     = {2451--2492},
  volume    = {25},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Hochreiter1997a,
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal   = {Neural Computation},
  title     = {Flat minima},
  year      = {1997},
  number    = {1},
  pages     = {1--42},
  volume    = {9},
  publisher = {MIT Press},
}

@Article{Jang2016,
  author  = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal = {arXiv preprint arXiv:1611.01144},
  title   = {Categorical reparameterization with gumbel-softmax},
  year    = {2016},
}

@Article{Maddison2016,
  author  = {Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye},
  journal = {arXiv preprint arXiv:1611.00712},
  title   = {The concrete distribution: A continuous relaxation of discrete random variables},
  year    = {2016},
}

@Article{Kusner2016,
  author  = {Kusner, Matt J and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal = {arXiv preprint arXiv:1611.04051},
  title   = {GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution},
  year    = {2016},
}

@Article{Subramanian2017,
  author  = {Subramanian, Sandeep and Rajeswar, Sai and Dutil, Francis and Pal, Christopher and Courville, Aaron},
  journal = {ACL 2017},
  title   = {Adversarial Generation of Natural Language},
  year    = {2017},
  pages   = {241},
}

@Article{Rissanen1983,
  author    = {Rissanen, Jorma},
  journal   = {The Annals of statistics},
  title     = {A universal prior for integers and estimation by minimum description length},
  year      = {1983},
  pages     = {416--431},
  publisher = {JSTOR},
}

@Article{Bahdanau2016,
  author  = {Bahdanau, Dzmitry and Brakel, Philemon and Xu, Kelvin and Goyal, Anirudh and Lowe, Ryan and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1607.07086},
  title   = {An actor-critic algorithm for sequence prediction},
  year    = {2016},
}

@Article{Ranzato2015,
  author  = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
  journal = {arXiv preprint arXiv:1511.06732},
  title   = {Sequence level training with recurrent neural networks},
  year    = {2015},
}

@InProceedings{Wiseman2016,
  author    = {Wiseman, Sam and Rush, Alexander M.},
  booktitle = {EMNLP},
  title     = {Sequence-to-Sequence Learning as Beam-Search Optimization},
  year      = {2016},
  month     = {November},
}

@InProceedings{Cettolo2014,
  author    = {Cettolo, Mauro and Niehues, Jan and St{\"u}ker, Sebastian and Bentivogli, Luisa and Federico, Marcello},
  booktitle = {Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam},
  title     = {Report on the 11th IWSLT evaluation campaign, IWSLT 2014},
  year      = {2014},
}

@InProceedings{Jean2015,
  author    = {Jean, S\'{e}bastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  title     = {On Using Very Large Target Vocabulary for Neural Machine Translation},
  year      = {2015},
  address   = {Beijing, China},
  month     = {July},
  pages     = {1--10},
  publisher = {Association for Computational Linguistics},
  url       = {http://www.aclweb.org/anthology/P15-1001},
}

@Article{Luong2015,
  author  = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1508.04025},
  title   = {Effective approaches to attention-based neural machine translation},
  year    = {2015},
}

@Article{Huang,
  author = {Huang, Po-Sen and Wang, Chong and Zhou, Dengyong and Deng, Li},
  title  = {Toward Neural Phrase-based Machine Translation},
}

@InProceedings{Papineni2002,
  author       = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle    = {Proceedings of the 40th annual meeting on association for computational linguistics},
  title        = {BLEU: a method for automatic evaluation of machine translation},
  year         = {2002},
  organization = {Association for Computational Linguistics},
  pages        = {311--318},
}

@Article{Shen2015,
  author  = {Shen, Shiqi and Cheng, Yong and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
  journal = {arXiv preprint arXiv:1512.02433},
  title   = {Minimum risk training for neural machine translation},
  year    = {2015},
}

@Article{Wiseman2016a,
  author  = {Wiseman, Sam and Rush, Alexander M},
  journal = {arXiv preprint arXiv:1606.02960},
  title   = {Sequence-to-sequence learning as beam-search optimization},
  year    = {2016},
}


@Article{Mandt2017,
  author  = {Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
  journal = {arXiv preprint arXiv:1704.04289},
  title   = {Stochastic gradient descent as approximate bayesian inference},
  year    = {2017},
}

@Article{Tay2020c,
  author  = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal = {arXiv preprint arXiv:2011.04006},
  title   = {Long range arena: A benchmark for efficient transformers},
  year    = {2020},
}

@Article{Gu2021,
  author  = {Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal = {Advances in neural information processing systems},
  title   = {Combining recurrent, convolutional, and continuous-time models with linear state space layers},
  year    = {2021},
  pages   = {572--585},
  volume  = {34},
}

@Article{Kidger2020,
  author  = {Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Neural controlled differential equations for irregular time series},
  year    = {2020},
  pages   = {6696--6707},
  volume  = {33},
}

@Article{Warden2018,
  author  = {Warden, Pete},
  journal = {arXiv preprint arXiv:1804.03209},
  title   = {Speech commands: A dataset for limited-vocabulary speech recognition},
  year    = {2018},
}

@Article{Gu2021a,
  author  = {Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal = {arXiv preprint arXiv:2111.00396},
  title   = {Efficiently modeling long sequences with structured state spaces},
  year    = {2021},
}

@Article{Kalchbrenner2016,
  author  = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
  journal = {arXiv preprint arXiv:1610.10099},
  title   = {Neural machine translation in linear time},
  year    = {2016},
}

@Article{Shazeer2017,
  author  = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal = {arXiv preprint arXiv:1701.06538},
  title   = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  year    = {2017},
}

@Article{Xia2017,
  author  = {Xia, Yingce and Qin, Tao and Chen, Wei and Bian, Jiang and Yu, Nenghai and Liu, Tie-Yan},
  journal = {arXiv preprint arXiv:1707.00415},
  title   = {Dual supervised learning},
  year    = {2017},
}

@InProceedings{Xia2017a,
  author    = {Xia, Yingce and Bian, Jiang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI)},
  title     = {Dual inference for machine learning},
  year      = {2017},
  pages     = {3112--3118},
}

@Article{Zilly2016,
  author  = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn{\'\i}k, Jan and Schmidhuber, J{\"u}rgen},
  journal = {arXiv preprint arXiv:1607.03474},
  title   = {Recurrent highway networks},
  year    = {2016},
}

@Article{Zoph2016,
  author  = {Zoph, Barret and Le, Quoc V},
  journal = {arXiv preprint arXiv:1611.01578},
  title   = {Neural architecture search with reinforcement learning},
  year    = {2016},
}

@Article{Melis2017,
  author  = {Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  journal = {arXiv preprint arXiv:1707.05589},
  title   = {On the state of the art of evaluation in neural language models},
  year    = {2017},
}

@InProceedings{Wan2013,
  author    = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle = {International Conference on Machine Learning},
  title     = {Regularization of neural networks using dropconnect},
  year      = {2013},
  pages     = {1058--1066},
}

@Article{Polyak1992,
  author    = {Polyak, Boris T and Juditsky, Anatoli B},
  journal   = {SIAM Journal on Control and Optimization},
  title     = {Acceleration of stochastic approximation by averaging},
  year      = {1992},
  number    = {4},
  pages     = {838--855},
  volume    = {30},
  publisher = {SIAM},
}

@InProceedings{Szegedy2016,
  author    = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Rethinking the inception architecture for computer vision},
  year      = {2016},
  pages     = {2818--2826},
}

@Article{LeiBa2016,
  author  = {Lei Ba, Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = {arXiv preprint arXiv:1607.06450},
  title   = {Layer normalization},
  year    = {2016},
}

@Article{Ott2018,
  author  = {Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  journal = {arXiv preprint arXiv:1806.00187},
  title   = {Scaling neural machine translation},
  year    = {2018},
}

 
@Book{Agirre2007,
  editor    = {Agirre, Eneko and M`arquez, Llu'is and Wicentowski, Richard.},
  publisher = {Association for Computational Linguistics},
  title     = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).},
  year      = {2007},
  address   = {Prague, Czech Republic},
  month     = {June},
}

 
@InProceedings{Williams2018,
  author    = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R.},
  booktitle = {Proceedings of NAACL-HLT.},
  title     = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  year      = {2018},
}

@Article{Clark2019,
  author  = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal = {ACL 2019},
  title   = {What does BERT look at? An Analysis of BERT’s Attention},
  year    = {2019},
  pages   = {276},
}

 
@InCollection{Dagan2006,
  author    = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo.},
  booktitle = {Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment.},
  publisher = {Springer},
  title     = {The {PASCAL} recognising textual entailment challenge},
  year      = {2006},
  pages     = {177--190},
}

 
@Article{BarHaim2006,
  author = {Bar Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan.},
  title  = {The second {PASCAL} recognising textual entailment challenge},
  year   = {2006},
}

 
@InProceedings{Giampiccolo2007,
  author       = {Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill.},
  booktitle    = {Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing.},
  title        = {The third {PASCAL} recognizing textual entailment challenge},
  year         = {2007},
  organization = {Association for Computational Linguistics},
  pages        = {1--9},
}

 
@Article{Bentivogli2009,
  author    = {Bentivogli, Luisa and Dagan, Ido and Dang, Hoa Trang and Giampiccolo, Danilo and Magnini, Bernardo.},
  title     = {The Fifth {PASCAL} Recognizing Textual Entailment Challenge},
  year      = {2009},
  booktitle = {TAC},
}

 
@InProceedings{Levesque2011,
  author    = {Levesque, Hector J and Davis, Ernest and Morgenstern, Leora.},
  booktitle = {{AAAI} Spring Symposium: Logical Formalizations of Commonsense Reasoning.},
  title     = {The {W}inograd schema challenge.},
  year      = {2011},
  pages     = {47},
  volume    = {46},
}

@Article{Chen2018a,
  author = {Chen, Z. and Zhang, H. and Zhang, X. and Zhao, L.},
  title  = {Quora question pairs},
  year   = {2018},
  url    = {https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs},
}

 
@InProceedings{Rajpurkar2016,
  author    = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy.},
  booktitle = {Proceedings of EMNLP.},
  title     = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
  year      = {2016},
  address   = {Austin, Texas},
  pages     = {2383--2392},
  publisher = {Association for Computational Linguistics},
}

 
 
@Article{Edunov2017,
  author  = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David and Ranzato, Marc'Aurelio},
  journal = {arXiv preprint arXiv:1711.04956},
  title   = {Classical structured prediction losses for sequence to sequence learning},
  year    = {2017},
}

@Article{Ahmed2017,
  author  = {Ahmed, Karim and Keskar, Nitish Shirish and Socher, Richard},
  journal = {arXiv preprint arXiv:1711.02132},
  title   = {Weighted transformer network for machine translation},
  year    = {2017},
}

@Article{Shaw2018,
  author  = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal = {arXiv preprint arXiv:1803.02155},
  title   = {Self-attention with relative position representations},
  year    = {2018},
}

@InProceedings{Ott2019,
  author    = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  title     = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  year      = {2019},
}

@Misc{Micikevicius2018,
  author        = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  title         = {Mixed Precision Training},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1710.03740},
  primaryclass  = {cs.AI},
}

@Article{Paszke2019,
  author  = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal = {Advances in Neural Information Processing Systems},
  title   = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year    = {2019},
  pages   = {8026--8037},
  volume  = {32},
}

@Article{Wu2019a,
  author  = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N and Auli, Michael},
  journal = {arXiv preprint arXiv:1901.10430},
  title   = {Pay Less Attention with Lightweight and Dynamic Convolutions},
  year    = {2019},
}

@Article{Radford,
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  title  = {Improving language understanding by generative pre-training},
}

@Article{Matthews1975,
  author    = {Matthews, Brian W},
  journal   = {Biochimica et Biophysica Acta (BBA)-Protein Structure},
  title     = {Comparison of the predicted and observed secondary structure of T4 phage lysozyme},
  year      = {1975},
  number    = {2},
  pages     = {442--451},
  volume    = {405},
  publisher = {Elsevier},
}

@Article{Liu2019,
  author  = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal = {arXiv preprint arXiv:1908.03265},
  title   = {On the Variance of the Adaptive Learning Rate and Beyond},
  year    = {2019},
}


@Article{Ghadimi2013,
  author    = {Ghadimi, Saeed and Lan, Guanghui},
  journal   = {SIAM Journal on Optimization},
  title     = {Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  year      = {2013},
  number    = {4},
  pages     = {2341--2368},
  volume    = {23},
  publisher = {SIAM},
}

@InProceedings{He2017,
  author    = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  title     = {Mask r-cnn},
  year      = {2017},
  pages     = {2961--2969},
}

@Article{Loshchilov2016,
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1608.03983},
  title   = {Sgdr: Stochastic gradient descent with warm restarts},
  year    = {2016},
}

@Article{Goyal2017,
  author  = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal = {arXiv preprint arXiv:1706.02677},
  title   = {Accurate, large minibatch sgd: Training imagenet in 1 hour},
  year    = {2017},
}

@InProceedings{Sutskever2014,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle = {Advances in neural information processing systems},
  title     = {Sequence to sequence learning with neural networks},
  year      = {2014},
  pages     = {3104--3112},
}


@Article{Popel2018,
  author    = {Popel, Martin and Bojar, Ond{\v{r}}ej},
  journal   = {The Prague Bulletin of Mathematical Linguistics},
  title     = {Training tips for the transformer model},
  year      = {2018},
  number    = {1},
  pages     = {43--70},
  volume    = {110},
  publisher = {De Gruyter Open},
}

@InProceedings{You2018,
  author       = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  booktitle    = {Proceedings of the 47th International Conference on Parallel Processing},
  title        = {Imagenet training in minutes},
  year         = {2018},
  organization = {ACM},
  pages        = {1},
}

@InProceedings{Kudo2018,
  author    = {Kudo, Taku and Richardson, John},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  title     = {SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  year      = {2018},
  pages     = {66--71},
}

@Article{Williams2017,
  author  = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal = {arXiv preprint arXiv:1704.05426},
  title   = {A broad-coverage challenge corpus for sentence understanding through inference},
  year    = {2017},
}

@Article{Rajpurkar2016a,
  author  = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal = {arXiv preprint arXiv:1606.05250},
  title   = {Squad: 100,000+ questions for machine comprehension of text},
  year    = {2016},
}

@InProceedings{Bentivogli2009a,
  author    = {Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  booktitle = {TAC},
  title     = {The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  year      = {2009},
}

@Article{Yang2019b,
  author  = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal = {Advances in Neural Information Processing Systems},
  title   = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  year    = {2019},
  pages   = {5753--5763},
  volume  = {32},
}

@Article{Wang2019,
  author  = {Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S},
  journal = {arXiv preprint arXiv:1906.01787},
  title   = {Learning Deep Transformer Models for Machine Translation},
  year    = {2019},
}

@Book{Wainwright2019,
  author    = {Wainwright, Martin J},
  publisher = {Cambridge University Press},
  title     = {High-dimensional statistics: A non-asymptotic viewpoint},
  year      = {2019},
  volume    = {48},
}

@InProceedings{Klein2018,
  author    = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Nguyen, Vincent and Senellart, Jean and Rush, Alexander},
  booktitle = {Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)},
  title     = {OpenNMT: Neural Machine Translation Toolkit},
  year      = {2018},
  pages     = {177--184},
  volume    = {1},
}

@Article{Ruder2016,
  author  = {Ruder, Sebastian},
  journal = {arXiv preprint arXiv:1609.04747},
  title   = {An overview of gradient descent optimization algorithms},
  year    = {2016},
}

@Article{Liu2019a,
  author  = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal = {arXiv preprint arXiv:1907.11692},
  title   = {Roberta: A robustly optimized bert pretraining approach},
  year    = {2019},
}

@InProceedings{Deng2009,
  author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle    = {2009 IEEE conference on computer vision and pattern recognition},
  title        = {Imagenet: A large-scale hierarchical image database},
  year         = {2009},
  organization = {Ieee},
  pages        = {248--255},
}

@Article{Clevert2015,
  author  = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  journal = {arXiv preprint arXiv:1511.07289},
  title   = {Fast and accurate deep network learning by exponential linear units (elus)},
  year    = {2015},
}

@InProceedings{Tsai2019,
  author    = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel},
  year      = {2019},
  pages     = {4335--4344},
}

@Misc{Loshchilov2018,
  author = {Ilya Loshchilov and Frank Hutter},
  title  = {Fixing Weight Decay Regularization in Adam},
  year   = {2018},
}

@Article{Merity2016,
  author  = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal = {arXiv preprint arXiv:1609.07843},
  title   = {Pointer sentinel mixture models},
  year    = {2016},
}

@Article{Song2021,
  author  = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  journal = {arXiv e-prints},
  title   = {Maximum Likelihood Training of Score-Based Diffusion Models},
  year    = {2021},
  pages   = {arXiv--2101},
}

@Article{Kingma2021,
  author  = {Kingma, Diederik P and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  journal = {arXiv preprint arXiv:2107.00630},
  title   = {Variational diffusion models},
  year    = {2021},
}

@Article{Grcic2021,
  author  = {Grci{\'c}, Matej and Grubi{\v{s}}i{\'c}, Ivan and {\v{S}}egvi{\'c}, Sini{\v{s}}a},
  journal = {arXiv preprint arXiv:2106.04627},
  title   = {Densely connected normalizing flows},
  year    = {2021},
}

@Article{Chrabaszcz2017,
  author  = {Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1707.08819},
  title   = {A downsampled variant of imagenet as an alternative to the cifar datasets},
  year    = {2017},
}

@Article{Inan2016,
  author  = {Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
  journal = {arXiv preprint arXiv:1611.01462},
  title   = {Tying word vectors and word classifiers: A loss framework for language modeling},
  year    = {2016},
}

@InProceedings{Kim2016,
  author    = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
  booktitle = {AAAI},
  title     = {Character-Aware Neural Language Models.},
  year      = {2016},
  pages     = {2741--2749},
}

@InProceedings{martins2022former,
  author    = {Martins, Pedro Henrique and Marinho, Zita and Martins, Andr{\'e} FT},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {∞-former: Infinite Memory Transformer-former: Infinite Memory Transformer},
  year      = {2022},
  pages     = {5468--5485},
}

@InProceedings{yang2016hierarchical,
  author    = {Zichao Yang and Diyi Yang and Chris Dyer and Xiaodong He and Alex Smola and Eduard Hovy},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {Hierarchical Attention Networks for Document Classification},
  year      = {2016},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/n16-1174},
}

@InProceedings{miculicich2018document,
  author    = {Lesly Miculicich and Dhananjay Ram and Nikolaos Pappas and James Henderson},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  title     = {Document-Level Neural Machine Translation with Hierarchical Attention Networks},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/d18-1325},
}

@InProceedings{guo2022longt5,
  author    = {Mandy Guo and Joshua Ainslie and David Uthus and Santiago Ontanon and Jianmo Ni and Yun-Hsuan Sung and Yinfei Yang},
  booktitle = {Findings of the Association for Computational Linguistics: {NAACL} 2022},
  title     = {{LongT}5: Efficient Text-To-Text Transformer for Long Sequences},
  year      = {2022},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2022.findings-naacl.55},
}

@InProceedings{fan2018hierarchical,
  author    = {Angela Fan and Mike Lewis and Yann Dauphin},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Hierarchical Neural Story Generation},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/p18-1082},
}

@inproceedings{
basu2021mirostat,
title={{\{}MIROSTAT{\}}: A {\{}NEURAL{\}} {\{}TEXT{\}} {\{}DECODING{\}} {\{}ALGORITHM{\}} {\{}THAT{\}} {\{}DIRECTLY{\}} {\{}CONTROLS{\}} {\{}PERPLEXITY{\}}},
author={Sourya Basu and Govardana Sachitanandam Ramachandran and Nitish Shirish Keskar and Lav R. Varshney},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=W1G1JZEIy5_}
}

@Article{yao2019plan,
  author    = {Lili Yao and Nanyun Peng and Ralph Weischedel and Kevin Knight and Dongyan Zhao and Rui Yan},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {Plan-and-Write: Towards Better Automatic Storytelling},
  year      = {2019},
  month     = {jul},
  number    = {01},
  pages     = {7378--7385},
  volume    = {33},
  doi       = {10.1609/aaai.v33i01.33017378},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}

@InProceedings{goldfarbtarrant2019plan,
  author    = {Seraphina Goldfarb-Tarrant and Haining Feng and Nanyun Peng},
  booktitle = {Proceedings of the 2019 Conference of the North},
  title     = {Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation},
  year      = {2019},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/n19-4016},
}

@InProceedings{rashkin2020plotmachines,
  author    = {Hannah Rashkin and Asli Celikyilmaz and Yejin Choi and Jianfeng Gao},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  title     = {{PlotMachines}: Outline-Conditioned Generation with Dynamic Plot State Tracking},
  year      = {2020},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.emnlp-main.349},
}

@Article{yang2022doc,
  author        = {Yang, Kevin and Klein, Dan and Peng, Nanyun and Tian, Yuandong},
  title         = {DOC: Improving Long Story Coherence With Detailed Outline Control},
  year          = {2022},
  month         = dec,
  abstract      = {We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged DOC to be much more controllable in an interactive generation setting.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2212.10077},
  eprint        = {2212.10077},
  file          = {:http\://arxiv.org/pdf/2212.10077v1:PDF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{rae2020compressive,
  author    = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Hillier, Chloe and Lillicrap, Timothy P},
  booktitle = {International Conference on Learning Representations},
  title     = {Compressive Transformers for Long-Range Sequence Modelling},
  year      = {2020},
}

@InProceedings{wu2022memorizing,
  author    = {Yuhuai Wu and Markus Norman Rabe and DeLesley Hutchins and Christian Szegedy},
  booktitle = {International Conference on Learning Representations},
  title     = {Memorizing Transformers},
  year      = {2022},
  url       = {https://openreview.net/forum?id=TrjbxzRcnf-},
}

@Article{fan2020addressing,
  author        = {Fan, Angela and Lavril, Thibaut and Grave, Edouard and Joulin, Armand and Sukhbaatar, Sainbayar},
  title         = {Addressing Some Limitations of Transformers with Feedback Memory},
  year          = {2020},
  month         = feb,
  abstract      = {Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2002.09402},
  eprint        = {2002.09402},
  file          = {:http\://arxiv.org/pdf/2002.09402v3:PDF},
  keywords      = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{sukhbaatar2019adaptive,
  author    = {Sainbayar Sukhbaatar and Edouard Grave and Piotr Bojanowski and Armand Joulin},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  title     = {Adaptive Attention Span in Transformers},
  year      = {2019},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/p19-1032},
}

@InProceedings{ainslie2020etc,
  author    = {Joshua Ainslie and Santiago Ontanon and Chris Alberti and Vaclav Cvicek and Zachary Fisher and Philip Pham and Anirudh Ravula and Sumit Sanghai and Qifan Wang and Li Yang},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  title     = {{ETC}: Encoding Long and Structured Inputs in Transformers},
  year      = {2020},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.emnlp-main.19},
}

@InProceedings{wu2020lite,
  author    = {Zhanghao Wu and Zhijian Liu and Ji Lin and Yujun Lin and Song Han},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Lite Transformer with Long-Short Range Attention},
  year      = {2020},
}

@Article{tay2022scaling,
  author  = {Yi Tay and Mostafa Dehghani and Samira Abnar and Hyung Won Chung and William Fedus and Jinfeng Rao and Sharan Narang and Vinh Q. Tran and Dani Yogatama and Donald Metzler},
  journal = {arXiv preprint arXiv: Arxiv-2207.10551},
  title   = {Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
  year    = {2022},
}

@Article{pope2022efficiently,
  author        = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  title         = {Efficiently Scaling Transformer Inference},
  year          = {2022},
  month         = nov,
  abstract      = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2211.05102},
  eprint        = {2211.05102},
  file          = {:http\://arxiv.org/pdf/2211.05102v1:PDF},
  keywords      = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{razdaibiedina2023progressive,
  author        = {Razdaibiedina, Anastasia and Mao, Yuning and Hou, Rui and Khabsa, Madian and Lewis, Mike and Almahairi, Amjad},
  title         = {Progressive Prompts: Continual Learning for Language Models},
  year          = {2023},
  month         = jan,
  abstract      = {We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2301.12314},
  eprint        = {2301.12314},
  file          = {:http\://arxiv.org/pdf/2301.12314v1:PDF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{chen2023accelerating,
  author        = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  title         = {Accelerating Large Language Model Decoding with Speculative Sampling},
  year          = {2023},
  month         = feb,
  abstract      = {We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2302.01318},
  eprint        = {2302.01318},
  file          = {:http\://arxiv.org/pdf/2302.01318v1:PDF},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{stern2018blockwise,
  author    = {Mitchell Stern and Noam M. Shazeer and Jakob Uszkoreit},
  journal   = {Neural Information Processing Systems},
  title     = {Blockwise Parallel Decoding for Deep Autoregressive Models},
  year      = {2018},
  bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/5e04881e91bff952d102d967c4ffb498ec30d4af},
}

@misc {gante2023assisted,
    author       = { {Joao Gante} },
    title        = { Assisted Generation: a new direction toward low-latency text generation },
    year         = 2023,
    url          = { https://huggingface.co/blog/assisted-generation },
    doi          = { 10.57967/hf/0638 },
    publisher    = { Hugging Face Blog }
}

@Article{leviathan2022fast,
  author        = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  title         = {Fast Inference from Transformers via Speculative Decoding},
  year          = {2022},
  month         = nov,
  abstract      = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method supports existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2211.17192},
  eprint        = {2211.17192},
  file          = {:http\://arxiv.org/pdf/2211.17192v1:PDF},
  keywords      = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@article{li2022decoupled,
  title   = {Decoupled Context Processing for Context Augmented Language Modeling},
  author  = {Zonglin Li and Ruiqi Guo and Sanjiv Kumar},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.05758}
}

@InProceedings{khandelwal2020generalization,
  author    = {Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
  booktitle = {International Conference on Learning Representations},
  title     = {Generalization through Memorization: Nearest Neighbor Language Models},
  year      = {2020},
  url       = {https://openreview.net/forum?id=HklBjCEKvH},
}

@InProceedings{guu2020retrieval,
  author    = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Retrieval Augmented Language Model Pre-Training},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {3929--3938},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
  pdf       = {http://proceedings.mlr.press/v119/guu20a/guu20a.pdf},
  url       = {https://proceedings.mlr.press/v119/guu20a.html},
}

@Article{lewis2020retrieval,
  author  = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
  year    = {2020},
  pages   = {9459--9474},
  volume  = {33},
}

@InProceedings{borgeaud2022improving,
  author    = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  title     = {Improving Language Models by Retrieving from Trillions of Tokens},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  month     = {17--23 Jul},
  pages     = {2206--2240},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  abstract  = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25{\texttimes} fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
  pdf       = {https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf},
  url       = {https://proceedings.mlr.press/v162/borgeaud22a.html},
}

@InProceedings{holtzman2020curious,
  author    = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
  booktitle = {International Conference on Learning Representations},
  title     = {The Curious Case of Neural Text Degeneration},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rygGQyrFvH},
}

@InProceedings{kasai2021finetuninga,
  author    = {Jungo Kasai and Hao Peng and Yizhe Zhang and Dani Yogatama and Gabriel Ilharco and Nikolaos Pappas and Yi Mao and Weizhu Chen and Noah A. Smith},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  title     = {Finetuning Pretrained Transformers into {RNNs}},
  year      = {2021},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.emnlp-main.830},
}

@Article{hutchins2022block,
  author  = {DeLesley Hutchins and Imanol Schlag and Yuhuai Wu and Ethan Dyer and Behnam Neyshabur},
  journal = {arXiv preprint arXiv: Arxiv-2203.07852},
  title   = {Block-Recurrent Transformers},
  year    = {2022},
}

@InProceedings{tay2020long,
  author    = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle = {International Conference on Learning Representations},
  title     = {Long Range Arena: A Benchmark for Efficient Transformers},
  year      = {2020},
}

@InProceedings{sun2021do,
  author    = {Simeng Sun and Kalpesh Krishna and Andrew Mattarella-Micke and Mohit Iyyer},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  title     = {Do Long-Range Language Models Actually Use Long-Range Context?},
  year      = {2021},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.emnlp-main.62},
}

@InProceedings{sukhbaatar2021not,
  author       = {Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela},
  booktitle    = {International Conference on Machine Learning},
  title        = {Not All Memories are Created Equal: Learning to Forget by Expiring},
  year         = {2021},
  organization = {PMLR},
  pages        = {9902--9912},
}

@Article{hassid2022how,
  author        = {Hassid, Michael and Peng, Hao and Rotem, Daniel and Kasai, Jungo and Montero, Ivan and Smith, Noah A. and Schwartz, Roy},
  title         = {How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers},
  year          = {2022},
  month         = nov,
  abstract      = {The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones -- the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance -- an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2211.03495},
  eprint        = {2211.03495},
  file          = {:http\://arxiv.org/pdf/2211.03495v1:PDF},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{hewitt2022truncation,
  author        = {Hewitt, John and Manning, Christopher D. and Liang, Percy},
  title         = {Truncation Sampling as Language Model Desmoothing},
  year          = {2022},
  month         = oct,
  abstract      = {Long samples of text from neural language models can be of poor quality. Truncation sampling algorithms--like top-$p$ or top-$k$ -- address this by setting some words' probabilities to zero at each step. This work provides framing for the aim of truncation, and an improved algorithm for that aim. We propose thinking of a neural language model as a mixture of a true distribution and a smoothing distribution that avoids infinite perplexity. In this light, truncation algorithms aim to perform desmoothing, estimating a subset of the support of the true distribution. Finding a good subset is crucial: we show that top-$p$ unnecessarily truncates high-probability words, for example causing it to truncate all words but Trump for a document that starts with Donald. We introduce $\eta$-sampling, which truncates words below an entropy-dependent probability threshold. Compared to previous algorithms, $\eta$-sampling generates more plausible long English documents according to humans, is better at breaking out of repetition, and behaves more reasonably on a battery of test distributions.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2210.15191},
  eprint        = {2210.15191},
  file          = {:http\://arxiv.org/pdf/2210.15191v1:PDF},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{hashimoto2019unifying,
  author    = {Tatsunori Hashimoto and Hugh Zhang and Percy Liang},
  booktitle = {Proceedings of the 2019 Conference of the North},
  title     = {Unifying Human and Statistical Evaluation for Natural Language Generation},
  year      = {2019},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/n19-1169},
}

@article{meister2023locally,
  title={Locally typical sampling},
  author={Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={102--121},
  year={2023},
  publisher={MIT Press}
}

@article{elfwing2017sigmoidweighted,
  title     = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
  author    = {Stefan Elfwing and E. Uchibe and K. Doya},
  journal   = {Neural Networks},
  year      = {2017},
  doi       = {10.1016/j.neunet.2017.12.012},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b587ee7c802a5bd222a69090f59285e0dfdb29f1}
}

@Article{meister2022probability,
  author        = {Meister, Clara and Wiher, Gian and Pimentel, Tiago and Cotterell, Ryan},
  title         = {On the probability-quality paradox in language generation},
  year          = {2022},
  month         = mar,
  abstract      = {When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text -- covering multiple tasks and common decoding strategies -- suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2203.17217},
  eprint        = {2203.17217},
  file          = {:http\://arxiv.org/pdf/2203.17217v1:PDF},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{honovich2022instruction,
  author        = {Honovich, Or and Shaham, Uri and Bowman, Samuel R. and Levy, Omer},
  title         = {Instruction Induction: From Few Examples to Natural Language Task Descriptions},
  year          = {2022},
  month         = may,
  abstract      = {Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2205.10782},
  eprint        = {2205.10782},
  file          = {:http\://arxiv.org/pdf/2205.10782v1:PDF},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{singh2022explaining,
  author        = {Singh, Chandan and Morris, John X. and Aneja, Jyoti and Rush, Alexander M. and Gao, Jianfeng},
  title         = {Explaining Patterns in Data with Language Models via Interpretable Autoprompting},
  year          = {2022},
  month         = oct,
  abstract      = {Large language models (LLMs) have displayed an impressive ability to harness natural language to perform complex tasks. In this work, we explore whether we can leverage this learned ability to find and explain patterns in data. Specifically, given a pre-trained LLM and data examples, we introduce interpretable autoprompting (iPrompt), an algorithm that generates a natural-language string explaining the data. iPrompt iteratively alternates between generating explanations with an LLM and reranking them based on their performance when used as a prompt. Experiments on a wide range of datasets, from synthetic mathematics to natural-language understanding, show that iPrompt can yield meaningful insights by accurately finding groundtruth dataset descriptions. Moreover, the prompts produced by iPrompt are simultaneously human-interpretable and highly effective for generalization: on real-world sentiment classification datasets, iPrompt produces prompts that match or even improve upon human-written prompts for GPT-3. Finally, experiments with an fMRI dataset show the potential for iPrompt to aid in scientific discovery. All code for using the methods and data here is made available on Github.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2210.01848},
  eprint        = {2210.01848},
  file          = {:http\://arxiv.org/pdf/2210.01848v2:PDF},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Neurons and Cognition (q-bio.NC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Biological sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{shin2020autoprompt,
  author    = {Taylor Shin and Yasaman Razeghi and Robert L. Logan IV and Eric Wallace and Sameer Singh},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  title     = {{AutoPrompt}: Eliciting Knowledge from Language Models with Automatically Generated Prompts},
  year      = {2020},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.emnlp-main.346},
}

@Article{zhou2022large,
  author        = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  title         = {Large Language Models Are Human-Level Prompt Engineers},
  year          = {2022},
  month         = nov,
  abstract      = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2211.01910},
  eprint        = {2211.01910},
  file          = {:http\://arxiv.org/pdf/2211.01910v1:PDF},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{wang2022selfa,
  author        = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  title         = {Self-Instruct: Aligning Language Model with Self Generated Instructions},
  year          = {2022},
  month         = dec,
  abstract      = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2212.10560},
  eprint        = {2212.10560},
  file          = {:http\://arxiv.org/pdf/2212.10560v1:PDF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: keypatterndefault:[auth:lower][year][veryshorttitle:lower];}

@Comment{jabref-meta: saveActions:disabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}

@misc{tiny_vicuna_1b,
  author = {Jiayi Pan},
  title = {Tiny Vicuna 1B},
  howpublished = {\url{https://huggingface.co/Jiayi-Pan/Tiny-Vicuna-1B}},
  year = {2023},
  note = {}
}

@misc{zhang2024tinyllama,
      title={TinyLlama: An Open-Source Small Language Model}, 
      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
      year={2024},
      eprint={2401.02385},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}