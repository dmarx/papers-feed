\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy,
  Lebr{\'o}n, and Sanghai]{ainslie2023gqa}
Ainslie, J., Lee-Thorp, J., de~Jong, M., Zemlyanskiy, Y., Lebr{\'o}n, F., and
  Sanghai, S.
\newblock Gqa: Training generalized multi-query transformer models from
  multi-head checkpoints.
\newblock \emph{arXiv preprint arXiv:2305.13245}, 2023.

\bibitem[Axolotl(2023)]{axolotl2023}
Axolotl.
\newblock {Axolotl}.
\newblock \url{https://github.com/OpenAccess-AI-Collective/axolotl}, 2023.

\bibitem[Basu et~al.(2021)Basu, Ramachandran, Keskar, and
  Varshney]{basu2021mirostat}
Basu, S., Ramachandran, G.~S., Keskar, N.~S., and Varshney, L.~R.
\newblock {\{}MIROSTAT{\}}: A {\{}neural{\}} {\{}text{\}} {\{}decoding{\}}
  {\{}algorithm{\}} {\{}that{\}} {\{}directly{\}} {\{}controls{\}}
  {\{}perplexity{\}}.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=W1G1JZEIy5_}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2023)Chen, Borgeaud, Irving, Lespiau, Sifre, and
  Jumper]{chen2023accelerating}
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock February 2023.
\newblock \doi{10.48550/ARXIV.2302.01318}.

\bibitem[Chen(2023)]{chen2023transformer}
Chen, L.
\newblock Dissecting batching effects in gpt inference.
\newblock \url{https://le.qun.ch/en/blog/2023/05/13/transformer-batching/},
  2023.
\newblock Blog.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L.,
  Zhuang, S., Zhuang, Y., Gonzalez, J.~E., Stoica, I., and Xing, E.~P.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Dettmers et~al.(2021)Dettmers, Lewis, Shleifer, and
  Zettlemoyer]{dettmers20218bit}
Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L.
\newblock 8-bit optimizers via block-wise quantization.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llm}
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and
  Zhou]{ding2023enhancing}
Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M., and
  Zhou, B.
\newblock Enhancing chat language models by scaling high-quality instructional
  conversations, 2023.

\bibitem[Dubois et~al.(2023)Dubois, Li, Taori, Zhang, Gulrajani, Ba, Guestrin,
  Liang, and Hashimoto]{dubois2023alpacafarm}
Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C.,
  Liang, P., and Hashimoto, T.~B.
\newblock Alpacafarm: A simulation framework for methods that learn from human
  feedback, 2023.

\bibitem[Elfwing et~al.(2017)Elfwing, Uchibe, and
  Doya]{elfwing2017sigmoidweighted}
Elfwing, S., Uchibe, E., and Doya, K.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock \emph{Neural Networks}, 2017.
\newblock \doi{10.1016/j.neunet.2017.12.012}.

\bibitem[Fan et~al.(2018)Fan, Lewis, and Dauphin]{fan2018hierarchical}
Fan, A., Lewis, M., and Dauphin, Y.
\newblock Hierarchical neural story generation.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}. Association for
  Computational Linguistics, 2018.
\newblock \doi{10.18653/v1/p18-1082}.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and
  Alistarh]{frantar2022gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Google(2023)]{google2023palm2}
Google.
\newblock Palm 2 technical report, 2023.
\newblock URL \url{https://ai.google/static/documents/palm2techreport.pdf}.

\bibitem[Hewitt et~al.(2022)Hewitt, Manning, and Liang]{hewitt2022truncation}
Hewitt, J., Manning, C.~D., and Liang, P.
\newblock Truncation sampling as language model desmoothing.
\newblock October 2022.
\newblock \doi{10.48550/ARXIV.2210.15191}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
  E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2020curious}
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and
  Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{ICLR}, 2021.

\bibitem[{Joao Gante}(2023)]{gante2023assisted}
{Joao Gante}.
\newblock Assisted generation: a new direction toward low-latency text
  generation, 2023.
\newblock URL \url{https://huggingface.co/blog/assisted-generation}.

\bibitem[Kim et~al.(2023)Kim, Hooper, Gholami, Dong, Li, Shen, Mahoney, and
  Keutzer]{kim2023squeezellm}
Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M.~W.,
  and Keutzer, K.
\newblock Squeezellm: Dense-and-sparse quantization.
\newblock \emph{arXiv preprint arXiv:2306.07629}, 2023.

\bibitem[Kim \& Rush(2016)Kim and Rush]{kim2016sequencelevel}
Kim, Y. and Rush, A.~M.
\newblock Sequence-level knowledge distillation.
\newblock \emph{EMNLP}, 2016.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and
  Liang]{kumar2022finetuning}
Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P.
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang,
  and Stoica]{kwon2023efficient}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J.~E.,
  Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with
  pagedattention.
\newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating
  Systems Principles}, 2023.

\bibitem[Leviathan et~al.(2022)Leviathan, Kalman, and
  Matias]{leviathan2022fast}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding.
\newblock November 2022.
\newblock \doi{10.48550/ARXIV.2211.17192}.

\bibitem[Li et~al.(2023)Li, Zhang, Dubois, Taori, Gulrajani, Guestrin, Liang,
  and Hashimoto]{alpaca_eval}
Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang,
  P., and Hashimoto, T.~B.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, and Han]{lin2023awq}
Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S.
\newblock Awq: Activation-aware weight quantization for llm compression and
  acceleration.
\newblock \emph{arXiv preprint arXiv:2306.00978}, 2023.

\bibitem[Meister et~al.(2022)Meister, Wiher, Pimentel, and
  Cotterell]{meister2022probability}
Meister, C., Wiher, G., Pimentel, T., and Cotterell, R.
\newblock On the probability-quality paradox in language generation.
\newblock March 2022.
\newblock \doi{10.48550/ARXIV.2203.17217}.

\bibitem[Meister et~al.(2023)Meister, Pimentel, Wiher, and
  Cotterell]{meister2023locally}
Meister, C., Pimentel, T., Wiher, G., and Cotterell, R.
\newblock Locally typical sampling.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  11:\penalty0 102--121, 2023.

\bibitem[Miao et~al.(2023)Miao, Oliaro, Zhang, Cheng, Wang, Wong, Chen, Arfeen,
  Abhyankar, and Jia]{miao2023specinfer}
Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong, R. Y.~Y., Chen, Z.,
  Arfeen, D., Abhyankar, R., and Jia, Z.
\newblock Specinfer: Accelerating generative llm serving with speculative
  inference and token tree verification.
\newblock \emph{arXiv preprint arXiv:2305.09781}, 2023.

\bibitem[NVIDIA()]{nvidia_a100_datasheet}
NVIDIA.
\newblock Nvidia a100 tensor core gpu.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P.,
  Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Pan(2023)]{tiny_vicuna_1b}
Pan, J.
\newblock Tiny vicuna 1b.
\newblock \url{https://huggingface.co/Jiayi-Pan/Tiny-Vicuna-1B}, 2023.

\bibitem[Pillutla et~al.(2021)Pillutla, Swayamdipta, Zellers, Thickstun,
  Welleck, Choi, and Harchaoui]{pillutla2021mauve}
Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J., Welleck, S., Choi,
  Y., and Harchaoui, Z.
\newblock {MAUVE}: Measuring the gap between neural text and human text using
  divergence frontiers.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Tqx7nJp7PR}.

\bibitem[Pope et~al.(2022)Pope, Douglas, Chowdhery, Devlin, Bradbury, Levskaya,
  Heek, Xiao, Agrawal, and Dean]{pope2022efficiently}
Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A.,
  Heek, J., Xiao, K., Agrawal, S., and Dean, J.
\newblock Efficiently scaling transformer inference.
\newblock November 2022.
\newblock \doi{10.48550/ARXIV.2211.05102}.

\bibitem[ShareGPT(2023)]{sharegpt2023}
ShareGPT.
\newblock {ShareGPT}.
\newblock
  \url{https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered}, 2023.

\bibitem[Shazeer(2019)]{shazeer2019fast}
Shazeer, N.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv preprint arXiv:1911.02150}, 2019.

\bibitem[Spector \& Re(2023)Spector and Re]{spector2023accelerating}
Spector, B. and Re, C.
\newblock Accelerating llm inference with staged speculative decoding.
\newblock \emph{arXiv preprint arXiv:2308.04623}, 2023.

\bibitem[Stern et~al.(2018)Stern, Shazeer, and Uszkoreit]{stern2018blockwise}
Stern, M., Shazeer, N.~M., and Uszkoreit, J.
\newblock Blockwise parallel decoding for deep autoregressive models.
\newblock \emph{Neural Information Processing Systems}, 2018.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul,
  Belkada, Huang, von Werra, Fourrier, Habib, Sarrazin, Sanseviero, Rush, and
  Wolf]{tunstall2023zephyr}
Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y.,
  Huang, S., von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero,
  O., Rush, A.~M., and Wolf, T.
\newblock Zephyr: Direct distillation of lm alignment, 2023.

\bibitem[Xia et~al.(2023)Xia, Ge, Chen, Wei, and Sui]{xia2023speculative}
Xia, H., Ge, T., Chen, S.-Q., Wei, F., and Sui, Z.
\newblock Speculative decoding: Lossless speedup of autoregressive translation,
  2023.
\newblock URL \url{https://openreview.net/forum?id=H-VlwsYvVi}.

\bibitem[Xiao et~al.(2023{\natexlab{a}})Xiao, Lin, Seznec, Wu, Demouth, and
  Han]{xiao2023smoothquant}
Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  38087--38099. PMLR, 2023{\natexlab{a}}.

\bibitem[Xiao et~al.(2023{\natexlab{b}})Xiao, Wu, Guo, Li, Zhang, Qin, and
  Liu]{xiao2023survey}
Xiao, Y., Wu, L., Guo, J., Li, J., Zhang, M., Qin, T., and Liu, T.-y.
\newblock A survey on non-autoregressive generation for neural machine
  translation and beyond.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2023{\natexlab{b}}.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying2021transformers}
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y.
\newblock Do transformers really perform badly for graph representation?
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 28877--28888, 2021.

\bibitem[Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu]{zhang2024tinyllama}
Zhang, P., Zeng, G., Wang, T., and Lu, W.
\newblock Tinyllama: An open-source small language model, 2024.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,
  Diab, M., Li, X., Lin, X.~V., et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Sheng, Zhou, Chen, Zheng, Cai, Song, Tian,
  R{\'e}, Barrett, et~al.]{zhang2023h}
Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian,
  Y., R{\'e}, C., Barrett, C., et~al.
\newblock H $ \_2 $ o: Heavy-hitter oracle for efficient generative inference
  of large language models.
\newblock \emph{arXiv preprint arXiv:2306.14048}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z.,
  Li, Z., Li, D., Xing, E.~P., Zhang, H., Gonzalez, J.~E., and Stoica, I.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\end{thebibliography}
