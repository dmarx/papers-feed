@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@ARTICLE{Chizat2018-sc,
  title    = "On Lazy Training in Differentiable Programming",
  author   = "Chizat, Lénaïc and Oyallon, Edouard and Bach, F",
  journal  = "Adv. Neural Inf. Process. Syst.",
  pages    = "2933--2943",
  abstract = "In a series of recent theoretical works, it was shown that
              strongly over-parameterized neural networks trained with
              gradient-based methods could converge exponentially fast to zero
              training loss, with their parameters hardly varying. In this work,
              we show that this ``lazy training'' phenomenon is not specific to
              over-parameterized neural networks, and is due to a choice of
              scaling, often implicit, that makes the model behave as its
              linearization around the initialization, thus yielding a model
              equivalent to learning with positive-definite kernels. Through a
              theoretical analysis, we exhibit various situations where this
              phenomenon arises in non-convex optimization and we provide bounds
              on the distance between the lazy and linearized optimization
              paths. Our numerical experiments bring a critical note, as we
              observe that the performance of commonly used non-linear deep
              convolutional neural networks in computer vision degrades when
              trained in the lazy regime. This makes it unlikely that ``lazy
              training'' is behind the many successes of neural networks in
              difficult high dimensional tasks.",
  month    =  dec,
  year     =  2018
}

@inproceedings{
Kumar2023-hz,
title={Grokking as the transition from lazy to rich training dynamics},
author={Tanishq Kumar and Blake Bordelon and Samuel J. Gershman and Cengiz Pehlevan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}


@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@ARTICLE{Liu2022-ne,
  title    = "Towards understanding grokking: An effective theory of
              representation learning",
  author   = "Liu, Ziming and Kitouni, O and Nolte, Niklas and Michaud, Eric J
              and Tegmark, Max and Williams, Mike",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   = "abs/2205.10343",
  month    =  may,
  year     =  2022
}

@inproceedings{
liu2023omnigrok,
title={Omnigrok: Grokking Beyond Algorithmic Data},
author={Ziming Liu and Eric J Michaud and Max Tegmark},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@ARTICLE{Thilak2022-qc,
  title         = "The Slingshot Mechanism: An Empirical Study of Adaptive
                   Optimizers and the Grokking Phenomenon",
  author        = "Thilak, Vimal and Littwin, Etai and Zhai, Shuangfei and
                   Saremi, Omid and Paiss, Roni and Susskind, Joshua",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2206.04817"
}

@ARTICLE{Rosenfeld2023-qd,
  title         = "Outliers with Opposing Signals Have an Outsized Effect on
                   Neural Network Optimization",
  author        = "Rosenfeld, Elan and Risteski, Andrej",
  abstract      = "We identify a new phenomenon in neural network optimization
                   which arises from the interaction of depth and a particular
                   heavy-tailed structure in natural data. Our result offers
                   intuitive explanations for several previously reported
                   observations about network training dynamics. In particular,
                   it implies a conceptually new cause for progressive
                   sharpening and the edge of stability; we also highlight
                   connections to other concepts in optimization and
                   generalization including grokking, simplicity bias, and
                   Sharpness-Aware Minimization. Experimentally, we demonstrate
                   the significant influence of paired groups of outliers in
                   the training data with strong opposing signals: consistent,
                   large magnitude features which dominate the network output
                   throughout training and provide gradients which point in
                   opposite directions. Due to these outliers, early
                   optimization enters a narrow valley which carefully balances
                   the opposing groups; subsequent sharpening causes their loss
                   to rise rapidly, oscillating between high on one group and
                   then the other, until the overall loss spikes. We describe
                   how to identify these groups, explore what sets them apart,
                   and carefully study their effect on the network's
                   optimization and behavior. We complement these experiments
                   with a mechanistic explanation on a toy example of opposing
                   signals and a theoretical analysis of a two-layer linear
                   network on a simple model. Our finding enables new
                   qualitative predictions of training behavior which we
                   confirm experimentally. It also provides a new lens through
                   which to study and improve modern training practices for
                   stochastic optimization, which we highlight via a case study
                   of Adam versus SGD.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2311.04163"
}

@inproceedings{
Nanda2023-hf,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}


@ARTICLE{Bansal2022-ck,
  title    = "Measures of information reflect memorization patterns",
  author   = "Bansal, Rachit and Pruthi, Danish and Belinkov, Yonatan",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   = "abs/2210.09404",
  month    =  oct,
  year     =  2022
}

@ARTICLE{Merrill2023-rp,
  title         = "A Tale of Two Circuits: Grokking as Competition of Sparse
                   and Dense Subnetworks",
  author        = "Merrill, William and Tsilivis, Nikolaos and Shukla, Aman",
  abstract      = "Grokking is a phenomenon where a model trained on an
                   algorithmic task first overfits but, then, after a large
                   amount of additional training, undergoes a phase transition
                   to generalize perfectly. We empirically study the internal
                   structure of networks undergoing grokking on the sparse
                   parity task, and find that the grokking phase transition
                   corresponds to the emergence of a sparse subnetwork that
                   dominates model predictions. On an optimization level, we
                   find that this subnetwork arises when a small subset of
                   neurons undergoes rapid norm growth, whereas the other
                   neurons in the network decay slowly in norm. Thus, we
                   suggest that the grokking phase transition can be understood
                   to emerge from competition of two largely distinct
                   subnetworks: a dense one that dominates before the
                   transition and generalizes poorly, and a sparse one that
                   dominates afterwards.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2303.11873"
}

@article{Varma2023,
  title={Explaining grokking through circuit efficiency},
  author={Varma, Vikrant and Shah, Rohin and Kenton, Zachary and Kram{\'a}r, J{\'a}nos and Kumar, Ramana},
  journal={arXiv preprint arXiv:2309.02390},
  year={2023}
}




@inproceedings{
Lyu2023-ga,
title={Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking},
author={Kaifeng Lyu and Jikai Jin and Zhiyuan Li and Simon Shaolei Du and Jason D. Lee and Wei Hu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}


@article{Gromov2023-nh,
  title={Grokking modular arithmetic},
  author={Gromov, Andrey},
  journal={arXiv preprint arXiv:2301.02679},
  year={2023}
}


@ARTICLE{Miller2023-bv,
  title         = "Grokking Beyond Neural Networks: An Empirical Exploration
                   with Model Complexity",
  author        = "Miller, Jack and O'Neill, Charles and Bui, Thang",
  abstract      = "In some settings neural networks exhibit a phenomenon known
                   as \textbackslashtextit\{grokking\}, where they achieve
                   perfect or near-perfect accuracy on the validation set long
                   after the same performance has been achieved on the training
                   set. In this paper, we discover that grokking is not limited
                   to neural networks but occurs in other settings such as
                   Gaussian process (GP) classification, GP regression, linear
                   regression and Bayesian neural networks. We also uncover a
                   mechanism by which to induce grokking on algorithmic
                   datasets via the addition of dimensions containing spurious
                   information. The presence of the phenomenon in non-neural
                   architectures shows that grokking is not restricted to
                   settings considered in current theoretical and empirical
                   studies. Instead, grokking may be possible in any model
                   where solution search is guided by complexity and error.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2310.17247"
}


@article{Barak2022-el,
  title={Hidden progress in deep learning: Sgd learns parities near the computational limit},
  author={Barak, Boaz and Edelman, Benjamin and Goel, Surbhi and Kakade, Sham and Malach, Eran and Zhang, Cyril},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}


@inproceedings{
mehta2021extreme,
title={Extreme Memorization via Scale of Initialization},
author={Harsh Mehta and Ashok Cutkosky and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Z4R1vxLbRLO}
}

@inproceedings{
Lyu2019-sc,
title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
author={Kaifeng Lyu and Jian Li},
booktitle={International Conference on Learning Representations},
year={2020}
}


@article{homogeneous_transformers,
  author       = {William Merrill and
                  Vivek Ramanujan and
                  Yoav Goldberg and
                  Roy Schwartz and
                  Noah A. Smith},
  title        = {Parameter Norm Growth During Training of Transformers},
  journal      = {CoRR},
  volume       = {abs/2010.09697},
  year         = {2020},
  eprinttype    = {arXiv},
  eprint       = {2010.09697}
}



%% ---------------- TOLGA --------
@article{wang2024grokked,
  title={Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization},
  author={Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
  journal={arXiv preprint arXiv:2405.15071},
  year={2024}
}
@article{fan2024deep,
  title={Deep Grokking: Would Deep Neural Networks Generalize Better?},
  author={Fan, Simin and Pascanu, Razvan and Jaggi, Martin},
  journal={arXiv preprint arXiv:2405.19454},
  year={2024}
}
@article{doshi2024grokking,
  title={Grokking Modular Polynomials},
  author={Doshi, Darshil and He, Tianyu and Das, Aritra and Gromov, Andrey},
  journal={arXiv preprint arXiv:2406.03495},
  year={2024}
}
@article{mallinar2024emergence,
  title={Emergence in non-neural models: grokking modular arithmetic via average gradient outer product},
  author={Mallinar, Neil and Beaglehole, Daniel and Zhu, Libin and Radhakrishnan, Adityanarayanan and Pandit, Parthe and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2407.20199},
  year={2024}
}
@article{clauw2024information,
  title={Information-Theoretic Progress Measures reveal Grokking is an Emergent Phase Transition},
  author={Clauw, Kenzo and Stramaglia, Sebastiano and Marinazzo, Daniele},
  journal={arXiv preprint arXiv:2408.08944},
  year={2024}
}
@article{mohamadi2024you,
  title={Why Do You Grok? A Theoretical Analysis of Grokking Modular Addition},
  author={Mohamadi, Mohamad Amin and Li, Zhiyuan and Wu, Lei and Sutherland, Danica J},
  journal={arXiv preprint arXiv:2407.12332},
  year={2024}
}
@inproceedings{humayun2024grokking,
  title={Grokking and the Geometry of Circuit Formation},
  author={Humayun, Ahmed Imtiaz and Balestriero, Randall and Baraniuk, Richard},
  booktitle={ICML Workshop on Mechanistic Interpretability},
  year={2024}
}
@article{kumar2023grokking,
  title={Grokking as the transition from lazy to rich training dynamics},
  author={Kumar, Tanishq and Bordelon, Blake and Gershman, Samuel J and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2310.06110},
  year={2023}
}
@inproceedings{
liu2023grokking,
title={Grokking as Simplification: A Nonlinear Complexity Perspective},
author={Ziming Liu and Ziqian Zhong and Max Tegmark},
booktitle={UniReps:  the First Workshop on Unifying Representations in Neural Models},
year={2023},
}
@inproceedings{
rubin2024grokking,
title={Grokking as a First Order Phase Transition in Two Layer Networks},
author={Noa Rubin and Inbar Seroussi and Zohar Ringel},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}
@article{tan2023understanding,
  title={Understanding grokking through a robustness viewpoint},
  author={Tan, Zhiquan and Huang, Weiran},
  journal={arXiv preprint arXiv:2311.06597},
  year={2023}
}
@article{humayun2024deep,
  title={Deep networks always grok and here is why},
  author={Humayun, Ahmed Imtiaz and Balestriero, Randall and Baraniuk, Richard},
  journal={arXiv preprint arXiv:2402.15555},
  year={2024}
}
@inproceedings{
stander2023grokking,
title={Grokking Group Multiplication with Cosets},
author={Dashiell Stander and Qinan Yu and Honglu Fan and Stella Biderman},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
}
@inproceedings{chughtai2023toy,
  title={A toy model of universality: Reverse engineering how networks learn group operations},
  author={Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
  booktitle={International Conference on Machine Learning},
  year={2023},
  organization={PMLR}
}
@article{huang2024unified,
  title={Unified view of grokking, double descent and emergent abilities: A perspective from circuits competition},
  author={Huang, Yufei and Hu, Shengding and Han, Xu and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.15175},
  year={2024}
}

@inproceedings{
kunin2023asymmetricmaximummarginbias,
title={The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks},
author={Daniel Kunin and Atsushi Yamamura and Chao Ma and Surya Ganguli},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}
@article{vzunkovivc2024grokking,
  title={Grokking phase transitions in learning local rules with gradient descent},
  author={{\v{Z}}unkovi{\v{c}}, Bojan and Ilievski, Enej},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={199},
  pages={1--52},
  year={2024}
}
% for intro
@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
}
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}
@inproceedings{qi2017pointnet,
  title={Pointnet: Deep learning on point sets for 3d classification and segmentation},
  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={652--660},
  year={2017}
}
@inproceedings{deng2018ppfnet,
  title={Ppfnet: Global context aware local features for robust 3d point matching},
  author={Deng, Haowen and Birdal, Tolga and Ilic, Slobodan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2018}
}
@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@article{birdal2021intrinsic,
  title={Intrinsic dimension, persistent homology and generalization in neural networks},
  author={Birdal, Tolga and Lou, Aaron and Guibas, Leonidas J and Simsekli, Umut},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{andreevatopological,
  title={Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms},
  author={Andreeva, Rayna and Dupuis, Benjamin and Sarkar, Rik and Birdal, Tolga and Simsekli, Umut},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year = {2024}
}
@article{lv2024language,
  title={Language Models" Grok" to Copy},
  author={Lv, Ang and Xie, Ruobing and Sun, Xingwu and Kang, Zhanhui and Yan, Rui},
  journal={arXiv preprint arXiv:2409.09281},
  year={2024}
}
@article{goldberg1991every,
  title={What every computer scientist should know about floating-point arithmetic},
  author={Goldberg, David},
  journal={ACM computing surveys (CSUR)},
  volume={23},
  number={1},
  pages={5--48},
  year={1991},
  publisher={ACM New York, NY, USA}
}
@article{park2018training,
  title={Training deep neural network in limited precision},
  author={Park, Hyunsun and Lee, Jun Haeng and Oh, Youngmin and Ha, Sangwon and Lee, Seungwon},
  journal={arXiv preprint arXiv:1810.05486},
  year={2018}
}
@article{ogita2005accurate,
  title={Accurate sum and dot product},
  author={Ogita, Takeshi and Rump, Siegfried M and Oishi, Shin'ichi},
  journal={SIAM Journal on Scientific Computing},
  volume={26},
  number={6},
  pages={1955--1988},
  year={2005},
  publisher={SIAM}
}
@article{andriushchenko2023needweightdecaymodern,
  title={Why do we need weight decay in modern deep learning?},
  author={D'Angelo, Francesco and Andriushchenko, Maksym and Varre, Aditya and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2310.04415},
  year={2023}
}
@inproceedings{andreeva2024topological,
  title={Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms},
  author={Andreeva, Rayna and Dupuis, Benjamin and Sarkar, Rik and Birdal, Tolga and {\c{S}}im{\c{s}}ekli, Umut},
  booktitle={Adv. Neural Inf. Process. Syst.},
  year={2024}
}
@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}
@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@inproceedings{slingshot-mechanism,
title = {The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon},
booktitle = {NeurIPS Workshop},
author = {Vimal Thilak and Etai Littwin and Shuangfei Zhai and Omid Saremi and Roni Paiss and Josh Susskind},
year = {2022}
}
 @article{Chizat_Oyallon_Bach_2018, title={On Lazy Training in Differentiable Programming}, ISSN={1049-5258}, abstractNote={In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this “lazy training” phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that “lazy training” is behind the many successes of neural networks in difficult high dimensional tasks.}, journal={Advances in neural information processing systems}, author={Chizat, Lénaïc and Oyallon, Edouard and Bach, F.}, year={2018}, month=dec, pages={2933–2943} }

@inproceedings{stability,
  title={DeepStability: A study of unstable numerical methods and their solutions in deep learning},
  author={Kloberdanz, Eliska and Kloberdanz, Kyle G and Le, Wei},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={586--597},
  year={2022}
}

@inproceedings{softplus,
 author = {Dugas, Charles and Bengio, Yoshua and B\'{e}lisle, Fran\c{c}ois and Nadeau, Claude and Garcia, Ren\'{e}},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Incorporating Second-Order Functional Knowledge for Better Option Pricing},
 volume = {13},
 year = {2000}
}
@inproceedings{NEURIPS2020_c76e4b2f,
 author = {Ji, Ziwei and Telgarsky, Matus},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17176--17186},
 publisher = {Curran Associates, Inc.},
 title = {Directional convergence and alignment in deep learning},
 volume = {33},
 year = {2020}
}
@inproceedings{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={7th International Conference on Learning Representations, ICLR},
  year={2019}
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
@article{ji20182,
  title={Risk and parameter convergence of logistic regression},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1803.07300},
  year={2018}
}

@article{wang2024achieving,
  title={Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling},
  author={Wang, Mingze and Min, Zeping and Wu, Lei},
  journal={arXiv preprint arXiv:2311.14387},
  year={2024}
}
@inproceedings{
adamp2020,
title={AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights},
author={Byeongho Heo and Sanghyuk Chun and Seong Joon Oh and Dongyoon Han and Sangdoo Yun and Gyuwan Kim and Youngjung Uh and Jung-Woo Ha},
booktitle={International Conference on Learning Representations},
year={2021},

}

@misc{
kosson2024rotational,
title={Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks},
author={Atli Kosson and Bettina Messmer and Martin Jaggi},
year={2024},}
%

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}