\vspace{-2mm}
\section{Conclusion and Discussion}
\vspace{-2mm}
In this work, we show that na√Øve loss minimization (NLM) and floating point errors can explain why generalization is delayed in grokking and why it often does not happen without regularization. Using this insight, we are able to explain the success of existing methods to induce grokking. Motivated by our findings, we further design a simple modification to the \softmax that induces grokking by avoiding floating point errors and an optimizer that avoids the delay in generalization in grokking by preventing NLM. 

\paragraph{Limitations \& future work} While this work explains several surprising aspects of grokking settings, several questions remain. Notably, we focus our study of NLM on homogeneous or approximately homogeneous models. A a formal characterization  quasi-homogenous models could shed light on this kind of dynamics for models including skip connections and bias terms. Additionally, our explanation for why weight decay causes grokking could be enhanced by an analysis of the impact of weight decay on the effective learning rate as a potential explanation for the sudden nature of grokking.

\footnotesize
\paragraph{Acknowledgments}
This work was supported by the UKRI Centre for Doctoral Training in Safe and Trusted AI
[EP/S0233356/1]. TB acknowledges support from the Engineering and Physical Sciences Research Council [grant EP/X011364/1].
TB was supported by a UKRI Future Leaders Fellowship [grant number MR/Y018818/1]. 