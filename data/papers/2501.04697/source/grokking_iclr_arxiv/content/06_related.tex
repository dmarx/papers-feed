\section{Related Work}
\paragraph{Grokking}
\cite{power2022grokking} introduced grokking and showed that weight decay can consistently induce it in algorithmic tasks. \cite{Nanda2023-hf} were able to reverse engineer the inner workings of a grokked transformer and found progress measures for grokking induced by weight decay. \cite{chughtai2023toy} generalized the findings from \cite{Nanda2023-hf} and showed grokked networks use group representations to solve group composition tasks, although some of these findings were disputed in \cite{stander2023grokking} which propose that grokked networks learn a coset based algorithm for these same tasks. \citet{mallinar2024emergence} has shown that grokking is not specific to neural networks or gradient-based optimization and cannot be predicted from the training or test loss. \cite{Varma2023} argued that grokking is driven by weight decay favoring more efficient solutions and \cite{liu2023grokking} hypothesized that the weight norm of the models needs to be in a ``Goldilock's zone'' to generalize. \cite{Kumar2023-hz} and \cite{Lyu2023-ga} connected grokking to a transition between ``lazy training'' \citep{Chizat_Oyallon_Bach_2018} and feature learning, and \cite{Kumar2023-hz} showed that this can happen without regularization in the case of shallow networks with MSE loss. Grokking has also been described as a phase transition by \cite{vzunkovivc2024grokking}, \cite{Lyu2023-ga} and \cite{rubin2024grokking}. \cite{humayun2024deep} show that in many settings, neural networks undergo grokking-like transitions in their adversarial robustness. This aligns with the findings of \cite{Lyu2019-sc} which attributed this increased robustness to a bias of SGD towards a max-margin solution which was proven for homogeneous models.

\paragraph{Numerical instability in deep learning} 
Numerical instability is a common issue in deep learning \cite{stability}, especially when dealing with mixed precision training \cite{andriushchenko2023needweightdecaymodern}. It is known that the \softmax function is particularly prone to numerical stability problems although this often comes in the form of overflow in the exponential \citep{stability} and not from absorption errors in the sum as observed in this case. In the grokking setting, \cite{Nanda2023-hf} showed that the slingshots observed in \cite{slingshot-mechanism} can be explained by a very similar mechanism to the one involved in SC, although \cite{Nanda2023-hf} do not use it to explain any grokking phenomena beyond these spikes that sometimes appear in the training process in grokking tasks. We believe the slingshots observed in \cite{slingshot-mechanism} could be a mechanism to prevent full SC, explaining why slingshots can lead to grokking without weight decay in some settings. This is further discussed in \cref{app:slingshots}. Issues with numerical instability when training beyond overfitting with increasing learning rates were also observed in \cite{Lyu2019-sc}.

\begin{comment}
    

Emergent abilities~\citep{huang2024unified}
\citet{power2022grokking} 
modular addition~\cite{} and multiplication~\cite{doshi2024grokking}
\cite{wang2024grokked} find that transformers can learn implicit reasoning, but only through grokking.
\cite{fan2024deep} reveal that deep neural networks can be more susceptible to grokking than its shallower counterparts. This is supported by \cite{humayun2024deep} arguing that deep networks always grok. These findings make the extent of our study even more interesting for practical settings. 
phase transition from lazy to rich regime~\citep{kumar2023grokking,mohamadi2024you,Lyu2023-ga,rubin2023droplets}, learning local rules~\cite{vzunkovivc2024grokking}.
phase transition and emergence~\citep{clauw2024information}, formation of geometric arrangement of circuits in the input space~\cite{humayun2024grokking}
Group theoretic~\cite{stander2023grokking,chughtai2023toy}

compression and robustness~\cite{liu2023grokking,tan2023understanding}: weight norm (metric) of the neural network is actually a
sufficient condition for grokking

\end{comment}

