\section{Setup}
\subsection{Datasets}

We show our findings on the most commonly studied grokking datasets, outlined in this section.

\begin{comment}
    \begin{enumerate}[itemsep=1mm,topsep=0mm,leftmargin=*]
    \item[\textbf{I. Modular Arithmetic}] 
\end{enumerate}
\end{comment}


\paragraph{I. Modular arithmetic}
The main results in this paper are shown on arithmetic modulo 113 \citep{power2022grokking, Nanda2023-hf}. This is a family of supervised learning tasks where two one-hot encoded inputs representing integers $a,b < p$ are used to predict the target $y=a*b \mod p$, where $*$ is some binary operation and $p$ is a prime number. In most of our results, the binary operation is addition, but we show additional results with multiplication and subtraction. 

Modular arithmetic tasks are characterized by a binary operation and a dataset size, with different behaviours being observed for different dataset sizes on the same binary operation. In these settings, we describe the dataset sizes as the percentage of the $113^2$ possible pairs that are used for training, with the rest of the data being used for testing as in \cite{Nanda2023-hf} and \cite{power2022grokking}. Our main results use a 40\%/60\% train/test split but we also include results using 60\%/40\% and 70\%/30\%. The input integers are represented as one-hot vectors. 

\paragraph{II. Sparse parity}
We also validate some of our results on the Sparse Parity task outlined in \cite{Barak2022-el}. This is a supervised learning setting where the target is the parity of $k$ bits out of a binary vector of length $n$, with $k\ll n$. In this work we use 2000 samples, split evenly between train and test data and we describe instances of this task by specifying the values of $n$ and $k$.

\paragraph{III. MNIST}
Finally, we provide some results on a subset the classic image classification dataset MNIST \citep{deng2012mnist}. For our experiments, we use a subset of 200 training samples from the training set as in \cite{liu2023grokking}, with evaluation on the full test set.


\subsection{Models}
 We study the grokking phenomenon on these datasets using a 2-hidden layer multi-layer perceptron (MLP) of width 200 as in \cite{liu2023omnigrok} and a one-layer transformer with 4 attention heads as \cite{Nanda2023-hf} and \cite{power2022grokking}. We train both of these models in a full batch setting, using ReLU activations and cross-entropy loss with AdamW and SGD, as well as our own variants of these optimizers, $\perp$AdamW and $\perp$SGD. Unless specified otherwise we set the weight decay parameter $\lambda=0$. For modular arithmetic datasets, inputs are concatenated as the input of the MLP resulting in a 226 dimensional vector, and treated as separate tokens in the case of the transformer.
