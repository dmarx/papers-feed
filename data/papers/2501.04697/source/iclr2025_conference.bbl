\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreeva et~al.(2024)Andreeva, Dupuis, Sarkar, Birdal, and {\c{S}}im{\c{s}}ekli]{andreeva2024topological}
Rayna Andreeva, Benjamin Dupuis, Rik Sarkar, Tolga Birdal, and Umut {\c{S}}im{\c{s}}ekli.
\newblock Topological generalization bounds for discrete-time stochastic optimization algorithms.
\newblock In \emph{Adv. Neural Inf. Process. Syst.}, 2024.

\bibitem[Barak et~al.(2022)Barak, Edelman, Goel, Kakade, Malach, and Zhang]{Barak2022-el}
Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
\newblock Hidden progress in deep learning: Sgd learns parities near the computational limit.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Birdal et~al.(2021)Birdal, Lou, Guibas, and Simsekli]{birdal2021intrinsic}
Tolga Birdal, Aaron Lou, Leonidas~J Guibas, and Umut Simsekli.
\newblock Intrinsic dimension, persistent homology and generalization in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Chizat et~al.(2018)Chizat, Oyallon, and Bach]{Chizat_Oyallon_Bach_2018}
Lénaïc Chizat, Edouard Oyallon, and F.~Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in neural information processing systems}, pp.\  2933–2943, December 2018.
\newblock ISSN 1049-5258.

\bibitem[Chughtai et~al.(2023)Chughtai, Chan, and Nanda]{chughtai2023toy}
Bilal Chughtai, Lawrence Chan, and Neel Nanda.
\newblock A toy model of universality: Reverse engineering how networks learn group operations.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2023.

\bibitem[D'Angelo et~al.(2023)D'Angelo, Andriushchenko, Varre, and Flammarion]{andriushchenko2023needweightdecaymodern}
Francesco D'Angelo, Maksym Andriushchenko, Aditya Varre, and Nicolas Flammarion.
\newblock Why do we need weight decay in modern deep learning?
\newblock \emph{arXiv preprint arXiv:2310.04415}, 2023.

\bibitem[Deng et~al.(2018)Deng, Birdal, and Ilic]{deng2018ppfnet}
Haowen Deng, Tolga Birdal, and Slobodan Ilic.
\newblock Ppfnet: Global context aware local features for robust 3d point matching.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2018.

\bibitem[Deng(2012)]{deng2012mnist}
Li~Deng.
\newblock The mnist database of handwritten digit images for machine learning research.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0 141--142, 2012.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), \emph{Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  4171--4186. Association for Computational Linguistics, June 2019.

\bibitem[Dugas et~al.(2000)Dugas, Bengio, B\'{e}lisle, Nadeau, and Garcia]{softplus}
Charles Dugas, Yoshua Bengio, Fran\c{c}ois B\'{e}lisle, Claude Nadeau, and Ren\'{e} Garcia.
\newblock Incorporating second-order functional knowledge for better option pricing.
\newblock In T.~Leen, T.~Dietterich, and V.~Tresp (eds.), \emph{Advances in Neural Information Processing Systems}, volume~13. MIT Press, 2000.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Gromov(2023)]{Gromov2023-nh}
Andrey Gromov.
\newblock Grokking modular arithmetic.
\newblock \emph{arXiv preprint arXiv:2301.02679}, 2023.

\bibitem[Heo et~al.(2021)Heo, Chun, Oh, Han, Yun, Kim, Uh, and Ha]{adamp2020}
Byeongho Heo, Sanghyuk Chun, Seong~Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, and Jung-Woo Ha.
\newblock Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Humayun et~al.(2024)Humayun, Balestriero, and Baraniuk]{humayun2024deep}
Ahmed~Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk.
\newblock Deep networks always grok and here is why.
\newblock \emph{arXiv preprint arXiv:2402.15555}, 2024.

\bibitem[Ji \& Telgarsky(2018)Ji and Telgarsky]{ji20182}
Ziwei Ji and Matus Telgarsky.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{arXiv preprint arXiv:1803.07300}, 2018.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{ji2018gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{7th International Conference on Learning Representations, ICLR}, 2019.

\bibitem[Ji \& Telgarsky(2020)Ji and Telgarsky]{NEURIPS2020_c76e4b2f}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  17176--17186. Curran Associates, Inc., 2020.

\bibitem[Kloberdanz et~al.(2022)Kloberdanz, Kloberdanz, and Le]{stability}
Eliska Kloberdanz, Kyle~G Kloberdanz, and Wei Le.
\newblock Deepstability: A study of unstable numerical methods and their solutions in deep learning.
\newblock In \emph{Proceedings of the 44th International Conference on Software Engineering}, pp.\  586--597, 2022.

\bibitem[Kosson et~al.(2024)Kosson, Messmer, and Jaggi]{kosson2024rotational}
Atli Kosson, Bettina Messmer, and Martin Jaggi.
\newblock Rotational equilibrium: How weight decay balances learning across neural networks, 2024.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, volume~25, pp.\  1097--1105, 2012.

\bibitem[Kumar et~al.(2024)Kumar, Bordelon, Gershman, and Pehlevan]{Kumar2023-hz}
Tanishq Kumar, Blake Bordelon, Samuel~J. Gershman, and Cengiz Pehlevan.
\newblock Grokking as the transition from lazy to rich training dynamics.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Kunin et~al.(2023)Kunin, Yamamura, Ma, and Ganguli]{kunin2023asymmetricmaximummarginbias}
Daniel Kunin, Atsushi Yamamura, Chao Ma, and Surya Ganguli.
\newblock The asymmetric maximum margin bias of quasi-homogeneous neural networks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Michaud, and Tegmark]{liu2023omnigrok}
Ziming Liu, Eric~J Michaud, and Max Tegmark.
\newblock Omnigrok: Grokking beyond algorithmic data.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Zhong, and Tegmark]{liu2023grokking}
Ziming Liu, Ziqian Zhong, and Max Tegmark.
\newblock Grokking as simplification: A nonlinear complexity perspective.
\newblock In \emph{UniReps: the First Workshop on Unifying Representations in Neural Models}, 2023{\natexlab{b}}.

\bibitem[Lv et~al.(2024)Lv, Xie, Sun, Kang, and Yan]{lv2024language}
Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Rui Yan.
\newblock Language models" grok" to copy.
\newblock \emph{arXiv preprint arXiv:2409.09281}, 2024.

\bibitem[Lyu \& Li(2020)Lyu and Li]{Lyu2019-sc}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lyu et~al.(2024)Lyu, Jin, Li, Du, Lee, and Hu]{Lyu2023-ga}
Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon~Shaolei Du, Jason~D. Lee, and Wei Hu.
\newblock Dichotomy of early and late phase implicit biases can provably induce grokking.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Mallinar et~al.(2024)Mallinar, Beaglehole, Zhu, Radhakrishnan, Pandit, and Belkin]{mallinar2024emergence}
Neil Mallinar, Daniel Beaglehole, Libin Zhu, Adityanarayanan Radhakrishnan, Parthe Pandit, and Mikhail Belkin.
\newblock Emergence in non-neural models: grokking modular arithmetic via average gradient outer product.
\newblock \emph{arXiv preprint arXiv:2407.20199}, 2024.

\bibitem[Merrill et~al.(2020)Merrill, Ramanujan, Goldberg, Schwartz, and Smith]{homogeneous_transformers}
William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah~A. Smith.
\newblock Parameter norm growth during training of transformers.
\newblock \emph{CoRR}, abs/2010.09697, 2020.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and Steinhardt]{Nanda2023-hf}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Qi et~al.(2017)Qi, Su, Mo, and Guibas]{qi2017pointnet}
Charles~R Qi, Hao Su, Kaichun Mo, and Leonidas~J Guibas.
\newblock Pointnet: Deep learning on point sets for 3d classification and segmentation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  652--660, 2017.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rubin et~al.(2024)Rubin, Seroussi, and Ringel]{rubin2024grokking}
Noa Rubin, Inbar Seroussi, and Zohar Ringel.
\newblock Grokking as a first order phase transition in two layer networks.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0 (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Stander et~al.(2024)Stander, Yu, Fan, and Biderman]{stander2023grokking}
Dashiell Stander, Qinan Yu, Honglu Fan, and Stella Biderman.
\newblock Grokking group multiplication with cosets.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Thilak et~al.(2022)Thilak, Littwin, Zhai, Saremi, Paiss, and Susskind]{slingshot-mechanism}
Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and Josh Susskind.
\newblock The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon.
\newblock In \emph{NeurIPS Workshop}, 2022.

\bibitem[Varma et~al.(2023)Varma, Shah, Kenton, Kram{\'a}r, and Kumar]{Varma2023}
Vikrant Varma, Rohin Shah, Zachary Kenton, J{\'a}nos Kram{\'a}r, and Ramana Kumar.
\newblock Explaining grokking through circuit efficiency.
\newblock \emph{arXiv preprint arXiv:2309.02390}, 2023.

\bibitem[Wang et~al.(2024)Wang, Min, and Wu]{wang2024achieving}
Mingze Wang, Zeping Min, and Lei Wu.
\newblock Achieving margin maximization exponentially fast via progressive norm rescaling.
\newblock \emph{arXiv preprint arXiv:2311.14387}, 2024.

\bibitem[{\v{Z}}unkovi{\v{c}} \& Ilievski(2024){\v{Z}}unkovi{\v{c}} and Ilievski]{vzunkovivc2024grokking}
Bojan {\v{Z}}unkovi{\v{c}} and Enej Ilievski.
\newblock Grokking phase transitions in learning local rules with gradient descent.
\newblock \emph{Journal of Machine Learning Research}, 25\penalty0 (199):\penalty0 1--52, 2024.

\end{thebibliography}
