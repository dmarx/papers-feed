\begin{thebibliography}{28}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{GPT3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:1877--1901.

\bibitem[{Chalkidis et~al.(2022)Chalkidis, Jana, Hartung, Bommarito,
  Androutsopoulos, Katz, and Aletras}]{LexGLUE}
Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion
  Androutsopoulos, Daniel Katz, and Nikolaos Aletras. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.297} {{L}ex{GLUE}: A
  benchmark dataset for legal language understanding in {E}nglish}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 4310--4330,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}}]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}. 2022.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:16344--16359.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{BERT}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Gee et~al.(2022)Gee, Zugarini, Rigutini, and Torroni}]{FVT}
Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, and Paolo Torroni. 2022.
\newblock \href {https://aclanthology.org/2022.emnlp-industry.41} {Fast
  vocabulary transfer for language model compression}.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing: Industry Track}, pages 409--416, Abu Dhabi, UAE.
  Association for Computational Linguistics.

\bibitem[{Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan}]{Precision}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
  2015.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{International conference on machine learning}, pages
  1737--1746. PMLR.

\bibitem[{Gurulingappa et~al.(2012)Gurulingappa, Rajput, Roberts, Fluck,
  Hofmann-Apitius, and Toldo}]{ADE}
Harsha Gurulingappa, Abdul~Mateen Rajput, Angus Roberts, Juliane Fluck, Martin
  Hofmann-Apitius, and Luca Toldo. 2012.
\newblock \href {https://doi.org/https://doi.org/10.1016/j.jbi.2012.04.008}
  {Development of a benchmark corpus to support the automatic extraction of
  drug-related adverse effects from medical case reports}.
\newblock \emph{Journal of Biomedical Informatics}, 45(5):885 -- 892.
\newblock Text Mining and Natural Language Processing in Pharmacogenomics.

\bibitem[{Hinton et~al.(2015)Hinton, Vinyals, Dean et~al.}]{Distillation}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al. 2015.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2(7).

\bibitem[{Jiao et~al.(2020)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu}]{TinyBERT}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu. 2020.
\newblock {TinyBERT}: Distilling {BERT} for natural language understanding.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 4163--4174.

\bibitem[{Kudo and Richardson(2018)}]{SentencePiece}
Taku Kudo and John Richardson. 2018.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}.

\bibitem[{Kumar and Thawani(2022)}]{MBPE}
Dipesh Kumar and Avijit Thawani. 2022.
\newblock Bpe beyond word boundary: How not to use multi word expressions in
  neural machine translation.
\newblock In \emph{Proceedings of the Third Workshop on Insights from Negative
  Results in NLP}, pages 172--179.

\bibitem[{Lample et~al.(2018)Lample, Ott, Conneau, Denoyer, and
  Ranzato}]{Phrase}
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio
  Ranzato. 2018.
\newblock Phrase-based \& neural unsupervised machine translation.
\newblock \emph{arXiv preprint arXiv:1804.07755}.

\bibitem[{Michel et~al.(2019)Michel, Levy, and Neubig}]{16Heads}
Paul Michel, Omer Levy, and Graham Neubig. 2019.
\newblock Are sixteen heads really better than one?
\newblock \emph{Advances in neural information processing systems}, 32.

\bibitem[{Mu et~al.(2023)Mu, Li, and Goodman}]{mu2023learning}
Jesse Mu, Xiang~Lisa Li, and Noah Goodman. 2023.
\newblock Learning to compress prompts with gist tokens.
\newblock \emph{arXiv preprint arXiv:2304.08467}.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock \href {http://arxiv.org/abs/2303.08774} {Gpt-4 technical report}.

\bibitem[{Otani et~al.(2020)Otani, Ozaki, Zhao, Li, St~Johns, and Levin}]{MWE}
Naoki Otani, Satoru Ozaki, Xingyuan Zhao, Yucen Li, Micaelah St~Johns, and Lori
  Levin. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.360}
  {Pre-tokenization of multi-word expressions in cross-lingual word
  embeddings}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 4451--4464, Online. Association
  for Computational Linguistics.

\bibitem[{Petrov et~al.(2023)Petrov, La~Malfa, Torr, and
  Bibi}]{petrov2023language}
Aleksandar Petrov, Emanuele La~Malfa, Philip~HS Torr, and Adel Bibi. 2023.
\newblock Language model tokenizers introduce unfairness between languages.
\newblock \emph{arXiv preprint arXiv:2305.15425}.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf}]{DistilBERT}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
\newblock \href {https://doi.org/10.48550/ARXIV.1910.01108} {Distilbert, a
  distilled version of bert: smaller, faster, cheaper and lighter}.

\bibitem[{Schuster and Nakajima(2012)}]{WordPiece}
Mike Schuster and Kaisuke Nakajima. 2012.
\newblock Japanese and korean voice search.
\newblock In \emph{2012 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 5149--5152. IEEE.

\bibitem[{Sennrich et~al.(2016)Sennrich, Haddow, and Birch}]{BPE}
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1715--1725.

\bibitem[{Sharma et~al.(2019)Sharma, Li, and Wang}]{PATENT}
Eva Sharma, Chen Li, and Lu~Wang. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1212} {{BIGPATENT}: A
  large-scale dataset for abstractive and coherent summarization}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 2204--2213, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and
  Keutzer}]{QBERT}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W Mahoney, and Kurt Keutzer. 2020.
\newblock {Q-BERT}: Hessian based ultra low precision quantization of {BERT}.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8815--8821.

\bibitem[{Sun et~al.(2020)Sun, Yu, Song, Liu, Yang, and Zhou}]{MobileBERT}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
  2020.
\newblock {MobileBERT}: a compact task-agnostic {BERT} for resource-limited
  devices.
\newblock \emph{arXiv preprint arXiv:2004.02984}.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al. 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Tuggener et~al.(2020)Tuggener, von D{\"a}niken, Peetz, and
  Cieliebak}]{LEDGAR}
Don Tuggener, Pius von D{\"a}niken, Thomas Peetz, and Mark Cieliebak. 2020.
\newblock Ledgar: A large-scale multi-label corpus for text classification of
  legal provisions in contracts.
\newblock In \emph{Proceedings of the 12th Language Resources and Evaluation
  Conference}, pages 1235--1241.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{Transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Wang et~al.(2020)Wang, Wei, Dong, Bao, Yang, and Zhou}]{MiniLM}
Wenhui Wang, Furu Wei, Li~Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020.
\newblock Minilm: Deep self-attention distillation for task-agnostic
  compression of pre-trained transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:5776--5788.

\bibitem[{Zhu and Gupta(2017)}]{Prune}
Michael Zhu and Suyog Gupta. 2017.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{arXiv preprint arXiv:1710.01878}.

\end{thebibliography}
