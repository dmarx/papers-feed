\vspace{-0.6cm}
\begin{abstract}
\vspace{-0.2cm}
A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing of feature maps. This is done either by strided convolution (donwscaling) or transposed convolution (upscaling). Such operations are limited to a fixed filter moving at predetermined integer steps (strides). Spatial sizes of consecutive layers are related by integer scale factors, predetermined at architectural design, and remain fixed throughout training and inference time. We propose a generalization of the common Conv-layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of feature maps, to any size, dynamically and consistently across scales. 
Once trained, the CC layer can be used to output any scale/size chosen at inference time. The scale  can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference time, or gradual architectures where the size changes by a small factor at each layer. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. We further show that current Conv-layers suffer from inherent misalignments, which are ameliorated by CC layers.
%
%We provide visualizations for the continuous filters. 
%Finally, we propose a Single Image Super-Resolution method based on CC.
% Finally, we show how CC-layers can give rise to ``scale-adaptive'' Super-Resolution.


% A basic operation in Convolutional Neural Networks (CNNs) is the spatial resizing of a feature map. This is usually done either by strided convolutions (donwscaling) or transposed convolutions (upscaling). These operations are limited to a fixed filter that shifts at predetermined integer strides. Integer strided convolution induces various problems: (i) consecutive network layers can typically only be related by an integer scale factor. (ii) these layer-to-layer scales are predetermined in the architectural design, and remain fixed during training and inference time. (iii)  integer strides induce an inherent lack of shift equivariance in current CNNs.
% In this paper we propose a generalization of the common Convolution layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of network layers, to any layer size, dynamically and consistently across scales. The scale factor is determined at inference time, can be non-integer and differ between different axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes, determined at inference time, or finely-gradual architectures where the size changes by a small factor at each layer. This has implications both for image-processing tasks as well as high level vision tasks. We exemplify the advantages of CC in standard classification tasks and show how it induces shift-invariance. We further show that current conv-layers suffer from inherent misalignment, that is ameliorated by CC layers. We provide visualizations for the continuous filters. Finally, we propose a Single Image Super-Resolution method based on CC, that tackles a novel challenge; Scale-Adaptive Super-Resolution-- upscaling an image to any size or shape, determined at test-time, in a single feed-forward pass.

% A basic operation in Convolutional Neural Networks (CNNs) is learned spatial resizing of feature maps. This is done either by strided (donwscaling) or transposed (upscaling) convolutions. These operations are limited to a fixed filter that moves at predetermined integer steps. Consequently, scale-factors relating consecutive layers can only be integers. Furthermore, they are predetermined in the architectural design, and remain fixed during training and inference time.
% In this paper we propose a generalization of the common Convolution layer; Continuous Convolution (CC) Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of network layers, to any layer size, dynamically and consistently across scales. The scale factor is determined at inference time, can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference, or finely-gradual architectures where the size changes by a small factor at each layer. These abilities have implications both for image-processing tasks as well as high level vision tasks. We exemplify the advantages of CC in standard classification tasks and show how it induces shift-invariance. We further show that current Conv-layers suffer from inherent misalignment, that is ameliorated by CC layers. We provide visualizations for the continuous filters. Finally, we propose a Single Image Super-Resolution method based on CC, that tackles a novel challenge; Scale-Adaptive Super-Resolution-- upscaling an image to any size or shape, determined at test-time, in a single feed-forward pass.






% A basic operation in Convolutional Neural Networks (CNNs) is the strided convolution operation, which to-date has always been limited to a fixed filter that moves at predetermined integer steps. This integer-strided coevolution induces various problems: (i) consecutive network layers are typically related by an integer scale factor; (ii) these layer-to-layer scales are predetermined in the architectural design, and remain fixed during training and inference time; (iii)  integer strides induce an inherent lack of shift invariance in current CNNs.

% These issues affect the accuracy, generalization capabilities and the number of required training data  (or augmentations) of current CNNs.

% %

% In this paper we propose a generalization of the common Convolution layer, from discrete layers to Continuous Convolution (CC) Layers. CC layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel/sub-neuronal coordinates, rather than a set of integer coordinates. This allows learnable and principled resizing of network layers, dynamically and consistently, to any layer size. The layer resizing scale factor is determined at inference time, and can be non-integer and differ between different axes. CC gives rise to new freedoms for architectural design, such as gradual architectures where the size changes by a small factor at each layer. This has implications on the accuracy and generalization capabilities of CNNs. We exemplify the power of CC using a variety of Computer Vision tasks, from low-level Vision tasks (e.g., Super-Resolution) to high-level Vision tasks (e.g., Classification). We show how this enables true shift-invariance in classification networks. We further show that current conv-layers suffer from inherent misalignments, that are ameliorated by CC layers.

% We believe that CC will open the door to new Deep Learning capabilities, regardless of their field or task at hand.









% We propose a generalization for the common 2d Convolution layer;  a Continuous  Convolution Layer (CC). CC naturally extends Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates, rather than a set of discrete values. This allows learnable resizing, dynamically and consistently to any size. The scale is determined at inference time, and can be non-integer and differ between the axes. Using this new layer has implications both for learning dynamics and for computer-vision tasks. Due to these properties, CC gives rise to new freedoms for architectural design, such as gradual architectures were the size changes by a small factor at each layer. We show how this enables true shift-invarinance in classification networks. We further show that common conv-layers suffer from inherent misalignment that is ameliorated by CC. Finally we exploit this ability to applications such as scale-adaptive Super-Resolution and Blind Deconvolution.


\end{abstract}