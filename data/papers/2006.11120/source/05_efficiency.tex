\section{Efficient Implementation}
\vspace{-0.3cm}
\label{sec:efficiency}
This section provides major implementation details which give rise to efficient implementation of the CC-layer. Code will be made available.
% Other details and performance analysis can be found in the supplementary material. \michal{Will we have SuppMaterial?}
%We may need to, given that we already have an excess of 1 page (and the experiments section is not fully written yet...)  But if not, we should make sure to remove this sentence.}

\textbf{No need to keep the Neighbors distances:} The distances $\varbold{\mathcal{D}}$  of a grid point  \varbold{$\textbf{g}_n$}  to all its discrete input Neighbors can actually be predicted by keeping only one Neighbor. The reason is simple: all neighbors are at a sequence of positions 1 `pixel' apart from each other. 
It suffices to keep only the distance of a grid point to its \emph{closest} `pixel' center in order to predict all other distances. Moreover, the distance to each Neighbor is already implied by the sub-`pixel' grid coordinates. Hence, we can train the Internal-Net
$\varbold{\mathcal{K}_\theta}$  to predict the weights \emph{directly from the grid} \varbold{$\textbf{g}$}, thus saving on memory. 

% \textbf{Efficient implementation using discrete convolutions:} Eq.~\ref{eq:grid} 
% suggests that special types of grids exist. We have already noted that for $1/int$ scales, CC collapses to a standard convolution. However, the equation further suggests that for any  scale-factor which is a \emph{rational-number} ($s=k/l$) the grid is periodic, where the period is the numerator $k$. This is a generalization of previous observations made by~\cite{romano2016raisr} and~\cite{freedman2011image}. We take advantage of this property: When the scale-factor is a rational number, with a numerator not too small (to allow for scale generalization), but not too large (to allow for efficient convolutions), we can calculate the sets of weights for one period only. We do not need to explicitly extract neighbors. Instead we use a set of standard discrete convolution filters, each with a different starting shift matching a different grid position within one period. We apply them to the input image and interleave their results. This allows us to use the existing, highly optimized, implementation of conv operation, and also avoid calculation of a huge weights tensor $\mathcal{W}$.

\textbf{Efficient implementation using discrete convolutions:} Eq.~\ref{eq:grid} 
suggests that special types of grids exist. We already noted that for $1/int$ scales, CC collapses to a standard convolution. However, Eq.~\ref{eq:grid}  further suggests that for \emph{rational-numbers}  scale-factors ($s=\nicefrac{k}{\ell}$) the grid is periodic, with a period equals the numerator $k$. This generalizes observations made by~\cite{romano2016raisr,freedman2011image}. We take advantage of this property by calculating the sets of weights for one period only, to avoid calculation of a huge weights tensor $\mathcal{W}$. We do not need to explicitly extract neighbors and multiply them with the weights. Instead, we use a set of standard discrete convolution filters, each with a different starting shift matching a different grid position within one period. We apply them to the input feature-map and interleave their results. This allows us to use the existing, highly optimized, implementation of conv operation. This approach offers a trade-off between speed (small $k$) and better generalization (large $k$). \nivc{the unattentive reviewer might understand from this section that all we do is use a grid of convs instead of one conv. we need to somehow emphasize that this grid can principally change at each iteration, unlike ordinary conv-grids}

% To account for the tradeoff between scale-generalization and efficient convolutions, we take the following 2 precautions: (i)~we choose our scale-factors to be rational numbers with a numerator $\approx 19$ (an irreducible fraction, e.g., 19/27, 19/100, 19/10), thus guaranteeing enough sub-`pixel' distance diversity in the grid, and (ii)~we train the CC-layer with a variety of rational numbers, to further guarantee this diversity.
% To account for the tradeoff between scale-generalization and efficient convolutions, we choose our scale-factors to be rational numbers with a numerator $\approx 19$ (an irreducible fraction, e.g., 19/27, 19/100, 19/10), thus guaranteeing enough diversity of sub-`pixel' distances  in the grid. \ben{I think it should be denominator, not numerator}


\textbf{Saving memory by keeping only $\mathcal{K}_\theta$:} The most memory consuming tensor in CC is the weights tensor $\mathcal{W}$. The second one is the Neighbors tensor $\mathcal{N}$. We can save almost all the memory used by a CC-layer by simply removing these tensors at each iteration after the output is calculated. In the back-propagation step we simply recalculate them, \ben{at some extra cost of time. The memory footprint can be further reduced by chunking the input and computing the per-chunk results sequentially}.
% at a minor extra cost of time (less than 10\%). 
%\ben{this trick can be used in other scenarios} 
%We also conceptually find this as the right way to save the CC layer down the stream; a layer is distinguished by its continuous kernel model  (Internal-Net) $\mathcal{K}_\theta$, 
%and not by the final weights calculated. 