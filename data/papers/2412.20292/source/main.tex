
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[export]{adjustbox}

\usepackage{amsmath,amsthm}
\usepackage{float}
\usepackage{calc}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{changepage}
\usepackage{circuitikz}
\usepackage{enumitem}
% \usepackage{subcaption}
\usepackage{graphicx}
\usepackage{dblfloatfix}
\usepackage{pifont}
% \usepackage{subfigure}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



\newcommand{\ihat}{\hat{{\i}}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pp}[2]{\frac{\partial^2 #1}{\partial #2 ^2}}
\newcommand{\pq}[3]{\frac{\partial^2 #1}{\partial #2 \,\partial #3}}
\newcommand\sg[1]{{\color{red}[SG: #1]}}
\newcommand\mk[1]{{\color{blue}[MK: #1]}}
\newcommand{\xmark}{\ding{55}}%

\DeclareMathOperator*{\argmin}{arg\,min}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{An analytic theory of creativity in convolutional diffusion models}

\begin{document}

\twocolumn[
\icmltitle{An analytic theory of creativity in convolutional diffusion models}



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

% \icmlsetsymbol{equal}{*}



\begin{icmlauthorlist}
\icmlauthor{Mason Kamb}{yyy}
\icmlauthor{Surya Ganguli}{yyy}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Applied Physics, Stanford University, California, United States}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Mason Kamb}{kambm@stanford.edu}
\icmlcorrespondingauthor{Surya Ganguli}{sganguli@stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}



\vskip 0.3in
]


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


% \Floatbarrier

\begin{abstract}

We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data.  But optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in a fully analytic, completely mechanistically interpretable, equivariant local score (ELS) machine that, (3) without {\it any training} can  quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST).  Our ELS machine reveals a {\it locally consistent patch mosaic} model of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches in different image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \sim 0.75$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.  

\end{abstract}

\section{Introduction and related work}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure1a_m.png}
    \caption{Our analytic theory (left columns) can accurately predict on a {\it case by case basis} the outputs of convolutional diffusion models (right columns),  with U-Net or ResNet architectures trained on MNIST, CIFAR10 and FashionMNIST (left to right), even when these outputs are highly creative and far from the training data. See Fig.~\ref{fig:enter-label}, App.~\ref{app:empirics}, Fig.~\ref{fig:a1} and Table~\ref{tab:correlation_results}, and App.~\ref{appendix:samples}, Fig.~\ref{fig:resnet-mnist-zeros} to Fig.~\ref{fig:cifar10-circular-resnet} for many more successful theory-experiment comparisons.}
    \label{fig:figure1}
\end{figure}

A deep puzzle of generative AI lies in understanding how it produces seemingly endless, apparently creative, output.  What is the origin of this creativity, and how precisely is it generated from a finite training set? We answer these questions for convolutional diffusion models of images by deriving the first analytic and interpretable theory of their creativity that can accurately predict their outputs on a {\it case-by-case basis} (Fig.~\ref{fig:figure1}), and explain how they are created out of {\it locally consistent patch mosaics} of the training data. 

Denoising probabilistic diffusion models (DDPMs) were established in  \cite{sohl2015deep,ho2020denoising} and then unified with score-matching \cite{song2019generative,song2020score}. Denoising diffusion implicit models (DDIMs), an alternative deterministic parameterization which we primarily use in this paper, were established in \cite{song2020denoising}.  Diffusion models now play an important role not only in image generation 
\cite{dhariwal2021diffusion,rombach2022high, ramesh2022hierarchical}, but also video generation \cite{ho2022imagen, ho2022video, blattmann2023stable}, drug design \cite{alakhdar2024diffusion}, protein folding \cite{watson2023novo}, and text generation \cite{li2023diffusion, li2022diffusion}.

These models are trained to reverse a forward diffusion process that turns the finite training set distribution (a sum of $\delta$-functions over the training points) into an isotropic Gaussian noise distribution, through a time-dependent family of mixtures of Gaussians centered at shrinking data points. Diffusion models are trained to reverse this process by learning and following a score function that points in gradient directions of increasing probability. But therein lies the puzzle of creativity in diffusion models:  if the network can learn this {\it ideal} score function exactly, then they will implement a perfect reversal of the forward process; this, in turn, will {\it only} be able to turn Gaussian noise into memorized training examples.  Thus, the fundamental creativity of diffusion models {\it must} lie in their {\it failure} to achieve the very objective they are trained on: learning the ideal score function.  But how can they fail in intelligent ways that lead to many sensible new examples {\it far} from the training set?    

Several theoretical and empirical works study the properties of diffusion models. Some works study the sampling properties of these models under the assumption that they learn the ideal score function exactly \cite{biroli2024dynamical, de2022convergence} or up to some small bounded error \cite{benton2024nearly}. Others establish accuracy guarantees on learning the ideal score function under various assumptions on the data distribution, and the hypothesis class of functions \cite{lee2022convergence,chen2023score,oko2023diffusion,ventura2024manifolds,Cui2023-mr,Cui2023-am}.

As noted above, a key limitation of studying diffusion models under the assumption that they (almost) learn the ideal score function is that such models can only generate memorized training examples, at odds with the creativity of diffusion models in practice. For example, they can compose aspects of their training data in combinatorially many novel ways \cite{sclocchi2024phase,okawa2024compositional}. This observation has motivated studies of mechanisms behind generalization in diffusion models that underfit the score-matching objective \cite{kadkhodaie2023generalization,zhang2023emergence,wang2024diffusion}. Other works connect creativity in diffusion models to the breakdown of memorization in modern Hopfield networks \cite{ambrogioni2023search,hoover2023memory,pham2024memorization}. However, none of these works can quantitatively predict individual creative samples from a trained diffusion model on a case-by-case basis. 

To develop theory beyond the memorization regime, we focus on diffusion models with a fully-convolutional backbone, without the self-attention layers introduced in \cite{ho2020denoising}.  We identify two fundamental inductive biases that prevent such models from learning the ideal score-function:  \textit{translational equivariance}, due to parameter sharing in convolutional layers, and \textit{locality}, due to the model's finite receptive field size. Remarkably, we show these two simple biases are {\it sufficient} to quantitatively explain the creative outputs of convolutional diffusion models. 

Relatedly, \cite{kadkhodaie2023learning} also identified locality as a limiting constraint in CNN-based diffusion models, but did not attempt to predict their individual creative outputs. Finally, the results of our analysis exhibit some similarity to very early patch-based texture synthesis methods \cite{efros1999texture}. Our contributions and paper outline are:

\begin{enumerate}[itemsep=-1pt]
    \item We review why diffusion models that learn the ideal score function can only memorize (Sec.~\ref{sec:ideal_score}).
    \item We derive minimum mean squared error (MMSE) approximations to the ideal score function subject to locality, equivariance, and/or partially broken equivariance due to image boundaries.   Remarkably, we find simple analytic solutions in all cases (Sec.~\ref{sec:equivlocal}.)
    \item These solutions lead to a boundary-broken equivariant local score (ELS) machine, which constitutes a fully analytic, mechanistically interpretable theory that can transform noise into creative, structured images without the need for any explicit training process. (Sec.~\ref{sec:equivlocal}).
    \item We theoretically characterize samples generated by the ELS machine and show how it achieves {\it exponential} creativity through {\it locally consistent patch mosaics} composed of different local training set image patches at different locations in each novel sample (Sec.~\ref{sec:creativitytheory}). 
    \item We show our boundary-broken ELS machine is not only analytic and interpretable but also {\it predictive}: it can predict, on a case-by-case basis, the outputs of trained UNets and ResNets, achieving median theory-experiment agreements of $r^2 \sim 0.94, 0.91, 0.90$ on MNIST, FashionMNIST, and CIFAR10 for the best architecture on each dataset (Sec.~\ref{sec:theoryexp}).
    \item Our comparison between theory and experiment reveals that trained diffusion models exhibit a coarse-to-fine generation of spatial structure over time and use image boundaries to anchor image generation (Sec.~\ref{sec:theoryexp}).
    \item Our theory reproduces the notorious behavior of diffusion models to generate spatially inconsistent images at fine spatial scales (e.g. incorrect numbers of limbs) and explains its origin in terms of excessive locality at late times in the reverse generative process. (Sec.~\ref{sec:theoryexp}).
    \item We compare our purely local ELS machine theory to more powerful trained UNet architectures with non-local self-attention (SA) layers. Our local theory can still partially predict their non-local outputs (median $r^2$ of 0.75 on CIFAR10), but reveal an interesting role for attention in carving out semantically coherent objects from the ELS machine's local patch mosaics (Sec.~\ref{sec:attention}). 
\end{enumerate}

Overall our work illuminates the mechanism of creativity in convolutional diffusion models and forms a foundation for studying more powerful attention-enabled counterparts.




\section{The ideal score machine only memorizes}
\label{sec:ideal_score}

We first discuss why any diffusion model that learns the ideal score function on a finite dataset can only memorize.   

The key idea behind diffusion models is to reverse a stochastic forward diffusion process that iteratively converts the data distribution $\pi_0(\phi)$, where $\phi \in \mathbb R^N$ is any data point, into a sequence of distributions $\pi_t(\phi)$ over time $t$, such that the final distribution $\pi_T(\phi)$ at time $T$ is an isotropic Gaussian $\mathcal{N}(0,I)$. The forward diffusion process usually shrinks the data points toward the origin while adding Gaussian noise, so that when conditioning on any {\it individual} data point $\varphi \sim \pi_0$, the conditional probability $\pi_t(\phi | \varphi)$ becomes the Gaussian $\mathcal{N}(\phi| \sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t ) I)$. The noise schedule $\bar{\alpha}_t$ decreases from $1$ at $t=0$ to $0$ at $t=T$ so that
the mean $\sqrt{\bar{\alpha}_t} \varphi$ of $\pi_t(\phi | \varphi)$ shrinks over time, and its variance increases, until $\pi_t(\phi | \varphi) \sim \mathcal{N}(0,I)$ for all initial points $\varphi$.   

A simple time reversal of this forward process can be obtained by sampling $\phi_T \sim \mathcal{N}(0,I)$ and then flowing it backwards in time from $T$ to $0$ under the deterministic flow
\begin{equation}
    -\dot{\phi} _t= \gamma_t(\phi_t + s_t(\phi_t)),
    \label{eq:reverseflow}
\end{equation}
where $s_t(\phi) \equiv \nabla_\phi \log \pi_t(\phi)$ is the {\it score function} of the distribution $\pi_t(\phi)$ under the forward process and $\gamma_t$ depends on the entire noise schedule $\bar{\alpha_t}$ (see App.~\ref{app:math} for details).  The flow in \eqref{eq:reverseflow} induces a sequence of reverse distributions $\pi^R_t(\phi)$ that exactly reverse the forward process in the sense that $\pi^R_t(\phi) = \pi_t(\phi)$ for all $t\in[0,T]$. Intuitively, this reversal occurs because, for any finite dataset $\mathcal D$, $\pi_t(\phi)$ is a mixture of Gaussians centered at shrunken data points, 
\begin{align}
    \pi_t(\phi) &= \frac{1}{|\mathcal{D}|} \sum_{\varphi \in \mathcal{D}} \mathcal{N}(\phi|\sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t)I),
    \label{eq:gaussmix}
\end{align}
and the score $s_t(\phi)$ points uphill on this mixture.  Thus the second term in \eqref{eq:reverseflow} flows $\phi_t$, as $t$ decreases, towards shrunken data points, and the first term undoes the shrinking.  

Motivated by this theory, score-based diffusion models attempt to sample the data distribution $\pi_0(\phi)$ by forming an estimate $\hat s_t(\phi)$ of the score function $s_t(\phi)$, and then plugging this estimate and initial noise $\phi_T \sim \mathcal N(0,I)$ into the reverse flow in \eqref{eq:reverseflow} to obtain a sample $\phi_0$.  We consider what happens when the estimate matches the ideal score function so $\hat s_t(\phi)= s_t(\phi)$ on any finite dataset $\mathcal D$.  Then the score of the Gaussian mixture $\pi_t(\phi)$ in \eqref{eq:gaussmix}, is (App.~\ref{app:math}):
\begin{align}\label{eq:optimal_discrete_score}
    s_t(\phi) &= \frac{1}{1 - \bar{\alpha}_t}\sum_{\varphi \in \mathcal{D}} (\sqrt{\bar{\alpha}_t} \varphi - \phi) W_t(\varphi|\phi), \\
    \label{eq:posteriorprob}
    W_t(\varphi|\phi) &= \frac{\mathcal{N}(\phi| \sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t ) I)}{\sum_{\varphi' \in \mathcal{D}} \mathcal{N}(\phi| \sqrt{\bar{\alpha}_t} \varphi', (1 - \bar{\alpha}_t ) I)} .
\end{align}
When $s_t$ in \eqref{eq:optimal_discrete_score} 
is inserted into \eqref{eq:reverseflow}, 
each term in \eqref{eq:optimal_discrete_score} acts as a force 
that pulls the sample $\phi$ towards 
a shrunken data point $\sqrt{\bar{\alpha}}_t\varphi$ as $t$ decreases, 
weighted by the posterior probability $W_t(\varphi|\phi)$ that $\phi$ at time $t$ would have originated from the datapoint $\varphi$ at time $0$ under the forward diffusion. %\eqref{eq:posteriorprob}.  

The combined reverse dynamics in \eqref{eq:reverseflow}, \eqref{eq:optimal_discrete_score} and \eqref{eq:posteriorprob}, which we call the {\it ideal score machine}, has an appealing Bayesian guessing game interpretation: the current sample $\phi$ at time $t$ optimally guesses which data point $\varphi$ it originated from in the {\it forward process}, thereby forming the posterior belief distribution $W_t(\varphi|\phi)$, and then flows to each (shrunken version) of the data points, weighted by this belief. 

Importantly, since the reverse flow provably reverses the forward diffusion, $\pi_0^R$ equals the empirical data distribution $\pi_0$, which is a sum of delta functions on the training set. Thus, \textit{the ideal score machine memorizes.} The mechanism behind memorization can be explained by positive feedback instabilities in the reverse flow.  In particular, the closer the sample $\phi$ is to a shrunken version of a data point $\varphi$, the higher the belief $W_t(\varphi|\phi)$ that $\phi$ originated from $\varphi$, and the stronger the force term  $(\sqrt{\bar{\alpha}_t} \varphi - \phi) W_t(\varphi|\phi)$ in \eqref{eq:optimal_discrete_score} pulling $\phi$ even closer to the shrunken $\varphi$, which in turn raises the belief $W_t(\varphi|\phi)$ at earlier $t$.  This positive feedback between belief and force causes the posterior belief distribution $W_t(\varphi|\phi)$ to rapidly concentrate onto a {\it single} data point $\varphi$, and so $\phi_t$ flows to this same point $\varphi$ under the reverse flow (Fig.\ref{fig:els-machine-structure} a.).  

Thus, any diffusion model that learns the true score $s_t$ on a finite dataset $\mathcal D$ {\it must} memorize the training data and {\it cannot} creatively generate new samples {\it far} from the training data.  While we have explained this memorization phenomenon intuitively using the ideal score machine, it has been well established in prior work (e.g. \cite{biroli2024dynamical}).   


\section{Equivariant and local score machines}
\label{sec:equivlocal}

\begin{figure}[t]
\centering


\begin{subfigure}[t]{0.368\linewidth}
\captionsetup{justification=centering,singlelinecheck=off}
\includegraphics[width=\linewidth]{IS1.png}
\caption{IS Machine}
\label{fig:els-machine-structurea}
\end{subfigure} %\hspace{1mm} 
\begin{subfigure}[t]{0.2875\linewidth} 
\captionsetup{justification=centering,singlelinecheck=off}
\includegraphics[width=\linewidth]{LS1.png}
\caption{LS Machine}
\label{fig:els-machine-structureb}
\end{subfigure} %\hspace{1mm} 
\begin{subfigure}[t]{0.2875\linewidth}
\captionsetup{justification=centering,singlelinecheck=off}
\includegraphics[width=\linewidth]{ELS1.png}
\caption{ELS Machine}
\label{fig:els-machine-structurec}
\end{subfigure} 


    
    \caption{Ideal score-matching under various constraints. (a) In the IS machine, the {\it entire} image (bottom) reverse flows to a {\it single} training set image from the training set (top stack). (b,c) In both the LS and ELS machines, different local patches of the image flow to different local patches in the training set. In the LS machine this final training patch must be drawn from the {\it same} location (b), while in the ELS machine, it can be drawn from {\it any} location (c).}
    \label{fig:els-machine-structure}
\end{figure}

The failure of creativity in the ideal score machine means that it {\it cannot} be a good model of what realistic diffusion models do beyond the memorization regime. We therefore seek simple inductive biases that {\it prevent} learning the ideal score function $s_t$ in \eqref{eq:optimal_discrete_score} on a finite dataset $\mathcal D$. By identifying these inductive biases, we hope to obtain a new theory of what diffusion models do when they creatively generate new samples far from the training data. 

The key observation is that many diffusion models use convolutional neural networks (CNNs) to form an estimate $\hat s(\phi)$ of the score function. Such CNNs have two prominent inductive biases. The first is translational equivariance due to weight sharing: translating the input image will correspondingly translate the CNN outputs. More generally, networks can be equivariant to arbitrary symmetry groups (e.g. \cite{cohen2016group},  \cite{hoogeboom2022equivariant}). The second is locality: since convolutional filters have narrow support, typical outputs of a CNN depend on their inputs only through a small receptive field of neighboring input pixels.   We therefore seek an optimal estimate $\hat s(\phi)$ of the ideal score in \eqref{eq:optimal_discrete_score} subject to locality and equivariance constraints.  


We start with formal definitions of equivariance and locality. Let $M_t[\phi]$ denote a model score function that takes an input image $\phi$ and outputs an estimated score $\hat s_t(\phi) = M_t[\phi]$.
\begin{definition}
A model $M_t$ is defined to be $G$-equivariant with respect to the action of a group $G$ on data if for any $U \in G$, $M_t$ satisfies $M_t[U \phi] = U M_t[\phi]$.
\end{definition}
In our case of images, $G$ is the spatial translation group in two dimensions, $U\phi$ is a translated image, and $U M_t[\phi]$ is the translated score function. In other words, translating the input translates the outputs of an equivariant model in the same way. CNNs are translation equivariant if we impose periodic boundary conditions on the pixels, so that, for example, left translation of the leftmost pixels move them to the rightmost pixels (i.e. circular padding). However, the common practice of zero-padding images at their boundary breaks translation-equivariance; we extend our theory to this case in Sec.~\ref{sec:borders}.

We next turn to locality. For image data, let $x$ be a pixel location, $\phi(x) \in \mathbb{R}^C$ be the pixel value of image $\phi$ at location $x$ (where $C$ is the number of color channels) and let $M_t[\phi](x) \in \mathbb{R}^C$ denote the model score function evaluated at pixel location $x$, which informs how the pixel value $\phi(x)$ should move under the reverse flow.  Also, at each pixel location $x$, let $\Omega_x$ denote a local neighborhood of $x$ consisting of a subset of pixels near $x$, and let $\phi_{\Omega_x} \in \mathbb{R}^{|\Omega_x|\times C}$ be the restriction of pixel values of the entire image $\phi$ to the $|\Omega_x|$ pixels in the neighborhood $\Omega_x$. We define locality as:
\begin{definition}
$M_t[\phi]$ is defined to be $\Omega$-local if, for all images $\phi$ and all pixel locations $x$, $M_t[\phi](x)$ depends on $\phi$ only through $\phi_{\Omega_x}$, i.e. $M_t[\phi](x) = M_t[\phi_{\Omega_x}](x)$. 
\end{definition}
Thus if an $\Omega$-local model $M_t[\phi]$ is used in place of $s(t)$ in \eqref{eq:reverseflow}, the instantaneous reverse flow of any pixel value $\phi(x)$ at location $x$ and time $t$ will {\it not} depend on pixel values at any locations {\it outside} the local neighborhood $\Omega_x$; it depends {\it only} on the image {\it in} neighborhood $\Omega_x$.  In particular, two pixels at distant locations $x$ and $y$ with non-overlapping neighborhoods $\Omega_x$ and $\Omega_y$ will make completely independent decisions as to which directions to reverse flow; the portion of the image $\phi_{\Omega_y}$ in the neighborhood $\Omega_y$ of $y$, cannot instanteously affect the flow direction of the pixel value $\phi(x)$, and vice versa.  

Next, we consider the optimal minimum mean squared error (MMSE) approximation to the ideal score function $s_t(\phi)$ in \eqref{eq:optimal_discrete_score} under locality and/or equivariance constraints. We provide full derivations in App.~\ref{app:proofs}, but the final answers, which we state below, are simple and intuitive. 



\subsection{The equivariant score (ES) machine}

We first impose equivariance without locality. The MMSE equivariant approximation to $s(t)$ in \eqref{eq:optimal_discrete_score}-\eqref{eq:posteriorprob} is identical in form to the ideal score, except the dataset $\mathcal D$ is augmented to the orbit of $\mathcal D$ under the equivariance group $G$, which we denote by $G(\mathcal D)$.  For example, in our case of images, $G(\mathcal D)$ corresponds to all possible spatial translations of all images in $\mathcal D$. Explicitly, the MMSE equivariant score is given by (see App.~\ref{app:equivariance} for a proof)  
\begin{align}
    \label{eq:MMSE_equiv_score}
    M_t[\phi](x) &= \frac{1}{1 - \bar{\alpha}_t}\sum_{\varphi \in G(\mathcal{D})} ( \sqrt{\bar{\alpha}_t} \varphi(x) - \phi ) W_t(\varphi|\phi)\\
    W_t(\varphi|\phi) &= \frac{\mathcal{N}(\phi|\sqrt{\bar{\alpha}_t} \varphi,(1 - \bar{\alpha}_t)I)}{\sum_{\varphi' \in G(\mathcal{D})} \mathcal{N}(\phi|\sqrt{\bar{\alpha}_t} \varphi',(1 - \bar{\alpha}_t)I)}.
\end{align}
Replacing the ideal score $s(t)$ in \eqref{eq:reverseflow} with \eqref{eq:MMSE_equiv_score} yields the equivariant score (ES) machine. While the ideal score machine memorizes the training data (see Sec.~\ref{sec:ideal_score}), the ES machine on images achieves only limited creativity: it can only generate any translate of any training image. 

\subsection{The local score (LS) machine}

We next impose locality without equivariance. The MMSE $\Omega$-local approximation to $s(t)$ in \eqref{eq:optimal_discrete_score}-\eqref{eq:posteriorprob} is given by
\begin{align}
    \label{eq:MMSE_local_score}
    M_t[\phi](x) &= \sum_{\varphi \in \mathcal{D}} \frac{( \sqrt{\bar{\alpha}_t} \varphi(x)- \phi(x))}{1 - \bar{\alpha}_t}W_t(\varphi_{\Omega_x}|\phi_{\Omega_x}),\\
    \label{eq:localposteriorprob}
    W_t(\varphi_{\Omega_x}|\phi_{\Omega_x}) &= \frac{\mathcal{N}(\phi_{\Omega_x}|\sqrt{\bar{\alpha}_t} \varphi_{\Omega_x}, (1 - \bar{\alpha}_t)I)}{\sum_{\varphi'\in \mathcal{D}} \mathcal{N}(\phi_{\Omega_x}|{\sqrt{\alpha}_t} \varphi_{\Omega_x}', (1 - \bar{\alpha}_t)I)}.
\end{align}
Each term in the local $M_t[\phi](x)$ in \eqref{eq:MMSE_local_score} is identical to each term in $s(t)$ in \eqref{eq:optimal_discrete_score}, yielding a force pulling the pixel value $\phi(x)$ towards a shrunken training set pixel value $\sqrt{\bar{\alpha}_t} \varphi(x)$ as before, {\it except} for the important change that the global posterior belief $W_t(\varphi|\phi)$ in \eqref{eq:optimal_discrete_score}-\eqref{eq:posteriorprob}, that is the same for {\it all} pixels $x$, is now replaced with a local $x$-dependent belief $W_t(\varphi_{\Omega_x}|\phi_{\Omega_x})$ in \eqref{eq:MMSE_local_score}-\eqref{eq:localposteriorprob}. $W_t(\varphi_{\Omega_x}|\phi_{\Omega_x})$ is the posterior probability that a sample image $\phi$ under the forward process at time $t$ originated from a training image $\varphi$ at time $0$, conditioned on the only information the model $M_t[\phi](x)$ can depend on, namely the restriction $\phi_{\Omega_x}$ of the image $\phi$ to the local neighborhood $\Omega_x$ at location $x$. The closer the local image patch $\phi_{\Omega_x}$ is to the co-located training image patch $\varphi_{\Omega_x}$, the larger the posterior $W_t(\varphi_{\Omega_x}|\phi_{\Omega_x})$ in \eqref{eq:localposteriorprob}.  

Replacing the ideal score $s(t)$ in \eqref{eq:reverseflow} with \eqref{eq:MMSE_local_score} yields the local score (LS) machine. The LS machine can achieve significant combinatorial creativity by allowing local image neighborhoods $\phi_{\Omega_x}$ and $\phi_{\Omega_{x'}}$ of different pixels $x$ and $x'$ to reverse flow close to training image patches $\varphi_{\Omega_x}$ and $\varphi'_{\Omega_{x'}}$ from {\it different} training images $\varphi$ and $\varphi'$ (Fig.\ref{fig:els-machine-structure}b).  Indeed the same positive feedback between belief and force that holds for the IS machine at a global level (Sec.~\ref{sec:ideal_score}), also holds for the LS machine at a local level, causing the posterior beliefs $W_t(\varphi|\phi_{\Omega_x})$ of all pixels $x$ to concentrate on a unique training image, but this training image could be different for different far away pixels. This flow decoupling of local image patches in $\phi_t$ empowers exponential creativity.  

However, an important limitation remains in the LS machine: a local image patch $\phi_{\Omega_x}$ at pixel location $x$ {\it must} reverse flow close to some local training image patch $\varphi_{\Omega_x}$ drawn from the {\it same} location $x$; it cannot flow to a training image patch $\varphi_{\Omega_{x'}}$ drawn from a {\it different} location $x'$.  We next see that adding equivariance removes this limitation. 

\begin{figure*}[t]
    \centering
    
    \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth,valign=b]{wf1.png}
    \label{fig:exceptional_pointsa}
    \vspace{-1em}
    \caption{}
    \end{subfigure}\hspace{8mm} 
    \begin{subfigure}[b]{0.42\linewidth}
    \includegraphics[width=\linewidth,valign=b]{wf2.png}
    \label{fig:exceptional_pointsb}
    \vspace{-1em}
    \caption{}
    \end{subfigure}\hspace{8mm} 
    \begin{subfigure}[b]{0.2\linewidth}
    \includegraphics[width=\linewidth,valign=b]{wf3.png}
    \caption{}
    \label{fig:exceptional_pointsc}
    \end{subfigure}

        
    \vspace{-1em}
    \caption{Exponential creativity through locally consistent patch mosaics. (a) A training set of two images (all black or all white). (b) Creative samples from any local score machine (LS or ELS) with a $3\times3$ locality window and periodic boundary conditions. Local consistency in this special case means every generated pixel is either black or white, and the majority color of every generated $3\times3$ patch equals the color of its central pixel. (c) We note that samples are generated by numerically integrating the reverse flow in \eqref{eq:reverseflow}. If the step size in this integration is too large, one can generate invalid samples with a few cases of broken local consistency (highlighted red patches). In practice in trained diffusion models, this local consistency would only hold approximately.}
    \label{fig:exceptional_points} %\vspace{-1.5em}
\end{figure*}
\subsection{The equivariant local score (ELS) machine}

Further constraining the LS machine with equivariance leads to the ELS machine in which any local image patch at any pixel location $x$ can now flow towards any local training set image patch drawn from {\it any} location $x'$ not necessarily equal to $x$, as in the LS machine. This is the local analog of how the IS machine can only generate training set images, but the equivariance constrained ES machine can generate training set images globally translated to any other location.  

To formally express this result, assume all local neighborhoods $\Omega_x$ for different $x$ have the same shape $\Omega$.  For concreteness, one can think of $\Omega$ as a $P \times P$ square patch of pixels for $P$ odd, with $\Omega_x$ centered at location $x$.  Then let $P_{\Omega}(\mathcal{D})$ denote the set of all possible $\Omega$ shaped local training image patches drawn from any training image centered at any location.  An element $\varphi \in P_{\Omega}(\mathcal{D})$ now lives in $\mathbb{R}^{P \times P \times C}$ and denotes the pixel values of some local $\Omega$-shaped training image patch centered at some location. Now the optimal MMSE approximation to the ideal score in \eqref{eq:optimal_discrete_score}, under {\it both} equivariance and locality constraints is (App.~\ref{app:proofs}):     
\begin{align}
\label{eq:MMSE_equiv_translational}
    M_t[\phi](x) &= \sum_{\varphi \in P_\Omega(\mathcal{D})} \frac{(\sqrt{\bar{\alpha}_t} \varphi(0)-\phi(x))}{1 - \bar{\alpha}_t}W_t(\varphi|\phi, x)\\
    \label{eq:ELSposteriorprob}
    W_t(\varphi|\phi,x) &= \frac{\mathcal{N}(\phi_{\Omega_x}|\sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t)I)}{\sum_{\varphi' \in P_{\Omega}(\mathcal{D})}  \mathcal{N}(\phi_{\Omega_x}|\sqrt{\bar{\alpha}_t} \varphi', (1 - \bar{\alpha}_t) I)}.
\end{align}

We note that \eqref{eq:MMSE_equiv_translational}-\eqref{eq:ELSposteriorprob} for the ELS machine is identical to \eqref{eq:MMSE_local_score}-\eqref{eq:localposteriorprob} for the LS machine except that: (1) the sum over local training set patches in \eqref{eq:MMSE_equiv_translational}-\eqref{eq:ELSposteriorprob} in determining the flow $M_t[\phi](x)$ for pixel $\phi(x)$ is no longer restricted to training patches centered at the same location as $x$; and (2) each pixel $x$ must now track a larger posterior belief state $W_t(\varphi|\phi,x)$ in \eqref{eq:ELSposteriorprob} about which local training set patch at {\it any} location $x'$ was the origin of $\phi_{\Omega_x}$, as opposed to the smaller belief state $W_t(\varphi_{\Omega_x}|\phi_{\Omega_x})$ in \eqref{eq:localposteriorprob} about which local training set patch at the {\it same} location $x$ was the origin of $\phi_{\Omega_x}$.  In essence, in the Bayesian guessing game interpretation, equivariance removes each pixel's knowledge of its location $x$, so to guess the origin of its local image patch $\phi_{\Omega_x}$, it must guess both the training image {\it and} the location in the training image that it came from under the forward process. This guess then informs the reverse flow.  

Taken together, the ELS machine can creatively generate exponentially many novel images by mixing and matching local training set patches from any location and placing elements of them at any location in the generated image.  We call this a {\it patch mosaic model of creativity.} 




\subsection{Breaking equivariance through boundaries}\label{sec:borders}


Due to the common practice of zero padding images at boundaries, CNNs actually break exact translational equivariance.  We can modify our ELS machine to handle this broken equivariance (see App. \ref{app:addingborders} for details).  The key idea is that breaking translation equivariance restores to each pixel some knowledge of its location within the image.  For example, if the local image patch $\phi_{\Omega_x}$ around pixel location $x$ contains many $0$ values, then the pixel can use these to infer its location with respect to the boundary, and use this knowledge in the Bayesian guessing game that determines the reverse flow. In essence, with additional conditioning about its relation to the boundary, $\phi_{\Omega_x}$ should {\it only} flow to training image patches that are consistent with the observed amount and location of zero-padding. For example, interior, edge, and corner image patches only flow to interior, edge and corner training image patches with the same boundary overlap (Fig.~\ref{fig:border-regions}).  This is a partial case of complete equivariance breaking in the LS machine, in which pixels know their exact location $x$, and the local image patch $\phi_{\Omega_x}$ only flows to training image patches at the {\it same} location $x$ (Fig.\ref{fig:els-machine-structure}b).  





\section{A theory of creativity after convergence}
\label{sec:creativitytheory}

It is clear that the reverse flow from Gaussian noise $\phi_T$ to final sample $\phi_0$ in the ideal score machine converges to a single training set image.  But what do the LS, ELS or boundary broken ELS machines converge to at the end of the reverse process if they creatively generate novel samples {\it far} from the training data? We answer this question by proving a theorem that characterizes the converged samples $\phi=\phi_0$ at the end of the reverse process (App.~\ref{app:sample_dist}).

\begin{theorem} For the LS, ELS, and boundary broken ELS machines, assuming 
$\lim_{t \to 0} \phi_t$ 
and $\lim_{t \to 0} \partial_t\phi_t$ exist, 
then for every pixel $x$, $\phi_0(x) = \varphi(0)$ for the unique patch $\varphi \in P_{\Omega}^x(\mathcal{D})$ for which $\phi_{\Omega_x}$ is closer in $L_2$ distance (in 
$\mathbb{R}^{|\Omega_x|\times C}$)
than other local training set patch $\varphi' \in P_{\Omega}^x(\mathcal{D})$. 
\end{theorem}


Intuitively, samples generated from these machines are {\it locally consistent} in the sense that they obeying $3$ local conditions: (1) every pixel $x$ can be uniquely assigned to a local training set patch $\varphi$; (2) the pixel value $\phi_0(x)$ is {\it exactly} equal to the central pixel $\varphi(0)$ of $\varphi$; (3) the rest of the local generated patch $\phi_{\Omega x}$ resembles the local training patch $\varphi$ more than any other possible training patch.  This result characterizes the creative outcome of locally constrained machines as creating {\it locally consistent} patch mosaics where every pixel of every local patch in the sample matches the central pixel of the $L_2$ closest local patch in the training set.   

\subsection{The simplest example of patch mosaic creativity}

As the simplest possible example illustrating the locally consistent patch mosaic model of creativity for the LS and ELS machines, consider a training set of {\it only} two images: an all black and an all white image (Fig.\ref{fig:exceptional_points}a).  A highly expressive diffusion model trained only on these two images would only generate these two images.  However, an LS or ELS machine with local $3\times 3$ neighborhoods generates exponentially many new samples that are locally consistent patch mosaics (Fig.\ref{fig:exceptional_points}b): every pixel is either black or white, indicating it is assigned to either an all black or all white $3\times 3$ local training set patch.  And any $3\times3$ local patch of a generated sample with a central black (white) pixel is closer to the all black (white) training set patch than the other training set patch.  Thus local consistency in this special case reduces to the simple condition that the majority color of any $3\times3$ locally generated patch must equal the color of its central pixel.   The reader can check that this local consistency holds (with appropriate circular wraparound) at every pixel in Fig.\ref{fig:exceptional_points}b.



\section{Tests of the theory on trained models}
\label{sec:theoryexp}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.39\linewidth}
    % \captionsetup{justification=centering}
    \includegraphics[width=\linewidth,valign=b]{sjf_1.png}
    \label{fig:scales_jacobiana}
    \vspace{-1em}
    \caption{}
    \end{subfigure}\hspace{10mm} 
    \begin{subfigure}[b]{0.17\linewidth}
    % \captionsetup{justification=centering}
    \includegraphics[width=\linewidth,valign=b]{sjf2.png}
    \label{fig:scales_jacobianb}
    \vspace{-1em}
    \caption{}
    \end{subfigure}\hspace{10mm} 
    \begin{subfigure}[b]{0.16\linewidth}
    % \captionsetup{justification=centering}
    \includegraphics[width=\linewidth,valign=b]{multiscale.png}
    \vspace{-1em}
    \caption{}
    \label{fig:scales_jacobianc}
    \end{subfigure}
    \vspace{-1em}
    \caption{Coarse to fine progression of spatial locality in the reverse flow. (a) A heatmap of the average absolute value of the Jacobian from the output score $M_t[\phi_t](x=0)$ at the center pixel $x=0$ back to all input pixels $\phi(x')$ as a function of $x'$.  This receptive field shrinks from large to small as time progresses from early (large $t$) to late (small $t$) in the reverse flow. (b) Optimally calibrated values of the spatial locality scale $P$ of the ELS machine as a function of time $t$ (see App.~\ref{sec:multiscale} for details of calibration). (c) A schematic view of the time-dependent LS and ELS machines in which the locality neighborhood shrinks as the reverse time flows from top to bottom.}
    \label{fig:scales_jacobian} \vspace{-1em}
\end{figure*}

We next test our theory on two CNN-based architectures, a standard UNet \cite{ronneberger2015u} and a ResNet \cite{he2016deep} trained on $3$ datasets, MNIST, FashionMNIST, and CIFAR10 (see App.~\ref{sec:exp_details} for details of architectures and training).  We restrict our attention to these simple datasets because our theory is for CNN-based diffusion models only, and more complex diffusion models with attention and latent spaces are required to model more complex datasets.

\subsection{Coarse-to-fine time dependent spatial locality scales}\label{sec:tdepfield}

To compare our theory of ELS and LS machines with experiments, we must first choose a locality scale for the size of the $P\times P$ local patch.  We therefore measure it in the trained UNet and ResNet and find, importantly, that it changes from large  to small scales as time passes from early (large $t$) to late (small $t$) in the reverse flow (Fig. \ref{fig:scales_jacobian}a).  We therefore promote the spatial size of the $P \times P$ locality window in our ELS and LS machines to a dynamic variable which we calibrate to the UNet and ResNet (Fig. \ref{fig:scales_jacobian}bc).  See App.~\ref{sec:multiscale}.  

\subsection{Theory predicts trained outputs case-by-case}\label{sec:tdepfield}


We first compare the outputs of the scale-calibrated boundary broken-ELS machine to the outputs of the ResNet and the UNet on a case-by-case basis for the same initial noise samples $\phi_T$ to both the theory and the ResNet or UNet, and we find an excellent match  (Fig.~\ref{fig:enter-label}ab). Indeed we find  \textbf{a remarkable and uniform \textit{quantitative} agreement between the CNN outputs and ELS machine outputs.} For ResNets, we find median $r^2$ values between theory and experiment of 0.94 on MNIST, 0.90 on FashionMNIST, and 0.90 on CIFAR10. For UNets, we find median $r^2$ values of 0.84 on MNIST, 0.91 on FashionMNIST, and 0.82 on CIFAR10 (see Fig.~\ref{fig:a1} for the full distribution of $r^2$ values). To our knowledge, this is {\it the first time} an analytic theory has explained the creative outputs of a trained neural network generative model to this level of accuracy. Importantly, the ELS machine explains all trained outputs far better than the IS machine (Fig.~\ref{fig:a1} and Table~\ref{tab:correlation_results}). See App.~\ref{appendix:samples}, Fig.~\ref{fig:resnet-mnist-zeros} to Fig.~\ref{fig:cifar10-circular-resnet} for many more successful case-by-case theory-experiment comparisons for the $2$ nets and $3$ datasets. 


\subsection{Boundary driven anchoring of diffusion models}

We also trained circularly padded ResNets on MNIST and CIFAR10, and found a good match between the {\it non}-boundary broken ELS machine and experiment (Figs.~\ref{fig:a2}, \ref{fig:mnist-circular-resnet} and \ref{fig:cifar10-circular-resnet}). Interestingly, in both theory and experiment for MNIST, circular padding yields more texture-like outputs and less localized digit-like outputs, indicating the fundamental importance of boundaries in anchoring diffusion models, for MNIST at least (compare Fig.~\ref{fig:mnist-circular-resnet} and Fig.~\ref{fig:resnet-mnist-zeros}). 



\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\linewidth,valign=t]{mnist_zeros_samps_resnet_small.png} \\\\\\
    \includegraphics[width=\linewidth]{cifar10_zeros_samps_resnet_small.png}\\\\
    \includegraphics[width=\linewidth]{fashion_mnist_zeros_samps_resnet_small.png}
    \caption{Theory (left) vs. ResNet (right)}
    \end{subfigure}%~
        \hspace{6mm}
    \begin{subfigure}[t]{0.335\textwidth}    
    \includegraphics[width=\linewidth,valign=t]{mnist_zeros_samps_unet_small.png}\\\\\\
    \includegraphics[width=\linewidth]{cifar10_zeros_samps_unet_small.png}\\\\
    \includegraphics[width=\linewidth]
    {fashion_mnist_zeros_samps_unet_small.png}
    \caption{Theory (left) vs. UNet (right)}
    \end{subfigure}%~
        \hspace{10mm}
    \begin{subfigure}[t]{0.15\textwidth}
    \includegraphics[height=6.37cm,valign=t]{fashion_mnist_inconsistent2.png}
    \caption{Inconsistencies in FashionMNIST}
    \label{fig:inconsistencies}
    \end{subfigure} 
    \vspace{-1em}
    \caption{Match between theory and experiment. (a,b) Each pair of images shows a striking match between the output of the boundary broken ELS machine (left image in each pair) and the output of a trained CNN diffusion model (right image in each pair) when both models are given the same initial noise input.  We compare theory with $2$ architectures (ResNet in (a), and UNet in (b)) on $3$ datasets (MNIST, CIFAR10 and FashionMNIST from top to bottom). See App.~\ref{appendix:samples}, Fig.~\ref{fig:resnet-mnist-zeros} to Fig.~\ref{fig:cifar10-circular-resnet} for many comparisons and Fig.~\ref{fig:a1} and Table~\ref{tab:correlation_results} for quantitative $r^2$ values indicating high match between theory and experiment. (c) Trained CNN diffusion models (right) produce well-known spatial inconsistencies (e.g. 3 legged pants (row 1,4), 3 armed tops (row 3,6), bifurcated shoes (row 2,5)). Remarkably, the ELS theory (left) predicts this behavior and mechanistically explains it through excessive spatial locality at late times in the reverse flow.} \vspace{-1em}
    \label{fig:enter-label}
\end{figure*}

\subsection{Spatial inconsistencies from excess late-time locality}

Diffusion models notoriously generate spatially inconsistent images at fine spatial scales, e.g. incorrect numbers of fingers and limbs. Indeed, these inconsistencies are considered a tell-tale sign of AI-generated images \cite{bird2024cifake, shen2024rethinking, lin2024detecting}. Our trained models on FashionMNIST also generate such inconsistencies, e.g. pants with too many or too few legs, shoes with more than one toe, and shirts with an incorrect number of arms. Remarkably, our theory, since it matches trained model outputs on a case by case basis, {\it also} reproduces these inconsistencies (Fig.~\ref{fig:inconsistencies}). Since our theory is completely mechanistically interpretable, it provides a clear explanation for the origin of these inconsistencies in terms of excessive locality at late stages of the reverse flow. The late-time ($t < 0.3$) locality for all models is less than about $5$ pixels (Fig.~\ref{fig:scales_jacobian}b). When the locality scale is of this small order, different parts of the image more than a few pixels away must decide whether to develop into e.g. an arm or a pant leg without knowing the total number of limbs in the image; this process frequently results in incorrect numbers of total limbs. 

\section{The relation between theory and attention}\label{sec:attention}
\begin{figure*}[b]
    \centering
    \includegraphics[width=0.97\linewidth]{sa_fig_line3.png} \vspace{-1em}
    \caption{Comparison between UNet+SA outputs (top row) and ELS machine outputs (bottom row) for the same noise inputs. For this class of inputs, the UNet+SA appears to carve out more semantically coherent objects out of the closely related ELS patch mosaic.}
    \label{fig:sa_fig}
\end{figure*}

While the local theory explains the outputs of CNN-based diffusion models on a case by case basis with high accuracy, many diffusion models also include highly non-local self-attention (SA) layers. For example \cite{ho2020denoising}) added SA layers to a UNet (which we call a UNet-SA architecture). The non-locality of SA strongly violates the assumptions of our local theory.  This violation raises an important question: do the predictions of our local theory bear {\it any resemblance at all} to the non-local outputs of trained UNet+SA models?  

To address this question, we compare our existing ELS machine theory with the outputs of a publicly available UNet+SA model pretrained on CIFAR10.  \cite{VSehwag_minimal_diffusion}. Strikingly, our ELS model, with no modification whatsoever, predicts the UNet+SA outputs on a case-by-case basis with a median of $r^2 \sim 0.75$ on $100$ sample images. This is substantially higher than the median $r^2 \sim 0.47$ of an IS machine baseline on the same images (see Fig.~\ref{fig:a3} for the entire distribution of $r^2$ values).


Qualitatively, the outputs of the UNet+SA model fall into three rough classes in which the UNet+SA produces: (1) a semantically incoherent image which nevertheless strongly resemblances the prediction of the ELS machine (Fig.~\ref{fig:cifar10-att-incoherent}); (2) a semantically coherent image which has some quantitative correlation with, but little qualitative resemblance to, the ELS machine prediction (Fig.~\ref{fig:cifar10-att-nomatch}); and (3) a semantically coherent image that \textit{also} has a strong resemblance to the less semantically coherent ELS machine outputs (Fig.~\ref{fig:sa_fig}). 

This third class is the largest of the three and perhaps the most interesting. Qualitatively, the UNet+SA appears to carve a semantically coherent object out of the patch mosaic of the ELS machine (compare top and bottom rows of Fig.~\ref{fig:sa_fig}). For example, the UNet+SA often cuts out a foreground object from the ELS patch mosaic, while smoothing the background and accentuating it from the foreground object.

Fig.~\ref{fig:cifar10-zeros-attention} shows a large set of comparisons between the ELS machine and UNet+SA outputs.  While these results show that the ELS theory bears in many cases both quantitative and qualitative resemblance to the UNet+SA outputs, a full quantitative theory of the role of attention in the creativity of diffusion models remains for future investigation.  However, the correspondences in Fig.~\ref{fig:sa_fig}, Fig.~\ref{fig:cifar10-att-incoherent}, and Fig.~\ref{fig:cifar10-zeros-attention} and the ELS correlations (y-axis) in Fig.~\ref{fig:a3}, suggest the ELS theory provides an important foundation for this endeavor. 


\vspace{-1em}
\paragraph{Discussion.}
Developing a mechanistic understanding of how generative models convert their training data into new creative outputs {\it far} from their training data is an important goal in the field of neural network interpretability.  We have developed such an understanding for convolutional diffusion models of images that accurately predicts {\it individual} outputs on fixed random inputs in terms of the training data, for standard architectures (ResNets and UNets), standard datasets (MNIST, FashionMNIST, CIFAR10), and standard loss functions (score-matching). Moreover, our mechanistically interpretable theory of diffusion models, namely the boundary broken ELS machine, is derived not from intensive and highly detailed analysis of the inner workings of trained networks (modulo matching spatial scales), as in most mechanistic interpretability works, but rather from a first principles approach stemming from analytic solutions for the optimal score subject to {\it only} $2$ posited inductive biases: locality and equivariance. The strong quantitative agreement between theory and experiment on a case-by-case basis suggests that these two inductive biases are {\it sufficient} to explain the creativity of convolution-only diffusion models. We hope this work provides a foundation for understanding the creativity of more powerful attention-enabled diffusion models trained on more complex datasets. 




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

% \begin{appendices}
\section{Mathematical Preliminaries}\label{app:math}

\subsection{Notation conventions}
In what follows, we use the following notation:
\begin{itemize}
    \item $\mathcal{D}$ will represent the training set.
    \item $\varphi \in \mathbb{R}^N$ will represent an example from the training set. For images of size $L$ pixels by $L$ pixels by $C$ channels, we have $N=L\times L \times C$.  
    \item $\phi$ will represent any arbitrary image (or other data) that we are plugging into the score function/diffusion model. 
    \item $x$ represents a pixel location in an image.
    \item For image data, $\phi(x)$ and $\varphi(x)$ will represent the pixel values of the images $\phi$ and $\varphi$ at pixel location $x$; both are elements of $\mathbb{R}^C$. 
    \item $M[\phi]: \mathbb{R}^N \to \mathbb{R}^N$ represents a model that takes in an image $\phi$ and produces a new image (e.g. an estimate of the score function).  We will denote by $M[\phi](x) \in \mathbb{R}^C$ the value of the outputs of this model, given an input $\phi$, at the pixel location $x$.
    \item $\phi_{\Omega_x}$ and $\varphi_{\Omega_x}$ will represent the restriction of images $\phi$ and $\varphi$ to a neighborhood $\Omega_x$ around a pixel $x$. We usually take $\Omega_x$ to be a square patch of size $P \times P$, with $P$ odd, containing pixel $x$ at the center. In this case, $\phi_{\Omega_x}$ and $\varphi_{\Omega_x}$ are vectors in $\mathbb{R}^{P\times P\times C}$. However, the theoretical framework supports arbitrary assignments from $x \to \Omega_x$.
    \item For a square image patch $\varphi$ with an odd-dimension side length, the value $\varphi(0) \in \mathbb{R}^C$ indicates the pixel at the center of the patch.
    \item $P_{\Omega}(\mathcal{D})$ will denote the set of all $\Omega$-shaped patches drawn from elements of $\mathcal{D}$. 
    \item $\mathcal{N}(x|\mu, \Sigma)$ represents the PDF of the normal distribution with mean $\mu$ and covariance $\Sigma$.  We also use the short-hand $\mathcal{N}(\mu, \Sigma)$ when we do not need to refer to the name of a specific random variable. 
\end{itemize}


\subsection{Stochastic differential equations (SDEs) and Probability Flow}
In probabilistic modeling, we are often confronted with the problem of sampling from a data distribution whose exact form we do not have access to, or whose form makes direct sampling difficult. Diffusion models are an approach to sampling from such distributions by learning a time-inhomogenous differential equation that transports samples from a simple Gaussian distribution to the more complex distribution of interest.

More formally, consider a time-dependent (It) stochastic differential equation, given as follows:
\begin{align}\label{eq:forward_sde}
    d\phi_t &= f_t(\phi_t) \,dt + g_t\,dW_t.
\end{align}
Here $W_t$ is a standard Wiener process and $dW_t$ is its differential. We call this stochastic process the `forward' process. It starts from the data distribution $\pi_0(\phi)$ and induces a flow on probability distributions $\pi_t(\phi)$ for $t\geq 0$ described by associated Fokker-Planck equation:
\begin{align}\label{eq:forward_flow}
    \frac{\partial \pi_t(\phi)}{\partial t} = -\nabla \cdot (f_t(\phi) \pi_t(\phi)) + \frac{1}{2} \nabla^2(g_t^2 \pi_t(\phi)).
\end{align}
We will imagine that our forward process is constructed so that as $t \to \infty$ (or as $t \to T$ for some finite time $T$), $\pi_t$ converges to some tractable $\pi_{\infty}$, typically a Gaussian with finite variance. 

The idea underpinning diffusion models (or, more technically, DDIMs, the deterministic variant of diffusion models considered for the most part in this paper) is to look for a \textit{deterministic, time-dependent vector field} $v_t(\phi)$ that induces the same flow on distributions as (\ref{eq:forward_flow}). Then one can simply reverse this flow to sample from $\pi_0(t)$ by first sampling from the simple distribution $\phi_T \sim \pi_T$, then evolving the sample deterministically backwards in time from $t = T$ to $t = 0$ under the ODE
\begin{align}
    \frac{d \phi_t}{dt} &= v_t(\phi_t).
\end{align}
This ODE induces a flow on probability distributions $\pi_t(\phi)$ described by the advection equation
\begin{align}\label{eq:advection}
    \frac{\partial \pi_t}{\partial t} &= -\nabla \cdot [v_t(\phi) \pi_t(\phi)].
\end{align}
We want this advection process above to induce the \textit{same flow} on distributions as the original flow (\ref{eq:forward_flow}), when run in reverse starting, from the simple final distribution $\pi_{T}$. (This setup is closely related to `flow matching' models: see \cite{lipman2022flow} for a review). Interestingly, $v_t(\phi)$ can be easily identified by rewriting the flow in (\ref{eq:forward_flow}) as
\begin{align}\label{eq:forward2}
    \frac{\partial \pi_t(\phi)}{\partial t} &= -\nabla \cdot([f_t(\phi) - \frac{1}{2} g_t^2 \nabla \log \pi_t(\phi)]\pi_t(\phi)).
\end{align}
By matching (\ref{eq:advection}) and (\ref{eq:forward2}), we find 
\begin{align}
    v_t(\phi) &= f_t(\phi) - \frac{1}{2} g_t^2 \nabla \log \pi_t(\phi).
\end{align}
This vector field is sometimes known as the `probability flow.' The function
\begin{align}
    s_t(\phi) &= \nabla \log \pi_t(\phi)
\end{align}
is known as the \textit{score function}, and contains all of the complicated dependency on the initial distribution $\pi_0(\phi)$ that we would like to capture in our model.

\subsection{Diffusion models}

The most common choice of forward process (\ref{eq:forward_sde}) is an inhomogenous OrnsteinUhlenbeck (OU) process process of the following form:
\begin{align}
    d\phi_t &= -\gamma_t \phi_t + \sqrt{2 \gamma_t} dW_t
\end{align}
for which the probability flow is given by
\begin{align}
    v_t(\phi) &= -\gamma_t (\phi + \nabla \log \pi_t(\phi)).
\end{align}
The reason for this choice is that the finite-time marginals $\pi_t$ for this distribution can be sampled from tractably. We can generate samples $\phi_t \sim \pi_t$ by computing the following linear linear combination:
\begin{align}\label{eq:noise_interp}
    \phi_t &= \sqrt{\bar{\alpha}_t} \phi_0 + \sqrt{1 - \bar{\alpha}_t} \eta_t
\end{align}
with $\phi_0 \sim \pi_0$ a sample from the target distribution and $\eta_t \sim \mathcal{N}(0,I)$ a vector of isotropic Gaussian noise. The values of $\bar{\alpha}_t$ depend on the choice of $\gamma_t$ via the following formula:
\begin{align}
    \bar{\alpha}_t &= \exp(-2\int_0^t \gamma_t\,dt).
\end{align}
In practice, the values $\bar{\alpha}_t$ are typically chosen first and $\gamma_t$ is then specified implicitly by this choice. The choice of $\bar{\alpha}_t$ is known as the `noise schedule' for a diffusion model; typically, we choose $\bar{\alpha}_0 = 1$ (so that $t = 0$ corresponds to uncorrupted sample) and $\bar{\alpha}_T = 0$ for some large but finite value of $T$ (so that the entire reverse process can take place in finite time). At a distributional level, the solution of (\ref{eq:forward_flow}) for this process is given by
\begin{align}
    \pi_t(\phi) &= \int \pi_0(\phi_0) \mathcal{N}(\phi| \sqrt{\bar{\alpha}_t} \phi_0, (1 - \bar{\alpha_t}) I) \,d\phi_0.
\end{align}
The score function for $\pi_t$ can then be obtained analytically in terms of $\pi_0$:
\begin{equation}\label{eq:score_analytic}
\begin{split}
    s_t(\phi) &=  -\frac{1}{1 - \bar{\alpha}_t}\int \frac{\pi_0(\phi_0) \mathcal{N}(\phi | \sqrt{\bar{\alpha}_t} \phi_0 ,(1 - \bar{\alpha}_t) I)}{\pi_t(\phi)} (\phi_t - \sqrt{\bar{\alpha}_t} \phi_0)\,d\phi_0\\
    &= -\frac{1}{1 - \bar{\alpha}_t} \int \mathbb{P}(\phi_0|\phi_t=\phi) (\phi_t - \sqrt{\bar{\alpha}_t} \phi_0)\,d\phi_0.
\end{split}
\end{equation}
There is an extremely convenient fact about this particular score function that we can take advantage of in order to learn it from data. Given a particular sample $\phi_t$ generated by the forward noising process, the score function is proportional to the conditional expectation of the added noise $\eta_t$ from (\ref{eq:noise_interp}), given $\phi_t$:
\begin{align}\label{eq:score_noise_connection}
    s_t(\phi) &= -\frac{1}{\sqrt{1 - \bar{\alpha}_t}} \langle \eta_t | \phi_t=\phi\rangle.
\end{align}
This result is known as Tweedie's theorem. A standard result in statistics is that the conditional expectation $\langle \eta_t | \phi_t \rangle$ is the functional optimum of the following loss function:
\begin{align}\label{eq:loss_func}
    \mathcal{L}_t(f) &= \mathbb{E}_{\phi_0 \sim \pi_0,\eta_t \sim \mathcal{N}(0,I)}[\norm{f(\phi_t(\phi_0,\eta_t)) - \eta_t}^2] 
\end{align}
for $\phi_t$ defined in (\ref{eq:noise_interp}); the following slightly rescaled loss can be used if score-matching is preferred:
\begin{align}\label{eq:noise_score_match}
    \mathcal{L}_t(f) &= \mathbb{E}_{\phi_0 \sim \pi_0,\eta_t \sim \mathcal{N}(0,I)}[\norm{f(\phi_t(\phi_0,\eta_t)) + (1 - \bar{\alpha}_t)^{-1/2}\eta_t}^2]. 
\end{align}
In practice, we model the score using a single neural network $f_\theta(x,t)$ for all times $t \in [0,T]$, using the following objective:
\begin{align}
    L(\theta) = \mathbb{E}_{t \sim U(0,T), \phi_0 \sim \pi_0,\eta_t \sim \mathcal{N}(0,I) }[\norm{f_\theta(\phi_t(\phi_0,\eta_t),t) - \eta_t}^2].
\end{align}

\subsection{The empirical score function}
In practice, we never have direct access to the data distribution $\pi_0$ that we are attempting to sample from; we only have access to the  discrete empirical prior defined by a particular training set $\mathcal{D}$:
\begin{align}\label{eq:empirical_prior}
    \pi_0(\phi) &= \frac{1}{|\mathcal{D}|} \sum_{\varphi \in \mathcal{D}} \delta(\phi - \varphi).
\end{align}
At finite time $t$, the empirical distribution of noised training examples is simply a mixture of Gaussians centered at the (rescaled) training data points:
\begin{align}
    \pi_t(\phi) &= \frac{1}{|\mathcal{D}|} \sum_{\varphi \in \mathcal{D}} \mathcal{N}(\phi|\sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t)I).
\end{align}
The score function (\ref{eq:score_analytic}) for this distribution is then simply given by
\begin{align}\label{eq:optimal_discrete_score2}
    s_t(\phi) &= -\frac{1}{1 - \bar{\alpha}_t}\sum_{\varphi \in \mathcal{D}} (\phi - \sqrt{\bar{\alpha}_t} \varphi) W_t(\varphi|\phi), \\
    W_t(\varphi|\phi) &= \frac{\mathcal{N}(\phi| \sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t ) I)}{\sum_{\varphi' \in \mathcal{D}} \mathcal{N}(\phi| \sqrt{\bar{\alpha}_t} \varphi', (1 - \bar{\alpha}_t ) I)}.
\end{align}
Intuitively, this corresponds to computing the conditional average over the added noise, by averaging the proposed noise vectors $\eta_t \propto (\phi - \sqrt{\bar{\alpha}_t}\varphi)$ between our observed example $\phi$ and each training example $\varphi$, weighted by the probability $W(\varphi|\phi)$ of $\varphi$ being the training example that $\phi$ originated from. This probability is in turn computed essentially by Bayes theorem: the probability of starting from a training example $\varphi$, given the observed $\phi$, is given by the likelihood of generating the noise needed to go from $\varphi$ to $\phi$, divided by the likelihood of going from $\varphi'$ to $\phi$ for all possible training examples $\varphi'$. Appealingly, the weights $W(\varphi|\phi)$ are given by computing a simple soft-max over a simple quadratic loss function $-\frac{1}{2(1 - \bar{\alpha}_t)}\norm{\phi - \sqrt{\bar{\alpha}_t} \varphi}^2$ for every point in the training set.

It should be emphasized at this point that the ideal score function is \textit{not} representative of real diffusion models. Primarily: it always memorizes the training data. More importantly in practice, this memorization property becomes manifest \textit{very early} in the reverse process for high dimensional data, due to the typically large separation between training points in Euclidean space. This is a manifestation of the curse of dimensionality it would require an amount of data \textit{exponential in the dimension} to provide sufficiently good coverage of the underlying space for the ideal \textit{empirical} score function to well-approximate the \textit{true} ideal score function over all inputs over all times.

The failure of the ideal score function as a model for realistic diffusion models suggests that we should try to understand the particular manner in which they fail to optimally solve the task that they are trained on. In particular, we are motivated to look for the \textit{implicit and explicit biases and constraints} that prevent these models from learning the ideal score function, and then understand what they do instead under these limitations.





\section{Formalism}\label{app:proofs}

\subsection{Optimal local translationally equivariant score matching}
Fully translationally equivariant local models $M_t$ can be written in the following way:
\begin{align}\label{eq:translational_kernel}
    M_t[\phi](x) = f[\phi_{\Omega_x}],
\end{align}
where $\phi_{\Omega_x}$ is the restriction of $\phi$ to the neighborhood $\Omega_x$ around pixel $x$. In this section, we will use circular boundary conditions, so that if $x$ is near an image border, the neighborhood $\phi_{\Omega_x}$ includes the pixels on the opposite side of the image near the corresponding border (we will revisit this in the next section). This functional form reflects the locality constraint by making manifest that the output at a pixel location $x$ depends only on the patch $\phi_{\Omega_x}$ around it. Equivariance is reflected in the fact that the output of the model at every point $x$ is determined by the same function of the input patch. $f$ should be thought of as a function mapping $\mathbb{R}^{C \times |\Omega|} \to \mathbb{R}^C$, where $C$ is the number of channels in the image and $|\Omega|$ is the number of pixels in the local patch $\Omega$. The problem of identifying the optimal local/equivariant model can thus be framed as finding the $f$ that minimizes the score matching objective:
\begin{align}
    \mathcal{L} = \sum_x \mathbb{E}_{\phi \sim \pi_t}[\norm{f[\phi_{\Omega_x}] - s_t[\phi](x)}^2]
\end{align}
Writing this out concretely gives 
\begin{align}
    \mathcal{L} = \int \pi_t(\phi) \sum_{x} \norm{f(\phi_{\Omega_x}) - s_t[\phi](x)}^2 \,d\phi.
\end{align}
To find the functional optimum, we vary the objective with respect to $f(\Phi)$, with $\Phi$ any arbitrary patch, and set this variation to zero. This yields the condition
\begin{align}
    0 &= \sum_x \int \pi_t(\phi) (f(\phi_{\Omega_x}) - s_t[\phi](x)) \delta(\phi_{\Omega_x} - \Phi)\,d\phi.
\end{align}
We can rearrange this into the following form:
\begin{align*}
    f(\Phi) \sum_x \pi_t(\phi_{\Omega_x} = \Phi) &= \sum_x \int \delta(\phi_{\Omega_x} - \Phi)\,\pi_t(\phi) s_t[\phi](x) \,d\phi\\
    &= \sum_x \int  \delta(\phi_{\Omega_x} - \Phi) \nabla_{\phi(x)} \pi_t(\phi)\,d\phi\\
    &= \sum_x \nabla_{\Phi(0)} \pi_t(\phi_{\Omega_x} = \Phi)
    % &= \partial_{\phi(0)} \sum_x \pi_t(\phi_{\Omega_x} = \Phi)
\end{align*}
Here $\Phi(0) \in \mathbb{R}^C$ is the pixel value in the center of the patch $\Phi$. $\pi_t(\phi_{\Omega_x} = \Phi)$ indicates the marginal probability under the distribution $\pi_t$ that the patch $\phi_{\Omega_x}$ equals the target patch $\Phi$. The distribution $\sum_{x} \pi_t(\phi_{\Omega_x} = \Phi)$ is then proportional to the marginal distribution that a randomly-selected $\Omega$-shaped-patch in the image $\phi$ equals $\Phi$. Dividing through by this marginal, we obtain
\begin{align}\label{eq:equivariant_marginal_score}
    f(\Phi) = \nabla_{\Phi(0)} \log \sum_x \pi_t(\phi_{\Omega_x} = \Phi)
\end{align}
i.e. we find that $f(\Phi)$ is simply the score function of the modified marginal density $\sum_x \pi_t(\phi_{\Omega_x} = \Phi)$. Since $\pi_t(\phi)$ is a mixture of Gaussians, the marginal $\pi_t(\phi_{\Omega_x} = \Phi)$ can be obtained simply and is given by
\begin{align}
    \pi_t(\phi_{\Omega_x} =\Phi) = \sum_{\varphi \in \mathcal{D}} \mathcal{N}(\Phi|\sqrt{\bar{\alpha}}_t \varphi_{\Omega_x}, (1 - \bar{\alpha}_t) I).
\end{align}
Summing over $x$ gives us
\begin{align}
    \sum_x \pi_t(\phi_{\Omega_x} = \Phi) = \sum_{\varphi \in P_{\Omega}(\mathcal{D})} \mathcal{N}(\Phi | \sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t) I)
\end{align}
where $P_{\Omega}(\mathcal{D})$ is the set of all $\Omega$ patches in the training set $\mathcal{D}$. Finally, taking the derivative with respect to $\Phi(0)$ and substituting $\phi_{\Omega_x}$ for $\Phi$ gives us the final answer for $f[\phi_{\Omega_{x}}]$, which, when inserted into (\ref{eq:translational_kernel}), yields the final answer for $M_t$:
\begin{align}\label{eq:translational_equivariant_idscore}
    M_t[\phi](x) &= -\frac{1}{1 - \bar{\alpha}_t} \sum_{\varphi \in P_{\Omega}(\mathcal{D})} (\phi(x) - \sqrt{\bar{\alpha}_t}\varphi(0)) W(\varphi|\phi_{\Omega_x}) \\
    W(\varphi|\phi_{\Omega_x}) &= \frac{\mathcal{N}(\phi_{\Omega_x}|\sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t) I)}{\sum_{\varphi' \in P_{\Omega}(\mathcal{D})} \mathcal{N}(\phi_{\Omega_x} |\sqrt{\bar{\alpha}_t} \varphi', (1 - \bar{\alpha}_t) I)}.
\end{align}
We term the reverse diffusion model parameterized by $M_t$ i (\ref{eq:translational_equivariant_idscore}) the Equivariant Local Score (ELS) Machine.

This result has a simple intuitive interpretation. Firstly, it should be noted that the form of the resulting approximation to the score function strongly resembles the form of the true score function \eqref{eq:optimal_discrete_score2}. In that case, the score function computation could be framed as guessing the added noise by finding the necessary added noise for each possible training set element, computing the likelihood of generating that noise under a Gaussian noise model, and then averaging the possible noises over the entire training set weighted by the Bayesian posterior over each possible noised example.

The ELS machine (\ref{eq:translational_equivariant_idscore}) can be interpreted similarly. However, a very important distinction is that the \textit{Bayes weights are pixelwise-decoupled.} Under the exact computation of the score function, the Bayes weights are computed based on all available information in the image, and shared across every pixel; under the locality-constrained approximation, each pixel independently computes a separate set of Bayes weights for each training set element, based on its local receptive field. This decoupling of the belief states of different pixels means that under the reverse denoising process parameterized by (\ref{eq:translational_equivariant_idscore}), \textit{different pixels will be drawn towards different elements of the training set}. At scales below the locality scale the final denoised images should (roughly) resemble part of a training set image; however, at larger scales, the resulting images will not resemble any particular training set image, but rather a kind of patchwork quilt/mosaic of randomly combined training set images. We make this result more precise in (\ref{app:sample_dist}).

The role played by equivariance can likewise be interpreted very simply as removing each pixel's ability to locate itself within the image. Position is therefore promoted to a latent variable that must be integrated over, in addition to the training set element itself. This results in needing to compute a Bayes weight not only for each correspondingly-located patch in the training set, but \textit{every possible patch} in the training set.


\subsection{Adding borders}
\label{app:addingborders}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\linewidth]{patch_locations2.png}
    \caption{In the presence of zero-padded borders, different dictionaries of training set patches are used for the ELS machine computation depending on the contextual information provided by the visible border within the patch. For a central patch (left, red patch) without border information, training set patches (green patches) are sourced from the entire image interior. For edge patches (middle, red patch), training set patches (green patches) are sourced from everywhere along the edge at the same distance from the border. For corner patches (right, red patch), only patches from that exact location are used in the computation. }
    \label{fig:border-regions}
\end{figure}

There is an ambiguity about the behavior of a convolutional neural network for pixels near enough to the boundary of an image such that the network's receptive field extends past that boundary. One option in that situation is to enforce circular boundary conditions, so that the convolution operation `wraps around' to the other side upon encountering the boundary. This approach is not typically used in practice; more commonly, `zero padding' is introduced, wherein pixels outside of the image are treated as zeros for the purposes of the convolution operation.

In the presence of zero-padding, the results given above concerning the optimal local equivariant approximation to the score are nearly identical; in fact, the fundamental identity (\ref{eq:translational_kernel}) still holds. However, we must modify the interpretation of the visible patch $\phi_{\Omega_x}$ for a pixel $x$ near the border. Instead of considering the patch to include `wrapped around' portions of the image, we instead simply extend it with zeros in all locations where it extends past the border. 

When the ELS machine takes as input the patch $\phi_{\Omega_x}$, it computes the conditional probability that it corresponds to a noising of each particular patch in the training set. Formally, getting an exactly zero value at any pixel location occurs with probability zero. Thus, observing a patch $\phi_{\Omega_x}$ with zero-padding indicates with probability 1 that the patch is a corruption of a training set patch that came from a location inside the image consistent with the observed border information. We are thus able to write the ELS machine in the presence of a zero-padded boundary as
\begin{align}
    M_t[\phi](x) &= -\frac{1}{1 - \bar{\alpha}_t} \sum_{\varphi \in P_\Omega^x(\mathcal{D})} (\phi(x) - \sqrt{\bar{\alpha}_t}\varphi(0)) \frac{\mathcal{N}(\phi_{\Omega_x}|\sqrt{\bar{\alpha}_t} \varphi, (1 - \bar{\alpha}_t) I)}{\sum_{\varphi' \in P_\Omega^x(\mathcal{D})} \mathcal{N}(\phi_{\Omega_x} |\sqrt{\bar{\alpha}_t} \varphi', (1 - \bar{\alpha}_t) I)} .
\end{align}
The only modification to the ELS machine (\ref{eq:translational_equivariant_idscore}) is that we have replaced the set of all patches $P_{\Omega}(\mathcal{D})$ in the sum with the $x$-dependent patch dictionary $P_{\Omega}^x(\mathcal{D})$, corresponding to the collection of patches consistent with the border data at location $x$. These collections are illustrated in figure \ref{fig:border-regions}.


\subsection{Optimal equivariant score matching for a general symmetry group}\label{app:equivariance}

In many diffusion model applications outside of computer vision, equivariance under more general symmetry groups is built in to the architecture of the backbone model. For instance, molecular diffusion models are sometimes made equivariant under $E(3)$, the group of isometries on Euclidean space \cite{hoogeboom2022equivariant}. Diffusion transformers \cite{peebles2023scalable} are also naturally equivariant under the group of sequence permutations, although this equivariance is broken in a controlled way by the inclusion of positional embeddings. We are thus motivated to study the question of optimality under the constraint of equivariance under a general group of symmetries $G$, which we define as follows:
\begin{definition}
Let $G$ be a particular group of transformations acting on data $\phi$. We say that a model $M_t$ is $G$-equivariant if, for any $U \in G$, our model satisfies
\begin{align}
    M_t[U \phi] &= U M_t[\phi].
\end{align}\end{definition}
The result is given here:
\begin{theorem}
The optimal $G$-equivariant approximation to the empirical score function (\ref{eq:optimal_discrete_score}) under the score matching objective (\ref{eq:noise_score_match}) is given by the empirical score function for the dataset $G(\mathcal{D})$ consisting of the orbit of the dataset $\mathcal{D}$ under the group $G$.
\end{theorem}
\begin{proof}
Let $M_t$ be a $G$-equivariant model. For simplicity, we will assume that $M_t$ is being optimized with the following loss:
\begin{align}\label{eq:score_matching}
    L_t &= \mathbb{E}_{\phi \sim \pi_t}[\norm{M_t[\phi] - s_t(\phi)}^2]
\end{align}
where $s_t = \nabla_{\phi} \log \pi_t(\phi)$ is the ideal score function. First consider the orbit of a single point $\phi_0$ under the group $G$, given by $G[\phi_0] = \{ \phi : \exists U \in G: U \phi_0 = \phi \}$. For any $\phi \in G[\phi_0]$, there is an element $U\in G$ such that $U^{-1} \phi = \phi_0$, and thus the output of an equivariant model $M_t[\phi]$ is simply $U M_t[\phi_0]$. The problem of picking an optimal $M_t[\phi]$ for any $\phi \in G[\phi_0]$ can thus be reduced to a standard linear regression for $M_t[\phi_0]$, under the loss
\begin{align*}
    \tilde{L}_t &= \mathbb{E}_{\phi \sim \pi_t | \phi \in G(\phi_0)}[\norm{M_t[\phi] - \nabla \log \pi_t(\phi)}^2]\\
    &= \int_G \frac{\pi_{t}[U^{-1}\phi_0]}{\pi_t(G[\phi_0])} \norm{ M_t[\phi] - U \nabla \log \pi_t(U^{-1} \phi_0)}^2 \,dU
\end{align*}
where in the second line we have used the property of unitaries that $\norm{U x}^2 = \norm{x}^2$. Here $\pi_t(G[\phi_0])$ indicates the probability density assigned to the entire orbit $G[\phi_0]$ by $\pi_t$. We have used the orbit-stabilizer property to write the integral over the orbit as an integral over the entire group. Despite its complexity this formula represents a standard least-squares objective for $M_t[\phi]$, the minimizer of which is simply the weighted average of the target function $U \nabla \log \pi_t(U^{-1} \phi_0)$ weighted by $\frac{\pi_t[U^{-1}\phi_0]}{\pi_t(G[\phi_0])}$.
In other words, our optimal $G$-equivariant model is
\begin{equation}
\begin{split}
    M_t[\phi] &= \int_{U \in G} U\, \nabla \log \pi_t[U^{-1} \phi]\,\frac{\pi_t(U^{-1}\phi)}{\int_{V\in G} \pi_t(V^{-1} \phi)\,dV} dU.
\end{split}
\end{equation}
We can do some simple algebra to write this experssion in a more interpretable form:
\begin{align*}
    \frac{\int_{U \in G} U\, \nabla \log \pi_t[U^{-1} \phi]\pi_t(U^{-1}\phi)\,dU}{\int_{U\in G} \pi_t(U^{-1} \phi)\,dU} = \frac{\int_{U \in G} U\nabla \pi_t[U^{-1}\phi]\,dU }{\int_{U \in G} \pi_{t}(U^{-1} \phi)\,dU}= \nabla_\phi \log \int_{U \in G} \pi_t[U^{-1} \phi]\,dU
\end{align*}
where in the last step we have used the fact that $U^{-1} = U^\dagger$ and that $\nabla_\phi f(U^\dagger \phi) = U [\nabla f](U^\dagger \phi) $. We now note that
\begin{align}
    \int_{U \in G} \pi_t[U^{-1} \phi]\,dU &= \frac{1}{|\mathcal{D}|} \sum_{\varphi \in \mathcal{D}} \int_{U \in G} \mathcal{N}(U^{-1} \phi; \varphi \sqrt{\bar{\alpha}_t}, (1 - \bar{\alpha}_t) I) dU.
\end{align}
Since $U$ is unitary, it follows that
\begin{align*}
    \mathcal{N}(U^{-1} \phi| \varphi\sqrt{\bar{\alpha}_t}, (1 - \bar{\alpha}_t)I) &\propto \exp(-\frac{\norm{U^{-1}\phi - \sqrt{\bar{\alpha}_t}\varphi}^2}{2(1-\bar{\alpha}_t)})\\
    &= \exp(-\frac{\norm{\phi - \sqrt{\bar{\alpha}_t} U \varphi}^2}{2(1 - \bar{\alpha}_t)})
\end{align*}
and thus our optimal model is the score function for the empirical noise distribution of the $G$-augmented dataset, i.e.
\begin{align}
    \pi_t^G(\phi) &= \frac{1}{|\mathcal{D}|} \sum_{\varphi \in \mathcal{D}} \int_{G(\varphi)}  \mathcal{N}(\phi; \sqrt{\bar{\alpha}_t}\varphi', (1 - \bar{\alpha}_t)I)\, d\varphi'\\
    M_t[\phi] = \nabla \log \pi_t^G(\phi) &= -\frac{1}{1 - \bar{\alpha}_t}\frac{\sum_{\varphi \in \mathcal{D}} \int_{G(\varphi)} (\phi - \sqrt{\bar{\alpha}_t} \varphi') \mathcal{N}(\phi|\sqrt{\bar{\alpha}_t} \varphi', (1 - \bar{\alpha}_t)I) \,d\varphi'}{\sum_{\varphi\in \mathcal{D}} \int_{G(\varphi)} \mathcal{N}(\phi|\sqrt{\bar{\alpha}_t} \varphi', (1 - \bar{\alpha}_t)I) \,d\varphi'}  
\end{align}


\end{proof}

\subsection{The sample distribution at $t = 0$ under a local score approximation}\label{app:sample_dist}

When the score is learned optimally, the reverse process concentrates the sample distribution on the training dataset as $t \to 0$. It is instructive for us to ask what the analogous constraint on the generated samples is for the locality-constrained models that we consider in this paper. The answer is that the flow will concentrate the probability on certain `locally consistent points' $\tilde{\phi}$, defined as follows. Suppose we are employing an $\Omega$-local approximation $M_t$ to the score function, with each individual pixel $x$ using a (possibly identical) dictionary of patches $\varphi \in P_\Omega^x$. A `locally consistent point' $\tilde{\phi}$ is a point such that for every pixel location $x$, the value $\tilde{\phi}(x)$ is equal to the center pixel $\varphi(0)$ of the $l_2$-closest patch $\varphi \in P_{\Omega}^x$ to the patch $\tilde{\phi}_{\Omega_x}$, i.e. the patch that minimizes $\norm{\varphi - \tilde{\phi}_{\Omega_x}}^2$ over all patches in $P_{\Omega}^x$.

The reverse flow approximation parameterized by $M_t$ will concentrate on locally consistent points. We can formalize this effect in the following theorem:
\begin{theorem}
    Suppose we sample an initial point $\phi_T$ from the Gaussian $\pi_T$, and we evolve this density under the standard reverse process
    \begin{align}\label{eq:simple_reverse}
        \partial_t \phi_t &= -\gamma_t(\phi_t + M_t(\phi_t))
    \end{align}
    where
    \begin{align}
        \gamma_t &= -\frac{\partial_t \bar{\alpha}_t}{2\bar{\alpha}_t}.
    \end{align}
    Suppose also that the limits $\lim_{t \to 0} \phi_t$ and $\lim_{t \to 0} \partial_t\phi_t$ exist for an initial point $\phi_T$. Then the limit must be a locally consistent point.
\end{theorem}
\begin{proof}
    The assumption that $\lim_{t \to 0} \partial_{t} \phi_t$ exists entails that for any point $\phi_t$ on a particular trajectory, the values of $\phi_t$ and $ -\gamma_t (\phi_t + M_t(\phi_t))$ must stay bounded as $t \to 0$, which in turn entails that $\gamma_t M_t(\phi_t)$ must likewise stay bounded as $t \to 0$. This latter quantity is given at pixel location $x$ by
    \begin{align}
        \lim_{t \to 0} \gamma_t M_t[\phi](x) &= \lim_{t \to 0} -\frac{\partial_t \bar{\alpha}_t}{2\bar{\alpha}_t(1 - \bar{\alpha}_t)} \sum_{\varphi \in P^x_{\Omega}} (\phi_t(x) - \sqrt{\bar{\alpha}_t} \varphi(0)) W(\varphi|\phi,x)\\
        W(\varphi|\phi,x) &= \frac{\mathcal{N}(\phi_{\Omega_x}|\sqrt{\bar{\alpha}_t} \varphi,(1- \bar{\alpha}_t)I)}{\sum_{\varphi' \in P_{\Omega}^x} \mathcal{N}(\phi_{\Omega_x}|\sqrt{\bar{\alpha}_t} \varphi',(1- \bar{\alpha}_t)I)}.
    \end{align}
    The prefactor goes to $\infty$ as $t^{-1}$ as $t \to 0$, so it follows that for the derivative to have a finite limit, the right-hand factor must go to zero. As $\bar{\alpha}_t \to 0$, the weights take the limiting values
    \begin{align}
        \lim_{t \to 0} W(\varphi|\phi, x) = \begin{cases}
            1 & \varphi = \argmin_{\varphi' \in P_\Omega^x}\{ \norm{\phi_{\Omega_x} - \varphi'}^2 \}\\
            0 & \text{else}
        \end{cases}
    \end{align}
    and thus the limiting value of the sum is simply
    \begin{align}
        (\tilde{\phi}(x) - \tilde{\varphi}(0))
    \end{align}
    where $\tilde{\varphi} = \argmin_{\varphi' \in P_\Omega^x}\{ \norm{\phi_{\Omega_x} - \varphi'}^2 \}$. This value is zero only when $\tilde{\phi}(x) = \tilde{\varphi}(0)$. The condition that this holds for all $x$ is the definition of a locally consistent point.
\end{proof}


\section{Empirics}\label{app:empirics}

\subsection{Experimental details}\label{sec:exp_details}

To our ELS machine model of CNN-based diffusion, we examine two different architectures:
\begin{enumerate}%[itemsep=0]
    \item UNet: we use a standard UNet \cite{ronneberger2015u} with three scales with channel dimensions of $64,128,256$ respectively. We use residual connections in each UNet block. This model is formally local, but has a maximum receptive field size larger than the $32\times 32$ images we consider.
    \item ResNet: we use a minimal 8-layer convolutional neural network, with an upscaling and downscaling layer and 6 intermediate convolutional layers at a channel dimension of 256. Each layer is a single convolutional layer with a kernel size of $3 \times 3$ and with residual connections \cite{he2016deep} between layers. This model has a formal maximum receptive field size of $17 \times 17$.
\end{enumerate}
For all experiments, we train each model for 300 epochs with Adam, using an initial learning rate of 1e-4, a batchsize of 128, and an exponential learning rate schedule that halves the learning rate over the course of 50 epochs. We do not employ normalization layers in any of our models in order to avoid the possibility of information being exchanged nonlocally throughout the image. We do not use weight decay. We sample images using a 20-step discretization of the reverse process, using the analytic form prescribed for DDIM-style models in \cite{song2020denoising}. We use a cosine noise schedule \cite{nichol2021improved} for each experiment.

We evaluate each architecture using zero-padded convolutions on the following datasets: MNIST, FashionMNIST, and CIFAR10. For each of the latter datasets, we use class-conditioning. We only train class-unconditional models on MNIST, since we observe that with class-conditioning the models have a strong propensity to memorize. In addition, we evaluate our ResNet architecture using \textit{circularly-padded convolutions} on CIFAR10 (class-conditional) and MNIST (class-unconditional). 

For each neural network on each dataset, we calibrate an associated multiscale ELS model of the network using the procedure described in \ref{sec:multiscale}. The ELS model inherits the class-conditionality of the neural network it is modeling. We then compute the outputs of each neural network on 100 distinct random noise inputs for each dataset, drawn iid from an isotropic normalized Gaussian distribution. For class-conditional models, we additionally sample a label for each seed. We compute the outputs of the corresponding ELS machine on the same seeds/labels. For each example, we compute the pixelwise $r^2$ between the ELS machine output and the network output. We also compute the outputs of the ideal score-matching diffusion model across all inputs/labels, and compute the pixelwise $r^2$ between this baseline and the neural network outputs. We report the median $r^2$ value across the 100 samples for all configurations in table \ref{tab:correlation_results}, and plot the distribution of ELS correlations/IS correlations in figures \ref{fig:a1} and \ref{fig:a2}.

We also repeat this analysis procedure for a pretrained self-attention-enabled UNet trained on CIFAR10 from \cite{VSehwag_minimal_diffusion}, with the exception that we re-use the scales calibrated for the CIFAR10 ResNet model rather than re-estimating them for the Self-Attention-enabled model in order to minimize bias. 

\subsection{Identifying multiscale behavior}\label{sec:multiscale}

In order to correctly recapitulate the behavior of the models we study, we need to account for a crucial empirical observation: \textit{convolutional diffusion models exhibit time-dependent effective receptive field sizes}. This behavior is illustrated in figure \ref{fig:scales_jacobian}. In the left panel, we display an average absolute value of the  gradient of the center pixel of the model outputs from two of our CIFAR10-trained diffusion models, with respect to the input image. We plot this at various time steps in the reverse process (with the center pixel omitted for visual clarity). This visualization highlights which areas of the image the center pixel's outputs are sensitive to, an indicator of the degree of locality in the model's output. At $T = 1.0$ (corresponding to an input of initial white noise), the average gradient spans a large range (for the ResNet, a range clearly constrained by its maximal receptive field size). As the noise level reduces throughout the reverse process, the width of the heatmap decreases, until at the last time step the heatmap is almost entirely concentrated in a single ($3 \times 3$) square.

To calibrate the time-dependent locality scale of our theoretical model, we compute the reverse trajectories under the CNN-parameterized neural networks for a random validation set. At each time step, we compare the predictions of the model for the added noise and the outputs of ELS machines with a range of scales via cosine similarity. We then pick the representative scale for each time step by picking the median optimal scale across the range of samples. The resulting calibrated scales are shown in the middle panel of figure \ref{fig:scales_jacobian}. We see that the UNet is better described by a larger-scale ELS machine early in the reverse process than its ResNet counterparts, a phenomenon that can probably be linked to the more stringent locality constraints in the latter model. However, as the reverse process continues, both models prefer monotonically smaller scales, until converging to the smallest scale ($3 \times 3$) for the final few denoising steps. These results are in accordance with the visual evidence from the gradient heatmaps in figure \ref{fig:scales_jacobian}.

At this stage we have no a-priori method for predicting the scales that the models choose to use at each time step. However, the general phenomenon where the model initially starts with a large field of view and decreases it over the course of the reverse process could be anticipated on general grounds. As the noise variance decreases, the noised training distribution separates into a multimodal distribution with a larger and larger number of modes; as $t \to 0$, the number of modes converges to the (very large!) number of training examples. Since the models we consider are equivariant, an optimal model must also in principle represent not just the modes corresponding to the training set, but also to every translated augmentation, which for a $32 \times 32$ image results in a 1024-fold increase in effective dataset size! However, the emergence of multimodality is delayed when considering only the marginal distributions with respect to a smaller scale, as there are fewer dimensions via which two distinct data points could be distinguished from each other. This suggests that the model may somehow be picking the largest scale that it can a) represent within its receptive field (a constraint more pertinent to the ResNet, which has a smaller maximum receptive field size) and b) for which it can represent the local/equivariant approximation to the score function in a \textit{reasonably parameter efficient way}, i.e. for which it need not model too many independent modes of the data distribution. However, more work needs to be done in order to understand this phenomenon. 


\subsection{Quantitative Results}
% \FloatBarrier
\begin{table*}
    \centering
    \caption{A summary of the experimental results of the paper for different datasets and model configurations for each architecture across each dataset. Pixelwise $r^2$ between theory and neural network images are computed using 100 image samples per configuration; the median across the sample is reported. We compare these results to a baseline consisting of the correlations of the model with the outputs of an ideal score (IS) model, which always outputs memorized training examples. We also report the percentage of samples on which the ELS machine outperforms the output from the ideal score-matching diffusion model. }
    \vspace{2mm}

    \begin{tabular}{c c c c|c c  c}
        \toprule
         Dataset & Arch. & Padding & Conditional  & ELS Corr. & IS Corr.  & ELS $>$ IS \% \\
         \hline
         MNIST & UNet & Zeros & \xmark & \textbf{0.84}  & 0.69 & 0.92 \\
        CIFAR10 & UNet & Zeros & \checkmark & \textbf{0.82} & 0.39 & 0.99 \\
         FashionMNIST & UNet & Zeros & \checkmark  & \textbf{0.91} & 0.80 &  0.90 \\
        \midrule
        MNIST & ResNet & Zeros & \xmark &  \textbf{0.94} & 0.62 &  0.97 \\
        MNIST & ResNet & Circular & \xmark &  \textbf{0.77} & 0.16 &  0.96 \\
        CIFAR10 & ResNet & Zeros & \checkmark  & \textbf{0.90} & 0.43 &  1.00 \\ 
        CIFAR10 & ResNet & Circular & \checkmark  & \textbf{0.90} & 0.36 &  1.00 \\ 
         FashionMNIST & ResNet & Zeros & \checkmark &  \textbf{0.90} & 0.74 &  0.97 \\
        \midrule
        CIFAR10 & UNet + SA & Zeros & \checkmark & \textbf{0.75} & 0.47 &  0.92 \\
        \bottomrule
    \end{tabular}


    \label{tab:correlation_results}
\end{table*}



% \FloatBarrier


\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{scatter_mnist_zeros_corrs_resnet_m.png}
    \includegraphics[width=0.4\linewidth]{scatter_mnist_zeros_corrs_unet_m.png}\\
    \includegraphics[width=0.4\linewidth]{scatter_fashion_mnist_zeros_corrs_resnet_m.png}
    \includegraphics[width=0.4\linewidth]{scatter_fashion_mnist_zeros_corrs_unet_m.png}\\
    \includegraphics[width=0.4\linewidth]{scatter_cifar10_zeros_corrs_resnet_m.png}
    \includegraphics[width=0.4\linewidth]{scatter_cifar10_zeros_corrs_unet_m.png}\\

    \caption{Correlations between model outputs and ELS machine/IS baseline on each dataset for zero-padded models. Y axis is ELS machine $r^2$, X axis is IS baseline $r^2$ for each data point in the sample. The ELS machine uniformly outperforms the baseline, with stronger performance for the ResNet on CIFAR10 and MNIST and a stronger performance for the UNet on FashionMNIST.}
    \label{fig:a1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{scatter_mnist_circular_corrs_resnet_m.png}
    \includegraphics[width=0.4\linewidth]{scatter_cifar10_circular_corrs_resnet_m.png}

    \caption{Correlations between model outputs and ELS machine/IS baseline on each dataset for circularly-padded models. Y axis is ELS machine $r^2$, X axis is IS baseline $r^2$ for each data point in the sample. The ELS machine uniformly outperforms the baseline. The performance of the ELS machine on circular MNIST is anomalously lower than other configurations, but the degree of outperformance of the ideal score baseline is higher.}
    \label{fig:a2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{scatter_cifar10_zeros_corrs_attention_m.png}

    \caption{Correlations between model outputs and ELS machine/IS baseline on CIFAR10 for a pretrained Attention-enabled UNet. Y axis is ELS machine $r^2$, X axis is IS baseline $r^2$ for each data point in the sample.}
    \label{fig:a3}
\end{figure}


% \FloatBarrier



\newpage

\section{Samples}\label{appendix:samples}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{mnist_zeros_samps_resnet_m.png}
    \caption{Further comparison between ResNet (right columns) and ELS Machine (left columns) samples for MNIST. Model is unconditional and has zero padding.}
    \label{fig:resnet-mnist-zeros}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{mnist_zeros_samps_unet_m.png}
    \caption{Further comparison between UNet (right columns) and ELS machine (left columns) samples for MNIST. Model is unconditional and has zero padding.}
    \label{fig:unet-mnist-zeros}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fashion_mnist_zeros_samps_resnet_m.png}
    \caption{Further comparison between ResNet (right columns) and ELS Machine (left columns) samples for FashionMNIST. Model is class conditional and has zero padding.}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fashion_mnist_zeros_samps_unet_m.png}
    \caption{Further comparison between UNet (right columns) and ELS machine (left columns) samples for FashionMNIST. Model is class conditional and has zero padding.}
    \label{fig:fashion_mnist-zeros}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{cifar10_zeros_samps_resnet_m.png}
    \caption{Further comparison between ResNet (right columns) and ELS machine (left columns) samples for CIFAR10. Model is class conditional and uses zero padding.}
    \label{fig:cifar10-zeros-resnet}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{cifar10_zeros_samps_unet_m.png}
    \caption{Further comparison between UNet (right columns) and ELS machine (left columns). Model is class conditional and has zero padding.}
    \label{fig:cifar10-zeros}
\end{figure}

% \FloatBarrier

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{mnist_circular_samps_resnet_m.png}
    \caption{Further comparison between ResNet (right columns) and ELS machine (left columns) on MNIST. Model is unconditional and has circular padding.}
    \label{fig:mnist-circular-resnet}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{cifar10_circular_samps_resnet_multiscale.png}
    \caption{Further comparison between ResNet (right columns) and ELS machine (left columns) samples on CIFAR10. Model is class conditional and has circular padding.}
    \label{fig:cifar10-circular-resnet}
\end{figure}


\begin{figure}
    \centering
    \captionsetup{belowskip=5pt}

    \begin{subfigure}{0.42\textwidth}
        \caption{Incoherent output}
        \includegraphics[width=\linewidth]{cifar10_zeros_samps_attention_incoherent.png}
        \label{fig:cifar10-att-incoherent}
    \end{subfigure}%
    \hspace{10mm}%
    \begin{subfigure}{0.42\textwidth}
        \caption{Non-matching output}
        \includegraphics[width=\linewidth]{cifar10_zeros_samps_attention_nomatch.png}
        \label{fig:cifar10-att-nomatch}
    \end{subfigure}

    \caption{ELS Machine (left) vs. attention-enabled UNet (right) pairs. Panel (a) shows outputs where the Attentive UNet produces semantically incoherent, uninterpretable outputs, which tend to match strongly with the ELS Machine outputs. Panel (b) shows examples where the Attentive UNet produces samples not obviously matched with the ELS machine.}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{cifar10_zeros_samps_attention_multiscale.png}
    \caption{Further comparison between attention-enabled UNet (right columns) and ELS machine (left columns) samples on CIFAR10. Model is class conditional and has zero padding.}
    \label{fig:cifar10-zeros-attention}
\end{figure}





% \end{appendices}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
