\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al-Rfou et~al.(2018)Al-Rfou, Choe, Constant, Guo, and
  Jones]{chartransformer}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.
\newblock Character-level language modeling with deeper self-attention.
\newblock \emph{CoRR}, abs/1808.04444, 2018.
\newblock URL \url{http://arxiv.org/abs/1808.04444}.

\bibitem[Andoni et~al.(2015)Andoni, Indyk, Laarhoven, Razenshteyn, and
  Schmidt]{andoni2015angularLSH}
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya~P. Razenshteyn, and Ludwig
  Schmidt.
\newblock Practical and optimal {LSH} for angular distance.
\newblock \emph{CoRR}, abs/1509.02897, 2015.
\newblock URL \url{http://arxiv.org/abs/1509.02897}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{layernorm2016}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.
\newblock URL \url{http://arxiv.org/abs/1607.06450}.

\bibitem[Bordes et~al.(2015)Bordes, Usunier, Chopra, and
  Weston]{large_mem_nets}
Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston.
\newblock Large-scale simple question answering with memory networks.
\newblock \emph{CoRR}, abs/1506.02075, 2015.
\newblock URL \url{http://arxiv.org/abs/1506.02075}.

\bibitem[Chandar et~al.(2016)Chandar, Ahn, Larochelle, Vincent, Tesauro, and
  Bengio]{hier_mem_nets}
Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro,
  and Yoshua Bengio.
\newblock Hierarchical memory networks.
\newblock \emph{arXiv preprint arXiv:1605.07427}, 2016.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019sparsetransformer}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{URL https://openai.com/blog/sparse-transformers}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018BERT}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{CoRR}, abs/1810.04805, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.04805}.

\bibitem[Gomez et~al.(2017)Gomez, Ren, Urtasun, and
  Grosse]{gomez2017reversible}
Aidan~N Gomez, Mengye Ren, Raquel Urtasun, and Roger~B Grosse.
\newblock The reversible residual network: Backpropagation without storing
  activations.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2214--2224, 2017.

\bibitem[Hill et~al.(2015)Hill, Bordes, Chopra, and Weston]{goldilocks}
Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston.
\newblock The goldilocks principle: Reading children's books with explicit
  memory representations.
\newblock \emph{CoRR}, abs/1511.02301, 2015.
\newblock URL \url{http://arxiv.org/abs/1511.02301}.

\bibitem[Huang et~al.(2018)Huang, Vaswani, Uszkoreit, Shazeer, Hawthorne, Dai,
  Hoffman, and Eck]{huang2018music}
Cheng-Zhi~Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis
  Hawthorne, Andrew~M Dai, Matthew~D Hoffman, and Douglas Eck.
\newblock Music transformer: Generating music with long-term structure.
\newblock \emph{arXiv preprint arXiv:1809.04281}, 2018.

\bibitem[Lample et~al.(2019)Lample, Sablayrolles, Ranzato, Denoyer, and
  J{\'{e}}gou]{lample2019productkeys}
Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic
  Denoyer, and Herv{\'{e}} J{\'{e}}gou.
\newblock Large memory layers with product keys.
\newblock \emph{CoRR}, abs/1907.05242, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.05242}.

\bibitem[Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer]{wikipedia}
Peter~J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam Shazeer.
\newblock Generating wikipedia by summarizing long sequences.
\newblock \emph{CoRR}, abs/1801.10198, 2018.
\newblock URL \url{http://arxiv.org/abs/1801.10198}.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli.
\newblock Scaling neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pp.\  1--9, Brussels, Belgium, October 2018. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-6301}.
\newblock URL \url{https://www.aclweb.org/anthology/W18-6301}.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, and
  Ku]{parmar2018imagetransformer}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, and
  Alexander Ku.
\newblock Image transformer.
\newblock \emph{CoRR}, abs/1802.05751, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.05751}.

\bibitem[Post(2018)]{sacrebleu}
Matt Post.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pp.\  186--191, Belgium, Brussels, October 2018.
  Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/W18-6319}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019GPT2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rae et~al.(2016)Rae, Hunt, Harley, Danihelka, Senior, Wayne, Graves,
  and Lillicrap]{jack_rae}
Jack~W Rae, Jonathan~J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg
  Wayne, Alex Graves, and Timothy~P Lillicrap.
\newblock Scaling memory-augmented neural networks with sparse reads and
  writes.
\newblock In \emph{Advances in Neural Information Processing Systems,
  ({NIPS})}, 2016.

\bibitem[Ramachandran et~al.(2019)Ramachandran, Parmar, Vaswani, Bello,
  Levskaya, and Shlens]{ramachandran2019attentionimage}
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,
  and Jonathon Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock \emph{CoRR}, abs/1906.05909, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.05909}.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro16}
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy~P.
  Lillicrap.
\newblock One-shot learning with memory-augmented neural networks.
\newblock \emph{CoRR}, abs/1605.06065, 2016.
\newblock URL \url{http://arxiv.org/abs/1605.06065}.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock \emph{CoRR}, abs/1804.04235, 2018.
\newblock URL \url{http://arxiv.org/abs/1804.04235}.

\bibitem[Shazeer et~al.(2018)Shazeer, Cheng, Parmar, Tran, Vaswani,
  Koanantakool, Hawkins, Lee, Hong, Young, Sepassi, and Hechtman]{meshtf}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn
  Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young,
  Ryan Sepassi, and Blake Hechtman.
\newblock Mesh-tensorflow: Deep learning for supercomputers.
\newblock \emph{CoRR}, abs/1811.02084, 2018.
\newblock URL \url{http://arxiv.org/abs/1811.02084}.

\bibitem[Sohoni et~al.(2019)Sohoni, Aberger, Leszczynski, Zhang, and
  R{\'{e}}]{sohoni2019lowmemory}
Nimit~Sharad Sohoni, Christopher~Richard Aberger, Megan Leszczynski, Jian
  Zhang, and Christopher R{\'{e}}.
\newblock Low-memory neural network training: {A} technical report.
\newblock \emph{CoRR}, abs/1904.10631, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.10631}.

\bibitem[Sukhbaatar et~al.(2019{\natexlab{a}})Sukhbaatar, Grave, Bojanowski,
  and Joulin]{sukhbaatar2019adaptiveattn}
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock \emph{CoRR}, abs/1905.07799, 2019{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1905.07799}.

\bibitem[Sukhbaatar et~al.(2019{\natexlab{b}})Sukhbaatar, Grave, Lample,
  J{\'{e}}gou, and Joulin]{sukhbaatar2019persistentmemory}
Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herv{\'{e}} J{\'{e}}gou,
  and Armand Joulin.
\newblock Augmenting self-attention with persistent memory.
\newblock \emph{CoRR}, abs/1907.01470, 2019{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1907.01470}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{CoRR}, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.03762}.

\bibitem[Weston et~al.(2014)Weston, Chopra, and Bordes]{mem_nets}
Jason Weston, Sumit Chopra, and Antoine Bordes.
\newblock Memory networks.
\newblock \emph{CoRR}, abs/1410.3916, 2014.
\newblock URL \url{http://arxiv.org/abs/1410.3916}.

\end{thebibliography}
