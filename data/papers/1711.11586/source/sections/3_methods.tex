\section{Multimodal Image-to-Image Translation}
\label{sec:methods}
Our goal is to learn a multi-modal mapping between two image domains, for example, edges and photographs, or night and day images, etc. 
Consider the input domain $\mathcal{A}\!\subset\!\mathds{R}^{H\!\times W\!\times 3}$, which is to be mapped to an output domain $\mathcal{B}\!\subset\!\mathds{R}^{H\!\times W\!\times 3}$. 
During training, we are given a dataset of paired instances from these domains, $\big\{(\A\!\in\!\mathcal{A}, \B\!\in\!\mathcal{B})\big\}$, which is representative of a joint distribution $p(\A,\B)$.
It is important to note that there could be multiple plausible paired instances $\B$ that would correspond to an input instance $\A$, but the training dataset usually contains only one such pair.
However, given a new instance $\A$ during test time, our model should be able to generate a diverse set of output $\Bh$'s, corresponding to different modes in the distribution $p(\B|\A)$.

While conditional GANs have achieved success in image-to-image translation tasks~\citep{pathakCVPR16context,sangkloy2017scribbler,xian2017texturegan,yang2016high,isola2016image,zhu2017unpaired}, they are primarily limited to generating a deterministic output $\Bh$ given the input image $\A$. 
On the other hand, we would like to learn the mapping that could sample the output $\Bh$ from true conditional distribution given $\A$, and produce results which are both diverse and realistic.
To do so, we learn a low-dimensional latent space $\z \in \mathds{R}^{Z}$, which encapsulates the ambiguous aspects of the output mode which are not present in the input image. For example, a sketch of a shoe could map to a variety of colors and textures, which could get compressed in this latent code. We then learn a deterministic mapping $\G:(\A,\z)\rightarrow \B$ to the output. To enable stochastic sampling, we desire the latent code vector $\z$ to be drawn from some prior distribution $p(\z)$; we use a standard Gaussian distribution $\mathcal{N}(0,I)$ in this work.

We first discuss a simple extension of existing methods and discuss its strengths and weakness, motivating the development of our proposed approach in the subsequent subsections.

\subsection{Baseline: \ppn ($\z \rightarrow \Bh$)}
The recently proposed \pp model~\citep{isola2016image} has shown high quality results in the image-to-image translation setting.
It uses conditional adversarial networks~\citep{goodfellow2014generative,mirza2014conditional} to help produce perceptually realistic results. GANs train a generator $\G$ and discriminator $\D$ by formulating their objective as an adversarial game. The discriminator attempts to differentiate between real images from the dataset and fake samples produced by the generator. Randomly drawn noise $\z$ is added to attempt to induce stochasticity.
We illustrate the formulation in Figure \ref{fig:fig2}(b) and describe it below.
\begin{equation}
\mathcal{L}_{\text{GAN}}(\G,\D) = \mathds{E}_{\A,\B\sim p(\A,\B)}[\log(\D(\A,\B))] + \mathds{E}_{\A\sim p(\A),\z\sim p(\z)}[ \log(1-\D(\A,\G(\A,\z)))]
\label{eqn:Lgan}
\end{equation}

To encourage the output of the generator to match the input as well as stabilize the training, we use an $\ell_1$ loss between the output and the ground truth image.
\begin{equation}
\mathcal{L}_{1}^{\text{image}}(\G) = \mathds{E}_{\A,\B\sim p(\A,\B),\z\sim p(\z)} ||\B - \G(\A,\z) ||_1
\label{eqn:L1}
\end{equation}

The final loss function uses the GAN and $\ell_1$ terms, balanced by $\lambda$. 
\begin{equation}
\G^{*} = \arg\min_{\G} \max_{\D} \quad \mathcal{L}_{\text{GAN}}(\G,\D) + \lambda \mathcal{L}_1^{\text{image}}(\G)
\end{equation}

In this scenario, there is little incentive for the generator to make use of the noise vector which encodes random information.
Isola et al.~\citep{isola2016image} note that the noise was ignored by the generator in preliminary experiments and was removed from the final experiments.
This was consistent with observations made in the conditional settings by~\citep{pathakCVPR16context,mathieu2015deep}, as well as the mode collapse phenomenon observed in unconditional cases~\citep{salimans2016improved,goodfellow2016nips}.
In this paper, we explore different ways to explicitly enforce
%the generator to use the latent encoding, by making it
the latent coding to
capture relevant information.

\subsection{Conditional Variational Autoencoder GAN: \cvaegan ($\B \rightarrow \z \rightarrow \widehat{\mathbf{B}}$)}
One way to force the latent code $\z$ to be ``useful" is to directly map the ground truth $\B$ to it using an encoding function $\E$.
The generator $\G$ then uses both the latent code and the input image $\A$ to synthesize the desired output $\widehat{\mathbf{B}}$.
The overall model can be easily understood as the reconstruction of $\B$, with latent encoding $\z$ concatenated with the paired $\A$ in the middle -- similar to an autoencoder~\citep{hinton2006reducing}. This interpretation is better shown in Figure \ref{fig:fig2}(c).

This approach has been successfully investigated in Variational Autoencoder~\citep{kingma2013auto} in the unconditional scenario without the adversarial objective. Extending it to conditional scenario, the distribution $Q(\z|\B)$ of latent code $\z$ using the encoder $\E$ with a Gaussian assumption, $Q(\z|\B)\triangleq\E(\B)$. To reflect this, Equation \ref{eqn:Lgan} is modified to sampling $\z\sim\E(\B)$ using the re-parameterization trick, allowing direct back-propagation~\citep{kingma2013auto}. 
\begin{equation}
\mathcal{L}_{\text{GAN}}^{\text{VAE}} = \mathds{E}_{\A,\B\sim p(\A,\B)}[\log(\D(\A,\B))] + \mathds{E}_{\A,\B\sim p(\A,\B),\z\sim E(\B)}[ \log(1-\D(\A,\G(\A,\z)))]
\label{eqn:LVAE}
\end{equation}
We make the corresponding change in the $\ell_1$ loss term in Equation \ref{eqn:L1} as well to obtain $\mathcal{L}_1^{\text{VAE}}(\G)=\mathds{E}_{\A,\B\sim p(\A,\B),\z\sim \E(\B)} ||\B - \G(\A,\z) ||_1$. Further, the latent distribution encoded by $E(B)$ is encouraged to be close to a random Gaussian to enable sampling at inference time, when $\B$ is not known.
\begin{equation}
\mathcal{L}_{\text{KL}}(\E) = \mathds{E}_{\B\sim p(\B)} [ \mathcal{D}_{\text{KL}}(\E(\B) || \;\mathcal{N}(0,I)) ],
\label{eqn:KL}
\end{equation}
where $\mathcal{D}_{\text{KL}}(p||q)=-\int  p(z) \log\frac{p(z)}{q(z)}dz $. This forms our \cvaegan objective, a conditional version of the VAE-GAN~\citep{larsen2016vaegan} as
\begin{equation}
\G^{*}, \E^{*} = \arg\min_{\G,\E} \max_{\D}  \quad \mathcal{L}_{\text{GAN}}^{\text{VAE}}(\G,\D,\E)
+ \lambda\mathcal{L}_1^{\text{VAE}}(\G, \E)+\lambda_{\text{KL}}\mathcal{L}_{\text{KL}}(\E).
\end{equation}

As a baseline, we also consider the deterministic version of this approach, i.e., dropping KL-divergence and encoding $\z=E(\B)$.
We call it \cae and show a comparison in the experiments.
There is no guarantee in \cae on the distribution of the latent space $\z$, which makes the test-time sampling of $\z$ difficult. 

% cite cvae-gan
\subsection{Conditional Latent Regressor GAN: \cinfogan ($\z \rightarrow \Bh \rightarrow \zh$)}
We explore another method of enforcing the generator network to utilize the latent code embedding $\z$, while staying close to the actual test time distribution $p(\z)$, but from the latent code's perspective.
As shown in Figure \ref{fig:fig2}(d), we start from a 
% randomly drawn latent code $\z$ and enforce $\E(\G(\A,\z))$ to map back to the same latent code,
randomly drawn latent code $\z$ and attempt to recover it with $\zh=\E(\G(\A,\z))$.
% using an $\ell_1$ loss.
Note that the encoder $\E$ here is producing a point estimate for $\zh$, whereas the encoder in the previous section was predicting a Gaussian distribution.
\begin{equation}
\mathcal{L}_{1}^{\text{latent}}(\G,\E) = \mathds{E}_{\A\sim p(\A),\z\sim p(\z)} ||\z - \E(\G(\A,\z)) ||_1
\label{eqn:L1_lacyc}
\end{equation}

We also include the discriminator loss $L_{\text{GAN}}(\G,\D)$ (Equation \ref{eqn:Lgan}) on $\Bh$ to encourage the network to generate realistic results, and the full loss can be written as:
\begin{equation}
\G^{*}, \E^{*} = \arg\min_{\G,\E} \max_\D \quad \mathcal{L}_{\text{GAN}}(\G,\D) + \lambda_{\text{latent}} \mathcal{L}_1^{\text{latent}}(\G,\E)
\label{fig:L}
\end{equation}

The $\ell_1$ loss for the ground truth image $\B$ is not used. Since the noise vector is randomly drawn, the predicted $\Bh$ does not necessarily need to be close to the ground truth but does need to be realistic. The above objective bears similarity to the ``latent regressor" model~\citep{donahue2016adversarial,dumoulin2016adversarially,xi2016infogan}, where the generated sample $\Bh$ is encoded to generate a latent vector.

\subsection{Our Hybrid Model: \bicycle}
\label{sec:finalMethod}
We combine the \cvaegan and \cinfogan objectives in a hybrid model. For \cvaegan, the encoding is learned from real data, but a random latent code may not yield realistic images at test time -- the KL loss may not be well optimized. Perhaps more importantly, the adversarial classifier $\D$ does not have a chance to see results sampled from the prior during training. 
%  latent space may not be organized in an easy manner to be sampled from
In \cinfogan, the latent space is easily sampled from a simple distribution, but the generator is trained without the benefit of seeing ground truth input-output pairs. We propose to train with constraints in both directions, aiming to take advantage of both cycles ($\B \rightarrow \z \rightarrow \widehat{\mathbf{B}}$ and $\z \rightarrow \Bh \rightarrow \zh$), hence the name \bicycle.

\vspace{-3mm}
\begin{equation}
\begin{split}
\G^{*}, \E^{*} = \arg\min_{\G,\E} \max_{\D} \quad  & \mathcal{L}_{\text{GAN}}^{\text{VAE}}(\G,\D,\E) + \lambda \mathcal{L}_1^{\text{VAE}}(\G,\E) \\
+& \mathcal{L}_{\text{GAN}}(\G,\D) + \lambda_{\text{latent}} \mathcal{L}_1^{\text{latent}}(\G,\E) + \lambda_{\text{KL}}\mathcal{L}_{\text{KL}}(\E),
\end{split}
\label{eq:L}
\end{equation}
where the hyper-parameters $\lambda$, $\lambda_{\text{latent}}$, and $\lambda_{\text{KL}}$ control the relative importance of each term. 

In the unconditional GAN setting, \citet{larsen2016vaegan} observe that using samples from both the prior $\mathcal{N}(0,I)$ and encoded $\E(\B)$ distributions further improves results.
Hence, we also report one variant which is the full objective shown above (Equation~\ref{eq:L}), but without the reconstruction loss on the latent space $\mathcal{L}_1^{\text{latent}}$. 
We call it \cvaeganp, as it is based on \cvaegan with an additional loss $\mathcal{L}_{\text{GAN}}(G, D)$, which allows the discriminator to see randomly drawn samples from the prior.
\vspace{-2mm}