\begin{thebibliography}{104}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl, Benhaim, Bilenko, Bjorck, Bubeck, Cai, Mendes, Chen, Chaudhary, Chopra, Giorno, de~Rosa, Dixon, Eldan, Iter, Goswami, Gunasekar, Haider, Hao, Hewett, Huynh, Javaheripi, Jin, Kauffmann, Karampatziakis, Kim, Khademi, Kurilenko, Lee, Lee, Li, Liang, Liu, Lin, Lin, Madan, Mitra, Modi, Nguyen, Norick, Patra, Perez-Becker, Portet, Pryzant, Qin, Radmilac, Rosset, Roy, Saarikivi, Saied, Salim, Santacroce, Shah, Shang, Sharma, Song, Ruwase, Wang, Ward, Wang, Witte, Wyatt, Xu, Xu, Yadav, Yang, Yang, Yu, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, and Zhou]{abdin2024phi3}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César~Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie~Del Giorno, Gustavo de~Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell~J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James~R. Lee, Yin~Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Olatunji Ruwase, Xin Wang, Rachel Ward, Guanhua Wang, Philipp
  Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li~Lyna Zhang, Yi~Zhang, Yunan Zhang, and Xiren Zhou.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{arXiv preprint arXiv: 2404.14219}, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.14219v1}.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebr'on, and Sanghai]{ainslie2023gqa}
J.~Ainslie, J.~Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr'on, and Sumit~K. Sanghai.
\newblock Gqa: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2023.
\newblock \doi{10.48550/arXiv.2305.13245}.
\newblock URL \url{https://arxiv.org/abs/2305.13245v3}.

\bibitem[Akyürek et~al.(2024)Akyürek, Wang, Kim, and Andreas]{akyürek2024incontext}
Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas.
\newblock In-context language learning: Architectures and algorithms.
\newblock \emph{arXiv preprint arXiv: 2401.12973}, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.12973v2}.

\bibitem[Arora et~al.(2023)Arora, Eyuboglu, Timalsina, Johnson, Poli, Zou, Rudra, and Ré]{arora2023zoology}
Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré.
\newblock Zoology: Measuring and improving recall in efficient language models.
\newblock \emph{arXiv preprint arXiv: 2312.04927}, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.04927v1}.

\bibitem[Arora et~al.(2024)Arora, Eyuboglu, Zhang, Timalsina, Alberti, Zinsley, Zou, Rudra, and R{\'e}]{arora2024simple}
Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher R{\'e}.
\newblock Simple linear attention language models balance the recall-throughput tradeoff.
\newblock \emph{arXiv preprint arXiv:2402.18668}, 2024.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton]{mbpp}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv: 2108.07732}, 2021.
\newblock URL \url{https://arxiv.org/abs/2108.07732v1}.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and translate.
\newblock \emph{International Conference On Learning Representations}, 2014.
\newblock URL \url{https://arxiv.org/abs/1409.0473v7}.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv: Arxiv-2004.05150}, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.05150v2}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{piqa}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock {PIQA:} reasoning about physical commonsense in natural language.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pp.\  7432--7439. {AAAI} Press, 2020.
\newblock \doi{10.1609/AAAI.V34I05.6239}.
\newblock URL \url{https://doi.org/10.1609/aaai.v34i05.6239}.

\bibitem[Botev et~al.(2024)Botev, De, Smith, Fernando, Muraru, Haroun, Berrada, Pascanu, Sessa, Dadashi, Hussenot, Ferret, Girgin, Bachem, Andreev, Kenealy, Mesnard, Hardin, Bhupatiraju, Pathak, Sifre, Rivière, Kale, Love, Tafti, Joulin, Fiedel, Senter, Chen, Srinivasan, Desjardins, Budden, Doucet, Vikram, Paszke, Gale, Borgeaud, Chen, Brock, Paterson, Brennan, Risdal, Gundluru, Devanathan, Mooney, Chauhan, Culliton, Martins, Bandy, Huntsperger, Cameron, Zucker, Warkentin, Peran, Giang, Ghahramani, Farabet, Kavukcuoglu, Hassabis, Hadsell, Teh, and de~Frietas]{botev2024recurrentgemma}
Aleksandar Botev, Soham De, Samuel~L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier~Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir~Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz~GUStavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur Zucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Clément Farabet, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee~Whye Teh, and Nando de~Frietas.
\newblock Recurrentgemma: Moving past transformers for efficient open language models.
\newblock \emph{arXiv preprint arXiv: 2404.07839}, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.07839v1}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165v4}.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, Nori, Palangi, Ribeiro, and Zhang]{bubeck2023sparks}
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco~Tulio Ribeiro, and Yi~Zhang.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv: 2303.12712}, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.12712v5}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{humaneval}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv: 2107.03374}, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374v2}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Wong, Chen, and Tian]{chen2023extending}
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.
\newblock Extending context window of large language models via positional interpolation.
\newblock \emph{arXiv preprint arXiv: 2306.15595}, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2306.15595v2}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Qian, Tang, Lai, Liu, Han, and Jia]{chen2023longlora}
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.
\newblock Longlora: Efficient fine-tuning of long-context large language models.
\newblock \emph{International Conference on Learning Representations}, 2023{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2309.12307}.
\newblock URL \url{https://arxiv.org/abs/2309.12307v1}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{PREPRINT}, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.10509v1}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{boolq}
Christopher Clark, Kenton Lee, Ming{-}Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)}, pp.\  2924--2936. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/N19-1300}.
\newblock URL \url{https://doi.org/10.18653/v1/n19-1300}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv: 1803.05457}, 2018.
\newblock URL \url{https://arxiv.org/abs/1803.05457v1}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv: 2110.14168}, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168v2}.

\bibitem[Dai et~al.(2022)Dai, Dong, Hao, Sui, Chang, and Wei]{dai2021knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock \emph{ACL}, 2022.
\newblock URL \url{https://arxiv.org/abs/2104.08696v2}.

\bibitem[Dao(2023)]{dao2023flashattention2}
Tri Dao.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock \emph{arXiv preprint arXiv: 2307.08691}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.08691v1}.

\bibitem[Dao et~al.(2022{\natexlab{a}})Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.

\bibitem[Dao et~al.(2022{\natexlab{b}})Dao, Fu, Saab, Thomas, Rudra, and Ré]{dao2022hungry}
Tri Dao, Daniel~Y. Fu, Khaled~Kamal Saab, A.~Thomas, A.~Rudra, and Christopher Ré.
\newblock Hungry hungry hippos: Towards language modeling with state space models.
\newblock \emph{International Conference On Learning Representations}, 2022{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2212.14052}.
\newblock URL \url{https://arxiv.org/abs/2212.14052v3}.

\bibitem[Dauphin et~al.(2016)Dauphin, Fan, Auli, and Grangier]{dauphin2016language}
Y.~Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock \emph{International Conference On Machine Learning}, 2016.
\newblock URL \url{https://arxiv.org/abs/1612.08083v3}.

\bibitem[De et~al.(2024)De, Smith, Fernando, Botev, Cristian-Muraru, Gu, Haroun, Berrada, Chen, Srinivasan, Desjardins, Doucet, Budden, Teh, Pascanu, Freitas, and Gulcehre]{de2024griffin}
Soham De, Samuel~L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee~Whye Teh, Razvan Pascanu, Nando~De Freitas, and Caglar Gulcehre.
\newblock Griffin: Mixing gated linear recurrences with local attention for efficient language models.
\newblock \emph{arXiv preprint arXiv: 2402.19427}, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.19427v1}.

\bibitem[Ding et~al.(2024)Ding, Zhang, Zhang, Xu, Shang, Xu, Yang, and Yang]{ding2024longrope}
Yiran Ding, L.~Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang.
\newblock Longrope: Extending llm context window beyond 2 million tokens.
\newblock \emph{International Conference on Machine Learning}, 2024.
\newblock \doi{10.48550/arXiv.2402.13753}.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, Goyal, Hartshorn, Yang, Mitra, Sravankumar, Korenev, Hinsvark, Rao, Zhang, Rodriguez, Gregerson, Spataru, Roziere, Biron, Tang, Chern, Caucheteux, Nayak, Bi, Marra, McConnell, Keller, Touret, Wu, Wong, Ferrer, Nikolaidis, Allonsius, Song, Pintz, Livshits, Esiobu, Choudhary, Mahajan, Garcia-Olano, Perino, Hupkes, Lakomkin, AlBadawy, Lobanova, Dinan, Smith, Radenovic, Zhang, Synnaeve, Lee, Anderson, Nail, Mialon, Pang, Cucurell, Nguyen, Korevaar, Xu, Touvron, Zarov, Ibarra, Kloumann, Misra, Evtimov, Copet, Lee, Geffert, Vranes, Park, Mahadeokar, Shah, van~der Linde, Billock, Hong, Lee, Fu, Chi, Huang, Liu, Wang, Yu, Bitton, Spisak, Park, Rocca, Johnstun, Saxe, Jia, Alwala, Upasani, Plawiak, Li, Heafield, Stone, El-Arini, Iyer, Malik, Chiu, Bhalla, Rantala-Yeary, van~der Maaten, Chen, Tan, Jenkins, Martin, Madaan, Malo, Blecher, Landzaat, de~Oliveira, Muzzi, Pasupuleti, Singh, Paluri, Kardas, Oldham, Rita,
  Pavlova, Kambadur, Lewis, Si, Singh, Hassan, Goyal, Torabi, Bashlykov, Bogoychev, Chatterji, Duchenne, Çelebi, Alrassy, Zhang, Li, Vasic, Weng, Bhargava, Dubal, Krishnan, Koura, Xu, He, Dong, Srinivasan, Ganapathy, Calderer, Cabral, Stojnic, Raileanu, Girdhar, Patel, Sauvestre, Polidoro, Sumbaly, Taylor, Silva, Hou, Wang, Hosseini, Chennabasappa, Singh, Bell, Kim, Edunov, Nie, Narang, Raparthy, Shen, Wan, Bhosale, Zhang, Vandenhende, Batra, Whitman, Sootla, Collot, Gururangan, Borodinsky, Herman, Fowler, Sheasha, Georgiou, Scialom, Speckbacher, Mihaylov, Xiao, Karn, Goswami, Gupta, Ramanathan, Kerkez, Gonguet, Do, Vogeti, Petrovic, Chu, Xiong, Fu, Meers, Martinet, Wang, Tan, Xie, Jia, Wang, Goldschlag, Gaur, Babaei, Wen, Song, Zhang, Li, Mao, Coudert, Yan, Chen, Papakipos, Singh, Grattafiori, Jain, Kelsey, Shajnfeld, Gangidi, Victoria, Goldstand, Menon, Sharma, Boesenberg, Vaughan, Baevski, Feinstein, Kallet, Sangani, Yunus, Lupu, Alvarado, Caples, Gu, Ho, Poulton, Ryan, Ramchandani, Franco, Saraf,
  Chowdhury, Gabriel, Bharambe, Eisenman, Yazdan, James, Maurer, Leonhardi, Huang, Loyd, Paola, Paranjape, Liu, Wu, Ni, Hancock, Wasti, Spence, Stojkovic, Gamido, Montalvo, Parker, Burton, Mejia, Wang, Kim, Zhou, Hu, Chu, Cai, Tindal, Feichtenhofer, Civin, Beaty, Kreymer, Li, Wyatt, Adkins, Xu, Testuggine, David, Parikh, Liskovich, Foss, Wang, Le, Holland, Dowling, Jamil, Montgomery, Presani, Hahn, Wood, Brinkman, Arcaute, Dunbar, Smothers, Sun, Kreuk, Tian, Ozgenel, Caggioni, Guzmán, Kanayet, Seide, Florez, Schwarz, Badeer, Swee, Halpern, Thattai, Herman, Sizov, Guangyi, Zhang, Lakshminarayanan, Shojanazeri, Zou, Wang, Zha, Habeeb, Rudolph, Suk, Aspegren, Goldman, Molybog, Tufanov, Veliche, Gat, Weissman, Geboski, Kohli, Asher, Gaya, Marcus, Tang, Chan, Zhen, Reizenstein, Teboul, Zhong, Jin, Yang, Cummings, Carvill, Shepard, McPhie, Torres, Ginsburg, Wang, Wu, U, Saxena, Prasad, Khandelwal, Zand, Matosich, Veeraraghavan, Michelena, Li, Huang, Chawla, Lakhotia, Huang, Chen, Garg, A, Silva, Bell, Zhang, Guo,
  Yu, Moshkovich, Wehrstedt, Khabsa, Avalani, Bhatt, Tsimpoukelli, Mankus, Hasson, Lennie, Reso, Groshev, Naumov, Lathi, Keneally, Seltzer, Valko, Restrepo, Patel, Vyatskov, Samvelyan, Clark, Macey, Wang, Hermoso, Metanat, Rastegari, Bansal, Santhanam, Parks, White, Bawa, Singhal, Egebo, Usunier, Laptev, Dong, Zhang, Cheng, Chernoguz, Hart, Salpekar, Kalinli, Kent, Parekh, Saab, Balaji, Rittner, Bontrager, Roux, Dollar, Zvyagina, Ratanchandani, Yuvraj, Liang, Alao, Rodriguez, Ayub, Murthy, Nayani, Mitra, Li, Hogan, Battey, Wang, Maheswari, Howes, Rinott, Bondu, Datta, Chugh, Hunt, Dhillon, Sidorov, Pan, Verma, Yamamoto, Ramaswamy, Lindsay, Lindsay, Feng, Lin, Zha, Shankar, Zhang, Zhang, Wang, Agarwal, Sajuyigbe, Chintala, Max, Chen, Kehoe, Satterfield, Govindaprasad, Gupta, Cho, Virk, Subramanian, Choudhury, Goldman, Remez, Glaser, Best, Kohler, Robinson, Li, Zhang, Matthews, Chou, Shaked, Vontimitta, Ajayi, Montanez, Mohan, Kumar, Mangla, Ionescu, Poenaru, Mihailescu, Ivanov, Li, Wang, Jiang, Bouaziz,
  Constable, Tang, Wang, Wu, Wang, Xia, Wu, Gao, Chen, Hu, Jia, Qi, Li, Zhang, Zhang, Adi, Nam, Yu, Wang, Hao, Qian, He, Rait, DeVito, Rosnbrick, Wen, Yang, and Zhao]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian~Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric~Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia~Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu~Xu, Hugo Touvron, Iliyan Zarov,
  Imanol~Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van~der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan~Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke~Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van~der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de~Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh~Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier
  Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit~Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo~Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun~Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu,
  Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing~Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi~Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie~Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto~De Paola, Bhargavi Paranjape, Bing Liu, Bo~Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
  Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela~Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli,
  Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam~Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael~L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel~Jubert Hermoso, Mo~Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha
  Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay~Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai~Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin~Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny
  Virk, Suraj Subramanian, Sy~Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay~Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad~Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye~Hu, Ye~Jia, Ye~Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv: 2407.21783}, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.21783v1}.

\bibitem[Elfwing et~al.(2017)Elfwing, Uchibe, and Doya]{silu}
Stefan Elfwing, E.~Uchibe, and K.~Doya.
\newblock Sigmoid-weighted linear units for neural network function approximation in reinforcement learning.
\newblock \emph{Neural Networks}, 2017.
\newblock \doi{10.1016/j.neunet.2017.12.012}.

\bibitem[Fathi et~al.(2023)Fathi, Pilault, Firat, Pal, Bacon, and Goroshin]{fathi2023blockstate}
Mahan Fathi, Jonathan Pilault, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, and Ross Goroshin.
\newblock Block-state transformers.
\newblock \emph{NEURIPS}, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.09539v4}.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Re]{h3}
Daniel~Y Fu, Tri Dao, Khaled~Kamal Saab, Armin~W Thomas, Atri Rudra, and Christopher Re.
\newblock Hungry hungry hippos: Towards language modeling with state space models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=COZDy0WYGg}.

\bibitem[Gu \& Dao(2023)Gu and Dao]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu et~al.(2021)Gu, Goel, and R'e]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher R'e.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{International Conference On Learning Representations}, 2021.

\bibitem[Gu et~al.(2022)Gu, Gupta, Goel, and Ré]{s4d}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré.
\newblock On the parameterization and initialization of diagonal state space models.
\newblock \emph{ARXIV.ORG}, 2022.
\newblock \doi{10.48550/arXiv.2206.11893}.

\bibitem[Han et~al.(2023)Han, Wang, Xiong, Chen, Ji, and Wang]{han2023lminfinite}
Chi Han, Qifan Wang, Wenhan Xiong, Yu~Chen, Heng Ji, and Sinong Wang.
\newblock Lm-infinite: Simple on-the-fly length generalization for large language models.
\newblock \emph{arXiv preprint arXiv: 2308.16137}, 2023.
\newblock URL \url{https://arxiv.org/abs/2308.16137v3}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{CVPR}, 2016.
\newblock URL \url{https://arxiv.org/abs/1512.03385v1}.

\bibitem[He et~al.(2019)He, Qian, Wang, Le, Hetang, Lyu, Wang, and Yue]{he2019depthwise}
Yihui He, Jianing Qian, Jianren Wang, Cindy~X. Le, Congrui Hetang, Qi~Lyu, Wenping Wang, and Tianwei Yue.
\newblock Depth-wise decomposition for accelerating separable convolutions in efficient convolutional neural networks.
\newblock \emph{arXiv preprint arXiv: 1910.09455}, 2019.
\newblock URL \url{https://arxiv.org/abs/1910.09455v3}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.09751v2}.

\bibitem[Huang et~al.(2021)Huang, Cao, Parulian, Ji, and Wang]{govreport2023}
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu~Wang.
\newblock Efficient attentions for long document summarization.
\newblock \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1419--1436, 2021.

\bibitem[Jelassi et~al.(2024)Jelassi, Brandfonbrener, Kakade, and Malach]{jelassi2024repeat}
Samy Jelassi, David Brandfonbrener, Sham~M. Kakade, and Eran Malach.
\newblock Repeat after me: Transformers are better than state space models at copying.
\newblock \emph{arXiv preprint arXiv: 2402.01032}, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.01032v1}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv: 2310.06825}, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.06825v1}.

\bibitem[Jin et~al.(2024)Jin, Han, Yang, Jiang, Liu, Chang, Chen, and Hu]{jin2024llm}
Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu.
\newblock Llm maybe longlm: Self-extend llm context window without tuning.
\newblock \emph{arXiv preprint arXiv: 2401.01325}, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.01325v1}.

\bibitem[Katsch(2023)]{katsch2023gateloop}
Tobias Katsch.
\newblock Gateloop: Fully data-controlled linear recurrence for sequence modeling.
\newblock \emph{arXiv preprint arXiv: 2311.01927}, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.01927v1}.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Li et~al.(2023)Li, Bubeck, Eldan, Giorno, Gunasekar, and Lee]{li2023textbooks}
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie~Del Giorno, Suriya Gunasekar, and Yin~Tat Lee.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock \emph{arXiv preprint arXiv: 2309.05463}, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.05463v1}.

\bibitem[Lieber et~al.(2024)Lieber, Lenz, Bata, Cohen, Osin, Dalmedigos, Safahi, Meirom, Belinkov, Shalev-Shwartz, Abend, Alon, Asida, Bergman, Glozman, Gokhman, Manevich, Ratner, Rozen, Shwartz, Zusman, and Shoham]{lieber2024jamba}
Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham.
\newblock Jamba: A hybrid transformer-mamba language model.
\newblock \emph{arXiv preprint arXiv: 2403.19887}, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.19887v1}.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin-etal-2022-truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock {T}ruthful{QA}: Measuring how models mimic human falsehoods.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3214--3252, Dublin, Ireland, may 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.229}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.229}.

\bibitem[Loshchilov \& Hutter(2018)Loshchilov and Hutter]{adw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ma et~al.(2023)Ma, Zhou, Kong, He, Gui, Neubig, May, and Zettlemoyer]{mega}
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.
\newblock Mega: Moving average equipped gated attention.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=qNLe3iq2El}.

\bibitem[Ma et~al.(2024)Ma, Yang, Xiong, Chen, Yu, Zhang, May, Zettlemoyer, Levy, and Zhou]{ma2024megalodon}
Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou.
\newblock Megalodon: Efficient llm pretraining and inference with unlimited context length.
\newblock \emph{arXiv preprint arXiv: 2404.08801}, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.08801v1}.

\bibitem[Martin \& Cundy(2018)Martin and Cundy]{parallrnn}
Eric Martin and Chris Cundy.
\newblock Parallelizing linear recurrent neural nets over sequence length.
\newblock In \emph{6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=HyUNwulC-}.

\bibitem[Mehta et~al.(2023)Mehta, Gupta, Cutkosky, and Neyshabur]{gss}
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/forum?id=5MkYIYCbva}.

\bibitem[Merrill et~al.(2024)Merrill, Petty, and Sabharwal]{merrill2024illusion}
William Merrill, Jackson Petty, and Ashish Sabharwal.
\newblock The illusion of state in state-space models.
\newblock \emph{arXiv preprint arXiv: 2404.08819}, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.08819v1}.

\bibitem[MetaAI(2024)]{llama3}
MetaAI.
\newblock Introducing meta llama 3: The most capable openly available llm to date, 2024.
\newblock URL: \url{https://ai.meta.com/blog/meta-llama-3/}.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{openbookqa}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2018.
\newblock \doi{10.18653/v1/D18-1260}.
\newblock URL \url{https://arxiv.org/abs/1809.02789v1}.

\bibitem[Mohtashami \& Jaggi(2023)Mohtashami and Jaggi]{pk}
Amirkeivan Mohtashami and Martin Jaggi.
\newblock Landmark attention: Random-access infinite context length for transformers.
\newblock \emph{arXiv preprint arXiv: 2305.16300}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.16300v2}.

\bibitem[Munkhdalai et~al.(2024)Munkhdalai, Faruqui, and Gopal]{munkhdalai2024leave}
Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.
\newblock Leave no context behind: Efficient infinite context transformers with infini-attention.
\newblock \emph{arXiv preprint arXiv: 2404.07143}, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.07143v1}.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{PREPRINT}, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.08774v4}.

\bibitem[Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu, and De]{lru}
Antonio Orvieto, Samuel~L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock \emph{International Conference on Machine Learning}, 2023.
\newblock \doi{10.48550/arXiv.2303.06349}.
\newblock URL \url{https://arxiv.org/abs/2303.06349v1}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fernández]{paperno2016lambada}
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Q.~N. Pham, R.~Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and R.~Fernández.
\newblock The lambada dataset: Word prediction requiring a broad discourse context.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2016.
\newblock \doi{10.18653/v1/P16-1144}.

\bibitem[Park et~al.(2024)Park, Park, Xiong, Lee, Cho, Oymak, Lee, and Papailiopoulos]{park2024mamba}
Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock Can mamba learn how to learn? a comparative study on in-context learning tasks.
\newblock \emph{arXiv preprint arXiv: 2402.04248}, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.04248v1}.

\bibitem[Poli et~al.(2023)Poli, Massaroli, Nguyen, Fu, Dao, Baccus, Bengio, Ermon, and Ré]{poli2023hyena}
Michael Poli, Stefano Massaroli, Eric~Q. Nguyen, Daniel~Y. Fu, Tri Dao, S.~Baccus, Y.~Bengio, Stefano Ermon, and Christopher Ré.
\newblock Hyena hierarchy: Towards larger convolutional language models.
\newblock \emph{International Conference On Machine Learning}, 2023.
\newblock \doi{10.48550/arXiv.2302.10866}.
\newblock URL \url{https://arxiv.org/abs/2302.10866v3}.

\bibitem[Press et~al.(2021)Press, Smith, and Lewis]{press2021train}
Ofir Press, Noah~A. Smith, and M.~Lewis.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock \emph{International Conference On Learning Representations}, 2021.
\newblock URL \url{https://arxiv.org/abs/2108.12409v2}.

\bibitem[Qin et~al.(2023)Qin, Yang, and Zhong]{hgrn}
Zhen Qin, Songlin Yang, and Yiran Zhong.
\newblock Hierarchically gated recurrent neural network for sequence modeling.
\newblock \emph{Neural Information Processing Systems}, 2023.
\newblock \doi{10.48550/arXiv.2311.04823}.
\newblock URL \url{https://arxiv.org/abs/2311.04823v1}.

\bibitem[Qin et~al.(2024)Qin, Yang, Sun, Shen, Li, Sun, and Zhong]{qin2024hgrn2}
Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong.
\newblock Hgrn2: Gated linear rnns with state expansion.
\newblock \emph{arXiv preprint arXiv: 2404.07904}, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.07904v1}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{gpt2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{arXiv preprint}, 2019.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:160025533}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{EMNLP}, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.05250v3}.

\bibitem[Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman]{rein2023gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R. Bowman.
\newblock Gpqa: A graduate-level google-proof qa benchmark.
\newblock \emph{arXiv preprint arXiv: 2311.12022}, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.12022v1}.

\bibitem[Ren et~al.(2023)Ren, Liu, Wang, Xu, Zhu, and Zhai]{ren2023sparse}
Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai.
\newblock Sparse modular activation for efficient sequence modeling.
\newblock \emph{NEURIPS}, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.11197v1}.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and Grangier]{routing}
Aurko Roy, M.~Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{International Conference On Topology, Algebra And Categories In Logic}, 2020.
\newblock \doi{10.1162/tacl_a_00353}.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.
\newblock URL \url{https://arxiv.org/abs/1907.10641v2}.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{siqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock \emph{arXiv preprint arXiv: 1904.09728}, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.09728v3}.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{fastweight}
Imanol Schlag, Kazuki Irie, and J{\"{u}}rgen Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In Marina Meila and Tong Zhang (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  9355--9366. PMLR, 2021.
\newblock URL \url{http://proceedings.mlr.press/v139/schlag21a.html}.

\bibitem[Shah et~al.(2024)Shah, Bikshandi, Zhang, Thakkar, Ramani, and Dao]{shah2024flashattention3}
Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao.
\newblock Flashattention-3: Fast and accurate attention with asynchrony and low-precision.
\newblock \emph{arXiv preprint arXiv: 2407.08608}, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.08608v2}.

\bibitem[Shaham et~al.(2023)Shaham, Ivgi, Efrat, Berant, and Levy]{shaham2023zeroscrolls}
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.
\newblock Zeroscrolls: A zero-shot benchmark for long text understanding.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2023.
\newblock \doi{10.48550/arXiv.2305.14196}.
\newblock URL \url{https://arxiv.org/abs/2305.14196v2}.

\bibitem[Shazeer(2019)]{shazeer2019fast}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv preprint arXiv: 1911.02150}, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.02150v1}.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv: 2002.05202}, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.05202v1}.

\bibitem[Smith et~al.(2023)Smith, Warrington, and Linderman]{s5}
Jimmy~T.H. Smith, Andrew Warrington, and Scott Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Ai8Hw3AXqks}.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey]{slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock Slimpajama: A 627b token cleaned and deduplicated version of redpajama, 2023.
\newblock URL: \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Murtadha, Wen, and Liu]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Ahmed Murtadha, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv preprint arXiv: 2104.09864}, 2021.

\bibitem[Sun et~al.(2023)Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{sun2023retentive}
Yutao Sun, Li~Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.
\newblock Retentive network: A successor to transformer for large language models.
\newblock \emph{arXiv preprint arXiv:2307.08621}, 2023.

\bibitem[Team(2024)]{team2024gemma}
Gemma Team.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv: 2403.08295}, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.08295v1}.

\bibitem[Team et~al.(2024)Team, Lenz, Arazi, Bergman, Manevich, Peleg, Aviram, Almagor, Fridman, Padnos, Gissin, Jannai, Muhlgay, Zimberg, Gerber, Dolev, Krakovsky, Safahi, Schwartz, Cohen, Shachaf, Rozenblum, Bata, Blass, Magar, Dalmedigos, Osin, Fadlon, Rozman, Danos, Gokhman, Zusman, Gidron, Ratner, Gat, Rozen, Fried, Leshno, Antverg, Abend, Lieber, Dagan, Cohavi, Alon, Belson, Cohen, Gilad, Glozman, Lev, Meirom, Delbari, Ness, Asida, Gal, Braude, Pumerantz, Cohen, Belinkov, Globerson, Levy, and Shoham]{team2024jamba15}
Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden~M Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or~Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom~Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval~Peleg Levy, and Yoav Shoham.
\newblock Jamba-1.5: Hybrid transformer-mamba models at scale.
\newblock \emph{arXiv preprint arXiv: 2408.12570}, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.12570v1}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv: 2307.09288}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.09288v2}.

\bibitem[Tworkowski et~al.(2023)Tworkowski, Staniszewski, Pacek, Wu, Michalewski, and Miłoś]{tworkowski2023focused}
Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś.
\newblock Focused transformer: Contrastive training for context scaling.
\newblock \emph{NEURIPS}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.03170v2}.

\bibitem[Varis \& Bojar(2021)Varis and Bojar]{varis-bojar-2021-sequence}
Dusan Varis and Ond{\v{r}}ej Bojar.
\newblock Sequence length is a domain: Length-based overfitting in transformer models.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  8246--8257, Online and Punta Cana, Dominican Republic, nov 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.650}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.650}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam~M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{NIPS}, 2017.

\bibitem[Wang et~al.(2022)Wang, Pang, Chen, Phang, and Bowman]{squality}
Alex Wang, Richard~Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel~R. Bowman.
\newblock Squality: Building a long-document summarization dataset the hard way.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2022.
\newblock \doi{10.48550/arXiv.2205.11465}.
\newblock URL \url{https://arxiv.org/abs/2205.11465v1}.

\bibitem[Wang et~al.(2024)Wang, Ma, Zhang, Ni, Chandra, Guo, Ren, Arulraj, He, Jiang, Li, Ku, Wang, Zhuang, Fan, Yue, and Chen]{wang2024mmlupro}
Yubo Wang, Xueguang Ma, Ge~Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen.
\newblock Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.
\newblock \emph{arXiv preprint arXiv: 2406.01574}, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.01574v4}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Xia, Le, and Zhou]{cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, E.~Chi, F.~Xia, Quoc Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Neural Information Processing Systems}, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.11903v6}.

\bibitem[Wen et~al.(2024)Wen, Dang, and Lyu]{wen2024rnns}
Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu.
\newblock Rnns are not transformers (yet): The key bottleneck on in-context retrieval.
\newblock \emph{arXiv preprint arXiv: 2402.18510}, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.18510v1}.

\bibitem[Wu et~al.(2022)Wu, Rabe, Hutchins, and Szegedy]{wu2022memorizing}
Yuhuai Wu, Markus~N. Rabe, DeLesley~S. Hutchins, and Christian Szegedy.
\newblock Memorizing transformers.
\newblock \emph{International Conference On Learning Representations}, 2022.
\newblock \doi{10.48550/arXiv.2203.08913}.
\newblock URL \url{https://arxiv.org/abs/2203.08913v1}.

\bibitem[Xiao et~al.(2023)Xiao, Tian, Chen, Han, and Lewis]{xiao2023efficient}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock \emph{arXiv preprint arXiv: 2309.17453}, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.17453v1}.

\bibitem[Xiao et~al.(2024)Xiao, Tian, Chen, Han, and Lewis]{streamllm}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=NG7sS51zVF}.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{prenorm}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie{-}Yan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  10524--10533. PMLR, 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/xiong20b.html}.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, Khabsa, Fang, Mehdad, Narang, Malik, Fan, Bhosale, Edunov, Lewis, Wang, and Ma]{xiong2023effective}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{arXiv preprint arXiv: 2309.16039}, 2023.

\bibitem[Yang \& Zhang(2024)Yang and Zhang]{yang2024fla}
Songlin Yang and Yu~Zhang.
\newblock Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024.
\newblock URL \url{https://github.com/sustcsonglin/flash-linear-attention}.

\bibitem[Yang et~al.(2023)Yang, Wang, Shen, Panda, and Kim]{yang2023gated}
Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim.
\newblock Gated linear attention transformers with hardware-efficient training.
\newblock \emph{arXiv preprint arXiv:2312.06635}, 2023.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti, Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang, et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 17283--17297, 2020.
\newblock URL \url{https://arxiv.org/abs/2007.14062v2}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2019.
\newblock \doi{10.18653/v1/P19-1472}.
\newblock URL \url{https://arxiv.org/abs/1905.07830v1}.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{rms}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock \emph{Neural Information Processing Systems}, 2019.
\newblock \doi{10.5167/UZH-177483}.
\newblock URL \url{https://arxiv.org/abs/1910.07467v1}.

\bibitem[Zhangir~Azerbayev \& Piotrowski(2022)Zhangir~Azerbayev and Piotrowski]{proofpile}
Edward~Ayers Zhangir~Azerbayev and Bartosz Piotrowski.
\newblock Proof-pile, 2022.
\newblock URL: \url{https://github.com/zhangir-azerbayev/proof-pile }.

\bibitem[Zheng et~al.(2015)Zheng, Yang, Liu, Liang, and Li]{softplus}
Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li.
\newblock Improving deep neural networks using softplus units.
\newblock \emph{2015 International Joint Conference on Neural Networks (IJCNN)}, pp.\  1--4, 2015.
\newblock \doi{10.1109/IJCNN.2015.7280459}.
\newblock URL \url{https://ieeexplore.ieee.org/document/7280459}.

\bibitem[Zuo et~al.(2022)Zuo, Liu, Jiao, Charles, Manavoglu, Zhao, and Gao]{zuo2022efficient}
Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao.
\newblock Efficient long sequence modeling via state space augmented transformer.
\newblock \emph{arXiv preprint arXiv: 2212.08136}, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.08136v1}.

\end{thebibliography}
