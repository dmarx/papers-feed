@article{arora2024simple,
  title={Simple linear attention language models balance the recall-throughput tradeoff},
  author={Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zinsley, Dylan and Zou, James and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2402.18668},
  year={2024}
}
@article{paperno2016lambada,
  title     = {The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author    = {Denis Paperno and Germán Kruszewski and Angeliki Lazaridou and Q. N. Pham and R. Bernardi and Sandro Pezzelle and Marco Baroni and Gemma Boleda and R. Fernández},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2016},
  doi       = {10.18653/v1/P16-1144},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/5ed791f810da580c78df6a052c6b9f2e258f6b0a}
}
@article{ding2024longrope,
  title     = {LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
  author    = {Yiran Ding and L. Zhang and Chengruidong Zhang and Yuanyuan Xu and Ning Shang and Jiahang Xu and Fan Yang and Mao Yang},
  journal   = {International Conference on Machine Learning},
  year      = {2024},
  doi       = {10.48550/arXiv.2402.13753},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/c9603ec967879c24973b5bd48861df2e5555932e}
}
@article{xiong2023effective,
  title   = {Effective Long-Context Scaling of Foundation Models},
  author  = {Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2309.16039}
}
@article{tworkowski2023focused,
  title   = {Focused Transformer: Contrastive Training for Context Scaling},
  author  = {Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Miłoś},
  year    = {2023},
  journal = {NEURIPS},
  url     = {https://arxiv.org/abs/2307.03170v2},
  pdf     = {https://arxiv.org/pdf/2307.03170.pdf}
}
@article{chen2023longlora,
  title     = {LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
  author    = {Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
  journal   = {International Conference on Learning Representations},
  year      = {2023},
  doi       = {10.48550/arXiv.2309.12307},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b6346f9fa093b8e85df712485a2b851b9f680dac},
  url       = {https://arxiv.org/abs/2309.12307v1},
  pdf       = {https://arxiv.org/pdf/2309.12307.pdf}
}
@inproceedings{streamllm,
  author    = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  title     = {Efficient Streaming Language Models with Attention Sinks},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher = {OpenReview.net},
  year      = {2024},
  url       = {https://openreview.net/forum?id=NG7sS51zVF},
  timestamp = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/XiaoTCHL24.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{shaham2023zeroscrolls,
  title     = {ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding},
  author    = {Uri Shaham and Maor Ivgi and Avia Efrat and Jonathan Berant and Omer Levy},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2023},
  doi       = {10.48550/arXiv.2305.14196},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/eb511ae6b9f04e4936891d26787f274b48b99d57},
  url       = {https://arxiv.org/abs/2305.14196v2},
  pdf       = {https://arxiv.org/pdf/2305.14196.pdf}
}
@article{de2024griffin,
  title   = {Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models},
  author  = {Soham De and Samuel L. Smith and Anushan Fernando and Aleksandar Botev and George Cristian-Muraru and Albert Gu and Ruba Haroun and Leonard Berrada and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and Arnaud Doucet and David Budden and Yee Whye Teh and Razvan Pascanu and Nando De Freitas and Caglar Gulcehre},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2402.19427},
  url     = {https://arxiv.org/abs/2402.19427v1},
  pdf     = {https://arxiv.org/pdf/2402.19427.pdf}
}
@article{park2024mamba,
  title   = {Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks},
  author  = {Jongho Park and Jaeseung Park and Zheyang Xiong and Nayoung Lee and Jaewoong Cho and Samet Oymak and Kangwook Lee and Dimitris Papailiopoulos},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2402.04248},
  url     = {https://arxiv.org/abs/2402.04248v1},
  pdf     = {https://arxiv.org/pdf/2402.04248.pdf}
}
@article{rein2023gpqa,
  title   = {GPQA: A Graduate-Level Google-Proof QA Benchmark},
  author  = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2311.12022},
  url     = {https://arxiv.org/abs/2311.12022v1},
  pdf     = {https://arxiv.org/pdf/2311.12022.pdf}
}

@article{fathi2023blockstate,
  title   = {Block-State Transformers},
  author  = {Mahan Fathi and Jonathan Pilault and Orhan Firat and Christopher Pal and Pierre-Luc Bacon and Ross Goroshin},
  year    = {2023},
  journal = {NEURIPS},
  url     = {https://arxiv.org/abs/2306.09539v4},
  pdf     = {https://arxiv.org/pdf/2306.09539.pdf}
}
@article{child2019generating,
  title   = {Generating Long Sequences with Sparse Transformers},
  author  = {Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
  year    = {2019},
  journal = {PREPRINT},
  url     = {https://arxiv.org/abs/1904.10509v1},
  pdf     = {https://arxiv.org/pdf/1904.10509.pdf}
}
@article{shah2024flashattention3,
  title   = {FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author  = {Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2407.08608},
  url     = {https://arxiv.org/abs/2407.08608v2},
  pdf     = {https://arxiv.org/pdf/2407.08608.pdf}
}
@article{jiang2023mistral,
  title   = {Mistral 7B},
  author  = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2310.06825},
  url     = {https://arxiv.org/abs/2310.06825v1},
  pdf     = {https://arxiv.org/pdf/2310.06825.pdf}
}


@inproceedings{varis-bojar-2021-sequence,
  title     = {Sequence Length is a Domain: Length-based Overfitting in Transformer Models},
  author    = {Varis, Dusan and Bojar, Ond{\v{r}}ej},
  editor    = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = {nov},
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.650},
  doi       = {10.18653/v1/2021.emnlp-main.650},
  pages     = {8246-8257},
  abstract  = {Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.},
  pdf       = {https://aclanthology.org/2021.emnlp-main.650.pdf}
}
@article{wang2024mmlupro,
  title   = {MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
  author  = {Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2406.01574},
  url     = {https://arxiv.org/abs/2406.01574v4},
  pdf     = {https://arxiv.org/pdf/2406.01574.pdf}
}
@article{zaheer2020big,
  title   = {Big bird: Transformers for longer sequences},
  author  = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {17283-17297},
  year    = {2020},
  url     = {https://arxiv.org/abs/2007.14062v2},
  pdf     = {https://arxiv.org/pdf/2007.14062.pdf}
}
@inproceedings{fastweight,
  author    = {Imanol Schlag and Kazuki Irie and J{\"{u}}rgen Schmidhuber},
  editor    = {Marina Meila and Tong Zhang},
  title     = {Linear Transformers Are Secretly Fast Weight Programmers},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {9355-9366},
  publisher = {PMLR},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/schlag21a.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/SchlagIS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {https://arxiv.org/pdf/2102.11174.pdf}
}

@article{dao2022hungry,
  title     = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author    = {Tri Dao and Daniel Y. Fu and Khaled Kamal Saab and A. Thomas and A. Rudra and Christopher Ré},
  journal   = {International Conference On Learning Representations},
  year      = {2022},
  doi       = {10.48550/arXiv.2212.14052},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/5a77b508302771fc083bf24e0bcda8553c9b5421},
  url       = {https://arxiv.org/abs/2212.14052v3},
  pdf       = {https://arxiv.org/pdf/2212.14052.pdf}
}
@article{munkhdalai2024leave,
  title   = {Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  author  = {Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2404.07143},
  url     = {https://arxiv.org/abs/2404.07143v1},
  pdf     = {https://arxiv.org/pdf/2404.07143.pdf}
}
@article{wu2022memorizing,
  title     = {Memorizing Transformers},
  author    = {Yuhuai Wu and Markus N. Rabe and DeLesley S. Hutchins and Christian Szegedy},
  journal   = {International Conference On Learning Representations},
  year      = {2022},
  doi       = {10.48550/arXiv.2203.08913},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/0e802c0739771acf70e60d59c2df51cd7e8c50c0},
  url       = {https://arxiv.org/abs/2203.08913v1},
  pdf       = {https://arxiv.org/pdf/2203.08913.pdf}
}

@inproceedings{parallrnn,
  author    = {Eric Martin and Chris Cundy},
  title     = {Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=HyUNwulC-},
  timestamp = {Thu, 25 Jul 2019 14:25:41 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MartinC18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {https://arxiv.org/pdf/1709.04057.pdf}
}
@article{team2024jamba15,
  title   = {Jamba-1.5: Hybrid Transformer-Mamba Models at Scale},
  author  = {Jamba Team and Barak Lenz and Alan Arazi and Amir Bergman and Avshalom Manevich and Barak Peleg and Ben Aviram and Chen Almagor and Clara Fridman and Dan Padnos and Daniel Gissin and Daniel Jannai and Dor Muhlgay and Dor Zimberg and Edden M Gerber and Elad Dolev and Eran Krakovsky and Erez Safahi and Erez Schwartz and Gal Cohen and Gal Shachaf and Haim Rozenblum and Hofit Bata and Ido Blass and Inbal Magar and Itay Dalmedigos and Jhonathan Osin and Julie Fadlon and Maria Rozman and Matan Danos and Michael Gokhman and Mor Zusman and Naama Gidron and Nir Ratner and Noam Gat and Noam Rozen and Oded Fried and Ohad Leshno and Omer Antverg and Omri Abend and Opher Lieber and Or Dagan and Orit Cohavi and Raz Alon and Ro'i Belson and Roi Cohen and Rom Gilad and Roman Glozman and Shahar Lev and Shaked Meirom and Tal Delbari and Tal Ness and Tomer Asida and Tom Ben Gal and Tom Braude and Uriya Pumerantz and Yehoshua Cohen and Yonatan Belinkov and Yuval Globerson and Yuval Peleg Levy and Yoav Shoham},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2408.12570},
  url     = {https://arxiv.org/abs/2408.12570v1},
  pdf     = {https://arxiv.org/pdf/2408.12570.pdf}
}
@article{jelassi2024repeat,
  title   = {Repeat After Me: Transformers are Better than State Space Models at Copying},
  author  = {Samy Jelassi and David Brandfonbrener and Sham M. Kakade and Eran Malach},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2402.01032},
  url     = {https://arxiv.org/abs/2402.01032v1},
  pdf     = {https://arxiv.org/pdf/2402.01032.pdf}
}
@article{akyürek2024incontext,
  title   = {In-Context Language Learning: Architectures and Algorithms},
  author  = {Ekin Akyürek and Bailin Wang and Yoon Kim and Jacob Andreas},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2401.12973},
  url     = {https://arxiv.org/abs/2401.12973v2},
  pdf     = {https://arxiv.org/pdf/2401.12973.pdf}
}
@inproceedings{gss,
  author    = {Harsh Mehta and Ankit Gupta and Ashok Cutkosky and Behnam Neyshabur},
  title     = {Long Range Language Modeling via Gated State Spaces},
  booktitle = {The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher = {OpenReview.net},
  year      = {2023},
  url       = {https://openreview.net/forum?id=5MkYIYCbva},
  timestamp = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/Mehta0CN23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {https://arxiv.org/pdf/2206.13947.pdf}
}
@article{ma2024megalodon,
  title   = {Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length},
  author  = {Xuezhe Ma and Xiaomeng Yang and Wenhan Xiong and Beidi Chen and Lili Yu and Hao Zhang and Jonathan May and Luke Zettlemoyer and Omer Levy and Chunting Zhou},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2404.08801},
  url     = {https://arxiv.org/abs/2404.08801v1},
  pdf     = {https://arxiv.org/pdf/2404.08801.pdf}
}
@article{dauphin2016language,
  title     = {Language Modeling with Gated Convolutional Networks},
  author    = {Y. Dauphin and Angela Fan and Michael Auli and David Grangier},
  journal   = {International Conference On Machine Learning},
  year      = {2016},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/88caa4a0253a8b0076176745ebc072864eab66e1},
  url       = {https://arxiv.org/abs/1612.08083v3},
  pdf       = {https://arxiv.org/pdf/1612.08083.pdf}
}
@article{wen2024rnns,
  title   = {RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval},
  author  = {Kaiyue Wen and Xingyu Dang and Kaifeng Lyu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2402.18510},
  url     = {https://arxiv.org/abs/2402.18510v1},
  pdf     = {https://arxiv.org/pdf/2402.18510.pdf}
}
@article{rajpurkar2016squad,
  title   = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author  = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  year    = {2016},
  journal = {EMNLP},
  url     = {https://arxiv.org/abs/1606.05250v3},
  pdf     = {https://arxiv.org/pdf/1606.05250.pdf}
}
@article{li2023textbooks,
  title   = {Textbooks Are All You Need II: phi-1.5 technical report},
  author  = {Yuanzhi Li and Sébastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2309.05463},
  url     = {https://arxiv.org/abs/2309.05463v1},
  pdf     = {https://arxiv.org/pdf/2309.05463.pdf}
}
@article{shazeer2019fast,
  title   = {Fast Transformer Decoding: One Write-Head is All You Need},
  author  = {Noam Shazeer},
  year    = {2019},
  journal = {arXiv preprint arXiv: 1911.02150},
  url     = {https://arxiv.org/abs/1911.02150v1},
  pdf     = {https://arxiv.org/pdf/1911.02150.pdf}
}
@article{wu2024retrieval,
  title   = {Retrieval Head Mechanistically Explains Long-Context Factuality},
  author  = {Wenhao Wu and Yizhong Wang and Guangxuan Xiao and Hao Peng and Yao Fu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2404.15574},
  url     = {https://arxiv.org/abs/2404.15574v1},
  pdf     = {https://arxiv.org/pdf/2404.15574.pdf}
}
@article{gpt3,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877-1901},
  year    = {2020},
  url     = {https://arxiv.org/abs/2005.14165v4},
  pdf     = {https://arxiv.org/pdf/2005.14165.pdf}
}


@article{govreport2023,
  title={Efficient Attentions for Long Document Summarization},
  author={Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
  journal={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1419--1436},
  year={2021},
  organization={Association for Computational Linguistics}
}

@inproceedings{zhong2021qmsum,
  title={{QMS}um: {A} {N}ew {B}enchmark for {Q}uery-based {M}ulti-domain {M}eeting {S}ummarization},
  author={Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Hassan Awadallah, Ahmed and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and Radev, Dragomir},
  booktitle={North American Association for Computational Linguistics (NAACL)},
  year={2021}
}

@inproceedings{chen2021summscreen,
    title = "{S}umm{S}creen: A Dataset for Abstractive Screenplay Summarization",
    author = "Chen, Mingda  and Chu, Zewei  and Wiseman, Sam  and Gimpel, Kevin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.589",
    pages = "8602--8615",
}

@inproceedings{dasigi2021qasper,
    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    author = "Pradeep Dasigi and Kyle Lo and Iz Beltagy and Arman Cohan and Noah A. Smith and Matt Gardner",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = "June",
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "4599–4610",
    doi = "10.18653/v1/2021.naacl-main.365",
    url = "https://aclanthology.org/2021.naacl-main.365"
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  journal = {arXiv preprint},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:160025533}
}

@article{bubeck2023sparks,
  title   = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  author  = {Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2303.12712},
  url     = {https://arxiv.org/abs/2303.12712v5},
  pdf     = {https://arxiv.org/pdf/2303.12712.pdf}
}
@article{openai2023gpt4,
  title   = {GPT-4 Technical Report},
  author  = {OpenAI},
  year    = {2023},
  journal = {PREPRINT},
  url     = {https://arxiv.org/abs/2303.08774v4},
  pdf     = {https://arxiv.org/pdf/2303.08774.pdf}
}

@article{bahdanau2014neural,
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal   = {International Conference On Learning Representations},
  year      = {2014},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5},
  url       = {https://arxiv.org/abs/1409.0473v7},
  pdf       = {https://arxiv.org/pdf/1409.0473.pdf}
}
@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}
@article{ainslie2023gqa,
  title     = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author    = {J. Ainslie and J. Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr'on and Sumit K. Sanghai},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2023},
  doi       = {10.48550/arXiv.2305.13245},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200},
  url       = {https://arxiv.org/abs/2305.13245v3},
  pdf       = {https://arxiv.org/pdf/2305.13245.pdf}
}


@article{yang2023gated,
  title={Gated linear attention transformers with hardware-efficient training},
  author={Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  journal={arXiv preprint arXiv:2312.06635},
  year={2023}
}
@article{press2021train,
  title     = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author    = {Ofir Press and Noah A. Smith and M. Lewis},
  journal   = {International Conference On Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/9ca329408813d209b1dcb36936f7f9cba82506bd},
  url       = {https://arxiv.org/abs/2108.12409v2},
  pdf       = {https://arxiv.org/pdf/2108.12409.pdf}
}
@article{beltagy2020longformer,
  title   = {Longformer: The Long-Document Transformer},
  author  = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2004.05150},
  url     = {https://arxiv.org/abs/2004.05150v2},
  pdf     = {https://arxiv.org/pdf/2004.05150.pdf}
}
@article{sun2023retentive,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}
@article{dai2021knowledge,
  title   = {Knowledge Neurons in Pretrained Transformers},
  author  = {Damai Dai and Li Dong and Yaru Hao and Zhifang Sui and Baobao Chang and Furu Wei},
  year    = {2022},
  journal = {ACL},
  url     = {https://arxiv.org/abs/2104.08696v2},
  pdf     = {https://arxiv.org/pdf/2104.08696.pdf}
}
@article{dao2023flashattention2,
  title   = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author  = {Tri Dao},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.08691},
  url     = {https://arxiv.org/abs/2307.08691v1},
  pdf     = {https://arxiv.org/pdf/2307.08691.pdf}
}
@article{touvron2023llama,
  title   = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author  = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.09288},
  url     = {https://arxiv.org/abs/2307.09288v2},
  pdf     = {https://arxiv.org/pdf/2307.09288.pdf}
}
@article{team2024gemma,
  title   = {Gemma: Open Models Based on Gemini Research and Technology},
  author  = {Gemma Team},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2403.08295},
  url     = {https://arxiv.org/abs/2403.08295v1},
  pdf     = {https://arxiv.org/pdf/2403.08295.pdf}
}

@article{botev2024recurrentgemma,
  title   = {RecurrentGemma: Moving Past Transformers for Efficient Open Language Models},
  author  = {Aleksandar Botev and Soham De and Samuel L Smith and Anushan Fernando and George-Cristian Muraru and Ruba Haroun and Leonard Berrada and Razvan Pascanu and Pier Giuseppe Sessa and Robert Dadashi and Léonard Hussenot and Johan Ferret and Sertan Girgin and Olivier Bachem and Alek Andreev and Kathleen Kenealy and Thomas Mesnard and Cassidy Hardin and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Armand Joulin and Noah Fiedel and Evan Senter and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and David Budden and Arnaud Doucet and Sharad Vikram and Adam Paszke and Trevor Gale and Sebastian Borgeaud and Charlie Chen and Andy Brock and Antonia Paterson and Jenny Brennan and Meg Risdal and Raj Gundluru and Nesh Devanathan and Paul Mooney and Nilay Chauhan and Phil Culliton and Luiz GUStavo Martins and Elisa Bandy and David Huntsperger and Glenn Cameron and Arthur Zucker and Tris Warkentin and Ludovic Peran and Minh Giang and Zoubin Ghahramani and Clément Farabet and Koray Kavukcuoglu and Demis Hassabis and Raia Hadsell and Yee Whye Teh and Nando de Frietas},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2404.07839},
  url     = {https://arxiv.org/abs/2404.07839v1},
  pdf     = {https://arxiv.org/pdf/2404.07839.pdf}
}

@article{mbpp,
  title   = {Program Synthesis with Large Language Models},
  author  = {Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
  year    = {2021},
  journal = {arXiv preprint arXiv: 2108.07732},
  url     = {https://arxiv.org/abs/2108.07732v1},
  pdf     = {https://arxiv.org/pdf/2108.07732.pdf}
}

@inproceedings{lin-etal-2022-truthfulqa,
  title     = {{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods},
  author    = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  editor    = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {may},
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.acl-long.229},
  doi       = {10.18653/v1/2022.acl-long.229},
  pages     = {3214-3252},
  abstract  = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58{\%} of questions, while human performance was 94{\%}. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  pdf       = {https://aclanthology.org/2022.acl-long.229.pdf}
}

@article{openbookqa,
  title     = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author    = {Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  doi       = {10.18653/v1/D18-1260},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/1536e8958697c5364f68b2e2448905dbbeb3a0ca},
  url       = {https://arxiv.org/abs/1809.02789v1},
  pdf       = {https://arxiv.org/pdf/1809.02789.pdf}
}

@inproceedings{boolq,
  author    = {Christopher Clark and Kenton Lee and Ming{-}Wei Chang and Tom Kwiatkowski and Michael Collins and Kristina Toutanova},
  editor    = {Jill Burstein and Christy Doran and Thamar Solorio},
  title     = {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)},
  pages     = {2924-2936},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1300},
  doi       = {10.18653/V1/N19-1300},
  timestamp = {Tue, 16 Aug 2022 23:04:27 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/ClarkLCK0T19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {https://arxiv.org/pdf/1905.10044.pdf}
}

@article{zellers2019hellaswag,
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  author    = {Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2019},
  doi       = {10.18653/v1/P19-1472},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad},
  url       = {https://arxiv.org/abs/1905.07830v1},
  pdf       = {https://arxiv.org/pdf/1905.07830.pdf}
}

@article{siqa,
  title   = {SocialIQA: Commonsense Reasoning about Social Interactions},
  author  = {Maarten Sap and Hannah Rashkin and Derek Chen and Ronan LeBras and Yejin Choi},
  year    = {2019},
  journal = {arXiv preprint arXiv: 1904.09728},
  url     = {https://arxiv.org/abs/1904.09728v3},
  pdf     = {https://arxiv.org/pdf/1904.09728.pdf}
}
@article{winogrande,
  title     = {Winogrande: An adversarial winograd schema challenge at scale},
  author    = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal   = {Communications of the ACM},
  volume    = {64},
  number    = {9},
  pages     = {99-106},
  year      = {2021},
  publisher = {ACM New York, NY, USA},
  url       = {https://arxiv.org/abs/1907.10641v2},
  pdf       = {https://arxiv.org/pdf/1907.10641.pdf}
}
@inproceedings{piqa,
  author    = {Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
  title     = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020},
  pages     = {7432-7439},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi       = {10.1609/AAAI.V34I05.6239},
  timestamp = {Thu, 11 Apr 2024 13:33:56 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {https://arxiv.org/pdf/1911.11641.pdf}
}
@article{arc,
  title   = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author  = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  year    = {2018},
  journal = {arXiv preprint arXiv: 1803.05457},
  url     = {https://arxiv.org/abs/1803.05457v1},
  pdf     = {https://arxiv.org/pdf/1803.05457.pdf}
}
@article{softplus,
  journal = {2015 International Joint Conference on Neural Networks (IJCNN)},
  pages   = {1-4},
  doi     = {10.1109/IJCNN.2015.7280459},
  title   = {Improving deep neural networks using softplus units},
  year    = {2015},
  author  = {Hao Zheng and Zhanlei Yang and Wenju Liu and Jizhong Liang and Yanpeng Li},
  url     = {https://ieeexplore.ieee.org/document/7280459},
  pdf     = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7280459}
}
@article{he2019depthwise,
  title   = {Depth-wise Decomposition for Accelerating Separable Convolutions in Efficient Convolutional Neural Networks},
  author  = {Yihui He and Jianing Qian and Jianren Wang and Cindy X. Le and Congrui Hetang and Qi Lyu and Wenping Wang and Tianwei Yue},
  year    = {2019},
  journal = {arXiv preprint arXiv: 1910.09455},
  url     = {https://arxiv.org/abs/1910.09455v3},
  pdf     = {https://arxiv.org/pdf/1910.09455.pdf}
}
@article{poli2023hyena,
  title     = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  author    = {Michael Poli and Stefano Massaroli and Eric Q. Nguyen and Daniel Y. Fu and Tri Dao and S. Baccus and Y. Bengio and Stefano Ermon and Christopher Ré},
  journal   = {International Conference On Machine Learning},
  year      = {2023},
  doi       = {10.48550/arXiv.2302.10866},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/998ac3e945857cf2676ee7efdbaf443a0c6f820a},
  url       = {https://arxiv.org/abs/2302.10866v3},
  pdf       = {https://arxiv.org/pdf/2302.10866.pdf}
}
@article{resnet,
  title   = {Deep Residual Learning for Image Recognition},
  author  = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year    = {2016},
  journal = {CVPR},
  url     = {https://arxiv.org/abs/1512.03385v1},
  pdf     = {https://arxiv.org/pdf/1512.03385.pdf}
}

@inproceedings{prenorm,
  author    = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie{-}Yan Liu},
  title     = {On Layer Normalization in the Transformer Architecture},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {10524-10533},
  publisher = {PMLR},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/xiong20b.html},
  timestamp = {Fri, 10 Nov 2023 21:09:38 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/XiongYHZZXZLWL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {https://arxiv.org/pdf/2002.04745.pdf}
}

@article{gsm8k,
  title   = {Training Verifiers to Solve Math Word Problems},
  author  = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  year    = {2021},
  journal = {arXiv preprint arXiv: 2110.14168},
  url     = {https://arxiv.org/abs/2110.14168v2},
  pdf     = {https://arxiv.org/pdf/2110.14168.pdf}
}

@article{humaneval,
  title   = {Evaluating Large Language Models Trained on Code},
  author  = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year    = {2021},
  journal = {arXiv preprint arXiv: 2107.03374},
  url     = {https://arxiv.org/abs/2107.03374v2},
  pdf     = {https://arxiv.org/pdf/2107.03374.pdf}
}
@inproceedings{mmlu,
  author    = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  title     = {Measuring Massive Multitask Language Understanding},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=d7KBjmI3GmQ},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HendrycksBBZMSS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {https://arxiv.org/pdf/2009.03300.pdf}
}

@article{shazeer2020glu,
  title   = {GLU Variants Improve Transformer},
  author  = {Noam Shazeer},
  year    = {2020},
  journal = {arXiv preprint arXiv: 2002.05202},
  url     = {https://arxiv.org/abs/2002.05202v1},
  pdf     = {https://arxiv.org/pdf/2002.05202.pdf}
}
@article{qin2024hgrn2,
  title   = {HGRN2: Gated Linear RNNs with State Expansion},
  author  = {Zhen Qin and Songlin Yang and Weixuan Sun and Xuyang Shen and Dong Li and Weigao Sun and Yiran Zhong},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2404.07904},
  url     = {https://arxiv.org/abs/2404.07904v1},
  pdf     = {https://arxiv.org/pdf/2404.07904.pdf}
}
@software{yang2024fla,
  title  = {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},
  author = {Yang, Songlin and Zhang, Yu},
  url    = {https://github.com/sustcsonglin/flash-linear-attention},
  month  = jan,
  year   = {2024}
}
@misc{proofpile,
    author = {Zhangir Azerbayev, Edward Ayers and Bartosz Piotrowski},
    title = {Proof-Pile},
    year = {2022},
    note = {URL: \url{https://github.com/zhangir-azerbayev/proof-pile }}
}
@article{holtzman2019curious,
  title     = {The Curious Case of Neural Text Degeneration},
  author    = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
  journal   = {International Conference on Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cf4aa38ae31b43fd07abe13b4ffdb265babb7be1},
  url       = {https://arxiv.org/abs/1904.09751v2},
  pdf       = {https://arxiv.org/pdf/1904.09751.pdf}
}
@article{rms,
  title     = {Root Mean Square Layer Normalization},
  author    = {Biao Zhang and Rico Sennrich},
  journal   = {Neural Information Processing Systems},
  year      = {2019},
  doi       = {10.5167/UZH-177483},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/10eda4521c032adabaa8e70d6569e17370b29dcd},
  url       = {https://arxiv.org/abs/1910.07467v1},
  pdf       = {https://arxiv.org/pdf/1910.07467.pdf}
}

@article{katsch2023gateloop,
  title   = {GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling},
  author  = {Tobias Katsch},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2311.01927},
  url     = {https://arxiv.org/abs/2311.01927v1},
  pdf     = {https://arxiv.org/pdf/2311.01927.pdf}
}
@article{lieber2024jamba,
  title   = {Jamba: A Hybrid Transformer-Mamba Language Model},
  author  = {Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2403.19887},
  url     = {https://arxiv.org/abs/2403.19887v1},
  pdf     = {https://arxiv.org/pdf/2403.19887.pdf}
}

@article{pk,
  title   = {Landmark Attention: Random-Access Infinite Context Length for Transformers},
  author  = {Amirkeivan Mohtashami and Martin Jaggi},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2305.16300},
  url     = {https://arxiv.org/abs/2305.16300v2},
  pdf     = {https://arxiv.org/pdf/2305.16300.pdf}
}
@article{lru,
  title     = {Resurrecting Recurrent Neural Networks for Long Sequences},
  author    = {Antonio Orvieto and Samuel L. Smith and Albert Gu and Anushan Fernando and Caglar Gulcehre and Razvan Pascanu and Soham De},
  journal   = {International Conference on Machine Learning},
  year      = {2023},
  doi       = {10.48550/arXiv.2303.06349},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/f393aff1593c2d370ec0ae004910d18e40524967},
  url       = {https://arxiv.org/abs/2303.06349v1},
  pdf       = {https://arxiv.org/pdf/2303.06349.pdf}
}

@article{hgrn,
  title     = {Hierarchically Gated Recurrent Neural Network for Sequence Modeling},
  author    = {Zhen Qin and Songlin Yang and Yiran Zhong},
  journal   = {Neural Information Processing Systems},
  year      = {2023},
  doi       = {10.48550/arXiv.2311.04823},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/434d751d355d7a7c20efa570e785c76286245e77},
  url       = {https://arxiv.org/abs/2311.04823v1},
  pdf       = {https://arxiv.org/pdf/2311.04823.pdf}
}
@misc{slimpajama,
    author = {Daria Soboleva and Faisal Al-Khateeb and Robert Myers and Jacob R Steeves and Joel Hestness and Nolan Dey},
    title = {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama},
    year = {2023},
    note = {URL: \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}}
}
@article{jin2024llm,
  title   = {LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning},
  author  = {Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2401.01325},
  url     = {https://arxiv.org/abs/2401.01325v1},
  pdf     = {https://arxiv.org/pdf/2401.01325.pdf}
}
@article{merrill2024illusion,
  title   = {The Illusion of State in State-Space Models},
  author  = {William Merrill and Jackson Petty and Ashish Sabharwal},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2404.08819},
  url     = {https://arxiv.org/abs/2404.08819v1},
  pdf     = {https://arxiv.org/pdf/2404.08819.pdf}
}
@article{ren2023sparse,
  title   = {Sparse Modular Activation for Efficient Sequence Modeling},
  author  = {Liliang Ren and Yang Liu and Shuohang Wang and Yichong Xu and Chenguang Zhu and ChengXiang Zhai},
  year    = {2023},
  journal = {NEURIPS},
  url     = {https://arxiv.org/abs/2306.11197v1},
  pdf     = {https://arxiv.org/pdf/2306.11197.pdf}
}
@article{han2023lminfinite,
  title   = {LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models},
  author  = {Chi Han and Qifan Wang and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2308.16137},
  url     = {https://arxiv.org/abs/2308.16137v3},
  pdf     = {https://arxiv.org/pdf/2308.16137.pdf}
}
@article{xiao2023efficient,
  title   = {Efficient Streaming Language Models with Attention Sinks},
  author  = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2309.17453},
  url     = {https://arxiv.org/abs/2309.17453v1},
  pdf     = {https://arxiv.org/pdf/2309.17453.pdf}
}
@article{abdin2024phi3,
  title   = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author  = {Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah and Hany Awadalla and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Caio César Teodoro Mendes and Weizhu Chen and Vishrav Chaudhary and Parul Chopra and Allie Del Giorno and Gustavo de Rosa and Matthew Dixon and Ronen Eldan and Dan Iter and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Jamie Huynh and Mojan Javaheripi and Xin Jin and Piero Kauffmann and Nikos Karampatziakis and Dongwoo Kim and Mahoud Khademi and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Chen Liang and Weishung Liu and Eric Lin and Zeqi Lin and Piyush Madan and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Corby Rosset and Sambudha Roy and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Xia Song and Olatunji Ruwase and Xin Wang and Rachel Ward and Guanhua Wang and Philipp Witte and Michael Wyatt and Can Xu and Jiahang Xu and Sonali Yadav and Fan Yang and Ziyi Yang and Donghan Yu and Chengruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yunan Zhang and Xiren Zhou},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2404.14219},
  url     = {https://arxiv.org/abs/2404.14219v1},
  pdf     = {https://arxiv.org/pdf/2404.14219.pdf}
}
@article{dubey2024llama,
  title   = {The Llama 3 Herd of Models},
  author  = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzmán and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Govind Thattai and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Maria Tsimpoukelli and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2407.21783},
  url     = {https://arxiv.org/abs/2407.21783v1},
  pdf     = {https://arxiv.org/pdf/2407.21783.pdf}
}
@misc{llama3,
    author = {MetaAI},
    title = {Introducing Meta Llama 3: The most capable openly available LLM to date},
    year = {2024},
    note = {URL: \url{https://ai.meta.com/blog/meta-llama-3/}}
}
@article{chen2023extending,
  title   = {Extending Context Window of Large Language Models via Positional Interpolation},
  author  = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2306.15595},
  url     = {https://arxiv.org/abs/2306.15595v2},
  pdf     = {https://arxiv.org/pdf/2306.15595.pdf}
}
@article{amer2019review,
  title={A review of modularization techniques in artificial neural networks},
  author={Amer, Mohammed and Maul, Tom{\'a}s},
  journal={Artificial Intelligence Review},
  volume={52},
  pages={527--561},
  year={2019},
  publisher={Springer}
}
@article{jaegle2021perceiver,
  title     = {Perceiver: General Perception with Iterative Attention},
  author    = {Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and João Carreira},
  journal   = {International Conference On Machine Learning},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/63a9daf15ae2d4c1a7859d3105c9e6710903e072},
  url       = {https://arxiv.org/abs/2103.03206v2},
  pdf       = {https://arxiv.org/pdf/2103.03206.pdf}
}
@article{goyal2019recurrent,
  title     = {Recurrent Independent Mechanisms},
  author    = {Anirudh Goyal and Alex Lamb and Jordan Hoffmann and Shagun Sodhani and S. Levine and Yoshua Bengio and B. Scholkopf},
  journal   = {International Conference on Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/67a9dde04f367efc903b6d06097df9bdd9887ae7},
  url       = {https://arxiv.org/abs/1909.10893v6},
  pdf       = {https://arxiv.org/pdf/1909.10893.pdf}
}
@article{radam,
  title     = {On the Variance of the Adaptive Learning Rate and Beyond},
  author    = {Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
  journal   = {International Conference on Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/2bf7c350a8280e7c593d46a60127f99b21517121},
  url       = {https://arxiv.org/abs/1908.03265v4},
  pdf       = {https://arxiv.org/pdf/1908.03265.pdf}
}
@inproceedings{s5,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}

@article{su2021roformer,
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
  year    = {2021},
  journal = {arXiv preprint arXiv: 2104.09864}
}
@article{baevski2020wav2vec,
  title   = {wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author  = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {12449-12460},
  year    = {2020}
}

@inproceedings{Seong2018TowardsFL,
  title={Towards Flatter Loss Surface via Nonmonotonic Learning Rate Scheduling},
  author={Sihyeon Seong and Yegang Lee and Youngwook Kee and Dongyoon Han and Junmo Kim},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  year={2018}
}

@article{shaw2018selfattention,
  title   = {Self-Attention with Relative Position Representations},
  author  = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year    = {2018},
  journal = {NAACL}
}

@inproceedings{gsoft,
  author    = {Eric Jang and Shixiang Gu and Ben Poole},
  title     = {Categorical Reparameterization with Gumbel-Softmax},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=rkE3y85ee},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{moe,
  journal = {Neural Computation},
  volume  = {3},
  pages   = {79-87},
  doi     = {10.1162/neco.1991.3.1.79},
  title   = {Adaptive Mixtures of Local Experts},
  year    = {1991},
  author  = {Robert A. Jacobs and Michael I. Jordan and Steven J. Nowlan and Geoffrey E. Hinton}
}
@article{dai2019transformerxl,
  title   = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  author  = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  year    = {2019},
  journal = {ACL}
}
@article{lepikhin2020gshard,
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author    = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and M. Krikun and Noam M. Shazeer and Z. Chen},
  journal   = {International Conference On Learning Representations},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/1882f194cb43828852cc052887671e55a80f945a}
}

@article{fedus2022switch,
  title     = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author    = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal   = {The Journal of Machine Learning Research},
  volume    = {23},
  number    = {1},
  pages     = {5232-5270},
  year      = {2022},
  publisher = {JMLRORG}
}

@article{zhou2022mixture,
  title   = {Mixture-of-experts with expert choice routing},
  author  = {Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {7103-7114},
  year    = {2022}
}

@article{roy2020efficient,
  title     = {Efficient Content-Based Sparse Attention with Routing Transformers},
  author    = {Aurko Roy and M. Saffar and Ashish Vaswani and David Grangier},
  journal   = {International Conference On Topology, Algebra And Categories In Logic},
  year      = {2020},
  doi       = {10.1162/tacl_a_00353},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/657329c633709dd1ac34a30d57341b186b1a47c2}
}

@article{hua2022transformer,
  title     = {Transformer Quality in Linear Time},
  author    = {Weizhe Hua and Zihang Dai and Hanxiao Liu and Quoc V. Le},
  journal   = {International Conference On Machine Learning},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/dc0102a51a9d33e104a4a3808a18cf17f057228c}
}

@article{act,
  title     = {Adaptive Computation Time for Recurrent Neural Networks},
  author    = {A. Graves},
  journal   = {ARXIV.ORG},
  year      = {2016},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/04cca8e341a5da42b29b0bc831cb25a0f784fa01}
}

@article{expire,
  title     = {Not All Memories are Created Equal: Learning to Forget by Expiring},
  author    = {Sainbayar Sukhbaatar and Da Ju and Spencer Poff and Stephen Roller and Arthur D. Szlam and J. Weston and Angela Fan},
  journal   = {International Conference On Machine Learning},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/64a29bee2e1ad29547d590a3cc26274f4c537145}
}

@inproceedings{ren-etal-2022-language,
  title     = {Language Model Pre-Training with Sparse Latent Typing},
  author    = {Ren, Liliang and Zhang, Zixuan and Wang, Han and Voss, Clare and Zhai, ChengXiang and Ji, Heng},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = {dec},
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.96},
  pages     = {1480-1494},
  abstract  = {Modern large-scale Pre-trained Language Models (PLMs) have achieved tremendous success on a wide range of downstream tasks. However, most of the LM pre-training objectives only focus on text reconstruction, but have not sought to learn latent-level interpretable representations of sentences. In this paper, we manage to push the language models to obtain a deeper understanding of sentences by proposing a new pre-training objective, Sparse Latent Typing, which enables the model to sparsely extract sentence-level keywords with diverse latent types. Experimental results show that our model is able to learn interpretable latent type categories in a self-supervised manner without using any external knowledge. Besides, the language model pre-trained with such an objective also significantly improves Information Extraction related downstream tasks in both supervised and few-shot settings. Our code is publicly available at https://github.com/renll/SparseLT.}
}
@article{barrett2020implicit,
  title     = {Implicit Gradient Regularization},
  author    = {D. Barrett and B. Dherin},
  journal   = {International Conference On Learning Representations},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/060eb1ad5da6a059320c244532ad5c9c0ab52485}
}

@inproceedings{guan-etal-2022-transkimmer,
  title     = {Transkimmer: Transformer Learns to Layer-wise Skim},
  author    = {Guan, Yue and Li, Zhengyi and Leng, Jingwen and Lin, Zhouhan and Guo, Minyi},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {may},
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.acl-long.502},
  doi       = {10.18653/v1/2022.acl-long.502},
  pages     = {7275-7286},
  abstract  = {Transformer architecture has become the de-facto model for many machine learning tasks from natural language processing and computer vision. As such, improving its computational efficiency becomes paramount. One of the major computational inefficiency of Transformer based models is that they spend the identical amount of computation throughout all layers. Prior works have proposed to augment the Transformer model with the capability of skimming tokens to improve its computational efficiency. However, they suffer from not having effectual and end-to-end optimization of the discrete skimming predictor. To address the above limitations, we propose the Transkimmer architecture, which learns to identify hidden state tokens that are not required by each layer. The skimmed tokens are then forwarded directly to the final output, thus reducing the computation of the successive layers. The key idea in Transkimmer is to add a parameterized predictor before each layer that learns to make the skimming decision. We also propose to adopt reparameterization trick and add skim loss for the end-to-end training of Transkimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark compared with vanilla BERT-base baseline with less than 1{\%} accuracy degradation.}
}
@article{routing,
  title     = {Efficient Content-Based Sparse Attention with Routing Transformers},
  author    = {Aurko Roy and M. Saffar and Ashish Vaswani and David Grangier},
  journal   = {International Conference On Topology, Algebra And Categories In Logic},
  year      = {2020},
  doi       = {10.1162/tacl_a_00353},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/657329c633709dd1ac34a30d57341b186b1a47c2}
}
@inproceedings{Atkinson1968HumanMA,
  title={Human Memory: A Proposed System and its Control Processes},
  author={Richard C. Atkinson and Richard M. Shiffrin},
  booktitle={The psychology of learning and motivation},
  year={1968}
}
@article{ainslie2020etc,
  title     = {ETC: Encoding Long and Structured Inputs in Transformers},
  author    = {J. Ainslie and Santiago Ontañón and Chris Alberti and V. Cvicek and Zachary Kenneth Fisher and Philip Pham and Anirudh Ravula and Sumit K. Sanghai and Qifan Wang and Li Yang},
  journal   = {Conference On Empirical Methods In Natural Language Processing},
  year      = {2020},
  doi       = {10.18653/v1/2020.emnlp-main.19},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/56676aef356ebb13cba77fc9e4d70760fbc151f5}
}
@article{so2021primer,
  title     = {Primer: Searching for Efficient Transformers for Language Modeling},
  author    = {David R. So and Wojciech Ma'nke and Hanxiao Liu and Zihang Dai and Noam M. Shazeer and Quoc V. Le},
  journal   = {ARXIV.ORG},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/4a8964ea0de47010fb458021b68fa3ef5c4b77b2}
}
@article{silu,
  title     = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
  author    = {Stefan Elfwing and E. Uchibe and K. Doya},
  journal   = {Neural Networks},
  year      = {2017},
  doi       = {10.1016/j.neunet.2017.12.012},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b587ee7c802a5bd222a69090f59285e0dfdb29f1}
}
@article{paszke2019pytorch,
  title   = {Pytorch: An imperative style, high-performance deep learning library},
  author  = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}
@article{gu2021efficiently,
  title     = {Efficiently Modeling Long Sequences with Structured State Spaces},
  author    = {Albert Gu and Karan Goel and Christopher R'e},
  journal   = {International Conference On Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51}
}
@article{hasani2022liquid,
  title={Liquid structural state-space models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}
@article{s4d,
  title     = {On the Parameterization and Initialization of Diagonal State Space Models},
  author    = {Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2206.11893},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ca444821352a4bd91884413d8070446e2960715a}
}
@article{zuo2022efficient,
  title   = {Efficient Long Sequence Modeling via State Space Augmented Transformer},
  author  = {Simiao Zuo and Xiaodong Liu and Jian Jiao and Denis Charles and Eren Manavoglu and Tuo Zhao and Jianfeng Gao},
  year    = {2022},
  journal = {arXiv preprint arXiv: 2212.08136},
  url     = {https://arxiv.org/abs/2212.08136v1},
  pdf     = {https://arxiv.org/pdf/2212.08136.pdf}
}
@article{arora2023zoology,
  title   = {Zoology: Measuring and Improving Recall in Efficient Language Models},
  author  = {Simran Arora and Sabri Eyuboglu and Aman Timalsina and Isys Johnson and Michael Poli and James Zou and Atri Rudra and Christopher Ré},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2312.04927},
  url     = {https://arxiv.org/abs/2312.04927v1},
  pdf     = {https://arxiv.org/pdf/2312.04927.pdf}
}
@article{squality,
  title     = {SQuALITY: Building a Long-Document Summarization Dataset the Hard Way},
  author    = {Alex Wang and Richard Yuanzhe Pang and Angelica Chen and Jason Phang and Samuel R. Bowman},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2022},
  doi       = {10.48550/arXiv.2205.11465},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b7422b7a7830cd899b47b03e514d8151ffb74c03},
  url       = {https://arxiv.org/abs/2205.11465v1},
  pdf       = {https://arxiv.org/pdf/2205.11465.pdf}
}

@article{cot,
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author    = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and E. Chi and F. Xia and Quoc Le and Denny Zhou},
  journal   = {Neural Information Processing Systems},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5},
  url       = {https://arxiv.org/abs/2201.11903v6},
  pdf       = {https://arxiv.org/pdf/2201.11903.pdf}
}

@article{gupta2022diagonal,
  title     = {Diagonal State Spaces are as Effective as Structured State Spaces},
  author    = {Ankit Gupta and Jonathan Berant},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2203.14343},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/71e15a9a52dcafca57bff5f310b95e2c7d0cfc87}
}


@inproceedings{hippo,
  author    = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1474-1487},
  publisher = {Curran Associates, Inc.},
  title     = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@article{tay2020long,
  title     = {Long Range Arena: A Benchmark for Efficient Transformers},
  author    = {Yi Tay and M. Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and J. Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
  journal   = {International Conference On Learning Representations},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/7e9ff94476f41041c75e253e84f487db00e9c861}
}

@article{xu2022survey,
  title     = {A Survey on Dynamic Neural Networks for Natural Language Processing},
  author    = {Canwen Xu and Julian McAuley},
  journal   = {EMNLP Findings},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568}
}

@article{gale2020sparse,
  title     = {Sparse GPU Kernels for Deep Learning},
  author    = {Trevor Gale and M. Zaharia and C. Young and Erich Elsen},
  journal   = {International Conference For High Performance Computing, Networking, Storage And Analysis},
  year      = {2020},
  doi       = {10.1109/SC41405.2020.00021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328}
}

@article{dynamic,
  journal = {IEEE Transactions on Computers},
  volume  = {71},
  pages   = {3165-3178},
  doi     = {10.1109/TC.2022.3208206},
  title   = {Dynamic Sparse Attention for Scalable Transformer Acceleration},
  year    = {2022},
  author  = {Liu Liu and Zheng Qu and Zhaodong Chen and Fengbin Tu and Yufei Ding and Yuan Xie}
}

@article{vardasbi2023state,
  title     = {State Spaces Aren't Enough: Machine Translation Needs Attention},
  author    = {Ali Vardasbi and Telmo Pires and Robin M. Schmidt and Stephan Peitz},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2304.12776},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/01e721df7fcf8b664deb2cdc97ff58d65553af6b}
}

@article{warden2018speech,
  title   = {Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition},
  author  = {Pete Warden},
  year    = {2018},
  journal = {arXiv preprint arXiv: Arxiv-1804.03209}
}

@article{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal   = {NIPS},
  year      = {2017},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776}
}

@inproceedings{
mega,
title={Mega: Moving Average Equipped Gated Attention},
author={Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=qNLe3iq2El}
}


@inproceedings{
h3,
title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
author={Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=COZDy0WYGg}
}

@inproceedings{gc,
 author = {Dherin, Benoit and Munn, Michael and Rosca, Mihaela and Barrett, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {2333--2349},
 publisher = {Curran Associates, Inc.},
 title = {Why neural networks find simple solutions:  The many regularizers of geometric complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/0ff3502bb29570b219967278db150a50-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{
unitrans,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr}
}

%DA
@article{ben2010theoryDA,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  number={1},
  pages={151--175},
  year={2010},
  publisher={Springer}
}
@inproceedings{adw,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{umnn,
  title={Unconstrained Monotonic Neural Networks},
  author={Wehenkel, Antoine and Louppe, Gilles},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1545--1555},
  year={2019}
}
@InProceedings{Ganin2015DA,
  title = 	 {Unsupervised Domain Adaptation by Backpropagation},
  author = 	 {Ganin, Yaroslav and Lempitsky, Victor},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1180--1189},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ganin15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ganin15.html},
  abstract = 	 {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.}
}

%DNN
@article{dann,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The journal of machine learning research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{ae,
author = {Ballard, Dana H.},
title = {Modular Learning in Neural Networks},
year = {1987},
isbn = {0934613427},
publisher = {AAAI Press},
abstract = {In the development of large-scale knowledge networks much recent progress has been
inspired by connections to neurobiology. An important component of any "neural" network
is an accompanying learning algorithm. Such an algorithm, to be biologically plausible,
must work for very large numbers of units. Studies of large-scale systems have so
far been restricted to systems Without internal units (units With no direct connections
to the input or output). Internal units are crucial to such systems as they are the
means by which a system can encode high-order regularities (or invariants) that are
Implicit in its inputs and outputs. Computer simulations of learning using internal
units have been restricted to small-scale systems. This paper describes away of coupling
autoassociative learning modules Into hierarchies that should greatly improve the
performance of learning algorithms in large-scale systems. The Idea has been tested
experimentally with positive results.},
booktitle = {Proceedings of the Sixth National Conference on Artificial Intelligence - Volume 1},
pages = {279–284},
numpages = {6},
location = {Seattle, Washington},
series = {AAAI'87}
}

@inproceedings{vae,
  author    = {Diederik P. Kingma and
               Max Welling},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%TL
@misc{zhang2021quantifying,
      title={Quantifying and Improving Transferability in Domain Generalization}, 
      author={Guojun Zhang and Han Zhao and Yaoliang Yu and Pascal Poupart},
      year={2021},
      eprint={2106.03632},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@ARTICLE{pan2010survey,  author={Pan, Sinno Jialin and Yang, Qiang},  journal={IEEE Transactions on Knowledge and Data Engineering},   title={A Survey on Transfer Learning},   year={2010},  volume={22},  number={10},  pages={1345-1359},  doi={10.1109/TKDE.2009.191}}

@InProceedings{Blitzer2008DA,
  author    = {Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Wortman, Jennifer},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning Bounds for Domain Adaptation},
  year      = {2008},
  editor    = {J. Platt and D. Koller and Y. Singer and S. Roweis},
  publisher = {Curran Associates, Inc.},
  volume    = {20},
  url       = {https://proceedings.neurips.cc/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Paper.pdf},
}

@InProceedings{BenDavid2007RepDA,
  author    = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Analysis of Representations for Domain Adaptation},
  year      = {2007},
  editor    = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
  publisher = {MIT Press},
  volume    = {19},
  url       = {https://proceedings.neurips.cc/paper/2006/file/b1b0432ceafb0ce714426e9114852ac7-Paper.pdf},
}

@Article{Bastani2021ProxiesTransferLearning,
  author   = {Bastani, Hamsa},
  journal  = {Management Science},
  title    = {Predicting with Proxies: Transfer Learning in High Dimension},
  year     = {2021},
  number   = {5},
  pages    = {2964-2984},
  volume   = {67},
  abstract = {Predictive analytics is increasingly used to guide decision making in many applications. However, in practice, we often have limited data on the true predictive task of interest and must instead rely on more abundant data on a closely related proxy predictive task. For example, e-commerce platforms use abundant customer click data (proxy) to make product recommendations rather than the relatively sparse customer purchase data (true outcome of interest); alternatively, hospitals often rely on medical risk scores trained on a different patient population (proxy) rather than their own patient population (true cohort of interest) to assign interventions. Yet, not accounting for the bias in the proxy can lead to suboptimal decisions. Using real data sets, we find that this bias can often be captured by a sparse function of the features. Thus, we propose a novel two-step estimator that uses techniques from high-dimensional statistics to efficiently combine a large amount of proxy data and a small amount of true data. We prove upper bounds on the error of our proposed estimator and lower bounds on several heuristics used by data scientists; in particular, our proposed estimator can achieve the same accuracy with exponentially less true data (in the number of features d). Finally, we demonstrate the effectiveness of our approach on e-commerce and healthcare data sets; in both cases, we achieve significantly better predictive accuracy as well as managerial insights into the nature of the bias in the proxy data.This paper was accepted by George Shanthikumar, big data and analytics.},
  doi      = {10.1287/mnsc.2020.3729},
  eprint   = {https://doi.org/10.1287/mnsc.2020.3729},
  url      = {https://doi.org/10.1287/mnsc.2020.3729},
}

@article{Bastani2019MetaDP,
  author    = {Hamsa Bastani and
               David Simchi{-}Levi and
               Ruihao Zhu},
  title     = {Meta Dynamic Pricing: Learning Across Experiments},
  journal   = {CoRR},
  volume    = {abs/1902.10918},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.10918},
  eprinttype = {arXiv},
  eprint    = {1902.10918},
  timestamp = {Tue, 21 May 2019 18:03:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-10918.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%missing data
@article{muthen1987structural,
  title={On structural equation modeling with data that are not missing completely at random},
  author={Muth{\'e}n, Bengt and Kaplan, David and Hollis, Michael},
  journal={Psychometrika},
  volume={52},
  number={3},
  pages={431--462},
  year={1987},
  publisher={Springer}
}

@article{takahashi2017misingdataMCMC,
author={Takahashi, M.}, 
year={2017},
title={Statistical Inference in Missing Data by MCMC and Non-MCMC Multiple Imputation Algorithms: Assessing the Effects of Between-Imputation Iterations},
 journal={Data Science Journal}, volume={16}, 
 page={37}}
 
 @misc{yoon2018gain,
      title={GAIN: Missing Data Imputation using Generative Adversarial Nets}, 
      author={Jinsung Yoon and James Jordon and Mihaela van der Schaar},
      year={2018},
      eprint={1806.02920},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%predictive
@InProceedings{Kao2009Regression,
  author    = {Kao, Yi-hao and Roy, Benjamin and Yan, Xiang},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Directed Regression},
  year      = {2009},
  editor    = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
  publisher = {Curran Associates, Inc.},
  volume    = {22},
  url       = {https://proceedings.neurips.cc/paper/2009/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},
}

@InProceedings{Taskar2005Prediction,
  author    = {Taskar, Ben and Chatalbashev, Vassil and Koller, Daphne and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
  title     = {Learning Structured Prediction Models: A Large Margin Approach},
  year      = {2005},
  address   = {New York, NY, USA},
  pages     = {896–903},
  publisher = {Association for Computing Machinery},
  series    = {ICML '05},
  abstract  = {We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for efficient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods.},
  doi       = {10.1145/1102351.1102464},
  isbn      = {1595931805},
  location  = {Bonn, Germany},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1102351.1102464},
}

@InProceedings{Osokin2017Prediction,
  author    = {Osokin, Anton and Bach, Francis and Lacoste-Julien, Simon},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {On Structured Prediction Theory with Calibrated Convex Surrogate Losses},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper/2017/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf},
}

%prescriptive analysis/optimization

@Article{Bertsimas2020Prescriptive,
  author   = {Bertsimas, Dimitris and Kallus, Nathan},
  journal  = {Management Science},
  title    = {From Predictive to Prescriptive Analytics},
  year     = {2020},
  number   = {3},
  pages    = {1025-1044},
  volume   = {66},
  abstract = {We combine ideas from machine learning (ML) and operations research and management science (OR/MS) in developing a framework, along with specific methods, for using data to prescribe optimal decisions in OR/MS problems. In a departure from other work on data-driven optimization, we consider data consisting, not only of observations of quantities with direct effect on costs/revenues, such as demand or returns, but also predominantly of observations of associated auxiliary quantities. The main problem of interest is a conditional stochastic optimization problem, given imperfect observations, where the joint probability distributions that specify the problem are unknown. We demonstrate how our proposed methods are generally applicable to a wide range of decision problems and prove that they are computationally tractable and asymptotically optimal under mild conditions, even when data are not independent and identically distributed and for censored observations. We extend these to the case in which some decision variables, such as price, may affect uncertainty and their causal effects are unknown. We develop the coefficient of prescriptiveness P to measure the prescriptive content of data and the efficacy of a policy from an operations perspective. We demonstrate our approach in an inventory management problem faced by the distribution arm of a large media company, shipping 1 billion units yearly. We leverage both internal data and public data harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational decisions that outperform baseline measures. Specifically, the data we collect, leveraged by our methods, account for an 88\% improvement as measured by our coefficient of prescriptiveness.This paper was accepted by Noah Gans, optimization.},
  doi      = {10.1287/mnsc.2018.3253},
  eprint   = {https://doi.org/10.1287/mnsc.2018.3253},
  url      = {https://doi.org/10.1287/mnsc.2018.3253},
}

@misc{bertsimas2017power,
      title={The Power and Limits of Predictive Approaches to Observational-Data-Driven Optimization}, 
      author={Dimitris Bertsimas and Nathan Kallus},
      year={2017},
      eprint={1605.02347},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@Article{Elmachtoub2021SPO,
  author   = {Elmachtoub, Adam N. and Grigas, Paul},
  journal  = {Management Science},
  title    = {Smart “Predict, then Optimize”},
  year     = {2021},
  number   = {0},
  pages    = {null},
  volume   = {0},
  abstract = {Many real-world analytics problems involve two significant challenges: prediction and optimization. Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize. By and large, machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in the downstream optimization problem. In contrast, we propose a new and very general framework, called Smart “Predict, then Optimize” (SPO), which directly leverages the optimization problem structure—that is, its objective and constraints—for designing better prediction models. A key component of our framework is the SPO loss function, which measures the decision error induced by a prediction. Training a prediction model with respect to the SPO loss is computationally challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path and portfolio-optimization problems show that the SPO framework can lead to significant improvement under the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. We find that linear models trained using SPO+ loss tend to dominate random-forest algorithms, even when the ground truth is highly nonlinear.This paper was accepted by Yinyu Ye, optimization.},
  doi      = {10.1287/mnsc.2020.3922},
  eprint   = {https://doi.org/10.1287/mnsc.2020.3922},
  url      = {https://doi.org/10.1287/mnsc.2020.3922},
}

@Article{Ferreira2016PO,
  author   = {Ferreira, Kris Johnson and Lee, Bin Hong Alex and Simchi-Levi, David},
  journal  = {Manufacturing \& Service Operations Management},
  title    = {Analytics for an Online Retailer: Demand Forecasting and Price Optimization},
  year     = {2016},
  number   = {1},
  pages    = {69-88},
  volume   = {18},
  abstract = {We present our work with an online retailer, Rue La La, as an example of how a retailer can use its wealth of data to optimize pricing decisions on a daily basis. Rue La La is in the online fashion sample sales industry, where they offer extremely limited-time discounts on designer apparel and accessories. One of the retailer’s main challenges is pricing and predicting demand for products that it has never sold before, which account for the majority of sales and revenue. To tackle this challenge, we use machine learning techniques to estimate historical lost sales and predict future demand of new products. The nonparametric structure of our demand prediction model, along with the dependence of a product’s demand on the price of competing products, pose new challenges on translating the demand forecasts into a pricing policy. We develop an algorithm to efficiently solve the subsequent multiproduct price optimization that incorporates reference price effects, and we create and implement this algorithm into a pricing decision support tool for Rue La La’s daily use. We conduct a field experiment and find that sales does not decrease because of implementing tool recommended price increases for medium and high price point products. Finally, we estimate an increase in revenue of the test group by approximately 9.7\% with an associated 90\% confidence interval of [2.3\%, 17.8\%].},
  doi      = {10.1287/msom.2015.0561},
  eprint   = {https://doi.org/10.1287/msom.2015.0561},
  url      = {https://doi.org/10.1287/msom.2015.0561},
}

@Book{Sen2017LearningOptimization,
  author    = {Sen, Suvrajeet and Deng, Yunxiao},
  publisher = {Humboldt-Universität zu Berlin},
  title     = {Learning Enabled Optimization: Towards a Fusion of Statistical Learning and Stochastic Optimization},
  year      = {2017},
  doi       = {http://dx.doi.org/10.18452/18087},
}


%SGD
@misc{bertsekas2017cvx,
      title={Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey}, 
      author={Dimitri P. Bertsekas},
      year={2017},
      eprint={1507.01030},
      archivePrefix={arXiv},
      primaryClass={cs.SY}
}
@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{Bottou2018optML,
title = "Optimization methods for large-scale machine learning",
abstract = "This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.",
keywords = "Algorithm complexity analysis, Machine learning, Noise reduction methods, Numerical optimization, Second-order methods, Stochastic gradient methods",
author = "L{\'e}on Bottou and Curtis, {Frank E.} and Jorge Nocedal",
note = "Funding Information: ∗Received by the editors June 16, 2016; accepted for publication (in revised form) April 19, 2017; published electronically May 8, 2018. http://www.siam.org/journals/sirev/60-2/M108017.html Funding: The work of the second author was supported by U.S. Department of Energy grant DE-SC0010615 and U.S. National Science Foundation grant DMS-1016291. The work of the third author was supported by Office of Naval Research grant N00014-14-1-0313 P00003 and Department of Energy grant DE-FG02-87ER25047s. †Facebook AI Research, New York, NY 10003 (leon@bottou.org). ‡Department of Industrial and Systems Engineering, Lehigh University, Bethlehem, PA 18015 (frank.e.curtis@gmail.com). §Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL 60201 (j-nocedal@northwestern.edu). Publisher Copyright: {\textcopyright} 2018 Society for Industrial and Applied Mathematics.",
year = "2018",
doi = "10.1137/16M1080173",
language = "English (US)",
volume = "60",
pages = "223--311",
journal = "SIAM Review",
issn = "0036-1445",
publisher = "Society for Industrial and Applied Mathematics Publications",
number = "2",
}

@article{nangia2018listops,
  title={Listops: A diagnostic dataset for latent tree learning},
  author={Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.06028},
  year={2018}
}

@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

@article{radev2013acl,
  title={The ACL anthology network corpus},
  author={Radev, Dragomir R and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  journal={Language Resources and Evaluation},
  volume={47},
  pages={919--944},
  year={2013},
  publisher={Springer}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{linsley2018learning,
  title={Learning long-range spatial dependencies with horizontal gated recurrent units},
  author={Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Windolf, Charles and Serre, Thomas},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@misc{enwiki8,
  title = {The human knowledge compression contest},
  author = {Hutter, Marcus},
  year = {2006},
  howpublished = {http://prize.hutter1.net/}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}
@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{ma2021luna,
  title={Luna: Linear unified nested attention},
  author={Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2441--2453},
  year={2021}
}

@article{sukhbaatar2019adaptive,
  title={Adaptive attention span in transformers},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:1905.07799},
  year={2019}
}

@article{post2018call,
  title={A call for clarity in reporting BLEU scores},
  author={Post, Matt},
  journal={arXiv preprint arXiv:1804.08771},
  year={2018}
}
@inproceedings{ott-etal-2018-scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
}