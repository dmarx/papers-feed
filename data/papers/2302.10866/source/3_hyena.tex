\section{Hyena: Definition and Properties}\label{allofhyena}
%
In this section, we define Hyena, a class of \textit{data-controlled} operators consisting of a recurrence of multiplicative gating interactions and long convolutions. Instead of seeking an approximation to attention, we guide our design by intentionally incorporating key computational properties of attention, including the decoupling of sequence length and parameter counts.
%
\subsection{${\sf Hyena}$ Recurrences}\label{hyena_op}
%
At a high level, ${\sf Hyena}$ consists of the following steps (setting $D = 1$ for clarity):
\begin{itemize}
    \item[$i.$] Compute a set of $N + 1$ linear projections of the input, similarly to attention. The number of projections $(v_t, x^1_t, \dots, x^N_t)$ need not be three. One projection takes the role of value, such that a linear input-output function can be defined as $y = \sH(u) v$ for some $\sH(u)$.
    \item[$ii.$] The matrix $\sH(u)$ is defined by interleaving implicit long convolutions and element-wise multiplication with one projection $x^i$ at a time, until all projections are exhausted. Evaluation of $\sH(u) v$ is done efficiently \textbf{without materializing} $\sH(u)$. By doing so, we implicitly define a data-controlled operator as a factorization of a matrix. The long convolutions forming $\sH(u)$ are parametrized implicitly to retain sublinear parameter scaling in sequence length.
\end{itemize}
%
Next, we formally define ${\sf Hyena}$, starting with its computational model. We leave the analysis of its data-controlled matrix form for the latter part of the section.
%
\begin{tcolorbox}[enhanced, sharp corners, drop fuzzy shadow, frame hidden, colback=yellow!15]
\begin{definition}[Order--$N$ $\sf Hyena$ Operator] 
Let $(v, x^1, \cdots, x^N)$ be projections of the input and let $h^1,\dots, h^N$ be a set of learnable filters. The ${\sf Hyena}_N$ operator is defined by the recurrence:
%
\begin{equation}\label{eq:hyena}
    \begin{aligned}
        z^1_t &={\ocra v}_t\\
        z^{n+1}_t & = x_t^n (h^n * z^n)_t & n=1,\dots,N\\
        {\color{blue!70}y}_t &= z_t^{N+1}
    \end{aligned} 
\end{equation}
\end{definition}
\end{tcolorbox}
%
\begin{remark}
The time complexity of a $\sf Hyena$ recurrence is $\mathcal{O}(N L \log_2 L)$. The input-output map can be rewritten as 
\[
y = x^N \cdot (h^N * ( x^{N-1} \cdot (h^{N-1} * (\cdots))))
\]
where each convolution is performed through the Fourier domain in $\mathcal{O}(L \log_2 L)$.
\end{remark}
%
Interestingly, the element-wise product in time domain corresponds to convolution in frequency domain, i.e.
%
\[ x_tu_t = (\hat x * \hat u)_t, \]
%
where $\hat x,\hat u$ denote the DFT of $x$ and $u$, respectively. Thus, $\sf Hyena$ is alternatively applying convolutions in the time and then the frequency domain (or alternatively applying element-wise products in the time and frequency domain). One potential explanation for the effectiveness of this procedure is that the convolution in the time domain (element-wise multiplication in the frequency domain) increases the memory length, allowing for a broader context to be taken into account. On the other hand, the element-wise multiplication in the time domain (convolution in the frequency domain) allows for more fine-grained selection of specific frequency components of the signal.
%
\subsection{${\sf Hyena}$ Matrices} 
%
${\sf Hyena}$ operators build on the ${\sf H3}$ mechanism developed by \citep{dao2022hungry}. For clarity of exposition, we once again consider the SISO case ($D=1$). Let $\sD_q$ and $\sD_k$ be the $L$-by-$L$ diagonal matrices whose respective main diagonal entries are the respective entries of $q$ and $k$. ${\sf H3}$ realizes a surrogate attention matrix with a data-controlled, parametrized decomposition in four terms:
%
\begin{equation}\label{eq:linear_attention}
    \begin{aligned}
        \sA(q,k) &= \sD_q \sS_\psi \sD_k \sS_\varphi \\
        {\sf H3}(q, k, v) &= \sA(q,k) v
    \end{aligned}
\end{equation}
%
where $\sS_\varphi,\sS_\psi$ are the Toeplitz matrices of learnable \textbf{causal} filters $\varphi,\psi$ parametrized via SSMs\footnote{For consistency with our discussion, we have swapped $k$ and $v$ compared to the notation in \citep{dao2022hungry}.}. Alongside the $qkv$-projections the filters constitute our degrees of freedom in the layer design. This decomposition allows evaluation of \eqref{eq:linear_attention} in just $\cO(L \log_2 L)$ time (two FFT convolutions and two element-wise products), i.e.
%
\begin{equation}
    \begin{aligned}
        z_{t} &= k_{t}(\varphi * v)_t \\
        y_{t} &= q_{t}(\psi * z)_t
    \end{aligned}
\end{equation}
%
$\small \sf Hyena$ represents a generalization of \eqref{eq:linear_attention} for an arbitrary number of projections -- not limited to three -- and with implicit free-form long filters for the convolutions. The resulting recurrence \eqref{eq:hyena} can be also represented in matrix form $y=\sH(u)v$. Let $\sD_x^n=\diag(x^n)\in\R^{L\x L}$ and let $\sS_h^n$ be the Toeplitz matrix corresponding to filter $h^n$. The resulting $\sf Hyena$ recurrence is linear in $v$ and can be rewritten in matrix form:
%
\[
    y = \sH(u)v = \sD_x^N\sS_h^N \cdots \sD_x^2\sS_h^2\sD_{x}^1\sS_h^1 v
\]
%
Figure~\ref{fig:hyena_matrices} visualizes an example decomposition.
%
\begin{remark}[$\sf Hyena$ generalizes $\sf H3$ and $\sf GSS$.]
    The $\sf H3$ mechanism \citep{dao2022hungry} corresponds to ${\sf Hyena}_2$ and $\sf GSS$ \citep{mehta2022long} is ${\sf Hyena}_{1}$, with a particular choice of parametrization for the long convolutions (SSMs).
\end{remark}
%
Analysis of the $\sf H3$ mechanism as a decomposition $\sD_q\sS_\psi\sD_k\sS_\varphi$ of its surrogate attention matrix\footnote{Some of this analysis is reported in the Appendix.} clarifies a connection to fast evaluation algorithms for matrix-vector multiplications. In particular, the generalization of \eqref{eq:linear_attention} to an arbitrary order is inspired by fast evaluation algorithms for structured dense matrices based on \textit{butterfly} decompositions \citep{li2015butterfly,dao2019learning, dao2022monarch}, with length of the decomposition closely tied to its expressivity (in the classes of matrices it can represent). The ${\sf Hyena}$ operator blends data control with a special case of butterfly decomposition.         
%
\begin{remark}
${\sf Hyena}$ operators have unbounded context. Namely, they are not artificially restricted by e.g., locality, and can learn long-range dependencies between any of the elements of $v$ via long convolutions, which we discuss next.
\end{remark}
%
\subsection{${\sf Hyena}$ Filters}\label{hyena_ker}
%
Here we provide details on the convolution parametrization. We represent the filters of each ${\sf Hyena}$ operator as a map from the time (or space) domain $t$ to values $h_t$, and learn it with a shallow feed-forward neural network ({$\sf FFN$}):
%
\begin{equation}\label{filt}
    h_t = {\sf Window}(t)\cdot({\sf FFN} \circ {\sf PositionalEncoding}) (t)
\end{equation}
%
This approach builds on the neural implicit representation literature \citep{mildenhall2021nerf,sitzmann2020implicit}, which has found application in long convolution layers \citep{romero2021ckconv, romero2021flexconv}. One advantage of \eqref{filt} is given by the decoupling of filter length and parameter cost. 
%
\paragraph{Specializing filters in Hyena}
%
The window and positional encoding functions are used to specialize filters in ${\sf Hyena}$ operators, biasing them towards a specific type. Figure \ref{fig:modul} provides an important example: we choose at least one of the convolutions in ${\sf Hyena}$ to be shaped towards exponential decay, mirroring the findings of \citep{li2022makes} in other applications.
%
\begin{figure}[t]
    \centering
    \input{figures/source/modulatinv2.tex}
    \vspace{-4mm}
    \caption{\textbf{[Top]:} Example of long convolution parametrization for ${\sf Hyena}$ operators, with a decay ${\sf Window}(t) = \exp\{- \alpha t\}$. Parameter $\alpha$ is modified across the independent channels of ${\sf Hyena}$ to regularize filters to be of different lengths. In practice, we add a bias term to our window, so that the filters are not constrained to be zeros after a length determined by the decay rate. 
    }
    \vspace{-4mm}
    \label{fig:modul}
\end{figure}
%
Interestingly, we find that long exponentially decaying filters display synergy with high-frequency filters, as they enable the operator to select specific inputs at specific steps\footnote{This observation finds mirrors in the parametrization of the convolutions in H3 \citep{dao2022hungry} as a shift SSM and a diagonal SSM.}. Similarly to \citep{romero2021ckconv}, we use high-frequency periodic activations (sine) in the {$\sf FFN$}. This allows \eqref{filt} to learn filters with high-frequency content, addressing the low-frequency bias of neural networks \citep{basri2020frequency}.
%
Owing to the {$\sf FFN$}, the parametrization in \eqref{filt} can approximate filters obtained through other means, such as S4 \citep{gu2020hippo,gu2021efficiently}, CKConv \citep{romero2021ckconv}, SGConv \citep{li2022makes} and \textit{Fourier Neural Operator} (FNO) \citep{li2020fourier}.
%

\paragraph{Preserving causality}
%
Causality is necessary to train autoregressive language models, in order for the output at a given position to depend only on the past. For example, Transformers mask the attention matrix to be lower triangular. In the case of ${\sf Hyena}$, causality can be guaranteed by parametrizing causal convolutions: 
%
\begin{proposition}[Causal Hyenas]\label{prop:causality}
    If each filter $h^n, ~n=1,\dots, N$ is causal, then the corresponding ${\sf Hyena}_N$ operator is causal.
    %
\end{proposition}
%
In practice, we need not constrain the learning of the filter \eqref{filt} to ensure its \textit{numerical} causality. If we use FFT-based convolution algorithms, all we need is to evaluate the filter at $t=0,\dots,L-1$ and zero-pad the input and filter sequences to $2 L - 1$ before taking FFT. 
%
\paragraph{Efficiency}
%
One bottleneck of long convolution models can be their low utilization of hardware accelerators, especially when they involve iterative numerical methods to materialize the filter\footnote{In contrast, deep learning primitives are designed for high GPU utilization, with {\sf FFNs} and attention usually reaching $50-70\%$ or higher, if optimized.}. Evaluation of \ref{filt} is fast, since it involves a single forward pass of an {$\sf FFN$}, and can be performed in parallel across sequence length and all orders of an ${\sf Hyena}$ operator as displayed in Algorithm \ref{alg:hyenaa}, increasing hardware utilization. An additional source of low utilization is the FFT, which is also shared by other long other convolutional layers. This bottleneck can be partially addressed by blocking \citep{selesnick2017fast}, and optimization of the underlying routines \citep{dao2022hungry}. We benchmark runtime in Section \ref{benchm}.
%

\subsection{{\sf Hyena} Algorithm}
%

A forward pass of {\sf Hyena} is summarized below. 

\setcounter{algorithm}{-1}
\begin{algorithm}[h]
    \caption{{\sf Projection}}\label{alg:hyenaa}
    \caption{{\sf Projection}}
    \begin{algorithmic}
    \REQUIRE Input sequence $u \in \R^{L \times D}$ \\ 
    \STATE {\small 1.} In parallel across $L$: $\hat{z} = {\sf Linear}(u)$, ${\sf Linear}:\R^{D} \rightarrow \R^{(N+1)D}$ \\
    \STATE {\small 2.} In parallel across $D$: $z = {\sf DepthwiseConv1d}(h, \hat{z} )$, $h$ is a short convolution filter \\
    \STATE {\small 3.} Reshape and split $z$ into $x^1, \dots, x^N, v$. Dimensions of one element are $x^n \in \R^{D \times L}$\\ 
    \STATE Return $x^1, \dots, x^N, v$, $x^n$
    \end{algorithmic}
\end{algorithm}

\setcounter{algorithm}{0}
\begin{algorithm}[h]
    \caption{${\sf HyenaFilter}$}\label{alg:hyenaa}
    \caption{${\sf Hyena}$ Filter}
    \begin{algorithmic}
    \REQUIRE Sequence length $L$, positional embedding dimension $D_e$ \\ 
    \STATE {\small 1.} $t  = {\sf PositionalEncoding}(L)$, $t \in \R^{L \times D_e}$ \\
    \STATE {\small 2.} In parallel across $N, L$: $\hat{h} = {\sf FFN}(t)$, ${\sf FFN}: \R^{D_e}\rightarrow \R^{N D}$, $\hat{h} \in \R^{L \times N D}$\\
    \STATE {\small 3.} Reshape to $\hat{h} \in \R^{N \times D \times L}$ \\
    \STATE {\small 4.} $h = \hat{h} \cdot {\sf Window}(t)$, $h \in \R^{N \times D \times L}$ \\
    \STATE {\small 5.} Split $h$ into $h^1, \dots, h^N$
    \STATE Return $h^1, \dots, h^N$
    \end{algorithmic}
\end{algorithm}

\setcounter{algorithm}{1}
\begin{algorithm}[h]
    \caption{${\sf Hyena}$ Operator}\label{alg:hyenaa}
    \caption{Forward pass of ${\sf Hyena}$}
    \begin{algorithmic}
    \REQUIRE Input sequence $u \in \R^{L \times D}$, order $N$, model width $D$, sequence length $L$, positional embedding dimension $D_e$ \\ 
    \STATE {\small 1.} $x^1, \dots, x^N, v = {\sf Projection}(u)$ \\
    \STATE {\small 2.} $h^1, \dots, h^N = {\sf HyenaFilter}(L, D_e)$ \\
    \FOR{$n = 1,\dots,N$}
    \STATE {\small 3.} In parallel across $D$: $v_{t} \leftarrow x^n_{t}\cdot {\sf FFTConv}(h^n, v)_{t}$ \\
    \ENDFOR  
    \STATE Return $y = v$
    \end{algorithmic}
\end{algorithm}

%
\begin{proposition}[Computational Complexity]
    The computational cost of processing an input $u\in\R^{L\x D}$ with an order-$N$ $\sf Hyena$ operator is
    % 
    \[
        %\cO(D N L\log_2 L + L N D^2)
        \cO(N D L(\log_2 L + D))
    \]
    %
\end{proposition}
%