\section{Experiments}\label{experiments}
%
\subsection{Shrinking the gap on in-context learning}\label{eval_ker}
%
We begin by empirically motivating the ${\sf Hyena}$ design, including the choice of long convolution parametrization. We consider the suite of tasks described in Table \ref{table:grok}.
%
\begin{table}[b]
    \small
    \centering
    \caption{A selection of our \textit{mechanistic design} benchmarks.}
    \setlength{\tabcolsep}{5.8pt}
    \label{table:grok}
    \begin{tabular}{@{}c|cc@{}}
    \toprule
    \textbf{Task} &\multicolumn{1}{c}{\textbf{Prompt}}&\multicolumn{1}{c}{\textbf{Target}}\\
    \midrule 
    Associative Recall & a, $1$, b, e, $3$, f, b & e\\  
    Majority & a, g, g, g, e, f, g & g \\
    Counting & a, b, b, b, a, c, b & 4\\
    ICL of Functions & $x_0$, $f(x_0)$, $\dots x_n$ & $f(x_n)$ \\ 
    Arithmetic & $1$, $3$, $5$, $+$, $6$, $8$, $3$ & $8$, $1$, $8$ \\
    \bottomrule
    \end{tabular}
\end{table}
%
\begin{figure*}[t]
  \centering
  \input{figures/source/assocrecall.tex}
  \caption{Benchmark of long convolution parametrizations in order $2$ ${\sf Hyena}$ operators on associative recall (\%). Our results show that implicit parametrizations scale more favorably in vocabulary size (number of possible values of tokens in the input) and length of the sequence.}
  \label{fig:synthetics1}
\end{figure*}
%
Our evaluation is grounded in recent work on mechanistic interpretability of Transformers \citep{elhage2021mathematical,power2022grokking,olsson2022context,zhang2022unveiling}. Recently, associative recall, in particular, has been successfully used to guide the design of H3 \citep{dao2022hungry}. We extend the suite of tasks from these works and include benchmarking more challenging versions of each task . For example, solving associative recall with a vocabulary size of only $10$ reveals whether a model is structurally capable of performing recall. Testing on much longer sequences and larger vocabularies reveals additional gaps in performance that are otherwise hidden.
%
\paragraph{How to parametrize long convolutions}
%
We compare the performance of the following long convolution parametrizations for $S^1$ and $S^2$ in an order $2$ Hyena:
%
\begin{itemize}[leftmargin=0.1in]
    \item Conv1d: Explicit convolutions (regular convolution layers with fixed filter size).
    \item FNO: Filters parametrized explicitly in the frequency-domain \citep{li2020fourier}.
    \item H3: Implicit parametrization using state-space models (SSMs), in particular the standard S4 \citep{gu2021efficiently}. 
    \item TransferFunc: Implicit parametrization via transfer functions, a classical system-theoretic generalization of SSMs\footnote{Transfer functions roughly correspond to a frequency-domain representation of SSMs.} 
    \item CKConv: Implicit parametrization using {$\sf FFN$s} \citep{romero2021ckconv}. 
    \item ${\sf Hyena}$: Combination of implicit parametrizations via {$\sf FFN$s} (with exponential decay modulation as shown in Figure \ref{fig:modul}), and short explicit filters.
\end{itemize}
%
All models have the same width and $2$ layers. Figure \ref{fig:synthetics1} shows implicit approaches based on {\sf FFNs} outperform other long convolutions, with the gap widening on longer sequences and larger vocabulary sizes. We train a different model on each setting of sequence length and vocabulary size. The ranking is correlated with the ability to decouple sequence length from parameter count (${\sf Hyena}$, CKConv, TransferFunc, H3) and expressivity (Hyena, CKConv). We observe similar trends on the other tasks.
%
\paragraph{Pushing sequence length to the limit}
%
Next, we evaluate associative recall performance on extremely long sequences of length $131$k. To the best of our knowledge, these represent the first empirical display of attention-free in-context learning on sequences of this length. The gap between parametrization schemes widens as shown in Appendix A, with ${\sf Hyena}$ outperforming CKConv by $80$ points.
%
\paragraph{Comparing operators}
%
We repeat our associative recall experiment, this time benchmarking different $2$ layer models rather than changing the convolution parametrization: an order $2$ Hyena, GSS \citep{mehta2022long}, H3 \citep{dao2022hungry}, AFT-conv \citep{zhai2021attention}, RWKV \citep{PENG_RWKV-LM_2021}, and a standard GPT \citep{brown2020language} using FlashAttention \citep{dao2022flashattention}. As shown in Table \ref{table:synthetic2}, ${\sf Hyena}$ is the only operator able to solve the task. Our results challenge the observation that only Transformers are capable of challenging in-context learning.
%
\begin{table}[t]
\small
\centering
\caption{Test accuracy (\%) for associative recall on longer sequences, vocabulary size $30$. The symbol \xmark~is used to mark settings where the model does not fit in memory.}
\label{table:synthetic2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|c|c|c|ccccccc@{}}
\toprule
Sequence length & ${\sf Hyena}$ & FlashTransformer & Transformer &\multicolumn{1}{c}{GSS}  &\multicolumn{1}{c}{H3} & \multicolumn{1}{c} {AFT} & \multicolumn{1}{c}{RWKV} \\
\midrule 
$30$k & $100.0$ & $32.4$ & \xmark & $5.3$ & $8.4$ & $2.3$ & $12.4$ \\
$64$k & $100.0$ & $26.7$ & \xmark & $2.1$ & $4.3$ & $1.2$ & $6.5$ \\
$131$k & $97.2$ & \xmark & \xmark & $0.1$ & $0.6$ & $0.8$ & $2.3$ \\
\bottomrule
\end{tabular}
\end{table}
%
Surprisingly, rankings of model performance at a fixed sequence length on {\sf The Pile} are consistent with rankings on aggregate scores on our synthetics (Appendix \ref{app:add_results}).
%
\paragraph{Generality of ${\sf Hyena}$ operators and filters}
%
${\sf Hyena}$ operators and filters can also applied successfully beyond language tasks. We experiment on sequential CIFAR, where pixels are flattened as a sequence, and use the same operator defined for language. We reach the accuracy of standard S4 \citep{gu2021efficiently} with same model size ($91\%$). In Section \ref{sec:image_classification} and Appendix \ref{appendix:experiment-details}, we discuss larger-scale image classification experiments with Hyena.
%
\subsection{Language Modeling}\label{res:lm}
%
Next, we verify the scaling of ${\sf Hyena}$ on autoregressive language modeling. We evaluate the perplexity on {\sc WikiText103} (Table \ref{wt103}) and {\sc The Pile} (Table \ref{pile}). On the {\sc The Pile}, we train different models for $5, 10, 15$ billion tokens (different runs), adjusting the learning rate scheduler. ${\sf Hyena}$ is the first attention-free, convolution architecture to match GPT quality with a $~20\%$\footnote{The FLOP reduction consists in the \textit{non-parametric} FLOPs of {$\sf SelfAttention$} devoted to attention matrix computation. The ratio of parametric to non-parametric FLOPs (and hence the gains) depend on the ratio of model width $D$ and sequence length $L$ used in training.} reduction in total FLOPs. Preliminary scaling laws are shown in Figure \ref{fig:hyena_laws}, collecting the training runs at $5, 10, 15$ billion tokens. Each curve represents a different training run.
In Appendix \ref{appendix:experiment-details}, we provide results on the PG-19 long-range benchmark \citep{raecompressive2019}.
%


\begin{figure}[t]
    \centering
    \input{figures/source/scaling_laws_corrected.tex}
    \vspace{-4mm}
    \caption{Preliminary "scaling law" of language models on {\sc The Pile}. Comparison of our approach (red) based on long convolutions and gating (${\sf Hyena}$) and a standard GPT (blue) \citep{brown2020language}. We reach perplexity of GPT with a smaller training FLOP budget.}
    \label{fig:hyena_laws}
\end{figure}


\begin{table}[t]
    \small
    \centering
    \begin{minipage}{.38\textwidth}
        \centering
        \caption{Perplexity on {\sc WikiText103} (same tokenizer). $^*$ are results from \citep{dao2022hungry}. Deeper and thinner models (Hyena-slim) achieve lower perplexity.}
        \vspace{2mm}
        \label{wt103}
        \setlength{\tabcolsep}{4pt}
        \begin{tabular}{@{}c|cc@{}}
        \toprule
        Model &\multicolumn{1}{c}{{\sc Perplexity}} \\
        \midrule 
        Transformer ($125$M) & $18.6$ & \\
        Hybrid H3 ($125$M) & $18.5^*$ \\
        Performer ($125$M) & $26.8^*$  \\ 
        Reformer  ($125$M)& $25.6^*$  \\ 
        \midrule
        AFT-conv ($125$M) & $28.2$ \\
        Linear Attention ($125$M) & $25.6^*$ \\
        \midrule
        ${\sf Hyena}$-$3$ ($125$M) & $18.6$ \\
        ${\sf Hyena}$-$3$-slim ($125$M) & $18.5$ \\
        \bottomrule
        \end{tabular}
    \end{minipage}
    %
    \hspace{0.7cm}
    %
    \begin{minipage}{.55\textwidth}
        \centering
        \vspace{-18mm}
        \caption{Perplexity on {\sc The Pile} for models trained until a total number of tokens e.g., $5$ billion (different runs for each token total). All models use the same tokenizer (GPT2). FLOP count is for the $15$ billion token run.}
        \vspace{2mm}
        \label{pile}
        \setlength{\tabcolsep}{4pt}
        \begin{tabular}{@{}c|cccc@{}}
        \toprule
        Model &\multicolumn{1}{c}{{\sc $5$B}} & \multicolumn{1}{c}{{\sc $10$B}} & \multicolumn{1}{c}{{\sc $15$B}} & \multicolumn{1}{c}{{\sc FLOPs ($10^{19}$)}}\\
        \midrule 
        GPT ($125$M) & $13.3$ & $11.9$ & $11.2$ & $1.88$ \\
        ${\sf Hyena}$-$2$ ($153$M)& $13.3$ & $11.8$ & $11.1$ & $\textbf{1.87}$ \\
        \midrule
        GPT ($355$M) & $11.4$ & $9.8$ & $9.1$ & $4.77$ \\
        ${\sf Hyena}$-$2$ ($355$M) & $11.3$ & $9.8$ & $9.2$ & $\textbf{3.93}$ \\
        \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}

% \begin{minipage}{.5\textwidth}
%     \begin{table}[t]
%     \small
%     \centering
%     \caption{Perplexity on {\sc The Pile} for models trained until a total number of tokens e.g., $5$ billion (different runs for each token total, scheduler is adjusted). All models use the same tokenizer (GPT2). FLOP count is for the $15$ billion token run.}
%     \label{pile}
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{@{}c|cccc@{}}
%     \toprule
%     Model &\multicolumn{1}{c}{{\sc $5$B}} & \multicolumn{1}{c}{{\sc $10$B}} & \multicolumn{1}{c}{{\sc $15$B}} & \multicolumn{1}{c}{{\sc FLOPs ($10^{19}$)}}\\
%     \midrule 
%     GPT ($125$M) & $13.3$ & $11.9$ & $11.2$ & $1.88$ \\
%     ${\sf Hyena}$-$2$ ($153$M)& $13.3$ & $11.8$ & $11.1$ & $\textbf{1.87}$ \\
%     \midrule
%     GPT ($355$M) & $11.4$ & $9.8$ & $9.1$ & $4.77$ \\
%     ${\sf Hyena}$-$2$ ($355$M) & $11.3$ & $9.8$ & $9.2$ & $\textbf{3.93}$ \\
%     \bottomrule
%     \end{tabular}
%     \end{table}
% \end{minipage}

\subsection{Downstream Evaluation}
%
We perform a downstream evaluation on SuperGLUE \citep{wang2019superglue} tasks. We compare ${\sf Hyena}$ (trained for $137$ billion tokens) with the best available pre-trained attention-free model, RWKV \citep{PENG_RWKV-LM_2021} (trained for $332$ billion tokens), and a reference GPTNeo \citep{gpt-neo} (trained for $300$ billion tokens) of the same size. Tables \ref{supergluezero} and \ref{supergluefew} summarize the results. ${\sf Hyena}$ performs similarly to other models despite having been trained on less than half the number of total tokens. We observe ${\sf Hyena}$ to display characteristic few-shot capabilities of standard Transformers, with some tasks e.g., MultiRC seeing a lift of more than $20\%$ accuracy over zero-shot when the model is provided additional prompts as context. The improvements are more noticeable in generation tasks, where the additional prompts can instruct the model on how it should be responding to the questions. We report an additional downstream evaluation on the LAMBADA  task \citep{paperno2016lambada} in Appendix \ref{appendix:experiment-details}.
%
\begin{table}[!h]
\small
\centering
\caption{Zero-shot accuracy ($\%$) on {\sc SuperGLUE} tasks for small models.}
\label{supergluezero}
\vspace{2mm}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Model &\multicolumn{1}{c}{{\sc WSC}} & \multicolumn{1}{c}{{\sc WIC}} & \multicolumn{1}{c}{{\sc RTE}} & \multicolumn{1}{c}{{\sc CB}} & \multicolumn{1}{c}{{\sc MultiRC}} &  \multicolumn{1}{c}{{\sc ReCoRD}} &  \multicolumn{1}{c}{{\sc BoolQ}} &  \multicolumn{1}{c}{{\sc COPA}} & \multicolumn{1}{c}{{\sc Average}} \\
\midrule 
GPTNeo \citep{gpt-neo} & $\mathbf{27.9}$ & $50.0$ & $45.1$ & $\mathbf{41.1}$ & $0.0$ & $\mathbf{61.7}$ & $\mathbf{62.2}$ & $62.0$ & $\mathbf{43.8}$ \\ 
RWKV \citep{PENG_RWKV-LM_2021} & $13.4$ & $\mathbf{52.3}$ & $\mathbf{46.9}$ & $25.0$ & $0.0$ & $58.5$ & $\underline{59.2}$ & $\underline{66.0}$ & $40.2$ \\
${\sf Hyena}$ & $\underline{21.2}$ & $\underline{50.5}$ & $\underline{46.6}$ & $\underline{39.3}$ & $\mathbf{1.1}$ & $\underline{59.4}$ & $51.8$ & $\mathbf{70.0}$ & $\underline{41.5}$ \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[!h]
\small
\centering
\caption{Few-shot ($3$) accuracy ($\%$) on {\sc SuperGLUE} tasks for small models.}
\label{supergluefew}
\vspace{2mm}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|ccccccccc@{}}
\toprule
Model &\multicolumn{1}{c}{{\sc WSC}} & \multicolumn{1}{c}{{\sc WIC}} & \multicolumn{1}{c}{{\sc RTE}} & \multicolumn{1}{c}{{\sc CB}} & \multicolumn{1}{c}{{\sc MultiRC}} &  \multicolumn{1}{c}{{\sc ReCoRD}} &  \multicolumn{1}{c}{{\sc BoolQ}} &  \multicolumn{1}{c}{{\sc COPA}} & \multicolumn{1}{c}{{\sc Average}} \\
\midrule 
GPTNeo \citep{gpt-neo} & $\underline{38.5}$ & $\underline{50.0}$ & $\mathbf{53.8}$ & $\underline{42.9}$ & $\underline{22.4}$ & $\mathbf{61.4}$ & $\mathbf{61.0}$ & $63.0$ & $\underline{49.1}$ \\ 
RWKV \citep{PENG_RWKV-LM_2021} & $32.7$ & $49.4$ & $47.2$ & $37.5$ & $0.0$ & $\underline{58.3}$ & $55.0$ & $\underline{64.0}$ & $43.0$ \\
${\sf Hyena}$ & $\mathbf{39.4}$ & $\mathbf{50.1}$ & $\underline{47.6}$ & $\mathbf{46.4}$ & $\mathbf{26.7}$ & $58.1$ & $\underline{56.0}$ & $\mathbf{70.0}$ & $\mathbf{49.3}$ \\
\bottomrule
\end{tabular}
\end{table}
%

%
\subsection{Benchmarking}
%
We benchmark runtime of an order $2$ ${\sf Hyena}$ operator compared to attention and FlashAttention layers \citep{dao2022flashattention}. ${\sf Hyena}$ uses a fused CUDA kernel to perform ${\sf FFTConv}$ \citep{dao2022hungry}. We set batch size to $64$ and measure runtime (in milliseconds). Results are provided in Figure \ref{fig:benchmarking_plot}. ${\sf Hyena}$ speedups reach $100\x$ at sequence length $64$K. Crossover points for ${\sf Hyena}$ and attention is at length $2048$, and for ${\sf Hyena}$ and FlashAttention is between $4096$ and $8196$. Despite the absolute reduction in FLOPs, speedups are achieved only on longer sequences when the gap grows sufficiently large. This occurs because hardware utilization of ${\sf Hyena}$ is lower than FlashAttention. We expect the gap between theoretical maximum speedup to shrink with improved implementations of ${\sf FFTConv}$ and specialized hardware.
%
\begin{figure}[t]
    \centering
    \input{figures/source/bench}
    \vspace{-5mm}
    \caption{Benchmarking runtime of Hyena, Attention and FlashAttention with varying sequence lengths. Batch size is set to $64$. The figure on the right is an inset showing a zoomed-in portion of the figure on the left.}
    \label{fig:benchmarking_plot}
\end{figure}

\subsection{Large-Scale Image Classification}\label{benchm}
\label{sec:image_classification}
Finally, we demonstrate the potential of ${\sf Hyena}$ as a general deep learning operator by applying it to image classification. On {\sf ImageNet}, we drop-in replace attention layers in the \textit{Vision Transformer} (ViT) \citep{dosovitskiy2020image} with the ${\sf Hyena}$ operator (without changes from its language counterpart) and match performance with ViT. We also show that using smaller image patches boosts performance in both attention and ${\sf Hyena}$. Since this results in longer sequence lengths, we expect ${\sf Hyena}$ to outperform in speed as patches get more fine-grained approaching pixel-level. On CIFAR-2D, we test a 2D version of ${\sf Hyena}$ long convolution filters in a standard convolutional architecture, which improves on the 2D long convolutional model S4ND \citep{nguyen2022s4nd} in accuracy with a $8\%$ speedup and 25\% fewer parameters. See Appendix \ref{appendix:image-classification} for additional vision architectures and training procedure details.

 %We show the potential of using smaller patch sizes (which increases the sequence length) to boost performance when using 8x8 patches (instead of 16x16)

\begin{table}[h]
\small
\centering
\caption{Image classification top-1 accuracy.}
\label{image_results}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|cccc@{}}
\toprule
Model & \multicolumn{1}{c} {{\sc Patch Size}} &  \multicolumn{1}{c} {{\sc Seq Len}} & \multicolumn{1}{c} {{\sc Dataset}} & \multicolumn{1}{c}{{\sc Acc (\%)}} \\
\midrule 
ViT ($87$M) & 16x16 & 196 & ImageNet-1k & 78.5 \\
Hyena-ViT ($88$M) & 16x16 & 196 & ImageNet-1k & 78.5 \\
\midrule
ViT ($87$M) & 8x8 & 1024 & ImageNet-1k & 80.0 \\
Hyena-ViT ($88$M) & 8x8 & 1024 & ImageNet-1k & 79.8 \\
\midrule
S4ND-ISO ($268$k) & - & - & CIFAR-10 & 89.9 \\
Hyena-ISO ($202$k) & - & - & CIFAR-10 & 91.2 \\
\bottomrule
\end{tabular}
\end{table}