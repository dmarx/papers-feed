\documentclass{article}

\title{\huge {Hyena Hierarchy:}\\Towards Larger Convolutional Language Models}
%
%
 \author{Michael Poli\footnote{Equal contribution. $\dagger$ Equal senior authorship. $^1$Stanford University. $^2$Mila and Universit\'e de Montr\'eal.}~$^{,1}$, Stefano Massaroli$^{*,2}$, Eric Nguyen$^{1,*}$, \\ Daniel Y. Fu$^1$, Tri Dao$^1$, Stephen Baccus$^1$, \\ Yoshua Bengio$^2$, Stefano Ermon$^{1,\dagger}$, Christopher R\'e$^{1,\dagger}$
 }
 
\date{\small{\footnotesize\sf Version}: submitted draft, {\footnotesize\sf  Last Compiled}: \today}
%
\input{_template/style}
\input{_template/math}

\begin{document}
\maketitle
%
 \begin{abstract}
    Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose \textbf{Hyena}, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized \textbf{long convolutions} and \textbf{data-controlled gating}. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than $50$ points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets ({\sc WikiText103} and {\sc The Pile}), reaching Transformer quality with a $20\%$ reduction in training compute required at sequence length $2$K. Hyena operators are twice as fast as highly optimized attention at sequence length $8$K, and $100\x$ faster at sequence length $64$K.
\end{abstract}
%

\setlength\abovedisplayshortskip{2pt}
\setlength\belowdisplayshortskip{2pt}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2pt}

% % % content here
\input{1_introduction.tex}
\input{2_preliminaries.tex}
\input{3_hyena.tex}
\input{4_experiments.tex}
\input{5_discussion.tex}
\input{6_ack.tex}

% % % bibliography
\bibliographystyle{abbrvnat}
\bibliography{_bibliography/main}
\clearpage
% % % appendix 
\appendix
%
\rule[0pt]{\columnwidth}{1pt}
\begin{center}
    \huge{Hyena Hierarchy} \\
    \vspace{0.3cm}
    \emph{Supplementary Material}
\end{center}
\rule[0pt]{\columnwidth}{1.5pt}
%
\doparttoc
\tableofcontents
%
\input{appendix/A.tex}
\input{appendix/B.tex}
\input{appendix/C.tex}
\input{appendix/D.tex}
%
\end{document}