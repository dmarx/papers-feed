\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2022)Arora, Narayan, Chen, Orr, Guha, Bhatia, Chami,
  Sala, and R{\'e}]{arora2022ask}
S.~Arora, A.~Narayan, M.~F. Chen, L.~J. Orr, N.~Guha, K.~Bhatia, I.~Chami,
  F.~Sala, and C.~R{\'e}.
\newblock Ask me anything: A simple strategy for prompting language models.
\newblock \emph{arXiv preprint arXiv:2210.02441}, 2022.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and
  Kritchman]{basri2020frequency}
R.~Basri, M.~Galun, A.~Geifman, D.~Jacobs, Y.~Kasten, and S.~Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{International Conference on Machine Learning}, pages
  685--694. PMLR, 2020.

\bibitem[Black et~al.(2021)Black, Gao, Wang, Leahy, and Biderman]{gpt-neo}
S.~Black, L.~Gao, P.~Wang, C.~Leahy, and S.~Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, Mar. 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5297715}.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen(1984)]{chen1984linear}
C.-T. Chen.
\newblock \emph{Linear system theory and design}.
\newblock Saunders college publishing, 1984.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Cramer(2021)]{cramer2021alphafold2}
P.~Cramer.
\newblock Alphafold2 and the future of structural biology.
\newblock \emph{Nature structural \& molecular biology}, 28\penalty0
  (9):\penalty0 704--705, 2021.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition workshops}, pages 702--703, 2020.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and R{\'e}]{dao2019learning}
T.~Dao, A.~Gu, M.~Eichhorn, A.~Rudra, and C.~R{\'e}.
\newblock Learning fast algorithms for linear transforms using butterfly
  factorizations.
\newblock In \emph{International conference on machine learning}, pages
  1517--1527. PMLR, 2019.

\bibitem[Dao et~al.(2022{\natexlab{a}})Dao, Chen, Sohoni, Desai, Poli, Grogan,
  Liu, Rao, Rudra, and R{\'e}]{dao2022monarch}
T.~Dao, B.~Chen, N.~S. Sohoni, A.~Desai, M.~Poli, J.~Grogan, A.~Liu, A.~Rao,
  A.~Rudra, and C.~R{\'e}.
\newblock Monarch: Expressive structured matrices for efficient and accurate
  training.
\newblock In \emph{International Conference on Machine Learning}, pages
  4690--4721. PMLR, 2022{\natexlab{a}}.

\bibitem[Dao et~al.(2022{\natexlab{b}})Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock \emph{arXiv preprint arXiv:2205.14135}, 2022{\natexlab{b}}.

\bibitem[Dao et~al.(2022{\natexlab{c}})Dao, Fu, Saab, Thomas, Rudra, and
  R{\'e}]{dao2022hungry}
T.~Dao, D.~Y. Fu, K.~K. Saab, A.~W. Thomas, A.~Rudra, and C.~R{\'e}.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models.
\newblock \emph{arXiv preprint arXiv:2212.14052}, 2022{\natexlab{c}}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
N.~Elhage, N.~Nanda, C.~Olsson, T.~Henighan, N.~Joseph, B.~Mann, A.~Askell,
  Y.~Bai, A.~Chen, T.~Conerly, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.

\bibitem[Fukushima and Miyake(1982)]{fukushima1982neocognitron}
K.~Fukushima and S.~Miyake.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of visual pattern recognition.
\newblock In \emph{Competition and cooperation in neural nets}, pages 267--285.
  Springer, 1982.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, et~al.]{gao2020pile}
L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang,
  H.~He, A.~Thite, N.~Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
S.~Garg, D.~Tsipras, P.~Liang, and G.~Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{arXiv preprint arXiv:2208.01066}, 2022.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020hippo}
A.~Gu, T.~Dao, S.~Ermon, A.~Rudra, and C.~R{\'e}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1474--1487, 2020.

\bibitem[Gu et~al.(2021)Gu, Goel, and R{\'e}]{gu2021efficiently}
A.~Gu, K.~Goel, and C.~R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{arXiv preprint arXiv:2111.00396}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
  Lakshminarayanan]{hendrycks2019augmix}
D.~Hendrycks, N.~Mu, E.~D. Cubuk, B.~Zoph, J.~Gilmer, and B.~Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock \emph{arXiv preprint arXiv:1912.02781}, 2019.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~d.~L. Casas, L.~A. Hendricks, J.~Welbl, A.~Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{huang2016deep}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In \emph{European conference on computer vision}, pages 646--661.
  Springer, 2016.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
N.~Kitaev, {\L}.~Kaiser, and A.~Levskaya.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2015)Li, Yang, Martin, Ho, and Ying]{li2015butterfly}
Y.~Li, H.~Yang, E.~R. Martin, K.~L. Ho, and L.~Ying.
\newblock Butterfly factorization.
\newblock \emph{Multiscale Modeling \& Simulation}, 13\penalty0 (2):\penalty0
  714--732, 2015.

\bibitem[Li et~al.(2022)Li, Cai, Zhang, Chen, and Dey]{li2022makes}
Y.~Li, T.~Cai, Y.~Zhang, D.~Chen, and D.~Dey.
\newblock What makes convolutional models great on long sequence modeling?
\newblock \emph{arXiv preprint arXiv:2210.09298}, 2022.

\bibitem[Li et~al.(2020)Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya,
  Stuart, and Anandkumar]{li2020fourier}
Z.~Li, N.~Kovachki, K.~Azizzadenesheli, B.~Liu, K.~Bhattacharya, A.~Stuart, and
  A.~Anandkumar.
\newblock Fourier neural operator for parametric partial differential
  equations.
\newblock \emph{arXiv preprint arXiv:2010.08895}, 2020.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic}
P.~Liang, R.~Bommasani, T.~Lee, D.~Tsipras, D.~Soylu, M.~Yasunaga, Y.~Zhang,
  D.~Narayanan, Y.~Wu, A.~Kumar, et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2211.09110}, 2022.

\bibitem[Massaroli et~al.(2020)Massaroli, Poli, Park, Yamashita, and
  Asama]{massaroli2020dissecting}
S.~Massaroli, M.~Poli, J.~Park, A.~Yamashita, and H.~Asama.
\newblock Dissecting neural odes.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3952--3963, 2020.

\bibitem[Mehta et~al.(2022)Mehta, Gupta, Cutkosky, and
  Neyshabur]{mehta2022long}
H.~Mehta, A.~Gupta, A.~Cutkosky, and B.~Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock \emph{arXiv preprint arXiv:2206.13947}, 2022.

\bibitem[Mildenhall et~al.(2021)Mildenhall, Srinivasan, Tancik, Barron,
  Ramamoorthi, and Ng]{mildenhall2021nerf}
B.~Mildenhall, P.~P. Srinivasan, M.~Tancik, J.~T. Barron, R.~Ramamoorthi, and
  R.~Ng.
\newblock Nerf: Representing scenes as neural radiance fields for view
  synthesis.
\newblock \emph{Communications of the ACM}, 65\penalty0 (1):\penalty0 99--106,
  2021.

\bibitem[Nguyen et~al.(2022)Nguyen, Goel, Gu, Downs, Shah, Dao, Baccus, and
  R{\'e}]{nguyen2022s4nd}
E.~Nguyen, K.~Goel, A.~Gu, G.~W. Downs, P.~Shah, T.~Dao, S.~A. Baccus, and
  C.~R{\'e}.
\newblock S4nd: Modeling images and videos as multidimensional signals using
  state spaces.
\newblock \emph{arXiv preprint arXiv:2210.06583}, 2022.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
C.~Olsson, N.~Elhage, N.~Nanda, N.~Joseph, N.~DasSarma, T.~Henighan, B.~Mann,
  A.~Askell, Y.~Bai, A.~Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Oppenheim et~al.(1997)Oppenheim, Willsky, Nawab, and
  Ding]{oppenheim1997signals}
A.~V. Oppenheim, A.~S. Willsky, S.~H. Nawab, and J.-J. Ding.
\newblock \emph{Signals and systems}, volume~2.
\newblock Prentice hall Upper Saddle River, NJ, 1997.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
D.~Paperno, G.~Kruszewski, A.~Lazaridou, Q.~N. Pham, R.~Bernardi, S.~Pezzelle,
  M.~Baroni, G.~Boleda, and R.~Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock \emph{arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[Peng(2021)]{PENG_RWKV-LM_2021}
B.~Peng.
\newblock {RWKV-LM}, 8 2021.
\newblock URL \url{https://github.com/BlinkDL/RWKV-LM}.

\bibitem[Polyak and Juditsky(1992)]{polyak1992ema}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM journal on control and optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and
  Misra]{power2022grokking}
A.~Power, Y.~Burda, H.~Edwards, I.~Babuschkin, and V.~Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Radford et~al.(2022)Radford, Kim, Xu, Brockman, McLeavey, and
  Sutskever]{radford2022robust}
A.~Radford, J.~W. Kim, T.~Xu, G.~Brockman, C.~McLeavey, and I.~Sutskever.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock \emph{arXiv preprint arXiv:2212.04356}, 2022.

\bibitem[Rae et~al.(2019)Rae, Potapenko, Jayakumar, Hillier, and
  Lillicrap]{raecompressive2019}
J.~W. Rae, A.~Potapenko, S.~M. Jayakumar, C.~Hillier, and T.~P. Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock \emph{arXiv preprint}, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.05507}.

\bibitem[Romero et~al.(2021{\natexlab{a}})Romero, Bruintjes, Tomczak, Bekkers,
  Hoogendoorn, and van Gemert]{romero2021flexconv}
D.~W. Romero, R.-J. Bruintjes, J.~M. Tomczak, E.~J. Bekkers, M.~Hoogendoorn,
  and J.~C. van Gemert.
\newblock Flexconv: Continuous kernel convolutions with differentiable kernel
  sizes.
\newblock \emph{arXiv preprint arXiv:2110.08059}, 2021{\natexlab{a}}.

\bibitem[Romero et~al.(2021{\natexlab{b}})Romero, Kuzina, Bekkers, Tomczak, and
  Hoogendoorn]{romero2021ckconv}
D.~W. Romero, A.~Kuzina, E.~J. Bekkers, J.~M. Tomczak, and M.~Hoogendoorn.
\newblock Ckconv: Continuous kernel convolution for sequential data.
\newblock \emph{arXiv preprint arXiv:2102.02611}, 2021{\natexlab{b}}.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer, 2015.

\bibitem[Roy et~al.(2021)Roy, Saffar, Vaswani, and Grangier]{roy2021efficient}
A.~Roy, M.~Saffar, A.~Vaswani, and D.~Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 53--68, 2021.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
I.~Schlag, K.~Irie, and J.~Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In \emph{International Conference on Machine Learning}, pages
  9355--9366. PMLR, 2021.

\bibitem[Selesnick and Burrus(2017)]{selesnick2017fast}
I.~W. Selesnick and C.~S. Burrus.
\newblock Fast convolution and filtering.
\newblock In \emph{The Digital Signal Processing Handbook}, pages 8--1. CRC
  Press, 2017.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and
  Wetzstein]{sitzmann2020implicit}
V.~Sitzmann, J.~N. Martel, A.~W. Bergman, D.~B. Lindell, and G.~Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock \emph{arXiv preprint arXiv:2006.09661}, 2020.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Tu et~al.(2022)Tu, Talebi, Zhang, Yang, Milanfar, Bovik, and
  Li]{tu2022maxvit}
Z.~Tu, H.~Talebi, H.~Zhang, F.~Yang, P.~Milanfar, A.~Bovik, and Y.~Li.
\newblock Maxvit: Multi-axis vision transformer.
\newblock \emph{arXiv preprint arXiv:2204.01697}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill,
  Levy, and Bowman]{wang2019superglue}
A.~Wang, Y.~Pruksachatkun, N.~Nangia, A.~Singh, J.~Michael, F.~Hill, O.~Levy,
  and S.~Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Wang, Yu, Shi, Jiang, Tay, Feng, and
  Yan]{yuan2021tokens}
L.~Yuan, Y.~Chen, T.~Wang, W.~Yu, Y.~Shi, Z.-H. Jiang, F.~E. Tay, J.~Feng, and
  S.~Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 558--567, 2021.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032, 2019.

\bibitem[Zhai et~al.(2021)Zhai, Talbott, Srivastava, Huang, Goh, Zhang, and
  Susskind]{zhai2021attention}
S.~Zhai, W.~Talbott, N.~Srivastava, C.~Huang, H.~Goh, R.~Zhang, and
  J.~Susskind.
\newblock An attention free transformer.
\newblock \emph{arXiv preprint arXiv:2105.14103}, 2021.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2022)Zhang, Backurs, Bubeck, Eldan, Gunasekar, and
  Wagner]{zhang2022unveiling}
Y.~Zhang, A.~Backurs, S.~Bubeck, R.~Eldan, S.~Gunasekar, and T.~Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task.
\newblock \emph{arXiv preprint arXiv:2206.04301}, 2022.

\bibitem[Zhong et~al.(2020)Zhong, Zheng, Kang, Li, and Yang]{zhong2020random}
Z.~Zhong, L.~Zheng, G.~Kang, S.~Li, and Y.~Yang.
\newblock Random erasing data augmentation.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 13001--13008, 2020.

\end{thebibliography}
