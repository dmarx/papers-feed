%
\section{Discussion and Additional Results}\label{app:add_results}
%

\paragraph{Vocabulary size scaling}
%

Table \ref{scaling_vsize} showcases interesting correlation between associative recall performance for varying vocabulary sizes and loss on the {\sc The Pile}. In this case, we fix sequence length for associative recall to be $2048$, the same sequence length used to train all models on the {\sc The Pile}.

We observe a similar phenomenon on other slices of tasks from our mechanistic design benchmarks, indicating that it may be possible to derive predictive laws for performance at scale, based on fast experimentation on synthetic tasks with models of $1$ or $2$ layers. Surprisingly, performance on our language synthetics appears to be further linked to performance as attention replacement in other domains (Appendix \ref{appendix:image-classification} for results on image classification).

\begin{table}[!bh]
\small
\centering
\caption{{\sf Hyena} Accuracy on associative recall with varying vocabulary size $10$, $20$, $30$, $40$ in relation to test loss on {\sc The Pile} after $5$ billion tokens. We notice a correlation between the two performance metrics, suggesting that slices of our mechanistic design synthetics may be potentially predictive of performance at scale.}
\vspace{2mm}
\label{scaling_vsize}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}c|ccccc@{}}
\toprule
Model & Acc @ $10$ & Acc @ $20$ & Acc @ $30$ & Acc @ $40$ & Loss @ $5$B on {\sc The Pile} \\
\midrule 
Conv1d & $32$ & $11$ & $10$ & $8$ & $4.21$\\ 
AFT-conv & $55$ & $21$ & $12$ & $10$ & $3.57$\\ 
H3 & $92$ & $60$ & $13$ & $10$ & $2.69$\\
Transformer & $100$ & $100$ & $92$ & $82$ & $2.59$\\ 
{\sf Hyena} & $100$ & $100$ & $98$ & $85$ & $2.59$\\ 
\bottomrule
\end{tabular}
\end{table}

\paragraph{Single layer recall}
%

All experiments on our synthetic tasks default to $2$ layer models. We choose $2$ as it is the canonical number for mechanistic analysis of Transformers \citep{elhage2021mathematical} based on \textit{circuits}. Interestingly, a single layer of {\sf Hyena} (width $64$) is capable of performing associative recall, solving the task completely even in the challenging setting with vocabulary size $40$. Reverse engineering exactly how the single {\sf Hyena} operator is able to perform recall is left for future work.

\subsection{Learning Arithmetic}
%
We showcase an additional task in our mechanistic design benchmark: learning arithmetic. We train {\sf Hyena} models of increasing depth ($1$, $2$ and $3$ layers) on a dataset of $D_n$-digit addition. As an example, a $3$-digit addition input sample is given by the sequence
\[ 
    {\tt 1, 2, 3, 9, 5, 4, 1, 0, 7, 7}
\]
where the first $6$ digits contain the two $3$ digits numbers to add, and the last $4$ the result. Our models are optimized using standard autoregressive training i.e., predicting the next token, since they are causal. In particular, we optimize models to learn a map $x \mapsto y$ where $x$ is the original prompt without the last element, and $y$ equal to $x$ shifted right by one position. We mask the first $2 D_n - 1$ elements of the loss for each sequence since they contain predictions for addends and not results.

We report results in Figure \ref{fig:arithmetic}. A single layer of {\sf Hyena} is able to learn to perform addition with up to $4$ digits. Longer numbers require deeper models. In our experiments, alternative architectures such as AFT-conv struggle to learn arithmetic, signaling a cap in capability.

%
\begin{figure}
    \centering
    \input{figures/source/addition_hyena}
    \vspace{-2mm}
    \caption{Test loss and accuracy of $\sf Hyena$ on addition with different numbers of digits and model depths. Each plot reports the results of a different experiment, with the curve tracing test results during training.}
    \label{fig:arithmetic}
\end{figure}