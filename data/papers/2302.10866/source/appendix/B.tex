\section{Theoretical Results and Details}
%
\subsection{Proofs}
%
\paragraph{Proof of Proposition \ref{prop:causality}}
\proof A discrete $L$-by-$L$ operator is causal if it is lower triangular, i.e., when there is no leakage of future input information to the output. The ${\sf Hyena}$ operator $\sH$ is the product of alternating diagonal and Toeplitz matrices. Thus, if all the Toeplitz matrices $\sS_h^n$ are lower triangular then $\sH$ is lower triangular. In turn, each $\sS_h^n$ is lower triangular if and only if the filter $h$ is causal, concluding the proof. 
\endproof
%
\subsection{Analysis of Data-Controlled Mechanisms}\label{app:surrogate_att}
%
 We discuss the surrogate attention mechanism of {\sf Hyena}-$2$: $q,k,v\mapsto y$:
%
\begin{equation}\label{eq:linear_attention}
    \begin{aligned}
        z_t &= k_t(\varphi * v)_t \\
        y_t &= q_t(\psi * z)_t
    \end{aligned}
\end{equation}
%
If $\varphi$ and $\psi$ are convolutions parametrized via state-space models (SSMs), the above resembles the H3 mechanism \citep{dao2022hungry}. We investigate the effect of the convolutional kernels $\varphi$ and $\psi$ on the attention layer. We start by introducing a matrix representation of the layer, and we isolate the \textit{attention matrix} $\sA_\varphi^\psi(q,k)$ such that
%
\begin{equation}\label{eq:linear_attention_matrix}
    \begin{aligned}
        y &= \sA_\varphi^\psi(q,k)v.
    \end{aligned}
\end{equation}
%
\paragraph{Isolating the surrogate attention matrix}
%
In the case of length-$L$ discrete sequences%, the convolutions become sums, and we can rewrite \eqref{eq:linear_attention_convolution2} as
%
\begin{equation}\label{eq:linear_attention_convolution3}
    \begin{aligned}
        z_t &= k_t \sum_{m=0}^{L-1} \varphi_{t-m} v_m \\
        y_t &= q_t \sum_{m=0}^{L-1} \psi_{t-m} z_m\\
    \end{aligned}
\end{equation}
%
Therefore we can rewrite \eqref{eq:linear_attention} as
%
\begin{equation}
    \begin{aligned}
        y_t &= q_t \sum_{m=0}^{L-1} \psi_{t-m} k_m \sum_{n=0}^{L-1} \varphi_{m-n} v_n\\
             &= q_t \sum_{m=0}^{L-1} \sum_{n=0}^{L-1} \psi_{t-m} k_m \varphi_{m-n} v_n &&\quad\text{Move $\psi$, $k$ inside inner sum}\\
             &= q_t \sum_{n=0}^{L-1} \sum_{m=0}^{L-1} \psi_{t-m} k_m \varphi_{m-n} v_n &&\quad\text{Index shift}\\
             &= \sum_{n=0}^{L-1} q_t \sum_{m=0}^{L-1} \psi_{t-m} k_{m} \varphi_{m-n} v_{n}\\
    \end{aligned}
\end{equation}
%
And we can define the surrogate attention matrix $\sA_\varphi^\psi(q,k)$
%
\begin{equation}\label{eq:linear_attention_matrix2}
    \begin{aligned}
        [\sA_\varphi^\psi(q,k))]_{t,t'} &= q_t \sum_{m=0}^{L-1} \psi_{t-m} k_{m} \varphi_{m - t'}. \\
    \end{aligned}
\end{equation}
%
\begin{tcolorbox}[enhanced, drop fuzzy shadow, breakable, frame hidden, sharp corners] {\bf Continuous Signals:} 
    We can also consider the case of continuous signals on a group $G$. In the continuous case, we can expand the convolutions in \eqref{eq:linear_attention} as
    %
    \begin{equation}\label{eq:linear_attention_convolution}
        \begin{aligned}
            (\varphi * v)_t = \int_G \varphi_{t-g} v_g \dd g,\qquad
            (\psi * z)_t = \int_G \psi_{t - g} z_g \dd g
        \end{aligned}
    \end{equation}
    %
    This allows us to rewrite \eqref{eq:linear_attention} as
    %
    \begin{equation}\label{eq:linear_attention_convolution2}
        \begin{aligned}
            y_t &= q_t(\psi * k(\varphi * v))_t \\
            &= q_t \int_G \psi_{t-g} \left[ k_g\int_G \varphi_{g - \tau} v_\tau \dd \tau \right] \dd g \\
            &= q_t \int_G \left[ \int_G \psi_{t-g} k_g \varphi_{g - \tau} v_\tau \dd \tau \right] \dd g\\
            &= q_t \int_G \left[ \int_G \psi_{t-g} k_g \varphi_{g - \tau} v_\tau \dd g \right] \dd \tau &&\quad\text{Variable swap}\\
            &= \int_G \left[ q_t \int_G \psi_{t-g} k_g \varphi_{g - \tau} v_\tau \dd g \right] \dd \tau &&\quad\text{Pull $q_t$ in $\tau $ integral}\\
            & = \int_G \left[ q_t \int_G \psi_{t-g} k_g \varphi_{g - \tau} \dd g \right] v_\tau \dd \tau &&\quad\text{Pull $v_\tau$ out of $g$ integral}.
        \end{aligned}
    \end{equation}
    %
    There is a linear operator $\cA: v \mapsto y=\cA v$ which we interpret as the surrogate attention operator. $\cA$ is conditioned on the \textit{query} $q$, \textit{key} $k$ and filters $\varphi$ and $\psi$, $\cA = \cA_{\varphi}^\psi(q,k)$. The kernel $\mathcal K$ of the operator is given by
    \begin{equation}
        \begin{aligned}
            \mathcal K(t,t') &= q_t\int_G \psi_{t-g} k_g \varphi_{g - t'} \dd g \\
        \end{aligned}
    \end{equation}
    % %
    % Then, we can rewrite \eqref{eq:linear_attention} as
    % %
    % \begin{equation}\label{eq:linear_attention_convolution2}
    %     \begin{aligned}
    %         y_t &= q_t(\psi * k_t(\varphi * v)_t)_t \\
    %         &= q_t \int \psi_{t-r} k_r\int \varphi_{r - s} v_s ds dr \\
    %         &= q_t \int\int \psi_{t-r} k_r \varphi_{r - s} v_s dr ds
    %     \end{aligned}
    % \end{equation}
    %
        %
\end{tcolorbox}
%
\paragraph{Operator decomposition of the surrogate attention matrix}
%
We can decompose the linear map $v\mapsto y;~ y = \sA_\varphi^\psi(q,k)v$ into a sequence of factors, each dependent on a projection of the input $\sA_\varphi^\psi(q,k) = \sA^\psi(q) \sA_\varphi(k)$. Let $\sD_q$ and $\sD_k$ be the $L$-by-$L$ diagonal matrices whose respective main diagonal entries are the respective entries of $q$ and $k$. Then, we have that
%
\begin{equation}\label{eq:linear_attention_matrix3}
    \begin{aligned}
        \sA^\psi(q) &= \sD_q \sS_\psi,\qquad \sD_q = \diag(q), \\
        \sA_\varphi(k) &= \sD_k\sS_\varphi,\qquad \sD_k = \diag(k).
    \end{aligned}
\end{equation}
%
The matrix has been decomposed into two terms $\sA^\psi(q)$ and $\sA_\varphi(k)$ constructed by multiplying the diagonal matrices $\sD_q$ and $\sD_k$ with the Toeplitz matrices $\sS_\psi$ and $\sS_\varphi$. $\sS_\psi$ and $\sS_\varphi$ are the kernels of the convolution operators with filter's impulse responses $\psi$ and $\varphi$ respectively. In the current applications of interest, $\psi$ and $\varphi$ are chosen to be causal, i.e. $\psi[t]=0 \text{ for } t<0$ and $\varphi[t]=0 \text{ for } t<0$. This results in $\sS_\psi$ and $\sS_\varphi$ to be lower triangular matrices 
%
\begin{equation}
    \sS_\psi = \begin{bmatrix}
        \psi_0 & 0 & \cdots & 0 \\
        \psi_1 & \psi_0 & \cdots & 0 \\
        \vdots & \ddots & \ddots & \vdots \\
        \psi_{L-1} & \psi_{L-2} & \cdots & \psi_0
    \end{bmatrix}, \qquad
    \sS_\varphi = \begin{bmatrix}
        \varphi_0 & 0 & \cdots & 0 \\
        \varphi_1 & \varphi_0 & \cdots & 0 \\
        \vdots & \ddots & \ddots & \vdots \\
        \varphi_{L-1} & \varphi_{L-2} & \cdots & \varphi_0
    \end{bmatrix}.
\end{equation}
%
The surrogate attention matrix is then given by
%
\begin{equation}
    \sA_\varphi^\psi(q,k) = \sD_q \sS_\psi \sD_k \sS_\varphi
\end{equation}
%
We can expand the matrix multiplications in \eqref{eq:linear_attention_matrix3} in the case of causal filters $\varphi$ and $\psi$ as
%
\begin{equation}\label{eq:linear_attention_matrix4}
    \begin{aligned}
        \underset{ 
        \begin{bmatrix}
            q_0 &  &  &  \\
             & q_1 &  &  \\
             &  & \ddots &\\
             &  &  & q_{L-1}
        \end{bmatrix}}{\displaystyle \sD_q}
        %
        \underset{ 
        \begin{bmatrix}
            \psi_0 & & &  \\
            \psi_1 & \psi_0 & &  \\
            \vdots & \ddots & \ddots & \\
            \psi_{L-1} & \psi_{L-2} & \cdots & \psi_0
        \end{bmatrix}}{\displaystyle \sS_\psi}
        %
        \underset{ 
        \begin{bmatrix}
            k_0 &  &  &  \\
             & k_1 &  &  \\
             &  & \ddots &\\
             &  &  & k_{L-1}
        \end{bmatrix}}{\displaystyle \sD_k}
        %
        \underset{
        \begin{bmatrix}
            \varphi_0 & & &  \\
            \varphi_1 & \varphi_0 & &  \\
            \vdots & \ddots & \ddots & \\
            \varphi_{L-1} & \varphi_{L-2} & \cdots & \varphi_0
        \end{bmatrix}}{\displaystyle \sS_\varphi} 
        %
        &\\
        %
        \underset{\displaystyle \sA_\psi(q)}{ 
        = \begin{bmatrix}
            q_0 \psi_0 &  &  &  \\
            q_1 \psi_1 & q_1 \psi_0 &  &  \\
            \vdots & \ddots & \ddots & \\
            q_{L-1} \psi_{L-1} & q_{L-1} \psi_{L-2} & \cdots & q_{L-1} \psi_0
        \end{bmatrix}}
        %
        \underset{\displaystyle \sA_\varphi(k)}{ 
        \begin{bmatrix}
            k_0 \varphi_0 &  &  &  \\
            k_1 \varphi_1 & k_1 \varphi_0 &  &  \\
            \vdots & \ddots & \ddots & \\
            k_{L-1} \varphi_{L-1} & k_{L-1} \varphi_{L-2} & \cdots & k_{L-1} \varphi_0
        \end{bmatrix}} &\\
        %
    \end{aligned}
\end{equation}
%
\begin{tcolorbox}[enhanced, breakable, frame hidden, drop fuzzy shadow, sharp corners] {\bf Fourier decomposition of convolution operators:} The kernels of the convolution operators $\sS_\psi$ and $\sS_\varphi$ are diagonalized by the Fourier transform matrix $\sW\in\bC^{L\times L},~ \sW_{nm} = z^{m}, ~ z = e^{j 2\pi n / L}$. The Fourier transform of the convolution operator $\sS_\psi$ is given by
%
\begin{equation}
    \sS_\psi = \sW^* \sD_\Psi \sW, \quad \sS_\Phi = \sW^* \sD_\Phi \sW
\end{equation}
%
where $\sD_\Psi, \sD_\Phi\in\bC^{L\times L}$ are diagonal matrices constructed from the frequency responses (the \textit{discrete Fourier transform}) $\Psi=\sW\psi,\Phi=\sW\varphi$, respectively. This decomposition can be used to simplify the matrix multiplication in \eqref{eq:linear_attention_matrix4}:
%
\begin{equation}
    \sA = \sD_q \sS_\psi \sD_k \sS_\varphi = \sD_q  \sW^* \sD_\Psi \sW \sD_k \sW^* \sD_\Phi \sW
\end{equation}
%
An important property of the above is the non-commutativity of $\sD_q$ and $\sS_k$ with $\sW*$. If the two operators commuted, we would obtain
%
\begin{equation}
    \boxed{
    \sA =  \sD_q  \sW^* \sD_\Psi \sW \sD_k \sW^* \sD_\Phi \sW = \sW^* \sD_q \sD_\Psi \sD_k \sD_\Phi \sW
    }
\end{equation}
%
which reduces the entire layer to a simple convolution. The non-commutativity of the \textit{gating} term acts as a non-linearity in chain of convolution operators.
%
\end{tcolorbox}

%

