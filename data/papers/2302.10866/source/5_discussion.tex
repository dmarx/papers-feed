%
\section{Discussion and Conclusion}
%
In this work, we introduced an attention-free drop-in replacement to the core building block of many large-scale language models. ${\sf Hyena}$ operators are a recurrence of gating and implicitly parametrized long convolutions, can be evaluated efficiently in subquadratic time, and can learn in-context on very long sequences. 
%
On {\sc The Pile}, deep stacks of ${\sf Hyena}$ operators constitute one of the first attention-free, convolutional architectures to match perplexity and downstream performance of Transformers with a significant reduction in training compute. 
%
Our promising results at the sub-billion parameter scale suggest that attention may not be all we need, and that simpler subquadratic designs such as ${\sf Hyena}$, informed by a set of simple guiding principles and evaluation on mechanistic interpretability benchmarks, may form the basis for efficient large models. We are excited about what new capabilities {\sf Hyena} opens up as we scale and optimize the inference speed of these models.