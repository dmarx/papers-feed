@inproceedings{transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}



@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}



@article{okano2000learning,
  title={Learning and memory},
  author={Okano, Hideyuki and Hirano, Tomoo and Balaban, Evan},
  journal={Proceedings of the National Academy of Sciences},
  volume={97},
  number={23},
  pages={12403--12404},
  year={2000},
  publisher={National Acad Sciences}
}



@article{LSTM,
  title={Long Short-term Memory},
  author={Schmidhuber, J{\"u}rgen and Hochreiter, Sepp},
  journal={Neural Computation MIT-Press},
  year={1997}
}


@article{hopfield1982neural,
  author={Hopfield, John J},
  title={Neural networks and physical systems with emergent collective computational abilities.},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@inproceedings{
liu2024itransformer,
title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
author={Yong Liu and Tengge Hu and Haoran Zhang and Haixu Wu and Shiyu Wang and Lintao Ma and Mingsheng Long},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=JePfAI8fah}
}




@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  pages={11121--11128},
  year={2023}
}


@inproceedings{zhang2022crossformer,
  title={Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting},
  author={Zhang, Yunhao and Yan, Junchi},
  booktitle={The eleventh international conference on learning representations},
  year={2023}
}

@article{li2023revisiting,
  title={Revisiting long-term time series forecasting: An investigation on linear mapping},
  author={Li, Zhe and Qi, Shiyi and Li, Yiduo and Xu, Zenglin},
  journal={arXiv preprint arXiv:2305.10721},
  year={2023}
}

@article{sun2024learning,
  title={Learning to (learn at test time): Rnns with expressive hidden states},
  author={Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and others},
  journal={arXiv preprint arXiv:2407.04620},
  year={2024}
}



@inproceedings{
wu2023timesnet,
title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ju_Uqw384Oq}
}



@article{
das2023longterm,
title={Long-term Forecasting with Ti{DE}: Time-series Dense Encoder},
author={Abhimanyu Das and Weihao Kong and Andrew Leach and Shaan K Mathur and Rajat Sen and Rose Yu},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=pCbC3aQB5W},
}



@inproceedings{
nie2023a,
title={A Time Series is Worth 64 Words:  Long-term Forecasting with Transformers},
author={Yuqi Nie and Nam H Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Jbdc0vTOcol}
}


@misc{patro2024simba,
      title={SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series}, 
      author={Badri N. Patro and Vijay S. Agneeswaran},
      year={2024},
      eprint={2403.15360},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}



@incollection{mandler2014structure,
  title={The structure of value: Accounting for taste},
  author={Mandler, George},
  booktitle={Affect and cognition},
  pages={3--36},
  year={2014},
  publisher={Psychology Press}
}



@article{bietti2024birth,
  title={Birth of a transformer: A memory viewpoint},
  author={Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wang2024longssm,
  title={LongSSM: On the Length Extension of State-space Models in Language Modelling},
  author={Wang, Shida},
  journal={arXiv preprint arXiv:2406.02080},
  year={2024}
}


@book{terry2017learning,
  title={Learning and memory: Basic principles, processes, and procedures},
  author={Terry, W Scott},
  year={2017},
  publisher={Routledge}
}

@article{cowan2008differences,
  title={What are the differences between long-term, short-term, and working memory?},
  author={Cowan, Nelson},
  journal={Progress in brain research},
  volume={169},
  pages={323--338},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{qin2024exploring,
  title={Exploring Transformer Extrapolation},
  author={Qin, Zhen and Zhong, Yiran and Deng, Hui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={18897--18905},
  year={2024}
}

@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}


@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}


@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}


@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{bulatov2022recurrent,
  title={Recurrent memory transformer},
  author={Bulatov, Aydar and Kuratov, Yury and Burtsev, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={11079--11091},
  year={2022}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}


@article{peng2024eagle,
  title={Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence},
  author={Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Du, Xingjian and Ferdinan, Teddy and Hou, Haowen and others},
  journal={arXiv preprint arXiv:2404.05892},
  year={2024}
}


@inproceedings{
hsieh2024ruler,
title={{RULER}: What{\textquoteright}s the Real Context Size of Your Long-Context Language Models?},
author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Boris Ginsburg},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=kIoBbc76Sy}
}




@inproceedings{zhang2023crossformer,
  title={Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting},
  author={Zhang, Yunhao and Yan, Junchi},
  booktitle={The eleventh international conference on learning representations},
  year={2023}
}

@article{liu2023itransformer,
  title={itransformer: Inverted transformers are effective for time series forecasting},
  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  journal={arXiv preprint arXiv:2310.06625},
  year={2023}
}

@article{nie2022time,
  title={A time series is worth 64 words: Long-term forecasting with transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal={arXiv preprint arXiv:2211.14730},
  year={2022}
}

@article{wang2024towards,
  title={Towards LifeSpan Cognitive Systems},
  author={Wang, Yu and Han, Chi and Wu, Tongtong and He, Xiaoxin and Zhou, Wangchunshu and Sadeq, Nafis and Chen, Xiusi and He, Zexue and Wang, Wei and Haffari, Gholamreza and others},
  journal={arXiv preprint arXiv:2409.13265},
  year={2024}
}


@inproceedings{irie2022dual,
  title={The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention},
  author={Irie, Kazuki and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9639--9659},
  year={2022},
  organization={PMLR}
}


@inproceedings{
wang2024memoryllm,
title={{MEMORYLLM}: Towards Self-Updatable Large Language Models},
author={Yu Wang and Yifan Gao and Xiusi Chen and Haoming Jiang and Shiyang Li and Jingfeng Yang and Qingyu Yin and Zheng Li and Xian Li and Bing Yin and Jingbo Shang and Julian McAuley},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=p0lKWzdikQ}
}


@inproceedings{
Khandelwal2020Generalization,
title={Generalization through Memorization: Nearest Neighbor Language Models},
author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HklBjCEKvH}
}

@article{he2024camelot,
  title={CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory},
  author={He, Zexue and Karlinsky, Leonid and Kim, Donghyun and McAuley, Julian and Krotov, Dmitry and Feris, Rogerio},
  journal={arXiv preprint arXiv:2402.13449},
  year={2024}
}


@misc{graves2014neuralturingmachines,
      title={Neural Turing Machines}, 
      author={Alex Graves and Greg Wayne and Ivo Danihelka},
      year={2014},
      eprint={1410.5401},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1410.5401}, 
}



@inproceedings{
kuratov2024babilong,
title={{BABIL}ong: Testing the Limits of {LLM}s with Long Context Reasoning-in-a-Haystack},
author={Yuri Kuratov and Aydar Bulatov and Petr Anokhin and Ivan Rodkin and Dmitry Igorevich Sorokin and Artyom Sorokin and Mikhail Burtsev},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=u7m2CG84BQ}
}


@article{munkhdalai2024leave,
  title={Leave no context behind: Efficient infinite context transformers with infini-attention},
  author={Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal={arXiv preprint arXiv:2404.07143},
  year={2024}
}


@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}



@inproceedings{zellers-etal-2019-hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      Farhadi, Ali  and
      Choi, Yejin",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M\`arquez, Llu\'\i s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1472/",
    doi = "10.18653/v1/P19-1472",
    pages = "4791--4800",
}


@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454/",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
}



@inproceedings{clark-etal-2019-boolq,
    title = "{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions",
    author = "Clark, Christopher  and
      Lee, Kenton  and
      Chang, Ming-Wei  and
      Kwiatkowski, Tom  and
      Collins, Michael  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1300/",
    doi = "10.18653/v1/N19-1300",
    pages = "2924--2936",
}


@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}



@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@inproceedings{paperno-etal-2016-lambada,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ\'an  and
      Lazaridou, Angeliki  and
      Pham, Ngoc Quan  and
      Bernardi, Raffaella  and
      Pezzelle, Sandro  and
      Baroni, Marco  and
      Boleda, Gemma  and
      Fern\'andez, Raquel",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1144/",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534"
}

@inproceedings{
merity2017pointer,
title={Pointer Sentinel Mixture Models},
author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Byj72udxe}
}

@article{nguyen2024hyenadna,
  title={Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution},
  author={Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Wornow, Michael and Birch-Sykes, Callum and Massaroli, Stefano and Patel, Aman and Rabideau, Clayton and Bengio, Yoshua and others},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}


@article{grevsova2023genomic,
  title={Genomic benchmarks: a collection of datasets for genomic sequence classification},
  author={Gre{\v{s}}ov{\'a}, Katar{\'\i}na and Martinek, Vlastimil and {\v{C}}ech{\'a}k, David and {\v{S}}ime{\v{c}}ek, Petr and Alexiou, Panagiotis},
  journal={BMC Genomic Data},
  volume={24},
  number={1},
  pages={25},
  year={2023},
  publisher={Springer}
}

@inproceedings{mullapudi2019online,
  title={Online model distillation for efficient video inference},
  author={Mullapudi, Ravi Teja and Chen, Steven and Zhang, Keyi and Ramanan, Deva and Fatahalian, Kayvon},
  booktitle={Proceedings of the IEEE/CVF International conference on computer vision},
  pages={3573--3582},
  year={2019}
}

@inproceedings{jain2011online,
  title={Online domain adaptation of a pre-trained cascade of classifiers},
  author={Jain, Vidit and Learned-Miller, Erik},
  booktitle={CVPR 2011},
  pages={577--584},
  year={2011},
  organization={IEEE}
}


@article{gandelsman2022test,
  title={Test-time training with masked autoencoders},
  author={Gandelsman, Yossi and Sun, Yu and Chen, Xinlei and Efros, Alexei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29374--29385},
  year={2022}
}



@inproceedings{zhang2006svm,
  title={SVM-KNN: Discriminative nearest neighbor classification for visual category recognition},
  author={Zhang, Hao and Berg, Alexander C and Maire, Michael and Malik, Jitendra},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  volume={2},
  pages={2126--2136},
  year={2006},
  organization={IEEE}
}


@article{bottou1992local,
  title={Local learning algorithms},
  author={Bottou, L{\'e}on and Vapnik, Vladimir},
  journal={Neural computation},
  volume={4},
  number={6},
  pages={888--900},
  year={1992},
  publisher={MIT Press}
}

@article{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{weston2014memory,
  title={Memory networks},
  author={Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  journal={arXiv preprint arXiv:1410.3916},
  year={2014}
}


@article{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}



@article{wu2022memorizing,
  title={Memorizing transformers},
  author={Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  journal={arXiv preprint arXiv:2203.08913},
  year={2022}
}


@article{rodkin2024associative,
  title={Associative recurrent memory transformer},
  author={Rodkin, Ivan and Kuratov, Yuri and Bulatov, Aydar and Burtsev, Mikhail},
  journal={arXiv preprint arXiv:2407.04841},
  year={2024}
}


@inproceedings{
zancato2024bmojo,
title={B'{MOJO}: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory},
author={Luca Zancato and Arjun Seshadri and Yonatan Dukler and Aditya Golatkar and Yantao Shen and Benjamin Bowman and Matthew Trager and Alessandro Achille and Stefano Soatto},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=RnQdRY1h5v}
}


@article{wu2020memformer,
  title={Memformer: A memory-augmented transformer for sequence modeling},
  author={Wu, Qingyang and Lan, Zhenzhong and Qian, Kun and Gu, Jing and Geramifard, Alborz and Yu, Zhou},
  journal={arXiv preprint arXiv:2010.06891},
  year={2020}
}


@article{wang2019r,
  title={R-transformer: Recurrent neural network enhanced transformer},
  author={Wang, Zhiwei and Ma, Yao and Liu, Zitao and Tang, Jiliang},
  journal={arXiv preprint arXiv:1907.05572},
  year={2019}
}


@inproceedings{le2020self,
  title={Self-attentive associative memory},
  author={Le, Hung and Tran, Truyen and Venkatesh, Svetha},
  booktitle={International conference on machine learning},
  pages={5682--5691},
  year={2020},
  organization={PMLR}
}


@article{bulatov2023scaling,
  title={Scaling transformer to 1m tokens and beyond with rmt},
  author={Bulatov, Aydar and Kuratov, Yuri and Kapushev, Yermek and Burtsev, Mikhail S},
  journal={arXiv preprint arXiv:2304.11062},
  year={2023}
}

@inproceedings{schmidhuber1993reducing,
  title={Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={ICANN’93: Proceedings of the International Conference on Artificial Neural Networks Amsterdam, The Netherlands 13--16 September 1993 3},
  pages={460--463},
  year={1993},
  organization={Springer}
}

@article{schmidhuber1992learning,
  title={Learning to control fast-weight memories: An alternative to recurrent nets. Accepted for publication in},
  author={Schmidhuber, JH},
  journal={Neural Computation},
  year={1992}
}

@inproceedings{schlag2021linear,
  title={Linear transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}


@article{hutchins2022block,
  title={Block-recurrent transformers},
  author={Hutchins, DeLesley and Schlag, Imanol and Wu, Yuhuai and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={33248--33261},
  year={2022}
}



@article{feng2022learn,
  title={Learn to remember: Transformer with recurrent memory for document-level machine translation},
  author={Feng, Yukun and Li, Feng and Song, Ziang and Zheng, Boyuan and Koehn, Philipp},
  journal={arXiv preprint arXiv:2205.01546},
  year={2022}
}



@article{dong2024flex,
  title={Flex Attention: A Programming Model for Generating Optimized Attention Kernels},
  author={Dong, Juechu and Feng, Boyuan and Guessous, Driss and Liang, Yanbo and He, Horace},
  journal={arXiv preprint arXiv:2412.05496},
  year={2024}
}


@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@inproceedings{munkhdalai2017neural,
  title={Neural semantic encoders},
  author={Munkhdalai, Tsendsuren and Yu, Hong},
  booktitle={Proceedings of the conference. Association for Computational Linguistics. Meeting},
  volume={1},
  pages={397},
  year={2017},
  organization={NIH Public Access}
}


@inproceedings{munkhdalai2017meta,
  title={Meta networks},
  author={Munkhdalai, Tsendsuren and Yu, Hong},
  booktitle={International conference on machine learning},
  pages={2554--2563},
  year={2017},
  organization={PMLR}
}

@article{irie2021going,
  title={Going beyond linear transformers with recurrent fast weight programmers},
  author={Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={7703--7717},
  year={2021}
}


@article{munkhdalai2019metalearned,
  title={Metalearned neural memory},
  author={Munkhdalai, Tsendsuren and Sordoni, Alessandro and Wang, Tong and Trischler, Adam},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@book{hebb2005organization,
  title={The organization of behavior: A neuropsychological theory},
  author={Hebb, Donald Olding},
  year={2005},
  publisher={Psychology press}
}

@article{prados1989neural,
  title={Neural network capacity using delta rule},
  author={Prados, DL and Kak, SC},
  journal={Electronics Letters},
  volume={25},
  number={3},
  pages={197--199},
  year={1989},
  publisher={IET}
}



@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{botev2024recurrentgemma,
  title={RecurrentGemma: Moving Past Transformers for Efficient Open Language Models},
  author={Botev, Aleksandar and De, Soham and Smith, Samuel L and Fernando, Anushan and Muraru, George-Cristian and Haroun, Ruba and Berrada, Leonard and Pascanu, Razvan and Sessa, Pier Giuseppe and Dadashi, Robert and others},
  journal={arXiv preprint arXiv:2404.07839},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{
qin2024hgrn,
title={{HGRN}2: Gated Linear {RNN}s with State Expansion},
author={Zhen Qin and Songlin Yang and Weixuan Sun and Xuyang Shen and Dong Li and Weigao Sun and Yiran Zhong},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=y6SqbJfCSk}
}


@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{
penedo2024the,
title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
author={Guilherme Penedo and Hynek Kydl{\'\i}{\v{c}}ek and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=n6SCkn2QaG}
}

@inproceedings{
mehta2023long,
title={Long Range Language Modeling via Gated State Spaces},
author={Harsh Mehta and Ankit Gupta and Ashok Cutkosky and Behnam Neyshabur},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5MkYIYCbva}
}


@inproceedings{hanLMInfinite2024,
    title = "{LM}-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
    author = "Han, Chi  and
      Wang, Qifan  and
      Peng, Hao  and
      Xiong, Wenhan  and
      Chen, Yu  and
      Ji, Heng  and
      Wang, Sinong",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.222",
    doi = "10.18653/v1/2024.naacl-long.222",
    pages = "3991--4008"
}



@inproceedings{
xiao2024efficient,
title={Efficient Streaming Language Models with Attention Sinks},
author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NG7sS51zVF}
}


@article{dong2024hymba,
  title={Hymba: A Hybrid-head Architecture for Small Language Models},
  author={Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Van Keirsbilck, Matthijs and Chen, Min-Hung and Suhara, Yoshi and others},
  journal={arXiv preprint arXiv:2411.13676},
  year={2024}
}


@article{sukhbaatar2019augmenting,
  title={Augmenting self-attention with persistent memory},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:1907.01470},
  year={2019}
}

@inproceedings{
gu2022efficiently,
title={Efficiently Modeling Long Sequences with Structured State Spaces},
author={Albert Gu and Karan Goel and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=uYLFoz1vlAC}
}

@inproceedings{
smith2023simplified,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@article{zhang2024memory,
  title={Memory Mosaics},
  author={Zhang, Jianyu and Nolte, Niklas and Sadhukhan, Ranajoy and Chen, Beidi and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:2405.06394},
  year={2024}
}

@article{cetin2024evolved,
  title={An Evolved Universal Transformer Memory},
  author={Cetin, Edoardo and Sun, Qi and Zhao, Tianyu and Tang, Yujin},
  journal={arXiv preprint arXiv:2410.13166},
  year={2024}
}

@article{berges2024memory,
  title={Memory Layers at Scale},
  author={Berges, Vincent-Pierre and O{\u{g}}uz, Barlas and Haziza, Daniel and Yih, Wen-tau and Zettlemoyer, Luke and Gosh, Gargi},
  journal={arXiv preprint arXiv:2412.09764},
  year={2024}
}


@inproceedings{zintgraf2019fast,
  title={Fast context adaptation via meta-learning},
  author={Zintgraf, Luisa and Shiarli, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={7693--7702},
  year={2019},
  organization={PMLR}
}

@article{nichol2018first,
  title={On first-order meta-learning algorithms},
  author={Nichol, A},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@article{bayat2024pitfalls,
  title={The Pitfalls of Memorization: When Memorization Hurts Generalization},
  author={Bayat, Reza and Pezeshki, Mohammad and Dohmatob, Elvis and Lopez-Paz, David and Vincent, Pascal},
  journal={arXiv preprint arXiv:2412.07684},
  year={2024}
}

@inproceedings{
staab2024beyond,
title={Beyond Memorization: Violating Privacy via Inference with Large Language Models},
author={Robin Staab and Mark Vero and Mislav Balunovic and Martin Vechev},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=kmn0BhQk7p}
}


@inproceedings{leybzon2024learning,
  title={Learning, Forgetting, Remembering: Insights From Tracking LLM Memorization During Training},
  author={Leybzon, Danny and Kervadec, Corentin},
  booktitle={Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP},
  pages={43--57},
  year={2024}
}

@article{schwarzschild2024rethinking,
  title={Rethinking llm memorization through the lens of adversarial compression},
  author={Schwarzschild, Avi and Feng, Zhili and Maini, Pratyush and Lipton, Zachary C and Kolter, J Zico},
  journal={arXiv preprint arXiv:2404.15146},
  year={2024}
}

@inproceedings{
yang2024parallelizing,
title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
author={Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=y8Rm4VNRPH}
}

@incollection{widrow1988adaptive,
  title={Adaptive switching circuits},
  author={Widrow, Bernard and Hoff, Marcian E},
  booktitle={Neurocomputing: foundations of research},
  pages={123--134},
  year={1988}
}

@inproceedings{
yang2024gatedattn,
title={Gated Linear Attention Transformers with Hardware-Efficient Training},
author={Songlin Yang and Bailin Wang and Yikang Shen and Rameswar Panda and Yoon Kim},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=ia5XvxFUJT}
}

@inproceedings{
kacham2024polysketchformer,
title={PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels},
author={Praneeth Kacham and Vahab Mirrokni and Peilin Zhong},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=ghYrfdJfjK}
}


@inproceedings{dai2019transformerxl,
  added-at = {2020-04-17T13:56:20.000+0200},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G. and Le, Quoc Viet and Salakhutdinov, Ruslan},
  biburl = {https://www.bibsonomy.org/bibtex/22fdc4a961b16b4fa36c61feedbfb82db/nosebrain},
  booktitle = {ACL (1)},
  editor = {Korhonen, Anna and Traum, David R. and Màrquez, Lluís},
  isbn = {978-1-950737-48-2},
  pages = {2978-2988},
  publisher = {Association for Computational Linguistics},
  title = {Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  year = 2019
}


@article{beck2024xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.04517},
  year={2024}
}

@article{tiezzi2024resurgence,
  title={On the resurgence of recurrent models for long sequences: Survey and research opportunities in the transformer era},
  author={Tiezzi, Matteo and Casoni, Michele and Betti, Alessandro and Guidi, Tommaso and Gori, Marco and Melacci, Stefano},
  journal={arXiv preprint arXiv:2402.08132},
  year={2024}
}


@article{van2018unreasonable,
  title={The unreasonable effectiveness of the forget gate},
  author={Van Der Westhuizen, Jos and Lasenby, Joan},
  journal={arXiv preprint arXiv:1804.04849},
  year={2018}
}

@article{behrouz2024mambamixer,
  title={Mambamixer: Efficient selective state space models with dual token and channel selection},
  author={Behrouz, Ali and Santacatterina, Michele and Zabih, Ramin},
  journal={arXiv preprint arXiv:2403.19888},
  year={2024}
}

@inproceedings{
hasani2023liquid,
title={Liquid Structural State-Space Models},
author={Ramin Hasani and Mathias Lechner and Tsun-Hsuan Wang and Makram Chahine and Alexander Amini and Daniela Rus},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=g4OTKRKfS7R}
}


@article{greff2016lstm,
  title={LSTM: A search space odyssey},
  author={Greff, Klaus and Srivastava, Rupesh K and Koutn{\'\i}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"u}rgen},
  journal={IEEE transactions on neural networks and learning systems},
  volume={28},
  number={10},
  pages={2222--2232},
  year={2016},
  publisher={IEEE}
}

@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT press}
}

@inproceedings{
peng2023rwkv,
title={{RWKV}: Reinventing {RNN}s for the Transformer Era},
author={Bo Peng and Eric Alcaide and Quentin Gregory Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Nguyen Chung and Leon Derczynski and Xingjian Du and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bart{\l}omiej Koptyra and Hayden Lau and Jiaju Lin and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Johan S. Wind and Stanis{\l}aw Wo{\'z}niak and Zhenyuan Zhang and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=7SaXczaBpG}
}


@article{de2024griffin,
  title={Griffin: Mixing gated linear recurrences with local attention for efficient language models},
  author={De, Soham and Smith, Samuel L and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and others},
  journal={arXiv preprint arXiv:2402.19427},
  year={2024}
}


@inproceedings{
merrill2024the,
title={The Illusion of State in State-Space Models},
author={William Merrill and Jackson Petty and Ashish Sabharwal},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=QZgo9JZpLq}
}


@article{liu2024longhorn,
  title={Longhorn: State space models are amortized online learners},
  author={Liu, Bo and Wang, Rui and Wu, Lemeng and Feng, Yihao and Stone, Peter and Liu, Qiang},
  journal={arXiv preprint arXiv:2407.14207},
  year={2024}
}

@software{rwkv-repo,
author = {Peng, Bo},
doi = {10.5281/zenodo.5196577},
month = aug,
title = {{RWKV-LM}},
url = {https://github.com/BlinkDL/RWKV-LM},
version = {1.0.0},
year = {2021}
}

@article{ren2024samba,
  title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling},
  author={Ren, Liliang and Liu, Yang and Lu, Yadong and Shen, Yelong and Liang, Chen and Chen, Weizhu},
  journal={arXiv preprint arXiv:2406.07522},
  year={2024}
}


@inproceedings{
arora2024simple,
title={Simple linear attention language models balance the recall-throughput tradeoff},
author={Simran Arora and Sabri Eyuboglu and Michael Zhang and Aman Timalsina and Silas Alberti and James Zou and Atri Rudra and Christopher Re},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=e93ffDcpH3}
}



@article{sun2023retentive,
  title={Retentive network: A successor to transformer for large language models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}

@article{chen2021scatterbrain,
  title={Scatterbrain: Unifying sparse and low-rank attention},
  author={Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17413--17426},
  year={2021}
}

@article{aksenov2024linear,
  title={Linear Transformers with Learnable Kernel Functions are Better In-Context Models},
  author={Aksenov, Yaroslav and Balagansky, Nikita and Vaina, Sofia Maria Lo Cicero and Shaposhnikov, Boris and Gorbatovski, Alexey and Gavrilov, Daniil},
  journal={arXiv preprint arXiv:2402.10644},
  year={2024}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}


@inproceedings{
choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}


@inproceedings{flashattention-1,
 author = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {16344--16359},
 publisher = {Curran Associates, Inc.},
 title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{
dao2024flashattention,
title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
author={Tri Dao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=mZn2Xyh9Ec}
}


@article{dao2024transformers,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@inproceedings{
gu2024mamba,
title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
author={Albert Gu and Tri Dao},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=tEYskw1VY2}
}


@inproceedings{orvieto2023resurrecting,
  title={Resurrecting recurrent neural networks for long sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle={International Conference on Machine Learning},
  pages={26670--26698},
  year={2023},
  organization={PMLR}
}

@article{yang2024gated,
  title={Gated Delta Networks: Improving Mamba2 with Delta Rule},
  author={Yang, Songlin and Kautz, Jan and Hatamizadeh, Ali},
  journal={arXiv preprint arXiv:2412.06464},
  year={2024}
}

@article{willingham1997systems,
  title={Systems of memory in the human brain},
  author={Willingham, Daniel B},
  journal={Neuron},
  volume={18},
  number={1},
  pages={5--8},
  year={1997},
  publisher={Elsevier}
}


@inproceedings{
fu2023hungry,
title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
author={Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=COZDy0WYGg}
}


@inproceedings{wu2019long,
  title={Long-term feature banks for detailed video understanding},
  author={Wu, Chao-Yuan and Feichtenhofer, Christoph and Fan, Haoqi and He, Kaiming and Krahenbuhl, Philipp and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={284--293},
  year={2019}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}






@article{williams1989learning,
  title={A learning algorithm for continually running fully recurrent neural networks},
  author={Williams, Ronald J and Zipser, David},
  journal={Neural computation},
  volume={1},
  number={2},
  pages={270--280},
  year={1989},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}