\section{Experiments}\label{sec:experiments}
\lettrine[lines=3]{N}{}ext, we evaluate the performance of Titans and its variants in language modeling, commonsense reasoning, needle in haystack, DNA modeling, and time series forecasting tasks\footnote{In the first version of the work, we aim to provide insights/evidences about why the learning paradigms of Titans are effective. We are working on finalizing the results of larger models and will report them in the next version.}. In more details, in this section, we answer the following empirical questions: (1) How do Titans perform compared to baselines in downstream tasks? (\textcolor{c1}{see \S}\ref{sec:exp-lm}, \textcolor{c1}{\S}\ref{sec:exp-timeseries}, \textcolor{c1}{and \S}\ref{sec:exp-DNA}); (2) What is the actual context length of Titans? (\textcolor{c1}{see \S}\ref{sec:exp-hystack} \textcolor{c1}{and \S}\ref{sec:exp-babilong}); (3) How do Titans scale with respect to context length? (\textcolor{c1}{see \S}\ref{sec:exp-efficiency}); (4) How the depth of memory can affect both performance and efficiency? (\textcolor{c1}{see \S}\ref{sec:deep-memory-exp}); and (5) What is the contribution of each Titans' component in its performance? (\textcolor{c1}{see~\S}\ref{sec:exp-ablation}).

\subsection{Experimental Setup}\label{sec:exp-setup}
\head{Models}
In our experiments, we focus on the three variants of Titans, which we refer to as: Titans with (1) Memory as a Context (MAC), (2) Memory as a Gate (MAG), and (3) Memory as a Layer (MAL) as well as (4) neural memory module alone. The reason behind using our long-term memory as a separate module is based on our definition of learning. As discussed in \autoref{sec:intro}, we define learning a process for acquiring effective and useful memory. Accordingly, we expect our long-term memory to effectively learn from data, even without attention. For each of these models, we consider four scales with: (i) 170M, (ii) 340M,  (iii) 400M, and (iv) 760M parameters. While the first three are trained on 15B tokens sampled from FineWeb-Edu dataset~\citep{penedo2024the}, the last one is trained on 30B tokens from the same dataset. 



\input{Tables/table-language-modeling}

\head{Baselines}
We compare our models with the state-of-the-art linear recurrent models, Transformers, and hybrid models (recurrent + attention). More specifically in language tasks, we compare with Transformer++~\citep{touvron2023llama}, RetNet~\citep{sun2023retentive}, Gated Linear Attention (GLA)~\citep{yang2024gatedattn}, Mamba~\citep{gu2024mamba}, Mamba2~\citep{dao2024transformers}, DeltaNet~\citep{yang2024parallelizing}, TTT~\citep{sun2024learning}, and Gated DeltaNet~\citep{yang2024gated}. In needle in haystack tasks, we also compare with GPT4~\citep{achiam2023gpt}, Llama3 with RAG~\citep{touvron2023llama}, RecurrentGemma2-9B~\citep{botev2024recurrentgemma}, and Mistral~\citep{jiang2023mistral} models, all of which are provided in the benchmark~\citep{kuratov2024babilong}. In time series tasks, we compare with Mamba-based~\citep{behrouz2024mambamixer}, Transformer-based~\citep{nie2022time, liu2023itransformer, zhang2023crossformer}, and linear models~\citep{das2023longterm, wu2023timesnet, zeng2023transformers, li2023revisiting}. 

\head{Training}
In the training, we follow the training procedure of \citet{yang2024gated}, and use LLama 2 tokenizer with a vocabulary size of 32K and use training length of 4K tokens. We employ AdamW optimizer with learning rate of $4e$-$4$ with cosine annealing schedule with batch size of 0.5M tokens, and weight decay of $0.1$.   


\subsection{Language Modeling}\label{sec:exp-lm}
We first focus on the perplexity in language modeling and also commonsense reasoning tasks. The results for Titans' variants and also baselines with three different sizes of 340M, 400M, and 760M are reported in \autoref{tab:lm_results}. Among non-hybrid models, including Transformer++, our neural memory module achieves the best performance in both perplexity and accuracy measures. Comparing our neural memory module and TTT, which is also a gradient-based recurrent model can show us the importance of our weight decay as well as the momentum. As discussed earlier, the weight decay can be interpreted as a gating mechanism to forget the past data, when it is needed. Also, momentum can help us better manage the memory by providing additional memory for the surprise metric. While some baselines also take advantage of gating mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, the superior performance of our neural memory module shows the importance of both our surprise mechanism and having deep and non-linear memory. We further discuss the later in \autoref{sec:deep-memory-exp}. 

Comparing the hybrid models, we found that all three variants of Titans (MAC, MAG, and MAL) outperform both Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + atttention). We attribute the superior performance of Titans (MAL) to the power of neural memory module as the architecture design and used attention are all the same. Comparing Titans (MAG) and (MAC), we find that while their performance are close, MAC performs better when dealing with longer dependencies in the data. Interestingly, both MAG and MAC outperform MAL variant, which due to using the same modules, we attribute this to the architecture design of these models. This finding is particularly important as the current hybrid models (except Hymba~\citep{dong2024hymba}) in the literature are using MAL-style combination of recurrent models and attention. 



\input{Tables/tab-hystack}

\subsection{Needle in a Haystack}\label{sec:exp-hystack}
Scaling a model to longer context window is not always equivalent to being effective for very long sequences~\citep{hsieh2024ruler}. The needle-in-a-haystack (NIAH) task is designed to measure the actual effective context length of models. In this task, we evaluate the model on retrieving a piece of information (i.e., the ``needle'') from long distractor texts (i.e., the ``haystack''). In this part, we use Single NIAH (S-NIAH) task from RULER benchmark~\citep{hsieh2024ruler} and evaluate Titans and baselines on sequences with length 2K, 4K, 8K, and 16K. The results are reported in \autoref{tab:hystack}. Neural Memory module achieves the best results compare to baselines in all three tasks. We attribute this superior performance to three key differences of Titans with existing sequence models: (1) Compared to TTT, our Neural Memory can better handle the memory capacity by using momentum and also the forgetting mechanism (i.e., weight decay). Therefore, with increasing the sequence length, the performance of Neural Memory does not drop and show a consistent trend; (2) Compared to Mamba2, which has the gating (forgetting) mechanism, Titans have deep non-linear memory, resulting in better memory management. Also, contrary to our neural memory and DeltaNet, Mamba2 is not capable of removing a memory and so we can see a significant drop in performance when increasing the sequence length; (3) Compared to DeltaNet, although it is capable of removing memory using delta rule, it cannot erase the memory, lacking forgetting mechanism. Finally, As expected we can see on par or better results when using Titans variants, where the best results correspond to MAC.  




\begin{figure*}[t!]
    \centering
    \begin{subfigure}{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/BABILong-few-shot.png}
    \caption{Few-shot Setup}
    \label{fig:babilong-zero-shot}
    \end{subfigure}~
    \centering~\hspace{4ex}
    \begin{subfigure}{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/BABILong-FT.png}
    \caption{Fine-Tuning Setup}
    \label{fig:babilong-fine-tune}
    \end{subfigure}
    \caption{Performance of Titans and baselines on BABILong benchmark. Titans (MAC) outperforms all baselines, including extremely large models, e.g., GPT4.}
    \vspace{-2ex}
\end{figure*}



\subsection{BABILong Benchmark}\label{sec:exp-babilong}
In the previous section we discussed the results on a simple NIAH tasks where a single needle needs to be retrieved. Although Titans showed better performance compared to baselines, their true advantage over very long sequences is still hidden. To this end, in this section, we use a harder task from BABILong benchmark~\citep{kuratov2024babilong}, in which the model needs to reason across facts distributed in extremely long documents. We follow the original experimental setup and training process in the benchmark. There are two settings: (1) Few-shot setting, in which we use large pre-trained models, and (2) fine-tuning setting, where we fine-tune the MAC variant of Titans to compare it with other fine-tuned baselines. The results for few-shot setting are reported in \autoref{fig:babilong-zero-shot}. In this setup, we can see Titans outperform all baselinesâ€“i.e., Mamba2.8B~\citep{gu2024mamba}, RWKV-6-7B~\citep{peng2024eagle}, RecurrentGemma-9B~\citep{botev2024recurrentgemma}, Gemma-9B~\citep{team2024gemma}, Llama3.1-8B~\citep{touvron2023llama}, GPT-4, and GPT4o-mini~\citep{achiam2023gpt}. These results are achieved while Titans (MAC) is having much less number of parameters than baselines. 


In the fine-tuning setup, we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba~\citep{gu2024mamba}, RMT~\citep{bulatov2022recurrent}, (ii) large models with Retrieval-Augmented Generation (RAG)~\citep{lewis2020retrieval} such as Llama3.1-8B~\citep{touvron2023llama}, and (iii) extremely large models such as GPT-4~\citep{achiam2023gpt}, GPT4o-mini, Qwen2.5-72B~\citep{yang2024qwen2}, and Llama3.1-70B~\citep{touvron2023llama}. Baseline results are reported by \citep{kuratov2024babilong}. The results of Titans and baselines are reported in \autoref{fig:babilong-fine-tune}. Titans outperform all models even extremely large models like GPT4. Also, compared to Transformer-based with memory models like RMT, Titans show better performance mainly due to their powerful memory. That is, RMT compress the historical data into 16 size vector-valued memory, while Titans with in-context online memory learner are capable of encoding the past into the parameters of the model. Interestingly, even augmenting Llama3.1-8B model with RAG performs worse than Titans with about $\times$70 less parameters.       










\begin{figure*}[t!]
    \centering
    \begin{subfigure}{0.333\linewidth}
        \includegraphics[width=\linewidth]{Figures/deep-memory-1.png}
    \caption{170M Parameters}
    \end{subfigure}~
    \centering
    \begin{subfigure}{0.333\linewidth}
        \includegraphics[width=\linewidth]{Figures/deep-memory-2.png}
    \caption{360M Parameters}
    \end{subfigure}~
    \centering
    \begin{subfigure}{0.333\linewidth}
        \includegraphics[width=\linewidth]{Figures/deep-memory-3.png}
    \caption{760M Parameters}
    \end{subfigure}
    \caption{The effect of memory depth on the perplexity. Deeper long-term memory results in better scaling in longer sequences.}
    \label{fig:effect-deep-memory}
\end{figure*}


\subsection{The Effect of Deep Memory}\label{sec:deep-memory-exp}
In this section, we evaluate the effect of deep memory in both wall-clock training time and model performance\footnote{Note that, in this experiment, we only focus on the neural memory module to evaluate the effect of memory depth in the memorization process. Combining neural memory with attention as we do in Titans variants, can additionally enhance the performance of the model over long sequences.}. To this end, we focus on different variants of our neural memory module, where $L_{\M} = 1, 2, 3, 4$. We also use Mamba as a baseline for the model performance. For a fair comparison, we use the same training process for all models and train them on a subset of the Pile dataset~\citep{gao2020pile}. 

We report the perplexity of our models and baselines as the function of the sequence length in \autoref{fig:effect-deep-memory}. Interestingly, with the increase of memory depth, $L_{\M}$, the model can achieve better perplexity over all sequence length. Also, deeper memory modules are more robust to the sequence length when the model has less number of parameters. With the increase of the number of parameters, all models show better performance on longer sequences.  


\begin{wrapfigure}{r}{0.33\linewidth}
    \centering
    \vspace{-4ex}
        \includegraphics[width=\linewidth]{Figures/deep-memory-efficiency.png}
    \caption{The effect of memory depth on training throughput}
    \label{fig:effect-deep-memory-efficiency}
    \vspace{-8ex}
\end{wrapfigure}

We also evaluate the effect of memory depth ($L_{\M} = 1, 2, 3, 4$) on the training throughput. We report the training throughput (the number of tokens per second) as the function of sequence length in \autoref{fig:effect-deep-memory-efficiency}. All models scale linearly with respect to the context length (i.e., constant trend in the number of tokens per second with respect to sequence length). Also, by increasing the memory depth, as expected, we can see a linear trend that a deeper memory results in a slower training. Therefore, it is not always efficient to use deeper memory modules, showing a trade-off between effectiveness and efficiency. 


\input{Tables/tab-timeseries}

\vspace{2ex}
\subsection{Time Series Forecasting}\label{sec:exp-timeseries}
To show the effectiveness of our memory module in a broader tasks, we also evaluate its performance in time series forecasting tasks. To this end, we use Simba framework~\citep{patro2024simba} for time series forecasting, and replace its Mamba module with our neural memory. We report the results on common time series forecasting benchmark datasetsâ€“ETT, ECL, Traffic, and Weather~\citep{zhou2021informer}. The results are reported in \autoref{tab:avg_baseline_results}. Our neural memory module is outperforming all baselines, including Mamba-based, linear-based, and Transformer-based architectures. 




\input{Tables/table-dna}

\subsection{DNA Modeling}\label{sec:exp-DNA}
In order to understand the capability of Titans beyond natural language, we further evaluate the performance of our neural memory module on DNA modeling tasks. To this end, we evaluate pre-trained models on the downstream tasks in GenomicsBenchmarks~\citep{grevsova2023genomic}. We follow the same experimental setups from \citet{nguyen2024hyenadna}, and re-use the reported results of baselines by \citet{arora2024simple}. The performance of Titans (LMM) and baselines are reported in \autoref{table:genomics_benchmarks}.  We find that LMM is competitive with state-of-the-art architectures across different downstream genomics tasks.





\subsection{Efficiency}\label{sec:exp-efficiency}
\begin{wrapfigure}{r}{0.33\linewidth}
    \centering
    \vspace{-3ex}
        \includegraphics[width=\linewidth]{Figures/efficiency.png}
    \caption{Training throughput comparison of Titans and baselines. }
    \label{fig:efficiency}
    \vspace{-6ex}
\end{wrapfigure}
In this part, we compare the efficiency of our neural memory as well as Titans with state-of-the-art sequence models. The training throughput of models for different \texttt{sequence length $\times$ batch size} are reported in \autoref{fig:efficiency}. Comparing recurrent models, including our neural memory module, we can see our memory module is slightly slower than Mamba2 and Gated DeltaNet, mainly due to: (1) having deep memory and more expressive
transition process (memory update), and (2) highly optimized kernel in the implementation of Mamba2. Interestingly, Titans (MAL) are faster than baselines as well as the memory module. The main reason for this better throughput is the highly optimized kernel of Flash-Attention~\citep{dao2024flashattention}, which is used for implementing SWA and full attention module in Titans.  



\subsection{Ablation Study}\label{sec:exp-ablation}
Finally, we perform ablation studies on the different architectural choices in Titans. We consider our neural memory module as a base model and then changing one component at a time: (1) replacing deep memory with linear memory, removing (2) convolution, (3) momentum in the surprise measure, (4) weight decay (or forgot mechanism), and (5) persistent memory. The results are reported in \autoref{tab:ablation}. All components of neural memory design are positively contributing to its performance, where the greatest contribution comes from weight decay, momentum, convolution, and persistent memory, respectively. 




\input{Tables/tab-ablation}

\head{The Effect of Architectural Design}
To evaluate the effect of architecture design, we compare the performance of three represented variants of Titans in three aspects of (i) language modeling, (ii) commen-sense reasoning, and (iii) long context NIAH (BABILong) tasks. The results are reported in \autoref{tab:ablation}. We find that MAC and MAG have close performance in language modeling and common-sense reasoning tasks, while MAC achieve significantly better performance in long-context NIAH. Both of these models achieve better performance than MAL. These results along with \autoref{fig:efficiency}, show a trade-off between fast training and more expressive design.  