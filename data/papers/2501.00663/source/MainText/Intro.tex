\vspace{5ex}
\section{Introduction}\label{sec:intro}
\vspace{-8ex}
\epigraph{``The true art of memory is the art of attention!"}{--- \textup{Samuel Johnson}, 1787}
\vspace{1ex}

\lettrine[lines=3]{T}{}ransformers, pure attention-based architectures~\citep{transformers}, have been firmly established as state-of-the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale~\citep{kaplan2020scaling}. The primary building blocks of Transformers–attention modules—function as associative memory blocks~\citep{bietti2024birth}, where they learn to store key-value associations and retrieve them by computing pairwise similarity between queries (i.e., search signals) and keys (i.e., contexts). Accordingly, by design, the output of a Transformer is exclusively conditioned on the direct dependencies of tokens in the \emph{current} context window. This accurate modeling of dependencies, however, comes with quadratic time and memory complexity in terms of the context length. In complex real-world tasks (e.g., language modeling~\citep{liu2024lost}, video understanding~\citep{wu2019long}, long-term time series forecasting~\citep{zhou2021informer}), the context window can become extremely large, making the applicability of Transformers challenging in these downstream tasks.  


To overcome the scalability issue of Transformers, recent studies aim to design different variants of linear Transformers~\citep{katharopoulos2020transformers, kacham2024polysketchformer, yang2024gatedattn}, where softmax is replaced by a kernel function in the attention (\textcolor{c1}{see \S}\ref{sec:background} \textcolor{c1}{for details}), resulting in a significant drop in memory consumption. Despite efficiency and the ability to scale to longer context, linear Transformers do not show competitive performance compared to Transformers as the kernel trick makes the model a linear recurrent network, in which the data is compressed into a matrix-valued states~\citep{katharopoulos2020transformers}. This, however, brings a contradictory fact about linear recurrent (or linear Transformers) models: On one hand, we use these linear models to enhance scalability and efficiency (linear vs. quadratic complexity), whose advantages is appeared for very long context; On the other hand, a very long context cannot be properly compressed in a small vector-valued or matrix-valued states~\citep{wang2024longssm}. 


Furthermore, beyond efficiency, most existing architectures–ranging from Hopfield Networks~\citep{hopfield1982neural} to LSTMs~\citep{LSTM} and Transformers~\citep{transformers}–face challenges when dealing with generalization, length extrapolation, and/or reasoning~\citep{anil2022exploring, qin2024exploring}, all of which are inseparable parts of many hard real-world tasks. Although these architectures draw inspiration from the human brain, each of which are missing: (1) a crucial component for learning process—such as short-term memory, long-term memory, meta-memory, attending to current context, etc.~\citep{cowan2008differences}; (2) how these components are interconnected systems that can operate independently; and/or (3) the ability to \emph{actively} learn from data and memorize the abstraction of past history. We argue that in an effective learning paradigm, similar to human brain, there are \emph{distinct} yet interconnected modules, each of which is responsible for a component crucial to the learning process.




\subsection*{\textcolor{c3}{Memory Perspective}}
Memory is a fundamental mental process and is an inseparable component of human learning~\citep{terry2017learning}. Without a properly functioning memory system, humans and animals would be restricted to basic reflexes and stereotyped behaviors. Accordingly, memory has been the inspiration for many seminal research in machine learning literature; e.g., Hopfield Networks~\citep{hopfield1982neural}, LSTMs~\citep{LSTM}, and Transformers~\citep{transformers}. 

Taking inspiration from the common definitions of memory and learning in neuropsychology literature~\citep{okano2000learning}, most existing architectures consider memory as a neural update caused by an input, and define learning as a process for acquiring effective and useful memory, given an objective. In this perspective, Recurrent Neural Networks (RNNs)~\citep{williams1989learning} can be defined as models with a vector-valued memory module $\M$ (also called hidden state) with two main steps: Given a new input $x_t$ at time $t$, the model (1) updates the memory using a function $f(\M_{t-1}, x_t)$ (with compression); and (2) retrieves the corresponding memory of input using a function $g(\M_{t}, x_t)$ (\textcolor{c1}{see \S}\ref{sec:background} \textcolor{c1}{for details}). Similarly, Transformers can be seen as architectures with a growing memory and two similar steps. That is, the pair of key and value matrices acts as the model's memory, and the model: (1) updates the memory by appending the key and value to the memory (without compression), and (2) retrieves query vectors' corresponding memory by finding the similarity of query and key vectors, which is then used to weight the value vectors for the output.

This perspective, can help us better understand existing paradigms, their critical differences, and design more effective architectures. For example, the main difference between Transformers~\citep{transformers} and \emph{linear} Transformers~\citep{katharopoulos2020transformers} is the memory structure as well as the memory updating step, in which linear Transformers compress the historical data into a fixed-size matrix-valued memory while Transformers keep all historical data (within the context length) without any compression. While both linear Transformers and linear RNNs (including state space models) compress the information in memory update step, the critical difference lies in the structure of the memory, where linear RNNs (vs. linear Transformers) use a vector-valued memory (vs. matrix-valued memory). Therefore, this perspective motivates us to ask: \textcolor{c3}{\textbf{(Q1)}} What constitute a good structure for the memory?
\textcolor{c3}{\textbf{(Q2)}} What is a proper memory update mechanism? and 
\textcolor{c3}{\textbf{(Q3)}} What is a good memory retrieval process?


Revisiting our understanding of human memory, it is neither a unitary process nor it serves a single function~\citep{cowan2008differences}. In fact, memory is a confederation of systems–e.g., short-term, working, and long-term memory–each serving a different function with different neural structures, and each capable of operating independently~\citep{willingham1997systems}. This fact motivates us to ask: \textcolor{c3}{\textbf{(Q4)}} How to design an efficient architecture that incorporates different interconnected memory modules. Finally, storing a memory is a neural process that requires to encode and store the abstraction of the past. It can be over-simplification to assume a single vector or a matrix, whose parameters are encoding the data in a linear manner, are enough for storing long-term history. \textcolor{c3}{\textbf{(Q5)}} Is a deep memory module needed to effectively store/remember long past?













\subsection*{\textcolor{c3}{Contributions and Roadmap}}
In this paper, we aim to answer the above five questions by designing a long-term neural memory module, that can efficiently and effectively learn to memorize at test time. Building upon its design, we discuss how it can be incorporated into an architecture.

\textcolor{c3}{\textbf{Neural Memory} (\S\ref{sec:mem-module})}.
We present a (deep) neural long-term memory that (as a meta in-context model) learns how to memorize/store the data into its parameters at test time. Inspired by human long-term memory system~\citep{mandler2014structure}, we design this memory module so an event that violates the expectations (being surprising) is more memorable. To this end, we measure the surprise of an input with the gradient of the neural network with respect to the input in \emph{associative memory loss} (\textcolor{c1}{see \S}\ref{sec:long-memory} \textcolor{c1}{for details}).  To better handle the limited memory, we present a decaying mechanism that consider the proportion of memory size and the amount of data surprise, resulting in better memory management. We show that this decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models \citep{dao2024transformers, yang2024gated, gu2024mamba}. Interestingly, we find that this mechanism is equivalent to optimizing a meta neural network with mini-batch gradient descent, momentum, and weight decay. Building upon tensorizing mini-batch gradient descent to use more matmul operations~\citep{sun2024learning}, we present a fast and parallelizable algorithm to train our deep neural long-term memory.

\noindent
\textcolor{c3}{\textbf{Titans Architectures} (\S\ref{sec:arch}).}
After designing the long-term neural memory, an important remaining question is how to effectively and efficiently incorporate memory into a deep learning architecture. We present Titans, a family of deep models that consists of three hyper-heads: (1) Core: this module consists of the short-term memory, and is responsible for the main flow of processing the data (we use attention with limited window size); (2) Long-term Memory: this branch is our neural long-term memory module that is responsible to store/remember long past; (3) Persistent Memory: this is a set of learnable but date-independent parameters that encodes the knowledge about a task. Finally, as a proof of concept, we present three variants of Titans, in which we incorporate memory as: (i) a context, (ii) a layer, and (iii) a gated branch. 




\textcolor{c3}{\textbf{Experimental Results} (\S\ref{sec:experiments})}.  
We perform experimental evaluations on language modeling, commonsense reasoning, recall-intensive, needle in haystack, time series forecasting, and DNA modeling tasks. We observe that our Titan architecture outperforms all modern recurrent models as well as their hybrid variants (combining with sliding-window attention) across a comprehensive set of benchmarks. Furthermore, Titans outperforms Transformers with the same context window, and show competitive performance with Transformers that use the entire context. This results are achieved while, contrary to Transformers, Titans scale to larger than 2M context window size.  
