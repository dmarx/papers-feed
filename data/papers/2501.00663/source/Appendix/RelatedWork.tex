\newpage
\section{Related Work}\label{app:rw}
There are diverse perspectives that can independently lead to the design of Titans or its components. Accordingly, to further situate our work in a broader context, we review three categories of studies:

\subsection{Linear Recurrent Models}
Recently, to address the computational cost of Transformers in both training and inference, linear recurrent models have attracted much attention~\citep{tiezzi2024resurgence}, mainly due to their fast inference and training. The first generation of models–such as RetNet~\citep{sun2023retentive}, LRU~\citep{orvieto2023resurrecting}, RWKV~\citep{peng2023rwkv}, S5~\citep{smith2023simplified}, and S4~\citep{gu2022efficiently}–uses data-independent transition matrix/decay mechanism. The second generation of such models started to incorporate gating mechanism, a widely used techniques in traditional RNNs~\citep{gers2000learning, greff2016lstm, van2018unreasonable}, into such linear architectures–e.g., Griffin~\citep{de2024griffin}, SSMs~\citep{hasani2023liquid, behrouz2024mambamixer, dao2024transformers, gu2024mamba}, RWKV6~\citep{peng2024eagle}. The third generation of linear recurrent models are based on more complex memory updating rule based on meta-learning, online learning, and/or delta-rule, resulting in more expressive and effective models such as: Longhorn~\citep{liu2024longhorn}, Gated DeltaNet~\citep{yang2024gated}, TTT~\citep{sun2024learning}, and DeltaNet~\citep{yang2024parallelizing}. Our LMM model can be seen as the next generation of such models, in which we incorporate the token flow into the memory updating mechanism, having more powerful memory updating process. See \autoref{app:MAS} for a detailed discussion of different recurrent models and Titans.




\subsection{Transformer-based Architectures}

\head{Transformers}
Transformers~\citep{transformers} as the de facto backbone for many deep learning models are based on attention mechanism~\citep{bahdanau2014neural}. They, however, suffer from quadratic computational cost, limiting their ability to scale to long context window. To improve the memory consumption and throughput of softmax attention for longer sequences, various studies focused on I/O aware implementations of attention~\citep{flashattention-1, dao2024flashattention}, designing more efficient attention mechanisms by sparsifying the attention matrix~\citep{choromanski2021rethinking, dai2019transformerxl, chen2021scatterbrain, roy2021efficient, chen2021scatterbrain, dong2024flex}, approximating the softmax~\citep{arora2024simple}, or developing kernel-based (linear) attentions~\citep{kacham2024polysketchformer, schlag2021linear, yang2024gatedattn, aksenov2024linear}.

\head{Segment-based Transformers}
Another line of research to improve the efficiency of Transformers is segment-based or Chunk Transformers~\citep{dai2019transformerxl}. The main drawback of chunk Transformers is that segments are fully separated and so the context window is limited to the length of the chunks. To address this issue, various studies discuss the importance of a memory so it can help the model to transfer information across chunks~\citep{bulatov2022recurrent, rodkin2024associative, wu2020memformer, zancato2024bmojo, hutchins2022block, feng2022learn, hutchins2022block, bulatov2023scaling, wang2019r, wu2020memformer, zancato2024bmojo}. The key differences of Titans with these models are: (1) The memory in such models are simple small size vectors, lacking expressive power to compress complex information; (2) The memory module lacks forget mechanism, leading to a fast memory overflow; (3) only focus on momentary surprise, missing the information flow. More specifically, recalling Recurrent Memory Transformers (RMT)~\citep{bulatov2022recurrent, rodkin2024associative, bulatov2023scaling}, one can treat Titans (MAC) as the generalization of RMT, where we use a neural memory module instead of a vector-valued small size memory. 



\head{Memory for Large Language Models}
Another interesting research direction has been to incorporate external memory modules to LLMs after training~\citep{he2024camelot, Khandelwal2020Generalization, wang2024memoryllm}. Such models are different from our approach as we incorporate the memory as a part of initial architecture and so we train it in an end-to-end manner. Also, most of these explicit memory modules suffer from the same limitations as chunk-based Transformers (mentioned above). For a detailed discussion of such models, we refer to the recent study of \citet{wang2024towards}.



\subsection{Test Time Training and Fast Weight Programs}


\head{Memory Design and Augmentation with Memory} 
In the literature, a substantial research effort have been toward designing memory modules that are capable of either memorizing the knowledge abstraction (e.g., persistent memory)~\citep{sukhbaatar2019augmenting}, or memorizing the data-dependent information (also known as contextual memory), through recurrence~\citep{zancato2024bmojo, bulatov2022recurrent, rodkin2024associative}, Transformers~\citep{munkhdalai2024leave, zhang2024memory, cetin2024evolved, berges2024memory, le2020self, feng2022learn}, gradient~\citep{munkhdalai2019metalearned, irie2022dual}, or other learning paradigms~\citep{weston2014memory, sukhbaatar2015end}. These memory models, however, either (1) are based on momentary surprise, missing the data flow and events,   (2) lack forget mechanisms to remove the memory, leading to a fast memory overflow (3) are fixed-size shallow (matrix valued) memory, resulting in poor performance in long context, and (4) are based on fixed parameters at test time, lacking test time adaption. 

\head{Fast Weight Programs}
The idea of seeing linear layers as the key-value (associative) memory system backs to fast weight programs, in which dynamic fast programs are incorporated into recurrent neural networks to serve as writable memory~\citep{schlag2021linear, schmidhuber1992learning, schmidhuber1993reducing}. The two learning rules of Hebbian~\citep{hebb2005organization} and delta~\citep{prados1989neural} are the most popular learning rules for fast weight programs, which have been extensively explored in various studies~\citep{munkhdalai2017neural, schmidhuber1992learning, munkhdalai2019metalearned, schlag2021linear, irie2021going, yang2024parallelizing, yang2024gated}. All these models, however, are based on momentary surprise, missing the token flow in the sequences (see \autoref{sec:long-memory}), and most of them lacks a forgetting gate, resulting in a poor memory management. 



\head{Test Time Training}
The key ideas of learning at test time or learning to learn (i.e.,~\citep{andrychowicz2016learning}) backs to very early studies on local learning~\cite{bottou1992local}, in which each test data sample is trained on its neighbors before making a prediction~\citep{zhang2006svm, gandelsman2022test}. This approach further has shown promising performance in vision tasks~\citep{jain2011online, mullapudi2019online}, mostly due to their ability to mitigate out-of-distribution samples. The most similar studies to ours in this direction are MNM~\citep{munkhdalai2019metalearned} and TTT-layer~\citep{sun2024learning}, which we discussed the key differences in \autoref{app:MAS}. 