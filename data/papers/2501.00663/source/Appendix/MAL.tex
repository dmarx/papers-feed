\section{Long-term Memory Module (LMM) as a Sequence Model}\label{app:MAS}
In this section, we discuss how LMM as a sequence model is connected to modern linear recurrent models. For the sake of simplicity, we start with a linear memory, where $\M_t = W_t \in \R^{d_{\text{in}} \times d_{\text{in}}}$. In this case, our objective function becomes $\ell(\M; x_t) = \frac{1}{2}\left\Vert \M_{t}\mathbf{k}_t - \mathbf{v}_t  \right \Vert_2^2$, in which we use gradient descent with momentum and weight decay for the optimization. Accordingly, revisiting the recurrent formula in \autoref{eq:all}:   
\begin{align}\label{eq:linear-mal}
    &\M_t =  \texttt{diag}\left(1 - \alpha_t\right)\M_t + S_t \\
    &S_t = \texttt{diag}\left(\eta_t \right)S_{t-1} - \texttt{diag}\left(\theta_t \right) \left( \M_{t-1} \mathbf{k}_t^\top \mathbf{k}_t - \mathbf{v}_t^{\top} \mathbf{k}_t \right).
\end{align}

\head{LMM is Generalized Gated DeltaNet} As discussed by \citet{yang2024gated}, DeltaNet~\citep{yang2024parallelizing} can alternatively be interpreted as an online learning problem that optimizes the $\mathcal{L} = \frac{1}{2}\left\Vert \mathbf{S}_{t}\mathbf{k}_t - \mathbf{v}_t  \right \Vert_2^2$, resulting in:
\begin{align}\label{eq:deltanet}
    \mathbf{S}_{t+1} = \mathbf{S}_{t} - \theta_t \nabla \mathcal{L} = \mathbf{S}_{t} \left( \mathbf{I} - \theta_t \mathbf{k}_t\mathbf{k}_t^{\top} \right) + \theta_t \mathbf{v}_t\mathbf{k}^{\top}_t.
\end{align}
In this formulation, Gated DeltaNet is the same as above but with an additional weight decay term~\citep{yang2024gated}. Comparing \autoref{eq:linear-mal} and \autoref{eq:deltanet}, we can see that setting $\eta_t = 0$ results in both formulations to be equivalent. Accordingly, we can say LMM is generalizing the very recent study of Gated DeltaNet~\citep{yang2024gated} from three aspects: 
\begin{itemize}
    \item \underline{Momentum-based Rule}: The Delta Rule is based on momentary surprise, meaning that the flow of tokens cannot affect the memory update rule. LMM, however, is based on a momentum rule, which consider \emph{both} past and momentary surprise.
    \item \underline{Deep Memory}: While Gated DeltaNet is limited to a linear (matrix-valued) memory as it requires finding the closed recurrence form, LMM allows using deep memory module by using a gradient-based formulation, resulting in higher expressive power.
    \item \underline{Non-Linear Recurrence}: While DeltaNet and Gated DeltaNet are based on linear recurrence, our LMM is using inter-chunk non-linear recurrence and intra-chunk  linear recurrence. This design allows LMM having a higher expressive power.
\end{itemize}

Here, we discussed Gated DeltaNet as a sample of recent generation of recurrent models. Similar approaches such as RWKV-7~\citep{rwkv-repo} are also using the same formulation and loss function, and so LMM is generalizing all such models.  


\head{LMM is Generalized Longhorn}
Similar to DeltaNet, Longhorn~\citep{liu2024longhorn} uses the same loss function but it derives the closed form using implicit online learning: 
\begin{align}
    \mathbf{S}_{t+1} = \mathbf{S}_{t} \left( \mathbf{I} - \delta_t \mathbf{k}_t\mathbf{k}_t^{\top} \right) + \delta_t \mathbf{v}_t\mathbf{k}^{\top}_t,
\end{align}
where $\delta_t = \frac{\theta_t}{1 + \theta_t \mathbf{k}_t\mathbf{k}_t^{\top}}$.
It, however, lacks a forgetting gate, resulting in a faster memory overflow. Therefore, in addition two the abovementioned aspects of (1)~\underline{Momentum-based Rule}, (2)~\underline{Deep Memory}, and (3)~\underline{Non-Linear Recurrence}, LMM has the advantage of using an additional (4) \underline{Forget Gate}, leading to a better memory management. 

\head{LMM is Generalized TTT Layer}
To the best of our knowledge, TTT~\citep{sun2024learning}, is the only modern linear recurrent models with a gradient-based updating rule. In addition to different architectural designs and also objective functions, our LMM has three key differences with presented TTT layers~\citep{sun2024learning}:
\begin{enumerate}
    \item \underline{Forgetting Mechanism}: TTT layers are updating memory at each time, without having the chance to forget the past data. Accordingly, when fixing the memory size, the model cannot manage the memory for long sequences. A forget mechanism, such as LMM's, allows clearing the memory when very past information is not needed anymore. We show that in a general case, this forget mechanism is equivalent to weight decay and provide a fast method to incorporate it into the parallel training.
    \item \underline{Momentum-based Update Rule}: TTT layers are based on momentary surprise, meaning that the flow of tokens cannot affect the memory update rule. LMM, however, is based on a momentum rule, which consider \emph{both} past and momentary surprise. See \autoref{sec:long-memory} for the motivation of this design.
    \item \underline{Deep Memory}: While TTT-layers allows for deeper memory, the advantages/disadvantages of such deeper memory modules have not been experimentally evaluated.  
\end{enumerate}

To the best of our knowledge, our neural long-term memory module is the first linear recurrent model with momentum-based update rule. 


Finally, as a key difference with all the above and other recent linear recurrent studies, note that the hybrid variants of modern linear models–such as Griffin~\citep{de2024griffin}, DeltaNet~\citep{yang2024parallelizing}, Gated DeltaNet~\citep{yang2024gated}, H3~\citep{fu2023hungry}, Mamba2~\citep{dao2024transformers}, Samba~\citep{ren2024samba}, etc.–all are based on sequential layer-wise design. We present Titans to show how effectively one can incorporate such memory modules into an architecture.