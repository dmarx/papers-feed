\section{Language Modeling and Common-sense Reasoning Datasets}\label{app:exp-details}
Following recent studies on linear recurrent models~\citep{yang2024gated, dao2024transformers, yang2024parallelizing}, we use Wikitext~\citep{merity2017pointer}, LMB~\citep{paperno-etal-2016-lambada}, PIQA~\citep{bisk2020piqa}, HellaSwag~\citep{zellers-etal-2019-hellaswag}, WinoGrande~\citep{sakaguchi2021winogrande},  ARC-easy (ARC-e) and ARC-challenge (ARC-c)~\citep{clark2018think}, SIQA~\citep{sap-etal-2019-social}, and BoolQ~\citep{clark-etal-2019-boolq}. Also, the baselines results for 400M models are from the reported results by \citet{yang2024gated}. 



