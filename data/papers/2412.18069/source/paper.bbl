\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asai et~al.(2024)Asai, Wu, Wang, Sil, and Hajishirzi]{asai2024selfrag}
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.
\newblock Self-{RAG}: Learning to retrieve, generate, and critique through self-reflection.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock \url{https://openreview.net/forum?id=hSyW5go0v8}.

\bibitem[Dhuliawala et~al.(2024)Dhuliawala, Komeili, Xu, Raileanu, Li, Celikyilmaz, and Weston]{dhuliawala-etal-2024-chain}
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston.
\newblock Chain-of-verification reduces hallucination in large language models.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Findings of the Association for Computational Linguistics ACL 2024}, pages 3563--3578, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-acl.212}.
\newblock \url{https://aclanthology.org/2024.findings-acl.212}.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, Goyal, Hartshorn, Yang, Mitra, and et. al.]{dubey2024llama3herdmodels}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, and Archie~Sravankumar et. al.
\newblock The llama 3 herd of models, 2024.
\newblock \url{https://arxiv.org/abs/2407.21783}.

\bibitem[Dubois et~al.(2023)Dubois, Li, Taori, Zhang, Gulrajani, Ba, Guestrin, Liang, and Hashimoto]{dubois2023alpacafarm}
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto.
\newblock {AlpacaFarm}: A simulation framework for methods that learn from human feedback.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock \url{https://openreview.net/forum?id=4hturzLcKX}.

\bibitem[Dubois et~al.(2024)Dubois, Liang, and Hashimoto]{dubois2024lengthcontrolled}
Yann Dubois, Percy Liang, and Tatsunori Hashimoto.
\newblock Length-controlled alpacaeval: A simple debiasing of automatic evaluators.
\newblock In \emph{First Conference on Language Modeling}, 2024.
\newblock \url{https://openreview.net/forum?id=CybBmzWBX0}.

\bibitem[Gao et~al.(2024)Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun, Wang, and Wang]{gao2024retrievalaugmentedgenerationlargelanguage}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai, Jiawei Sun, Meng Wang, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey, 2024.
\newblock \url{https://arxiv.org/abs/2312.10997}.

\bibitem[Gardent et~al.(2017)Gardent, Shimorina, Narayan, and Perez-Beltrachini]{webnlg}
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini.
\newblock The {W}eb{NLG} challenge: Generating text from {RDF} data.
\newblock In \emph{Proc. INLG}, pages 124--133, 2017.

\bibitem[Izacard et~al.(2022)Izacard, Caron, Hosseini, Riedel, Bojanowski, Joulin, and Grave]{izacard2022unsupervised}
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave.
\newblock Unsupervised dense information retrieval with contrastive learning.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.
\newblock \url{https://openreview.net/forum?id=jKN1pXi7b0}.

\bibitem[Jiang et~al.(2023)Jiang, Xu, Gao, Sun, Liu, Dwivedi-Yu, Yang, Callan, and Neubig]{jiang-etal-2023-active}
Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Active retrieval augmented generation.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 7969--7992, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.495}.
\newblock \url{https://aclanthology.org/2023.emnlp-main.495}.

\bibitem[K\"{o}pf et~al.(2023)K\"{o}pf, Kilcher, von R\"{u}tte, Anagnostidis, Tam, Stevens, Barhoum, Nguyen, Stanley, Nagyfi, ES, Suri, Glushkov, Dantuluri, Maguire, Schuhmann, Nguyen, and Mattick]{Kopf:OpenAssistant23}
Andreas K\"{o}pf, Yannic Kilcher, Dimitri von R\"{u}tte, Sotiris Anagnostidis, Zhi~Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich\'{a}rd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick.
\newblock Openassistant conversations - democratizing large language model alignment.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 47669--47681. Curran Associates, Inc., 2023.
\newblock \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/949f0f8f32267d297c2d4e3ee10a2e7e-Paper-Datasets_and_Benchmarks.pdf}.

\bibitem[Lee et~al.(2024)Lee, Chen, Dai, Dua, Sachan, Boratko, Luan, Arnold, Perot, Dalmia, Hu, Lin, Pasupat, Amini, Cole, Riedel, Naim, Chang, and Guu]{lee2024longcontextlanguagemodelssubsume}
Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra~Singh Sachan, Michael Boratko, Yi~Luan, SÃ©bastien M.~R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy~R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu.
\newblock Can long-context language models subsume retrieval, rag, sql, and more?, 2024.
\newblock \url{https://arxiv.org/abs/2406.13121}.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"{u}}ttler, Lewis, Yih, Rockt{\"{a}}schel, Riedel, and Kiela]{lewis-etal-2021-rag}
Patrick S.~H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"{u}}ttler, Mike Lewis, Wen{-}tau Yih, Tim Rockt{\"{a}}schel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock \url{https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html}.

\bibitem[Li et~al.(2024)Li, Chen, Holtzman, Chen, Lin, Yih, and Lin]{li2024nearest}
Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, and Xi~Victoria Lin.
\newblock Nearest neighbor speculative decoding for llm generation and attribution.
\newblock \emph{arXiv preprint arXiv:2405.19325}, 2024.

\bibitem[Lin et~al.(2024)Lin, Gao, Oguz, Xiong, Lin, Yih, and Chen]{lin2024flame}
Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, and Xilun Chen.
\newblock Flame: Factuality-aware alignment for large language models.
\newblock \emph{arXiv preprint arXiv:2405.01525}, 2024.

\bibitem[Lu et~al.(2024)Lu, Wang, Rong, Chen, and Tang]{lu2024turborag}
Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua Tang.
\newblock {TurboRAG}: Accelerating retrieval-augmented generation with precomputed kv caches for chunked text.
\newblock \emph{arXiv preprint arXiv:2410.07590}, 2024.
\newblock \url{https://arxiv.org/abs/2410.07590}.

\bibitem[Min et~al.(2023)Min, Krishna, Lyu, Lewis, Yih, Koh, Iyyer, Zettlemoyer, and Hajishirzi]{min-etal-2023-factscore}
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock {FA}ct{S}core: Fine-grained atomic evaluation of factual precision in long form text generation.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 12076--12100, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.741}.
\newblock \url{https://aclanthology.org/2023.emnlp-main.741}.

\bibitem[Mishra et~al.(2024)Mishra, Asai, Balachandran, Wang, Neubig, Tsvetkov, and Hajishirzi]{mishra2024finegrained}
Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi.
\newblock Fine-grained hallucination detection and editing for language models.
\newblock In \emph{First Conference on Language Modeling}, 2024.
\newblock \url{https://openreview.net/forum?id=dJMTn3QOWO}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{JMLR:v21:20-074}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (140):\penalty0 1--67, 2020.
\newblock \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rajani et~al.(2023)Rajani, Tunstall, Beeching, Lambert, Rush, and Wolf]{no_robots}
Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander~M. Rush, and Thomas Wolf.
\newblock No robots.
\newblock \emph{Hugging Face repository}, 2023.

\bibitem[Shao et~al.(2024)Shao, He, Asai, Shi, Dettmers, Min, Zettlemoyer, and Koh]{shao2024scaling}
Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang~Wei Koh.
\newblock Scaling retrieval-based language models with a trillion-token datastore.
\newblock \emph{arXiv preprint arXiv:2407.12854}, 2024.

\bibitem[Shao et~al.(2023)Shao, Gong, Shen, Huang, Duan, and Chen]{shao-etal-2023-enhancing}
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.
\newblock Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 9248--9274, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-emnlp.620}.
\newblock \url{https://aclanthology.org/2023.findings-emnlp.620}.

\bibitem[Shi et~al.(2024)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih]{shi-etal-2024-replug}
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih.
\newblock {REPLUG}: Retrieval-augmented black-box language models.
\newblock In Kevin Duh, Helena Gomez, and Steven Bethard, editors, \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 8371--8384, Mexico City, Mexico, June 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.naacl-long.463}.
\newblock \url{https://aclanthology.org/2024.naacl-long.463}.

\bibitem[Song et~al.(2024)Song, Kim, and Iyyer]{song-etal-2024-veriscore}
Yixiao Song, Yekyung Kim, and Mohit Iyyer.
\newblock {V}eri{S}core: Evaluating the factuality of verifiable claims in long-form text generation.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 9447--9474, Miami, Florida, USA, November 2024. Association for Computational Linguistics.
\newblock \url{https://aclanthology.org/2024.findings-emnlp.552}.

\bibitem[Su et~al.(2024)Su, Tang, Ai, Wu, and Liu]{su-etal-2024-dragin}
Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu.
\newblock {DRAGIN}: Dynamic retrieval augmented generation based on the real-time information needs of large language models.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 12991--13013, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.acl-long.702}.
\newblock \url{https://aclanthology.org/2024.acl-long.702}.

\bibitem[Wang et~al.(2023)Wang, Dong, Cheng, Liu, Yan, Gao, and Wei]{Wang-augmenting-2023}
Weizhi Wang, Li~Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei.
\newblock Augmenting language models with long-term memory.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 74530--74543. Curran Associates, Inc., 2023.
\newblock \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/ebd82705f44793b6f9ade5a669d0f0bf-Paper-Conference.pdf}.

\bibitem[Wei et~al.(2024)Wei, Yang, Song, Lu, Hu, Huang, Tran, Peng, Liu, Huang, Du, and Le]{wei2024longform}
Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan~Zixia Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da~Huang, Cosmo Du, and Quoc~V Le.
\newblock Long-form factuality in large language models.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.
\newblock \url{https://openreview.net/forum?id=4M9f8VMt2C}.

\bibitem[Wu et~al.(2022)Wu, Rabe, Hutchins, and Szegedy]{wu2022memorizing}
Yuhuai Wu, Markus~Norman Rabe, DeLesley Hutchins, and Christian Szegedy.
\newblock Memorizing transformers.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock \url{https://openreview.net/forum?id=TrjbxzRcnf-}.

\bibitem[Yang et~al.(2024)Yang, Lin, Wang, Wu, Li, Tang, Wei, Wang, Tang, Song, et~al.]{yang2024text}
Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo~Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, et~al.
\newblock Memory$^3$: Language modeling with explicit memory.
\newblock \emph{arXiv preprint arXiv:2407.01178}, 2024.
\newblock \url{https://arxiv.org/abs/2407.01178}.

\end{thebibliography}
