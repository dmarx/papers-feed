\begin{table}[t]
    \centering
    \begin{tabular}{lcccccccc} \toprule
        \textbf{Model} & \multicolumn{2}{c}{\textbf{\lf}} & \multicolumn{2}{c}{\textbf{\fava}} & \multicolumn{2}{c}{\textbf{\alpaca}} & \multicolumn{2}{c}{\textbf{\bio}} \\
        & F$_1$ & WR & F$_1$ & WR & F$_1$ & WR & F$_1$ & WR \\\midrule
        Llama-3.1$_{\text{70B}}$ & 64.3 & - & 52.0 & - & 63.8 & - & 37.1 & - \\
        +RA & 64.6 & 41.0 & 56.7 & 37.3 & 64.9 & 43.3 & 41.7 & 49.8 \\
        +\nest  & 62.1 & 8.5 & 49.0 & 23.3 & 57.6 & 30.4 & 39.5 & 21.7 \\
        +\dragin & 64.8 & 37.6 & 57.2 & 33.8 & 63.8 & 31.2 & 39.8 & 33.3 \\
        +\cove & 63.8 & 39.3 & 49.5 & 33.4 & 61.5 & 33.3 & 37.7 & 31.3 \\
        +\cove w/ Retrieval & 64.6 & 31.5 & 52.8 & 22.9 & 63.9 & 28.5 & 38.9 & 29.6 \\
        +\model & \bf 70.7 & \bf 50.2  & \bf 61.1 & \bf 50.2 & \bf 65.8 & \bf 49.7 & \bf 47.6 & \bf 50.6 \\ \midrule
        Llama-3.1$_{\text{8B}}$ & 63.1 & \bf 40.6 & 51.0 & \bf 36.5 &\bf  65.3 & 26.7 & 28.9 & \bf 24.2 \\
        +RA & 66.5 & 27.4 & 51.7 & 16.4 & 64.3 & 18.8 & 37.1 & 20.6 \\
        +\nest & 61.9 & 3.4 & 50.1 & 13.7 & 57.4 & 8.4 & \bf 39.0 & 21.4 \\
        +\dragin & 63.9 & 15.9 & 51.1 & 10.0 & 61.4 & 11.0 & 34.3 & 11.5 \\
        +\cove & 44.1 & 8.8 & 38.7 & 11.0 & 51.3 & 15.1 & 25.1 & 13.3 \\
        +\cove w/ Retrieval & 53.2 & 12.4 & 39.7 & 5.2 & 54.5 & 12.8 & 25.7 & 9.3 \\
        +\model & \bf 67.2 & \bf 40.7 & \bf 53.2 & 36.1 & \bf 65.4 & \bf 28.2 & \bf 39.1 & 21.0 \\ \bottomrule
    \hline
    \end{tabular}
    \caption{Evaluation on factuality and helpfulness of the model responses to prompts provided in four long-form question answering datasets. For each dataset, we report F$_1$ scores from \vs and win rates (WR) from \alpacaE. We use Llama-3.1$_{\text{70B}}$ as the baseline method in \alpacaE win rate experiments.}
    \label{tab:main_result}
\end{table}
