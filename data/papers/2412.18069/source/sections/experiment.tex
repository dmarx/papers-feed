\section{Experiments}
\label{sec:exp}

We present the main experimental results of \model in this section, along with details of the datasets and evaluation metrics we used, and the baseline we compared with. In this set of experiments, we set the retrieval and verification timesteps, $T_r$ and $T_v$, to be 1 and 8, respectively.

\subsection{Evaluation Datasets}
We evaluated \model and baseline models using four fact-seeking long-form generation datasets: \lf~\citep{wei2024longform}, \fava~\citep{mishra2024finegrained}, \alpaca~\citep{dubois2023alpacafarm,lin2024flame} and \bio~\citep{min-etal-2023-factscore}.

\paragraph{\lf} Designed to probe the factuality of a model of which response consists of at least several paragraphs, \lf was created by prompting GPT-4 to generate questions regarding a specific concept or object within a given topic.
In our experiments, we use the 250 prompts from the \lf-Objects dataset, selected by \citet{wei2024longform}.  
\paragraph{\fava} As a new fine-grained hallucination benchmark, \fava constructed 200 information-seeking queries that require factual knowledge to give accurate long-form answers from multiple sources, including Open Assistant~\citep{Kopf:OpenAssistant23}, No Robots~\citep{no_robots}, WebNLG~\citep{webnlg} and instructions written by the authors~\citep{mishra2024finegrained}. Following \citet{lin2024flame}, we selected 141 prompts from this collection for our experiments.
\paragraph{\alpaca} Originally collected from real-world interactions with various users, the 805 instructions in AlpacaFarm~\citep{dubois2023alpacafarm} was used for evaluating the instruction-following ability of different LLMs.
To focus our evaluation on factuality, we used a subset of 241 fact-seeking instructions selected by \citet{lin2024flame} in this work.
\paragraph{\bio} To demonstrate the effectiveness of the factuality metric \fs, \citet{min-etal-2023-factscore} selected 183 names of famous people found in Wikipedia, and applied the ``\textit{Tell me a bio of} [\texttt{Person Name}]'' template to create a collection of prompts called \bio. As this set of prompts have been used extensively in several recent papers, we include it in our study as well.

When using these prompts, we appended the instruction ``\textit{Provide as many specific details and examples as possible (such as names of people, numbers, events, locations, dates, times, etc.)}'' to encourage models to generate more detailed responses that cover multiple factoids, following \citet{wei2024longform}.


\subsection{Evaluation Metrics} 

We assess the quality of model responses to fact-seeking questions based on two key axes: \textit{factuality} and \textit{helpfulness}.
For evaluating factuality, we considered multiple automatic metrics, such as \fs~\citep{min-etal-2023-factscore} and \safe~\citep{wei2024longform}, but ultimately chose \vs~\citep{song-etal-2024-veriscore} as our primary evaluation metric. 
Although these metrics share a similar design that decomposes sentences into ``atomic claims'' and checks their support against an external knowledge source, \vs focuses on extracting more sensible verifiable claims and uses Google search snippets instead of Wikipedia as the knowledge source.
As a result, \vs can be applied to responses on more diverse topics and is also more efficient, requiring fewer but more meaningful claims to be checked. 
We report the F$_1$ score from \vs, which is the harmonic mean of the precision and recall of the claims. 
Following \citet{song-etal-2024-veriscore}, we set the minimum number of facts required for a model's response to achieve perfect recall as the median number of extracted claims per dataset\footnote{The median numbers of extracted facts for \lf, \fava, \alpaca, \bio are 55, 49, 31, 43, respectively.}. We also used their fine-tuned models for claim extraction and verification, provided in their package\footnote{\url{https://github.com/Yixiao-Song/VeriScore}}. 

To make sure that a model with a high factuality score does not simply give irrelevant but correct factual statements, we also need to check whether the response is helpful to the user.
Following \cite{lin2024flame}, we use \alpacaE~\citep{dubois2024lengthcontrolled} to compare the target model and baseline model in terms of their instruction-following ability. 
For the responses to the same input prompt, a large language model is used as judge to determine which of the two is better\footnote{We used GPT-4o as the judge.}, and the win rate is thus used as a measure of helpfulness\footnote{We found that the length-controlled win rates in \alpacaE could conflate hallucinations and length effects, and thus report the version without length normalization.}.



\subsection{Baselines}

We used instruction-tuned Llama-3.1 70B and 8B as the base models and compared \model with five baselines: base model only, retrieval augmentation (RA), Chain of verification (\cove)\footnote{We adapted an implementation from \url{https://github.com/ritun16/chain-of-verification}}, an iterative retrieval approach \dragin~\citep{su-etal-2024-dragin}\footnote{We used the authors' implementation \url{https://github.com/oneal2000/DRAGIN}}, and a recently proposed semi-parametric decoding method \nest~\citep{li2024nearest}.
For base model only, Llama-3.1$_{\text{70B}}$ or Llama-3.1$_{\text{8B}}$, we simply gave the language model the prompt in the dataset and the instruction of requesting detailed information, without other additional information. 
With retrieval augmentation, we retrieved 20 passages using the input prompts as queries and then prepended the  passages to the input\footnote{Using more than 20 passages does not provide significant benefits in our preliminary experiments, so we limit our retrieval to the top 20 passages.}.
\nest is a strong retrieval-based decoding algorithm. Following the original setup, we retrieved 100 passages to use as candidates.
For \cove, we employ the ``factor+revise'' method, which \citet{dhuliawala-etal-2024-chain} demonstrated to be the most effective. Additionally, we improve \cove by integrating retrieved passages from our retrieval datastore during the verification step. This augmentation helps us establish a stronger and more comparable baseline method, considering that most other baseline methods also utilize retrieval.
For all our experiments, the maximum generation step was set to 1024.
Llama-3.1$_{\text{70B}}$ is used as the baseline method for all \alpacaE comparisons. 


\subsection{Results}

\input{tables/main}






Our main results are shown in Table~\ref{tab:main_result}. For the Llama-3.1$_{\text{70B}}$ base model, we find that in terms of factuality, retrieval augmentation generally improves the results consistently across different datasets. 
This is expected as for fact-seeking prompts, specifically conditioning generation on relevant factual knowledge has been demonstrated to be an effective way to mitigate hallucinations.
\nest performs better than the base model on the \bio dataset, but not on others, and it appears that the \vs F$_1$ is lower than the standard retrieval augmentation.
It might suggest that the configuration or hyperparameter settings of \nest need to be further optimized, as \nest was originally evaluated by \bio with Llama-2. \dragin performs similarly to RA, likely because their query formulation method is not optimized for long-form generation, resulting in less useful retrieved passages. Similarly, with \cove, we notice that it often produces shorter model responses, leading to significantly lower recall performance despite high precision, which results in a less favorable \vs F$_1$. While augmenting \cove with retrieval slightly alleviates this issue, it still lags behind.
Perhaps more interestingly, with online fact-checking feedback and refreshed knowledge from retrieval, \model achieves the highest \vs F$_1$ on all datasets. On the helpfulness of the responses, it appears that \alpacaE generally prefers the output from the base model, except for \model, where the win rates are roughly 50\%.

When using Llama-3.1$_{\text{8B}}$ as the base model, we have observed a similar trend. Retrieval augmentation improves factuality in terms of \vs F$_1$ and \model still gives the best factuality results. However, compared to the models based on Llama-3.1$_{\text{70B}}$, we notice that the improvement is generally smaller.
We hypothesize that the smaller base language model is less capable in leveraging feedback, and may not always regenerate a sentence that is factually correct.
In terms of helpfulness, we can see that \model generally performs comparably to its base model Llama-3.1$_{\text{8B}}$, as they have similar win rates when judged against the output of the same Llama-3.1$_{\text{70B}}$ base model.  
