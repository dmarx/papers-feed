\section{Conclusion}
\label{sec:conclusion}
We present \model, a novel system that incorporates a working memory mechanism during the generation process. 
\model pauses at given intervals and refreshes its working memory based on feedback from retrieval and fact-checking models, ensuring that the generated content remains accurate and relevant.
By integrating this working memory into each attention layer of the Transformer architectures, \model can be easily adapted to various large language models.
Our experiments demonstrate the effectiveness of \model by benchmarking it on 8B and 70B Llama-3.1 models, resulting in significant improvements in both factuality and helpfulness across four fact-seeking long-form generation datasets. 
Furthermore, our analysis reveals that updating the working memory with more relevant information at each timestep, allowing attention to focus on each passage, and utilizing high-quality retrieval datastores with extensive knowledge coverage are crucial factors for improving factuality of models.