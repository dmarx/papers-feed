---
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
---





# Score-based variational inference with orthogonal function expansions

In this section we use orthogonal function expansions to develop new variational families for approximate probabilistic inference. In , we review the basic properties of these expansions. In , we introduce a score-based divergence for VI with these families; notably, for this divergence, the optimization for VI reduces to an eigenvalue problem. Finally in , we consider how to use these variational approximations for unstandardized distributions; in these settings we must carefully manage the trade-off between expressiveness and computational cost.

## Orthogonal function expansions

Let $\mathcal{Z}\subseteq\mathbb{R}^D$ denote the support of the target distribution $p$. Suppose that there exists a complete set of orthonormal basis functions $\{\phi_k(z)\}_{k=1}^\infty$ on this set. By *complete*, we mean that any sufficiently well-behaved function can be approximated, to arbitrary accuracy, by a particular weighted sum of these basis functions, and by *orthonormal*, we mean that the basis functions satisfy $$\begin{aligned}
\label{eq-orthonormal}
  \int\!\phi_k(z)\phi_{k'}(z)\, dz =
  \left\{
  \begin{array}{rr} 1 & \mbox{if $k=k'$}, \\
    0 & \mbox{otherwise,}\end{array}\right.
\end{aligned}$$ where the integral is over $\mathcal{Z}$. Define the $K^\text{th}$-order variational family $\mathcal{Q}_K$ to be the set containing all distributions of the form $$\begin{aligned}
  q(z) =  \left(\sum_{k=1}^{K} \alpha_k \phi_k(z)\right)^2\quad\mbox{where}\quad\sum_{k=1}^{K} \alpha_k^2=1,
\label{eq:OF-1}
\end{aligned}$$ and where $\alpha_k\in\R$ for $k=1,\ldots, K$ are the parameters of the family $\mathcal{Q}_K.$ In words, $\mathcal{Q}_K$ contains all distributions that can be obtained by taking weighted sums of the first $K$ basis functions and then *squaring* the result.

involves a squaring operation, a sum-of-squares constraint, and a weighted sum. The squaring operation ensures that the density functions in $\mathcal{Q}_K$ are nonnegative (i.e., with $q(z)\!\geq\!0$ for all $z\in\mathcal{Z}$), while the sum-of-squares constraint ensures that they are normalized: $$\begin{aligned}
  \int\! q(z)\, dz\, =\, \int\! \left(\sum_{k=1}^{K} \alpha_k \phi_k(z)\right)^2\!\! dz\, =\,
  \int\! \sum_{k,k'=1}^{K} \alpha_k \alpha_{k'} \phi_k(z)\phi_{k'}(z)\, dz\, =\, \sum_{k=1}^{K} \alpha_k^2 = 1.
\end{aligned}$$

The weighted sum in bears a superficial similarity to a mixture model, but note that neither the basis functions $\phi_k(z)$ nor the weights $\alpha_k$ in are constrained to be nonnegative. Distributions of this form arise naturally in physics from the quantum-mechanical *wave functions* that satisfy Schrödinger’s equation . In that setting, though, it is typical to consider complex-valued weights and basis functions, whereas here we only consider real-valued ones.

The simplest examples of orthogonal function expansions arise in one dimension. For example, functions on the interval $[-1,1]$ can be represented as weighted sums of Legendre polynomials, while functions on the unit circle can be represented by Fourier series of sines and cosines; see . Distributions on unbounded intervals can also be represented in this way. On the real line, for example, we may consider approximations of the form in where $$\phi_{k+1}(z) = \left(\sqrt{2\pi}k!\right)^{-\frac{1}{2}}\left(e^{-\frac{1}{2}z^2}\right)^{\frac{1}{2}}\,\text{H}_{k}(z),
\label{eq:phi-hermite}$$ and $\text{H}_k(z)$ are the *probabilist’s* Hermite polynomials given by $$\text{H}_k(z) = (-1)^k e^{\frac{z^2}{2}} \frac{d^k}{dz^k}\left[e^{-\frac{z^2}{2}}\right].
\label{eq:hermite}$$ Note how the lowest-order basis function $\phi_1(z)$ in this family gives rise (upon squaring) to a Gaussian distribution with zero mean and unit variance.

shows how various multimodal distributions with one-dimensional support can be approximated by computing weighted sums of basis functions and squaring their result. We emphasize that *the more basis functions in the sum, the better the approximation*.

<figure id="fig:onedim">
<figure>
<span class="image placeholder" data-original-image-src="figs/1d_legendre2.pdf" data-original-image-title=""></span>
<figcaption>Legendre polynomial expansion</figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figs/1d_fourier2.pdf" data-original-image-title=""></span>
<figcaption>Fourier series expansion</figcaption>
</figure>
<figure id="fig:onedim:c">
<span class="image placeholder" data-original-image-src="figs/1d_hermite2.pdf" data-original-image-title=""></span>
<figcaption>Hermite polynomial expansion</figcaption>
</figure>
<figcaption>Target probability distributions (black dashed curves) on the interval <span class="math inline">\([-1,1]\)</span> (left), the unit circle (middle), and the real line (right), and their approximations by orthogonal function expansions from different families and of different orders; see  and .</figcaption>
</figure>

Orthogonal function expansions in one dimension are also important because their Cartesian products can be used to generate orthogonal function expansions in higher dimensions. For example, we can approximate distributions over (say) $\mathbb{R}^3$ by $$q(z_1,z_2,z_3) =  \left(\sum_{i=1}^{K_1}\sum_{j=1}^{K_2}\sum_{k=1}^{K_3} \beta_{ijk}\, \phi_i(z_1)\phi_j(z_2)\phi_k(z_3)\right)^2\quad\mbox{where}\quad \sum_{ijk}\beta^2_{ijk} = 1,
\label{eq:3d}$$ where $\beta_{ijk}\in \R$ now parametrize the family. Note that there are a total $K_1 K_2 K_3$ parameters in the above expansion, so that this method of Cartesian products does not scale well to high dimensions if multiple basis functions are used per dimension. Note that the same strategy can also be used for random variables of mixed type: for example, from , we can create a variational family of distributions over $\mathbb{R}\!\times\![-1,1]\!\times\![0,\infty)$ from the Cartesian product of orthogonal function expansions involving Hermite, Legendre, and Laguerre polynomials.

As shown in , the approximating distributions from $K^\text{th}$-order expansions can model the presence of multiple modes as well as many types of asymmetry, and this expressiveness also extends to higher dimensions. Nevertheless, it remains tractable to sample from these distributions and even to calculate (analytically) their low-order moments, as we show in .

For concreteness, consider the distribution over $\mathbb{R}^3$ in . The marginal distribution $q(z_1)$ is $$\begin{aligned}
q(z_1) = \int\! q(z_1,z_2,z_3)\,dz_2\, dz_3 = \sum_{ii'} \left[\sum_{jk} \beta_{ijk}\beta_{i'jk}\right] \phi_i(z_1)\phi_{i'}(z_1),
\label{eq:marginal}
\end{aligned}$$ and from this expression, moments such as $\mathbb{E}[z_1]$ and $\text{Var}[z_1]$ can be calculated by evaluating integrals involving the elementary functions in . (In practice, these integrals are further simplified by recursion relations that relate basis functions of different orders; we demonstrate how to compute the first two moments for the normalized Hermite family in .)

To generate samples $\{z^{(t)}\}$, each dimension is sampled as follows: we draw $z_1^{(t)} \sim q(z_1)$ by computing the cumulative distribution function (CDF) of this marginal distribution and then numerically inverting this CDF. Finally, extending these ideas, we can calculate higher-order moments and obtain joint samples via the nested draws $$\begin{aligned}
\label{eq:sample}
    z_1^{(t)} \sim q(z_1),\quad z_2^{(t)} \sim q(z_2 \given z_1),\quad
    z_3^{(t)} \sim q(z_3  \given z_1,z_2).
\end{aligned}$$ The overall complexity of these procedures scales no worse than quadratically in the number of basis functions in the expansion. These extensions are discussed further in .

## EigenVI

In variational inference, we posit a parameterized family of approximating distributions and then compute the particular approximation in this family that is closest to a target distribution of interest. constructs a variational family $\Q_K$ from the orthogonal functions $\{\phi_k(z)\}_{k=1}^K$ whose variational parameters are the weights $\{\alpha_k\}_{k=1}^{K}$. We now derive *EigenVI*, a method to find $q \in \Q_K$ that is close to the target distribution $p(z)$.

We first define the measure of closeness that we will minimize. EigenVI measures the quality of an approximate density by the *Fisher divergence* , $$\begin{aligned}
\label{eq-divergence}
    \D(q,p) = \int \norm{\nabla \log q(z) - \nabla\log p(z)}^2 q(z) dz,
\end{aligned}$$ where $\nabla \log q(z)$ and $\nabla \log p(z)$ are the score functions of the variational approximation and target, respectively. Suppose that $q$ and $p$ have the same support; then the Fisher divergence vanishes if and only if the scores of $q$ and $p$ are everywhere equal.

Though $p$ is, by assumption, intractable to compute, in many applications it is possible to efficiently compute the score $\nabla \log p$ at any point $z\in\mathcal{Z}$. For example, in Bayesian models the score of the target posterior is equal to the gradient of the log joint. This observation is the main motivation for score-based methods in probabilistic modeling .

Here we seek the $q\!\in\!\Q_K$ that minimizes $\D(q,p)$. But now a challenge arises: it is generally difficult to evaluate the integral for $\D(q,p)$ in , let alone to minimize it as a function of $q$. While it is possible to sample from the distribution $q$, it is not straightforward to simultaneously sample from $q$ and optimize over the variational parameters $\{\alpha_k\}_{k=1}^K$ in terms of which it is defined. Instead, we construct an unbiased estimator of $\D(q,p)$ by importance sampling, which also decouples the sampling distribution from the optimization. Let $\{z^1,z^2,\ldots z^B\}$ denote a batch of $B$ samples drawn from some proposal distribution $\pi$ on $\mathcal{Z}$. From these samples we can form the unbiased estimator $$\begin{aligned}
  \label{eq-empirical-divergence}
  \widehat\D_{\pi}(q, p) =
  \sum_{b=1}^B \frac{q(z^b)}{\pi(z^b)} \, \big\|\nabla \log q(z^b) - \nabla\log p(z^b)\big\|^2.
\end{aligned}$$ This estimator should be accurate for appropriately broad proposal distributions and for sufficiently large batch sizes. We can therefore attempt to minimize in place of .

Now we show that the minimization of over $q\in\Q_K$ simplifies to a minimum eigenvalue problem for the weights $\{\alpha_k\}_{k=1}^{K}$. To obtain the eigenvalue problem, we substitute the orthogonal function expansion in into for the unbiased estimator of $\D(q,p)$. As an intermediate step, we differentiate to obtain the scores $$\label{eq:scores}
\nabla\log q(z^b) = \frac{2\sum_k \alpha_k \nabla \phi_k(z^b)}{\sum_k \alpha_k \phi_k(z^b)}.$$ Further substitution of the scores provides the key result behind our approach: the unbiased estimator in is a simple quadratic form in the weights $\alpha := [\alpha_1,\ldots,\alpha_K]^\top$ of the orthogonal function expansion, $$\widehat\D_{\pi}(q,p) =
\alpha^\top\! M\alpha,
\label{eq:quadform}$$ where the coefficients of the quadratic form are given by $$\label{eq:M}
  M_{jk} = \sum_{b=1}^B\frac{1}{\pi(z^b)}
    \left[2\nabla \phi_j(z^b) - \phi_j(z^b)\nabla\log p(z^b)\right]\cdot\left[2\nabla \phi_k(z^b) -
    \phi_k(z^b)\nabla\log p(z^b)\right].$$ Note that the elements of the $K\!\times\! K$ symmetric matrix $M$ capture all of the dependence on the batch of samples $\{z^b\}_{b=1}^B$, the scores of $p$ and $q$ at these samples, and the choice of the family of orthogonal functions. Next we minimize the quadratic form in subject to the sum-of-squares constraint $\sum_k \alpha_k^2 = 1$ in . In this way we obtain the eigenvalue problem  $$\begin{aligned}
  \label{eq:eigenVI-solution}
\min_{q\in\Q_K}\left[\widehat{\D}_{\pi}(q,p)\right] =
  \min_{\|\alpha\|=1}
  \left[\alpha^\top M\alpha\right] =:
  \lambda_{\text{min}}(M),
\end{aligned}$$ where $\lambda_{\text{min}}(M)$ is the minimal eigenvalue of $M$, and the optimal weights are given (up to an arbitrary sign) by its corresponding eigenvector; see for a proof. EigenVI solves .

We note that the eigenvalue problem in EigenVI arises from the curious alignment of three particular choices—namely, (i) the choice of variational family (based on orthogonal function expansions), (ii) the choice of divergence (based on score-matching), and (iii) the choice of estimator for the divergence (based on importance sampling). The simplicity of this eigenvalue problem stands in contrast to the many heuristics of gradient-based optimizations—involving learning rates, terminating criteria, and perhaps other algorithmic hyperparameters—that are typically required for ELBO-based BBVI . But EigenVI is also not entirely free of heuristics; to compute the estimator in we must also specify the proposal distribution $\pi$ and the number of samples $B$; see for a discussion.

The size of the eigenvalue problem in EigenVI is equal to the number of basis functions $K$ in the orthogonal function expansion of . The eigenvalue problem also generalizes to orthogonal function expansions that are formed from Cartesian products of one-dimensional families, but in this case, if multiple basis functions are used per dimension, then the overall basis size grows exponentially in the dimensionality. Thus, for example, the eigenvalue problem would be of size $K_1 K_2 K_3$ for the approximation in , as can be seen by “flattening” the tensor of weights $\beta$ in into the vector of weights $\alpha=\textbf{vec}(\beta)$ in . Finally, we note that EigenVI only needs to compute the minimal eigenvector of $M$ in , and therefore it can benefit from specialized routines that are much less expensive than a full diagonalization.

## EigenVI in $\reals^D$: the Hermite family and standardization

We now discuss the specific case of EigenVI for $\mathcal{Z}\!=\!\reals^D$ with the Hermite-based variational family in . For this case, we propose a transformation of the domain that serves to precondition or *standardize* the target distribution before applying EigenVI. While this standardization is not required to use EigenVI, it helps to reduce the number of basis functions needed to approximate the target, leading to a more computationally efficient procedure. It also suggests natural default choices for the proposal distribution $\pi$ in .

Recall that the eigenvalue problem grows linearly in size with the number of basis functions. Before applying EigenVI, our goal is therefore to transform the domain in a way that reduces the number of basis functions needed for a good approximation. To meet this goal for distributions over $\reals^D$, we observe that the lowest-order basis function of the Hermite family in yields (upon squaring) a standard multivariate Gaussian, with zero mean and unit covariance. Intuitively, we might expect the approximation of EigenVI to require fewer basis functions if the statistics of the target distribution nearly match those of this lowest-order basis function. The goal of standardization is to achieve this match, to whatever extent possible, by a suitable transformation of the underlying domain. Having done so, EigenVI in $\reals^D$ can then be viewed as a systematic framework to model non-Gaussian effects via a small number of higher-order terms in its orthogonal function expansion.

Concretely, we consider a linear transformation of the domain: $$\begin{aligned}
    \tilde{z} =
    %\T(z) =
    \Sigma^{-\frac{1}{2}}(z\!-\!\mu),
\end{aligned}$$ where $\mu$ and $\Sigma$ are estimates of the mean and covariance obtained from some other algorithm (e.g., a Laplace approximation, Gaussian variational inference, Monte Carlo, or domain-specific knowledge). We then apply the EigenVI to fit a $K$th-order variational approximation $\tilde q(\tilde z)$ to the target distribution $\tilde{p}(\tilde{z})$ that is induced by this transformation; afterwards, we reverse the change-of-variables to obtain the final approximation to $p(z)$, i.e., $$\begin{aligned}
q(z) = \tilde{q}(\tilde{z})|\Sigma|^{-1/2}.
\end{aligned}$$

shows why it is more difficult to approximate distributions that are badly centered or poorly scaled. The left panel shows the effect of translating a standard Gaussian *away* from the origin and *shrinking* its variance; note how a comparable approximation to the uncentered Gaussian now requires a 16th-order expansion. On the other hand, after standardization, the target can be perfectly fitted by the base distribution in the orthogonal family of reweighted Hermite polynomials. The right panel shows the similar effect of translating the mixture distribution in (right panel); comparing these panels, we see that twice as many basis functions ($K\!=\!14$ versus $K\!=\!7$) are required to provide a comparable fit of the uncentered mixture.

<figure id="fig:standardize">
<figure>
<span class="image placeholder" data-original-image-src="figs/1d_precond_gauss2.pdf" data-original-image-title=""></span>
<figcaption>Gaussian target, mean <span class="math inline">\(3\)</span> and variance <span class="math inline">\(\frac{1}{8}\)</span></figcaption>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figs/1d_precond_mixture2.pdf" data-original-image-title=""></span>
<figcaption>Mixture target (translation of )</figcaption>
</figure>
<figcaption>Higher-order expansions may be required to approximate target distributions (black) that are not standardized. <em>Left:</em> approximation of a non-standardized Gaussian. <em>Right:</em> approximation of the mixture distribution in  after translating its largest modes away from the origin. </figcaption>
</figure>

Finally, we note another benefit of standardizing the target before fitting EigenVI; when the target has nearly zero mean and unit covariance, it becomes simpler to identify natural choices for the proposal distribution $\pi$. Intuitively, in this case, we want a proposal distribution that has the same mean but heavier tails than a standard Gaussian. In our experiments, we use two types of centered proposal distributions—uniform and isotropic Gaussian—whose variances are greater than one.
