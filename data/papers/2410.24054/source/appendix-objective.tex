%\documentclass[main]{subfiles}

%\begin{document}

\section{Eigenvalue problem}
\label{app:eigen}

In this appendix we show in detail how the optimization for EigenVI reduces to a minimum eigenvalue problem. In particular we prove the following.

\begin{lemma}\label{lem:obj-min-eigen}
Let $\{\phi_k(z)\}_{k=1}^\infty$ be an orthogonal function expansion, and let $q\in\mathcal{Q}_K$ be the variational approximation parameterized by
     \begin{align}
        q(z) = \left[\sum_{k=1}^K \alpha_k \phi_k(z)\right]^2,
    \end{align}
where the weights satisfy $\sum_{k=1}^K \alpha_k^2=1$, thus ensuring that the distribution is normalized.
Suppose furthermore that $q$ is chosen to minimize the empirical estimate of the Fisher divergence given, as in eq.~(\ref{eq-empirical-divergence}), by
\begin{displaymath}
 \widehat\D_{\pi}(q, p) =
    \sum_{b=1}^B \frac{q(z^b)}{\pi(z^b)} \, \big\|\nabla \log q(z^b) - \nabla\log p(z^b)\big\|^2.
\end{displaymath}
Then the optimal variational approximation $q$ in this family
    can be computed by solving the minimum eigenvalue problem
    \begin{align}
        \label{eq:eigenVI-solution-ap}
\min_{q\in\Q_K}\left[\widehat{\D}_{\pi}(q,p)\right] = \min_{\|\alpha\|=1}
      \alpha^\top M \alpha
        %\left[\sum_{j,k=1}^{K} M_{jk}\alpha_j\alpha_k\right]
        =: \lambda_{\text{min}}(M),
      \end{align}
      where $M$ is given in~\Cref{eq:M} and  $\alpha =[\alpha_1, \ldots, \alpha_K]\in \R^K$.
%
    The optimal weights $\alpha$ are given (up to an arbitrary sign) by the corresponding eigenvector of this minimal eigenvalue.
\end{lemma}
\begin{proof}
    The scores of $q$ in this variational family are given by
\[\nabla\log q(z^b) = \frac{2\sum_k \alpha_k \nabla \phi_k(z^b)}{\sum_k \alpha_k \phi_k(z^b)}. \]
Substituting the above into the empirical divergence, we find that
\begin{align*}
    \widehat\D_{\pi}(q, p) &=   \sum_{b=1}^B \frac{q(z^b)}{\pi(z^b)} \, \big\|\nabla \log q(z^b) - \nabla\log p(z^b)\big\|^2\\
    &=  \sum_{b=1}^B \frac{\big(\sum_{k} \alpha_k \phi_k(z^b)\big)^2}{\pi(z^b)} \, \left\|\frac{2\sum_k \alpha_k \nabla \phi_k(z^b)}{\sum_k \alpha_k \phi_k(z^b)} - \nabla\log p(z^b)\right\|^2 \\
    &=  \sum_{b=1}^B \frac{1}{\pi(z^b)} \, \left\|2\sum_k \alpha_k \nabla \phi_k(z^b) - \bigg[\sum_{k}
    \alpha_k \phi_k(z^b)\bigg]\nabla\log p(z^b)\right\|^2\\
    &= \sum_{b=1}^B \frac{1}{\pi(z^b)} \, \left\|\sum_k \alpha_k \left[ 2\nabla \phi_k(z^b) -
\phi_k(z^b)\nabla\log p(z^b)\right]\right\|^2 \\
    &= \alpha^\top M \alpha,
\end{align*}
where $M$ is given in~\eqref{eq:M} and  $\alpha =[\alpha_1, \ldots, \alpha_K]\in \R^K$. Thus the optimal weights $\alpha$ are found by minimizing the quadratic form $\alpha^\top M\alpha$ subject to the constraint $\alpha^\top\alpha=1$. Equivalently, a solution can be found by minimizing the Rayleigh quotient
\begin{equation}
\argmin_v \frac{v^\top M v}{v^\top v}
\end{equation}
and setting $\alpha=v/\|v\|$. It then follows from the Rayleigh-Ritz theorem~\citep{courant1924methoden} for symmetric matrices that $\alpha$ is the eigenvector corresponding to the minimal eigenvalue of $M$, and this proves the lemma.

\iffalse
Thus the empirical divergence is a convex quadratic function of $\alpha$. Furthermore, since the gradient of the constraint $\alpha^\top\alpha=1$ is always non-zero, it follows that the constraint qualification holds and the solution to
    \Cref{eq:eigenVI-solution-ap} must satisfy the KKT equations.

The Lagrangian associated with \Cref{eq:eigenVI-solution-ap} is given by
\begin{align*}
    L(\alpha,\mu) := \widehat\D_{\pi}(q, p) + \mu \left(\sum_k \alpha_k^2 -1\right),
\end{align*}
where $\mu \in \R$ is the Lagrange multiplier. The associated KKT equations
are
\begin{align}
    0 &= \nabla_{\alpha }\widehat\D_{\pi}(q, p) + \mu  \nabla_{\alpha }\left(\sum_k \alpha_k^2 -1\right), \\
   0 &= \sum_k \alpha_k^2 -1.
\end{align}
Computing the gradients in $\alpha$ of the above, we find that
\begin{equation}\label{eq:eloz8od8jze}
%M \alpha + \mu \alpha =0.
2 M \alpha + 2\mu \alpha =0.
     %M \alpha + 2\mu \alpha =0.
    \end{equation}
Left multiplying the above by $\alpha^\top$, then enforcing the constraint, we find that
\[ \alpha^\top M \alpha +\mu  =0,\]
%\[ \alpha^\top M \alpha +2\mu  =0,\]
or equivalently that
\[\mu = -\alpha^\top M \alpha.\]
%\[\mu = -\tfrac{1}{2}\alpha^\top M \alpha.\]
Finally, isolating $\mu$ and substituting back into~\eqref{eq:eloz8od8jze} gives
\[ M \alpha = (\alpha^\top M \alpha) \alpha. \]
Consequently $\alpha$ is an eigenvector of $M$, with an associated eigenvalue $(\alpha^\top M \alpha)$. Though any eigenvalue-eigenvector pair provides a valid solution to the KKT equations, we want the solution that minimizes the objective function.
Since ths objective is equivalent to $\alpha^\top M\alpha$,
we have that $\alpha$ should be the eigenvector associated to the smallest eigenvalue. Since $M$ is also a symmetric matrix, we have that
the smallest eigenvalue is given by minimizing the Rayleigh quotient~\citep{courant1924methoden},
which is equivalent to the right hand side of \Cref{eq:eigenVI-solution-ap}.
\fi
\end{proof}


%\end{document}

