%\documentclass[main]{subfiles}

%\begin{document}

Probabilistic modeling is a cornerstone of modern data analysis,
uncertainty quantification, and decision making. A key challenge of
probabilistic inference is computing a target distribution of
interest; for instance, in Bayesian modeling, the goal is to compute a
posterior distribution, which is often intractable. Variational
inference (VI) \citep{jordan1999vi,wainwright2008graphical,blei2017vi}
is a popular method for scalable probabilistic inference that has worked across a range of applications. The idea behind VI is to approximate the target distribution by the closest member of some tractable family.

One major focus of research is to develop \textit{black-box} algorithms for variational inference
\citep{ranganath2014black,kingma2013auto,titsias2014doubly,kucukelbir2017automatic,  locatello2018boosting,giordano2023black,wang2022dual,modi2023,cai2024}.
Algorithms for black-box variational inference (BBVI) can be used to approximate any target distribution that is differentiable and computable up to some multiplicative (normalizing) constant; as such, they are extremely flexible. These algorithms have been widely implemented in popular probabilistic programming languages, and they are part of the modern toolbox for practitioners in computational statistics and data analysis~\citep{salvatier2016probabilistic,carpenter2017stan,ge2018turing,bingham2019pyro,abril2023pymc}.


Traditionally, the variational approximations in BBVI
are optimized by minimizing the Kullback-Leibler (KL)
divergence between the variational family and the target
(equivalently, maximizing the ELBO).
This strategy is powerful and
scalable, but it relies on stochastic gradient descent (SGD), which can be
difficult to tune
\citep{dhaka2020robust,dhaka2021challenges,zhang2022pathfinder}.
These difficulties can be acute even for Gaussian variational approximations~\citep{ranganath2014black,kucukelbir2017automatic}, particularly if these approximations employ full covariance matrices.

More recently, researchers have proposed algorithms for Gaussian BBVI that
do not require the use of SGD \citep{modi2023,cai2024}. Instead of minimizing the KL divergence, these methods aim to match the \emph{scores}, or the gradients of the log densities,
between the variational distribution and the target density. These methods exploit the special form of Gaussian distributions to derive closed-form proximal point
updates for score-matching. These updates are as inexpensive as SGD, but not as brittle. They show that score-based BBVI can be applied in an elegant way to Gaussian variational families.


In this paper, we show that score-based BBVI also yields simple, closed-form updates for a much broader family of variational approximations. Specifically, we
propose a new class of variational families constructed from
\textit{orthogonal function expansions} and inspired by solutions to the Schr\"odinger equation in quantum mechanics. These families are expressive enough to
parameterize a wide range of target distributions; at the same time, the distributions in these families are sufficiently tractable that one can calculate low-order moments and draw samples from them. In this paper, we mostly use orthogonal function expansions to construct distributions supported on $\reals^D$; in this case, the lowest-order term in the expansion is sufficient to model Gaussian behavior, while higher-order terms account for increasing amounts of non-Gaussianity. More generally, we also show how different basis sets of orthogonal functions can be used to construct variational families over other spaces.


To optimize over a variational family from this class, we minimize an
estimate of the Fisher divergence, which measures the scores of the
variational distribution against those of the target distribution. We
show that this optimization reduces to a minimum
eigenvalue problem, thus avoiding the need for gradient-based methods.
For this reason, we call our approach \emph{EigenVI}.

We study EigenVI with a variational family constructed from weighted
Hermite polynomials. We first demonstrate the expressiveness of this family on a
variety of
multimodal, asymmetric, and heavy-tailed distributions. We then use EigenVI to approximate a diverse collection of
non-Gaussian target distributions from \texttt{posteriordb}
\citep{magnusson2022posteriordb}, a benchmark suite of Bayesian
hierarchical models. On these problems, EigenVI provides more accurate posterior approximations than leading implementations of Gaussian
BBVI based on KL minimization and score-matching.


The organization of this paper is as follows.
In \Cref{sec:scorebasedVI} we introduce the variational families that arise from orthgonal function expansions, and we
show how score-matching in these families reduces to an
eigenvalue problem. In \Cref{sec:related} we review the literature related
to EigenVI. In \Cref{sec:experiments}, we evaluate EigenVI on a
variety of synthetic and real-data targets. Finally, in \Cref{sec:conclusion},
we discuss limitations and future work.




%\end{document}
%
