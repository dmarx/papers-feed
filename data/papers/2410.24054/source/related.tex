%\documentclass[main]{subfiles}

%\begin{document}

\section{Related work}
\label{sec:related}
\vspace{-4pt}

Several recent works have considered BBVI
methods based on score-matching.
These methods take a particularly simple form for
Gaussian variational families~\citep{modi2023,cai2024}.
%
The Fisher divergence \citep{hyvarinen2005estimation}
has been previously studied as a divergence
for variational inference \citep{yang2019variational}.
\citet{yu2023semiimplicit} propose minimizing a Fisher divergence
for semi-implicit (non-Gaussian) variational families; the divergence is minimized
using gradient-based optimization.
In another line of work,
\citet{zhang2018variational}
consider variational families of energy-based models
and derive a closed-form solution to minimize the Fisher divergence in this setting.

{More generally, there have many studies of VI with non-Gaussian variational families.
One common extension is to consider families of mixture models
\citep{guo2016boosting,miller2017variational,gershman2012nonparametric};
these are typically optimized via ELBO maximization.
}
BBVI algorithms have also been derived for
more expressive variational families of
energy-based models \citep{zhu1998filters,lecun2006tutorial,kim2016deep,dai2019kernel,lawson2019energy,zoltowski2021slice}
and normalizing flows
\citep{rezende2015variational,kingma2016improved,louizos2017multiplicative,berg2018sylvester,kobyzev2020normalizing,papamakarios2021normalizing}.
However the performance of these models, especially the normalizing flows, is often sensitive to the hyperparameters of the flow architecture and optimizer, as well as the parameters of the base distribution~\citep{dhaka2021challenges, agrawal2020advances}.
Other aspects of these variational approximations are also less straightforward; for example, one cannot compute their low-order moments, and one cannot easily evaluate or draw samples from the densities of energy-based models.

The variational approximation in EigenVI is based on the idea of squaring a weighted sum of basis functions. Probability distributions of this form arise most famously in quantum mechanics \citep{griffiths2018introduction}.
This idea has also been used to model distributions in machine learning, though not quite in the way proposed here.
\citet{novikov2021tensor} propose a tensor train-based model for density estimation, but they do not
consider orthogonal basis sets.
Similarly, \citet{loconte2024subtractive} obtain distributions by squaring a mixture model with
negative weights, and they study this model in conjunction with probabilistic circuits.
%
By contrast in this work, we consider this idea in the context of variational inference,
and we focus specifically on the use of orthogonal function expansions, which have many simplifying
properties;
additionally, the specific objective we optimize leads to a minimum eigenvalue problem.



%\end{document}

