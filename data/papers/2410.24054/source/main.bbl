\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abril-Pla et~al.(2023)Abril-Pla, Andreani, Carroll, Dong, Fonnesbeck,
  Kochurov, Kumar, Lao, Luhmann, Martin, et~al.]{abril2023pymc}
O.~Abril-Pla, V.~Andreani, C.~Carroll, L.~Dong, C.~J. Fonnesbeck, M.~Kochurov,
  R.~Kumar, J.~Lao, C.~C. Luhmann, O.~A. Martin, et~al.
\newblock {PyMC}: a modern, and comprehensive probabilistic programming
  framework in {P}ython.
\newblock \emph{PeerJ Computer Science}, 9:\penalty0 e1516, 2023.

\bibitem[Agrawal et~al.(2020)Agrawal, Sheldon, and Domke]{agrawal2020advances}
A.~Agrawal, D.~R. Sheldon, and J.~Domke.
\newblock Advances in black-box {VI}: Normalizing flows, importance weighting,
  and optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Berg et~al.(2018)Berg, Hasenclever, Tomczak, and
  Welling]{berg2018sylvester}
R.~v.~d. Berg, L.~Hasenclever, J.~M. Tomczak, and M.~Welling.
\newblock Sylvester normalizing flows for variational inference.
\newblock \emph{Uncertainty in Artificial Intelligence}, 2018.

\bibitem[Bingham et~al.(2019)Bingham, Chen, Jankowiak, Obermeyer, Pradhan,
  Karaletsos, Singh, Szerlip, Horsfall, and Goodman]{bingham2019pyro}
E.~Bingham, J.~P. Chen, M.~Jankowiak, F.~Obermeyer, N.~Pradhan, T.~Karaletsos,
  R.~Singh, P.~Szerlip, P.~Horsfall, and N.~D. Goodman.
\newblock Pyro: Deep universal probabilistic programming.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 973--978, 2019.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017vi}
D.~M. Blei, A.~Kucukelbir, and J.~D. McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Cai et~al.(2024)Cai, Modi, Pillaud-Vivien, Margossian, Gower, Blei,
  and Saul]{cai2024}
D.~Cai, C.~Modi, L.~Pillaud-Vivien, C.~Margossian, R.~Gower, D.~Blei, and
  L.~Saul.
\newblock Batch and match: black-box variational inference with a score-based
  divergence.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Carpenter et~al.(2017)Carpenter, Gelman, Hoffman, Lee, Goodrich,
  Betancourt, Brubaker, Guo, Li, and Riddell]{carpenter2017stan}
B.~Carpenter, A.~Gelman, M.~D. Hoffman, D.~Lee, B.~Goodrich, M.~Betancourt,
  M.~Brubaker, J.~Guo, P.~Li, and A.~Riddell.
\newblock Stan: A probabilistic programming language.
\newblock \emph{Journal of Statistical Software}, 76\penalty0 (1):\penalty0
  1--32, 2017.

\bibitem[Courant and Hilbert(1924)]{courant1924methoden}
R.~Courant and D.~Hilbert.
\newblock \emph{Methoden der Mathematischen Physik}, volume~1.
\newblock Julius Springer, Berlin, 1924.

\bibitem[Dai et~al.(2019)Dai, Dai, Gretton, Song, Schuurmans, and
  He]{dai2019kernel}
B.~Dai, H.~Dai, A.~Gretton, L.~Song, D.~Schuurmans, and N.~He.
\newblock Kernel exponential family estimation via doubly dual embedding.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2019.

\bibitem[Dhaka et~al.(2020)Dhaka, Catalina, Andersen, Magnusson, Huggins, and
  Vehtari]{dhaka2020robust}
A.~K. Dhaka, A.~Catalina, M.~R. Andersen, M.~Magnusson, J.~Huggins, and
  A.~Vehtari.
\newblock Robust, accurate stochastic optimization for variational inference.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Dhaka et~al.(2021)Dhaka, Catalina, Welandawe, Andersen, Huggins, and
  Vehtari]{dhaka2021challenges}
A.~K. Dhaka, A.~Catalina, M.~Welandawe, M.~R. Andersen, J.~Huggins, and
  A.~Vehtari.
\newblock Challenges and opportunities in high dimensional variational
  inference.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Dinh et~al.(2017)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
L.~Dinh, J.~Sohl-Dickstein, and S.~Bengio.
\newblock Density estimation using real {NVP}.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Ge et~al.(2018)Ge, Xu, and Ghahramani]{ge2018turing}
H.~Ge, K.~Xu, and Z.~Ghahramani.
\newblock Turing: a language for flexible probabilistic inference.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2018.

\bibitem[Gershman et~al.(2012)Gershman, Hoffman, and
  Blei]{gershman2012nonparametric}
S.~Gershman, M.~Hoffman, and D.~Blei.
\newblock Nonparametric variational inference.
\newblock \emph{International Conference on Machine Learning}, 2012.

\bibitem[Giordano et~al.(2024)Giordano, Ingram, and
  Broderick]{giordano2023black}
R.~Giordano, M.~Ingram, and T.~Broderick.
\newblock Black box variational inference with a deterministic objective:
  Faster, more accurate, and even more black box.
\newblock \emph{Journal of Machine Learning Research}, 25\penalty0
  (18):\penalty0 1--39, 2024.

\bibitem[Griffiths and Schroeter(2018)]{griffiths2018introduction}
D.~J. Griffiths and D.~F. Schroeter.
\newblock \emph{Introduction to Quantum Mechanics}.
\newblock Cambridge University Press, 2018.

\bibitem[Guo et~al.(2016)Guo, Wang, Fan, Broderick, and
  Dunson]{guo2016boosting}
F.~Guo, X.~Wang, K.~Fan, T.~Broderick, and D.~B. Dunson.
\newblock Boosting variational inference.
\newblock \emph{arXiv preprint arXiv:1611.05559}, 2016.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
A.~Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (4), 2005.

\bibitem[Jones and Pewsey(2009)]{jones2009sinh}
C.~Jones and A.~Pewsey.
\newblock Sinh-arcsinh distributions.
\newblock \emph{Biometrika}, 96\penalty0 (4):\penalty0 761--780, 2009.

\bibitem[Jones and Pewsey(2019)]{jones2019sinh}
C.~Jones and A.~Pewsey.
\newblock The sinh-arcsinh normal distribution.
\newblock \emph{Significance}, 16\penalty0 (2):\penalty0 6--7, 2019.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{jordan1999vi}
M.~I. Jordan, Z.~Ghahramani, T.~S. Jaakkola, and L.~K. Saul.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine Learning}, 37:\penalty0 183--233, 1999.

\bibitem[Kim and Bengio(2016)]{kim2016deep}
T.~Kim and Y.~Bengio.
\newblock Deep directed generative models with energy-based probability
  estimation.
\newblock \emph{arXiv preprint arXiv:1606.03439}, 2016.

\bibitem[Kingma and Welling(2014)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
D.~P. Kingma, T.~Salimans, R.~Jozefowicz, X.~Chen, I.~Sutskever, and
  M.~Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Kobyzev et~al.(2020)Kobyzev, Prince, and
  Brubaker]{kobyzev2020normalizing}
I.~Kobyzev, S.~J. Prince, and M.~A. Brubaker.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 43\penalty0 (11):\penalty0 3964--3979, 2020.

\bibitem[K{\"o}hler et~al.(2021)K{\"o}hler, Kr{\"a}mer, and
  No{\'e}]{kohler2021smooth}
J.~K{\"o}hler, A.~Kr{\"a}mer, and F.~No{\'e}.
\newblock Smooth normalizing flows.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Kucukelbir et~al.(2017)Kucukelbir, Tran, Ranganath, Gelman, and
  Blei]{kucukelbir2017automatic}
A.~Kucukelbir, D.~Tran, R.~Ranganath, A.~Gelman, and D.~M. Blei.
\newblock Automatic differentiation variational inference.
\newblock \emph{Journal of Machine Learning Research}, 2017.

\bibitem[Lawson et~al.(2019)Lawson, Tucker, Dai, and
  Ranganath]{lawson2019energy}
J.~Lawson, G.~Tucker, B.~Dai, and R.~Ranganath.
\newblock Energy-inspired models: Learning with sampler-induced distributions.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun2006tutorial}
Y.~LeCun, S.~Chopra, R.~Hadsell, M.~Ranzato, and F.~Huang.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting Structured Data}, 1\penalty0 (0), 2006.

\bibitem[Lehoucq et~al.(1998)Lehoucq, Sorensen, and Yang]{arpack1998}
R.~B. Lehoucq, D.~C. Sorensen, and C.~Yang.
\newblock \emph{{ARPACK Users' Guide: Solution of Large-Scale Eigenvalue
  Problems with Implicitly Restarted Arnoldi Methods}}.
\newblock SIAM, 1998.
\newblock Available at \url{http://www.caam.rice.edu/software/ARPACK/}.

\bibitem[Liu and Wang(2016)]{liu2016stein}
Q.~Liu and D.~Wang.
\newblock Stein variational gradient descent:\ a general purpose {B}ayesian
  inference algorithm.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Locatello et~al.(2018)Locatello, Dresdner, Khanna, Valera, and
  R{\"a}tsch]{locatello2018boosting}
F.~Locatello, G.~Dresdner, R.~Khanna, I.~Valera, and G.~R{\"a}tsch.
\newblock Boosting black box variational inference.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Loconte et~al.(2024)Loconte, Sladek, Mengel, Trapp, Solin, Gillis, and
  Vergari]{loconte2024subtractive}
L.~Loconte, A.~M. Sladek, S.~Mengel, M.~Trapp, A.~Solin, N.~Gillis, and
  A.~Vergari.
\newblock Subtractive mixture models via squaring: Representation and learning.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Louizos and Welling(2017)]{louizos2017multiplicative}
C.~Louizos and M.~Welling.
\newblock Multiplicative normalizing flows for variational {B}ayesian neural
  networks.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2017.

\bibitem[Magnusson et~al.(2022)Magnusson, Bürkner, and
  Vehtari]{magnusson2022posteriordb}
M.~Magnusson, P.~Bürkner, and A.~Vehtari.
\newblock posteriordb: a set of posteriors for {B}ayesian inference and
  probabilistic programming.
\newblock \url{https://github.com/stan-dev/posteriordb}, 2022.

\bibitem[Miller et~al.(2017)Miller, Foti, and Adams]{miller2017variational}
A.~C. Miller, N.~J. Foti, and R.~P. Adams.
\newblock Variational boosting: Iteratively refining posterior approximations.
\newblock In \emph{International Conference on Machine Learning}, pages
  2420--2429. PMLR, 2017.

\bibitem[Modi et~al.(2023)Modi, Margossian, Yao, Gower, Blei, and
  Saul]{modi2023}
C.~Modi, C.~Margossian, Y.~Yao, R.~Gower, D.~Blei, and L.~Saul.
\newblock Variational inference with {G}aussian score matching.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Novikov et~al.(2021)Novikov, Panov, and Oseledets]{novikov2021tensor}
G.~S. Novikov, M.~E. Panov, and I.~V. Oseledets.
\newblock Tensor-train density estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1321--1331.
  PMLR, 2021.

\bibitem[Papamakarios et~al.(2021)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2021normalizing}
G.~Papamakarios, E.~Nalisnick, D.~J. Rezende, S.~Mohamed, and
  B.~Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (57):\penalty0 1--64, 2021.

\bibitem[Ranganath et~al.(2014)Ranganath, Gerrish, and
  Blei]{ranganath2014black}
R.~Ranganath, S.~Gerrish, and D.~Blei.
\newblock Black box variational inference.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 814--822.
  PMLR, 2014.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
D.~Rezende and S.~Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2015.

\bibitem[Roualdes et~al.(2023)Roualdes, Ward, Axen, and
  Carpenter]{roualdes2023bridgestan}
E.~Roualdes, B.~Ward, S.~Axen, and B.~Carpenter.
\newblock Bridge{S}tan: Efficient in-memory access to {S}tan programs through
  {P}ython, {J}ulia, and {R}.
\newblock \url{https://github.com/roualdes/bridgestan}, 2023.

\bibitem[Salvatier et~al.(2016)Salvatier, Wiecki, and
  Fonnesbeck]{salvatier2016probabilistic}
J.~Salvatier, T.~V. Wiecki, and C.~Fonnesbeck.
\newblock Probabilistic programming in {P}ython using {PyMC3}.
\newblock \emph{PeerJ Computer Science}, 2:\penalty0 e55, 2016.

\bibitem[Titsias and L{\'a}zaro-Gredilla(2014)]{titsias2014doubly}
M.~Titsias and M.~L{\'a}zaro-Gredilla.
\newblock Doubly stochastic variational {B}ayes for non-conjugate inference.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2014.

\bibitem[Wainwright et~al.(2008)Wainwright, Jordan,
  et~al.]{wainwright2008graphical}
M.~J. Wainwright, M.~I. Jordan, et~al.
\newblock Graphical models, exponential families, and variational inference.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  1\penalty0 (1--2):\penalty0 1--305, 2008.

\bibitem[Wang et~al.(2024)Wang, Geffner, and Domke]{wang2022dual}
X.~Wang, T.~Geffner, and J.~Domke.
\newblock Dual control variate for faster black-box variational inference.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2024.

\bibitem[Yang et~al.(2019)Yang, Martin, and Bondell]{yang2019variational}
Y.~Yang, R.~Martin, and H.~Bondell.
\newblock Variational approximations using {F}isher divergence.
\newblock \emph{arXiv preprint arXiv:1905.05284}, 2019.

\bibitem[Yu and Zhang(2023)]{yu2023semiimplicit}
L.~Yu and C.~Zhang.
\newblock Semi-implicit variational inference via score matching.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Zeghal et~al.(2022)Zeghal, Lanusse, Boucaud, Remy, and
  Aubourg]{Zeghal2022npe}
J.~Zeghal, F.~Lanusse, A.~Boucaud, B.~Remy, and E.~Aubourg.
\newblock {Neural Posterior Estimation with Differentiable Simulators}.
\newblock In \emph{{International Conference on Machine Learning Conference}},
  2022.

\bibitem[Zhang et~al.(2018)Zhang, Shahbaba, and Zhao]{zhang2018variational}
C.~Zhang, B.~Shahbaba, and H.~Zhao.
\newblock Variational {H}amiltonian {M}onte {C}arlo via score matching.
\newblock \emph{Bayesian Analysis}, 13\penalty0 (2):\penalty0 485, 2018.

\bibitem[Zhang et~al.(2022)Zhang, Carpenter, Gelman, and
  Vehtari]{zhang2022pathfinder}
L.~Zhang, B.~Carpenter, A.~Gelman, and A.~Vehtari.
\newblock Pathfinder: Parallel quasi-newton variational inference.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (306):\penalty0 1--49, 2022.

\bibitem[Zhu et~al.(1998)Zhu, Wu, and Mumford]{zhu1998filters}
S.~C. Zhu, Y.~Wu, and D.~Mumford.
\newblock Filters, random fields and maximum entropy ({FRAME}): Towards a
  unified theory for texture modeling.
\newblock \emph{International Journal of Computer Vision}, 27:\penalty0
  107--126, 1998.

\bibitem[Zoltowski et~al.(2021)Zoltowski, Cai, and Adams]{zoltowski2021slice}
D.~Zoltowski, D.~Cai, and R.~P. Adams.
\newblock Slice sampling reparameterization gradients.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 23532--23544, 2021.

\end{thebibliography}
