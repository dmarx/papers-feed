%\documentclass[main]{subfiles}

%\begin{document}

\section{Sampling from orthogonal function expansions}
\label{app:sampling}

In this appendix we show how to sample from a density on $\mathbb{R}^D$ constructed from a Cartesian product of orthogonal function expansions. Specifically, we assume that the density is of the form
\begin{equation}
    q(z_1,z_2,\ldots,z_D) = \left(\sum_{k_1=1}^{K_1} \cdots \sum_{k_D=1}^{K_D} \alpha_{k_1 k_2 \ldots k_D}\phi_{k_1}(z_1)\phi_{k_2}(z_2)\cdots\phi_{k_D}(z_D)\right)^2,
\end{equation}
where $\{\phi_{k}(\cdot)\}_{k=1}^\infty$ define a family of orthonormal functions on $\mathbb{R}$ and where the density is normalized by requiring that
\begin{equation}
\sum_{k_1 k_2\ldots k_D} \alpha_{k_1 k_2\ldots k_D}^2=1.
\end{equation}
To draw samples from this density, we describe a sequential procedure based on inverse transform sampling. In particular, we obtain a sample $z\in\R^D$ by the sequence of draws
\begin{align}
\label{eq:draw1}
z_1 & \sim  q(z_1),  \\
z_2 & \sim  q(z_2|z_1), \\
    \vdots & \qquad\quad \nonumber  \\
\label{eq:drawD}
z_D & \sim  q(z_D|z_1,z_2,\ldots,z_{D-1}).
\end{align}
This basic strategy can also be used to sample from distributions whose domains are Cartesian products of different one-dimensional spaces.

In what follows, we first introduce a ``core primitive'' density,
    and we show how to sample efficiently from its distribution.
    We then show how the sampling procedure in \Crefrange{eq:draw1}{eq:drawD}
    reduces to sampling from this core primitive; a key component of this procedure
    is the property of orthogonality, which helps facilitate the efficient computation of
    marginal distributions.

\subsubsection*{Core primitive}

First we describe the core primitive that we will use for each of the draws in
\Crefrange{eq:draw1}{eq:drawD}.
To begin, we observe the following: if $S$ is any positive semidefinite matrix with $\text{trace}(S)\!=\!1$, then
\begin{equation}
\rho(\xi) = \sum_{k,\ell=1}^K S_{k\ell} \phi_k(\xi)\phi_\ell(\xi),
\label{eq:psd-density}
\end{equation}
defines a normalized density over $\mathbb{R}$. In particular, since $S\succeq 0$, it follows that $\rho(\xi)\!\geq\! 0$ for all $\xi\!\in\!\mathbb{R}$, and since $\text{trace}(S)\!=\!1$, it follows that
\begin{equation}
\int_{-\infty}^\infty\!\! \rho(\xi)\, d\xi
  = \sum_{k,\ell=1}^K S_{k\ell} \int_{-\infty}^\infty\!\! \phi_k(\xi)\phi_\ell(\xi)\, d\xi
  = \sum_{k,\ell=1}^K S_{k\ell} \delta_{kl}
  = \text{trace}(S)
  = 1.
\end{equation}
The core primitive that we need is an efficient procedure to sample
from a normalized density of this form. We will see later that all of the densities
in
\Crefrange{eq:draw1}{eq:drawD}
can be expressed in this~form.

\subsubsection*{Inverse transform sampling}
Since the density in
\Cref{eq:psd-density}
is one-dimensional, we can obtain the draw we need by inverse transform sampling.
In particular, let $\C(\xi)$ denote the cumulative distribution function (CDF)
associated with \Cref{eq:psd-density}, which is given by
\begin{equation}
\C(\xi) = \int_{-\infty}^\xi\!\! \rho(z)\,dz,
\label{eq:CDF}
\end{equation}
and let $\C^{-1}(\xi)$ denote the inverse CDF.
Then at least in principle, we can draw a sample from $\rho$ by the two-step procedure
\begin{align}
    u & \sim \text{Uniform}[0,1], \\
    \xi & = \C^{-1}(u). \label{eq:invCDF}
\end{align}
Next we consider how to implement this procedure efficiently in practice,
and in particular, how to calculate the definite integral for the CDF in \Cref{eq:CDF}.
As shorthand, we define the doubly-indexed set of real-valued functions
\begin{equation}
    \Phi_{k\ell}(\xi) = \int_{-\infty}^\xi \phi_k(z)\phi_\ell(z)\, dz.
\end{equation}
It follows from orthogonality that $\Phi_{kl}(+\infty) = \delta_{kl}$ and from the Cauchy-Schwartz inequality that $|\Phi_{k\ell}(\xi)|\leq 1$ for all $\xi\in\mathbb{R}$. Our interest in these functions stems from the observation that
\begin{equation}
\C(\xi) = \sum_{k,\ell=1}^K S_{k\ell} \Phi_{kl}(\xi) = \text{trace}[S\Phi(\xi)],
\label{eq:CDF-trace}
\end{equation}
so that if we have already computed the functions $\Phi_{k\ell}(\xi)$,
then we can use \Cref{eq:CDF-trace} to compute the CDF whose inverse we need in
\Cref{eq:invCDF}.
In practice, we can use numerical quadrature to pre-compute $\Phi_{k\ell}(\xi)$
for many values along the real line and then solve \Cref{eq:invCDF} quickly by interpolation;
that is, given $u$, we find $\xi$ satisfying $\text{trace}[S\Phi(\xi)]=u$.
The result is an unbiased sample drawn from the density $\rho(\xi)$ in \Cref{eq:psd-density}.

\subsubsection*{Sequential sampling}
Finally we show that each draw in
\Crefrange{eq:draw1}{eq:drawD}
%eqs.~(\ref{eq:draw1}--\ref{eq:drawD})
reduces to the problem described above.
As in \Cref{sec:orth}, we work out the steps specifically for an example in $D\!=\!3$,
where we must draw the samples $z_1\sim q(z_1)$, $z_2\sim q(z_2|z_1)$ and
$z_3\sim q(z_3|z_1,z_2)$.
This example illustrates all the ideas needed for the general case but with a
minimum of indices.

Consider the joint distribution given by
\begin{equation}
q(z_1,z_2,z_3) = \left(\sum_{i=1}^{K_1}\sum_{j=1}^{K_2}\sum_{k=1}^{K_3} \beta_{ijk}\, \phi_i(z_1)\phi_j(z_2)\phi_k(z_3)\right)^2\quad\mbox{where}\quad \sum_{ijk}\beta^2_{ijk} = 1.
\label{eq:3d-redux}
\end{equation}
From this joint distribution, we can compute marginal distributions by
integrating out subsets of variables, and each integration over $\mathbb{R}$
gives rise to a contraction of indices, as in \Cref{eq:marginal}, due to the
property of orthogonality.
{In particular, expanding the square in
\Cref{eq:3d-redux}, we can write this joint distribution as
%
\begin{equation}
q(z_1,z_2,z_3) =
    \sum_{k,k'=1}^{K_3}
    \left[\sum_{i,i'=1}^{K_1}
    \sum_{j,j'=1}^{K_2}
\beta_{ijk}\,
\beta_{i'j'k'}\,
    \phi_i(z_1) \phi_{i'}(z_1) \phi_j(z_2) \phi_{j'}(z_2)
    \right] \phi_k(z_3) \phi_{k'}(z_3),
\label{eq:3d-redux2}
\end{equation}
and we can then contract the index $k'$ when integrating over $z_3$, since $\int \phi_k(z_3) \phi_{k'}(z_3) dz_3 = \delta_{kk'}$.
}

In this way we find that the marginal distributions are
\begin{align}
q(z_1,z_2) &= \sum_{j,j'=1}^{K_2} \left[\sum_{i,i'=1}^{K_1}\sum_{k=1}^{K_3} \beta_{ijk}\beta_{i'j'k} \phi_i(z_1)\phi_{i'}(z_1)\right]\phi_j(z_2)\phi_{j'}(z_2), \label{eq:marg2} \\
q(z_1) &= \sum_{i,i'=1}^{K_1} \left[\sum_{j=1}^{K_2}\sum_{k=1}^{K_3} \beta_{ijk}\beta_{i'jk}\right] \phi_i(z_1)\phi_{i'}(z_1).\label{eq:marg1}
\end{align}

Now note from the brackets in \Cref{eq:marg1} that this marginal distribution
is already in the quadratic form of \Cref{eq:psd-density} with coefficients
\begin{equation}
    S^{(1)}_{ii'} = \sum_{j=1}^{K_2}\sum_{k=1}^{K_3} \beta_{ijk}\beta_{i'jk}.
\end{equation}
From this first quadratic form, we can therefore use inverse transform sampling
to obtain a draw $z_1 \sim q(z_1)$.

Next we consider how to sample from the conditional
$q(z_2|z_1) = q(z_1,z_2)/q(z_1)$.
Again, from the brackets in \Cref{eq:marg2}, we see that this
conditional distribution is also in the quadratic form of \Cref{eq:psd-density}
with coefficients
\begin{equation}
    S_{jj'}^{(2)} = \frac{\sum_{i,i'=1}^{K_1}\sum_{k=1}^{K_3} \beta_{ijk}\beta_{i'j'k}\phi_i(z_1)\phi_{i'}(z_1)}{q(z_1)}.
\end{equation}
From this second quadratic form, we can therefore use inverse transform sampling
to obtain a draw $z_2 \sim q(z_2|z_1)$.
%
Finally, we consider how to sample from $q(z_3|z_1,z_2) = q(z_1,z_2,z_3)/q(z_1,z_2)$.
From \Cref{eq:3d-redux2}, we see that this conditional distribution is also in
the quadratic form of \Cref{eq:psd-density} with coefficients
\begin{equation}
    S_{kk'}^{(3)} = \frac{\sum_{i,i'=1}^{K_1}\sum_{j,j'=1}^{K_2}\beta_{ijk}\beta_{i'j'k'}\phi_i(z_1)\phi_{i'}(z_1)\phi_j(z_2)\phi_{j'}(z_2)}
    {q(z_1,z_2)}
    \label{eq:sampling3}
\end{equation}
From this third quadratic form, we can therefore use inverse transform sampling to obtain a draw $z_3\sim q(z_3|z_1,z_2)$.
Finally, from the sums in \Cref{eq:sampling3}, we see that the overall cost of
this procedure is $\O(K_1^2 K_2^2 K_3^2)$, or quadratic in the total number of
basis functions.


%\end{document}
