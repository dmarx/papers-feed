%\documentclass[main]{subfiles}

%\begin{document}

\section{Discussion of limitations and future work}
\label{sec:conclusion}
%\vspace{-10pt}

In this work, we introduced EigenVI, a new approach for score-based variational inference based on orthgonal function expansions.
% We propose a new variational family built from orthogonal function expansions that supports analytical moments and exact sampling.
%
The score-based objective for EigenVI is minimized by solving an eigenvalue problem, and thus this framework provides an alternative to gradient-based methods for BBVI.
% We show that minimizing a score-based objective is equivalent to solving an eigenvalue problem, which leads to an alternative approach to gradient-based optimization.
Importantly, many computations in EigenVI can be parallelized with respect to the batch of samples,
unlike in iterative methods. We applied EigenVI to many synthetic and real-world targets, and these experiments show that EigenVI provides
a principled way of improving upon Gaussian variational families.
%\paragraph{Limitations.}

Many future directions remain.
First, the approach described in this paper relies on importance sampling, and thus it may benefit from
% thus has the usual challenge of selecting a good importance distribution.
% Exploration of
more sophisticated methods for adaptive importance sampling. % We leave this for future work.
Second, it may be useful to construct  variational families from different orthogonal function expansions. Our empirical study focused on the family built from
normalized Hermite polynomials. But this family may require a very high-order expansion to model highly non-Gaussian targets, and such an expansion will be very expensive in high dimensions.
\iffalse
Without utilizing higher-order function expansions,
which are expensive in higher dimensions,
% which become prohibitively expensive with the dimension,
this family is limited to target functions that are close to Gaussian.
\fi
Though this family was sufficient for many of the targets we simulated, others will be crucial for modeling highly non-Gaussian targets.
\iffalse
As we observed in our simulation studies, this was sufficient for many of the targets
we considered.
In future work, designing new orthogonal basis sets will be crucial for
extension to highly non-Gaussian targets.
\fi
%
Another direction is to develop variational families whose orthogonal function
expansions scale more favorably with the dimension, perhaps by incorporating low rank structure in the target's covariance.
%
%\NA{
% In addition, many targets may have a lower effective dimension that may be useful when applying EigenVI.
Finally, it would be interesting to explore iterative versions of EigenVI in which each iteration solves a minimum eigenvalue problem on some subsample of data points. With such an approach, EigenVI could potentially be applied to very large-scale problems in Bayesian inference.
% where each iteration involves a minimum eigenvalue solve; such an approach would allow the method to be applied in large-scale data problems using subsampling, where the scores are computed from a subsampled batch of data points.
%of a particular
%matrixâ€”here each iteration could also take in scores from a subsampled batch of data pois.
%}


%\begin{enumerate}
%    \item Need to choose an importance distribution. Future work: adaptive IS or
%        iterative algorithm...
%    \item Hermite basis function needs a large order when the target is highly non-Gaussian.
%        This becomes prohibitively expensive for higher dimensions.
%        Thus, this method is best used an improvement on Gaussian BBVI.
%        Thus, for more complex targets in higher dimensions, crucial to use better basis functions
%        adaptive to the problem. E.g., wavelet basis.
%        Thus, future work involves developing better orthogonal basis sets for modeling
%        e.g. multimodality.
%    \item Limited dimensionality: in general to model richer target distributions
%        for higher dimensions is challenging.
%        Experiment with other approaches for scaling to high dimensions,
%        such as low-rank representations of the tensor, etc.
%\end{enumerate}




%\end{document}

