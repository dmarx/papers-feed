'1201.1717':
  abstract: "Hyperbolicity is a property of a graph that may be viewed as being a\
    \ \"soft\"\nversion of a tree, and recent empirical and theoretical work has suggested\
    \ that\nmany graphs arising in Internet and related data applications have hyperbolic\n\
    properties. We consider Gromov's notion of \\delta-hyperbolicity, and establish\n\
    several results for small-world and tree-like random graph models. First, we\n\
    study the hyperbolicity of Kleinberg small-world random graphs and show that\n\
    the hyperbolicity of these random graphs is not significantly improved\ncomparing\
    \ to graph diameter even when it greatly improves decentralized\nnavigation. Next\
    \ we study a class of tree-like graphs called ringed trees that\nhave constant\
    \ hyperbolicity. We show that adding random links among the leaves\nsimilar to\
    \ the small-world graph constructions may easily destroy the\nhyperbolicity of\
    \ the graphs, except for a class of random edges added using an\nexponentially\
    \ decaying probability function based on the ring distance among\nthe leaves.\n\
    \  Our study provides one of the first significant analytical results on the\n\
    hyperbolicity of a rich class of random graphs, which shed light on the\nrelationship\
    \ between hyperbolicity and navigability of random graphs, as well\nas on the\
    \ sensitivity of hyperbolic {\\delta} to noises in random graphs."
  arxivId: '1201.1717'
  arxiv_tags:
  - cs.SI
  - cs.DM
  - physics.soc-ph
  authors: Wei Chen, Wenjie Fang, Guangda Hu, Michael W. Mahoney
  created_at: '2024-12-29T02:53:15.614663'
  issue_number: 445
  issue_url: https://github.com/dmarx/arxiv-archive/issues/445
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:41:46.909Z'
  main_tex_file: null
  published_date: '2012-01-09T09:30:38Z'
  state: open
  title: On the Hyperbolicity of Small-World and Tree-Like Random Graphs
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1201.1717
'1503.02531':
  abstract: 'A very simple way to improve the performance of almost any machine learning

    algorithm is to train many different models on the same data and then to

    average their predictions. Unfortunately, making predictions using a whole

    ensemble of models is cumbersome and may be too computationally expensive to

    allow deployment to a large number of users, especially if the individual

    models are large neural nets. Caruana and his collaborators have shown that it

    is possible to compress the knowledge in an ensemble into a single model which

    is much easier to deploy and we develop this approach further using a different

    compression technique. We achieve some surprising results on MNIST and we show

    that we can significantly improve the acoustic model of a heavily used

    commercial system by distilling the knowledge in an ensemble of models into a

    single model. We also introduce a new type of ensemble composed of one or more

    full models and many specialist models which learn to distinguish fine-grained

    classes that the full models confuse. Unlike a mixture of experts, these

    specialist models can be trained rapidly and in parallel.'
  arxivId: '1503.02531'
  arxiv_tags:
  - stat.ML
  - cs.LG
  - cs.NE
  authors: Geoffrey Hinton, Oriol Vinyals, Jeff Dean
  created_at: '2024-12-27T10:15:07.627240'
  issue_number: 93
  issue_url: https://github.com/dmarx/arxiv-archive/issues/93
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-21T16:05:50.608Z'
  main_tex_file: null
  published_date: '2015-03-09T15:44:49Z'
  state: open
  title: Distilling the Knowledge in a Neural Network
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1503.02531
'1503.03585':
  abstract: 'A central problem in machine learning involves modeling complex data-sets

    using highly flexible families of probability distributions in which learning,

    sampling, inference, and evaluation are still analytically or computationally

    tractable. Here, we develop an approach that simultaneously achieves both

    flexibility and tractability. The essential idea, inspired by non-equilibrium

    statistical physics, is to systematically and slowly destroy structure in a

    data distribution through an iterative forward diffusion process. We then learn

    a reverse diffusion process that restores structure in data, yielding a highly

    flexible and tractable generative model of the data. This approach allows us to

    rapidly learn, sample from, and evaluate probabilities in deep generative

    models with thousands of layers or time steps, as well as to compute

    conditional and posterior probabilities under the learned model. We

    additionally release an open source reference implementation of the algorithm.'
  arxivId: '1503.03585'
  arxiv_tags:
  - cs.LG
  - cond-mat.dis-nn
  - q-bio.NC
  - stat.ML
  authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli
  created_at: '2024-12-27T08:36:57.788009'
  issue_number: 118
  issue_url: https://github.com/dmarx/arxiv-archive/issues/118
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T10:13:49.405825'
  last_visited: '2024-12-22T07:09:20.505Z'
  main_tex_file: null
  published_date: '2015-03-12T04:51:37Z'
  state: open
  title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics
  total_reading_time_seconds: 241
  url: https://arxiv.org/abs/1503.03585
'1705.08039':
  abstract: 'Representation learning has become an invaluable approach for learning
    from

    symbolic data such as text and graphs. However, while complex symbolic datasets

    often exhibit a latent hierarchical structure, state-of-the-art methods

    typically learn embeddings in Euclidean vector spaces, which do not account for

    this property. For this purpose, we introduce a new approach for learning

    hierarchical representations of symbolic data by embedding them into hyperbolic

    space -- or more precisely into an n-dimensional Poincar\''e ball. Due to the

    underlying hyperbolic geometry, this allows us to learn parsimonious

    representations of symbolic data by simultaneously capturing hierarchy and

    similarity. We introduce an efficient algorithm to learn the embeddings based

    on Riemannian optimization and show experimentally that Poincar\''e embeddings

    outperform Euclidean embeddings significantly on data with latent hierarchies,

    both in terms of representation capacity and in terms of generalization

    ability.'
  arxivId: '1705.08039'
  arxiv_tags:
  - cs.AI
  - cs.LG
  - stat.ML
  authors: Maximilian Nickel, Douwe Kiela
  created_at: '2024-12-29T02:53:03.618161'
  issue_number: 447
  issue_url: https://github.com/dmarx/arxiv-archive/issues/447
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-29T02:53:12.635537'
  last_visited: '2024-12-29T01:43:25.284Z'
  main_tex_file: null
  published_date: '2017-05-22T23:14:36Z'
  state: open
  title: Poincaré Embeddings for Learning Hierarchical Representations
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1705.08039
'1705.08926':
  abstract: 'Cooperative multi-agent systems can be naturally used to model many real

    world problems, such as network packet routing and the coordination of

    autonomous vehicles. There is a great need for new reinforcement learning

    methods that can efficiently learn decentralised policies for such systems. To

    this end, we propose a new multi-agent actor-critic method called

    counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised

    critic to estimate the Q-function and decentralised actors to optimise the

    agents'' policies. In addition, to address the challenges of multi-agent credit

    assignment, it uses a counterfactual baseline that marginalises out a single

    agent''s action, while keeping the other agents'' actions fixed. COMA also uses
    a

    critic representation that allows the counterfactual baseline to be computed

    efficiently in a single forward pass. We evaluate COMA in the testbed of

    StarCraft unit micromanagement, using a decentralised variant with significant

    partial observability. COMA significantly improves average performance over

    other multi-agent actor-critic methods in this setting, and the best performing

    agents are competitive with state-of-the-art centralised controllers that get

    access to the full state.'
  arxivId: '1705.08926'
  arxiv_tags:
  - cs.AI
  - cs.MA
  authors: Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli,
    Shimon Whiteson
  created_at: '2024-12-27T10:14:40.745766'
  issue_number: 127
  issue_url: https://github.com/dmarx/arxiv-archive/issues/127
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:25:41.427Z'
  main_tex_file: null
  published_date: '2017-05-24T18:52:17Z'
  state: open
  title: Counterfactual Multi-Agent Policy Gradients
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1705.08926
'1705.10359':
  abstract: 'Neural embeddings have been used with great success in Natural Language

    Processing (NLP). They provide compact representations that encapsulate word

    similarity and attain state-of-the-art performance in a range of linguistic

    tasks. The success of neural embeddings has prompted significant amounts of

    research into applications in domains other than language. One such domain is

    graph-structured data, where embeddings of vertices can be learned that

    encapsulate vertex similarity and improve performance on tasks including edge

    prediction and vertex labelling. For both NLP and graph based tasks, embeddings

    have been learned in high-dimensional Euclidean spaces. However, recent work

    has shown that the appropriate isometric space for embedding complex networks

    is not the flat Euclidean space, but negatively curved, hyperbolic space. We

    present a new concept that exploits these recent insights and propose learning

    neural embeddings of graphs in hyperbolic space. We provide experimental

    evidence that embedding graphs in their natural geometry significantly improves

    performance on downstream tasks for several real-world public datasets.'
  arxivId: '1705.10359'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth
  created_at: '2024-12-29T02:53:06.623351'
  issue_number: 448
  issue_url: https://github.com/dmarx/arxiv-archive/issues/448
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-29T02:53:12.636825'
  last_visited: '2024-12-29T01:43:26.258Z'
  main_tex_file: null
  published_date: '2017-05-29T18:47:30Z'
  state: open
  title: Neural Embeddings of Graphs in Hyperbolic Space
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/1705.10359
'1710.09412':
  abstract: 'Large deep neural networks are powerful, but exhibit undesirable behaviors

    such as memorization and sensitivity to adversarial examples. In this work, we

    propose mixup, a simple learning principle to alleviate these issues. In

    essence, mixup trains a neural network on convex combinations of pairs of

    examples and their labels. By doing so, mixup regularizes the neural network to

    favor simple linear behavior in-between training examples. Our experiments on

    the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show

    that mixup improves the generalization of state-of-the-art neural network

    architectures. We also find that mixup reduces the memorization of corrupt

    labels, increases the robustness to adversarial examples, and stabilizes the

    training of generative adversarial networks.'
  arxivId: '1710.09412'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz
  created_at: '2024-12-27T10:13:58.628761'
  issue_number: 162
  issue_url: https://github.com/dmarx/arxiv-archive/issues/162
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T17:42:57.473Z'
  main_tex_file: null
  published_date: '2017-10-25T18:30:49Z'
  state: open
  title: 'mixup: Beyond Empirical Risk Minimization'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1710.09412
'1711.11586':
  abstract: 'Many image-to-image translation problems are ambiguous, as a single input

    image may correspond to multiple possible outputs. In this work, we aim to

    model a \emph{distribution} of possible outputs in a conditional generative

    modeling setting. The ambiguity of the mapping is distilled in a

    low-dimensional latent vector, which can be randomly sampled at test time. A

    generator learns to map the given input, combined with this latent code, to the

    output. We explicitly encourage the connection between output and the latent

    code to be invertible. This helps prevent a many-to-one mapping from the latent

    code to the output during training, also known as the problem of mode collapse,

    and produces more diverse results. We explore several variants of this approach

    by employing different training objectives, network architectures, and methods

    of injecting the latent code. Our proposed method encourages bijective

    consistency between the latent encoding and output modes. We present a

    systematic comparison of our method and other variants on both perceptual

    realism and diversity.'
  arxivId: '1711.11586'
  arxiv_tags:
  - cs.CV
  - cs.GR
  - stat.ML
  authors: Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros,
    Oliver Wang, Eli Shechtman
  created_at: '2024-12-27T10:15:19.630788'
  issue_number: 14
  issue_url: https://github.com/dmarx/arxiv-archive/issues/14
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T10:15:19.631582'
  last_visited: '2024-12-15T07:21:47.081Z'
  main_tex_file: null
  published_date: '2017-11-30T18:59:01Z'
  state: open
  title: Toward Multimodal Image-to-Image Translation
  total_reading_time_seconds: 60
  url: https://arxiv.org/abs/1711.11586
'1803.00567':
  abstract: 'Optimal transport (OT) theory can be informally described using the words
    of

    the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in

    hand has to move a large pile of sand lying on a construction site. The goal of

    the worker is to erect with all that sand a target pile with a prescribed shape

    (for example, that of a giant sand castle). Naturally, the worker wishes to

    minimize her total effort, quantified for instance as the total distance or

    time spent carrying shovelfuls of sand. Mathematicians interested in OT cast

    that problem as that of comparing two probability distributions, two different

    piles of sand of the same volume. They consider all of the many possible ways

    to morph, transport or reshape the first pile into the second, and associate a

    "global" cost to every such transport, using the "local" consideration of how

    much it costs to move a grain of sand from one place to another. Recent years

    have witnessed the spread of OT in several fields, thanks to the emergence of

    approximate solvers that can scale to sizes and dimensions that are relevant to

    data sciences. Thanks to this newfound scalability, OT is being increasingly

    used to unlock various problems in imaging sciences (such as color or texture

    processing), computer vision and graphics (for shape manipulation) or machine

    learning (for regression, classification and density fitting). This short book

    reviews OT with a bias toward numerical methods and their applications in data

    sciences, and sheds lights on the theoretical properties of OT that make it

    particularly useful for some of these applications.'
  arxivId: '1803.00567'
  arxiv_tags:
  - stat.ML
  authors: Gabriel Peyré, Marco Cuturi
  created_at: '2024-12-27T10:14:22.628302'
  issue_number: 138
  issue_url: https://github.com/dmarx/arxiv-archive/issues/138
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T08:08:46.564Z'
  main_tex_file: null
  published_date: '2018-03-01T18:28:43Z'
  state: open
  title: Computational Optimal Transport
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1803.00567
'1804.03329':
  abstract: 'Hyperbolic embeddings offer excellent quality with few dimensions when

    embedding hierarchical data structures like synonym or type hierarchies. Given

    a tree, we give a combinatorial construction that embeds the tree in hyperbolic

    space with arbitrarily low distortion without using optimization. On WordNet,

    our combinatorial embedding obtains a mean-average-precision of 0.989 with only

    two dimensions, while Nickel et al.''s recent construction obtains 0.87 using

    200 dimensions. We provide upper and lower bounds that allow us to characterize

    the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To

    embed general metric spaces, we propose a hyperbolic generalization of

    multidimensional scaling (h-MDS). We show how to perform exact recovery of

    hyperbolic points from distances, provide a perturbation analysis, and give a

    recovery result that allows us to reduce dimensionality. The h-MDS approach

    offers consistently low distortion even with few dimensions across several

    datasets. Finally, we extract lessons from the algorithms and theory above to

    design a PyTorch-based implementation that can handle incomplete information

    and is scalable.'
  arxivId: '1804.03329'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Christopher De Sa, Albert Gu, Christopher Ré, Frederic Sala
  created_at: '2024-12-29T02:53:01.242504'
  issue_number: 457
  issue_url: https://github.com/dmarx/arxiv-archive/issues/457
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T02:34:35.575Z'
  main_tex_file: null
  published_date: '2018-04-10T03:39:16Z'
  state: open
  title: Representation Tradeoffs for Hyperbolic Embeddings
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1804.03329
'1804.08838':
  abstract: 'Many recently trained neural networks employ large numbers of parameters
    to

    achieve good performance. One may intuitively use the number of parameters

    required as a rough gauge of the difficulty of a problem. But how accurate are

    such notions? How many parameters are really needed? In this paper we attempt

    to answer this question by training networks not in their native parameter

    space, but instead in a smaller, randomly oriented subspace. We slowly increase

    the dimension of this subspace, note at which dimension solutions first appear,

    and define this to be the intrinsic dimension of the objective landscape. The

    approach is simple to implement, computationally tractable, and produces

    several suggestive conclusions. Many problems have smaller intrinsic dimensions

    than one might suspect, and the intrinsic dimension for a given dataset varies

    little across a family of models with vastly different sizes. This latter

    result has the profound implication that once a parameter space is large enough

    to solve a problem, extra parameters serve directly to increase the

    dimensionality of the solution manifold. Intrinsic dimension allows some

    quantitative comparison of problem difficulty across supervised, reinforcement,

    and other types of learning where we conclude, for example, that solving the

    inverted pendulum problem is 100 times easier than classifying digits from

    MNIST, and playing Atari Pong from pixels is about as hard as classifying

    CIFAR-10. In addition to providing new cartography of the objective landscapes

    wandered by parameterized models, the method is a simple technique for

    constructively obtaining an upper bound on the minimum description length of a

    solution. A byproduct of this construction is a simple approach for compressing

    networks, in some cases by more than 100 times.'
  arxivId: '1804.08838'
  arxiv_tags:
  - cs.LG
  - cs.NE
  - stat.ML
  authors: Chunyuan Li, Heerad Farkhoor, Rosanne Liu, Jason Yosinski
  created_at: '2024-12-27T08:37:30.796183'
  issue_number: 198
  issue_url: https://github.com/dmarx/arxiv-archive/issues/198
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:30.797641'
  last_visited: '2024-12-24T02:11:53.179Z'
  main_tex_file: null
  published_date: '2018-04-24T04:29:10Z'
  state: open
  title: Measuring the Intrinsic Dimension of Objective Landscapes
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/1804.08838
'1810.01588':
  abstract: 'Interpreting the prediction mechanism of complex models is currently
    one of

    the most important tasks in the machine learning field, especially with layered

    neural networks, which have achieved high predictive performance with various

    practical data sets. To reveal the global structure of a trained neural network

    in an interpretable way, a series of clustering methods have been proposed,

    which decompose the units into clusters according to the similarity of their

    inference roles. The main problems in these studies were that (1) we have no

    prior knowledge about the optimal resolution for the decomposition, or the

    appropriate number of clusters, and (2) there was no method with which to

    acquire knowledge about whether the outputs of each cluster have a positive or

    negative correlation with the input and output dimension values. In this paper,

    to solve these problems, we propose a method for obtaining a hierarchical

    modular representation of a layered neural network. The application of a

    hierarchical clustering method to a trained network reveals a tree-structured

    relationship among hidden layer units, based on their feature vectors defined

    by their correlation with the input and output dimension values.'
  arxivId: '1810.01588'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Chihiro Watanabe
  created_at: '2024-12-27T08:37:16.425853'
  issue_number: 230
  issue_url: https://github.com/dmarx/arxiv-archive/issues/230
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:16.427681'
  last_visited: '2024-12-24T03:07:38.277Z'
  main_tex_file: null
  published_date: '2018-10-03T05:38:26Z'
  state: open
  title: "Interpreting Layered Neural Networks via Hierarchical Modular\n  Representation"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/1810.01588
'1901.10159':
  abstract: 'To understand the dynamics of optimization in deep neural networks, we

    develop a tool to study the evolution of the entire Hessian spectrum throughout

    the optimization process. Using this, we study a number of hypotheses

    concerning smoothness, curvature, and sharpness in the deep learning

    literature. We then thoroughly analyze a crucial structural feature of the

    spectra: in non-batch normalized networks, we observe the rapid appearance of

    large isolated eigenvalues in the spectrum, along with a surprising

    concentration of the gradient in the corresponding eigenspaces. In batch

    normalized networks, these two effects are almost absent. We characterize these

    effects, and explain how they affect optimization speed through both theory and

    experiments. As part of this work, we adapt advanced tools from numerical

    linear algebra that allow scalable and accurate estimation of the entire

    Hessian spectrum of ImageNet-scale neural networks; this technique may be of

    independent interest in other applications.'
  arxivId: '1901.10159'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Behrooz Ghorbani, Shankar Krishnan, Ying Xiao
  created_at: '2024-12-27T08:37:06.795951'
  issue_number: 239
  issue_url: https://github.com/dmarx/arxiv-archive/issues/239
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:06.798545'
  last_visited: '2024-12-24T03:33:51.377Z'
  main_tex_file: null
  published_date: '2019-01-29T08:24:10Z'
  state: open
  title: "An Investigation into Neural Net Optimization via Hessian Eigenvalue\n \
    \ Density"
  total_reading_time_seconds: 102
  url: https://arxiv.org/abs/1901.10159
'1904.08779':
  abstract: 'We present SpecAugment, a simple data augmentation method for speech

    recognition. SpecAugment is applied directly to the feature inputs of a neural

    network (i.e., filter bank coefficients). The augmentation policy consists of

    warping the features, masking blocks of frequency channels, and masking blocks

    of time steps. We apply SpecAugment on Listen, Attend and Spell networks for

    end-to-end speech recognition tasks. We achieve state-of-the-art performance on

    the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.

    On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language

    model, and 5.8% WER with shallow fusion with a language model. This compares to

    the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we

    achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5''00 test set

    without the use of a language model, and 6.8%/14.1% with shallow fusion, which

    compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.'
  arxivId: '1904.08779'
  arxiv_tags:
  - eess.AS
  - cs.CL
  - cs.LG
  - cs.SD
  - stat.ML
  authors: Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
    Ekin D. Cubuk, Quoc V. Le
  created_at: '2024-12-27T11:12:01.994058'
  issue_number: 60
  issue_url: https://github.com/dmarx/arxiv-archive/issues/60
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-17T14:41:05.563Z'
  main_tex_file: null
  published_date: '2019-04-18T17:53:38Z'
  state: open
  title: "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1904.08779
'1906.04358':
  abstract: 'Not all neural network architectures are created equal, some perform
    much

    better than others for certain tasks. But how important are the weight

    parameters of a neural network compared to its architecture? In this work, we

    question to what extent neural network architectures alone, without learning

    any weight parameters, can encode solutions for a given task. We propose a

    search method for neural network architectures that can already perform a task

    without any explicit weight training. To evaluate these networks, we populate

    the connections with a single shared weight parameter sampled from a uniform

    random distribution, and measure the expected performance. We demonstrate that

    our method can find minimal neural network architectures that can perform

    several reinforcement learning tasks without weight training. On a supervised

    learning domain, we find network architectures that achieve much higher than

    chance accuracy on MNIST using random weights. Interactive version of this

    paper at https://weightagnostic.github.io/'
  arxivId: '1906.04358'
  arxiv_tags:
  - cs.LG
  - cs.NE
  - stat.ML
  authors: Adam Gaier, David Ha
  created_at: '2024-12-27T08:37:09.834733'
  issue_number: 235
  issue_url: https://github.com/dmarx/arxiv-archive/issues/235
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:09.836754'
  last_visited: '2024-12-24T03:27:56.957Z'
  main_tex_file: null
  published_date: '2019-06-11T02:40:11Z'
  state: open
  title: Weight Agnostic Neural Networks
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1906.04358
'1906.05433':
  abstract: 'Climate change is one of the greatest challenges facing humanity, and
    we, as

    machine learning experts, may wonder how we can help. Here we describe how

    machine learning can be a powerful tool in reducing greenhouse gas emissions

    and helping society adapt to a changing climate. From smart grids to disaster

    management, we identify high impact problems where existing gaps can be filled

    by machine learning, in collaboration with other fields. Our recommendations

    encompass exciting research questions as well as promising business

    opportunities. We call on the machine learning community to join the global

    effort against climate change.'
  arxivId: '1906.05433'
  arxiv_tags:
  - cs.CY
  - cs.AI
  - cs.LG
  - stat.ML
  authors: David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre
    Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques,
    Anna Waldman-Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik
    Mukkavilli, Konrad P. Kording, Carla Gomes, Andrew Y. Ng, Demis Hassabis, John
    C. Platt, Felix Creutzig, Jennifer Chayes, Yoshua Bengio
  created_at: '2024-12-27T10:15:04.667989'
  issue_number: 111
  issue_url: https://github.com/dmarx/arxiv-archive/issues/111
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T06:23:13.395Z'
  main_tex_file: null
  published_date: '2019-06-10T17:51:47Z'
  state: open
  title: Tackling Climate Change with Machine Learning
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1906.05433
'1912.02757':
  abstract: 'Deep ensembles have been empirically shown to be a promising approach
    for

    improving accuracy, uncertainty and out-of-distribution robustness of deep

    learning models. While deep ensembles were theoretically motivated by the

    bootstrap, non-bootstrap ensembles trained with just random initialization also

    perform well in practice, which suggests that there could be other explanations

    for why deep ensembles work well. Bayesian neural networks, which learn

    distributions over the parameters of the network, are theoretically

    well-motivated by Bayesian principles, but do not perform as well as deep

    ensembles in practice, particularly under dataset shift. One possible

    explanation for this gap between theory and practice is that popular scalable

    variational Bayesian methods tend to focus on a single mode, whereas deep

    ensembles tend to explore diverse modes in function space. We investigate this

    hypothesis by building on recent work on understanding the loss landscape of

    neural networks and adding our own exploration to measure the similarity of

    functions in the space of predictions. Our results show that random

    initializations explore entirely different modes, while functions along an

    optimization trajectory or sampled from the subspace thereof cluster within a

    single mode predictions-wise, while often deviating significantly in the weight

    space. Developing the concept of the diversity--accuracy plane, we show that

    the decorrelation power of random initializations is unmatched by popular

    subspace sampling methods. Finally, we evaluate the relative effects of

    ensembling, subspace based methods and ensembles of subspace based methods, and

    the experimental results validate our hypothesis.'
  arxivId: '1912.02757'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan
  created_at: '2024-12-27T08:37:18.794896'
  issue_number: 226
  issue_url: https://github.com/dmarx/arxiv-archive/issues/226
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T03:02:43.131Z'
  main_tex_file: null
  published_date: '2019-12-05T17:48:18Z'
  state: open
  title: 'Deep Ensembles: A Loss Landscape Perspective'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1912.02757
'2001.04063':
  abstract: 'This paper presents a new sequence-to-sequence pre-training model called

    ProphetNet, which introduces a novel self-supervised objective named future

    n-gram prediction and the proposed n-stream self-attention mechanism. Instead

    of optimizing one-step-ahead prediction in the traditional sequence-to-sequence

    model, the ProphetNet is optimized by n-step ahead prediction that predicts the

    next n tokens simultaneously based on previous context tokens at each time

    step. The future n-gram prediction explicitly encourages the model to plan for

    the future tokens and prevent overfitting on strong local correlations. We

    pre-train ProphetNet using a base scale dataset (16GB) and a large-scale

    dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail,

    Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question

    generation tasks. Experimental results show that ProphetNet achieves new

    state-of-the-art results on all these datasets compared to the models using the

    same scale pre-training corpus.'
  arxivId: '2001.04063'
  arxiv_tags:
  - cs.CL
  authors: Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen,
    Ruofei Zhang, Ming Zhou
  created_at: '2024-12-27T08:36:45.863546'
  issue_number: 286
  issue_url: https://github.com/dmarx/arxiv-archive/issues/286
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:45.864400'
  last_visited: '2024-12-26T17:17:59.219Z'
  main_tex_file: null
  published_date: '2020-01-13T05:12:38Z'
  state: open
  title: "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence\n  Pre-training"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2001.04063
'2006.11120':
  abstract: 'A basic operation in Convolutional Neural Networks (CNNs) is spatial
    resizing

    of feature maps. This is done either by strided convolution (donwscaling) or

    transposed convolution (upscaling). Such operations are limited to a fixed

    filter moving at predetermined integer steps (strides). Spatial sizes of

    consecutive layers are related by integer scale factors, predetermined at

    architectural design, and remain fixed throughout training and inference time.

    We propose a generalization of the common Conv-layer, from a discrete layer to

    a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by

    representing the filter as a learned continuous function over sub-pixel

    coordinates. This allows learnable and principled resizing of feature maps, to

    any size, dynamically and consistently across scales. Once trained, the CC

    layer can be used to output any scale/size chosen at inference time. The scale

    can be non-integer and differ between the axes. CC gives rise to new freedoms

    for architectural design, such as dynamic layer shapes at inference time, or

    gradual architectures where the size changes by a small factor at each layer.

    This gives rise to many desired CNN properties, new architectural design

    capabilities, and useful applications. We further show that current Conv-layers

    suffer from inherent misalignments, which are ameliorated by CC layers.'
  arxivId: '2006.11120'
  arxiv_tags:
  - cs.LG
  - cs.CV
  - stat.ML
  authors: Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani
  created_at: '2024-12-27T08:36:54.792937'
  issue_number: 262
  issue_url: https://github.com/dmarx/arxiv-archive/issues/262
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:54.794180'
  last_visited: '2024-12-25T05:28:48.326Z'
  main_tex_file: null
  published_date: '2020-06-19T13:16:06Z'
  state: open
  title: From Discrete to Continuous Convolution Layers
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2006.11120
'2010.15110':
  abstract: 'In suitably initialized wide networks, small learning rates transform
    deep

    neural networks (DNNs) into neural tangent kernel (NTK) machines, whose

    training dynamics is well-approximated by a linear weight expansion of the

    network at initialization. Standard training, however, diverges from its

    linearization in ways that are poorly understood. We study the relationship

    between the training dynamics of nonlinear deep networks, the geometry of the

    loss landscape, and the time evolution of a data-dependent NTK. We do so

    through a large-scale phenomenological analysis of training, synthesizing

    diverse measures characterizing loss landscape geometry and NTK dynamics. In

    multiple neural architectures and datasets, we find these diverse measures

    evolve in a highly correlated manner, revealing a universal picture of the deep

    learning process. In this picture, deep network training exhibits a highly

    chaotic rapid initial transient that within 2 to 3 epochs determines the final

    linearly connected basin of low loss containing the end point of training.

    During this chaotic transient, the NTK changes rapidly, learning useful

    features from the training data that enables it to outperform the standard

    initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid

    chaotic transient, the NTK changes at constant velocity, and its performance

    matches that of full network training in 15% to 45% of training time. Overall,

    our analysis reveals a striking correlation between a diverse set of metrics

    over training time, governed by a rapid chaotic to stable transition in the

    first few epochs, that together poses challenges and opportunities for the

    development of more accurate theories of deep learning.'
  arxivId: '2010.15110'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
    Daniel M. Roy, Surya Ganguli
  created_at: '2024-12-27T08:37:03.796154'
  issue_number: 245
  issue_url: https://github.com/dmarx/arxiv-archive/issues/245
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-24T04:03:01.478Z'
  main_tex_file: null
  published_date: '2020-10-28T17:53:01Z'
  state: open
  title: "Deep learning versus kernel learning: an empirical study of loss\n  landscape\
    \ geometry and the time evolution of the Neural Tangent Kernel"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2010.15110
'2012.13255':
  abstract: 'Although pretrained language models can be fine-tuned to produce

    state-of-the-art results for a very wide range of language understanding tasks,

    the dynamics of this process are not well understood, especially in the low

    data regime. Why can we use relatively vanilla gradient descent algorithms

    (e.g., without strong regularization) to tune a model with hundreds of millions

    of parameters on datasets with only hundreds or thousands of labeled examples?

    In this paper, we argue that analyzing fine-tuning through the lens of

    intrinsic dimension provides us with empirical and theoretical intuitions to

    explain this remarkable phenomenon. We empirically show that common pre-trained

    models have a very low intrinsic dimension; in other words, there exists a low

    dimension reparameterization that is as effective for fine-tuning as the full

    parameter space. For example, by optimizing only 200 trainable parameters

    randomly projected back into the full space, we can tune a RoBERTa model to

    achieve 90\% of the full parameter performance levels on MRPC. Furthermore, we

    empirically show that pre-training implicitly minimizes intrinsic dimension

    and, perhaps surprisingly, larger models tend to have lower intrinsic dimension

    after a fixed number of pre-training updates, at least in part explaining their

    extreme effectiveness. Lastly, we connect intrinsic dimensionality with low

    dimensional task representations and compression based generalization bounds to

    provide intrinsic-dimension-based generalization bounds that are independent of

    the full parameter count.'
  arxivId: '2012.13255'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta
  created_at: '2024-12-27T08:37:21.793523'
  issue_number: 208
  issue_url: https://github.com/dmarx/arxiv-archive/issues/208
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:21.794281'
  last_visited: '2024-12-24T02:33:23.234Z'
  main_tex_file: null
  published_date: '2020-12-22T07:42:30Z'
  state: open
  title: "Intrinsic Dimensionality Explains the Effectiveness of Language Model\n\
    \  Fine-Tuning"
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2012.13255
'2105.05720':
  abstract: "Recent trend towards increasing large machine learning models require\
    \ both\ntraining and inference tasks to be distributed. Considering the huge cost\
    \ of\ntraining these models, it is imperative to unlock optimizations in computation\n\
    and communication to obtain best performance. However, current logical\nseparation\
    \ between computation and communication kernels in deep learning\nframeworks misses\
    \ the optimization opportunities across such barrier. Breaking\nthis abstraction\
    \ with a holistic consideration can provide many optimizations\nto provide performance\
    \ improvements in distributed workloads. Manually applying\nthese optimizations\
    \ needs modifications in underlying computation and\ncommunication libraries for\
    \ each scenario, which is time consuming and\nerror-prone.\n  Therefore, we present\
    \ CoCoNeT, with a DSL to express a program with both\ncomputation and communication.\
    \ CoCoNeT contains several machine learning aware\ntransformations to optimize\
    \ a program and a compiler to generate high\nperformance kernels. Providing both\
    \ computation and communication as first\nclass constructs allows users to work\
    \ on a high-level abstraction and apply\npowerful optimizations, such as fusion\
    \ or overlapping of communication and\ncomputation. CoCoNeT enables us to optimize\
    \ data-, model-and pipeline-parallel\nworkloads in large language models with\
    \ only a few lines of code. Experiments\nshow CoCoNeT significantly outperforms\
    \ state-of-the-art distributed machine\nlearning implementations."
  arxivId: '2105.05720'
  arxiv_tags:
  - cs.DC
  - cs.LG
  - cs.PL
  authors: Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed
    Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi
  created_at: '2024-12-27T11:12:13.651335'
  issue_number: 49
  issue_url: https://github.com/dmarx/arxiv-archive/issues/49
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T11:12:13.652097'
  last_visited: '2024-12-16T05:53:22.033Z'
  main_tex_file: null
  published_date: '2021-05-12T15:13:43Z'
  state: open
  title: "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed\
    \ Machine Learning Workloads"
  total_reading_time_seconds: 120
  url: https://arxiv.org/abs/2105.05720
'2106.04647':
  abstract: 'Adapting large-scale pretrained language models to downstream tasks via

    fine-tuning is the standard method for achieving state-of-the-art performance

    on NLP benchmarks. However, fine-tuning all weights of models with millions or

    billions of parameters is sample-inefficient, unstable in low-resource

    settings, and wasteful as it requires storing a separate copy of the model for

    each task. Recent work has developed parameter-efficient fine-tuning methods,

    but these approaches either still require a relatively large number of

    parameters or underperform standard fine-tuning. In this work, we propose

    Compacter, a method for fine-tuning large-scale language models with a better

    trade-off between task performance and the number of trainable parameters than

    prior work. Compacter accomplishes this by building on top of ideas from

    adapters, low-rank optimization, and parameterized hypercomplex multiplication

    layers. Specifically, Compacter inserts task-specific weight matrices into a

    pretrained model''s weights, which are computed efficiently as a sum of

    Kronecker products between shared "slow" weights and "fast" rank-one matrices

    defined per Compacter layer. By only training 0.047% of a pretrained model''s

    parameters, Compacter performs on par with standard fine-tuning on GLUE and

    outperforms standard fine-tuning on SuperGLUE and low-resource settings. Our

    code is publicly available at~\url{https://github.com/rabeehk/compacter}.'
  arxivId: '2106.04647'
  arxiv_tags:
  - cs.CL
  authors: Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder
  created_at: '2024-12-27T08:37:27.793777'
  issue_number: 202
  issue_url: https://github.com/dmarx/arxiv-archive/issues/202
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:27.794917'
  last_visited: '2024-12-24T02:31:09.658Z'
  main_tex_file: null
  published_date: '2021-06-08T19:17:04Z'
  state: open
  title: 'Compacter: Efficient Low-Rank Hypercomplex Adapter Layers'
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2106.04647
'2112.04215':
  abstract: 'Self-supervised models have been shown to produce comparable or better
    visual

    representations than their supervised counterparts when trained offline on

    unlabeled data at scale. However, their efficacy is catastrophically reduced in

    a Continual Learning (CL) scenario where data is presented to the model

    sequentially. In this paper, we show that self-supervised loss functions can be

    seamlessly converted into distillation mechanisms for CL by adding a predictor

    network that maps the current state of the representations to their past state.

    This enables us to devise a framework for Continual self-supervised visual

    representation Learning that (i) significantly improves the quality of the

    learned representations, (ii) is compatible with several state-of-the-art

    self-supervised objectives, and (iii) needs little to no hyperparameter tuning.

    We demonstrate the effectiveness of our approach empirically by training six

    popular self-supervised models in various CL settings.'
  arxivId: '2112.04215'
  arxiv_tags:
  - cs.CV
  - cs.LG
  authors: Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci,
    Karteek Alahari, Julien Mairal
  created_at: '2024-12-28T07:11:56.025562'
  issue_number: 407
  issue_url: https://github.com/dmarx/arxiv-archive/issues/407
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:29:58.974Z'
  main_tex_file: null
  published_date: '2021-12-08T10:39:13Z'
  state: open
  title: Self-Supervised Models are Continual Learners
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2112.04215
'2203.15556':
  abstract: 'We investigate the optimal model size and number of tokens for training
    a

    transformer language model under a given compute budget. We find that current

    large language models are significantly undertrained, a consequence of the

    recent focus on scaling language models whilst keeping the amount of training

    data constant. By training over 400 language models ranging from 70 million to

    over 16 billion parameters on 5 to 500 billion tokens, we find that for

    compute-optimal training, the model size and the number of training tokens

    should be scaled equally: for every doubling of model size the number of

    training tokens should also be doubled. We test this hypothesis by training a

    predicted compute-optimal model, Chinchilla, that uses the same compute budget

    as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla

    uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1

    (178B), and Megatron-Turing NLG (530B) on a large range of downstream

    evaluation tasks. This also means that Chinchilla uses substantially less

    compute for fine-tuning and inference, greatly facilitating downstream usage.

    As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%

    on the MMLU benchmark, greater than a 7% improvement over Gopher.'
  arxivId: '2203.15556'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den
    Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen,
    Jack W. Rae, Oriol Vinyals, Laurent Sifre
  created_at: '2024-12-27T11:11:59.168364'
  issue_number: 67
  issue_url: https://github.com/dmarx/arxiv-archive/issues/67
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-18T22:03:26.021Z'
  main_tex_file: null
  published_date: '2022-03-29T13:38:03Z'
  state: open
  title: Training Compute-Optimal Large Language Models
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2203.15556
'2204.00595':
  abstract: 'Large neural networks excel in many domains, but they are expensive to
    train

    and fine-tune. A popular approach to reduce their compute or memory

    requirements is to replace dense weight matrices with structured ones (e.g.,

    sparse, low-rank, Fourier transform). These methods have not seen widespread

    adoption (1) in end-to-end training due to unfavorable efficiency--quality

    tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable

    algorithms to approximate a given dense weight matrix. To address these issues,

    we propose a class of matrices (Monarch) that is hardware-efficient (they are

    parameterized as products of two block-diagonal matrices for better hardware

    utilization) and expressive (they can represent many commonly used transforms).

    Surprisingly, the problem of approximating a dense weight matrix with a Monarch

    matrix, though nonconvex, has an analytical optimal solution. These properties

    of Monarch matrices unlock new ways to train and fine-tune sparse and dense

    models. We empirically validate that Monarch can achieve favorable

    accuracy-efficiency tradeoffs in several end-to-end sparse training

    applications: speeding up ViT and GPT-2 training on ImageNet classification and

    Wikitext-103 language modeling by 2x with comparable model quality, and

    reducing the error on PDE solving and MRI reconstruction tasks by 40%. In

    sparse-to-dense training, with a simple technique called "reverse

    sparsification," Monarch matrices serve as a useful intermediate representation

    to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The

    same technique brings 23% faster BERT pretraining than even the very optimized

    implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse

    fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds

    up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.'
  arxivId: '2204.00595'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan,
    Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré
  created_at: '2024-12-28T06:18:33.790879'
  issue_number: 335
  issue_url: https://github.com/dmarx/arxiv-archive/issues/335
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T06:18:33.792789'
  last_visited: '2024-12-28T06:07:58.885Z'
  main_tex_file: null
  published_date: '2022-04-01T17:37:29Z'
  state: open
  title: "Monarch: Expressive Structured Matrices for Efficient and Accurate\n  Training"
  total_reading_time_seconds: 35
  url: https://arxiv.org/abs/2204.00595
'2205.13509':
  abstract: 'The presence of obstacles is intuitively expected to hinder the diffusive

    transport of micro-swimmers. However, for chiral micro-swimmers, a low density

    of obstacles near a surface can enhance their diffusive behavior, due to the

    rectification of the chiral motion by the obstacles. Here, we study numerically

    the role that disorder plays in determining the transport dynamics of chiral

    micro-swimmers on surfaces with obstacles. We consider different densities of

    regularly spaced obstacles and distinct types of disorder: noise in the

    dynamics of the micro-swimmer, quenched noise in the positions of the obstacles

    as well as obstacle size polydispersity. We show that, depending on the type

    and strength of the disorder, the presence of obstacles can either enhance or

    hinder transport, and discuss implications for the control of active transport

    in disordered media.'
  arxivId: '2205.13509'
  arxiv_tags:
  - cond-mat.soft
  - cond-mat.stat-mech
  authors: Danne M. van Roon, Giorgio Volpe, Margarida M. Telo da Gama, Nuno A. M.
    Araújo
  created_at: '2024-12-28T07:11:58.418730'
  issue_number: 405
  issue_url: https://github.com/dmarx/arxiv-archive/issues/405
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:35:33.619759'
  last_visited: '2024-12-28T07:25:12.900Z'
  main_tex_file: null
  published_date: '2022-05-26T17:22:50Z'
  state: open
  title: "The role of disorder in the motion of chiral swimmers in the presence of\n\
    \  obstacles"
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2205.13509
'2207.10342':
  abstract: 'Prompted models have demonstrated impressive few-shot learning abilities.

    Repeated interactions at test-time with a single model, or the composition of

    multiple models together, further expands capabilities. These compositions are

    probabilistic models, and may be expressed in the language of graphical models

    with random variables whose values are complex data types such as strings.

    Cases with control flow and dynamic structure require techniques from

    probabilistic programming, which allow implementing disparate model structures

    and inference strategies in a unified language. We formalize several existing

    techniques from this perspective, including scratchpads / chain of thought,

    verifiers, STaR, selection-inference, and tool use. We refer to the resulting

    programs as language model cascades.'
  arxivId: '2207.10342'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael
    Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein,
    Kevin Murphy, Charles Sutton
  created_at: '2024-12-28T09:13:00.534873'
  issue_number: 470
  issue_url: https://github.com/dmarx/arxiv-archive/issues/470
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-29T10:05:52.495491'
  last_visited: '2024-12-29T10:05:16.735000+00:00'
  main_tex_file: null
  published_date: '2022-07-21T07:35:18Z'
  state: open
  title: Language Model Cascades
  total_reading_time_seconds: 47
  url: https://arxiv.org/abs/2207.10342
'2208.11665':
  abstract: 'The Manifold Hypothesis is a widely accepted tenet of Machine Learning
    which

    asserts that nominally high-dimensional data are in fact concentrated near a

    low-dimensional manifold, embedded in high-dimensional space. This phenomenon

    is observed empirically in many real world situations, has led to development

    of a wide range of statistical methods in the last few decades, and has been

    suggested as a key factor in the success of modern AI technologies. We show

    that rich and sometimes intricate manifold structure in data can emerge from a

    generic and remarkably simple statistical model -- the Latent Metric Model --

    via elementary concepts such as latent variables, correlation and stationarity.

    This establishes a general statistical explanation for why the Manifold

    Hypothesis seems to hold in so many situations. Informed by the Latent Metric

    Model we derive procedures to discover and interpret the geometry of

    high-dimensional data, and explore hypotheses about the data generating

    mechanism. These procedures operate under minimal assumptions and make use of

    well known, scaleable graph-analytic algorithms.'
  arxivId: '2208.11665'
  arxiv_tags:
  - stat.ME
  - cs.LG
  - stat.ML
  - 62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30
  authors: Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy
  created_at: '2024-12-29T02:53:12.633908'
  issue_number: 452
  issue_url: https://github.com/dmarx/arxiv-archive/issues/452
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-29T02:26:31.276Z'
  main_tex_file: null
  published_date: '2022-08-24T17:00:16Z'
  state: open
  title: Statistical exploration of the Manifold Hypothesis
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2208.11665
'2210.14891':
  abstract: 'We present a smoothly broken power law functional form (that we refer
    to as a

    Broken Neural Scaling Law (BNSL)) that accurately models & extrapolates the

    scaling behaviors of deep neural networks (i.e. how the evaluation metric of

    interest varies as amount of compute used for training (or inference), number

    of model parameters, training dataset size, model input size, number of

    training steps, or upstream performance varies) for various architectures & for

    each of various tasks within a large & diverse set of upstream & downstream

    tasks, in zero-shot, prompted, & finetuned settings. This set includes

    large-scale vision, language, audio, video, diffusion, generative modeling,

    multimodal learning, contrastive learning, AI alignment, AI capabilities,

    robotics, out-of-distribution (OOD) generalization, continual learning,

    transfer learning, uncertainty estimation / calibration, OOD detection,

    adversarial robustness, distillation, sparsity, retrieval, quantization,

    pruning, fairness, molecules, computer programming/coding, math word problems,

    "emergent phase transitions", arithmetic, supervised learning,

    unsupervised/self-supervised learning, & reinforcement learning (single agent
    &

    multi-agent). When compared to other functional forms for neural scaling, this

    functional form yields extrapolations of scaling behavior that are considerably

    more accurate on this set. Moreover, this functional form accurately models &

    extrapolates scaling behavior that other functional forms are incapable of

    expressing such as the nonmonotonic transitions present in the scaling behavior

    of phenomena such as double descent & the delayed, sharp inflection points

    present in the scaling behavior of tasks such as arithmetic. Lastly, we use

    this functional form to glean insights about the limit of the predictability of

    scaling behavior. Code is available at

    https://github.com/ethancaballero/broken_neural_scaling_laws'
  arxivId: '2210.14891'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger
  created_at: '2024-12-27T10:15:16.790885'
  issue_number: 85
  issue_url: https://github.com/dmarx/arxiv-archive/issues/85
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-21T05:51:08.748Z'
  main_tex_file: null
  published_date: '2022-10-26T17:45:01Z'
  state: open
  title: Broken Neural Scaling Laws
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2210.14891
'2211.14453':
  abstract: 'Spectral analysis provides one of the most effective paradigms for

    information-preserving dimensionality reduction, as simple descriptions of

    naturally occurring signals are often obtained via few terms of periodic basis

    functions. In this work, we study deep neural networks designed to harness the

    structure in frequency domain for efficient learning of long-range correlations

    in space or time: frequency-domain models (FDMs). Existing FDMs are based on

    complex-valued transforms i.e. Fourier Transforms (FT), and layers that perform

    computation on the spectrum and input data separately. This design introduces

    considerable computational overhead: for each layer, a forward and inverse FT.

    Instead, this work introduces a blueprint for frequency domain learning through

    a single transform: transform once (T1). To enable efficient, direct learning

    in the frequency domain we derive a variance-preserving weight initialization

    scheme and investigate methods for frequency selection in reduced-order FDMs.

    Our results noticeably streamline the design process of FDMs, pruning redundant

    transforms, and leading to speedups of 3x to 10x that increase with data

    resolution and model size. We perform extensive experiments on learning the

    solution operator of spatio-temporal dynamics, including incompressible

    Navier-Stokes, turbulent flows around airfoils and high-resolution video of

    smoke. T1 models improve on the test performance of FDMs while requiring

    significantly less computation (5 hours instead of 32 for our large-scale

    experiment), with over 20% reduction in average predictive error across tasks.'
  arxivId: '2211.14453'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.SY
  - eess.SY
  authors: Michael Poli, Stefano Massaroli, Federico Berto, Jinykoo Park, Tri Dao,
    Christopher Ré, Stefano Ermon
  created_at: '2024-12-28T07:12:01.550818'
  issue_number: 404
  issue_url: https://github.com/dmarx/arxiv-archive/issues/404
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:35:33.621019'
  last_visited: '2024-12-28T07:23:40.569000+00:00'
  main_tex_file: null
  published_date: '2022-11-26T01:56:05Z'
  state: open
  title: 'Transform Once: Efficient Operator Learning in Frequency Domain'
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2211.14453
'2212.07677':
  abstract: 'At present, the mechanisms of in-context learning in Transformers are
    not

    well understood and remain mostly an intuition. In this paper, we suggest that

    training Transformers on auto-regressive objectives is closely related to

    gradient-based meta-learning formulations. We start by providing a simple

    weight construction that shows the equivalence of data transformations induced

    by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on
    a

    regression loss. Motivated by that construction, we show empirically that when

    training self-attention-only Transformers on simple regression tasks either the

    models learned by GD and Transformers show great similarity or, remarkably, the

    weights found by optimization match the construction. Thus we show how trained

    Transformers become mesa-optimizers i.e. learn models by gradient descent in

    their forward pass. This allows us, at least in the domain of regression

    problems, to mechanistically understand the inner workings of in-context

    learning in optimized Transformers. Building on this insight, we furthermore

    identify how Transformers surpass the performance of plain gradient descent by

    learning an iterative curvature correction and learn linear models on deep data

    representations to solve non-linear regression tasks. Finally, we discuss

    intriguing parallels to a mechanism identified to be crucial for in-context

    learning termed induction-head (Olsson et al., 2022) and show how it could be

    understood as a specific case of in-context learning by gradient descent

    learning within Transformers. Code to reproduce the experiments can be found at

    https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
    .'
  arxivId: '2212.07677'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CL
  authors: Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento,
    Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov
  created_at: '2024-12-27T10:15:10.617541'
  issue_number: 91
  issue_url: https://github.com/dmarx/arxiv-archive/issues/91
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-21T08:19:06.235Z'
  main_tex_file: null
  published_date: '2022-12-15T09:21:21Z'
  state: open
  title: Transformers learn in-context by gradient descent
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2212.07677
'2212.14052':
  abstract: 'State space models (SSMs) have demonstrated state-of-the-art sequence

    modeling performance in some modalities, but underperform attention in language

    modeling. Moreover, despite scaling nearly linearly in sequence length instead

    of quadratically, SSMs are still slower than Transformers due to poor hardware

    utilization. In this paper, we make progress on understanding the expressivity

    gap between SSMs and attention in language modeling, and on reducing the

    hardware barrier between SSMs and attention. First, we use synthetic language

    modeling tasks to understand the gap between SSMs and attention. We find that

    existing SSMs struggle with two capabilities: recalling earlier tokens in the

    sequence and comparing tokens across the sequence. To understand the impact on

    language modeling, we propose a new SSM layer, H3, that is explicitly designed

    for these abilities. H3 matches attention on the synthetic languages and comes

    within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid

    125M-parameter H3-attention model that retains two attention layers

    surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to

    improve the efficiency of training SSMs on modern hardware, we propose

    FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on

    sequences up to 8K, and introduces a novel state passing algorithm that

    exploits the recurrent properties of SSMs to scale to longer sequences.

    FlashConv yields 2$\times$ speedup on the long-range arena benchmark and allows

    hybrid language models to generate text 2.4$\times$ faster than Transformers.

    Using FlashConv, we scale hybrid H3-attention language models up to 2.7B

    parameters on the Pile and find promising initial results, achieving lower

    perplexity than Transformers and outperforming Transformers in zero- and

    few-shot learning on a majority of tasks in the SuperGLUE benchmark.'
  arxivId: '2212.14052'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher
    Ré
  created_at: '2024-12-28T07:12:04.417818'
  issue_number: 402
  issue_url: https://github.com/dmarx/arxiv-archive/issues/402
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:12:04.418588'
  last_visited: '2024-12-28T07:22:13.066Z'
  main_tex_file: null
  published_date: '2022-12-28T17:56:03Z'
  state: open
  title: 'Hungry Hungry Hippos: Towards Language Modeling with State Space Models'
  total_reading_time_seconds: 19
  url: https://arxiv.org/abs/2212.14052
'2302.05543':
  abstract: 'We present ControlNet, a neural network architecture to add spatial

    conditioning controls to large, pretrained text-to-image diffusion models.

    ControlNet locks the production-ready large diffusion models, and reuses their

    deep and robust encoding layers pretrained with billions of images as a strong

    backbone to learn a diverse set of conditional controls. The neural

    architecture is connected with "zero convolutions" (zero-initialized

    convolution layers) that progressively grow the parameters from zero and ensure

    that no harmful noise could affect the finetuning. We test various conditioning

    controls, eg, edges, depth, segmentation, human pose, etc, with Stable

    Diffusion, using single or multiple conditions, with or without prompts. We

    show that the training of ControlNets is robust with small (<50k) and large

    (>1m) datasets. Extensive results show that ControlNet may facilitate wider

    applications to control image diffusion models.'
  arxivId: '2302.05543'
  arxiv_tags:
  - cs.CV
  - cs.AI
  - cs.GR
  - cs.HC
  - cs.MM
  authors: Lvmin Zhang, Anyi Rao, Maneesh Agrawala
  created_at: '2024-12-27T08:37:24.799283'
  issue_number: 205
  issue_url: https://github.com/dmarx/arxiv-archive/issues/205
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:24.800097'
  last_visited: '2024-12-24T02:32:45.904Z'
  main_tex_file: null
  published_date: '2023-02-10T23:12:37Z'
  state: open
  title: Adding Conditional Control to Text-to-Image Diffusion Models
  total_reading_time_seconds: 23
  url: https://arxiv.org/abs/2302.05543
'2302.10866':
  abstract: 'Recent advances in deep learning have relied heavily on the use of large

    Transformers due to their ability to learn at scale. However, the core building

    block of Transformers, the attention operator, exhibits quadratic cost in

    sequence length, limiting the amount of context accessible. Existing

    subquadratic methods based on low-rank and sparse approximations need to be

    combined with dense attention layers to match Transformers, indicating a gap in

    capability. In this work, we propose Hyena, a subquadratic drop-in replacement

    for attention constructed by interleaving implicitly parametrized long

    convolutions and data-controlled gating. In recall and reasoning tasks on

    sequences of thousands to hundreds of thousands of tokens, Hyena improves

    accuracy by more than 50 points over operators relying on state-spaces and

    other implicit and explicit methods, matching attention-based models. We set a

    new state-of-the-art for dense-attention-free architectures on language

    modeling in standard datasets (WikiText103 and The Pile), reaching Transformer

    quality with a 20% reduction in training compute required at sequence length

    2K. Hyena operators are twice as fast as highly optimized attention at sequence

    length 8K, and 100x faster at sequence length 64K.'
  arxivId: '2302.10866'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen
    Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré
  created_at: '2024-12-28T07:12:07.460213'
  issue_number: 401
  issue_url: https://github.com/dmarx/arxiv-archive/issues/401
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:17:27.699Z'
  main_tex_file: null
  published_date: '2023-02-21T18:29:25Z'
  state: open
  title: 'Hyena Hierarchy: Towards Larger Convolutional Language Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2302.10866
'2302.11529':
  abstract: 'Transfer learning has recently become the dominant paradigm of machine

    learning. Pre-trained models fine-tuned for downstream tasks achieve better

    performance with fewer labelled examples. Nonetheless, it remains unclear how

    to develop models that specialise towards multiple tasks without incurring

    negative interference and that generalise systematically to non-identically

    distributed tasks. Modular deep learning has emerged as a promising solution to

    these challenges. In this framework, units of computation are often implemented

    as autonomous parameter-efficient modules. Information is conditionally routed

    to a subset of modules and subsequently aggregated. These properties enable

    positive transfer and systematic generalisation by separating computation from

    routing and updating modules locally. We offer a survey of modular

    architectures, providing a unified view over several threads of research that

    evolved independently in the scientific literature. Moreover, we explore

    various additional purposes of modularity, including scaling language models,

    causal inference, programme induction, and planning in reinforcement learning.

    Finally, we report various concrete applications where modularity has been

    successfully deployed such as cross-lingual and cross-modal knowledge transfer.

    Related talks and projects to this survey, are available at

    https://www.modulardeeplearning.com/.'
  arxivId: '2302.11529'
  arxiv_tags:
  - cs.LG
  authors: Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, Edoardo Maria Ponti
  created_at: '2024-12-27T08:37:12.875322'
  issue_number: 227
  issue_url: https://github.com/dmarx/arxiv-archive/issues/227
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:16.428802'
  last_visited: '2024-12-24T03:05:18.719Z'
  main_tex_file: null
  published_date: '2023-02-22T18:11:25Z'
  state: open
  title: Modular Deep Learning
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/2302.11529
'2302.13714':
  abstract: 'In this work, we investigate a challenging problem, which has been considered

    to be an important criterion in designing codewords for DNA computing purposes,

    namely secondary structure avoidance in single-stranded DNA molecules. In

    short, secondary structure refers to the tendency of a single-stranded DNA

    sequence to fold back upon itself, thus becoming inactive in the computation

    process. While some design criteria that reduces the possibility of secondary

    structure formation has been proposed by Milenkovic and Kashyap (2006), the

    main contribution of this work is to provide an explicit construction of DNA

    codes that completely avoid secondary structure of arbitrary stem length.

    Formally, given codeword length n and arbitrary integer m>=2, we provide

    efficient methods to construct DNA codes of length n that avoid secondary

    structure of any stem length more than or equal to m. Particularly, when m = 3,

    our constructions yield a family of DNA codes of rate 1.3031 bits/nt, while the

    highest rate found in the prior art was 1.1609 bits/nt. In addition, for

    m>=3log n + 4, we provide an efficient encoder that incurs only one redundant

    symbol.'
  arxivId: '2302.13714'
  arxiv_tags:
  - cs.IT
  - math.CO
  - math.IT
  authors: Tuan Thanh Nguyen, Kui Cai, Han Mao Kiah, Duc Tu Dao, Kees A. Schouhamer
    Immink
  created_at: '2024-12-28T07:12:10.687211'
  issue_number: 400
  issue_url: https://github.com/dmarx/arxiv-archive/issues/400
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:35:33.622237'
  last_visited: '2024-12-28T07:13:40.029Z'
  main_tex_file: null
  published_date: '2023-02-27T12:22:07Z'
  state: open
  title: "On the Design of Codes for DNA Computing: Secondary Structure Avoidance\n\
    \  Codes"
  total_reading_time_seconds: 19
  url: https://arxiv.org/abs/2302.13714
'2303.08500':
  abstract: 'Protecting personal data against exploitation of machine learning models
    is

    crucial. Recently, availability attacks have shown great promise to provide an

    extra layer of protection against the unauthorized use of data to train neural

    networks. These methods aim to add imperceptible noise to clean data so that

    the neural networks cannot extract meaningful patterns from the protected data,

    claiming that they can make personal data "unexploitable." This paper provides

    a strong countermeasure against such approaches, showing that unexploitable

    data might only be an illusion. In particular, we leverage the power of

    diffusion models and show that a carefully designed denoising process can

    counteract the effectiveness of the data-protecting perturbations. We

    rigorously analyze our algorithm, and theoretically prove that the amount of

    required denoising is directly related to the magnitude of the data-protecting

    perturbations. Our approach, called AVATAR, delivers state-of-the-art

    performance against a suite of recent availability attacks in various

    scenarios, outperforming adversarial training even under distribution mismatch

    between the diffusion model and the protected data. Our findings call for more

    research into making personal data unexploitable, showing that this goal is far

    from over. Our implementation is available at this repository:

    https://github.com/hmdolatabadi/AVATAR.'
  arxivId: '2303.08500'
  arxiv_tags:
  - cs.LG
  - cs.CR
  - cs.CV
  authors: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
  created_at: '2024-12-27T10:14:13.862291'
  issue_number: 156
  issue_url: https://github.com/dmarx/arxiv-archive/issues/156
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:41:41.995Z'
  main_tex_file: null
  published_date: '2023-03-15T10:20:49Z'
  state: open
  title: "The Devil's Advocate: Shattering the Illusion of Unexploitable Data\n  using\
    \ Diffusion Models"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2303.08500
'2303.09489':
  abstract: 'Time series modeling is a well-established problem, which often requires
    that

    methods (1) expressively represent complicated dependencies, (2) forecast long

    horizons, and (3) efficiently train over long sequences. State-space models

    (SSMs) are classical models for time series, and prior works combine SSMs with

    deep learning layers for efficient sequence modeling. However, we find

    fundamental limitations with these prior approaches, proving their SSM

    representations cannot express autoregressive time series processes. We thus

    introduce SpaceTime, a new state-space time series architecture that improves

    all three criteria. For expressivity, we propose a new SSM parameterization

    based on the companion matrix -- a canonical representation for discrete-time

    processes -- which enables SpaceTime''s SSM layers to learn desirable

    autoregressive processes. For long horizon forecasting, we introduce a

    "closed-loop" variation of the companion SSM, which enables SpaceTime to

    predict many future time-steps by generating its own layer-wise inputs. For

    efficient training and inference, we introduce an algorithm that reduces the

    memory and compute of a forward pass with the companion matrix. With sequence

    length $\ell$ and state-space size $d$, we go from $\tilde{O}(d \ell)$

    na\"ively to $\tilde{O}(d + \ell)$. In experiments, our contributions lead to

    state-of-the-art results on extensive and diverse benchmarks, with best or

    second-best AUROC on 6 / 7 ECG and speech time series classification, and best

    MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1)

    fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more

    accurately on longer horizons than prior state-of-the-art, and (3) speeds up

    training on real-world ETTh1 data by 73% and 80% relative wall-clock time over

    Transformers and LSTMs.'
  arxivId: '2303.09489'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Michael Zhang, Khaled K. Saab, Michael Poli, Tri Dao, Karan Goel, Christopher
    Ré
  created_at: '2024-12-28T06:17:57.902338'
  issue_number: 394
  issue_url: https://github.com/dmarx/arxiv-archive/issues/394
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T06:17:57.903137'
  last_visited: '2024-12-28T07:09:58.237Z'
  main_tex_file: null
  published_date: '2023-03-16T17:08:21Z'
  state: open
  title: Effectively Modeling Time Series with Simple Discrete State Spaces
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2303.09489
'2303.11435':
  abstract: 'Inversion by Direct Iteration (InDI) is a new formulation for supervised

    image restoration that avoids the so-called "regression to the mean" effect and

    produces more realistic and detailed images than existing regression-based

    methods. It does this by gradually improving image quality in small steps,

    similar to generative denoising diffusion models. Image restoration is an

    ill-posed problem where multiple high-quality images are plausible

    reconstructions of a given low-quality input. Therefore, the outcome of a

    single step regression model is typically an aggregate of all possible

    explanations, therefore lacking details and realism. The main advantage of InDI

    is that it does not try to predict the clean target image in a single step but

    instead gradually improves the image in small steps, resulting in better

    perceptual quality. While generative denoising diffusion models also work in

    small steps, our formulation is distinct in that it does not require knowledge

    of any analytic form of the degradation process. Instead, we directly learn an

    iterative restoration process from low-quality and high-quality paired

    examples. InDI can be applied to virtually any image degradation, given paired

    training data. In conditional denoising diffusion image restoration the

    denoising network generates the restored image by repeatedly denoising an

    initial image of pure noise, conditioned on the degraded input. Contrary to

    conditional denoising formulations, InDI directly proceeds by iteratively

    restoring the input low-quality image, producing high-quality results on a

    variety of image restoration tasks, including motion and out-of-focus

    deblurring, super-resolution, compression artifact removal, and denoising.'
  arxivId: '2303.11435'
  arxiv_tags:
  - eess.IV
  - cs.CV
  - cs.LG
  authors: Mauricio Delbracio, Peyman Milanfar
  created_at: '2024-12-27T10:14:19.620710'
  issue_number: 140
  issue_url: https://github.com/dmarx/arxiv-archive/issues/140
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T08:12:43.437Z'
  main_tex_file: null
  published_date: '2023-03-20T20:28:17Z'
  state: open
  title: "Inversion by Direct Iteration: An Alternative to Denoising Diffusion for\n\
    \  Image Restoration"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2303.11435
'2304.02234':
  abstract: 'Recently developed text-to-image diffusion models make it easy to edit
    or

    create high-quality images. Their ease of use has raised concerns about the

    potential for malicious editing or deepfake creation. Imperceptible

    perturbations have been proposed as a means of protecting images from malicious

    editing by preventing diffusion models from generating realistic images.

    However, we find that the aforementioned perturbations are not robust to JPEG

    compression, which poses a major weakness because of the common usage and

    availability of JPEG. We discuss the importance of robustness for additive

    imperceptible perturbations and encourage alternative approaches to protect

    images against editing.'
  arxivId: '2304.02234'
  arxiv_tags:
  - cs.LG
  - cs.CR
  - cs.CV
  authors: Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein
  created_at: '2024-12-27T10:14:10.609600'
  issue_number: 157
  issue_url: https://github.com/dmarx/arxiv-archive/issues/157
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:44:11.039Z'
  main_tex_file: null
  published_date: '2023-04-05T05:30:09Z'
  state: open
  title: JPEG Compressed Images Can Bypass Protections Against AI Editing
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2304.02234
'2304.15004':
  abstract: 'Recent work claims that large language models display emergent abilities,

    abilities not present in smaller-scale models that are present in larger-scale

    models. What makes emergent abilities intriguing is two-fold: their sharpness,

    transitioning seemingly instantaneously from not present to present, and their

    unpredictability, appearing at seemingly unforeseeable model scales. Here, we

    present an alternative explanation for emergent abilities: that for a

    particular task and model family, when analyzing fixed model outputs, emergent

    abilities appear due to the researcher''s choice of metric rather than due to

    fundamental changes in model behavior with scale. Specifically, nonlinear or

    discontinuous metrics produce apparent emergent abilities, whereas linear or

    continuous metrics produce smooth, continuous predictable changes in model

    performance. We present our alternative explanation in a simple mathematical

    model, then test it in three complementary ways: we (1) make, test and confirm

    three predictions on the effect of metric choice using the InstructGPT/GPT-3

    family on tasks with claimed emergent abilities; (2) make, test and confirm two

    predictions about metric choices in a meta-analysis of emergent abilities on

    BIG-Bench; and (3) show to choose metrics to produce never-before-seen

    seemingly emergent abilities in multiple vision tasks across diverse deep

    networks. Via all three analyses, we provide evidence that alleged emergent

    abilities evaporate with different metrics or with better statistics, and may

    not be a fundamental property of scaling AI models.'
  arxivId: '2304.15004'
  arxiv_tags:
  - cs.AI
  - cs.LG
  authors: Rylan Schaeffer, Brando Miranda, Sanmi Koyejo
  created_at: '2024-12-27T10:15:13.879181'
  issue_number: 89
  issue_url: https://github.com/dmarx/arxiv-archive/issues/89
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-21T06:06:11.226Z'
  main_tex_file: null
  published_date: '2023-04-28T17:52:11Z'
  state: open
  title: Are Emergent Abilities of Large Language Models a Mirage?
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2304.15004
'2305.06161':
  abstract: 'The BigCode community, an open-scientific collaboration working on the

    responsible development of Large Language Models for Code (Code LLMs),

    introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context

    length, infilling capabilities and fast large-batch inference enabled by

    multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced

    from The Stack, a large collection of permissively licensed GitHub repositories

    with inspection tools and an opt-out process. We fine-tuned StarCoderBase on

    35B Python tokens, resulting in the creation of StarCoder. We perform the most

    comprehensive evaluation of Code LLMs to date and show that StarCoderBase

    outperforms every open Code LLM that supports multiple programming languages

    and matches or outperforms the OpenAI code-cushman-001 model. Furthermore,

    StarCoder outperforms every model that is fine-tuned on Python, can be prompted

    to achieve 40\% pass@1 on HumanEval, and still retains its performance on other

    programming languages. We take several important steps towards a safe

    open-access model release, including an improved PII redaction pipeline and a

    novel attribution tracing tool, and make the StarCoder models publicly

    available under a more commercially viable version of the Open Responsible AI

    Model license.'
  arxivId: '2305.06161'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.PL
  - cs.SE
  authors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
    Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii
    Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
    Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,
    Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham
    Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry
    Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,
    Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor
    Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger,
    Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
    Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel
    Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes,
    Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
  created_at: '2024-12-28T06:18:00.787682'
  issue_number: 392
  issue_url: https://github.com/dmarx/arxiv-archive/issues/392
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:11:55.170834'
  last_visited: '2024-12-28T07:09:41.282Z'
  main_tex_file: null
  published_date: '2023-05-09T08:16:42Z'
  state: open
  title: 'StarCoder: may the source be with you!'
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2305.06161
'2305.19693':
  abstract: 'Generative diffusion models have recently emerged as a leading approach
    for

    generating high-dimensional data. In this paper, we show that the dynamics of

    these models exhibit a spontaneous symmetry breaking that divides the

    generative dynamics into two distinct phases: 1) A linear steady-state dynamics

    around a central fixed-point and 2) an attractor dynamics directed towards the

    data manifold. These two "phases" are separated by the change in stability of

    the central fixed-point, with the resulting window of instability being

    responsible for the diversity of the generated samples. Using both theoretical

    and empirical evidence, we show that an accurate simulation of the early

    dynamics does not significantly contribute to the final generation, since early

    fluctuations are reverted to the central fixed point. To leverage this insight,

    we propose a Gaussian late initialization scheme, which significantly improves

    model performance, achieving up to 3x FID improvements on fast samplers, while

    also increasing sample diversity (e.g., racial composition of generated CelebA

    images). Our work offers a new way to understand the generative dynamics of

    diffusion models that has the potential to bring about higher performance and

    less biased fast-samplers.'
  arxivId: '2305.19693'
  arxiv_tags:
  - cs.LG
  - cs.CV
  authors: Gabriel Raya, Luca Ambrogioni
  created_at: '2024-12-27T11:12:16.804672'
  issue_number: 24
  issue_url: https://github.com/dmarx/arxiv-archive/issues/24
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-15T09:58:39.593Z'
  main_tex_file: null
  published_date: '2023-05-31T09:36:34Z'
  state: open
  title: Spontaneous Symmetry Breaking in Generative Diffusion Models
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2305.19693
'2307.08691':
  abstract: 'Scaling Transformers to longer sequence lengths has been a major problem
    in

    the last several years, promising to improve performance in language modeling

    and high-resolution image understanding, as well as to unlock new applications

    in code, audio, and video generation. The attention layer is the main

    bottleneck in scaling to longer sequences, as its runtime and memory increase

    quadratically in the sequence length. FlashAttention exploits the asymmetric

    GPU memory hierarchy to bring significant memory saving (linear instead of

    quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines),

    with no approximation. However, FlashAttention is still not nearly as fast as

    optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the

    theoretical maximum FLOPs/s. We observe that the inefficiency is due to

    suboptimal work partitioning between different thread blocks and warps on the

    GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We

    propose FlashAttention-2, with better work partitioning to address these

    issues. In particular, we (1) tweak the algorithm to reduce the number of

    non-matmul FLOPs (2) parallelize the attention computation, even for a single

    head, across different thread blocks to increase occupancy, and (3) within each

    thread block, distribute the work between warps to reduce communication through

    shared memory. These yield around 2$\times$ speedup compared to FlashAttention,

    reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close

    to the efficiency of GEMM operations. We empirically validate that when used

    end-to-end to train GPT-style models, FlashAttention-2 reaches training speed

    of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).'
  arxivId: '2307.08691'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao
  created_at: '2024-12-28T06:18:03.976779'
  issue_number: 391
  issue_url: https://github.com/dmarx/arxiv-archive/issues/391
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T06:18:03.977600'
  last_visited: '2024-12-28T07:08:37.022Z'
  main_tex_file: null
  published_date: '2023-07-17T17:50:36Z'
  state: open
  title: "FlashAttention-2: Faster Attention with Better Parallelism and Work\n  Partitioning"
  total_reading_time_seconds: 10
  url: https://arxiv.org/abs/2307.08691
'2307.09288':
  abstract: 'In this work, we develop and release Llama 2, a collection of pretrained
    and

    fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70

    billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for

    dialogue use cases. Our models outperform open-source chat models on most

    benchmarks we tested, and based on our human evaluations for helpfulness and

    safety, may be a suitable substitute for closed-source models. We provide a

    detailed description of our approach to fine-tuning and safety improvements of

    Llama 2-Chat in order to enable the community to build on our work and

    contribute to the responsible development of LLMs.'
  arxivId: '2307.09288'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
  created_at: '2024-12-28T21:11:58.698759'
  issue_number: 434
  issue_url: https://github.com/dmarx/arxiv-archive/issues/434
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T21:11:58.699512'
  last_visited: '2024-12-28T20:46:35.699Z'
  main_tex_file: null
  published_date: '2023-07-18T14:31:57Z'
  state: open
  title: 'Llama 2: Open Foundation and Fine-Tuned Chat Models'
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2307.09288
'2308.10718':
  abstract: "Recent years have witnessed success in AIGC (AI Generated Content). People\n\
    can make use of a pre-trained diffusion model to generate images of high\nquality\
    \ or freely modify existing pictures with only prompts in nature\nlanguage. More\
    \ excitingly, the emerging personalization techniques make it\nfeasible to create\
    \ specific-desired images with only a few images as\nreferences. However, this\
    \ induces severe threats if such advanced techniques\nare misused by malicious\
    \ users, such as spreading fake news or defaming\nindividual reputations. Thus,\
    \ it is necessary to regulate personalization\nmodels (i.e., concept censorship)\
    \ for their development and advancement.\n  In this paper, we focus on the personalization\
    \ technique dubbed Textual\nInversion (TI), which is becoming prevailing for its\
    \ lightweight nature and\nexcellent performance. TI crafts the word embedding\
    \ that contains detailed\ninformation about a specific object. Users can easily\
    \ download the word\nembedding from public websites like Civitai and add it to\
    \ their own stable\ndiffusion model without fine-tuning for personalization. To\
    \ achieve the concept\ncensorship of a TI model, we propose leveraging the backdoor\
    \ technique for good\nby injecting backdoors into the Textual Inversion embeddings.\
    \ Briefly, we\nselect some sensitive words as triggers during the training of\
    \ TI, which will\nbe censored for normal use. In the subsequent generation stage,\
    \ if the triggers\nare combined with personalized embeddings as final prompts,\
    \ the model will\noutput a pre-defined target image rather than images including\
    \ the desired\nmalicious concept.\n  To demonstrate the effectiveness of our approach,\
    \ we conduct extensive\nexperiments on Stable Diffusion, a prevailing open-sourced\
    \ text-to-image model.\nOur code, data, and results are available at\nhttps://concept-censorship.github.io."
  arxivId: '2308.10718'
  arxiv_tags:
  - cs.CR
  - cs.CV
  authors: Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang
  created_at: '2024-12-27T10:14:04.628360'
  issue_number: 160
  issue_url: https://github.com/dmarx/arxiv-archive/issues/160
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:47:15.076Z'
  main_tex_file: null
  published_date: '2023-08-21T13:39:04Z'
  state: open
  title: Backdooring Textual Inversion for Concept Censorship
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2308.10718
'2309.03060':
  abstract: 'Many areas of machine learning and science involve large linear algebra

    problems, such as eigendecompositions, solving linear systems, computing matrix

    exponentials, and trace estimation. The matrices involved often have Kronecker,

    convolutional, block diagonal, sum, or product structure. In this paper, we

    propose a simple but general framework for large-scale linear algebra problems

    in machine learning, named CoLA (Compositional Linear Algebra). By combining a

    linear operator abstraction with compositional dispatch rules, CoLA

    automatically constructs memory and runtime efficient numerical algorithms.

    Moreover, CoLA provides memory efficient automatic differentiation, low

    precision computation, and GPU acceleration in both JAX and PyTorch, while also

    accommodating new objects, operations, and rules in downstream packages via

    multiple dispatch. CoLA can accelerate many algebraic operations, while making

    it easy to prototype matrix structures and algorithms, providing an appealing

    drop-in tool for virtually any computational effort that requires linear

    algebra. We showcase its efficacy across a broad range of applications,

    including partial differential equations, Gaussian processes, equivariant model

    construction, and unsupervised learning.'
  arxivId: '2309.03060'
  arxiv_tags:
  - cs.LG
  - cs.NA
  - math.NA
  - stat.ML
  authors: Andres Potapczynski, Marc Finzi, Geoff Pleiss, Andrew Gordon Wilson
  created_at: '2024-12-27T10:14:28.634237'
  issue_number: 132
  issue_url: https://github.com/dmarx/arxiv-archive/issues/132
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:34:16.085Z'
  main_tex_file: null
  published_date: '2023-09-06T14:59:38Z'
  state: open
  title: "CoLA: Exploiting Compositional Structure for Automatic and Efficient\n \
    \ Numerical Linear Algebra"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2309.03060
'2309.07965':
  abstract: 'In this work, we introduce the concept of relative Lipschitz saturation,

    along with its key categorical and algebraic properties, and demonstrate how

    such a structure always gives rise to a radicial algebra.'
  arxivId: '2309.07965'
  arxiv_tags:
  - math.AC
  - 13B22
  authors: Thiago da Silva, Guilherme Schultz Netto
  created_at: '2024-12-28T06:17:52.025240'
  issue_number: 398
  issue_url: https://github.com/dmarx/arxiv-archive/issues/398
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:11:55.168699'
  last_visited: '2024-12-28T07:12:00.313000+00:00'
  main_tex_file: null
  published_date: '2023-09-14T18:02:12Z'
  state: open
  title: "A survey on relative Lipschitz saturation of algebras and its relation\n\
    \  with radicial algebras"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2309.07965
'2309.12032':
  abstract: 'Structure learning is the crux of causal inference. Notably, causal discovery

    (CD) algorithms are brittle when data is scarce, possibly inferring imprecise

    causal relations that contradict expert knowledge -- especially when

    considering latent confounders. To aggravate the issue, most CD methods do not

    provide uncertainty estimates, making it hard for users to interpret results

    and improve the inference process. Surprisingly, while CD is a human-centered

    affair, no works have focused on building methods that both 1) output

    uncertainty estimates that can be verified by experts and 2) interact with

    those experts to iteratively refine CD. To solve these issues, we start by

    proposing to sample (causal) ancestral graphs proportionally to a belief

    distribution based on a score function, such as the Bayesian information

    criterion (BIC), using generative flow networks. Then, we leverage the

    diversity in candidate graphs and introduce an optimal experimental design to

    iteratively probe the expert about the relations among variables, effectively

    reducing the uncertainty of our belief over ancestral graphs. Finally, we

    update our samples to incorporate human feedback via importance sampling.

    Importantly, our method does not require causal sufficiency (i.e., unobserved

    confounders may exist). Experiments with synthetic observational data show that

    our method can accurately sample from distributions over ancestral graphs and

    that we can greatly improve inference quality with human aid.'
  arxivId: '2309.12032'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Tiago da Silva, Eliezer Silva, António Góis, Dominik Heider, Samuel Kaski,
    Diego Mesquita, Adèle Ribeiro
  created_at: '2024-12-28T06:18:06.793253'
  issue_number: 389
  issue_url: https://github.com/dmarx/arxiv-archive/issues/389
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:11:55.173121'
  last_visited: '2024-12-28T07:08:27.780000+00:00'
  main_tex_file: null
  published_date: '2023-09-21T12:53:45Z'
  state: open
  title: "Human-in-the-Loop Causal Discovery under Latent Confounding using\n  Ancestral\
    \ GFlowNets"
  total_reading_time_seconds: 16
  url: https://arxiv.org/abs/2309.12032
'2310.05736':
  abstract: 'Large language models (LLMs) have been applied in various applications
    due to

    their astonishing capabilities. With advancements in technologies such as

    chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed

    to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of

    tokens. To accelerate model inference and reduce cost, this paper presents

    LLMLingua, a coarse-to-fine prompt compression method that involves a budget

    controller to maintain semantic integrity under high compression ratios, a

    token-level iterative compression algorithm to better model the interdependence

    between compressed contents, and an instruction tuning based method for

    distribution alignment between language models. We conduct experiments and

    analysis over four datasets from different scenarios, i.e., GSM8K, BBH,

    ShareGPT, and Arxiv-March23; showing that the proposed approach yields

    state-of-the-art performance and allows for up to 20x compression with little

    performance loss. Our code is available at https://aka.ms/LLMLingua.'
  arxivId: '2310.05736'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu
  created_at: '2024-12-27T08:36:39.791647'
  issue_number: 299
  issue_url: https://github.com/dmarx/arxiv-archive/issues/299
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:39.794510'
  last_visited: '2024-12-26T22:53:26.354Z'
  main_tex_file: null
  published_date: '2023-10-09T14:10:21Z'
  state: open
  title: "LLMLingua: Compressing Prompts for Accelerated Inference of Large\n  Language\
    \ Models"
  total_reading_time_seconds: 38
  url: https://arxiv.org/abs/2310.05736
'2310.17157':
  abstract: 'Large language models (LLMs) with hundreds of billions of parameters
    have

    sparked a new wave of exciting AI applications. However, they are

    computationally expensive at inference time. Sparsity is a natural approach to

    reduce this cost, but existing methods either require costly retraining, have

    to forgo LLM''s in-context learning ability, or do not yield wall-clock time

    speedup on modern hardware. We hypothesize that contextual sparsity, which are

    small, input-dependent sets of attention heads and MLP parameters that yield

    approximately the same output as the dense model for a given input, can address

    these issues. We show that contextual sparsity exists, that it can be

    accurately predicted, and that we can exploit it to speed up LLM inference in

    wall-clock time without compromising LLM''s quality or in-context learning

    ability. Based on these insights, we propose DejaVu, a system that uses a

    low-cost algorithm to predict contextual sparsity on the fly given inputs to

    each layer, along with an asynchronous and hardware-aware implementation that

    speeds up LLM inference. We validate that DejaVu can reduce the inference

    latency of OPT-175B by over 2X compared to the state-of-the-art

    FasterTransformer, and over 6X compared to the widely used Hugging Face

    implementation, without compromising model quality. The code is available at

    https://github.com/FMInference/DejaVu.'
  arxivId: '2310.17157'
  arxiv_tags:
  - cs.LG
  authors: Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
    Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen
  created_at: '2024-12-28T06:18:09.790054'
  issue_number: 384
  issue_url: https://github.com/dmarx/arxiv-archive/issues/384
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:11:55.174870'
  last_visited: '2024-12-28T07:07:05.762Z'
  main_tex_file: null
  published_date: '2023-10-26T05:01:09Z'
  state: open
  title: 'Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time'
  total_reading_time_seconds: 55
  url: https://arxiv.org/abs/2310.17157
'2311.17910':
  abstract: 'Recent advances in neural rendering have improved both training and rendering

    times by orders of magnitude. While these methods demonstrate state-of-the-art

    quality and speed, they are designed for photogrammetry of static scenes and do

    not generalize well to freely moving humans in the environment. In this work,

    we introduce Human Gaussian Splats (HUGS) that represents an animatable human

    together with the scene using 3D Gaussian Splatting (3DGS). Our method takes

    only a monocular video with a small number of (50-100) frames, and it

    automatically learns to disentangle the static scene and a fully animatable

    human avatar within 30 minutes. We utilize the SMPL body model to initialize

    the human Gaussians. To capture details that are not modeled by SMPL (e.g.

    cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.

    Utilizing 3D Gaussians for animated humans brings new challenges, including the

    artifacts created when articulating the Gaussians. We propose to jointly

    optimize the linear blend skinning weights to coordinate the movements of

    individual Gaussians during animation. Our approach enables novel-pose

    synthesis of human and novel view synthesis of both the human and the scene. We

    achieve state-of-the-art rendering quality with a rendering speed of 60 FPS

    while being ~100x faster to train over previous work. Our code will be

    announced here: https://github.com/apple/ml-hugs'
  arxivId: '2311.17910'
  arxiv_tags:
  - cs.CV
  - cs.GR
  authors: Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag
    Ranjan
  created_at: '2024-12-27T11:12:19.251933'
  issue_number: 18
  issue_url: https://github.com/dmarx/arxiv-archive/issues/18
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-15T08:20:12.812Z'
  main_tex_file: null
  published_date: '2023-11-29T18:56:32Z'
  state: open
  title: 'HUGS: Human Gaussian Splats'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2311.17910
'2312.00752':
  abstract: 'Foundation models, now powering most of the exciting applications in
    deep

    learning, are almost universally based on the Transformer architecture and its

    core attention module. Many subquadratic-time architectures such as linear

    attention, gated convolution and recurrent models, and structured state space

    models (SSMs) have been developed to address Transformers'' computational

    inefficiency on long sequences, but they have not performed as well as

    attention on important modalities such as language. We identify that a key

    weakness of such models is their inability to perform content-based reasoning,

    and make several improvements. First, simply letting the SSM parameters be

    functions of the input addresses their weakness with discrete modalities,

    allowing the model to selectively propagate or forget information along the

    sequence length dimension depending on the current token. Second, even though

    this change prevents the use of efficient convolutions, we design a

    hardware-aware parallel algorithm in recurrent mode. We integrate these

    selective SSMs into a simplified end-to-end neural network architecture without

    attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$

    higher throughput than Transformers) and linear scaling in sequence length, and

    its performance improves on real data up to million-length sequences. As a

    general sequence model backbone, Mamba achieves state-of-the-art performance

    across several modalities such as language, audio, and genomics. On language

    modeling, our Mamba-3B model outperforms Transformers of the same size and

    matches Transformers twice its size, both in pretraining and downstream

    evaluation.'
  arxivId: '2312.00752'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Albert Gu, Tri Dao
  created_at: '2024-12-28T06:18:12.769623'
  issue_number: 383
  issue_url: https://github.com/dmarx/arxiv-archive/issues/383
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T06:55:41.314Z'
  main_tex_file: null
  published_date: '2023-12-01T18:01:34Z'
  state: open
  title: 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2312.00752
'2312.07731':
  abstract: 'Recent work proposed a new mechanism to remove protective perturbation
    added

    by Glaze in order to again enable mimicry of art styles from images protected

    by Glaze. Despite promising results shown in the original paper, our own tests

    with the authors'' code demonstrated several limitations of the proposed

    purification approach. The main limitations are 1) purification has a limited

    effect when tested on artists that are not well-known historical artists

    already embedded in original training data, 2) problems in evaluation metrics,

    and 3) collateral damage on mimicry result for clean images. We believe these

    limitations should be carefully considered in order to understand real world

    usability of the purification attack.'
  arxivId: '2312.07731'
  arxiv_tags:
  - cs.CR
  authors: Shawn Shan, Stanley Wu, Haitao Zheng, Ben Y. Zhao
  created_at: '2024-12-27T10:14:01.615089'
  issue_number: 161
  issue_url: https://github.com/dmarx/arxiv-archive/issues/161
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:49:35.138Z'
  main_tex_file: null
  published_date: '2023-12-12T20:52:27Z'
  state: open
  title: A Response to Glaze Purification via IMPRESS
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2312.07731
'2401.10774':
  abstract: "Large Language Models (LLMs) employ auto-regressive decoding that requires\n\
    sequential computation, with each step reliant on the previous one's output.\n\
    This creates a bottleneck as each step necessitates moving the full model\nparameters\
    \ from High-Bandwidth Memory (HBM) to the accelerator's cache. While\nmethods\
    \ such as speculative decoding have been suggested to address this issue,\ntheir\
    \ implementation is impeded by the challenges associated with acquiring and\n\
    maintaining a separate draft model. In this paper, we present Medusa, an\nefficient\
    \ method that augments LLM inference by adding extra decoding heads to\npredict\
    \ multiple subsequent tokens in parallel. Using a tree-based attention\nmechanism,\
    \ Medusa constructs multiple candidate continuations and verifies them\nsimultaneously\
    \ in each decoding step. By leveraging parallel processing, Medusa\nsubstantially\
    \ reduces the number of decoding steps required. We present two\nlevels of fine-tuning\
    \ procedures for Medusa to meet the needs of different use\ncases: Medusa-1: Medusa\
    \ is directly fine-tuned on top of a frozen backbone LLM,\nenabling lossless inference\
    \ acceleration. Medusa-2: Medusa is fine-tuned\ntogether with the backbone LLM,\
    \ enabling better prediction accuracy of Medusa\nheads and higher speedup but\
    \ needing a special training recipe that preserves\nthe backbone model's capabilities.\n\
    \  Moreover, we propose several extensions that improve or expand the utility\
    \ of\nMedusa, including a self-distillation to handle situations where no training\n\
    data is available and a typical acceptance scheme to boost the acceptance rate\n\
    while maintaining generation quality. We evaluate Medusa on models of various\n\
    sizes and training procedures. Our experiments demonstrate that Medusa-1 can\n\
    achieve over 2.2x speedup without compromising generation quality, while\nMedusa-2\
    \ further improves the speedup to 2.3-3.6x."
  arxivId: '2401.10774'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming
    Chen, Tri Dao
  created_at: '2024-12-28T06:17:54.804839'
  issue_number: 395
  issue_url: https://github.com/dmarx/arxiv-archive/issues/395
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:11:55.169411'
  last_visited: '2024-12-28T07:11:26.727Z'
  main_tex_file: null
  published_date: '2024-01-19T15:48:40Z'
  state: open
  title: "Medusa: Simple LLM Inference Acceleration Framework with Multiple\n  Decoding\
    \ Heads"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2401.10774
'2401.17671':
  abstract: 'Recent advancements in artificial intelligence have sparked interest
    in the

    parallels between large language models (LLMs) and human neural processing,

    particularly in language comprehension. While prior research has established

    similarities in the representation of LLMs and the brain, the underlying

    computational principles that cause this convergence, especially in the context

    of evolving LLMs, remain elusive. Here, we examined a diverse selection of

    high-performance LLMs with similar parameter sizes to investigate the factors

    contributing to their alignment with the brain''s language processing

    mechanisms. We find that as LLMs achieve higher performance on benchmark tasks,

    they not only become more brain-like as measured by higher performance when

    predicting neural responses from LLM embeddings, but also their hierarchical

    feature extraction pathways map more closely onto the brain''s while using fewer

    layers to do the same encoding. We also compare the feature extraction pathways

    of the LLMs to each other and identify new ways in which high-performing models

    have converged toward similar hierarchical processing mechanisms. Finally, we

    show the importance of contextual information in improving model performance

    and brain similarity. Our findings reveal the converging aspects of language

    processing in the brain and LLMs and offer new directions for developing models

    that align more closely with human cognitive processing.'
  arxivId: '2401.17671'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - q-bio.NC
  authors: Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, Nima
    Mesgarani
  created_at: '2024-12-27T11:11:56.309209'
  issue_number: 70
  issue_url: https://github.com/dmarx/arxiv-archive/issues/70
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-19T11:12:49.772Z'
  main_tex_file: null
  published_date: '2024-01-31T08:48:35Z'
  state: open
  title: "Contextual Feature Extraction Hierarchies Converge in Large Language\n \
    \ Models and the Brain"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2401.17671
'2402.10193':
  abstract: 'Large Language Models (LLMs) are typically trained in two phases:

    pre-training on large internet-scale datasets, and fine-tuning for downstream

    tasks. Given the higher computational demand of pre-training, it''s intuitive
    to

    assume that fine-tuning adds less new information to the model, and is thus

    more compressible. We explore this assumption by decomposing the weights of

    fine-tuned models into their pre-trained components and an additional delta. We

    introduce a simple method, BitDelta, which successfully quantizes this delta

    down to 1 bit without compromising performance. This interesting finding not

    only highlights the potential redundancy of information added during

    fine-tuning, but also has significant implications for the multi-tenant serving

    and multi-tenant storage of fine-tuned models. By enabling the use of a single

    high-precision base model accompanied by multiple 1-bit deltas, BitDelta

    dramatically reduces GPU memory requirements by more than 10x, which can also

    be translated to enhanced generation latency in multi-tenant settings. We

    validate BitDelta through experiments across Llama-2 and Mistral model

    families, and on models up to 70B parameters, showcasing minimal performance

    degradation over all tested settings.'
  arxivId: '2402.10193'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle
    Cai
  created_at: '2024-12-28T06:18:15.794618'
  issue_number: 379
  issue_url: https://github.com/dmarx/arxiv-archive/issues/379
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:11:55.178347'
  last_visited: '2024-12-28T06:52:07.941000+00:00'
  main_tex_file: null
  published_date: '2024-02-15T18:50:06Z'
  state: open
  title: 'BitDelta: Your Fine-Tune May Only Be Worth One Bit'
  total_reading_time_seconds: 73
  url: https://arxiv.org/abs/2402.10193
'2402.14903':
  abstract: 'Tokenization, the division of input text into input tokens, is an often

    overlooked aspect of the large language model (LLM) pipeline and could be the

    source of useful or harmful inductive biases. Historically, LLMs have relied on

    byte pair encoding, without care to specific input domains. With the increased

    use of LLMs for reasoning, various number-specific tokenization schemes have

    been adopted, with popular models like LLaMa and PaLM opting for single-digit

    tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and

    3-digit numbers. In this work, we study the effect this choice has on numerical

    reasoning through the use of arithmetic tasks. We consider left-to-right and

    right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left

    tokenization (enforced by comma separating numbers at inference time) leads to

    largely improved performance. Furthermore, we find that model errors when using

    standard left-to-right tokenization follow stereotyped error patterns,

    suggesting that model computations are systematic rather than approximate. We

    show that the model is able to convert between tokenizations easily, thus

    allowing chain-of-thought-inspired approaches to recover performance on

    left-to-right tokenized inputs. We also find the gap between tokenization

    directions decreases when models are scaled, possibly indicating that larger

    models are better able to override this tokenization-dependent inductive bias.

    In summary, our work performs the first study of how number tokenization

    choices lead to differences in model performance on arithmetic tasks,

    accompanied by a thorough analysis of error patterns. We hope this work

    inspires practitioners to more carefully ablate number tokenization-related

    choices when working towards general models of numerical reasoning.'
  arxivId: '2402.14903'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Aaditya K. Singh, DJ Strouse
  created_at: '2024-12-27T10:15:25.613996'
  issue_number: 78
  issue_url: https://github.com/dmarx/arxiv-archive/issues/78
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-19T22:43:04.367Z'
  main_tex_file: null
  published_date: '2024-02-22T18:14:09Z'
  state: open
  title: "Tokenization counts: the impact of tokenization on arithmetic in\n  frontier\
    \ LLMs"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2402.14903
'2402.16670':
  abstract: 'Over the last 70 years, we, humans, have created an economic market where

    attention is being captured and turned into money thanks to advertising. During

    the last two decades, leveraging research in psychology, sociology,

    neuroscience and other domains, Web platforms have brought the process of

    capturing attention to an unprecedented scale. With the initial commonplace

    goal of making targeted advertising more effective, the generalization of

    attention-capturing techniques and their use of cognitive biases and emotions

    have multiple detrimental side effects such as polarizing opinions, spreading

    false information and threatening public health, economies and democracies.

    This is clearly a case where the Web is not used for the common good and where,

    in fact, all its users become a vulnerable population. This paper brings

    together contributions from a wide range of disciplines to analyze current

    practices and consequences thereof. Through a set of propositions and

    principles that could be used do drive further works, it calls for actions

    against these practices competing to capture our attention on the Web, as it

    would be unsustainable for a civilization to allow attention to be wasted with

    impunity on a world-wide scale.'
  arxivId: '2402.16670'
  arxiv_tags:
  - cs.SI
  authors: Franck Michel, Fabien Gandon
  created_at: '2024-12-27T22:12:39.053178'
  issue_number: 330
  issue_url: https://github.com/dmarx/arxiv-archive/issues/330
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-27T22:07:23.174Z'
  main_tex_file: null
  published_date: '2024-02-26T15:46:43Z'
  state: open
  title: "Pay Attention: a Call to Regulate the Attention Market and Prevent\n  Algorithmic\
    \ Emotional Governance"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2402.16670
'2402.19173':
  abstract: 'The BigCode project, an open-scientific collaboration focused on the

    responsible development of Large Language Models for Code (Code LLMs),

    introduces StarCoder2. In partnership with Software Heritage (SWH), we build

    The Stack v2 on top of the digital commons of their source code archive.

    Alongside the SWH repositories spanning 619 programming languages, we carefully

    select other high-quality data sources, such as GitHub pull requests, Kaggle

    notebooks, and code documentation. This results in a training set that is 4x

    larger than the first StarCoder dataset. We train StarCoder2 models with 3B,

    7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate

    them on a comprehensive set of Code LLM benchmarks. We find that our small

    model, StarCoder2-3B, outperforms other Code LLMs of similar size on most

    benchmarks, and also outperforms StarCoderBase-15B. Our large model,

    StarCoder2- 15B, significantly outperforms other models of comparable size. In

    addition, it matches or outperforms CodeLlama-34B, a model more than twice its

    size. Although DeepSeekCoder- 33B is the best-performing model at code

    completion for high-resource languages, we find that StarCoder2-15B outperforms

    it on math and code reasoning benchmarks, as well as several low-resource

    languages. We make the model weights available under an OpenRAIL license and

    ensure full transparency regarding the training data by releasing the SoftWare

    Heritage persistent IDentifiers (SWHIDs) of the source code data.'
  arxivId: '2402.19173'
  arxiv_tags:
  - cs.SE
  - cs.AI
  authors: Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,
    Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu,
    Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu,
    Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li,
    Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu,
    Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun
    Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki,
    Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel
    Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten
    Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados,
    Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming
    Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
  created_at: '2024-12-28T06:18:18.974890'
  issue_number: 377
  issue_url: https://github.com/dmarx/arxiv-archive/issues/377
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:11:55.179294'
  last_visited: '2024-12-28T06:49:56.113000+00:00'
  main_tex_file: null
  published_date: '2024-02-29T13:53:35Z'
  state: open
  title: 'StarCoder 2 and The Stack v2: The Next Generation'
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2402.19173
'2403.00231':
  abstract: 'Large vision-language models (LVLMs) excel across diverse tasks involving

    concrete images from natural scenes. However, their ability to interpret

    abstract figures, such as geometry shapes and scientific plots, remains limited

    due to a scarcity of training datasets in scientific domains. To fill this gap,

    we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for

    enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset

    comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers

    spanning various scientific domains. Drawing from ArXivCap, we introduce

    ArXivQA, a question-answering dataset generated by prompting GPT-4V based on

    scientific figures. ArXivQA greatly enhances open-sourced LVLMs'' mathematical

    reasoning capabilities, achieving a 10.4\% absolute accuracy gain on a

    multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap,

    we devise four vision-to-text tasks for benchmarking LVLMs. Evaluation results

    with state-of-the-art LVLMs underscore their struggle with the nuanced

    semantics of academic figures, while domain-specific training yields

    substantial performance gains. Our error analysis uncovers misinterpretations

    of visual context, recognition errors, and the production of overly simplified

    captions by current LVLMs, shedding light on future improvements.'
  arxivId: '2403.00231'
  arxiv_tags:
  - cs.CV
  - cs.CL
  authors: Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong,
    Qi Liu
  created_at: '2024-12-27T08:36:34.072876'
  issue_number: 311
  issue_url: https://github.com/dmarx/arxiv-archive/issues/311
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:34.074812'
  last_visited: '2024-12-27T05:14:48.788Z'
  main_tex_file: null
  published_date: '2024-03-01T02:21:30Z'
  state: open
  title: "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of\n\
    \  Large Vision-Language Models"
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2403.00231
'2403.10304':
  abstract: 'We present a Wikidata-based framework, called KIF, for virtually integrating

    heterogeneous knowledge sources. KIF is written in Python and is released as

    open-source. It leverages Wikidata''s data model and vocabulary plus

    user-defined mappings to construct a unified view of the underlying sources

    while keeping track of the context and provenance of their statements. The

    underlying sources can be triplestores, relational databases, CSV files, etc.,

    which may or may not use the vocabulary and RDF encoding of Wikidata. The end

    result is a virtual knowledge base which behaves like an "extended Wikidata"

    and which can be queried using a simple but expressive pattern language,

    defined in terms of Wikidata''s data model. In this paper, we present the design

    and implementation of KIF, discuss how we have used it to solve a real

    integration problem in the domain of chemistry (involving Wikidata, PubChem,

    and IBM CIRCA), and present experimental results on the performance and

    overhead of KIF'
  arxivId: '2403.10304'
  arxiv_tags:
  - cs.AI
  - cs.DB
  authors: Guilherme Lima, João M. B. Rodrigues, Marcelo Machado, Elton Soares, Sandro
    R. Fiorini, Raphael Thiago, Leonardo G. Azevedo, Viviane T. da Silva, Renato Cerqueira
  created_at: '2024-12-28T06:18:21.801617'
  issue_number: 375
  issue_url: https://github.com/dmarx/arxiv-archive/issues/375
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T06:18:21.802429'
  last_visited: '2024-12-28T06:48:13.707Z'
  main_tex_file: null
  published_date: '2024-03-15T13:46:36Z'
  state: open
  title: "KIF: A Wikidata-Based Framework for Integrating Heterogeneous Knowledge\n\
    \  Sources"
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2403.10304
'2403.14781':
  abstract: 'In this study, we introduce a methodology for human image animation by

    leveraging a 3D human parametric model within a latent diffusion framework to

    enhance shape alignment and motion guidance in curernt human generative

    techniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear)

    model as the 3D human parametric model to establish a unified representation of

    body shape and pose. This facilitates the accurate capture of intricate human

    geometry and motion characteristics from source videos. Specifically, we

    incorporate rendered depth images, normal maps, and semantic maps obtained from

    SMPL sequences, alongside skeleton-based motion guidance, to enrich the

    conditions to the latent diffusion model with comprehensive 3D shape and

    detailed pose attributes. A multi-layer motion fusion module, integrating

    self-attention mechanisms, is employed to fuse the shape and motion latent

    representations in the spatial domain. By representing the 3D human parametric

    model as the motion guidance, we can perform parametric shape alignment of the

    human body between the reference image and the source video motion.

    Experimental evaluations conducted on benchmark datasets demonstrate the

    methodology''s superior ability to generate high-quality human animations that

    accurately capture both pose and shape variations. Furthermore, our approach

    also exhibits superior generalization capabilities on the proposed in-the-wild

    dataset. Project page: https://fudan-generative-vision.github.io/champ.'
  arxivId: '2403.14781'
  arxiv_tags:
  - cs.CV
  authors: Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Qingkun Su, Yinghui Xu, Xun
    Cao, Yao Yao, Hao Zhu, Siyu Zhu
  created_at: '2024-12-27T08:37:33.793502'
  issue_number: 197
  issue_url: https://github.com/dmarx/arxiv-archive/issues/197
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-23T21:24:01.786Z'
  main_tex_file: null
  published_date: '2024-03-21T18:52:58Z'
  state: open
  title: "Champ: Controllable and Consistent Human Image Animation with 3D\n  Parametric\
    \ Guidance"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2403.14781
'2405.04434':
  abstract: 'We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model

    characterized by economical training and efficient inference. It comprises 236B

    total parameters, of which 21B are activated for each token, and supports a

    context length of 128K tokens. DeepSeek-V2 adopts innovative architectures

    including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees

    efficient inference through significantly compressing the Key-Value (KV) cache

    into a latent vector, while DeepSeekMoE enables training strong models at an

    economical cost through sparse computation. Compared with DeepSeek 67B,

    DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves

    42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum

    generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality

    and multi-source corpus consisting of 8.1T tokens, and further perform

    Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock

    its potential. Evaluation results show that, even with only 21B activated

    parameters, DeepSeek-V2 and its chat versions still achieve top-tier

    performance among open-source models.'
  arxivId: '2405.04434'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang
    Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen,
    Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei
    Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo
    Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li,
    Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan,
    Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun
    Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan
    Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R.
    L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan
    Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang
    Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu
    Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao
    Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang,
    Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang
    Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan
    Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang,
    Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang,
    Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan
    Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou,
    Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren,
    Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao,
    Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin
    Li, Ziwei Xie
  created_at: '2024-12-27T08:36:36.875369'
  issue_number: 281
  issue_url: https://github.com/dmarx/arxiv-archive/issues/281
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:45.867060'
  last_visited: '2024-12-26T13:01:17.331Z'
  main_tex_file: null
  published_date: '2024-05-07T15:56:43Z'
  state: open
  title: "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts\n  Language\
    \ Model"
  total_reading_time_seconds: 46
  url: https://arxiv.org/abs/2405.04434
'2405.07987':
  abstract: 'We argue that representations in AI models, particularly deep networks,
    are

    converging. First, we survey many examples of convergence in the literature:

    over time and across multiple domains, the ways by which different neural

    networks represent data are becoming more aligned. Next, we demonstrate

    convergence across data modalities: as vision models and language models get

    larger, they measure distance between datapoints in a more and more alike way.

    We hypothesize that this convergence is driving toward a shared statistical

    model of reality, akin to Plato''s concept of an ideal reality. We term such a

    representation the platonic representation and discuss several possible

    selective pressures toward it. Finally, we discuss the implications of these

    trends, their limitations, and counterexamples to our analysis.'
  arxivId: '2405.07987'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  - cs.NE
  authors: Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola
  created_at: '2024-12-29T02:53:18.943422'
  issue_number: 439
  issue_url: https://github.com/dmarx/arxiv-archive/issues/439
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:39:54.598Z'
  main_tex_file: null
  published_date: '2024-05-13T17:58:30Z'
  state: open
  title: The Platonic Representation Hypothesis
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.07987
'2405.12399':
  abstract: 'World models constitute a promising approach for training reinforcement

    learning agents in a safe and sample-efficient manner. Recent world models

    predominantly operate on sequences of discrete latent variables to model

    environment dynamics. However, this compression into a compact discrete

    representation may ignore visual details that are important for reinforcement

    learning. Concurrently, diffusion models have become a dominant approach for

    image generation, challenging well-established methods modeling discrete

    latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As
    a

    Model Of eNvironment Dreams), a reinforcement learning agent trained in a

    diffusion world model. We analyze the key design choices that are required to

    make diffusion suitable for world modeling, and demonstrate how improved visual

    details can lead to improved agent performance. DIAMOND achieves a mean human

    normalized score of 1.46 on the competitive Atari 100k benchmark; a new best

    for agents trained entirely within a world model. We further demonstrate that

    DIAMOND''s diffusion world model can stand alone as an interactive neural game

    engine by training on static Counter-Strike: Global Offensive gameplay. To

    foster future research on diffusion for world modeling, we release our code,

    agents, videos and playable world models at https://diamond-wm.github.io.'
  arxivId: '2405.12399'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey,
    Tim Pearce, François Fleuret
  created_at: '2024-12-27T08:36:42.797996'
  issue_number: 296
  issue_url: https://github.com/dmarx/arxiv-archive/issues/296
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:42.798867'
  last_visited: '2024-12-26T22:06:50.951Z'
  main_tex_file: null
  published_date: '2024-05-20T22:51:05Z'
  state: open
  title: 'Diffusion for World Modeling: Visual Details Matter in Atari'
  total_reading_time_seconds: 12
  url: https://arxiv.org/abs/2405.12399
'2405.17399':
  abstract: "The poor performance of transformers on arithmetic tasks seems to stem\
    \ in\nlarge part from their inability to keep track of the exact position of each\n\
    digit inside of a large span of digits. We mend this problem by adding an\nembedding\
    \ to each digit that encodes its position relative to the start of the\nnumber.\
    \ In addition to the boost these embeddings provide on their own, we show\nthat\
    \ this fix enables architectural modifications such as input injection and\nrecurrent\
    \ layers to improve performance even further.\n  With positions resolved, we can\
    \ study the logical extrapolation ability of\ntransformers. Can they solve arithmetic\
    \ problems that are larger and more\ncomplex than those in their training data?\
    \ We find that training on only 20\ndigit numbers with a single GPU for one day,\
    \ we can reach state-of-the-art\nperformance, achieving up to 99% accuracy on\
    \ 100 digit addition problems.\nFinally, we show that these gains in numeracy\
    \ also unlock improvements on other\nmulti-step reasoning tasks including sorting\
    \ and multiplication."
  arxivId: '2405.17399'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian
    R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild,
    Tom Goldstein
  created_at: '2024-12-27T10:15:22.611101'
  issue_number: 79
  issue_url: https://github.com/dmarx/arxiv-archive/issues/79
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-19T22:43:44.883Z'
  main_tex_file: null
  published_date: '2024-05-27T17:49:18Z'
  state: open
  title: Transformers Can Do Arithmetic with the Right Embeddings
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.17399
'2405.17472':
  abstract: 'Text-to-image diffusion models can be fine-tuned in custom domains to
    adapt

    to specific user preferences, but such adaptability has also been utilized for

    illegal purposes, such as forging public figures'' portraits, duplicating

    copyrighted artworks and generating explicit contents. Existing work focused on

    detecting the illegally generated contents, but cannot prevent or mitigate

    illegal adaptations of diffusion models. Other schemes of model unlearning and

    reinitialization, similarly, cannot prevent users from relearning the knowledge

    of illegal model adaptation with custom data. In this paper, we present

    FreezeAsGuard, a new technique that addresses these limitations and enables

    irreversible mitigation of illegal adaptations of diffusion models. Our

    approach is that the model publisher selectively freezes tensors in pre-trained

    diffusion models that are critical to illegal model adaptations, to mitigate

    the fine-tuned model''s representation power in illegal adaptations, but

    minimize the impact on other legal adaptations. Experiment results in multiple

    text-to-image application domains show that FreezeAsGuard provides 37% stronger

    power in mitigating illegal model adaptations compared to competitive

    baselines, while incurring less than 5% impact on legal model adaptations. The

    source code is available at: https://github.com/pittisl/FreezeAsGuard.'
  arxivId: '2405.17472'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CR
  - cs.CV
  authors: Kai Huang, Haoming Wang, Wei Gao
  created_at: '2024-12-27T10:13:52.622762'
  issue_number: 167
  issue_url: https://github.com/dmarx/arxiv-archive/issues/167
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-22T18:25:59.099Z'
  main_tex_file: null
  published_date: '2024-05-24T03:23:51Z'
  state: open
  title: "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via\n \
    \ Selective Tensor Freezing"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.17472
'2405.21060':
  abstract: 'While Transformers have been the main architecture behind deep learning''s

    success in language modeling, state-space models (SSMs) such as Mamba have

    recently been shown to match or outperform Transformers at small to medium

    scale. We show that these families of models are actually quite closely

    related, and develop a rich framework of theoretical connections between SSMs

    and variants of attention, connected through various decompositions of a

    well-studied class of structured semiseparable matrices. Our state space

    duality (SSD) framework allows us to design a new architecture (Mamba-2) whose

    core layer is an a refinement of Mamba''s selective SSM that is 2-8X faster,

    while continuing to be competitive with Transformers on language modeling.'
  arxivId: '2405.21060'
  arxiv_tags:
  - cs.LG
  authors: Tri Dao, Albert Gu
  created_at: '2024-12-28T06:18:24.773320'
  issue_number: 374
  issue_url: https://github.com/dmarx/arxiv-archive/issues/374
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:11:55.180470'
  last_visited: '2024-12-28T06:47:09.217000+00:00'
  main_tex_file: null
  published_date: '2024-05-31T17:50:01Z'
  state: open
  title: "Transformers are SSMs: Generalized Models and Efficient Algorithms\n  Through\
    \ Structured State Space Duality"
  total_reading_time_seconds: 7
  url: https://arxiv.org/abs/2405.21060
'2406.01506':
  abstract: 'The linear representation hypothesis is the informal idea that semantic

    concepts are encoded as linear directions in the representation spaces of large

    language models (LLMs). Previous work has shown how to make this notion precise

    for representing binary concepts that have natural contrasts (e.g., {male,

    female}) as directions in representation space. However, many natural concepts

    do not have natural contrasts (e.g., whether the output is about an animal). In

    this work, we show how to extend the formalization of the linear representation

    hypothesis to represent features (e.g., is_animal) as vectors. This allows us

    to immediately formalize the representation of categorical concepts as

    polytopes in the representation space. Further, we use the formalization to

    prove a relationship between the hierarchical structure of concepts and the

    geometry of their representations. We validate these theoretical results on the

    Gemma and LLaMA-3 large language models, estimating representations for 900+

    hierarchically related concepts using data from WordNet.'
  arxivId: '2406.01506'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  - stat.ML
  authors: Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch
  created_at: '2024-12-29T02:53:22.028186'
  issue_number: 438
  issue_url: https://github.com/dmarx/arxiv-archive/issues/438
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:39:53.364Z'
  main_tex_file: null
  published_date: '2024-06-03T16:34:01Z'
  state: open
  title: "The Geometry of Categorical and Hierarchical Concepts in Large Language\n\
    \  Models"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.01506
'2406.06158':
  abstract: 'While the impressive performance of modern neural networks is often

    attributed to their capacity to efficiently extract task-relevant features from

    data, the mechanisms underlying this rich feature learning regime remain

    elusive, with much of our theoretical understanding stemming from the opposing

    lazy regime. In this work, we derive exact solutions to a minimal model that

    transitions between lazy and rich learning, precisely elucidating how

    unbalanced layer-specific initialization variances and learning rates determine

    the degree of feature learning. Our analysis reveals that they conspire to

    influence the learning regime through a set of conserved quantities that

    constrain and modify the geometry of learning trajectories in parameter and

    function space. We extend our analysis to more complex linear models with

    multiple neurons, outputs, and layers and to shallow nonlinear networks with

    piecewise linear activation functions. In linear networks, rapid feature

    learning only occurs from balanced initializations, where all layers learn at

    similar speeds. While in nonlinear networks, unbalanced initializations that

    promote faster learning in earlier layers can accelerate rich learning. Through

    a series of experiments, we provide evidence that this unbalanced rich regime

    drives feature learning in deep finite-width networks, promotes

    interpretability of early layers in CNNs, reduces the sample complexity of

    learning hierarchical data, and decreases the time to grokking in modular

    arithmetic. Our theory motivates further exploration of unbalanced

    initializations to enhance efficient feature learning.'
  arxivId: '2406.06158'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - stat.ML
  authors: Daniel Kunin, Allan Raventós, Clémentine Dominé, Feng Chen, David Klindt,
    Andrew Saxe, Surya Ganguli
  created_at: '2024-12-27T10:14:52.610863'
  issue_number: 120
  issue_url: https://github.com/dmarx/arxiv-archive/issues/120
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:10:41.441Z'
  main_tex_file: null
  published_date: '2024-06-10T10:42:37Z'
  state: open
  title: "Get rich quick: exact solutions reveal how unbalanced initializations\n\
    \  promote rapid feature learning"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.06158
'2406.06248':
  abstract: 'Dense linear layers are the dominant computational bottleneck in foundation

    models. Identifying more efficient alternatives to dense matrices has enormous

    potential for building more compute-efficient models, as exemplified by the

    success of convolutional networks in the image domain. In this work, we

    systematically explore structured matrices as replacements for dense matrices.

    We show that different structures often require drastically different

    initialization scales and learning rates, which are crucial to performance,

    especially as models scale. Using insights from the Maximal Update

    Parameterization, we determine the optimal scaling for initialization and

    learning rates of these unconventional layers. Finally, we measure the scaling

    laws of different structures to compare how quickly their performance improves

    with compute. We propose a novel matrix family containing Monarch matrices, the

    Block Tensor-Train (BTT), which we show performs better than dense matrices for

    the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT

    achieves exponentially lower training loss than dense when training MLPs and

    ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less

    compute and is more efficient than dense for training small GPT-2 language

    models.'
  arxivId: '2406.06248'
  arxiv_tags:
  - cs.LG
  authors: Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, Andrew Gordon
    Wilson
  created_at: '2024-12-27T10:14:34.797218'
  issue_number: 130
  issue_url: https://github.com/dmarx/arxiv-archive/issues/130
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:34:10.041Z'
  main_tex_file: null
  published_date: '2024-06-10T13:25:43Z'
  state: open
  title: 'Compute Better Spent: Replacing Dense Layers with Structured Matrices'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.06248
'2406.09162':
  abstract: 'Recent advancements in image generation have enabled the creation of

    high-quality images from text conditions. However, when facing multi-modal

    conditions, such as text combined with reference appearances, existing methods

    struggle to balance multiple conditions effectively, typically showing a

    preference for one modality over others. To address this challenge, we

    introduce EMMA, a novel image generation model accepting multi-modal prompts

    built upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA

    seamlessly incorporates additional modalities alongside text to guide image

    generation through an innovative Multi-modal Feature Connector design, which

    effectively integrates textual and supplementary modal information using a

    special attention mechanism. By freezing all parameters in the original T2I

    diffusion model and only adjusting some additional layers, we reveal an

    interesting finding that the pre-trained T2I diffusion model can secretly

    accept multi-modal prompts. This interesting property facilitates easy

    adaptation to different existing frameworks, making EMMA a flexible and

    effective tool for producing personalized and context-aware images and even

    videos. Additionally, we introduce a strategy to assemble learned EMMA modules

    to produce images conditioned on multiple modalities simultaneously,

    eliminating the need for additional training with mixed multi-modal prompts.

    Extensive experiments demonstrate the effectiveness of EMMA in maintaining high

    fidelity and detail in generated images, showcasing its potential as a robust

    solution for advanced multi-modal conditional image generation tasks.'
  arxivId: '2406.09162'
  arxiv_tags:
  - cs.CV
  authors: Yucheng Han, Rui Wang, Chi Zhang, Juntao Hu, Pei Cheng, Bin Fu, Hanwang
    Zhang
  created_at: '2024-12-28T09:13:03.099485'
  issue_number: 410
  issue_url: https://github.com/dmarx/arxiv-archive/issues/410
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T09:13:03.817654'
  last_visited: '2024-12-28T08:40:07.461Z'
  main_tex_file: null
  published_date: '2024-06-13T14:26:43Z'
  state: open
  title: "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal\n\
    \  Prompts"
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2406.09162
'2406.12027':
  abstract: 'Artists are increasingly concerned about advancements in image generation

    models that can closely replicate their unique artistic styles. In response,

    several protection tools against style mimicry have been developed that

    incorporate small adversarial perturbations into artworks published online. In

    this work, we evaluate the effectiveness of popular protections -- with

    millions of downloads -- and show they only provide a false sense of security.

    We find that low-effort and "off-the-shelf" techniques, such as image

    upscaling, are sufficient to create robust mimicry methods that significantly

    degrade existing protections. Through a user study, we demonstrate that all

    existing protections can be easily bypassed, leaving artists vulnerable to

    style mimicry. We caution that tools based on adversarial perturbations cannot

    reliably protect artists from the misuse of generative AI, and urge the

    development of alternative non-technological solutions.'
  arxivId: '2406.12027'
  arxiv_tags:
  - cs.CR
  authors: Robert Hönig, Javier Rando, Nicholas Carlini, Florian Tramèr
  created_at: '2024-12-27T10:14:16.599993'
  issue_number: 154
  issue_url: https://github.com/dmarx/arxiv-archive/issues/154
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:38:53.342Z'
  main_tex_file: null
  published_date: '2024-06-17T18:51:45Z'
  state: open
  title: "Adversarial Perturbations Cannot Reliably Protect Artists From\n  Generative\
    \ AI"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.12027
'2406.19354':
  abstract: 'The model editing problem concerns how language models should learn new
    facts

    about the world over time. While empirical research on model editing has drawn

    widespread attention, the conceptual foundations of model editing remain shaky

    -- perhaps unsurprisingly, since model editing is essentially belief revision,

    a storied problem in philosophy that has eluded succinct solutions for decades.

    Model editing nonetheless demands a solution, since we need to be able to

    control the knowledge within language models. With this goal in mind, this

    paper critiques the standard formulation of the model editing problem and

    proposes a formal testbed for model editing research. We first describe 12 open

    problems with model editing, based on challenges with (1) defining the problem,

    (2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the

    first place. Many of these challenges are extremely difficult to address, e.g.

    determining far-reaching consequences of edits, labeling probabilistic

    entailments between facts, and updating beliefs of agent simulators. Next, we

    introduce a semi-synthetic dataset for model editing based on Wikidata, where

    we can evaluate edits against labels given by an idealized Bayesian agent. This

    enables us to say exactly how belief revision in language models falls short of

    a desirable epistemic standard. We encourage further research exploring

    settings where such a gold standard can be compared against. Our code is

    publicly available at: https://github.com/peterbhase/LLM-belief-revision'
  arxivId: '2406.19354'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal
  created_at: '2024-12-27T10:14:37.794517'
  issue_number: 129
  issue_url: https://github.com/dmarx/arxiv-archive/issues/129
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:28:19.231Z'
  main_tex_file: null
  published_date: '2024-06-27T17:33:03Z'
  state: open
  title: "Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision\
    \ Work in LLMs?"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.19354
'2407.01392':
  abstract: 'This paper presents Diffusion Forcing, a new training paradigm where
    a

    diffusion model is trained to denoise a set of tokens with independent

    per-token noise levels. We apply Diffusion Forcing to sequence generative

    modeling by training a causal next-token prediction model to generate one or

    several future tokens without fully diffusing past ones. Our approach is shown

    to combine the strengths of next-token prediction models, such as

    variable-length generation, with the strengths of full-sequence diffusion

    models, such as the ability to guide sampling to desirable trajectories. Our

    method offers a range of additional capabilities, such as (1) rolling-out

    sequences of continuous tokens, such as video, with lengths past the training

    horizon, where baselines diverge and (2) new sampling and guiding schemes that

    uniquely profit from Diffusion Forcing''s variable-horizon and causal

    architecture, and which lead to marked performance gains in decision-making and

    planning tasks. In addition to its empirical success, our method is proven to

    optimize a variational lower bound on the likelihoods of all subsequences of

    tokens drawn from the true joint distribution. Project website:

    https://boyuan.space/diffusion-forcing'
  arxivId: '2407.01392'
  arxiv_tags:
  - cs.LG
  - cs.CV
  - cs.RO
  authors: Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake,
    Vincent Sitzmann
  created_at: '2024-12-27T08:36:30.880723'
  issue_number: 295
  issue_url: https://github.com/dmarx/arxiv-archive/issues/295
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:30.883756'
  last_visited: '2024-12-26T22:05:31.374Z'
  main_tex_file: null
  published_date: '2024-07-01T15:43:25Z'
  state: open
  title: 'Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion'
  total_reading_time_seconds: 34
  url: https://arxiv.org/abs/2407.01392
'2407.05872':
  abstract: 'Robust and effective scaling of models from small to large width typically

    requires the precise adjustment of many algorithmic and architectural details,

    such as parameterization and optimizer choices. In this work, we propose a new

    perspective on parameterization by investigating a key assumption in prior work

    about the alignment between parameters and data and derive new theoretical

    results under weaker assumptions and a broader set of optimizers. Our extensive

    empirical investigation includes tens of thousands of models trained with all

    combinations of three optimizers, four parameterizations, several alignment

    assumptions, more than a dozen learning rates, and fourteen model sizes up to

    26.8B parameters. We find that the best learning rate scaling prescription

    would often have been excluded by the assumptions in prior work. Our results

    show that all parameterizations, not just maximal update parameterization

    (muP), can achieve hyperparameter transfer; moreover, our novel per-layer

    learning rate prescription for standard parameterization outperforms muP.

    Finally, we demonstrate that an overlooked aspect of parameterization, the

    epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow

    and propose Adam-atan2, a new numerically stable, scale-invariant version of

    Adam that eliminates the epsilon hyperparameter entirely.'
  arxivId: '2407.05872'
  arxiv_tags:
  - cs.LG
  authors: Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman
    Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling,
    Jaehoon Lee, Jeffrey Pennington
  created_at: '2024-12-27T08:36:51.877134'
  issue_number: 255
  issue_url: https://github.com/dmarx/arxiv-archive/issues/255
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:51.877977'
  last_visited: '2024-12-24T21:09:44.860Z'
  main_tex_file: null
  published_date: '2024-07-08T12:32:51Z'
  state: open
  title: Scaling Exponents Across Parameterizations and Optimizers
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2407.05872
'2407.08608':
  abstract: 'Attention, as a core layer of the ubiquitous Transformer architecture,
    is the

    bottleneck for large language models and long-context applications.

    FlashAttention elaborated an approach to speed up attention on GPUs through

    minimizing memory reads/writes. However, it has yet to take advantage of new

    capabilities present in recent hardware, with FlashAttention-2 achieving only

    35% utilization on the H100 GPU. We develop three main techniques to speed up

    attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to

    (1) overlap overall computation and data movement via warp-specialization and

    (2) interleave block-wise matmul and softmax operations, and (3) block

    quantization and incoherent processing that leverages hardware support for FP8

    low-precision. We demonstrate that our method, FlashAttention-3, achieves

    speedup on H100 GPUs by 1.5-2.0$\times$ with FP16 reaching up to 740 TFLOPs/s

    (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate

    that FP8 FlashAttention-3 achieves 2.6$\times$ lower numerical error than a

    baseline FP8 attention.'
  arxivId: '2407.08608'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani,
    Tri Dao
  created_at: '2024-12-28T06:18:27.911592'
  issue_number: 408
  issue_url: https://github.com/dmarx/arxiv-archive/issues/408
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T07:35:33.618208'
  last_visited: '2024-12-28T07:33:54.849Z'
  main_tex_file: null
  published_date: '2024-07-11T15:44:48Z'
  state: open
  title: "FlashAttention-3: Fast and Accurate Attention with Asynchrony and\n  Low-precision"
  total_reading_time_seconds: 42
  url: https://arxiv.org/abs/2407.08608
'2407.17465':
  abstract: 'The Maximal Update Parametrization ($\mu$P) aims to make the optimal

    hyperparameters (HPs) of a model independent of its size, allowing them to be

    swept using a cheap proxy model rather than the full-size target model. We

    present a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with

    Unit Scaling, a method for designing models that makes them easy to train in

    low-precision. The two techniques have a natural affinity: $\mu$P ensures that

    the scale of activations is independent of model size, and Unit Scaling ensures

    that activations, weights and gradients begin training with a scale of one.

    This synthesis opens the door to a simpler scheme, whose default values are

    near-optimal. This in turn facilitates a more efficient sweeping strategy, with

    u-$\mu$P models reaching a loss that is equal to or lower than comparable

    $\mu$P models and working out-of-the-box in FP8.'
  arxivId: '2407.17465'
  arxiv_tags:
  - cs.LG
  authors: Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y.
    Prince, Björn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach,
    Douglas Orr
  created_at: '2024-12-27T10:15:28.617370'
  issue_number: 72
  issue_url: https://github.com/dmarx/arxiv-archive/issues/72
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-19T18:35:47.488Z'
  main_tex_file: null
  published_date: '2024-07-24T17:58:42Z'
  state: open
  title: 'u-$μ$P: The Unit-Scaled Maximal Update Parametrization'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2407.17465
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is

    a critical step towards building generally self-improving agents that can

    operate on open-ended natural language. In this paper, we study the scaling of

    inference-time computation in LLMs, with a focus on answering the question: if

    an LLM is allowed to use a fixed but non-trivial amount of inference-time

    compute, how much can it improve its performance on a challenging prompt?

    Answering this question has implications not only on the achievable performance

    of LLMs, but also on the future of LLM pretraining and how one should tradeoff

    inference-time and pre-training compute. Despite its importance, little

    research attempted to understand the scaling behaviors of various test-time

    inference methods. Moreover, current work largely provides negative results for

    a number of these strategies. In this work, we analyze two primary mechanisms

    to scale test-time computation: (1) searching against dense, process-based

    verifier reward models; and (2) updating the model''s distribution over a

    response adaptively, given the prompt at test time. We find that in both cases,

    the effectiveness of different approaches to scaling test-time compute

    critically varies depending on the difficulty of the prompt. This observation

    motivates applying a "compute-optimal" scaling strategy, which acts to most

    effectively allocate test-time compute adaptively per prompt. Using this

    compute-optimal strategy, we can improve the efficiency of test-time compute

    scaling by more than 4x compared to a best-of-N baseline. Additionally, in a

    FLOPs-matched evaluation, we find that on problems where a smaller base model

    attains somewhat non-trivial success rates, test-time compute can be used to

    outperform a 14x larger model.'
  arxivId: '2408.03314'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-27T11:12:04.964632'
  issue_number: 57
  issue_url: https://github.com/dmarx/arxiv-archive/issues/57
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-17T13:48:02.489Z'
  main_tex_file: null
  published_date: '2024-08-06T17:35:05Z'
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2408.03314
'2408.11810':
  abstract: 'Diffusion Models have emerged as powerful generative models for high-quality

    image synthesis, with many subsequent image editing techniques based on them.

    However, the ease of text-based image editing introduces significant risks,

    such as malicious editing for scams or intellectual property infringement.

    Previous works have attempted to safeguard images from diffusion-based editing

    by adding imperceptible perturbations. These methods are costly and

    specifically target prevalent Latent Diffusion Models (LDMs), while

    Pixel-domain Diffusion Models (PDMs) remain largely unexplored and robust

    against such attacks. Our work addresses this gap by proposing a novel

    attacking framework with a feature representation attack loss that exploits

    vulnerabilities in denoising UNets and a latent optimization strategy to

    enhance the naturalness of protected images. Extensive experiments demonstrate

    the effectiveness of our approach in attacking dominant PDM-based editing

    methods (e.g., SDEdit) while maintaining reasonable protection fidelity and

    robustness against common defense methods. Additionally, our framework is

    extensible to LDMs, achieving comparable performance to existing approaches.'
  arxivId: '2408.11810'
  arxiv_tags:
  - cs.CV
  authors: Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao, Ernie Chu, Cheng-Fu Chou, Jun-Cheng
    Chen
  created_at: '2024-12-27T10:14:07.612298'
  issue_number: 158
  issue_url: https://github.com/dmarx/arxiv-archive/issues/158
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T16:45:08.960Z'
  main_tex_file: null
  published_date: '2024-08-21T17:56:34Z'
  state: open
  title: "Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain\n \
    \ Diffusion Models"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2408.11810
'2408.14837':
  abstract: 'We present GameNGen, the first game engine powered entirely by a neural
    model

    that enables real-time interaction with a complex environment over long

    trajectories at high quality. GameNGen can interactively simulate the classic

    game DOOM at over 20 frames per second on a single TPU. Next frame prediction

    achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are

    only slightly better than random chance at distinguishing short clips of the

    game from clips of the simulation. GameNGen is trained in two phases: (1) an

    RL-agent learns to play the game and the training sessions are recorded, and

    (2) a diffusion model is trained to produce the next frame, conditioned on the

    sequence of past frames and actions. Conditioning augmentations enable stable

    auto-regressive generation over long trajectories.'
  arxivId: '2408.14837'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter
  created_at: '2024-12-27T08:36:28.173575'
  issue_number: 290
  issue_url: https://github.com/dmarx/arxiv-archive/issues/290
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:42.801601'
  last_visited: '2024-12-26T21:43:51.081Z'
  main_tex_file: null
  published_date: '2024-08-27T07:46:07Z'
  state: open
  title: Diffusion Models Are Real-Time Game Engines
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/2408.14837
'2409.09347':
  abstract: 'Mass transport problems arise in many areas of machine learning whereby
    one

    wants to compute a map transporting one distribution to another. Generative

    modeling techniques like Generative Adversarial Networks (GANs) and Denoising

    Diffusion Models (DDMs) have been successfully adapted to solve such transport

    problems, resulting in CycleGAN and Bridge Matching respectively. However,

    these methods do not approximate Optimal Transport (OT) maps, which are known

    to have desirable properties. Existing techniques approximating OT maps for

    high-dimensional data-rich problems, such as DDM-based Rectified Flow and

    Schr\"odinger Bridge procedures, require fully training a DDM-type model at

    each iteration, or use mini-batch techniques which can introduce significant

    errors. We propose a novel algorithm to compute the Schr\"odinger Bridge, a

    dynamic entropy-regularised version of OT, that eliminates the need to train

    multiple DDM-like models. This algorithm corresponds to a discretisation of a

    flow of path measures, which we call the Schr\"odinger Bridge Flow, whose only

    stationary point is the Schr\"odinger Bridge. We demonstrate the performance of

    our algorithm on a variety of unpaired data translation tasks.'
  arxivId: '2409.09347'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Valentin De Bortoli, Iryna Korshunova, Andriy Mnih, Arnaud Doucet
  created_at: '2024-12-27T10:14:25.836239'
  issue_number: 136
  issue_url: https://github.com/dmarx/arxiv-archive/issues/136
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T08:05:51.003Z'
  main_tex_file: null
  published_date: '2024-09-14T07:34:30Z'
  state: open
  title: Schrödinger Bridge Flow for Unpaired Data Translation
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.09347
'2410.01131':
  abstract: 'We propose a novel neural network architecture, the normalized Transformer

    (nGPT) with representation learning on the hypersphere. In nGPT, all vectors

    forming the embeddings, MLP, attention matrices and hidden states are unit norm

    normalized. The input stream of tokens travels on the surface of a hypersphere,

    with each layer contributing a displacement towards the target output

    predictions. These displacements are defined by the MLP and attention blocks,

    whose vector components also reside on the same hypersphere. Experiments show

    that nGPT learns much faster, reducing the number of training steps required to

    achieve the same accuracy by a factor of 4 to 20, depending on the sequence

    length.'
  arxivId: '2410.01131'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg
  created_at: '2024-12-27T08:36:48.854357'
  issue_number: 270
  issue_url: https://github.com/dmarx/arxiv-archive/issues/270
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:36:48.856096'
  last_visited: '2024-12-25T20:54:14.596Z'
  main_tex_file: null
  published_date: '2024-10-01T23:50:09Z'
  state: open
  title: "nGPT: Normalized Transformer with Representation Learning on the\n  Hypersphere"
  total_reading_time_seconds: 13
  url: https://arxiv.org/abs/2410.01131
'2410.02117':
  abstract: 'Dense linear layers are the dominant computational bottleneck in large
    neural

    networks, presenting a critical need for more efficient alternatives. Previous

    efforts focused on a small number of hand-crafted structured matrices and

    neglected to investigate whether these structures can surpass dense layers in

    terms of compute-optimal scaling laws when both the model size and training

    examples are optimally allocated. In this work, we present a unifying framework

    that enables searching among all linear operators expressible via an Einstein

    summation. This framework encompasses many previously proposed structures, such

    as low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch,

    along with many novel structures. To analyze the framework, we develop a

    taxonomy of all such operators based on their computational and algebraic

    properties and show that differences in the compute-optimal scaling laws are

    mostly governed by a small number of variables that we introduce. Namely, a

    small $\omega$ (which measures parameter sharing) and large $\psi$ (which

    measures the rank) reliably led to better scaling laws. Guided by the insight

    that full-rank structures that maximize parameters per unit of compute perform

    the best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture

    obtained by sparsifying computation in the BTT structure. In contrast to the

    standard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE

    in every single linear layer of the model, including the projection matrices in

    the attention blocks. We find BTT-MoE provides a substantial compute-efficiency

    gain over dense layers and standard MoE.'
  arxivId: '2410.02117'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen,
    Micah Goldblum, Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson
  created_at: '2024-12-27T10:14:31.803411'
  issue_number: 131
  issue_url: https://github.com/dmarx/arxiv-archive/issues/131
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:34:12.263Z'
  main_tex_file: null
  published_date: '2024-10-03T00:44:50Z'
  state: open
  title: "Searching for Efficient Linear Layers over a Continuous Space of\n  Structured\
    \ Matrices"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.02117
'2410.02423':
  abstract: 'In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm

    for solving imaging inverse problems. PnP methods leverage the strength of

    pre-trained denoisers, often deep neural networks, by integrating them in

    optimization schemes. While they achieve state-of-the-art performance on

    various inverse problems in imaging, PnP approaches face inherent limitations

    on more generative tasks like inpainting. On the other hand, generative models

    such as Flow Matching pushed the boundary in image sampling yet lack a clear

    method for efficient use in image restoration. We propose to combine the PnP

    framework with Flow Matching (FM) by defining a time-dependent denoiser using
    a

    pre-trained FM model. Our algorithm alternates between gradient descent steps

    on the data-fidelity term, reprojections onto the learned FM path, and

    denoising. Notably, our method is computationally efficient and

    memory-friendly, as it avoids backpropagation through ODEs and trace

    computations. We evaluate its performance on denoising, super-resolution,

    deblurring, and inpainting tasks, demonstrating superior results compared to

    existing PnP algorithms and Flow Matching based state-of-the-art methods.'
  arxivId: '2410.02423'
  arxiv_tags:
  - cs.CV
  - cs.LG
  authors: Ségolène Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl
  created_at: '2024-12-27T10:14:58.607396'
  issue_number: 115
  issue_url: https://github.com/dmarx/arxiv-archive/issues/115
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T06:43:50.011Z'
  main_tex_file: null
  published_date: '2024-10-03T12:13:56Z'
  state: open
  title: 'PnP-Flow: Plug-and-Play Image Restoration with Flow Matching'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.02423
'2410.10733':
  abstract: 'We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder

    models for accelerating high-resolution diffusion models. Existing autoencoder

    models have demonstrated impressive results at a moderate spatial compression

    ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for

    high spatial compression ratios (e.g., 64x). We address this challenge by

    introducing two key techniques: (1) Residual Autoencoding, where we design our

    models to learn residuals based on the space-to-channel transformed features to

    alleviate the optimization difficulty of high spatial-compression autoencoders;

    (2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases

    training strategy for mitigating the generalization penalty of high

    spatial-compression autoencoders. With these designs, we improve the

    autoencoder''s spatial compression ratio up to 128 while maintaining the

    reconstruction quality. Applying our DC-AE to latent diffusion models, we

    achieve significant speedup without accuracy drop. For example, on ImageNet

    512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup

    on H100 GPU for UViT-H while achieving a better FID, compared with the widely

    used SD-VAE-f8 autoencoder. Our code is available at

    https://github.com/mit-han-lab/efficientvit.'
  arxivId: '2410.10733'
  arxiv_tags:
  - cs.CV
  - cs.AI
  authors: Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang,
    Muyang Li, Yao Lu, Song Han
  created_at: '2024-12-28T18:17:10.747868'
  issue_number: 424
  issue_url: https://github.com/dmarx/arxiv-archive/issues/424
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T19:10:15.130038'
  last_visited: '2024-12-28T18:34:36.063000+00:00'
  main_tex_file: null
  published_date: '2024-10-14T17:15:07Z'
  state: open
  title: "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n \
    \ Models"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2410.10733
'2410.11900':
  abstract: 'Modern Question Answering (QA) and Reasoning approaches based on Large

    Language Models (LLMs) commonly use prompting techniques, such as

    Chain-of-Thought (CoT), assuming the resulting generation will have a more

    granular exploration and reasoning over the question space and scope. However,

    such methods struggle with generating outputs that are faithful to the

    intermediate chain of reasoning produced by the model. On the other end of the

    spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to

    combine LLMs with external symbolic solvers. While such approaches boast a high

    degree of faithfulness, they usually require a model trained for code

    generation and struggle with tasks that are ambiguous or hard to formalise

    strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided

    $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel

    interpretable approach for traversing the problem space using task

    decompositions. We use the LLM to plan a solution, soft-formalise the query

    into facts and predicates using a logic programming code and simulate that code

    execution using an exhaustive multi-hop search over the defined space. Our

    method allows us to compute the faithfulness of the reasoning process w.r.t.

    the generated code and analyse the steps of the multi-hop search without

    relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$

    out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model

    faithfulness positively correlates with overall performance and further

    demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors

    sufficient for and leading to the correct answer with optimal reasoning during

    the multi-hop search.'
  arxivId: '2410.11900'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.LG
  - cs.LO
  authors: Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle
    Augenstein
  created_at: '2024-12-27T10:14:49.640145'
  issue_number: 121
  issue_url: https://github.com/dmarx/arxiv-archive/issues/121
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:17:19.744Z'
  main_tex_file: null
  published_date: '2024-10-14T19:39:11Z'
  state: open
  title: 'FLARE: Faithful Logic-Aided Reasoning and Exploration'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.11900
'2410.15815':
  abstract: 'We present a method for computing free-energy differences using thermodynamic

    integration with a neural network potential that interpolates between two

    target Hamiltonians. The interpolation is defined at the sample distribution

    level, and the neural network potential is optimized to match the corresponding

    equilibrium potential at every intermediate time-step. Once the interpolating

    potentials and samples are well-aligned, the free-energy difference can be

    estimated using (neural) thermodynamic integration. To target molecular

    systems, we simultaneously couple Lennard-Jones and electrostatic interactions

    and model the rigid-body rotation of molecules. We report accurate results for

    several benchmark systems: a Lennard-Jones particle in a Lennard-Jones fluid,

    as well as the insertion of both water and methane solutes in a water solvent

    at atomistic resolution using a simple three-body neural-network potential.'
  arxivId: '2410.15815'
  arxiv_tags:
  - cond-mat.stat-mech
  - cs.LG
  authors: Bálint Máté, François Fleuret, Tristan Bereau
  created_at: '2024-12-27T10:14:46.624158'
  issue_number: 123
  issue_url: https://github.com/dmarx/arxiv-archive/issues/123
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:19:55.200Z'
  main_tex_file: null
  published_date: '2024-10-21T09:28:46Z'
  state: open
  title: Solvation Free Energies from Neural Thermodynamic Integration
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.15815
'2411.00999':
  abstract: 'Per-example gradient norms are a vital ingredient for estimating gradient

    noise scale (GNS) with minimal variance. Observing the tensor contractions

    required to compute them, we propose a method with minimal FLOPs in 3D or

    greater tensor regimes by simultaneously computing the norms while computing

    the parameter gradients. Using this method we are able to observe the GNS of

    different layers at higher accuracy than previously possible. We find that the

    total GNS of contemporary transformer models is predicted well by the GNS of

    only the normalization layers. As a result, focusing only on the normalization

    layer, we develop a custom kernel to compute the per-example gradient norms

    while performing the LayerNorm backward pass with zero throughput overhead.

    Tracking GNS on only those layers, we are able to guide a practical batch size

    schedule that reduces training time by 18% on a Chinchilla-optimal language

    model.'
  arxivId: '2411.00999'
  arxiv_tags:
  - cs.LG
  - stat.ML
  - I.2.6
  authors: Gavia Gray, Aman Tiwari, Shane Bergsma, Joel Hestness
  created_at: '2024-12-28T18:17:08.156757'
  issue_number: 427
  issue_url: https://github.com/dmarx/arxiv-archive/issues/427
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T18:08:42.178Z'
  main_tex_file: null
  published_date: '2024-11-01T19:50:00Z'
  state: open
  title: "Normalization Layer Per-Example Gradients are Sufficient to Predict\n  Gradient\
    \ Noise Scale in Transformers"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2411.00999
'2411.04282':
  abstract: 'Large language models (LLMs) have shown impressive capabilities, but
    still

    struggle with complex reasoning tasks requiring multiple steps. While

    prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at

    inference time, optimizing reasoning capabilities during training remains

    challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled

    framework that formulates reasoning as sampling from a latent distribution and

    optimizes it via variational approaches. LaTRO enables LLMs to concurrently

    improve both their reasoning process and ability to evaluate reasoning quality,

    without requiring external feedback or reward models. We validate LaTRO through

    experiments on GSM8K and ARC-Challenge datasets using multiple model

    architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of

    12.5% over base models and 9.6% over supervised fine-tuning across

    Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that

    pre-trained LLMs possess latent reasoning capabilities that can be unlocked and

    enhanced through our proposed optimization approach in a self-improvement

    manner. The code of LaTRO is available at

    \url{https://github.com/SalesforceAIResearch/LaTRO}.'
  arxivId: '2411.04282'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.LG
  - stat.ML
  - I.2.7
  authors: Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby
    Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang
  created_at: '2024-12-27T10:13:49.778363'
  issue_number: 169
  issue_url: https://github.com/dmarx/arxiv-archive/issues/169
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T21:35:41.978Z'
  main_tex_file: null
  published_date: '2024-11-06T22:02:30Z'
  state: open
  title: "Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities\
    \ via Self-Rewarding"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2411.04282
'2411.05899':
  abstract: 'Bayes'' rule naturally allows for inference refinement in a streaming
    fashion,

    without the need to recompute posteriors from scratch whenever new data

    arrives. In principle, Bayesian streaming is straightforward: we update our

    prior with the available data and use the resulting posterior as a prior when

    processing the next data chunk. In practice, however, this recipe entails i)

    approximating an intractable posterior at each time step; and ii) encapsulating

    results appropriately to allow for posterior propagation. For continuous state

    spaces, variational inference (VI) is particularly convenient due to its

    scalability and the tractability of variational posteriors. For discrete state

    spaces, however, state-of-the-art VI results in analytically intractable

    approximations that are ill-suited for streaming settings. To enable streaming

    Bayesian inference over discrete parameter spaces, we propose streaming Bayes

    GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed

    GFlowNets -- a powerful class of amortized samplers for discrete compositional

    objects. Notably, SB-GFlowNet approximates the initial posterior using a

    standard GFlowNet and subsequently updates it using a tailored procedure that

    requires only the newly observed data. Our case studies in linear preference

    learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets

    in sampling from an unnormalized posterior in a streaming setting. As expected,

    we also observe that SB-GFlowNets is significantly faster than repeatedly

    training a GFlowNet from scratch to sample from the full posterior.'
  arxivId: '2411.05899'
  arxiv_tags:
  - cs.LG
  authors: Tiago da Silva, Daniel Augusto de Souza, Diego Mesquita
  created_at: '2024-12-28T06:18:30.779504'
  issue_number: 339
  issue_url: https://github.com/dmarx/arxiv-archive/issues/339
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T06:18:30.781245'
  last_visited: '2024-12-28T06:09:10.852Z'
  main_tex_file: null
  published_date: '2024-11-08T15:53:56Z'
  state: open
  title: Streaming Bayes GFlowNets
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2411.05899
'2411.19722':
  abstract: 'Removing modeling constraints and unifying architectures across domains
    has

    been a key driver of the recent progress in training large multimodal models.

    However, most of these models still rely on many separately trained components

    such as modality-specific encoders and decoders. In this work, we further

    streamline joint generative modeling of images and text. We propose an

    autoregressive decoder-only transformer - JetFormer - which is trained to

    directly maximize the likelihood of raw data, without relying on any separately

    pretrained components, and can understand and generate both text and images.

    Specifically, we leverage a normalizing flow model to obtain a soft-token image

    representation that is jointly trained with an autoregressive multimodal

    transformer. The normalizing flow model serves as both an image encoder for

    perception tasks and an image decoder for image generation tasks during

    inference. JetFormer achieves text-to-image generation quality competitive with

    recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained

    image autoencoders, which are trained with a complex mixture of losses,

    including perceptual ones. At the same time, JetFormer demonstrates robust

    image understanding capabilities. To the best of our knowledge, JetFormer is

    the first model that is capable of generating high-fidelity images and

    producing strong log-likelihood bounds.'
  arxivId: '2411.19722'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Michael Tschannen, André Susano Pinto, Alexander Kolesnikov
  created_at: '2024-12-27T11:12:07.614500'
  issue_number: 44
  issue_url: https://github.com/dmarx/arxiv-archive/issues/44
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T11:12:13.655189'
  last_visited: '2024-12-15T22:53:55.309Z'
  main_tex_file: null
  published_date: '2024-11-29T14:14:59Z'
  state: open
  title: 'JetFormer: An Autoregressive Generative Model of Raw Images and Text'
  total_reading_time_seconds: 300
  url: https://arxiv.org/abs/2411.19722
'2412.01023':
  abstract: 'Most real-world datasets consist of a natural hierarchy between classes
    or an

    inherent label structure that is either already available or can be constructed

    cheaply. However, most existing representation learning methods ignore this

    hierarchy, treating labels as permutation invariant. Recent work [Zeng et al.,

    2022] proposes using this structured information explicitly, but the use of

    Euclidean distance may distort the underlying semantic context [Chen et al.,

    2013]. In this work, motivated by the advantage of hyperbolic spaces in

    modeling hierarchical relationships, we propose a novel approach HypStructure:

    a Hyperbolic Structured regularization approach to accurately embed the label

    hierarchy into the learned representations. HypStructure is a

    simple-yet-effective regularizer that consists of a hyperbolic tree-based

    representation loss along with a centering loss, and can be combined with any

    standard task loss to learn hierarchy-informed features. Extensive experiments

    on several large-scale vision benchmarks demonstrate the efficacy of

    HypStructure in reducing distortion and boosting generalization performance

    especially under low dimensional scenarios. For a better understanding of

    structured representation, we perform eigenvalue analysis that links the

    representation geometry to improved Out-of-Distribution (OOD) detection

    performance seen empirically. The code is available at

    \url{https://github.com/uiuctml/HypStructure}.'
  arxivId: '2412.01023'
  arxiv_tags:
  - cs.LG
  - cs.CV
  authors: Aditya Sinha, Siqi Zeng, Makoto Yamada, Han Zhao
  created_at: '2024-12-29T02:53:09.612574'
  issue_number: 440
  issue_url: https://github.com/dmarx/arxiv-archive/issues/440
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-29T02:53:15.617548'
  last_visited: '2024-12-29T01:41:03.660Z'
  main_tex_file: null
  published_date: '2024-12-02T00:56:44Z'
  state: open
  title: Learning Structured Representations with Hyperbolic Embeddings
  total_reading_time_seconds: 81
  url: https://arxiv.org/abs/2412.01023
'2412.04619':
  abstract: 'Language models (LMs), like other neural networks, often favor shortcut

    heuristics based on surface-level patterns. Although LMs behave like n-gram

    models early in training, they must eventually learn hierarchical syntactic

    representations to correctly apply grammatical rules out-of-distribution (OOD).

    In this work, we use case studies of English grammar to explore how complex,

    diverse training data drives models to generalize OOD. We construct a framework

    that unifies our understanding of random variation with training dynamics, rule

    selection with memorization, and data diversity with complexity. We show that

    these factors are nuanced, and that intermediate levels of diversity and

    complexity lead to inconsistent behavior across random seeds and to unstable

    training dynamics. Our findings emphasize the critical role of training data in

    shaping generalization patterns and illuminate how competing model strategies

    lead to inconsistent generalization outcomes across random seeds. Code is

    available at https://github.com/sunnytqin/concept_comp.git.'
  arxivId: '2412.04619'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tian Qin, Naomi Saphra, David Alvarez-Melis
  created_at: '2024-12-27T10:13:56.050907'
  issue_number: 164
  issue_url: https://github.com/dmarx/arxiv-archive/issues/164
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T10:13:56.051710'
  last_visited: '2024-12-22T17:55:18.862Z'
  main_tex_file: null
  published_date: '2024-12-05T21:12:37Z'
  state: open
  title: 'Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization'
  total_reading_time_seconds: 60
  url: https://arxiv.org/abs/2412.04619
'2412.05265':
  abstract: 'This manuscript gives a big-picture, up-to-date overview of the field
    of

    (deep) reinforcement learning and sequential decision making, covering

    value-based RL, policy-gradient methods, model-based methods, and various other

    topics (including a very brief discussion of RL+LLMs).'
  arxivId: '2412.05265'
  arxiv_tags:
  - cs.AI
  - cs.LG
  authors: Kevin Murphy
  created_at: '2024-12-27T10:14:44.060122'
  issue_number: 125
  issue_url: https://github.com/dmarx/arxiv-archive/issues/125
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T07:23:45.214Z'
  main_tex_file: null
  published_date: '2024-12-06T18:53:49Z'
  state: open
  title: 'Reinforcement Learning: An Overview'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.05265
'2412.06264':
  abstract: 'Flow Matching (FM) is a recent framework for generative modeling that
    has

    achieved state-of-the-art performance across various domains, including image,

    video, audio, speech, and biological structures. This guide offers a

    comprehensive and self-contained review of FM, covering its mathematical

    foundations, design choices, and extensions. By also providing a PyTorch

    package featuring relevant examples (e.g., image and text generation), this

    work aims to serve as a resource for both novice and experienced researchers

    interested in understanding, applying and further developing FM.'
  arxivId: '2412.06264'
  arxiv_tags:
  - cs.LG
  authors: Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian
    Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, Itai Gat
  created_at: '2024-12-27T10:14:55.606276'
  issue_number: 117
  issue_url: https://github.com/dmarx/arxiv-archive/issues/117
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T06:52:20.913Z'
  main_tex_file: null
  published_date: '2024-12-09T07:22:38Z'
  state: open
  title: Flow Matching Guide and Code
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.06264
'2412.08821':
  abstract: "LLMs have revolutionized the field of artificial intelligence and have\n\
    emerged as the de-facto tool for many tasks. The current established technology\n\
    of LLMs is to process input and generate output at the token level. This is in\n\
    sharp contrast to humans who operate at multiple levels of abstraction, well\n\
    beyond single words, to analyze information and to generate creative content.\n\
    In this paper, we present an attempt at an architecture which operates on an\n\
    explicit higher-level semantic representation, which we name a concept.\nConcepts\
    \ are language- and modality-agnostic and represent a higher level idea\nor action\
    \ in a flow. Hence, we build a \"Large Concept Model\". In this study, as\nproof\
    \ of feasibility, we assume that a concept corresponds to a sentence, and\nuse\
    \ an existing sentence embedding space, SONAR, which supports up to 200\nlanguages\
    \ in both text and speech modalities.\n  The Large Concept Model is trained to\
    \ perform autoregressive sentence\nprediction in an embedding space. We explore\
    \ multiple approaches, namely MSE\nregression, variants of diffusion-based generation,\
    \ and models operating in a\nquantized SONAR space. These explorations are performed\
    \ using 1.6B parameter\nmodels and training data in the order of 1.3T tokens.\
    \ We then scale one\narchitecture to a model size of 7B parameters and training\
    \ data of about 2.7T\ntokens. We perform an experimental evaluation on several\
    \ generative tasks,\nnamely summarization and a new task of summary expansion.\
    \ Finally, we show that\nour model exhibits impressive zero-shot generalization\
    \ performance to many\nlanguages, outperforming existing LLMs of the same size.\
    \ The training code of\nour models is freely available."
  arxivId: '2412.08821'
  arxiv_tags:
  - cs.CL
  authors: LCM team, Loïc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov,
    Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-jussà,
    David Dale, Hady Elsahar, Kevin Heffernan, João Maria Janeiro, Tuan Tran, Christophe
    Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem,
    Holger Schwenk
  created_at: '2024-12-28T16:15:12.320019'
  issue_number: 419
  issue_url: https://github.com/dmarx/arxiv-archive/issues/419
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T16:15:12.321429'
  last_visited: '2024-12-28T15:26:36.997Z'
  main_tex_file: null
  published_date: '2024-12-11T23:36:20Z'
  state: open
  title: "Large Concept Models: Language Modeling in a Sentence Representation\n \
    \ Space"
  total_reading_time_seconds: 37
  url: https://arxiv.org/abs/2412.08821
'2412.09349':
  abstract: 'Controllable human image animation aims to generate videos from reference

    images using driving videos. Due to the limited control signals provided by

    sparse guidance (e.g., skeleton pose), recent works have attempted to introduce

    additional dense conditions (e.g., depth map) to ensure motion alignment.

    However, such strict dense guidance impairs the quality of the generated video

    when the body shape of the reference character differs significantly from that

    of the driving video. In this paper, we present DisPose to mine more

    generalizable and effective control signals without additional dense input,

    which disentangles the sparse skeleton pose in human image animation into

    motion field guidance and keypoint correspondence. Specifically, we generate a

    dense motion field from a sparse motion field and the reference image, which

    provides region-level dense guidance while maintaining the generalization of

    the sparse pose control. We also extract diffusion features corresponding to

    pose keypoints from the reference image, and then these point features are

    transferred to the target pose to provide distinct identity information. To

    seamlessly integrate into existing models, we propose a plug-and-play hybrid

    ControlNet that improves the quality and consistency of generated videos while

    freezing the existing model parameters. Extensive qualitative and quantitative

    experiments demonstrate the superiority of DisPose compared to current methods.

    Code:

    \href{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.'
  arxivId: '2412.09349'
  arxiv_tags:
  - cs.CV
  authors: Hongxiang Li, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng,
    Long Chen
  created_at: '2024-12-27T08:37:36.793096'
  issue_number: 191
  issue_url: https://github.com/dmarx/arxiv-archive/issues/191
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T10:13:49.397575'
  last_visited: '2024-12-23T21:16:01.436Z'
  main_tex_file: null
  published_date: '2024-12-12T15:15:59Z'
  state: open
  title: "DisPose: Disentangling Pose Guidance for Controllable Human Image\n  Animation"
  total_reading_time_seconds: 350
  url: https://arxiv.org/abs/2412.09349
'2412.09621':
  abstract: 'Learning to understand dynamic 3D scenes from imagery is crucial for

    applications ranging from robotics to scene reconstruction. Yet, unlike other

    problems where large-scale supervised training has enabled rapid progress,

    directly supervising methods for recovering 3D motion remains challenging due

    to the fundamental difficulty of obtaining ground truth annotations. We present

    a system for mining high-quality 4D reconstructions from internet stereoscopic,

    wide-angle videos. Our system fuses and filters the outputs of camera pose

    estimation, stereo depth estimation, and temporal tracking methods into

    high-quality dynamic 3D reconstructions. We use this method to generate

    large-scale data in the form of world-consistent, pseudo-metric 3D point clouds

    with long-term motion trajectories. We demonstrate the utility of this data by

    training a variant of DUSt3R to predict structure and 3D motion from real-world

    image pairs, showing that training on our reconstructed data enables

    generalization to diverse real-world scenes. Project page:

    https://stereo4d.github.io'
  arxivId: '2412.09621'
  arxiv_tags:
  - cs.CV
  authors: Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander
    Holynski
  created_at: '2024-12-27T11:12:10.752813'
  issue_number: 52
  issue_url: https://github.com/dmarx/arxiv-archive/issues/52
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T11:12:10.753566'
  last_visited: '2024-12-16T08:58:26.647Z'
  main_tex_file: null
  published_date: '2024-12-12T18:59:54Z'
  state: open
  title: 'Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos'
  total_reading_time_seconds: 60
  url: https://arxiv.org/abs/2412.09621
'2412.11766':
  abstract: 'Modeling human behavior is essential to accurately predict epidemic spread,

    with behaviors like vaccine hesitancy complicating control efforts. While

    epidemic spread is often treated as a simple contagion, vaccine uptake may

    follow complex contagion dynamics, where individuals'' decisions depend on

    multiple social contacts. Recently, the concept of complex contagion has

    received strong theoretical underpinnings thanks to the generalization of

    spreading phenomena from pairwise to higher-order interactions. Although

    several potential applications have been suggested, examples of complex

    contagions motivated by real data remain scarce. Surveys on COVID-19 vaccine

    hesitancy in the US suggest that vaccination attitudes may indeed depend on the

    vaccination status of social peers, aligning with complex contagion principles.

    In this work, we examine the interactions between epidemic spread, vaccination,

    and vaccine uptake attitudes under complex contagion. Using the SIR model with

    a dynamic, threshold-based vaccination campaign, we simulate scenarios on an

    age-structured multilayer network informed by US contact data. Our results

    offer insights into the role of social dynamics in shaping vaccination behavior

    and epidemic outcomes.'
  arxivId: '2412.11766'
  arxiv_tags:
  - physics.soc-ph
  - cs.SI
  authors: Alfonso de Miguel-Arribas, Alberto Aleta, Yamir Moreno
  created_at: '2024-12-28T06:18:36.818215'
  issue_number: 332
  issue_url: https://github.com/dmarx/arxiv-archive/issues/332
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-28T06:18:36.819343'
  last_visited: '2024-12-28T05:44:44.089Z'
  main_tex_file: null
  published_date: '2024-12-16T13:37:27Z'
  state: open
  title: "Interplay of epidemic spreading and vaccine uptake under complex social\n\
    \  contagion"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.11766
'2412.11768':
  abstract: 'In this work, we question the necessity of adaptive gradient methods
    for

    training deep neural networks. SGD-SaI is a simple yet effective enhancement to

    stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning

    rate Scaling at Initialization (SaI) to distinct parameter groups, guided by

    their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning

    rates without relying on adaptive second-order momentum, SGD-SaI helps prevent

    training imbalances from the very first iteration and cuts the optimizer''s

    memory usage by half compared to AdamW. Despite its simplicity and efficiency,

    SGD-SaI consistently matches or outperforms AdamW in training a variety of

    Transformer-based tasks, effectively overcoming a long-standing challenge of

    using SGD for training Transformers. SGD-SaI excels in ImageNet-1K

    classification with Vision Transformers(ViT) and GPT-2 pretraining for large

    language models (LLMs, transformer decoder-only), demonstrating robustness to

    hyperparameter variations and practicality for diverse applications. We further

    tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion

    models, where it consistently outperforms state-of-the-art optimizers. From a

    memory efficiency perspective, SGD-SaI achieves substantial memory savings for

    optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)

    and 25.15 GB for Llama2-7B compared to AdamW in full-precision training

    settings.'
  arxivId: '2412.11768'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen
  created_at: '2024-12-28T19:10:15.127797'
  issue_number: 467
  issue_url: https://github.com/dmarx/arxiv-archive/issues/467
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-29T10:02:12.895155'
  last_visited: '2024-12-29T09:59:06.438Z'
  main_tex_file: null
  published_date: '2024-12-16T13:41:37Z'
  state: open
  title: 'No More Adam: Learning Rate Scaling at Initialization is All You Need'
  total_reading_time_seconds: 15
  url: https://arxiv.org/abs/2412.11768
'2412.13663':
  abstract: 'Encoder-only transformer models such as BERT offer a great performance-size

    tradeoff for retrieval and classification tasks with respect to larger

    decoder-only models. Despite being the workhorse of numerous production

    pipelines, there have been limited Pareto improvements to BERT since its

    release. In this paper, we introduce ModernBERT, bringing modern model

    optimizations to encoder-only models and representing a major Pareto

    improvement over older encoders. Trained on 2 trillion tokens with a native

    8192 sequence length, ModernBERT models exhibit state-of-the-art results on a

    large pool of evaluations encompassing diverse classification tasks and both

    single and multi-vector retrieval on different domains (including code). In

    addition to strong downstream performance, ModernBERT is also the most speed

    and memory efficient encoder and is designed for inference on common GPUs.'
  arxivId: '2412.13663'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar
    Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom
    Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli
  created_at: '2024-12-27T10:15:01.628696'
  issue_number: 113
  issue_url: https://github.com/dmarx/arxiv-archive/issues/113
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-22T06:30:57.155Z'
  main_tex_file: null
  published_date: '2024-12-18T09:39:44Z'
  state: open
  title: "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast,\
    \ Memory Efficient, and Long Context Finetuning and Inference"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.13663
'2412.17799':
  abstract: 'With the recent Nobel Prize awarded for radical advances in protein

    discovery, foundation models (FMs) for exploring large combinatorial spaces

    promise to revolutionize many scientific fields. Artificial Life (ALife) has

    not yet integrated FMs, thus presenting a major opportunity for the field to

    alleviate the historical burden of relying chiefly on manual design and

    trial-and-error to discover the configurations of lifelike simulations. This

    paper presents, for the first time, a successful realization of this

    opportunity using vision-language FMs. The proposed approach, called Automated

    Search for Artificial Life (ASAL), (1) finds simulations that produce target

    phenomena, (2) discovers simulations that generate temporally open-ended

    novelty, and (3) illuminates an entire space of interestingly diverse

    simulations. Because of the generality of FMs, ASAL works effectively across a

    diverse range of ALife substrates including Boids, Particle Life, Game of Life,

    Lenia, and Neural Cellular Automata. A major result highlighting the potential

    of this technique is the discovery of previously unseen Lenia and Boids

    lifeforms, as well as cellular automata that are open-ended like Conway''s Game

    of Life. Additionally, the use of FMs allows for the quantification of

    previously qualitative phenomena in a human-aligned way. This new paradigm

    promises to accelerate ALife research beyond what is possible through human

    ingenuity alone.'
  arxivId: '2412.17799'
  arxiv_tags:
  - cs.AI
  - cs.NE
  authors: Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip
    Isola, David Ha
  created_at: '2024-12-27T08:37:00.881996'
  issue_number: 247
  issue_url: https://github.com/dmarx/arxiv-archive/issues/247
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-27T08:37:00.884084'
  last_visited: '2024-12-24T04:41:56.160Z'
  main_tex_file: null
  published_date: '2024-12-23T18:57:00Z'
  state: open
  title: Automating the Search for Artificial Life with Foundation Models
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2412.17799
