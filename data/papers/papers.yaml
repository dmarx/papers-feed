'1310.6753':
  abstract: 'A crucial task in the analysis of on-line social-networking systems is
    to

    identify important people --- those linked by strong social ties --- within an

    individual''s network neighborhood. Here we investigate this question for a

    particular category of strong ties, those involving spouses or romantic

    partners. We organize our analysis around a basic question: given all the

    connections among a person''s friends, can you recognize his or her romantic

    partner from the network structure alone? Using data from a large sample of

    Facebook users, we find that this task can be accomplished with high accuracy,

    but doing so requires the development of a new measure of tie strength that we

    term `dispersion'' --- the extent to which two people''s mutual friends are not

    themselves well-connected. The results offer methods for identifying types of

    structurally significant people in on-line applications, and suggest a

    potential expansion of existing theories of tie strength.'
  arxivId: '1310.6753'
  arxiv_tags:
  - cs.SI
  - physics.soc-ph
  - H.2.8
  authors: Lars Backstrom, Jon Kleinberg
  created_at: '2025-01-04T06:52:00.637611'
  issue_number: 770
  issue_url: https://github.com/dmarx/papers-feed/issues/770
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:00.641659'
  last_visited: '2025-01-03T18:28:19.059Z'
  main_tex_file: null
  published_date: '2013-10-24T20:00:18Z'
  state: open
  title: "Romantic Partnerships and the Dispersion of Social Ties: A Network\n  Analysis\
    \ of Relationship Status on Facebook"
  total_reading_time_seconds: 79
  url: https://arxiv.org/abs/1310.6753
'1602.03483':
  abstract: 'Unsupervised methods for learning distributed representations of words
    are

    ubiquitous in today''s NLP research, but far less is known about the best ways

    to learn distributed phrase or sentence representations from unlabelled data.

    This paper is a systematic comparison of models that learn such

    representations. We find that the optimal approach depends critically on the

    intended application. Deeper, more complex models are preferable for

    representations to be used in supervised systems, but shallow log-linear models

    work best for building representation spaces that can be decoded with simple

    spatial distance metrics. We also propose two new unsupervised

    representation-learning objectives designed to optimise the trade-off between

    training time, domain portability and performance.'
  arxivId: '1602.03483'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Felix Hill, Kyunghyun Cho, Anna Korhonen
  created_at: '2025-01-04T06:51:57.839597'
  issue_number: 777
  issue_url: https://github.com/dmarx/papers-feed/issues/777
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:51:57.840814'
  last_visited: '2025-01-03T20:13:43.540Z'
  main_tex_file: null
  published_date: '2016-02-10T18:49:58Z'
  state: open
  title: Learning Distributed Representations of Sentences from Unlabelled Data
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/1602.03483
'1705.08039':
  abstract: 'Representation learning has become an invaluable approach for learning
    from

    symbolic data such as text and graphs. However, while complex symbolic datasets

    often exhibit a latent hierarchical structure, state-of-the-art methods

    typically learn embeddings in Euclidean vector spaces, which do not account for

    this property. For this purpose, we introduce a new approach for learning

    hierarchical representations of symbolic data by embedding them into hyperbolic

    space -- or more precisely into an n-dimensional Poincar\''e ball. Due to the

    underlying hyperbolic geometry, this allows us to learn parsimonious

    representations of symbolic data by simultaneously capturing hierarchy and

    similarity. We introduce an efficient algorithm to learn the embeddings based

    on Riemannian optimization and show experimentally that Poincar\''e embeddings

    outperform Euclidean embeddings significantly on data with latent hierarchies,

    both in terms of representation capacity and in terms of generalization

    ability.'
  arxivId: '1705.08039'
  arxiv_tags:
  - cs.AI
  - cs.LG
  - stat.ML
  authors: Maximilian Nickel, Douwe Kiela
  created_at: '2025-01-04T14:49:33.224758'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:42.247780'
  last_visited: '2024-12-29T01:44:01.360000+00:00'
  main_tex_file: null
  published_date: '2017-05-22T23:14:36Z'
  state: open
  title: Poincaré Embeddings for Learning Hierarchical Representations
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1705.08039
'1705.10359':
  abstract: 'Neural embeddings have been used with great success in Natural Language

    Processing (NLP). They provide compact representations that encapsulate word

    similarity and attain state-of-the-art performance in a range of linguistic

    tasks. The success of neural embeddings has prompted significant amounts of

    research into applications in domains other than language. One such domain is

    graph-structured data, where embeddings of vertices can be learned that

    encapsulate vertex similarity and improve performance on tasks including edge

    prediction and vertex labelling. For both NLP and graph based tasks, embeddings

    have been learned in high-dimensional Euclidean spaces. However, recent work

    has shown that the appropriate isometric space for embedding complex networks

    is not the flat Euclidean space, but negatively curved, hyperbolic space. We

    present a new concept that exploits these recent insights and propose learning

    neural embeddings of graphs in hyperbolic space. We provide experimental

    evidence that embedding graphs in their natural geometry significantly improves

    performance on downstream tasks for several real-world public datasets.'
  arxivId: '1705.10359'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth
  created_at: '2025-01-04T14:49:36.224476'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:42.248652'
  last_visited: '2024-12-29T01:43:53.617000+00:00'
  main_tex_file: null
  published_date: '2017-05-29T18:47:30Z'
  state: open
  title: Neural Embeddings of Graphs in Hyperbolic Space
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/1705.10359
'1802.03426':
  abstract: 'UMAP (Uniform Manifold Approximation and Projection) is a novel manifold

    learning technique for dimension reduction. UMAP is constructed from a

    theoretical framework based in Riemannian geometry and algebraic topology. The

    result is a practical scalable algorithm that applies to real world data. The

    UMAP algorithm is competitive with t-SNE for visualization quality, and

    arguably preserves more of the global structure with superior run time

    performance. Furthermore, UMAP has no computational restrictions on embedding

    dimension, making it viable as a general purpose dimension reduction technique

    for machine learning.'
  arxivId: '1802.03426'
  arxiv_tags:
  - stat.ML
  - cs.CG
  - cs.LG
  authors: Leland McInnes, John Healy, James Melville
  created_at: '2025-01-04T14:49:00.223341'
  issue_number: 533
  issue_url: https://github.com/dmarx/papers-feed/issues/533
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T03:35:58.743Z'
  main_tex_file: null
  published_date: '2018-02-09T19:39:33Z'
  state: open
  title: "UMAP: Uniform Manifold Approximation and Projection for Dimension\n  Reduction"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1802.03426
'1804.03329':
  abstract: 'Hyperbolic embeddings offer excellent quality with few dimensions when

    embedding hierarchical data structures like synonym or type hierarchies. Given

    a tree, we give a combinatorial construction that embeds the tree in hyperbolic

    space with arbitrarily low distortion without using optimization. On WordNet,

    our combinatorial embedding obtains a mean-average-precision of 0.989 with only

    two dimensions, while Nickel et al.''s recent construction obtains 0.87 using

    200 dimensions. We provide upper and lower bounds that allow us to characterize

    the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To

    embed general metric spaces, we propose a hyperbolic generalization of

    multidimensional scaling (h-MDS). We show how to perform exact recovery of

    hyperbolic points from distances, provide a perturbation analysis, and give a

    recovery result that allows us to reduce dimensionality. The h-MDS approach

    offers consistently low distortion even with few dimensions across several

    datasets. Finally, we extract lessons from the algorithms and theory above to

    design a PyTorch-based implementation that can handle incomplete information

    and is scalable.'
  arxivId: '1804.03329'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Christopher De Sa, Albert Gu, Christopher Ré, Frederic Sala
  created_at: '2025-01-04T14:49:30.230276'
  issue_number: 457
  issue_url: https://github.com/dmarx/papers-feed/issues/457
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T02:34:35.575Z'
  main_tex_file: null
  published_date: '2018-04-10T03:39:16Z'
  state: open
  title: Representation Tradeoffs for Hyperbolic Embeddings
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1804.03329
'1810.00363':
  abstract: 'We propose a new point of view for regularizing deep neural networks
    by using

    the norm of a reproducing kernel Hilbert space (RKHS). Even though this norm

    cannot be computed, it admits upper and lower approximations leading to various

    practical strategies. Specifically, this perspective (i) provides a common

    umbrella for many existing regularization principles, including spectral norm

    and gradient penalties, or adversarial training, (ii) leads to new effective

    regularization penalties, and (iii) suggests hybrid strategies combining lower

    and upper bounds to get better approximations of the RKHS norm. We

    experimentally show this approach to be effective when learning on small

    datasets, or to obtain adversarially robust models.'
  arxivId: '1810.00363'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Alberto Bietti, Grégoire Mialon, Dexiong Chen, Julien Mairal
  created_at: '2025-01-04T14:49:18.222858'
  issue_number: 478
  issue_url: https://github.com/dmarx/papers-feed/issues/478
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:18.223709'
  last_visited: '2024-12-29T10:35:11.499Z'
  main_tex_file: null
  published_date: '2018-09-30T11:40:59Z'
  state: open
  title: A Kernel Perspective for Regularizing Deep Neural Networks
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/1810.00363
'2103.13413':
  abstract: 'We introduce dense vision transformers, an architecture that leverages
    vision

    transformers in place of convolutional networks as a backbone for dense

    prediction tasks. We assemble tokens from various stages of the vision

    transformer into image-like representations at various resolutions and

    progressively combine them into full-resolution predictions using a

    convolutional decoder. The transformer backbone processes representations at a

    constant and relatively high resolution and has a global receptive field at

    every stage. These properties allow the dense vision transformer to provide

    finer-grained and more globally coherent predictions when compared to

    fully-convolutional networks. Our experiments show that this architecture

    yields substantial improvements on dense prediction tasks, especially when a

    large amount of training data is available. For monocular depth estimation, we

    observe an improvement of up to 28% in relative performance when compared to a

    state-of-the-art fully-convolutional network. When applied to semantic

    segmentation, dense vision transformers set a new state of the art on ADE20K

    with 49.02% mIoU. We further show that the architecture can be fine-tuned on

    smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets

    the new state of the art. Our models are available at

    https://github.com/intel-isl/DPT.'
  arxivId: '2103.13413'
  arxiv_tags:
  - cs.CV
  authors: René Ranftl, Alexey Bochkovskiy, Vladlen Koltun
  created_at: '2025-01-04T14:48:27.532008'
  issue_number: 783
  issue_url: https://github.com/dmarx/papers-feed/issues/783
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-04T14:47:34.354Z'
  main_tex_file: null
  published_date: '2021-03-24T18:01:17Z'
  state: open
  title: Vision Transformers for Dense Prediction
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2103.13413
'2105.05720':
  abstract: "Recent trend towards increasing large machine learning models require\
    \ both\ntraining and inference tasks to be distributed. Considering the huge cost\
    \ of\ntraining these models, it is imperative to unlock optimizations in computation\n\
    and communication to obtain best performance. However, current logical\nseparation\
    \ between computation and communication kernels in deep learning\nframeworks misses\
    \ the optimization opportunities across such barrier. Breaking\nthis abstraction\
    \ with a holistic consideration can provide many optimizations\nto provide performance\
    \ improvements in distributed workloads. Manually applying\nthese optimizations\
    \ needs modifications in underlying computation and\ncommunication libraries for\
    \ each scenario, which is time consuming and\nerror-prone.\n  Therefore, we present\
    \ CoCoNeT, with a DSL to express a program with both\ncomputation and communication.\
    \ CoCoNeT contains several machine learning aware\ntransformations to optimize\
    \ a program and a compiler to generate high\nperformance kernels. Providing both\
    \ computation and communication as first\nclass constructs allows users to work\
    \ on a high-level abstraction and apply\npowerful optimizations, such as fusion\
    \ or overlapping of communication and\ncomputation. CoCoNeT enables us to optimize\
    \ data-, model-and pipeline-parallel\nworkloads in large language models with\
    \ only a few lines of code. Experiments\nshow CoCoNeT significantly outperforms\
    \ state-of-the-art distributed machine\nlearning implementations."
  arxivId: '2105.05720'
  arxiv_tags:
  - cs.DC
  - cs.LG
  - cs.PL
  authors: Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed
    Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi
  created_at: '2025-01-04T14:48:39.270195'
  issue_number: 581
  issue_url: https://github.com/dmarx/papers-feed/issues/581
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:39.272603'
  last_visited: '2024-12-30T07:58:09.742Z'
  main_tex_file: null
  published_date: '2021-05-12T15:13:43Z'
  state: open
  title: "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed\
    \ Machine Learning Workloads"
  total_reading_time_seconds: 38
  url: https://arxiv.org/abs/2105.05720
'2106.10165':
  abstract: 'This book develops an effective theory approach to understanding deep
    neural

    networks of practical relevance. Beginning from a first-principles

    component-level picture of networks, we explain how to determine an accurate

    description of the output of trained networks by solving layer-to-layer

    iteration equations and nonlinear learning dynamics. A main result is that the

    predictions of networks are described by nearly-Gaussian distributions, with

    the depth-to-width aspect ratio of the network controlling the deviations from

    the infinite-width Gaussian description. We explain how these effectively-deep

    networks learn nontrivial representations from training and more broadly

    analyze the mechanism of representation learning for nonlinear models. From a

    nearly-kernel-methods perspective, we find that the dependence of such models''

    predictions on the underlying learning algorithm can be expressed in a simple

    and universal way. To obtain these results, we develop the notion of

    representation group flow (RG flow) to characterize the propagation of signals

    through the network. By tuning networks to criticality, we give a practical

    solution to the exploding and vanishing gradient problem. We further explain

    how RG flow leads to near-universal behavior and lets us categorize networks

    built from different activation functions into universality classes.

    Altogether, we show that the depth-to-width ratio governs the effective model

    complexity of the ensemble of trained networks. By using information-theoretic

    techniques, we estimate the optimal aspect ratio at which we expect the network

    to be practically most useful and show how residual connections can be used to

    push this scale to arbitrary depths. With these tools, we can learn in detail

    about the inductive bias of architectures, hyperparameters, and optimizers.'
  arxivId: '2106.10165'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - hep-th
  - stat.ML
  authors: Daniel A. Roberts, Sho Yaida, Boris Hanin
  created_at: '2025-01-04T14:49:06.220612'
  issue_number: 523
  issue_url: https://github.com/dmarx/papers-feed/issues/523
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T22:46:13.679Z'
  main_tex_file: null
  published_date: '2021-06-18T15:00:00Z'
  state: open
  title: The Principles of Deep Learning Theory
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2106.10165
'2112.04215':
  abstract: 'Self-supervised models have been shown to produce comparable or better
    visual

    representations than their supervised counterparts when trained offline on

    unlabeled data at scale. However, their efficacy is catastrophically reduced in

    a Continual Learning (CL) scenario where data is presented to the model

    sequentially. In this paper, we show that self-supervised loss functions can be

    seamlessly converted into distillation mechanisms for CL by adding a predictor

    network that maps the current state of the representations to their past state.

    This enables us to devise a framework for Continual self-supervised visual

    representation Learning that (i) significantly improves the quality of the

    learned representations, (ii) is compatible with several state-of-the-art

    self-supervised objectives, and (iii) needs little to no hyperparameter tuning.

    We demonstrate the effectiveness of our approach empirically by training six

    popular self-supervised models in various CL settings.'
  arxivId: '2112.04215'
  arxiv_tags:
  - cs.CV
  - cs.LG
  authors: Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci,
    Karteek Alahari, Julien Mairal
  created_at: '2025-01-04T14:49:51.228547'
  issue_number: 407
  issue_url: https://github.com/dmarx/papers-feed/issues/407
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:29:58.974Z'
  main_tex_file: null
  published_date: '2021-12-08T10:39:13Z'
  state: open
  title: Self-Supervised Models are Continual Learners
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2112.04215
'2205.13509':
  abstract: 'The presence of obstacles is intuitively expected to hinder the diffusive

    transport of micro-swimmers. However, for chiral micro-swimmers, a low density

    of obstacles near a surface can enhance their diffusive behavior, due to the

    rectification of the chiral motion by the obstacles. Here, we study numerically

    the role that disorder plays in determining the transport dynamics of chiral

    micro-swimmers on surfaces with obstacles. We consider different densities of

    regularly spaced obstacles and distinct types of disorder: noise in the

    dynamics of the micro-swimmer, quenched noise in the positions of the obstacles

    as well as obstacle size polydispersity. We show that, depending on the type

    and strength of the disorder, the presence of obstacles can either enhance or

    hinder transport, and discuss implications for the control of active transport

    in disordered media.'
  arxivId: '2205.13509'
  arxiv_tags:
  - cond-mat.soft
  - cond-mat.stat-mech
  authors: Danne M. van Roon, Giorgio Volpe, Margarida M. Telo da Gama, Nuno A. M.
    Araújo
  created_at: '2025-01-04T14:49:54.237647'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:54.238526'
  last_visited: '2024-12-28T07:25:17.631000+00:00'
  main_tex_file: null
  published_date: '2022-05-26T17:22:50Z'
  state: open
  title: "The role of disorder in the motion of chiral swimmers in the presence of\n\
    \  obstacles"
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2205.13509
'2207.10342':
  abstract: 'Prompted models have demonstrated impressive few-shot learning abilities.

    Repeated interactions at test-time with a single model, or the composition of

    multiple models together, further expands capabilities. These compositions are

    probabilistic models, and may be expressed in the language of graphical models

    with random variables whose values are complex data types such as strings.

    Cases with control flow and dynamic structure require techniques from

    probabilistic programming, which allow implementing disparate model structures

    and inference strategies in a unified language. We formalize several existing

    techniques from this perspective, including scratchpads / chain of thought,

    verifiers, STaR, selection-inference, and tool use. We refer to the resulting

    programs as language model cascades.'
  arxivId: '2207.10342'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael
    Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein,
    Kevin Murphy, Charles Sutton
  created_at: '2025-01-04T14:49:24.257305'
  issue_number: 415
  issue_url: https://github.com/dmarx/papers-feed/issues/415
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:42.250385'
  last_visited: '2024-12-28T09:09:37.944Z'
  main_tex_file: null
  published_date: '2022-07-21T07:35:18Z'
  state: open
  title: Language Model Cascades
  total_reading_time_seconds: 51
  url: https://arxiv.org/abs/2207.10342
'2208.11665':
  abstract: 'The Manifold Hypothesis is a widely accepted tenet of Machine Learning
    which

    asserts that nominally high-dimensional data are in fact concentrated near a

    low-dimensional manifold, embedded in high-dimensional space. This phenomenon

    is observed empirically in many real world situations, has led to development

    of a wide range of statistical methods in the last few decades, and has been

    suggested as a key factor in the success of modern AI technologies. We show

    that rich and sometimes intricate manifold structure in data can emerge from a

    generic and remarkably simple statistical model -- the Latent Metric Model --

    via elementary concepts such as latent variables, correlation and stationarity.

    This establishes a general statistical explanation for why the Manifold

    Hypothesis seems to hold in so many situations. Informed by the Latent Metric

    Model we derive procedures to discover and interpret the geometry of

    high-dimensional data, and explore hypotheses about the data generating

    mechanism. These procedures operate under minimal assumptions and make use of

    well known, scaleable graph-analytic algorithms.'
  arxivId: '2208.11665'
  arxiv_tags:
  - stat.ME
  - cs.LG
  - stat.ML
  - 62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30
  authors: Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy
  created_at: '2025-01-04T14:49:42.245960'
  issue_number: 452
  issue_url: https://github.com/dmarx/papers-feed/issues/452
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-29T02:26:31.276Z'
  main_tex_file: null
  published_date: '2022-08-24T17:00:16Z'
  state: open
  title: Statistical exploration of the Manifold Hypothesis
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2208.11665
'2303.00383':
  abstract: 'We propose a robust and computationally efficient algorithm to generically

    construct first return maps of dynamical systems from time series without the

    need for embedding. Typically, a first return map is constructed using a

    heuristic convenience (maxima or zero-crossings of the time series, for

    example) or a computationally delicate geometric approach (explicitly

    constructing a Poincar\''e section from a hyper-surface normal to the flow and

    then interpolating to determine intersections with trajectories). Our approach

    relies on ordinal partitions of the time series and builds the first return map

    from successive intersections with particular ordinal sequences. Generically,

    we can obtain distinct first return maps for each ordinal sequence. We define

    entropy-based measures to guide our selection of the ordinal sequence for a

    ``good'''' first return map and show that this method can robustly be applied
    to

    time series from classical chaotic systems to extract the underlying first

    return map dynamics. The results are shown on several well-known dynamical

    systems (Lorenz, R{\"o}ssler and Mackey-Glass in chaotic regimes).'
  arxivId: '2303.00383'
  arxiv_tags:
  - math.DS
  authors: Zahra Shahriari, Shannon Dee Algar, David M. Walker, Michael Small
  created_at: '2025-01-04T06:52:03.682689'
  issue_number: 760
  issue_url: https://github.com/dmarx/papers-feed/issues/760
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:03.683895'
  last_visited: '2025-01-03T08:49:43.332Z'
  main_tex_file: null
  published_date: '2023-03-01T10:09:57Z'
  state: open
  title: "Ordinal Poincaré Sections: Reconstructing the First Return Map from an\n\
    \  Ordinal Segmentation of Time Series"
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2303.00383
'2305.13169':
  abstract: 'Pretraining is the preliminary and fundamental step in developing capable

    language models (LM). Despite this, pretraining data design is critically

    under-documented and often guided by empirically unsupported intuitions. To

    address this, we pretrain 28 1.5B parameter decoder-only models, training on

    data curated (1) at different times, (2) with varying toxicity and quality

    filters, and (3) with different domain compositions. First, we quantify the

    effect of pretraining data age. A temporal shift between evaluation data and

    pretraining data leads to performance degradation, which is not overcome by

    finetuning. Second, we explore the effect of quality and toxicity filters,

    showing a trade-off between performance on standard benchmarks and risk of

    toxic generations. Our findings indicate there does not exist a

    one-size-fits-all solution to filtering training data. We also find that the

    effects of different types of filtering are not predictable from text domain

    characteristics. Lastly, we empirically validate that the inclusion of

    heterogeneous data sources, like books and web, is broadly beneficial and

    warrants greater prioritization. These findings constitute the largest set of

    experiments to validate, quantify, and expose many undocumented intuitions

    about text pretraining, which we hope will help support more informed

    data-centric decisions in LM development.'
  arxivId: '2305.13169'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts,
    Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito
  created_at: '2025-01-04T06:53:03.610654'
  issue_number: 657
  issue_url: https://github.com/dmarx/papers-feed/issues/657
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:20:50.556Z'
  main_tex_file: null
  published_date: '2023-05-22T15:57:53Z'
  state: open
  title: "A Pretrainer's Guide to Training Data: Measuring the Effects of Data\n \
    \ Age, Domain Coverage, Quality, & Toxicity"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2305.13169
'2307.09288':
  abstract: 'In this work, we develop and release Llama 2, a collection of pretrained
    and

    fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70

    billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for

    dialogue use cases. Our models outperform open-source chat models on most

    benchmarks we tested, and based on our human evaluations for helpfulness and

    safety, may be a suitable substitute for closed-source models. We provide a

    detailed description of our approach to fine-tuning and safety improvements of

    Llama 2-Chat in order to enable the community to build on our work and

    contribute to the responsible development of LLMs.'
  arxivId: '2307.09288'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
  created_at: '2025-01-04T14:49:21.224810'
  issue_number: 472
  issue_url: https://github.com/dmarx/papers-feed/issues/472
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:21.229139'
  last_visited: '2024-12-29T10:07:22.174Z'
  main_tex_file: null
  published_date: '2023-07-18T14:31:57Z'
  state: open
  title: 'Llama 2: Open Foundation and Fine-Tuned Chat Models'
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/2307.09288
'2308.06259':
  abstract: 'We present a scalable method to build a high quality instruction following

    language model by automatically labelling human-written text with corresponding

    instructions. Our approach, named instruction backtranslation, starts with a

    language model finetuned on a small amount of seed data, and a given web

    corpus. The seed model is used to construct training examples by generating

    instruction prompts for web documents (self-augmentation), and then selecting

    high quality examples from among these candidates (self-curation). This data is

    then used to finetune a stronger model. Finetuning LLaMa on two iterations of

    our approach yields a model that outperforms all other LLaMa-based models on

    the Alpaca leaderboard not relying on distillation data, demonstrating highly

    effective self-alignment.'
  arxivId: '2308.06259'
  arxiv_tags:
  - cs.CL
  authors: Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer,
    Jason Weston, Mike Lewis
  created_at: '2025-01-04T14:48:45.526814'
  issue_number: 575
  issue_url: https://github.com/dmarx/papers-feed/issues/575
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T04:57:10.471Z'
  main_tex_file: null
  published_date: '2023-08-11T17:47:54Z'
  state: open
  title: Self-Alignment with Instruction Backtranslation
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2308.06259
'2309.14556':
  abstract: 'Researchers have argued that large language models (LLMs) exhibit

    high-quality writing capabilities from blogs to stories. However, evaluating

    objectively the creativity of a piece of writing is challenging. Inspired by

    the Torrance Test of Creative Thinking (TTCT), which measures creativity as a

    process, we use the Consensual Assessment Technique [3] and propose the

    Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product.

    TTCW consists of 14 binary tests organized into the original dimensions of

    Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative

    writers and implement a human assessment of 48 stories written either by

    professional authors or LLMs using TTCW. Our analysis shows that LLM-generated

    stories pass 3-10X less TTCW tests than stories written by professionals. In

    addition, we explore the use of LLMs as assessors to automate the TTCW

    evaluation, revealing that none of the LLMs positively correlate with the

    expert assessments.'
  arxivId: '2309.14556'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.HC
  authors: Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan,
    Chien-Sheng Wu
  created_at: '2025-01-04T14:48:36.292112'
  issue_number: 585
  issue_url: https://github.com/dmarx/papers-feed/issues/585
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T14:42:55.576Z'
  main_tex_file: null
  published_date: '2023-09-25T22:02:46Z'
  state: open
  title: "Art or Artifice? Large Language Models and the False Promise of\n  Creativity"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2309.14556
'2310.01889':
  abstract: 'Transformers have emerged as the architecture of choice for many

    state-of-the-art AI models, showcasing exceptional performance across a wide

    range of AI applications. However, the memory demands imposed by Transformers

    limit their ability to handle long sequences, thereby posing challenges in

    utilizing videos, actions, and other long-form sequences and modalities in

    complex environments. We present a novel approach, Ring Attention with

    Blockwise Transformers (Ring Attention), which leverages blockwise computation

    of self-attention and feedforward to distribute long sequences across multiple

    devices while fully overlapping the communication of key-value blocks with the

    computation of blockwise attention. Our approach enables training and inference

    of sequences that are up to device count times longer than those achievable by

    prior memory-efficient Transformers, without resorting to approximations or

    incurring additional communication and computation overheads. Extensive

    experiments on language modeling and reinforcement learning tasks demonstrate

    the effectiveness of our approach in allowing millions of tokens context size

    and improving performance.'
  arxivId: '2310.01889'
  arxiv_tags:
  - cs.CL
  authors: Hao Liu, Matei Zaharia, Pieter Abbeel
  created_at: '2025-01-04T06:52:27.607291'
  issue_number: 712
  issue_url: https://github.com/dmarx/papers-feed/issues/712
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:27.608961'
  last_visited: '2025-01-02T07:40:21.565Z'
  main_tex_file: null
  published_date: '2023-10-03T08:44:50Z'
  state: open
  title: Ring Attention with Blockwise Transformers for Near-Infinite Context
  total_reading_time_seconds: 40
  url: https://arxiv.org/abs/2310.01889
'2312.17127':
  abstract: "We study semantic models of probabilistic programming languages over\
    \ graphs,\nand establish a connection to graphons from graph theory and combinatorics.\
    \ We\nshow that every well-behaved equational theory for our graph probabilistic\n\
    programming language corresponds to a graphon, and conversely, every graphon\n\
    arises in this way.\n  We provide three constructions for showing that every graphon\
    \ arises from an\nequational theory. The first is an abstract construction, using\
    \ Markov\ncategories and monoidal indeterminates. The second and third are more\
    \ concrete.\nThe second is in terms of traditional measure theoretic probability,\
    \ which\ncovers 'black-and-white' graphons. The third is in terms of probability\
    \ monads\non the nominal sets of Gabbay and Pitts. Specifically, we use a variation\
    \ of\nnominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\\
    'enyi\ngraphons. In this way, we build new models of graph probabilistic programming\n\
    from graphons."
  arxivId: '2312.17127'
  arxiv_tags:
  - cs.PL
  - cs.LO
  - math.PR
  authors: Nathanael L. Ackerman, Cameron E. Freer, Younesse Kaddar, Jacek Karwowski,
    Sean K. Moss, Daniel M. Roy, Sam Staton, Hongseok Yang
  created_at: '2025-01-04T14:49:15.402966'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:15.404627'
  last_visited: '2024-12-29T19:53:34.293000+00:00'
  main_tex_file: null
  published_date: '2023-12-28T17:04:50Z'
  state: open
  title: "Probabilistic programming interfaces for random graphs: Markov\n  categories,\
    \ graphons, and nominal sets"
  total_reading_time_seconds: 10
  url: https://arxiv.org/abs/2312.17127
'2403.08540':
  abstract: 'Scaling laws are useful guides for derisking expensive training runs,
    as they

    predict performance of large models using cheaper, small-scale experiments.

    However, there remain gaps between current scaling studies and how language

    models are ultimately trained and evaluated. For instance, scaling is usually

    studied in the compute-optimal training regime (i.e., "Chinchilla optimal"

    regime). In contrast, models are often over-trained to reduce inference costs.

    Moreover, scaling laws mostly predict loss on next-token prediction, but models

    are usually compared on downstream task performance. To address both

    shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters

    trained with various numbers of tokens on three data distributions. First, we

    fit scaling laws that extrapolate in both the amount of over-training and the

    number of model parameters. This enables us to predict the validation loss of
    a

    1.4B parameter, 900B token run (i.e., 32$\times$ over-trained) and a 6.9B

    parameter, 138B token run (i.e., a compute-optimal run)$\unicode{x2014}$each

    from experiments that take 300$\times$ less compute. Second, we relate the

    perplexity of a language model to its downstream task performance by proposing

    a power law. We use this law to predict top-1 error averaged over downstream

    tasks for the two aforementioned models, using experiments that take 20$\times$

    less compute. Our experiments are available at

    https://github.com/mlfoundations/scaling.'
  arxivId: '2403.08540'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan,
    Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh,
    Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Luca Soldaini, Alexandros
    G. Dimakis, Gabriel Ilharco, Pang Wei Koh, Shuran Song, Thomas Kollar, Yair Carmon,
    Achal Dave, Reinhard Heckel, Niklas Muennighoff, Ludwig Schmidt
  created_at: '2025-01-04T06:53:18.643645'
  issue_number: 599
  issue_url: https://github.com/dmarx/papers-feed/issues/599
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.648664'
  last_visited: '2024-12-30T19:37:31.972Z'
  main_tex_file: null
  published_date: '2024-03-13T13:54:00Z'
  state: open
  title: "Language models scale reliably with over-training and on downstream\n  tasks"
  total_reading_time_seconds: 22
  url: https://arxiv.org/abs/2403.08540
'2403.14554':
  abstract: 'We propose Gaussian Frosting, a novel mesh-based representation for

    high-quality rendering and editing of complex 3D effects in real-time. Our

    approach builds on the recent 3D Gaussian Splatting framework, which optimizes

    a set of 3D Gaussians to approximate a radiance field from images. We propose

    first extracting a base mesh from Gaussians during optimization, then building

    and refining an adaptive layer of Gaussians with a variable thickness around

    the mesh to better capture the fine details and volumetric effects near the

    surface, such as hair or grass. We call this layer Gaussian Frosting, as it

    resembles a coating of frosting on a cake. The fuzzier the material, the

    thicker the frosting. We also introduce a parameterization of the Gaussians to

    enforce them to stay inside the frosting layer and automatically adjust their

    parameters when deforming, rescaling, editing or animating the mesh. Our

    representation allows for efficient rendering using Gaussian splatting, as well

    as editing and animation by modifying the base mesh. We demonstrate the

    effectiveness of our method on various synthetic and real scenes, and show that

    it outperforms existing surface-based approaches. We will release our code and

    a web-based viewer as additional contributions. Our project page is the

    following: https://anttwo.github.io/frosting/'
  arxivId: '2403.14554'
  arxiv_tags:
  - cs.CV
  - cs.GR
  authors: Antoine Guédon, Vincent Lepetit
  created_at: '2025-01-04T06:52:06.631074'
  issue_number: 758
  issue_url: https://github.com/dmarx/papers-feed/issues/758
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:06.631879'
  last_visited: '2025-01-03T02:52:11.974Z'
  main_tex_file: null
  published_date: '2024-03-21T16:53:03Z'
  state: open
  title: "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time\n  Rendering"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2403.14554
'2405.04517':
  abstract: 'In the 1990s, the constant error carousel and gating were introduced
    as the

    central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have

    stood the test of time and contributed to numerous deep learning success

    stories, in particular they constituted the first Large Language Models (LLMs).

    However, the advent of the Transformer technology with parallelizable

    self-attention at its core marked the dawn of a new era, outpacing LSTMs at

    scale. We now raise a simple question: How far do we get in language modeling

    when scaling LSTMs to billions of parameters, leveraging the latest techniques

    from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we

    introduce exponential gating with appropriate normalization and stabilization

    techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM

    with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that

    is fully parallelizable with a matrix memory and a covariance update rule.

    Integrating these LSTM extensions into residual block backbones yields xLSTM

    blocks that are then residually stacked into xLSTM architectures. Exponential

    gating and modified memory structures boost xLSTM capabilities to perform

    favorably when compared to state-of-the-art Transformers and State Space

    Models, both in performance and scaling.'
  arxivId: '2405.04517'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - stat.ML
  authors: Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra
    Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter
  created_at: '2025-01-04T06:53:15.665066'
  issue_number: 649
  issue_url: https://github.com/dmarx/papers-feed/issues/649
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:15.667071'
  last_visited: '2024-12-30T20:10:41.007000+00:00'
  main_tex_file: null
  published_date: '2024-05-07T17:50:21Z'
  state: open
  title: 'xLSTM: Extended Long Short-Term Memory'
  total_reading_time_seconds: 31
  url: https://arxiv.org/abs/2405.04517
'2406.01981':
  abstract: 'The size of large language models (LLMs) has scaled dramatically in recent

    years and their computational and data requirements have surged

    correspondingly. State-of-the-art language models, even at relatively smaller

    sizes, typically require training on at least a trillion tokens. This rapid

    advancement has eclipsed the growth of open-source datasets available for

    large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset),

    a dataset under a permissive license comprising 1.3 trillion tokens, assembled

    by integrating several major respected open-source datasets into a single,

    high-quality corpus. We apply rigorous filtering and deduplication processes,

    both within and across datasets, to maintain and enhance the quality derived

    from the original datasets. Our evaluations show that Zyda not only competes

    favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but

    also substantially improves the performance of comparable models from the

    Pythia suite. Our rigorous data processing methods significantly enhance Zyda''s

    effectiveness, outperforming even the best of its constituent datasets when

    used independently.'
  arxivId: '2406.01981'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Yury Tokpanov, Beren Millidge, Paolo Glorioso, Jonathan Pilault, Adam Ibrahim,
    James Whittington, Quentin Anthony
  created_at: '2025-01-04T06:52:48.608606'
  issue_number: 641
  issue_url: https://github.com/dmarx/papers-feed/issues/641
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.650339'
  last_visited: '2024-12-30T20:04:46.858Z'
  main_tex_file: null
  published_date: '2024-06-04T05:47:17Z'
  state: open
  title: 'Zyda: A 1.3T Dataset for Open Language Modeling'
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2406.01981
'2406.09162':
  abstract: 'Recent advancements in image generation have enabled the creation of

    high-quality images from text conditions. However, when facing multi-modal

    conditions, such as text combined with reference appearances, existing methods

    struggle to balance multiple conditions effectively, typically showing a

    preference for one modality over others. To address this challenge, we

    introduce EMMA, a novel image generation model accepting multi-modal prompts

    built upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA

    seamlessly incorporates additional modalities alongside text to guide image

    generation through an innovative Multi-modal Feature Connector design, which

    effectively integrates textual and supplementary modal information using a

    special attention mechanism. By freezing all parameters in the original T2I

    diffusion model and only adjusting some additional layers, we reveal an

    interesting finding that the pre-trained T2I diffusion model can secretly

    accept multi-modal prompts. This interesting property facilitates easy

    adaptation to different existing frameworks, making EMMA a flexible and

    effective tool for producing personalized and context-aware images and even

    videos. Additionally, we introduce a strategy to assemble learned EMMA modules

    to produce images conditioned on multiple modalities simultaneously,

    eliminating the need for additional training with mixed multi-modal prompts.

    Extensive experiments demonstrate the effectiveness of EMMA in maintaining high

    fidelity and detail in generated images, showcasing its potential as a robust

    solution for advanced multi-modal conditional image generation tasks.'
  arxivId: '2406.09162'
  arxiv_tags:
  - cs.CV
  authors: Yucheng Han, Rui Wang, Chi Zhang, Juntao Hu, Pei Cheng, Bin Fu, Hanwang
    Zhang
  created_at: '2025-01-04T14:49:45.263062'
  issue_number: 410
  issue_url: https://github.com/dmarx/papers-feed/issues/410
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:45.266317'
  last_visited: '2024-12-28T08:40:07.461Z'
  main_tex_file: null
  published_date: '2024-06-13T14:26:43Z'
  state: open
  title: "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal\n\
    \  Prompts"
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2406.09162
'2406.10670':
  abstract: "Selecting high-quality data for pre-training is crucial in shaping the\n\
    downstream task performance of language models. A major challenge lies in\nidentifying\
    \ this optimal subset, a problem generally considered intractable,\nthus necessitating\
    \ scalable and effective heuristics. In this work, we propose\na data selection\
    \ method, CoLoR-Filter (Conditional Loss Reduction Filtering),\nwhich leverages\
    \ an empirical Bayes-inspired approach to derive a simple and\ncomputationally\
    \ efficient selection criterion based on the relative loss values\nof two auxiliary\
    \ models.\n  In addition to the modeling rationale, we evaluate CoLoR-Filter empirically\n\
    on two language modeling tasks: (1) selecting data from C4 for domain\nadaptation\
    \ to evaluation on Books and (2) selecting data from C4 for a suite of\ndownstream\
    \ multiple-choice question answering tasks. We demonstrate favorable\nscaling\
    \ both as we subselect more aggressively and using small auxiliary models\nto\
    \ select data for large target models. As one headline result, CoLoR-Filter\n\
    data selected using a pair of 150m parameter auxiliary models can train a 1.2b\n\
    parameter target model to match a 1.2b parameter model trained on 25b randomly\n\
    selected tokens with 25x less data for Books and 11x less data for the\ndownstream\
    \ tasks.\n  Code: https://github.com/davidbrandfonbrener/color-filter-olmo\n \
    \ Filtered data:\nhttps://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4"
  arxivId: '2406.10670'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CL
  authors: David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan Richard Schwarz,
    Sham Kakade
  created_at: '2025-01-04T06:53:27.614374'
  issue_number: 630
  issue_url: https://github.com/dmarx/papers-feed/issues/630
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:27.615183'
  last_visited: '2024-12-30T20:02:17.082Z'
  main_tex_file: null
  published_date: '2024-06-15T15:28:02Z'
  state: open
  title: "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language\n\
    \  Model Pre-training"
  total_reading_time_seconds: 7
  url: https://arxiv.org/abs/2406.10670
'2407.01492':
  abstract: 'The data mixture for large language model pre-training significantly
    impacts

    performance, yet how to determine an effective mixture remains unclear. We

    propose RegMix to automatically identify a high-performing data mixture by

    formulating it as a regression task. RegMix involves training a set of small

    models with diverse data mixtures and fitting a regression model to predict

    their performance given their respective mixtures. With the fitted regression

    model, we simulate the top-ranked mixture and use it to train a large-scale

    model with orders of magnitude more compute. To empirically validate RegMix, we

    train 512 models with 1M parameters for 1B tokens of different mixtures to fit

    the regression model and find the optimal mixture. Using this mixture we train

    a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we

    find performs best among 64 candidate 1B parameter models with other mixtures.

    Further, our method demonstrates superior performance compared to human

    selection and achieves results that match or surpass DoReMi, while utilizing

    only 10% of the compute budget. Our experiments also show that (1) Data

    mixtures significantly impact performance with single-task performance

    variations of up to 14.6%; (2) Web corpora rather than data perceived as

    high-quality like Wikipedia have the strongest positive correlation with

    downstream performance; (3) Domains interact in complex ways often

    contradicting common sense, thus automatic approaches like RegMix are needed;

    (4) Data mixture effects transcend scaling laws, and our approach captures the

    complexity by considering all domains together. Our code is available at

    https://github.com/sail-sg/regmix.'
  arxivId: '2407.01492'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou,
    Tianyu Pang, Jing Jiang, Min Lin
  created_at: '2025-01-04T06:53:09.609827'
  issue_number: 654
  issue_url: https://github.com/dmarx/papers-feed/issues/654
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:09.611828'
  last_visited: '2024-12-30T20:17:16.629000+00:00'
  main_tex_file: null
  published_date: '2024-07-01T17:31:03Z'
  state: open
  title: 'RegMix: Data Mixture as Regression for Language Model Pre-training'
  total_reading_time_seconds: 42
  url: https://arxiv.org/abs/2407.01492
'2407.08608':
  abstract: 'Attention, as a core layer of the ubiquitous Transformer architecture,
    is the

    bottleneck for large language models and long-context applications.

    FlashAttention elaborated an approach to speed up attention on GPUs through

    minimizing memory reads/writes. However, it has yet to take advantage of new

    capabilities present in recent hardware, with FlashAttention-2 achieving only

    35% utilization on the H100 GPU. We develop three main techniques to speed up

    attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to

    (1) overlap overall computation and data movement via warp-specialization and

    (2) interleave block-wise matmul and softmax operations, and (3) block

    quantization and incoherent processing that leverages hardware support for FP8

    low-precision. We demonstrate that our method, FlashAttention-3, achieves

    speedup on H100 GPUs by 1.5-2.0$\times$ with FP16 reaching up to 740 TFLOPs/s

    (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate

    that FP8 FlashAttention-3 achieves 2.6$\times$ lower numerical error than a

    baseline FP8 attention.'
  arxivId: '2407.08608'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani,
    Tri Dao
  created_at: '2025-01-04T14:49:48.246613'
  issue_number: 408
  issue_url: https://github.com/dmarx/papers-feed/issues/408
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:48.247503'
  last_visited: '2024-12-28T07:33:54.849Z'
  main_tex_file: null
  published_date: '2024-07-11T15:44:48Z'
  state: open
  title: "FlashAttention-3: Fast and Accurate Attention with Asynchrony and\n  Low-precision"
  total_reading_time_seconds: 13
  url: https://arxiv.org/abs/2407.08608
'2407.21783':
  abstract: 'Modern artificial intelligence (AI) systems are powered by foundation
    models.

    This paper presents a new set of foundation models, called Llama 3. It is a

    herd of language models that natively support multilinguality, coding,

    reasoning, and tool usage. Our largest model is a dense Transformer with 405B

    parameters and a context window of up to 128K tokens. This paper presents an

    extensive empirical evaluation of Llama 3. We find that Llama 3 delivers

    comparable quality to leading language models such as GPT-4 on a plethora of

    tasks. We publicly release Llama 3, including pre-trained and post-trained

    versions of the 405B parameter language model and our Llama Guard 3 model for

    input and output safety. The paper also presents the results of experiments in

    which we integrate image, video, and speech capabilities into Llama 3 via a

    compositional approach. We observe this approach performs competitively with

    the state-of-the-art on image, video, and speech recognition tasks. The

    resulting models are not yet being broadly released as they are still under

    development.'
  arxivId: '2407.21783'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.CV
  authors: Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan,
    Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra,
    Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien
    Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh
    Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,
    Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,
    Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
    Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan,
    Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy,
    Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán,
    Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai,
    Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah
    Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann,
    Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana
    Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer
    Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen
    Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca,
    Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad,
    Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid
    El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia,
    Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins,
    Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de
    Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin
    Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie
    Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes
    Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier
    Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic,
    Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,
    Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,
    Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit
    Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross
    Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa,
    Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan
    Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang,
    Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot,
    Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha,
    Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao,
    Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez,
    Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei
    Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang
    Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle
    Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue
    Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos,
    Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya
    Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg,
    Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus,
    Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton,
    Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita
    Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman,
    Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth
    Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock,
    Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
    Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao
    Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer,
    Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins,
    David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem
    Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine
    Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman,
    Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian,
    Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide,
    Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern,
    Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,
    Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph,
    Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor
    Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman,
    James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff
    Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul,
    Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard,
    Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou
    U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan,
    Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang,
    Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng
    Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani,
    Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim
    Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer,
    Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan,
    Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad
    Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata
    Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich
    Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar,
    Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,
    Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,
    Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham
    Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah
    Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin
    Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha
    Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh
    Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy
    Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang,
    Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen,
    Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng,
    Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal
    Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun
    Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi,
    Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu,
    Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,
    Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan
    Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi,
    Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao,
    Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick,
    Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, Zhiyu Ma
  created_at: '2025-01-04T06:52:42.685103'
  issue_number: 702
  issue_url: https://github.com/dmarx/papers-feed/issues/702
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T21:58:38.440Z'
  main_tex_file: null
  published_date: '2024-07-31T17:54:27Z'
  state: open
  title: The Llama 3 Herd of Models
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2407.21783
'2407.21787':
  abstract: 'Scaling the amount of compute used to train language models has dramatically

    improved their capabilities. However, when it comes to inference, we often

    limit models to making only one attempt at a problem. Here, we explore

    inference compute as another axis for scaling, using the simple technique of

    repeatedly sampling candidate solutions from a model. Across multiple tasks and

    models, we observe that coverage -- the fraction of problems that are solved by

    any generated sample -- scales with the number of samples over four orders of

    magnitude. Interestingly, the relationship between coverage and the number of

    samples is often log-linear and can be modelled with an exponentiated power

    law, suggesting the existence of inference-time scaling laws. In domains like

    coding and formal proofs, where answers can be automatically verified, these

    increases in coverage directly translate into improved performance. When we

    apply repeated sampling to SWE-bench Lite, the fraction of issues solved with

    DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250

    samples, outperforming the single-sample state-of-the-art of 43%. In domains

    without automatic verifiers, we find that common methods for picking from a

    sample collection (majority voting and reward models) plateau beyond several

    hundred samples and fail to fully scale with the sample budget.'
  arxivId: '2407.21787'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le,
    Christopher Ré, Azalia Mirhoseini
  created_at: '2025-01-04T14:49:12.234492'
  issue_number: 513
  issue_url: https://github.com/dmarx/papers-feed/issues/513
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:12.237648'
  last_visited: '2024-12-29T20:31:06.783Z'
  main_tex_file: null
  published_date: '2024-07-31T17:57:25Z'
  state: open
  title: 'Large Language Monkeys: Scaling Inference Compute with Repeated Sampling'
  total_reading_time_seconds: 17
  url: https://arxiv.org/abs/2407.21787
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is

    a critical step towards building generally self-improving agents that can

    operate on open-ended natural language. In this paper, we study the scaling of

    inference-time computation in LLMs, with a focus on answering the question: if

    an LLM is allowed to use a fixed but non-trivial amount of inference-time

    compute, how much can it improve its performance on a challenging prompt?

    Answering this question has implications not only on the achievable performance

    of LLMs, but also on the future of LLM pretraining and how one should tradeoff

    inference-time and pre-training compute. Despite its importance, little

    research attempted to understand the scaling behaviors of various test-time

    inference methods. Moreover, current work largely provides negative results for

    a number of these strategies. In this work, we analyze two primary mechanisms

    to scale test-time computation: (1) searching against dense, process-based

    verifier reward models; and (2) updating the model''s distribution over a

    response adaptively, given the prompt at test time. We find that in both cases,

    the effectiveness of different approaches to scaling test-time compute

    critically varies depending on the difficulty of the prompt. This observation

    motivates applying a "compute-optimal" scaling strategy, which acts to most

    effectively allocate test-time compute adaptively per prompt. Using this

    compute-optimal strategy, we can improve the efficiency of test-time compute

    scaling by more than 4x compared to a best-of-N baseline. Additionally, in a

    FLOPs-matched evaluation, we find that on problems where a smaller base model

    attains somewhat non-trivial success rates, test-time compute can be used to

    outperform a 14x larger model.'
  arxivId: '2408.03314'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2025-01-04T14:49:09.218184'
  issue_number: 515
  issue_url: https://github.com/dmarx/papers-feed/issues/515
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:12.236101'
  last_visited: '2024-12-29T20:31:12.333Z'
  main_tex_file: null
  published_date: '2024-08-06T17:35:05Z'
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_seconds: 39
  url: https://arxiv.org/abs/2408.03314
'2408.06072':
  abstract: 'We present CogVideoX, a large-scale text-to-video generation model based
    on

    diffusion transformer, which can generate 10-second continuous videos aligned

    with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360

    pixels. Previous video generation models often had limited movement and short

    durations, and is difficult to generate videos with coherent narratives based

    on text. We propose several designs to address these issues. First, we propose

    a 3D Variational Autoencoder (VAE) to compress videos along both spatial and

    temporal dimensions, to improve both compression rate and video fidelity.

    Second, to improve the text-video alignment, we propose an expert transformer

    with the expert adaptive LayerNorm to facilitate the deep fusion between the

    two modalities. Third, by employing a progressive training and multi-resolution

    frame pack technique, CogVideoX is adept at producing coherent, long-duration,

    different shape videos characterized by significant motions. In addition, we

    develop an effective text-video data processing pipeline that includes various

    data preprocessing strategies and a video captioning method, greatly

    contributing to the generation quality and semantic alignment. Results show

    that CogVideoX demonstrates state-of-the-art performance across both multiple

    machine metrics and human evaluations. The model weight of both 3D Causal VAE,

    Video caption model and CogVideoX are publicly available at

    https://github.com/THUDM/CogVideo.'
  arxivId: '2408.06072'
  arxiv_tags:
  - cs.CV
  authors: Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng
    Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu,
    Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, Jie Tang
  created_at: '2025-01-04T06:53:30.623623'
  issue_number: 597
  issue_url: https://github.com/dmarx/papers-feed/issues/597
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:30.624413'
  last_visited: '2024-12-30T15:18:13.864Z'
  main_tex_file: null
  published_date: '2024-08-12T11:47:11Z'
  state: open
  title: 'CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer'
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/2408.06072
'2409.04431':
  abstract: 'Attention is a key part of the transformer architecture. It is a

    sequence-to-sequence mapping that transforms each sequence element into a

    weighted sum of values. The weights are typically obtained as the softmax of

    dot products between keys and queries. Recent work has explored alternatives to

    softmax attention in transformers, such as ReLU and sigmoid activations. In

    this work, we revisit sigmoid attention and conduct an in-depth theoretical and

    empirical analysis. Theoretically, we prove that transformers with sigmoid

    attention are universal function approximators and benefit from improved

    regularity compared to softmax attention. Through detailed empirical analysis,

    we identify stabilization of large initial attention norms during the early

    stages of training as a crucial factor for the successful training of models

    with sigmoid attention, outperforming prior attempts. We also introduce

    FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid

    attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100

    GPUs. Experiments across language, vision, and speech show that properly

    normalized sigmoid attention matches the strong performance of softmax

    attention on a wide range of domains and scales, which previous attempts at

    sigmoid attention were unable to fully achieve. Our work unifies prior art and

    establishes best practices for sigmoid attention as a drop-in softmax

    replacement in transformers.'
  arxivId: '2409.04431'
  arxiv_tags:
  - cs.LG
  authors: Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge,
    Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, Russ
    Webb
  created_at: '2025-01-04T06:52:15.619463'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:15.620261'
  last_visited: '2025-01-02T19:38:28.512000+00:00'
  main_tex_file: null
  published_date: '2024-09-06T17:53:26Z'
  state: open
  title: Theory, Analysis, and Best Practices for Sigmoid Self-Attention
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2409.04431
'2409.05816':
  abstract: 'Quality pretraining data is often seen as the key to high-performance

    language models. However, progress in understanding pretraining data has been

    slow due to the costly pretraining runs required for data selection

    experiments. We present a framework that avoids these costs and selects

    high-quality pretraining data without any LLM training of our own. Our work is

    based on a simple observation: LLM losses on many pretraining texts are

    correlated with downstream benchmark performance, and selecting

    high-correlation documents is an effective pretraining data selection method.

    We build a new statistical framework for data selection centered around

    estimates of perplexity-benchmark correlations and perform data selection using

    a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of

    thousands of web domains. In controlled pretraining experiments at the 160M

    parameter scale on 8 benchmarks, our approach outperforms DSIR on every

    benchmark, while matching the best data selector found in DataComp-LM, a

    hand-engineered bigram classifier.'
  arxivId: '2409.05816'
  arxiv_tags:
  - cs.CL
  - cs.LG
  - stat.ML
  authors: Tristan Thrush, Christopher Potts, Tatsunori Hashimoto
  created_at: '2025-01-04T06:53:00.603803'
  issue_number: 658
  issue_url: https://github.com/dmarx/papers-feed/issues/658
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:00.604590'
  last_visited: '2024-12-30T20:20:51.220Z'
  main_tex_file: null
  published_date: '2024-09-09T17:23:29Z'
  state: open
  title: Improving Pretraining Data Using Perplexity Correlations
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2409.05816
'2409.08514':
  abstract: 'Audio restoration has become increasingly significant in modern society,
    not

    only due to the demand for high-quality auditory experiences enabled by

    advanced playback devices, but also because the growing capabilities of

    generative audio models necessitate high-fidelity audio. Typically, audio

    restoration is defined as a task of predicting undistorted audio from damaged

    input, often trained using a GAN framework to balance perception and

    distortion. Since audio degradation is primarily concentrated in mid- and

    high-frequency ranges, especially due to codecs, a key challenge lies in

    designing a generator capable of preserving low-frequency information while

    accurately reconstructing high-quality mid- and high-frequency content.

    Inspired by recent advancements in high-sample-rate music separation, speech

    enhancement, and audio codec models, we propose Apollo, a generative model

    designed for high-sample-rate audio restoration. Apollo employs an explicit

    frequency band split module to model the relationships between different

    frequency bands, allowing for more coherent and higher-quality restored audio.

    Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently

    outperforms existing SR-GAN models across various bit rates and music genres,

    particularly excelling in complex scenarios involving mixtures of multiple

    instruments and vocals. Apollo significantly improves music restoration quality

    while maintaining computational efficiency. The source code for Apollo is

    publicly available at https://github.com/JusperLee/Apollo.'
  arxivId: '2409.08514'
  arxiv_tags:
  - cs.SD
  - cs.AI
  - eess.AS
  authors: Kai Li, Yi Luo
  created_at: '2025-01-04T14:48:42.234049'
  issue_number: 576
  issue_url: https://github.com/dmarx/papers-feed/issues/576
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:42.234855'
  last_visited: '2024-12-30T05:02:38.295Z'
  main_tex_file: null
  published_date: '2024-09-13T03:25:34Z'
  state: open
  title: 'Apollo: Band-sequence Modeling for High-Quality Audio Restoration'
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2409.08514
'2409.13731':
  abstract: 'The recently developed retrieval-augmented generation (RAG) technology
    has

    enabled the efficient construction of domain-specific applications. However, it

    also has limitations, including the gap between vector similarity and the

    relevance of knowledge reasoning, as well as insensitivity to knowledge logic,

    such as numerical values, temporal relations, expert rules, and others, which

    hinder the effectiveness of professional knowledge services. In this work, we

    introduce a professional domain knowledge service framework called Knowledge

    Augmented Generation (KAG). KAG is designed to address the aforementioned

    challenges with the motivation of making full use of the advantages of

    knowledge graph(KG) and vector retrieval, and to improve generation and

    reasoning performance by bidirectionally enhancing large language models (LLMs)

    and KGs through five key aspects: (1) LLM-friendly knowledge representation,

    (2) mutual-indexing between knowledge graphs and original chunks, (3)

    logical-form-guided hybrid reasoning engine, (4) knowledge alignment with

    semantic reasoning, and (5) model capability enhancement for KAG. We compared

    KAG with existing RAG methods in multihop question answering and found that it

    significantly outperforms state-of-theart methods, achieving a relative

    improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We

    have successfully applied KAG to two professional knowledge Q&A tasks of Ant

    Group, including E-Government Q&A and E-Health Q&A, achieving significant

    improvement in professionalism compared to RAG methods.'
  arxivId: '2409.13731'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong,
    Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu,
    Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou
  created_at: '2025-01-04T06:52:18.611027'
  issue_number: 720
  issue_url: https://github.com/dmarx/papers-feed/issues/720
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T15:07:25.587Z'
  main_tex_file: null
  published_date: '2024-09-10T02:00:28Z'
  state: open
  title: "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.13731
'2409.16986':
  abstract: 'Data selection is of great significance in pre-training large language

    models, given the variation in quality within the large-scale available

    training corpora. To achieve this, researchers are currently investigating the

    use of data influence to measure the importance of data instances, $i.e.,$ a

    high influence score indicates that incorporating this instance to the training

    set is likely to enhance the model performance. Consequently, they select the

    top-$k$ instances with the highest scores. However, this approach has several

    limitations. (1) Computing the influence of all available data is

    time-consuming. (2) The selected data instances are not diverse enough, which

    may hinder the pre-trained model''s ability to generalize effectively to various

    downstream tasks. In this paper, we introduce \texttt{Quad}, a data selection

    approach that considers both quality and diversity by using data influence to

    achieve state-of-the-art pre-training results. In particular, noting that

    attention layers capture extensive semantic details, we have adapted the

    accelerated $iHVP$ computation methods for attention layers, enhancing our

    ability to evaluate the influence of data, $i.e.,$ its quality. For the

    diversity, \texttt{Quad} clusters the dataset into similar data instances

    within each cluster and diverse instances across different clusters. For each

    cluster, if we opt to select data from it, we take some samples to evaluate the

    influence to prevent processing all instances. To determine which clusters to

    select, we utilize the classic Multi-Armed Bandit method, treating each cluster

    as an arm. This approach favors clusters with highly influential instances

    (ensuring high quality) or clusters that have been selected less frequently

    (ensuring diversity), thereby well balancing between quality and diversity.'
  arxivId: '2409.16986'
  arxiv_tags:
  - cs.AI
  authors: Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin
    Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ju Fan, Ye Yuan, Guoren Wang, Conghui
    He
  created_at: '2025-01-04T06:52:54.606152'
  issue_number: 637
  issue_url: https://github.com/dmarx/papers-feed/issues/637
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.651949'
  last_visited: '2024-12-30T20:04:00.391Z'
  main_tex_file: null
  published_date: '2024-09-25T14:49:29Z'
  state: open
  title: "Harnessing Diversity for Important Data Selection in Pretraining Large\n\
    \  Language Models"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2409.16986
'2409.19606':
  abstract: 'We present hyper-connections, a simple yet effective method that can
    serve as

    an alternative to residual connections. This approach specifically addresses

    common drawbacks observed in residual connection variants, such as the seesaw

    effect between gradient vanishing and representation collapse. Theoretically,

    hyper-connections allow the network to adjust the strength of connections

    between features at different depths and dynamically rearrange layers. We

    conduct experiments focusing on the pre-training of large language models,

    including dense and sparse models, where hyper-connections show significant

    performance improvements over residual connections. Additional experiments

    conducted on vision tasks also demonstrate similar improvements. We anticipate

    that this method will be broadly applicable and beneficial across a wide range

    of AI problems.'
  arxivId: '2409.19606'
  arxiv_tags:
  - cs.LG
  - cs.CL
  - cs.CV
  - cs.NE
  authors: Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu,
    Qiyang Min, Xun Zhou
  created_at: '2025-01-04T06:52:21.616669'
  issue_number: 718
  issue_url: https://github.com/dmarx/papers-feed/issues/718
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T08:05:54.690Z'
  main_tex_file: null
  published_date: '2024-09-29T07:57:07Z'
  state: open
  title: Hyper-Connections
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.19606
'2410.08800':
  abstract: 'This paper presents a comprehensive overview of the data preparation
    pipeline

    developed for the OpenGPT-X project, a large-scale initiative aimed at creating

    open and high-performance multilingual large language models (LLMs). The

    project goal is to deliver models that cover all major European languages, with

    a particular focus on real-world applications within the European Union. We

    explain all data processing steps, starting with the data selection and

    requirement definition to the preparation of the final datasets for model

    training. We distinguish between curated data and web data, as each of these

    categories is handled by distinct pipelines, with curated data undergoing

    minimal filtering and web data requiring extensive filtering and deduplication.

    This distinction guided the development of specialized algorithmic solutions

    for both pipelines. In addition to describing the processing methodologies, we

    provide an in-depth analysis of the datasets, increasing transparency and

    alignment with European data regulations. Finally, we share key insights and

    challenges faced during the project, offering recommendations for future

    endeavors in large-scale multilingual data preparation for LLMs.'
  arxivId: '2410.08800'
  arxiv_tags:
  - cs.CL
  - H.3.1; I.2.7
  authors: Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick, Lennard Helmer,
    Benny Jörg Stein, Pavel Denisov, Qasid Saleem, Michael Fromm, Mehdi Ali, Richard
    Rutmann, Farzad Naderi, Mohamad Saif Agy, Alexander Schwirjow, Fabian Küch, Luzian
    Hahn, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Dennis Wegener, Nicolas
    Flores-Herr, Joachim Köhler, Johannes Leveling
  created_at: '2025-01-04T06:52:45.621160'
  issue_number: 645
  issue_url: https://github.com/dmarx/papers-feed/issues/645
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:45.622955'
  last_visited: '2024-12-30T20:06:07.375Z'
  main_tex_file: null
  published_date: '2024-10-11T13:34:24Z'
  state: open
  title: Data Processing for the OpenGPT-X Model Family
  total_reading_time_seconds: 47
  url: https://arxiv.org/abs/2410.08800
'2410.10792':
  abstract: 'Generative models transform random noise into images; their inversion
    aims to

    transform images back to structured noise for recovery and editing. This paper

    addresses two key tasks: (i) inversion and (ii) editing of a real image using

    stochastic equivalents of rectified flow models (such as Flux). Although

    Diffusion Models (DMs) have recently dominated the field of generative modeling

    for images, their inversion presents faithfulness and editability challenges

    due to nonlinearities in drift and diffusion. Existing state-of-the-art DM

    inversion approaches rely on training of additional parameters or test-time

    optimization of latent variables; both are expensive in practice. Rectified

    Flows (RFs) offer a promising alternative to diffusion models, yet their

    inversion has been underexplored. We propose RF inversion using dynamic optimal

    control derived via a linear quadratic regulator. We prove that the resulting

    vector field is equivalent to a rectified stochastic differential equation.

    Additionally, we extend our framework to design a stochastic sampler for Flux.

    Our inversion method allows for state-of-the-art performance in zero-shot

    inversion and editing, outperforming prior works in stroke-to-image synthesis

    and semantic image editing, with large-scale human evaluations confirming user

    preference.'
  arxivId: '2410.10792'
  arxiv_tags:
  - cs.LG
  - cs.CV
  - stat.ML
  authors: Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai,
    Wen-Sheng Chu
  created_at: '2025-01-04T14:48:30.274158'
  issue_number: 590
  issue_url: https://github.com/dmarx/papers-feed/issues/590
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T15:14:34.707Z'
  main_tex_file: null
  published_date: '2024-10-14T17:56:24Z'
  state: open
  title: "Semantic Image Inversion and Editing using Rectified Stochastic\n  Differential\
    \ Equations"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.10792
'2410.13835':
  abstract: "Practitioners have consistently observed three puzzling phenomena in\n\
    transformer-based large language models (LLMs): attention sinks, value-state\n\
    drains, and residual-state peaks, collectively referred to as extreme-token\n\
    phenomena. These phenomena are characterized by certain so-called \"sink tokens\"\
    \nreceiving disproportionately high attention weights, exhibiting significantly\n\
    smaller value states, and having much larger residual-state norms than those of\n\
    other tokens. These extreme tokens give rise to various challenges in LLM\ninference,\
    \ quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token\
    \ phenomena. First, we show\nthat these phenomena arise in very simple architectures\
    \ -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy\
    \ (BB) task.\nIn this setting, we identify an active-dormant mechanism, where\
    \ attention heads\nbecome sinks for specific input domains while remaining non-sinks\
    \ for others.\nOur theoretical analysis of the training dynamics reveals that\
    \ these phenomena\nare driven by a mutual reinforcement mechanism. Building on\
    \ these insights, we\npropose strategies to mitigate extreme-token phenomena during\
    \ pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next,\
    \ we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing\
    \ that many\nattention heads exhibit a similar active-dormant mechanism as in\
    \ the BB task,\nand that the mutual reinforcement mechanism also governs the emergence\
    \ of\nextreme-token phenomena during LLM pretraining. Our results reveal that\
    \ many of\nthe static and dynamic properties of extreme-token phenomena predicted\
    \ by the\nBB task align with observations in pretrained LLMs."
  arxivId: '2410.13835'
  arxiv_tags:
  - cs.LG
  authors: Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei
  created_at: '2025-01-04T06:52:39.613983'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:39.614755'
  last_visited: '2024-12-30T22:17:13.815000+00:00'
  main_tex_file: null
  published_date: '2024-10-17T17:54:06Z'
  state: open
  title: "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token\
    \ Phenomena in LLMs"
  total_reading_time_seconds: 16
  url: https://arxiv.org/abs/2410.13835
'2410.21265':
  abstract: 'An old idea in optimization theory says that since the gradient is a
    dual

    vector it may not be subtracted from the weights without first being mapped to

    the primal space where the weights reside. We take this idea seriously in this

    paper and construct such a duality map for general neural networks. Our map,

    which we call modular dualization, forms a unifying theoretical basis for

    training algorithms that are a) fast and b) scalable. Modular dualization

    involves first assigning operator norms to layers based on the semantics of

    each layer, and then using these layerwise norms to recursively induce a

    duality map on the weight space of the full neural architecture. We conclude by

    deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers

    -- the latter two methods are based on a rectangular Newton-Schulz iteration

    (Kovarik, 1970; Bj\"orck & Bowie, 1971). A variant of our methods was used to

    set speed records for training NanoGPT. Overall, we hope that our theory of

    modular duality will yield a next generation of fast and scalable optimizers

    for general neural architectures.'
  arxivId: '2410.21265'
  arxiv_tags:
  - cs.LG
  - cs.NE
  - stat.ML
  authors: Jeremy Bernstein, Laker Newhouse
  created_at: '2025-01-04T06:52:09.651323'
  issue_number: 757
  issue_url: https://github.com/dmarx/papers-feed/issues/757
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T19:43:53.760Z'
  main_tex_file: null
  published_date: '2024-10-28T17:57:31Z'
  state: open
  title: Modular Duality in Deep Learning
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.21265
'2411.04330':
  abstract: 'Low precision training and inference affect both the quality and cost
    of

    language models, but current scaling laws do not account for this. In this

    work, we devise "precision-aware" scaling laws for both training and inference.

    We propose that training in lower precision reduces the model''s "effective

    parameter count," allowing us to predict the additional loss incurred from

    training in low precision and post-train quantization. For inference, we find

    that the degradation introduced by post-training quantization increases as

    models are trained on more data, eventually making additional pretraining data

    actively harmful. For training, our scaling laws allow us to predict the loss

    of a model with different parts in different precisions, and suggest that

    training larger models in lower precision may be compute optimal. We unify the

    scaling laws for post and pretraining quantization to arrive at a single

    functional form that predicts degradation from training and inference in varied

    precisions. We fit on over 465 pretraining runs and validate our predictions on

    model sizes up to 1.7B parameters trained on up to 26B tokens.'
  arxivId: '2411.04330'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas
    Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan
  created_at: '2025-01-04T06:53:12.611718'
  issue_number: 651
  issue_url: https://github.com/dmarx/papers-feed/issues/651
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:12.612468'
  last_visited: '2024-12-30T20:16:37.184Z'
  main_tex_file: null
  published_date: '2024-11-07T00:10:10Z'
  state: open
  title: Scaling Laws for Precision
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2411.04330
'2411.06068':
  abstract: 'In this technical report, we present Zyda-2: a five trillion token dataset

    for language model pretraining. Zyda-2 was used to train our Zamba2 series of

    models which are state-of-the-art for their weight class. We build Zyda-2 by

    collating high-quality open-source tokens such as FineWeb and DCLM, then

    distilling them to the highest-quality subset via cross-deduplication and

    model-based quality filtering. Zyda-2 is released under a permissive open

    license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2'
  arxivId: '2411.06068'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge
  created_at: '2025-01-04T06:52:36.611803'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:36.612646'
  last_visited: '2024-12-30T22:18:39.627000+00:00'
  main_tex_file: null
  published_date: '2024-11-09T04:57:41Z'
  state: open
  title: 'Zyda-2: a 5 Trillion Token High-Quality Dataset'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2411.06068
'2411.12372':
  abstract: 'Large language models are increasingly becoming a cornerstone technology
    in

    artificial intelligence, the sciences, and society as a whole, yet the optimal

    strategies for dataset composition and filtering remain largely elusive. Many

    of the top-performing models lack transparency in their dataset curation and

    model development processes, posing an obstacle to the development of fully

    open language models. In this paper, we identify three core data-related

    challenges that must be addressed to advance open-source language models. These

    include (1) transparency in model development, including the data curation

    process, (2) access to large quantities of high-quality data, and (3)

    availability of artifacts and metadata for dataset curation and analysis. To

    address these challenges, we release RedPajama-V1, an open reproduction of the

    LLaMA training dataset. In addition, we release RedPajama-V2, a massive

    web-only dataset consisting of raw, unfiltered text data together with quality

    signals and metadata. Together, the RedPajama datasets comprise over 100

    trillion tokens spanning multiple domains and with their quality signals

    facilitate the filtering of data, aiming to inspire the development of numerous

    new datasets. To date, these datasets have already been used in the training of

    strong language models used in production, such as Snowflake Arctic,

    Salesforce''s XGen and AI2''s OLMo. To provide insight into the quality of

    RedPajama, we present a series of analyses and ablation studies with

    decoder-only language models with up to 1.6B parameters. Our findings

    demonstrate how quality signals for web data can be effectively leveraged to

    curate high-quality subsets of the dataset, underscoring the potential of

    RedPajama to advance the development of transparent and high-performing

    language models at scale.'
  arxivId: '2411.12372'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton
    Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun,
    Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher
    Ré, Irina Rish, Ce Zhang
  created_at: '2025-01-04T06:53:06.611354'
  issue_number: 656
  issue_url: https://github.com/dmarx/papers-feed/issues/656
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:18:27.856Z'
  main_tex_file: null
  published_date: '2024-11-19T09:35:28Z'
  state: open
  title: 'RedPajama: an Open Dataset for Training Large Language Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2411.12372
'2411.19108':
  abstract: 'As a fundamental backbone for video generation, diffusion models are

    challenged by low inference speed due to the sequential nature of denoising.

    Previous methods speed up the models by caching and reusing model outputs at

    uniformly selected timesteps. However, such a strategy neglects the fact that

    differences among model outputs are not uniform across timesteps, which hinders

    selecting the appropriate model outputs to cache, leading to a poor balance

    between inference efficiency and visual quality. In this study, we introduce

    Timestep Embedding Aware Cache (TeaCache), a training-free caching approach

    that estimates and leverages the fluctuating differences among model outputs

    across timesteps. Rather than directly using the time-consuming model outputs,

    TeaCache focuses on model inputs, which have a strong correlation with the

    modeloutputs while incurring negligible computational cost. TeaCache first

    modulates the noisy inputs using the timestep embeddings to ensure their

    differences better approximating those of model outputs. TeaCache then

    introduces a rescaling strategy to refine the estimated differences and

    utilizes them to indicate output caching. Experiments show that TeaCache

    achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%

    Vbench score) degradation of visual quality.'
  arxivId: '2411.19108'
  arxiv_tags:
  - cs.CV
  authors: Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao,
    Yingya Zhang, Qixiang Ye, Fang Wan
  created_at: '2025-01-04T14:48:33.226940'
  issue_number: 586
  issue_url: https://github.com/dmarx/papers-feed/issues/586
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:33.229442'
  last_visited: '2024-12-30T15:10:27.225Z'
  main_tex_file: null
  published_date: '2024-11-28T12:50:05Z'
  state: open
  title: 'Timestep Embedding Tells: It''s Time to Cache for Video Diffusion Model'
  total_reading_time_seconds: 40
  url: https://arxiv.org/abs/2411.19108
'2412.01023':
  abstract: 'Most real-world datasets consist of a natural hierarchy between classes
    or an

    inherent label structure that is either already available or can be constructed

    cheaply. However, most existing representation learning methods ignore this

    hierarchy, treating labels as permutation invariant. Recent work [Zeng et al.,

    2022] proposes using this structured information explicitly, but the use of

    Euclidean distance may distort the underlying semantic context [Chen et al.,

    2013]. In this work, motivated by the advantage of hyperbolic spaces in

    modeling hierarchical relationships, we propose a novel approach HypStructure:

    a Hyperbolic Structured regularization approach to accurately embed the label

    hierarchy into the learned representations. HypStructure is a

    simple-yet-effective regularizer that consists of a hyperbolic tree-based

    representation loss along with a centering loss, and can be combined with any

    standard task loss to learn hierarchy-informed features. Extensive experiments

    on several large-scale vision benchmarks demonstrate the efficacy of

    HypStructure in reducing distortion and boosting generalization performance

    especially under low dimensional scenarios. For a better understanding of

    structured representation, we perform eigenvalue analysis that links the

    representation geometry to improved Out-of-Distribution (OOD) detection

    performance seen empirically. The code is available at

    \url{https://github.com/uiuctml/HypStructure}.'
  arxivId: '2412.01023'
  arxiv_tags:
  - cs.LG
  - cs.CV
  authors: Aditya Sinha, Siqi Zeng, Makoto Yamada, Han Zhao
  created_at: '2025-01-04T14:49:39.264149'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T14:49:39.264976'
  last_visited: '2024-12-29T02:32:00.838000+00:00'
  main_tex_file: null
  published_date: '2024-12-02T00:56:44Z'
  state: open
  title: Learning Structured Representations with Hyperbolic Embeddings
  total_reading_time_seconds: 16
  url: https://arxiv.org/abs/2412.01023
'2412.02595':
  abstract: 'Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved

    significant benchmark gains via aggressive model-based filtering, but at the

    cost of removing 90% of data. This limits their suitability for long token

    horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how

    to achieve better trade-offs between accuracy and data quantity by a

    combination of classifier ensembling, synthetic data rephrasing, and reduced

    reliance on heuristic filters. When training 8B parameter models for 1T tokens,

    using a high-quality subset of our data improves MMLU by 5.6 over DCLM,

    demonstrating the efficacy of our methods for boosting accuracies over a

    relatively short token horizon. Furthermore, our full 6.3T token dataset

    matches DCLM on MMLU, but contains four times more unique real tokens than

    DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B

    parameter model trained for 15T tokens, of which 7.2T came from our dataset, is

    better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5

    on average across ten diverse tasks. The dataset is available at

    https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html'
  arxivId: '2412.02595'
  arxiv_tags:
  - cs.CL
  authors: Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl,
    Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro
  created_at: '2025-01-04T06:53:24.608407'
  issue_number: 632
  issue_url: https://github.com/dmarx/papers-feed/issues/632
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:02:25.502Z'
  main_tex_file: null
  published_date: '2024-12-03T17:28:50Z'
  state: open
  title: "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon\n  Pretraining\
    \ Dataset"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.02595
'2412.03603':
  abstract: 'Recent advancements in video generation have significantly impacted daily

    life for both individuals and industries. However, the leading video generation

    models remain closed-source, resulting in a notable performance gap between

    industry capabilities and those available to the public. In this report, we

    introduce HunyuanVideo, an innovative open-source video foundation model that

    demonstrates performance in video generation comparable to, or even surpassing,

    that of leading closed-source models. HunyuanVideo encompasses a comprehensive

    framework that integrates several key elements, including data curation,

    advanced architectural design, progressive model scaling and training, and an

    efficient infrastructure tailored for large-scale model training and inference.

    As a result, we successfully trained a video generative model with over 13

    billion parameters, making it the largest among all open-source models. We

    conducted extensive experiments and implemented a series of targeted designs to

    ensure high visual quality, motion dynamics, text-video alignment, and advanced

    filming techniques. According to evaluations by professionals, HunyuanVideo

    outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,

    and three top-performing Chinese video generative models. By releasing the code

    for the foundation model and its applications, we aim to bridge the gap between

    closed-source and open-source communities. This initiative will empower

    individuals within the community to experiment with their ideas, fostering a

    more dynamic and vibrant video generation ecosystem. The code is publicly

    available at https://github.com/Tencent/HunyuanVideo.'
  arxivId: '2412.03603'
  arxiv_tags:
  - cs.CV
  authors: Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng
    Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin
    Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan,
    Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai
    Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng,
    Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang
    Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang,
    Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong
  created_at: '2025-01-04T06:53:33.619685'
  issue_number: 591
  issue_url: https://github.com/dmarx/papers-feed/issues/591
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:27.533139'
  last_visited: '2024-12-30T15:16:27.366Z'
  main_tex_file: null
  published_date: '2024-12-03T23:52:37Z'
  state: open
  title: 'HunyuanVideo: A Systematic Framework For Large Video Generative Models'
  total_reading_time_seconds: 74
  url: https://arxiv.org/abs/2412.03603
'2412.06769':
  abstract: 'Large language models (LLMs) are restricted to reason in the "language

    space", where they typically express the reasoning process with a

    chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue

    that language space may not always be optimal for reasoning. For example, most

    word tokens are primarily for textual coherence and not essential for

    reasoning, while some critical tokens require complex planning and pose huge

    challenges to LLMs. To explore the potential of LLM reasoning in an

    unrestricted latent space instead of using natural language, we introduce a new

    paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden

    state of the LLM as a representation of the reasoning state (termed "continuous

    thought"). Rather than decoding this into a word token, we feed it back to the

    LLM as the subsequent input embedding directly in the continuous space.

    Experiments show that Coconut can effectively augment the LLM on several

    reasoning tasks. This novel latent reasoning paradigm leads to emergent

    advanced reasoning patterns: the continuous thought can encode multiple

    alternative next reasoning steps, allowing the model to perform a breadth-first

    search (BFS) to solve the problem, rather than prematurely committing to a

    single deterministic path like CoT. Coconut outperforms CoT in certain logical

    reasoning tasks that require substantial backtracking during planning, with

    fewer thinking tokens during inference. These findings demonstrate the promise

    of latent reasoning and offer valuable insights for future research.'
  arxivId: '2412.06769'
  arxiv_tags:
  - cs.CL
  authors: Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston,
    Yuandong Tian
  created_at: '2025-01-04T06:52:12.616167'
  issue_number: 752
  issue_url: https://github.com/dmarx/papers-feed/issues/752
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:12.618753'
  last_visited: '2025-01-02T19:39:10.844Z'
  main_tex_file: null
  published_date: '2024-12-09T18:55:56Z'
  state: open
  title: Training Large Language Models to Reason in a Continuous Latent Space
  total_reading_time_seconds: 32
  url: https://arxiv.org/abs/2412.06769
'2412.06845':
  abstract: 'Recently, Large Language Models (LLMs) have undergone a significant

    transformation, marked by a rapid rise in both their popularity and

    capabilities. Leading this evolution are proprietary LLMs like GPT-4 and

    GPT-o1, which have captured widespread attention in the AI community due to

    their remarkable performance and versatility. Simultaneously, open-source LLMs,

    such as LLaMA and Mistral, have made great contributions to the ever-increasing

    popularity of LLMs due to the ease to customize and deploy the models across

    diverse applications. Although open-source LLMs present unprecedented

    opportunities for innovation and research, the commercialization of LLMs has

    raised concerns about transparency, reproducibility, and safety. Many

    open-source LLMs fail to meet fundamental transparency requirements by

    withholding essential components like training code and data, and some use

    restrictive licenses whilst claiming to be "open-source," which may hinder

    further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a

    fully open-source LLM developed in accordance with the Model Openness Framework

    (MOF), a ranked classification system that evaluates AI models based on model

    completeness and openness, adhering to principles of open science, open source,

    open data, and open access. Our model achieves the highest MOF classification

    level of "open science" through the comprehensive release of pre-training code

    and configurations, training and fine-tuning datasets, and intermediate and

    final checkpoints. Experiments show that our model achieves superior

    performance in zero-shot evaluation compared with popular 7B models and

    performs competitively in few-shot evaluation.'
  arxivId: '2412.06845'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht,
    Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue
    Chen, Yong He, Yanzhi Wang
  created_at: '2025-01-04T06:52:51.688399'
  issue_number: 639
  issue_url: https://github.com/dmarx/papers-feed/issues/639
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.651113'
  last_visited: '2024-12-30T20:04:21.071Z'
  main_tex_file: null
  published_date: '2024-12-08T02:01:46Z'
  state: open
  title: Fully Open Source Moxin-7B Technical Report
  total_reading_time_seconds: 53
  url: https://arxiv.org/abs/2412.06845
'2412.10271':
  abstract: 'The development and evaluation of Large Language Models (LLMs) has primarily

    focused on their task-solving capabilities, with recent models even surpassing

    human performance in some areas. However, this focus often neglects whether

    machine-generated language matches the human level of diversity, in terms of

    vocabulary choice, syntactic construction, and expression of meaning, raising

    questions about whether the fundamentals of language generation have been fully

    addressed. This paper emphasizes the importance of examining the preservation

    of human linguistic richness by language models, given the concerning surge in

    online content produced or aided by LLMs. We propose a comprehensive framework

    for evaluating LLMs from various linguistic diversity perspectives including

    lexical, syntactic, and semantic dimensions. Using this framework, we benchmark

    several state-of-the-art LLMs across all diversity dimensions, and conduct an

    in-depth case study for syntactic diversity. Finally, we analyze how different

    development and deployment choices impact the linguistic diversity of LLM

    outputs.'
  arxivId: '2412.10271'
  arxiv_tags:
  - cs.CL
  authors: Yanzhu Guo, Guokan Shang, Chloé Clavel
  created_at: '2025-01-04T06:52:57.610109'
  issue_number: 635
  issue_url: https://github.com/dmarx/papers-feed/issues/635
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.652874'
  last_visited: '2024-12-30T20:03:18.882Z'
  main_tex_file: null
  published_date: '2024-12-13T16:46:03Z'
  state: open
  title: Benchmarking Linguistic Diversity of Large Language Models
  total_reading_time_seconds: 52
  url: https://arxiv.org/abs/2412.10271
'2412.11768':
  abstract: 'In this work, we question the necessity of adaptive gradient methods
    for

    training deep neural networks. SGD-SaI is a simple yet effective enhancement to

    stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning

    rate Scaling at Initialization (SaI) to distinct parameter groups, guided by

    their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning

    rates without relying on adaptive second-order momentum, SGD-SaI helps prevent

    training imbalances from the very first iteration and cuts the optimizer''s

    memory usage by half compared to AdamW. Despite its simplicity and efficiency,

    SGD-SaI consistently matches or outperforms AdamW in training a variety of

    Transformer-based tasks, effectively overcoming a long-standing challenge of

    using SGD for training Transformers. SGD-SaI excels in ImageNet-1K

    classification with Vision Transformers(ViT) and GPT-2 pretraining for large

    language models (LLMs, transformer decoder-only), demonstrating robustness to

    hyperparameter variations and practicality for diverse applications. We further

    tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion

    models, where it consistently outperforms state-of-the-art optimizers. From a

    memory efficiency perspective, SGD-SaI achieves substantial memory savings for

    optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)

    and 25.15 GB for Llama2-7B compared to AdamW in full-precision training

    settings.'
  arxivId: '2412.11768'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen
  created_at: '2025-01-04T14:49:03.584492'
  issue_number: 463
  issue_url: https://github.com/dmarx/papers-feed/issues/463
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:24.262700'
  last_visited: '2024-12-29T08:57:36.656Z'
  main_tex_file: null
  published_date: '2024-12-16T13:41:37Z'
  state: open
  title: 'No More Adam: Learning Rate Scaling at Initialization is All You Need'
  total_reading_time_seconds: 53
  url: https://arxiv.org/abs/2412.11768
'2412.15285':
  abstract: 'Pretraining large language models effectively requires strategic data

    selection, blending and ordering. However, key details about data mixtures

    especially their scalability to longer token horizons and larger model sizes

    remain underexplored due to limited disclosure by model developers. To address

    this, we formalize the concept of two-phase pretraining and conduct an

    extensive systematic study on how to select and mix data to maximize model

    accuracies for the two phases. Our findings illustrate that a two-phase

    approach for pretraining outperforms random data ordering and natural

    distribution of tokens by 3.4% and 17% on average accuracies. We provide

    in-depth guidance on crafting optimal blends based on quality of the data

    source and the number of epochs to be seen. We propose to design blends using

    downsampled data at a smaller scale of 1T tokens and then demonstrate effective

    scaling of our approach to larger token horizon of 15T tokens and larger model

    size of 25B model size. These insights provide a series of steps practitioners

    can follow to design and scale their data blends.'
  arxivId: '2412.15285'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Steven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad
    Shoeybi, Bryan Catanzaro
  created_at: '2025-01-04T06:52:33.613492'
  issue_number: 643
  issue_url: https://github.com/dmarx/papers-feed/issues/643
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.649544'
  last_visited: '2024-12-30T20:04:52.739Z'
  main_tex_file: null
  published_date: '2024-12-18T18:41:18Z'
  state: open
  title: "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase\n\
    \  Pretraining"
  total_reading_time_seconds: 49
  url: https://arxiv.org/abs/2412.15285
'2412.17758':
  abstract: 'ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily

    due to an evaluation setup that prevents direct comparison of answer choices

    rather than inherent complexity. Although some researchers have quietly shifted

    to a more appropriate scheme over the last year, the implications of this

    change have yet to be widely acknowledged. We highlight this overlooked shift,

    show how similar evaluation practices falsely imply reasoning deficits in other

    benchmarks, and demonstrate that fairer methods dramatically reduce performance

    gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing

    so, we reveal how evaluation shapes perceived difficulty and offer guidelines

    to ensure that multiple-choice evaluations accurately reflect actual model

    capabilities.'
  arxivId: '2412.17758'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Łukasz Borchmann
  created_at: '2025-01-04T14:49:27.229963'
  issue_number: 458
  issue_url: https://github.com/dmarx/papers-feed/issues/458
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:49:27.230757'
  last_visited: '2024-12-29T08:28:16.461Z'
  main_tex_file: null
  published_date: '2024-12-23T18:14:36Z'
  state: open
  title: 'In Case You Missed It: ARC ''Challenge'' Is Not That Challenging'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.17758
'2412.17805':
  abstract: 'Learning a robust video Variational Autoencoder (VAE) is essential for

    reducing video redundancy and facilitating efficient video generation. Directly

    applying image VAEs to individual frames in isolation can result in temporal

    inconsistencies and suboptimal compression rates due to a lack of temporal

    compression. Existing Video VAEs have begun to address temporal compression;

    however, they often suffer from inadequate reconstruction performance. In this

    paper, we present a novel and powerful video autoencoder capable of

    high-fidelity video encoding. First, we observe that entangling spatial and

    temporal compression by merely extending the image VAE to a 3D VAE can

    introduce motion blur and detail distortion artifacts. Thus, we propose

    temporal-aware spatial compression to better encode and decode the spatial

    information. Additionally, we integrate a lightweight motion compression model

    for further temporal compression. Second, we propose to leverage the textual

    information inherent in text-to-video datasets and incorporate text guidance

    into our model. This significantly enhances reconstruction quality,

    particularly in terms of detail preservation and temporal stability. Third, we

    further improve the versatility of our model through joint training on both

    images and videos, which not only enhances reconstruction quality but also

    enables the model to perform both image and video autoencoding. Extensive

    evaluations against strong recent baselines demonstrate the superior

    performance of our method. The project website can be found

    at~\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.'
  arxivId: '2412.17805'
  arxiv_tags:
  - cs.CV
  authors: Yazhou Xing, Yang Fei, Yingqing He, Jingye Chen, Jiaxin Xie, Xiaowei Chi,
    Qifeng Chen
  created_at: '2025-01-04T06:52:24.663099'
  issue_number: 716
  issue_url: https://github.com/dmarx/papers-feed/issues/716
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T08:03:08.915Z'
  main_tex_file: null
  published_date: '2024-12-23T18:58:24Z'
  state: open
  title: Large Motion Video Autoencoding with Cross-modal Video VAE
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.17805
'2412.17847':
  abstract: 'Progress in AI is driven largely by the scale and quality of training
    data.

    Despite this, there is a deficit of empirical analysis examining the attributes

    of well-established datasets beyond text. In this work we conduct the largest

    and first-of-its-kind longitudinal audit across modalities--popular text,

    speech, and video datasets--from their detailed sourcing trends and use

    restrictions to their geographical and linguistic representation. Our manual

    analysis covers nearly 4000 public datasets between 1990-2024, spanning 608

    languages, 798 sources, 659 organizations, and 67 countries. We find that

    multimodal machine learning applications have overwhelmingly turned to

    web-crawled, synthetic, and social media platforms, such as YouTube, for their

    training sets, eclipsing all other sources since 2019. Secondly, tracing the

    chain of dataset derivations we find that while less than 33% of datasets are

    restrictively licensed, over 80% of the source content in widely-used text,

    speech, and video datasets, carry non-commercial restrictions. Finally, counter

    to the rising number of languages and geographies represented in public AI

    training datasets, our audit demonstrates measures of relative geographical and

    multilingual representation have failed to significantly improve their coverage

    since 2013. We believe the breadth of our audit enables us to empirically

    examine trends in data sourcing, restrictions, and Western-centricity at an

    ecosystem-level, and that visibility into these questions are essential to

    progress in responsible AI. As a contribution to ongoing improvements in

    dataset transparency and responsible use, we release our entire multimodal

    audit, allowing practitioners to trace data provenance across text, speech, and

    video.'
  arxivId: '2412.17847'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.CY
  - cs.LG
  - cs.MM
  authors: Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska,
    William Brannon, Robert Mahari, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad
    Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Naana Obeng-Marnu, Da Yin, Kun
    Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas,
    Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm,
    Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda,
    Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui
    Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker,
    Jad Kabbara
  created_at: '2025-01-04T06:53:21.616757'
  issue_number: 633
  issue_url: https://github.com/dmarx/papers-feed/issues/633
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:21.617516'
  last_visited: '2024-12-30T20:03:13.039Z'
  main_tex_file: null
  published_date: '2024-12-19T01:30:19Z'
  state: open
  title: Bridging the Data Provenance Gap Across Text, Speech and Video
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2412.17847
'2412.18082':
  abstract: 'The item cold-start problem is crucial for online recommender systems,
    as the

    success of the cold-start phase determines whether items can transition into

    popular ones. Prompt learning, a powerful technique used in natural language

    processing (NLP) to address zero- or few-shot problems, has been adapted for

    recommender systems to tackle similar challenges. However, existing methods

    typically rely on content-based properties or text descriptions for prompting,

    which we argue may be suboptimal for cold-start recommendations due to 1)

    semantic gaps with recommender tasks, 2) model bias caused by warm-up items

    contribute most of the positive feedback to the model, which is the core of the

    cold-start problem that hinders the recommender quality on cold-start items. We

    propose to leverage high-value positive feedback, termed pinnacle feedback as

    prompt information, to simultaneously resolve the above two problems. We

    experimentally prove that compared to the content description proposed in

    existing works, the positive feedback is more suitable to serve as prompt

    information by bridging the semantic gaps. Besides, we propose item-wise

    personalized prompt networks to encode pinnaclce feedback to relieve the model

    bias by the positive feedback dominance problem. Extensive experiments on four

    real-world datasets demonstrate the superiority of our model over

    state-of-the-art methods. Moreover, PROMO has been successfully deployed on a

    popular short-video sharing platform, a billion-user scale commercial

    short-video application, achieving remarkable performance gains across various

    commercial metrics within cold-start scenarios'
  arxivId: '2412.18082'
  arxiv_tags:
  - cs.IR
  - cs.AI
  authors: Yuezihan Jiang, Gaode Chen, Wenhan Zhang, Jingchi Wang, Yinjie Jiang, Qi
    Zhang, Jingjian Lin, Peng Jiang, Kaigui Bian
  created_at: '2025-01-04T14:48:54.229792'
  issue_number: 570
  issue_url: https://github.com/dmarx/papers-feed/issues/570
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:54.231751'
  last_visited: '2024-12-30T04:50:12.821000+00:00'
  main_tex_file: null
  published_date: '2024-12-24T01:38:19Z'
  state: open
  title: Prompt Tuning for Item Cold-start Recommendation
  total_reading_time_seconds: 19
  url: https://arxiv.org/abs/2412.18082
'2412.18860':
  abstract: 'We introduce a bootstrapping approach to train long-context language
    models

    by exploiting their short-context capabilities only. Our method utilizes a

    simple agent workflow to synthesize diverse long-context instruction tuning

    data, thereby eliminating the necessity for manual data collection and

    annotation. The proposed data synthesis workflow requires only a short-context

    language model, a text retriever, and a document collection, all of which are

    readily accessible within the open-source ecosystem. Subsequently, language

    models are fine-tuned using the synthesized data to extend their context

    lengths. In this manner, we effectively transfer the short-context capabilities

    of language models to long-context scenarios through a bootstrapping process.

    We conduct experiments with the open-source Llama-3 family of models and

    demonstrate that our method can successfully extend the context length to up to

    1M tokens, achieving superior performance across various benchmarks.'
  arxivId: '2412.18860'
  arxiv_tags:
  - cs.CL
  - cs.IR
  authors: Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei
  created_at: '2025-01-04T14:48:48.243492'
  issue_number: 539
  issue_url: https://github.com/dmarx/papers-feed/issues/539
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:54.232387'
  last_visited: '2024-12-30T04:37:40.310Z'
  main_tex_file: null
  published_date: '2024-12-25T10:08:54Z'
  state: open
  title: Bootstrap Your Own Context Length
  total_reading_time_seconds: 44
  url: https://arxiv.org/abs/2412.18860
'2412.18956':
  abstract: 'When you have a question, the most effective way to have the question

    answered is to directly connect with experts on the topic and have a

    conversation with them. Prior to the invention of writing, this was the only

    way. Although effective, this solution exhibits scalability challenges. Writing

    allowed knowledge to be materialized, preserved, and replicated, enabling the

    development of different technologies over the centuries to connect information

    seekers with relevant information. This progression ultimately culminated in

    the ten-blue-links web search paradigm we''re familiar with, just before the

    recent emergence of generative AI. However, we often forget that consuming

    static content is an imperfect solution. With the advent of large language

    models, it has become possible to develop a superior experience by allowing

    users to directly engage with experts. These interactions can of course satisfy

    information needs, but expert models can do so much more. This coming future

    requires reimagining search.'
  arxivId: '2412.18956'
  arxiv_tags:
  - cs.IR
  authors: Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne
  created_at: '2025-01-04T14:48:51.238557'
  issue_number: 572
  issue_url: https://github.com/dmarx/papers-feed/issues/572
  labels:
  - paper
  - rating:downvote
  last_read: '2025-01-04T14:48:51.239805'
  last_visited: '2024-12-30T04:52:50.157000+00:00'
  main_tex_file: null
  published_date: '2024-12-25T18:09:34Z'
  state: open
  title: 'Musings About the Future of Search: A Return to the Past?'
  total_reading_time_seconds: 25
  url: https://arxiv.org/abs/2412.18956
'2412.19442':
  abstract: 'Large Language Models (LLMs) have revolutionized a wide range of domains
    such

    as natural language processing, computer vision, and multi-modal tasks due to

    their ability to comprehend context and perform logical reasoning. However, the

    computational and memory demands of LLMs, particularly during inference, pose

    significant challenges when scaling them to real-world, long-context, and

    real-time applications. Key-Value (KV) cache management has emerged as a

    critical optimization technique for accelerating LLM inference by reducing

    redundant computations and improving memory utilization. This survey provides
    a

    comprehensive overview of KV cache management strategies for LLM acceleration,

    categorizing them into token-level, model-level, and system-level

    optimizations. Token-level strategies include KV cache selection, budget

    allocation, merging, quantization, and low-rank decomposition, while

    model-level optimizations focus on architectural innovations and attention

    mechanisms to enhance KV reuse. System-level approaches address memory

    management, scheduling, and hardware-aware designs to improve efficiency across

    diverse computing environments. Additionally, the survey provides an overview

    of both text and multimodal datasets and benchmarks used to evaluate these

    strategies. By presenting detailed taxonomies and comparative analyses, this

    work aims to offer useful insights for researchers and practitioners to support

    the development of efficient and scalable KV cache management techniques,

    contributing to the practical deployment of LLMs in real-world applications.

    The curated paper list for KV cache management is in:

    \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.'
  arxivId: '2412.19442'
  arxiv_tags:
  - cs.AI
  - cs.DC
  authors: Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen,
    Nicole Hu, Wei Dong, Qing Li, Lei Chen
  created_at: '2025-01-04T14:48:57.229727'
  issue_number: 535
  issue_url: https://github.com/dmarx/papers-feed/issues/535
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T14:48:57.232180'
  last_visited: '2024-12-30T04:35:23.041Z'
  main_tex_file: null
  published_date: '2024-12-27T04:17:57Z'
  state: open
  title: "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2412.19442
'2412.19792':
  abstract: 'Language model alignment has become a critical step in training modern

    generative language models. The goal of alignment is to finetune a reference

    model such that the win rate of a sample from the aligned model over a sample

    from the reference model is high, subject to a KL divergence constraint. Today,

    we are increasingly using inference-time algorithms (e.g., Best-of-N,

    controlled decoding, tree search) to decode from language models rather than

    standard sampling. However, the alignment objective does not capture such

    inference-time decoding procedures. We show that the existing alignment

    framework is sub-optimal in view of such inference-time methods. We then modify

    the alignment objective and propose a framework for inference-aware alignment

    (IAPO). We prove that for any inference-time decoding algorithm, the optimal

    solution that optimizes the inference-time win rate of the aligned policy

    against the reference policy is the solution to the typical RLHF problem with
    a

    transformation of the reward. This motivates us to provide the KL-regularized

    calibrate-and-transform RL (CTRL) algorithm to solve this problem, which

    involves a reward calibration step and a KL-regularized reward maximization

    step with a transformation of the calibrated reward. We particularize our study

    to two important inference-time strategies: best-of-N sampling and best-of-N

    jailbreaking, where N responses are sampled from the model and the one with the

    highest or lowest reward is selected. We propose specific transformations for

    these strategies and demonstrate that our framework offers significant

    improvements over existing state-of-the-art methods for language model

    alignment. Empirically, we outperform baselines that are designed without

    taking inference-time decoding into consideration by 8-12% and 4-9% on

    inference-time win rates over the Anthropic helpfulness and harmlessness dialog

    benchmark datasets.'
  arxivId: '2412.19792'
  arxiv_tags:
  - cs.LG
  - cs.CL
  - cs.IT
  - math.IT
  authors: Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael
    Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha,
    Ananda Theertha Suresh, Ahmad Beirami
  created_at: '2025-01-04T06:51:54.729533'
  issue_number: 763
  issue_url: https://github.com/dmarx/papers-feed/issues/763
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:00.645791'
  last_visited: '2025-01-03T08:59:10.679Z'
  main_tex_file: null
  published_date: '2024-12-27T18:45:36Z'
  state: open
  title: 'InfAlign: Inference-aware language model alignment'
  total_reading_time_seconds: 49
  url: https://arxiv.org/abs/2412.19792
'2412.20292':
  abstract: 'We obtain the first analytic, interpretable and predictive theory of

    creativity in convolutional diffusion models. Indeed, score-based diffusion

    models can generate highly creative images that lie far from their training

    data. But optimal score-matching theory suggests that these models should only

    be able to produce memorized training examples. To reconcile this

    theory-experiment gap, we identify two simple inductive biases, locality and

    equivariance, that: (1) induce a form of combinatorial creativity by preventing

    optimal score-matching; (2) result in a fully analytic, completely

    mechanistically interpretable, equivariant local score (ELS) machine that, (3)

    without any training can quantitatively predict the outputs of trained

    convolution only diffusion models (like ResNets and UNets) with high accuracy

    (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our

    ELS machine reveals a locally consistent patch mosaic model of creativity, in

    which diffusion models create exponentially many novel images by mixing and

    matching different local training set patches in different image locations. Our

    theory also partially predicts the outputs of pre-trained self-attention

    enabled UNets (median $r^2 \sim 0.75$ on CIFAR10), revealing an intriguing role

    for attention in carving out semantic coherence from local patch mosaics.'
  arxivId: '2412.20292'
  arxiv_tags:
  - cs.LG
  - cond-mat.dis-nn
  - cs.AI
  - q-bio.NC
  - stat.ML
  - I.2.10
  authors: Mason Kamb, Surya Ganguli
  created_at: '2025-01-04T06:52:30.603137'
  issue_number: 708
  issue_url: https://github.com/dmarx/papers-feed/issues/708
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:30.604171'
  last_visited: '2025-01-01T16:00:07.088Z'
  main_tex_file: null
  published_date: '2024-12-28T22:33:29Z'
  state: open
  title: An analytic theory of creativity in convolutional diffusion models
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2412.20292
