'1310.6753':
  abstract: 'A crucial task in the analysis of on-line social-networking systems is
    to

    identify important people --- those linked by strong social ties --- within an

    individual''s network neighborhood. Here we investigate this question for a

    particular category of strong ties, those involving spouses or romantic

    partners. We organize our analysis around a basic question: given all the

    connections among a person''s friends, can you recognize his or her romantic

    partner from the network structure alone? Using data from a large sample of

    Facebook users, we find that this task can be accomplished with high accuracy,

    but doing so requires the development of a new measure of tie strength that we

    term `dispersion'' --- the extent to which two people''s mutual friends are not

    themselves well-connected. The results offer methods for identifying types of

    structurally significant people in on-line applications, and suggest a

    potential expansion of existing theories of tie strength.'
  arxivId: '1310.6753'
  arxiv_tags:
  - cs.SI
  - physics.soc-ph
  - H.2.8
  authors: Lars Backstrom, Jon Kleinberg
  created_at: '2025-01-04T06:52:00.637611'
  issue_number: 770
  issue_url: https://github.com/dmarx/papers-feed/issues/770
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:00.641659'
  last_visited: '2025-01-03T18:28:19.059Z'
  main_tex_file: null
  published_date: '2013-10-24T20:00:18Z'
  state: open
  title: "Romantic Partnerships and the Dispersion of Social Ties: A Network\n  Analysis\
    \ of Relationship Status on Facebook"
  total_reading_time_seconds: 79
  url: https://arxiv.org/abs/1310.6753
'1602.03483':
  abstract: 'Unsupervised methods for learning distributed representations of words
    are

    ubiquitous in today''s NLP research, but far less is known about the best ways

    to learn distributed phrase or sentence representations from unlabelled data.

    This paper is a systematic comparison of models that learn such

    representations. We find that the optimal approach depends critically on the

    intended application. Deeper, more complex models are preferable for

    representations to be used in supervised systems, but shallow log-linear models

    work best for building representation spaces that can be decoded with simple

    spatial distance metrics. We also propose two new unsupervised

    representation-learning objectives designed to optimise the trade-off between

    training time, domain portability and performance.'
  arxivId: '1602.03483'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Felix Hill, Kyunghyun Cho, Anna Korhonen
  created_at: '2025-01-04T06:51:57.839597'
  issue_number: 777
  issue_url: https://github.com/dmarx/papers-feed/issues/777
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:51:57.840814'
  last_visited: '2025-01-03T20:13:43.540Z'
  main_tex_file: null
  published_date: '2016-02-10T18:49:58Z'
  state: open
  title: Learning Distributed Representations of Sentences from Unlabelled Data
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/1602.03483
'2303.00383':
  abstract: 'We propose a robust and computationally efficient algorithm to generically

    construct first return maps of dynamical systems from time series without the

    need for embedding. Typically, a first return map is constructed using a

    heuristic convenience (maxima or zero-crossings of the time series, for

    example) or a computationally delicate geometric approach (explicitly

    constructing a Poincar\''e section from a hyper-surface normal to the flow and

    then interpolating to determine intersections with trajectories). Our approach

    relies on ordinal partitions of the time series and builds the first return map

    from successive intersections with particular ordinal sequences. Generically,

    we can obtain distinct first return maps for each ordinal sequence. We define

    entropy-based measures to guide our selection of the ordinal sequence for a

    ``good'''' first return map and show that this method can robustly be applied
    to

    time series from classical chaotic systems to extract the underlying first

    return map dynamics. The results are shown on several well-known dynamical

    systems (Lorenz, R{\"o}ssler and Mackey-Glass in chaotic regimes).'
  arxivId: '2303.00383'
  arxiv_tags:
  - math.DS
  authors: Zahra Shahriari, Shannon Dee Algar, David M. Walker, Michael Small
  created_at: '2025-01-04T06:52:03.682689'
  issue_number: 760
  issue_url: https://github.com/dmarx/papers-feed/issues/760
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:03.683895'
  last_visited: '2025-01-03T08:49:43.332Z'
  main_tex_file: null
  published_date: '2023-03-01T10:09:57Z'
  state: open
  title: "Ordinal Poincaré Sections: Reconstructing the First Return Map from an\n\
    \  Ordinal Segmentation of Time Series"
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2303.00383
'2305.13169':
  abstract: 'Pretraining is the preliminary and fundamental step in developing capable

    language models (LM). Despite this, pretraining data design is critically

    under-documented and often guided by empirically unsupported intuitions. To

    address this, we pretrain 28 1.5B parameter decoder-only models, training on

    data curated (1) at different times, (2) with varying toxicity and quality

    filters, and (3) with different domain compositions. First, we quantify the

    effect of pretraining data age. A temporal shift between evaluation data and

    pretraining data leads to performance degradation, which is not overcome by

    finetuning. Second, we explore the effect of quality and toxicity filters,

    showing a trade-off between performance on standard benchmarks and risk of

    toxic generations. Our findings indicate there does not exist a

    one-size-fits-all solution to filtering training data. We also find that the

    effects of different types of filtering are not predictable from text domain

    characteristics. Lastly, we empirically validate that the inclusion of

    heterogeneous data sources, like books and web, is broadly beneficial and

    warrants greater prioritization. These findings constitute the largest set of

    experiments to validate, quantify, and expose many undocumented intuitions

    about text pretraining, which we hope will help support more informed

    data-centric decisions in LM development.'
  arxivId: '2305.13169'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts,
    Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito
  created_at: '2025-01-04T06:53:03.610654'
  issue_number: 657
  issue_url: https://github.com/dmarx/papers-feed/issues/657
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:20:50.556Z'
  main_tex_file: null
  published_date: '2023-05-22T15:57:53Z'
  state: open
  title: "A Pretrainer's Guide to Training Data: Measuring the Effects of Data\n \
    \ Age, Domain Coverage, Quality, & Toxicity"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2305.13169
'2310.01889':
  abstract: 'Transformers have emerged as the architecture of choice for many

    state-of-the-art AI models, showcasing exceptional performance across a wide

    range of AI applications. However, the memory demands imposed by Transformers

    limit their ability to handle long sequences, thereby posing challenges in

    utilizing videos, actions, and other long-form sequences and modalities in

    complex environments. We present a novel approach, Ring Attention with

    Blockwise Transformers (Ring Attention), which leverages blockwise computation

    of self-attention and feedforward to distribute long sequences across multiple

    devices while fully overlapping the communication of key-value blocks with the

    computation of blockwise attention. Our approach enables training and inference

    of sequences that are up to device count times longer than those achievable by

    prior memory-efficient Transformers, without resorting to approximations or

    incurring additional communication and computation overheads. Extensive

    experiments on language modeling and reinforcement learning tasks demonstrate

    the effectiveness of our approach in allowing millions of tokens context size

    and improving performance.'
  arxivId: '2310.01889'
  arxiv_tags:
  - cs.CL
  authors: Hao Liu, Matei Zaharia, Pieter Abbeel
  created_at: '2025-01-04T06:52:27.607291'
  issue_number: 712
  issue_url: https://github.com/dmarx/papers-feed/issues/712
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:27.608961'
  last_visited: '2025-01-02T07:40:21.565Z'
  main_tex_file: null
  published_date: '2023-10-03T08:44:50Z'
  state: open
  title: Ring Attention with Blockwise Transformers for Near-Infinite Context
  total_reading_time_seconds: 40
  url: https://arxiv.org/abs/2310.01889
'2403.08540':
  abstract: 'Scaling laws are useful guides for derisking expensive training runs,
    as they

    predict performance of large models using cheaper, small-scale experiments.

    However, there remain gaps between current scaling studies and how language

    models are ultimately trained and evaluated. For instance, scaling is usually

    studied in the compute-optimal training regime (i.e., "Chinchilla optimal"

    regime). In contrast, models are often over-trained to reduce inference costs.

    Moreover, scaling laws mostly predict loss on next-token prediction, but models

    are usually compared on downstream task performance. To address both

    shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters

    trained with various numbers of tokens on three data distributions. First, we

    fit scaling laws that extrapolate in both the amount of over-training and the

    number of model parameters. This enables us to predict the validation loss of
    a

    1.4B parameter, 900B token run (i.e., 32$\times$ over-trained) and a 6.9B

    parameter, 138B token run (i.e., a compute-optimal run)$\unicode{x2014}$each

    from experiments that take 300$\times$ less compute. Second, we relate the

    perplexity of a language model to its downstream task performance by proposing

    a power law. We use this law to predict top-1 error averaged over downstream

    tasks for the two aforementioned models, using experiments that take 20$\times$

    less compute. Our experiments are available at

    https://github.com/mlfoundations/scaling.'
  arxivId: '2403.08540'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan,
    Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh,
    Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Luca Soldaini, Alexandros
    G. Dimakis, Gabriel Ilharco, Pang Wei Koh, Shuran Song, Thomas Kollar, Yair Carmon,
    Achal Dave, Reinhard Heckel, Niklas Muennighoff, Ludwig Schmidt
  created_at: '2025-01-04T06:53:18.643645'
  issue_number: 599
  issue_url: https://github.com/dmarx/papers-feed/issues/599
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.648664'
  last_visited: '2024-12-30T19:37:31.972Z'
  main_tex_file: null
  published_date: '2024-03-13T13:54:00Z'
  state: open
  title: "Language models scale reliably with over-training and on downstream\n  tasks"
  total_reading_time_seconds: 22
  url: https://arxiv.org/abs/2403.08540
'2403.14554':
  abstract: 'We propose Gaussian Frosting, a novel mesh-based representation for

    high-quality rendering and editing of complex 3D effects in real-time. Our

    approach builds on the recent 3D Gaussian Splatting framework, which optimizes

    a set of 3D Gaussians to approximate a radiance field from images. We propose

    first extracting a base mesh from Gaussians during optimization, then building

    and refining an adaptive layer of Gaussians with a variable thickness around

    the mesh to better capture the fine details and volumetric effects near the

    surface, such as hair or grass. We call this layer Gaussian Frosting, as it

    resembles a coating of frosting on a cake. The fuzzier the material, the

    thicker the frosting. We also introduce a parameterization of the Gaussians to

    enforce them to stay inside the frosting layer and automatically adjust their

    parameters when deforming, rescaling, editing or animating the mesh. Our

    representation allows for efficient rendering using Gaussian splatting, as well

    as editing and animation by modifying the base mesh. We demonstrate the

    effectiveness of our method on various synthetic and real scenes, and show that

    it outperforms existing surface-based approaches. We will release our code and

    a web-based viewer as additional contributions. Our project page is the

    following: https://anttwo.github.io/frosting/'
  arxivId: '2403.14554'
  arxiv_tags:
  - cs.CV
  - cs.GR
  authors: Antoine Guédon, Vincent Lepetit
  created_at: '2025-01-04T06:52:06.631074'
  issue_number: 758
  issue_url: https://github.com/dmarx/papers-feed/issues/758
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:06.631879'
  last_visited: '2025-01-03T02:52:11.974Z'
  main_tex_file: null
  published_date: '2024-03-21T16:53:03Z'
  state: open
  title: "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time\n  Rendering"
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2403.14554
'2405.04517':
  abstract: 'In the 1990s, the constant error carousel and gating were introduced
    as the

    central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have

    stood the test of time and contributed to numerous deep learning success

    stories, in particular they constituted the first Large Language Models (LLMs).

    However, the advent of the Transformer technology with parallelizable

    self-attention at its core marked the dawn of a new era, outpacing LSTMs at

    scale. We now raise a simple question: How far do we get in language modeling

    when scaling LSTMs to billions of parameters, leveraging the latest techniques

    from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we

    introduce exponential gating with appropriate normalization and stabilization

    techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM

    with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that

    is fully parallelizable with a matrix memory and a covariance update rule.

    Integrating these LSTM extensions into residual block backbones yields xLSTM

    blocks that are then residually stacked into xLSTM architectures. Exponential

    gating and modified memory structures boost xLSTM capabilities to perform

    favorably when compared to state-of-the-art Transformers and State Space

    Models, both in performance and scaling.'
  arxivId: '2405.04517'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - stat.ML
  authors: Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra
    Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter
  created_at: '2025-01-04T06:53:15.665066'
  issue_number: 649
  issue_url: https://github.com/dmarx/papers-feed/issues/649
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:15.667071'
  last_visited: '2024-12-30T20:10:41.007000+00:00'
  main_tex_file: null
  published_date: '2024-05-07T17:50:21Z'
  state: open
  title: 'xLSTM: Extended Long Short-Term Memory'
  total_reading_time_seconds: 31
  url: https://arxiv.org/abs/2405.04517
'2406.01981':
  abstract: 'The size of large language models (LLMs) has scaled dramatically in recent

    years and their computational and data requirements have surged

    correspondingly. State-of-the-art language models, even at relatively smaller

    sizes, typically require training on at least a trillion tokens. This rapid

    advancement has eclipsed the growth of open-source datasets available for

    large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset),

    a dataset under a permissive license comprising 1.3 trillion tokens, assembled

    by integrating several major respected open-source datasets into a single,

    high-quality corpus. We apply rigorous filtering and deduplication processes,

    both within and across datasets, to maintain and enhance the quality derived

    from the original datasets. Our evaluations show that Zyda not only competes

    favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but

    also substantially improves the performance of comparable models from the

    Pythia suite. Our rigorous data processing methods significantly enhance Zyda''s

    effectiveness, outperforming even the best of its constituent datasets when

    used independently.'
  arxivId: '2406.01981'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Yury Tokpanov, Beren Millidge, Paolo Glorioso, Jonathan Pilault, Adam Ibrahim,
    James Whittington, Quentin Anthony
  created_at: '2025-01-04T06:52:48.608606'
  issue_number: 641
  issue_url: https://github.com/dmarx/papers-feed/issues/641
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.650339'
  last_visited: '2024-12-30T20:04:46.858Z'
  main_tex_file: null
  published_date: '2024-06-04T05:47:17Z'
  state: open
  title: 'Zyda: A 1.3T Dataset for Open Language Modeling'
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2406.01981
'2406.10670':
  abstract: "Selecting high-quality data for pre-training is crucial in shaping the\n\
    downstream task performance of language models. A major challenge lies in\nidentifying\
    \ this optimal subset, a problem generally considered intractable,\nthus necessitating\
    \ scalable and effective heuristics. In this work, we propose\na data selection\
    \ method, CoLoR-Filter (Conditional Loss Reduction Filtering),\nwhich leverages\
    \ an empirical Bayes-inspired approach to derive a simple and\ncomputationally\
    \ efficient selection criterion based on the relative loss values\nof two auxiliary\
    \ models.\n  In addition to the modeling rationale, we evaluate CoLoR-Filter empirically\n\
    on two language modeling tasks: (1) selecting data from C4 for domain\nadaptation\
    \ to evaluation on Books and (2) selecting data from C4 for a suite of\ndownstream\
    \ multiple-choice question answering tasks. We demonstrate favorable\nscaling\
    \ both as we subselect more aggressively and using small auxiliary models\nto\
    \ select data for large target models. As one headline result, CoLoR-Filter\n\
    data selected using a pair of 150m parameter auxiliary models can train a 1.2b\n\
    parameter target model to match a 1.2b parameter model trained on 25b randomly\n\
    selected tokens with 25x less data for Books and 11x less data for the\ndownstream\
    \ tasks.\n  Code: https://github.com/davidbrandfonbrener/color-filter-olmo\n \
    \ Filtered data:\nhttps://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4"
  arxivId: '2406.10670'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CL
  authors: David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan Richard Schwarz,
    Sham Kakade
  created_at: '2025-01-04T06:53:27.614374'
  issue_number: 630
  issue_url: https://github.com/dmarx/papers-feed/issues/630
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:27.615183'
  last_visited: '2024-12-30T20:02:17.082Z'
  main_tex_file: null
  published_date: '2024-06-15T15:28:02Z'
  state: open
  title: "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language\n\
    \  Model Pre-training"
  total_reading_time_seconds: 7
  url: https://arxiv.org/abs/2406.10670
'2407.01492':
  abstract: 'The data mixture for large language model pre-training significantly
    impacts

    performance, yet how to determine an effective mixture remains unclear. We

    propose RegMix to automatically identify a high-performing data mixture by

    formulating it as a regression task. RegMix involves training a set of small

    models with diverse data mixtures and fitting a regression model to predict

    their performance given their respective mixtures. With the fitted regression

    model, we simulate the top-ranked mixture and use it to train a large-scale

    model with orders of magnitude more compute. To empirically validate RegMix, we

    train 512 models with 1M parameters for 1B tokens of different mixtures to fit

    the regression model and find the optimal mixture. Using this mixture we train

    a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we

    find performs best among 64 candidate 1B parameter models with other mixtures.

    Further, our method demonstrates superior performance compared to human

    selection and achieves results that match or surpass DoReMi, while utilizing

    only 10% of the compute budget. Our experiments also show that (1) Data

    mixtures significantly impact performance with single-task performance

    variations of up to 14.6%; (2) Web corpora rather than data perceived as

    high-quality like Wikipedia have the strongest positive correlation with

    downstream performance; (3) Domains interact in complex ways often

    contradicting common sense, thus automatic approaches like RegMix are needed;

    (4) Data mixture effects transcend scaling laws, and our approach captures the

    complexity by considering all domains together. Our code is available at

    https://github.com/sail-sg/regmix.'
  arxivId: '2407.01492'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou,
    Tianyu Pang, Jing Jiang, Min Lin
  created_at: '2025-01-04T06:53:09.609827'
  issue_number: 654
  issue_url: https://github.com/dmarx/papers-feed/issues/654
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:09.611828'
  last_visited: '2024-12-30T20:17:16.629000+00:00'
  main_tex_file: null
  published_date: '2024-07-01T17:31:03Z'
  state: open
  title: 'RegMix: Data Mixture as Regression for Language Model Pre-training'
  total_reading_time_seconds: 42
  url: https://arxiv.org/abs/2407.01492
'2407.21783':
  abstract: 'Modern artificial intelligence (AI) systems are powered by foundation
    models.

    This paper presents a new set of foundation models, called Llama 3. It is a

    herd of language models that natively support multilinguality, coding,

    reasoning, and tool usage. Our largest model is a dense Transformer with 405B

    parameters and a context window of up to 128K tokens. This paper presents an

    extensive empirical evaluation of Llama 3. We find that Llama 3 delivers

    comparable quality to leading language models such as GPT-4 on a plethora of

    tasks. We publicly release Llama 3, including pre-trained and post-trained

    versions of the 405B parameter language model and our Llama Guard 3 model for

    input and output safety. The paper also presents the results of experiments in

    which we integrate image, video, and speech capabilities into Llama 3 via a

    compositional approach. We observe this approach performs competitively with

    the state-of-the-art on image, video, and speech recognition tasks. The

    resulting models are not yet being broadly released as they are still under

    development.'
  arxivId: '2407.21783'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.CV
  authors: Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan,
    Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra,
    Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien
    Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh
    Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,
    Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,
    Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
    Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan,
    Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy,
    Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán,
    Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai,
    Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah
    Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann,
    Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana
    Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer
    Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen
    Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca,
    Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad,
    Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid
    El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia,
    Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins,
    Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de
    Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin
    Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie
    Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes
    Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier
    Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic,
    Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,
    Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,
    Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit
    Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross
    Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa,
    Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan
    Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang,
    Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot,
    Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha,
    Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao,
    Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez,
    Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei
    Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang
    Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle
    Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue
    Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos,
    Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya
    Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg,
    Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus,
    Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton,
    Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita
    Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman,
    Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth
    Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock,
    Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
    Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao
    Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer,
    Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins,
    David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem
    Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine
    Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman,
    Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian,
    Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide,
    Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern,
    Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,
    Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph,
    Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor
    Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman,
    James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff
    Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul,
    Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard,
    Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou
    U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan,
    Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang,
    Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng
    Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani,
    Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim
    Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer,
    Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan,
    Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad
    Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata
    Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich
    Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar,
    Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,
    Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,
    Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham
    Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah
    Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin
    Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha
    Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh
    Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy
    Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang,
    Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen,
    Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng,
    Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal
    Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun
    Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi,
    Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu,
    Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,
    Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan
    Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi,
    Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao,
    Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick,
    Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, Zhiyu Ma
  created_at: '2025-01-04T06:52:42.685103'
  issue_number: 702
  issue_url: https://github.com/dmarx/papers-feed/issues/702
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T21:58:38.440Z'
  main_tex_file: null
  published_date: '2024-07-31T17:54:27Z'
  state: open
  title: The Llama 3 Herd of Models
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2407.21783
'2408.06072':
  abstract: 'We present CogVideoX, a large-scale text-to-video generation model based
    on

    diffusion transformer, which can generate 10-second continuous videos aligned

    with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360

    pixels. Previous video generation models often had limited movement and short

    durations, and is difficult to generate videos with coherent narratives based

    on text. We propose several designs to address these issues. First, we propose

    a 3D Variational Autoencoder (VAE) to compress videos along both spatial and

    temporal dimensions, to improve both compression rate and video fidelity.

    Second, to improve the text-video alignment, we propose an expert transformer

    with the expert adaptive LayerNorm to facilitate the deep fusion between the

    two modalities. Third, by employing a progressive training and multi-resolution

    frame pack technique, CogVideoX is adept at producing coherent, long-duration,

    different shape videos characterized by significant motions. In addition, we

    develop an effective text-video data processing pipeline that includes various

    data preprocessing strategies and a video captioning method, greatly

    contributing to the generation quality and semantic alignment. Results show

    that CogVideoX demonstrates state-of-the-art performance across both multiple

    machine metrics and human evaluations. The model weight of both 3D Causal VAE,

    Video caption model and CogVideoX are publicly available at

    https://github.com/THUDM/CogVideo.'
  arxivId: '2408.06072'
  arxiv_tags:
  - cs.CV
  authors: Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng
    Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu,
    Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, Jie Tang
  created_at: '2025-01-04T06:53:30.623623'
  issue_number: 597
  issue_url: https://github.com/dmarx/papers-feed/issues/597
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:30.624413'
  last_visited: '2024-12-30T15:18:13.864Z'
  main_tex_file: null
  published_date: '2024-08-12T11:47:11Z'
  state: open
  title: 'CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer'
  total_reading_time_seconds: 26
  url: https://arxiv.org/abs/2408.06072
'2409.04431':
  abstract: 'Attention is a key part of the transformer architecture. It is a

    sequence-to-sequence mapping that transforms each sequence element into a

    weighted sum of values. The weights are typically obtained as the softmax of

    dot products between keys and queries. Recent work has explored alternatives to

    softmax attention in transformers, such as ReLU and sigmoid activations. In

    this work, we revisit sigmoid attention and conduct an in-depth theoretical and

    empirical analysis. Theoretically, we prove that transformers with sigmoid

    attention are universal function approximators and benefit from improved

    regularity compared to softmax attention. Through detailed empirical analysis,

    we identify stabilization of large initial attention norms during the early

    stages of training as a crucial factor for the successful training of models

    with sigmoid attention, outperforming prior attempts. We also introduce

    FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid

    attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100

    GPUs. Experiments across language, vision, and speech show that properly

    normalized sigmoid attention matches the strong performance of softmax

    attention on a wide range of domains and scales, which previous attempts at

    sigmoid attention were unable to fully achieve. Our work unifies prior art and

    establishes best practices for sigmoid attention as a drop-in softmax

    replacement in transformers.'
  arxivId: '2409.04431'
  arxiv_tags:
  - cs.LG
  authors: Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge,
    Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, Russ
    Webb
  created_at: '2025-01-04T06:52:15.619463'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:15.620261'
  last_visited: '2025-01-02T19:38:28.512000+00:00'
  main_tex_file: null
  published_date: '2024-09-06T17:53:26Z'
  state: open
  title: Theory, Analysis, and Best Practices for Sigmoid Self-Attention
  total_reading_time_seconds: 8
  url: https://arxiv.org/abs/2409.04431
'2409.05816':
  abstract: 'Quality pretraining data is often seen as the key to high-performance

    language models. However, progress in understanding pretraining data has been

    slow due to the costly pretraining runs required for data selection

    experiments. We present a framework that avoids these costs and selects

    high-quality pretraining data without any LLM training of our own. Our work is

    based on a simple observation: LLM losses on many pretraining texts are

    correlated with downstream benchmark performance, and selecting

    high-correlation documents is an effective pretraining data selection method.

    We build a new statistical framework for data selection centered around

    estimates of perplexity-benchmark correlations and perform data selection using

    a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of

    thousands of web domains. In controlled pretraining experiments at the 160M

    parameter scale on 8 benchmarks, our approach outperforms DSIR on every

    benchmark, while matching the best data selector found in DataComp-LM, a

    hand-engineered bigram classifier.'
  arxivId: '2409.05816'
  arxiv_tags:
  - cs.CL
  - cs.LG
  - stat.ML
  authors: Tristan Thrush, Christopher Potts, Tatsunori Hashimoto
  created_at: '2025-01-04T06:53:00.603803'
  issue_number: 658
  issue_url: https://github.com/dmarx/papers-feed/issues/658
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:00.604590'
  last_visited: '2024-12-30T20:20:51.220Z'
  main_tex_file: null
  published_date: '2024-09-09T17:23:29Z'
  state: open
  title: Improving Pretraining Data Using Perplexity Correlations
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2409.05816
'2409.13731':
  abstract: 'The recently developed retrieval-augmented generation (RAG) technology
    has

    enabled the efficient construction of domain-specific applications. However, it

    also has limitations, including the gap between vector similarity and the

    relevance of knowledge reasoning, as well as insensitivity to knowledge logic,

    such as numerical values, temporal relations, expert rules, and others, which

    hinder the effectiveness of professional knowledge services. In this work, we

    introduce a professional domain knowledge service framework called Knowledge

    Augmented Generation (KAG). KAG is designed to address the aforementioned

    challenges with the motivation of making full use of the advantages of

    knowledge graph(KG) and vector retrieval, and to improve generation and

    reasoning performance by bidirectionally enhancing large language models (LLMs)

    and KGs through five key aspects: (1) LLM-friendly knowledge representation,

    (2) mutual-indexing between knowledge graphs and original chunks, (3)

    logical-form-guided hybrid reasoning engine, (4) knowledge alignment with

    semantic reasoning, and (5) model capability enhancement for KAG. We compared

    KAG with existing RAG methods in multihop question answering and found that it

    significantly outperforms state-of-theart methods, achieving a relative

    improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We

    have successfully applied KAG to two professional knowledge Q&A tasks of Ant

    Group, including E-Government Q&A and E-Health Q&A, achieving significant

    improvement in professionalism compared to RAG methods.'
  arxivId: '2409.13731'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong,
    Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu,
    Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou
  created_at: '2025-01-04T06:52:18.611027'
  issue_number: 720
  issue_url: https://github.com/dmarx/papers-feed/issues/720
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T15:07:25.587Z'
  main_tex_file: null
  published_date: '2024-09-10T02:00:28Z'
  state: open
  title: "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.13731
'2409.16986':
  abstract: 'Data selection is of great significance in pre-training large language

    models, given the variation in quality within the large-scale available

    training corpora. To achieve this, researchers are currently investigating the

    use of data influence to measure the importance of data instances, $i.e.,$ a

    high influence score indicates that incorporating this instance to the training

    set is likely to enhance the model performance. Consequently, they select the

    top-$k$ instances with the highest scores. However, this approach has several

    limitations. (1) Computing the influence of all available data is

    time-consuming. (2) The selected data instances are not diverse enough, which

    may hinder the pre-trained model''s ability to generalize effectively to various

    downstream tasks. In this paper, we introduce \texttt{Quad}, a data selection

    approach that considers both quality and diversity by using data influence to

    achieve state-of-the-art pre-training results. In particular, noting that

    attention layers capture extensive semantic details, we have adapted the

    accelerated $iHVP$ computation methods for attention layers, enhancing our

    ability to evaluate the influence of data, $i.e.,$ its quality. For the

    diversity, \texttt{Quad} clusters the dataset into similar data instances

    within each cluster and diverse instances across different clusters. For each

    cluster, if we opt to select data from it, we take some samples to evaluate the

    influence to prevent processing all instances. To determine which clusters to

    select, we utilize the classic Multi-Armed Bandit method, treating each cluster

    as an arm. This approach favors clusters with highly influential instances

    (ensuring high quality) or clusters that have been selected less frequently

    (ensuring diversity), thereby well balancing between quality and diversity.'
  arxivId: '2409.16986'
  arxiv_tags:
  - cs.AI
  authors: Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin
    Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ju Fan, Ye Yuan, Guoren Wang, Conghui
    He
  created_at: '2025-01-04T06:52:54.606152'
  issue_number: 637
  issue_url: https://github.com/dmarx/papers-feed/issues/637
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.651949'
  last_visited: '2024-12-30T20:04:00.391Z'
  main_tex_file: null
  published_date: '2024-09-25T14:49:29Z'
  state: open
  title: "Harnessing Diversity for Important Data Selection in Pretraining Large\n\
    \  Language Models"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2409.16986
'2409.19606':
  abstract: 'We present hyper-connections, a simple yet effective method that can
    serve as

    an alternative to residual connections. This approach specifically addresses

    common drawbacks observed in residual connection variants, such as the seesaw

    effect between gradient vanishing and representation collapse. Theoretically,

    hyper-connections allow the network to adjust the strength of connections

    between features at different depths and dynamically rearrange layers. We

    conduct experiments focusing on the pre-training of large language models,

    including dense and sparse models, where hyper-connections show significant

    performance improvements over residual connections. Additional experiments

    conducted on vision tasks also demonstrate similar improvements. We anticipate

    that this method will be broadly applicable and beneficial across a wide range

    of AI problems.'
  arxivId: '2409.19606'
  arxiv_tags:
  - cs.LG
  - cs.CL
  - cs.CV
  - cs.NE
  authors: Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu,
    Qiyang Min, Xun Zhou
  created_at: '2025-01-04T06:52:21.616669'
  issue_number: 718
  issue_url: https://github.com/dmarx/papers-feed/issues/718
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T08:05:54.690Z'
  main_tex_file: null
  published_date: '2024-09-29T07:57:07Z'
  state: open
  title: Hyper-Connections
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2409.19606
'2410.08800':
  abstract: 'This paper presents a comprehensive overview of the data preparation
    pipeline

    developed for the OpenGPT-X project, a large-scale initiative aimed at creating

    open and high-performance multilingual large language models (LLMs). The

    project goal is to deliver models that cover all major European languages, with

    a particular focus on real-world applications within the European Union. We

    explain all data processing steps, starting with the data selection and

    requirement definition to the preparation of the final datasets for model

    training. We distinguish between curated data and web data, as each of these

    categories is handled by distinct pipelines, with curated data undergoing

    minimal filtering and web data requiring extensive filtering and deduplication.

    This distinction guided the development of specialized algorithmic solutions

    for both pipelines. In addition to describing the processing methodologies, we

    provide an in-depth analysis of the datasets, increasing transparency and

    alignment with European data regulations. Finally, we share key insights and

    challenges faced during the project, offering recommendations for future

    endeavors in large-scale multilingual data preparation for LLMs.'
  arxivId: '2410.08800'
  arxiv_tags:
  - cs.CL
  - H.3.1; I.2.7
  authors: Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick, Lennard Helmer,
    Benny Jörg Stein, Pavel Denisov, Qasid Saleem, Michael Fromm, Mehdi Ali, Richard
    Rutmann, Farzad Naderi, Mohamad Saif Agy, Alexander Schwirjow, Fabian Küch, Luzian
    Hahn, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Dennis Wegener, Nicolas
    Flores-Herr, Joachim Köhler, Johannes Leveling
  created_at: '2025-01-04T06:52:45.621160'
  issue_number: 645
  issue_url: https://github.com/dmarx/papers-feed/issues/645
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:45.622955'
  last_visited: '2024-12-30T20:06:07.375Z'
  main_tex_file: null
  published_date: '2024-10-11T13:34:24Z'
  state: open
  title: Data Processing for the OpenGPT-X Model Family
  total_reading_time_seconds: 47
  url: https://arxiv.org/abs/2410.08800
'2410.13835':
  abstract: "Practitioners have consistently observed three puzzling phenomena in\n\
    transformer-based large language models (LLMs): attention sinks, value-state\n\
    drains, and residual-state peaks, collectively referred to as extreme-token\n\
    phenomena. These phenomena are characterized by certain so-called \"sink tokens\"\
    \nreceiving disproportionately high attention weights, exhibiting significantly\n\
    smaller value states, and having much larger residual-state norms than those of\n\
    other tokens. These extreme tokens give rise to various challenges in LLM\ninference,\
    \ quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token\
    \ phenomena. First, we show\nthat these phenomena arise in very simple architectures\
    \ -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy\
    \ (BB) task.\nIn this setting, we identify an active-dormant mechanism, where\
    \ attention heads\nbecome sinks for specific input domains while remaining non-sinks\
    \ for others.\nOur theoretical analysis of the training dynamics reveals that\
    \ these phenomena\nare driven by a mutual reinforcement mechanism. Building on\
    \ these insights, we\npropose strategies to mitigate extreme-token phenomena during\
    \ pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next,\
    \ we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing\
    \ that many\nattention heads exhibit a similar active-dormant mechanism as in\
    \ the BB task,\nand that the mutual reinforcement mechanism also governs the emergence\
    \ of\nextreme-token phenomena during LLM pretraining. Our results reveal that\
    \ many of\nthe static and dynamic properties of extreme-token phenomena predicted\
    \ by the\nBB task align with observations in pretrained LLMs."
  arxivId: '2410.13835'
  arxiv_tags:
  - cs.LG
  authors: Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei
  created_at: '2025-01-04T06:52:39.613983'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:39.614755'
  last_visited: '2024-12-30T22:17:13.815000+00:00'
  main_tex_file: null
  published_date: '2024-10-17T17:54:06Z'
  state: open
  title: "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token\
    \ Phenomena in LLMs"
  total_reading_time_seconds: 16
  url: https://arxiv.org/abs/2410.13835
'2410.21265':
  abstract: 'An old idea in optimization theory says that since the gradient is a
    dual

    vector it may not be subtracted from the weights without first being mapped to

    the primal space where the weights reside. We take this idea seriously in this

    paper and construct such a duality map for general neural networks. Our map,

    which we call modular dualization, forms a unifying theoretical basis for

    training algorithms that are a) fast and b) scalable. Modular dualization

    involves first assigning operator norms to layers based on the semantics of

    each layer, and then using these layerwise norms to recursively induce a

    duality map on the weight space of the full neural architecture. We conclude by

    deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers

    -- the latter two methods are based on a rectangular Newton-Schulz iteration

    (Kovarik, 1970; Bj\"orck & Bowie, 1971). A variant of our methods was used to

    set speed records for training NanoGPT. Overall, we hope that our theory of

    modular duality will yield a next generation of fast and scalable optimizers

    for general neural architectures.'
  arxivId: '2410.21265'
  arxiv_tags:
  - cs.LG
  - cs.NE
  - stat.ML
  authors: Jeremy Bernstein, Laker Newhouse
  created_at: '2025-01-04T06:52:09.651323'
  issue_number: 757
  issue_url: https://github.com/dmarx/papers-feed/issues/757
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T19:43:53.760Z'
  main_tex_file: null
  published_date: '2024-10-28T17:57:31Z'
  state: open
  title: Modular Duality in Deep Learning
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2410.21265
'2411.04330':
  abstract: 'Low precision training and inference affect both the quality and cost
    of

    language models, but current scaling laws do not account for this. In this

    work, we devise "precision-aware" scaling laws for both training and inference.

    We propose that training in lower precision reduces the model''s "effective

    parameter count," allowing us to predict the additional loss incurred from

    training in low precision and post-train quantization. For inference, we find

    that the degradation introduced by post-training quantization increases as

    models are trained on more data, eventually making additional pretraining data

    actively harmful. For training, our scaling laws allow us to predict the loss

    of a model with different parts in different precisions, and suggest that

    training larger models in lower precision may be compute optimal. We unify the

    scaling laws for post and pretraining quantization to arrive at a single

    functional form that predicts degradation from training and inference in varied

    precisions. We fit on over 465 pretraining runs and validate our predictions on

    model sizes up to 1.7B parameters trained on up to 26B tokens.'
  arxivId: '2411.04330'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas
    Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan
  created_at: '2025-01-04T06:53:12.611718'
  issue_number: 651
  issue_url: https://github.com/dmarx/papers-feed/issues/651
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:12.612468'
  last_visited: '2024-12-30T20:16:37.184Z'
  main_tex_file: null
  published_date: '2024-11-07T00:10:10Z'
  state: open
  title: Scaling Laws for Precision
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2411.04330
'2411.06068':
  abstract: 'In this technical report, we present Zyda-2: a five trillion token dataset

    for language model pretraining. Zyda-2 was used to train our Zamba2 series of

    models which are state-of-the-art for their weight class. We build Zyda-2 by

    collating high-quality open-source tokens such as FineWeb and DCLM, then

    distilling them to the highest-quality subset via cross-deduplication and

    model-based quality filtering. Zyda-2 is released under a permissive open

    license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2'
  arxivId: '2411.06068'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge
  created_at: '2025-01-04T06:52:36.611803'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2025-01-04T06:52:36.612646'
  last_visited: '2024-12-30T22:18:39.627000+00:00'
  main_tex_file: null
  published_date: '2024-11-09T04:57:41Z'
  state: open
  title: 'Zyda-2: a 5 Trillion Token High-Quality Dataset'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2411.06068
'2411.12372':
  abstract: 'Large language models are increasingly becoming a cornerstone technology
    in

    artificial intelligence, the sciences, and society as a whole, yet the optimal

    strategies for dataset composition and filtering remain largely elusive. Many

    of the top-performing models lack transparency in their dataset curation and

    model development processes, posing an obstacle to the development of fully

    open language models. In this paper, we identify three core data-related

    challenges that must be addressed to advance open-source language models. These

    include (1) transparency in model development, including the data curation

    process, (2) access to large quantities of high-quality data, and (3)

    availability of artifacts and metadata for dataset curation and analysis. To

    address these challenges, we release RedPajama-V1, an open reproduction of the

    LLaMA training dataset. In addition, we release RedPajama-V2, a massive

    web-only dataset consisting of raw, unfiltered text data together with quality

    signals and metadata. Together, the RedPajama datasets comprise over 100

    trillion tokens spanning multiple domains and with their quality signals

    facilitate the filtering of data, aiming to inspire the development of numerous

    new datasets. To date, these datasets have already been used in the training of

    strong language models used in production, such as Snowflake Arctic,

    Salesforce''s XGen and AI2''s OLMo. To provide insight into the quality of

    RedPajama, we present a series of analyses and ablation studies with

    decoder-only language models with up to 1.6B parameters. Our findings

    demonstrate how quality signals for web data can be effectively leveraged to

    curate high-quality subsets of the dataset, underscoring the potential of

    RedPajama to advance the development of transparent and high-performing

    language models at scale.'
  arxivId: '2411.12372'
  arxiv_tags:
  - cs.CL
  - cs.LG
  authors: Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton
    Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun,
    Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher
    Ré, Irina Rish, Ce Zhang
  created_at: '2025-01-04T06:53:06.611354'
  issue_number: 656
  issue_url: https://github.com/dmarx/papers-feed/issues/656
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:18:27.856Z'
  main_tex_file: null
  published_date: '2024-11-19T09:35:28Z'
  state: open
  title: 'RedPajama: an Open Dataset for Training Large Language Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2411.12372
'2412.02595':
  abstract: 'Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved

    significant benchmark gains via aggressive model-based filtering, but at the

    cost of removing 90% of data. This limits their suitability for long token

    horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how

    to achieve better trade-offs between accuracy and data quantity by a

    combination of classifier ensembling, synthetic data rephrasing, and reduced

    reliance on heuristic filters. When training 8B parameter models for 1T tokens,

    using a high-quality subset of our data improves MMLU by 5.6 over DCLM,

    demonstrating the efficacy of our methods for boosting accuracies over a

    relatively short token horizon. Furthermore, our full 6.3T token dataset

    matches DCLM on MMLU, but contains four times more unique real tokens than

    DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B

    parameter model trained for 15T tokens, of which 7.2T came from our dataset, is

    better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5

    on average across ten diverse tasks. The dataset is available at

    https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html'
  arxivId: '2412.02595'
  arxiv_tags:
  - cs.CL
  authors: Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl,
    Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro
  created_at: '2025-01-04T06:53:24.608407'
  issue_number: 632
  issue_url: https://github.com/dmarx/papers-feed/issues/632
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T20:02:25.502Z'
  main_tex_file: null
  published_date: '2024-12-03T17:28:50Z'
  state: open
  title: "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon\n  Pretraining\
    \ Dataset"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.02595
'2412.03603':
  abstract: 'Recent advancements in video generation have significantly impacted daily

    life for both individuals and industries. However, the leading video generation

    models remain closed-source, resulting in a notable performance gap between

    industry capabilities and those available to the public. In this report, we

    introduce HunyuanVideo, an innovative open-source video foundation model that

    demonstrates performance in video generation comparable to, or even surpassing,

    that of leading closed-source models. HunyuanVideo encompasses a comprehensive

    framework that integrates several key elements, including data curation,

    advanced architectural design, progressive model scaling and training, and an

    efficient infrastructure tailored for large-scale model training and inference.

    As a result, we successfully trained a video generative model with over 13

    billion parameters, making it the largest among all open-source models. We

    conducted extensive experiments and implemented a series of targeted designs to

    ensure high visual quality, motion dynamics, text-video alignment, and advanced

    filming techniques. According to evaluations by professionals, HunyuanVideo

    outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,

    and three top-performing Chinese video generative models. By releasing the code

    for the foundation model and its applications, we aim to bridge the gap between

    closed-source and open-source communities. This initiative will empower

    individuals within the community to experiment with their ideas, fostering a

    more dynamic and vibrant video generation ecosystem. The code is publicly

    available at https://github.com/Tencent/HunyuanVideo.'
  arxivId: '2412.03603'
  arxiv_tags:
  - cs.CV
  authors: Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng
    Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin
    Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan,
    Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai
    Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng,
    Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang
    Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang,
    Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong
  created_at: '2025-01-04T06:53:33.619685'
  issue_number: 595
  issue_url: https://github.com/dmarx/papers-feed/issues/595
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:33.622226'
  last_visited: '2024-12-30T15:17:10.593000+00:00'
  main_tex_file: null
  published_date: '2024-12-03T23:52:37Z'
  state: open
  title: 'HunyuanVideo: A Systematic Framework For Large Video Generative Models'
  total_reading_time_seconds: 53
  url: https://arxiv.org/abs/2412.03603
'2412.06769':
  abstract: 'Large language models (LLMs) are restricted to reason in the "language

    space", where they typically express the reasoning process with a

    chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue

    that language space may not always be optimal for reasoning. For example, most

    word tokens are primarily for textual coherence and not essential for

    reasoning, while some critical tokens require complex planning and pose huge

    challenges to LLMs. To explore the potential of LLM reasoning in an

    unrestricted latent space instead of using natural language, we introduce a new

    paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden

    state of the LLM as a representation of the reasoning state (termed "continuous

    thought"). Rather than decoding this into a word token, we feed it back to the

    LLM as the subsequent input embedding directly in the continuous space.

    Experiments show that Coconut can effectively augment the LLM on several

    reasoning tasks. This novel latent reasoning paradigm leads to emergent

    advanced reasoning patterns: the continuous thought can encode multiple

    alternative next reasoning steps, allowing the model to perform a breadth-first

    search (BFS) to solve the problem, rather than prematurely committing to a

    single deterministic path like CoT. Coconut outperforms CoT in certain logical

    reasoning tasks that require substantial backtracking during planning, with

    fewer thinking tokens during inference. These findings demonstrate the promise

    of latent reasoning and offer valuable insights for future research.'
  arxivId: '2412.06769'
  arxiv_tags:
  - cs.CL
  authors: Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston,
    Yuandong Tian
  created_at: '2025-01-04T06:52:12.616167'
  issue_number: 752
  issue_url: https://github.com/dmarx/papers-feed/issues/752
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:12.618753'
  last_visited: '2025-01-02T19:39:10.844Z'
  main_tex_file: null
  published_date: '2024-12-09T18:55:56Z'
  state: open
  title: Training Large Language Models to Reason in a Continuous Latent Space
  total_reading_time_seconds: 32
  url: https://arxiv.org/abs/2412.06769
'2412.06845':
  abstract: 'Recently, Large Language Models (LLMs) have undergone a significant

    transformation, marked by a rapid rise in both their popularity and

    capabilities. Leading this evolution are proprietary LLMs like GPT-4 and

    GPT-o1, which have captured widespread attention in the AI community due to

    their remarkable performance and versatility. Simultaneously, open-source LLMs,

    such as LLaMA and Mistral, have made great contributions to the ever-increasing

    popularity of LLMs due to the ease to customize and deploy the models across

    diverse applications. Although open-source LLMs present unprecedented

    opportunities for innovation and research, the commercialization of LLMs has

    raised concerns about transparency, reproducibility, and safety. Many

    open-source LLMs fail to meet fundamental transparency requirements by

    withholding essential components like training code and data, and some use

    restrictive licenses whilst claiming to be "open-source," which may hinder

    further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a

    fully open-source LLM developed in accordance with the Model Openness Framework

    (MOF), a ranked classification system that evaluates AI models based on model

    completeness and openness, adhering to principles of open science, open source,

    open data, and open access. Our model achieves the highest MOF classification

    level of "open science" through the comprehensive release of pre-training code

    and configurations, training and fine-tuning datasets, and intermediate and

    final checkpoints. Experiments show that our model achieves superior

    performance in zero-shot evaluation compared with popular 7B models and

    performs competitively in few-shot evaluation.'
  arxivId: '2412.06845'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht,
    Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue
    Chen, Yong He, Yanzhi Wang
  created_at: '2025-01-04T06:52:51.688399'
  issue_number: 639
  issue_url: https://github.com/dmarx/papers-feed/issues/639
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.651113'
  last_visited: '2024-12-30T20:04:21.071Z'
  main_tex_file: null
  published_date: '2024-12-08T02:01:46Z'
  state: open
  title: Fully Open Source Moxin-7B Technical Report
  total_reading_time_seconds: 53
  url: https://arxiv.org/abs/2412.06845
'2412.10271':
  abstract: 'The development and evaluation of Large Language Models (LLMs) has primarily

    focused on their task-solving capabilities, with recent models even surpassing

    human performance in some areas. However, this focus often neglects whether

    machine-generated language matches the human level of diversity, in terms of

    vocabulary choice, syntactic construction, and expression of meaning, raising

    questions about whether the fundamentals of language generation have been fully

    addressed. This paper emphasizes the importance of examining the preservation

    of human linguistic richness by language models, given the concerning surge in

    online content produced or aided by LLMs. We propose a comprehensive framework

    for evaluating LLMs from various linguistic diversity perspectives including

    lexical, syntactic, and semantic dimensions. Using this framework, we benchmark

    several state-of-the-art LLMs across all diversity dimensions, and conduct an

    in-depth case study for syntactic diversity. Finally, we analyze how different

    development and deployment choices impact the linguistic diversity of LLM

    outputs.'
  arxivId: '2412.10271'
  arxiv_tags:
  - cs.CL
  authors: Yanzhu Guo, Guokan Shang, Chloé Clavel
  created_at: '2025-01-04T06:52:57.610109'
  issue_number: 635
  issue_url: https://github.com/dmarx/papers-feed/issues/635
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.652874'
  last_visited: '2024-12-30T20:03:18.882Z'
  main_tex_file: null
  published_date: '2024-12-13T16:46:03Z'
  state: open
  title: Benchmarking Linguistic Diversity of Large Language Models
  total_reading_time_seconds: 52
  url: https://arxiv.org/abs/2412.10271
'2412.15285':
  abstract: 'Pretraining large language models effectively requires strategic data

    selection, blending and ordering. However, key details about data mixtures

    especially their scalability to longer token horizons and larger model sizes

    remain underexplored due to limited disclosure by model developers. To address

    this, we formalize the concept of two-phase pretraining and conduct an

    extensive systematic study on how to select and mix data to maximize model

    accuracies for the two phases. Our findings illustrate that a two-phase

    approach for pretraining outperforms random data ordering and natural

    distribution of tokens by 3.4% and 17% on average accuracies. We provide

    in-depth guidance on crafting optimal blends based on quality of the data

    source and the number of epochs to be seen. We propose to design blends using

    downsampled data at a smaller scale of 1T tokens and then demonstrate effective

    scaling of our approach to larger token horizon of 15T tokens and larger model

    size of 25B model size. These insights provide a series of steps practitioners

    can follow to design and scale their data blends.'
  arxivId: '2412.15285'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  authors: Steven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad
    Shoeybi, Bryan Catanzaro
  created_at: '2025-01-04T06:52:33.613492'
  issue_number: 643
  issue_url: https://github.com/dmarx/papers-feed/issues/643
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:18.649544'
  last_visited: '2024-12-30T20:04:52.739Z'
  main_tex_file: null
  published_date: '2024-12-18T18:41:18Z'
  state: open
  title: "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase\n\
    \  Pretraining"
  total_reading_time_seconds: 49
  url: https://arxiv.org/abs/2412.15285
'2412.17805':
  abstract: 'Learning a robust video Variational Autoencoder (VAE) is essential for

    reducing video redundancy and facilitating efficient video generation. Directly

    applying image VAEs to individual frames in isolation can result in temporal

    inconsistencies and suboptimal compression rates due to a lack of temporal

    compression. Existing Video VAEs have begun to address temporal compression;

    however, they often suffer from inadequate reconstruction performance. In this

    paper, we present a novel and powerful video autoencoder capable of

    high-fidelity video encoding. First, we observe that entangling spatial and

    temporal compression by merely extending the image VAE to a 3D VAE can

    introduce motion blur and detail distortion artifacts. Thus, we propose

    temporal-aware spatial compression to better encode and decode the spatial

    information. Additionally, we integrate a lightweight motion compression model

    for further temporal compression. Second, we propose to leverage the textual

    information inherent in text-to-video datasets and incorporate text guidance

    into our model. This significantly enhances reconstruction quality,

    particularly in terms of detail preservation and temporal stability. Third, we

    further improve the versatility of our model through joint training on both

    images and videos, which not only enhances reconstruction quality but also

    enables the model to perform both image and video autoencoding. Extensive

    evaluations against strong recent baselines demonstrate the superior

    performance of our method. The project website can be found

    at~\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.'
  arxivId: '2412.17805'
  arxiv_tags:
  - cs.CV
  authors: Yazhou Xing, Yang Fei, Yingqing He, Jingye Chen, Jiaxin Xie, Xiaowei Chi,
    Qifeng Chen
  created_at: '2025-01-04T06:52:24.663099'
  issue_number: 716
  issue_url: https://github.com/dmarx/papers-feed/issues/716
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2025-01-02T08:03:08.915Z'
  main_tex_file: null
  published_date: '2024-12-23T18:58:24Z'
  state: open
  title: Large Motion Video Autoencoding with Cross-modal Video VAE
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2412.17805
'2412.17847':
  abstract: 'Progress in AI is driven largely by the scale and quality of training
    data.

    Despite this, there is a deficit of empirical analysis examining the attributes

    of well-established datasets beyond text. In this work we conduct the largest

    and first-of-its-kind longitudinal audit across modalities--popular text,

    speech, and video datasets--from their detailed sourcing trends and use

    restrictions to their geographical and linguistic representation. Our manual

    analysis covers nearly 4000 public datasets between 1990-2024, spanning 608

    languages, 798 sources, 659 organizations, and 67 countries. We find that

    multimodal machine learning applications have overwhelmingly turned to

    web-crawled, synthetic, and social media platforms, such as YouTube, for their

    training sets, eclipsing all other sources since 2019. Secondly, tracing the

    chain of dataset derivations we find that while less than 33% of datasets are

    restrictively licensed, over 80% of the source content in widely-used text,

    speech, and video datasets, carry non-commercial restrictions. Finally, counter

    to the rising number of languages and geographies represented in public AI

    training datasets, our audit demonstrates measures of relative geographical and

    multilingual representation have failed to significantly improve their coverage

    since 2013. We believe the breadth of our audit enables us to empirically

    examine trends in data sourcing, restrictions, and Western-centricity at an

    ecosystem-level, and that visibility into these questions are essential to

    progress in responsible AI. As a contribution to ongoing improvements in

    dataset transparency and responsible use, we release our entire multimodal

    audit, allowing practitioners to trace data provenance across text, speech, and

    video.'
  arxivId: '2412.17847'
  arxiv_tags:
  - cs.AI
  - cs.CL
  - cs.CY
  - cs.LG
  - cs.MM
  authors: Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska,
    William Brannon, Robert Mahari, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad
    Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Naana Obeng-Marnu, Da Yin, Kun
    Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas,
    Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm,
    Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda,
    Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui
    Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker,
    Jad Kabbara
  created_at: '2025-01-04T06:53:21.616757'
  issue_number: 633
  issue_url: https://github.com/dmarx/papers-feed/issues/633
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:53:21.617516'
  last_visited: '2024-12-30T20:03:13.039Z'
  main_tex_file: null
  published_date: '2024-12-19T01:30:19Z'
  state: open
  title: Bridging the Data Provenance Gap Across Text, Speech and Video
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2412.17847
'2412.19792':
  abstract: 'Language model alignment has become a critical step in training modern

    generative language models. The goal of alignment is to finetune a reference

    model such that the win rate of a sample from the aligned model over a sample

    from the reference model is high, subject to a KL divergence constraint. Today,

    we are increasingly using inference-time algorithms (e.g., Best-of-N,

    controlled decoding, tree search) to decode from language models rather than

    standard sampling. However, the alignment objective does not capture such

    inference-time decoding procedures. We show that the existing alignment

    framework is sub-optimal in view of such inference-time methods. We then modify

    the alignment objective and propose a framework for inference-aware alignment

    (IAPO). We prove that for any inference-time decoding algorithm, the optimal

    solution that optimizes the inference-time win rate of the aligned policy

    against the reference policy is the solution to the typical RLHF problem with
    a

    transformation of the reward. This motivates us to provide the KL-regularized

    calibrate-and-transform RL (CTRL) algorithm to solve this problem, which

    involves a reward calibration step and a KL-regularized reward maximization

    step with a transformation of the calibrated reward. We particularize our study

    to two important inference-time strategies: best-of-N sampling and best-of-N

    jailbreaking, where N responses are sampled from the model and the one with the

    highest or lowest reward is selected. We propose specific transformations for

    these strategies and demonstrate that our framework offers significant

    improvements over existing state-of-the-art methods for language model

    alignment. Empirically, we outperform baselines that are designed without

    taking inference-time decoding into consideration by 8-12% and 4-9% on

    inference-time win rates over the Anthropic helpfulness and harmlessness dialog

    benchmark datasets.'
  arxivId: '2412.19792'
  arxiv_tags:
  - cs.LG
  - cs.CL
  - cs.IT
  - math.IT
  authors: Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael
    Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha,
    Ananda Theertha Suresh, Ahmad Beirami
  created_at: '2025-01-04T06:51:54.729533'
  issue_number: 763
  issue_url: https://github.com/dmarx/papers-feed/issues/763
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:00.645791'
  last_visited: '2025-01-03T08:59:10.679Z'
  main_tex_file: null
  published_date: '2024-12-27T18:45:36Z'
  state: open
  title: 'InfAlign: Inference-aware language model alignment'
  total_reading_time_seconds: 49
  url: https://arxiv.org/abs/2412.19792
'2412.20292':
  abstract: 'We obtain the first analytic, interpretable and predictive theory of

    creativity in convolutional diffusion models. Indeed, score-based diffusion

    models can generate highly creative images that lie far from their training

    data. But optimal score-matching theory suggests that these models should only

    be able to produce memorized training examples. To reconcile this

    theory-experiment gap, we identify two simple inductive biases, locality and

    equivariance, that: (1) induce a form of combinatorial creativity by preventing

    optimal score-matching; (2) result in a fully analytic, completely

    mechanistically interpretable, equivariant local score (ELS) machine that, (3)

    without any training can quantitatively predict the outputs of trained

    convolution only diffusion models (like ResNets and UNets) with high accuracy

    (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our

    ELS machine reveals a locally consistent patch mosaic model of creativity, in

    which diffusion models create exponentially many novel images by mixing and

    matching different local training set patches in different image locations. Our

    theory also partially predicts the outputs of pre-trained self-attention

    enabled UNets (median $r^2 \sim 0.75$ on CIFAR10), revealing an intriguing role

    for attention in carving out semantic coherence from local patch mosaics.'
  arxivId: '2412.20292'
  arxiv_tags:
  - cs.LG
  - cond-mat.dis-nn
  - cs.AI
  - q-bio.NC
  - stat.ML
  - I.2.10
  authors: Mason Kamb, Surya Ganguli
  created_at: '2025-01-04T06:52:30.603137'
  issue_number: 708
  issue_url: https://github.com/dmarx/papers-feed/issues/708
  labels:
  - paper
  - rating:novote
  last_read: '2025-01-04T06:52:30.604171'
  last_visited: '2025-01-01T16:00:07.088Z'
  main_tex_file: null
  published_date: '2024-12-28T22:33:29Z'
  state: open
  title: An analytic theory of creativity in convolutional diffusion models
  total_reading_time_seconds: 3
  url: https://arxiv.org/abs/2412.20292
