'1201.1717':
  abstract: "Hyperbolicity is a property of a graph that may be viewed as being a\
    \ \"soft\"\nversion of a tree, and recent empirical and theoretical work has suggested\
    \ that\nmany graphs arising in Internet and related data applications have hyperbolic\n\
    properties. We consider Gromov's notion of \\delta-hyperbolicity, and establish\n\
    several results for small-world and tree-like random graph models. First, we\n\
    study the hyperbolicity of Kleinberg small-world random graphs and show that\n\
    the hyperbolicity of these random graphs is not significantly improved\ncomparing\
    \ to graph diameter even when it greatly improves decentralized\nnavigation. Next\
    \ we study a class of tree-like graphs called ringed trees that\nhave constant\
    \ hyperbolicity. We show that adding random links among the leaves\nsimilar to\
    \ the small-world graph constructions may easily destroy the\nhyperbolicity of\
    \ the graphs, except for a class of random edges added using an\nexponentially\
    \ decaying probability function based on the ring distance among\nthe leaves.\n\
    \  Our study provides one of the first significant analytical results on the\n\
    hyperbolicity of a rich class of random graphs, which shed light on the\nrelationship\
    \ between hyperbolicity and navigability of random graphs, as well\nas on the\
    \ sensitivity of hyperbolic {\\delta} to noises in random graphs."
  arxivId: '1201.1717'
  arxiv_tags:
  - cs.SI
  - cs.DM
  - physics.soc-ph
  authors: Wei Chen, Wenjie Fang, Guangda Hu, Michael W. Mahoney
  created_at: '2024-12-30T08:28:10.543868'
  issue_number: 445
  issue_url: https://github.com/dmarx/papers-feed/issues/445
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:41:46.909Z'
  main_tex_file: null
  published_date: '2012-01-09T09:30:38Z'
  state: open
  title: On the Hyperbolicity of Small-World and Tree-Like Random Graphs
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1201.1717
'1602.05897':
  abstract: 'We develop a general duality between neural networks and compositional

    kernels, striving towards a better understanding of deep learning. We show that

    initial representations generated by common random initializations are

    sufficiently rich to express all functions in the dual kernel space. Hence,

    though the training objective is hard to optimize in the worst case, the

    initial weights form a good starting point for optimization. Our dual view also

    reveals a pragmatic and aesthetic perspective of neural networks and

    underscores their expressive power.'
  arxivId: '1602.05897'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CC
  - cs.DS
  - stat.ML
  authors: Amit Daniely, Roy Frostig, Yoram Singer
  created_at: '2024-12-30T08:27:37.538781'
  issue_number: 496
  issue_url: https://github.com/dmarx/papers-feed/issues/496
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T11:06:20.722Z'
  main_tex_file: null
  published_date: '2016-02-18T18:14:19Z'
  state: open
  title: "Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization\
    \ and a Dual View on Expressivity"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1602.05897
'1705.08039':
  abstract: 'Representation learning has become an invaluable approach for learning
    from

    symbolic data such as text and graphs. However, while complex symbolic datasets

    often exhibit a latent hierarchical structure, state-of-the-art methods

    typically learn embeddings in Euclidean vector spaces, which do not account for

    this property. For this purpose, we introduce a new approach for learning

    hierarchical representations of symbolic data by embedding them into hyperbolic

    space -- or more precisely into an n-dimensional Poincar\''e ball. Due to the

    underlying hyperbolic geometry, this allows us to learn parsimonious

    representations of symbolic data by simultaneously capturing hierarchy and

    similarity. We introduce an efficient algorithm to learn the embeddings based

    on Riemannian optimization and show experimentally that Poincar\''e embeddings

    outperform Euclidean embeddings significantly on data with latent hierarchies,

    both in terms of representation capacity and in terms of generalization

    ability.'
  arxivId: '1705.08039'
  arxiv_tags:
  - cs.AI
  - cs.LG
  - stat.ML
  authors: Maximilian Nickel, Douwe Kiela
  created_at: '2024-12-30T08:27:58.558111'
  issue_number: 447
  issue_url: https://github.com/dmarx/papers-feed/issues/447
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:07.585743'
  last_visited: '2024-12-29T01:43:25.284Z'
  main_tex_file: null
  published_date: '2017-05-22T23:14:36Z'
  state: open
  title: Poincaré Embeddings for Learning Hierarchical Representations
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/1705.08039
'1705.10359':
  abstract: 'Neural embeddings have been used with great success in Natural Language

    Processing (NLP). They provide compact representations that encapsulate word

    similarity and attain state-of-the-art performance in a range of linguistic

    tasks. The success of neural embeddings has prompted significant amounts of

    research into applications in domains other than language. One such domain is

    graph-structured data, where embeddings of vertices can be learned that

    encapsulate vertex similarity and improve performance on tasks including edge

    prediction and vertex labelling. For both NLP and graph based tasks, embeddings

    have been learned in high-dimensional Euclidean spaces. However, recent work

    has shown that the appropriate isometric space for embedding complex networks

    is not the flat Euclidean space, but negatively curved, hyperbolic space. We

    present a new concept that exploits these recent insights and propose learning

    neural embeddings of graphs in hyperbolic space. We provide experimental

    evidence that embedding graphs in their natural geometry significantly improves

    performance on downstream tasks for several real-world public datasets.'
  arxivId: '1705.10359'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth
  created_at: '2024-12-30T08:28:01.537841'
  issue_number: 448
  issue_url: https://github.com/dmarx/papers-feed/issues/448
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:07.586810'
  last_visited: '2024-12-29T01:43:26.258Z'
  main_tex_file: null
  published_date: '2017-05-29T18:47:30Z'
  state: open
  title: Neural Embeddings of Graphs in Hyperbolic Space
  total_reading_time_seconds: 14
  url: https://arxiv.org/abs/1705.10359
'1802.04956':
  abstract: 'For many machine learning problem settings, particularly with structured

    inputs such as sequences or sets of objects, a distance measure between inputs

    can be specified more naturally than a feature representation. However, most

    standard machine models are designed for inputs with a vector feature

    representation. In this work, we consider the estimation of a function

    $f:\mathcal{X} \rightarrow \R$ based solely on a dissimilarity measure

    $d:\mathcal{X}\times\mathcal{X} \rightarrow \R$ between inputs. In particular,

    we propose a general framework to derive a family of \emph{positive definite

    kernels} from a given dissimilarity measure, which subsumes the widely-used

    \emph{representative-set method} as a special case, and relates to the

    well-known \emph{distance substitution kernel} in a limiting case. We show that

    functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are

    Lipschitz-continuous w.r.t. the given distance metric. We provide a tractable

    algorithm to estimate a function from this RKHS, and show that it enjoys better

    generalizability than Nearest-Neighbor estimates. Our approach draws from the

    literature of Random Features, but instead of deriving feature maps from an

    existing kernel, we construct novel kernels from a random feature map, that we

    specify given the distance measure. We conduct classification experiments with

    such disparate domains as strings, time series, and sets of vectors, where our

    proposed framework compares favorably to existing distance-based learning

    methods such as $k$-nearest-neighbors, distance-substitution kernels,

    pseudo-Euclidean embedding, and the representative-set method.'
  arxivId: '1802.04956'
  arxiv_tags:
  - stat.ML
  - cs.LG
  authors: Lingfei Wu, Ian En-Hsu Yen, Fangli Xu, Pradeep Ravikumar, Michael Witbrock
  created_at: '2024-12-30T08:27:43.532109'
  issue_number: 493
  issue_url: https://github.com/dmarx/papers-feed/issues/493
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T10:54:09.487Z'
  main_tex_file: null
  published_date: '2018-02-14T04:58:13Z'
  state: open
  title: 'D2KE: From Distance to Kernel and Embedding'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1802.04956
'1804.03329':
  abstract: 'Hyperbolic embeddings offer excellent quality with few dimensions when

    embedding hierarchical data structures like synonym or type hierarchies. Given

    a tree, we give a combinatorial construction that embeds the tree in hyperbolic

    space with arbitrarily low distortion without using optimization. On WordNet,

    our combinatorial embedding obtains a mean-average-precision of 0.989 with only

    two dimensions, while Nickel et al.''s recent construction obtains 0.87 using

    200 dimensions. We provide upper and lower bounds that allow us to characterize

    the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To

    embed general metric spaces, we propose a hyperbolic generalization of

    multidimensional scaling (h-MDS). We show how to perform exact recovery of

    hyperbolic points from distances, provide a perturbation analysis, and give a

    recovery result that allows us to reduce dimensionality. The h-MDS approach

    offers consistently low distortion even with few dimensions across several

    datasets. Finally, we extract lessons from the algorithms and theory above to

    design a PyTorch-based implementation that can handle incomplete information

    and is scalable.'
  arxivId: '1804.03329'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Christopher De Sa, Albert Gu, Christopher Ré, Frederic Sala
  created_at: '2024-12-30T08:27:55.545975'
  issue_number: 457
  issue_url: https://github.com/dmarx/papers-feed/issues/457
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T02:34:35.575Z'
  main_tex_file: null
  published_date: '2018-04-10T03:39:16Z'
  state: open
  title: Representation Tradeoffs for Hyperbolic Embeddings
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/1804.03329
'1806.00468':
  abstract: 'We show that gradient descent on full-width linear convolutional networks
    of

    depth $L$ converges to a linear predictor related to the $\ell_{2/L}$ bridge

    penalty in the frequency domain. This is in contrast to linearly fully

    connected networks, where gradient descent converges to the hard margin linear

    support vector machine solution, regardless of depth.'
  arxivId: '1806.00468'
  arxiv_tags:
  - cs.LG
  - stat.ML
  authors: Suriya Gunasekar, Jason Lee, Daniel Soudry, Nathan Srebro
  created_at: '2024-12-30T08:27:34.531778'
  issue_number: 495
  issue_url: https://github.com/dmarx/papers-feed/issues/495
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:34.533947'
  last_visited: '2024-12-29T11:05:59.663Z'
  main_tex_file: null
  published_date: '2018-06-01T17:58:58Z'
  state: open
  title: Implicit Bias of Gradient Descent on Linear Convolutional Networks
  total_reading_time_seconds: 23
  url: https://arxiv.org/abs/1806.00468
'2105.05720':
  abstract: "Recent trend towards increasing large machine learning models require\
    \ both\ntraining and inference tasks to be distributed. Considering the huge cost\
    \ of\ntraining these models, it is imperative to unlock optimizations in computation\n\
    and communication to obtain best performance. However, current logical\nseparation\
    \ between computation and communication kernels in deep learning\nframeworks misses\
    \ the optimization opportunities across such barrier. Breaking\nthis abstraction\
    \ with a holistic consideration can provide many optimizations\nto provide performance\
    \ improvements in distributed workloads. Manually applying\nthese optimizations\
    \ needs modifications in underlying computation and\ncommunication libraries for\
    \ each scenario, which is time consuming and\nerror-prone.\n  Therefore, we present\
    \ CoCoNeT, with a DSL to express a program with both\ncomputation and communication.\
    \ CoCoNeT contains several machine learning aware\ntransformations to optimize\
    \ a program and a compiler to generate high\nperformance kernels. Providing both\
    \ computation and communication as first\nclass constructs allows users to work\
    \ on a high-level abstraction and apply\npowerful optimizations, such as fusion\
    \ or overlapping of communication and\ncomputation. CoCoNeT enables us to optimize\
    \ data-, model-and pipeline-parallel\nworkloads in large language models with\
    \ only a few lines of code. Experiments\nshow CoCoNeT significantly outperforms\
    \ state-of-the-art distributed machine\nlearning implementations."
  arxivId: '2105.05720'
  arxiv_tags:
  - cs.DC
  - cs.LG
  - cs.PL
  authors: Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed
    Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi
  created_at: '2024-12-30T08:26:40.759202'
  issue_number: 581
  issue_url: https://github.com/dmarx/papers-feed/issues/581
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:26:40.761102'
  last_visited: '2024-12-30T07:58:09.742Z'
  main_tex_file: null
  published_date: '2021-05-12T15:13:43Z'
  state: open
  title: "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed\
    \ Machine Learning Workloads"
  total_reading_time_seconds: 38
  url: https://arxiv.org/abs/2105.05720
'2106.10165':
  abstract: 'This book develops an effective theory approach to understanding deep
    neural

    networks of practical relevance. Beginning from a first-principles

    component-level picture of networks, we explain how to determine an accurate

    description of the output of trained networks by solving layer-to-layer

    iteration equations and nonlinear learning dynamics. A main result is that the

    predictions of networks are described by nearly-Gaussian distributions, with

    the depth-to-width aspect ratio of the network controlling the deviations from

    the infinite-width Gaussian description. We explain how these effectively-deep

    networks learn nontrivial representations from training and more broadly

    analyze the mechanism of representation learning for nonlinear models. From a

    nearly-kernel-methods perspective, we find that the dependence of such models''

    predictions on the underlying learning algorithm can be expressed in a simple

    and universal way. To obtain these results, we develop the notion of

    representation group flow (RG flow) to characterize the propagation of signals

    through the network. By tuning networks to criticality, we give a practical

    solution to the exploding and vanishing gradient problem. We further explain

    how RG flow leads to near-universal behavior and lets us categorize networks

    built from different activation functions into universality classes.

    Altogether, we show that the depth-to-width ratio governs the effective model

    complexity of the ensemble of trained networks. By using information-theoretic

    techniques, we estimate the optimal aspect ratio at which we expect the network

    to be practically most useful and show how residual connections can be used to

    push this scale to arbitrary depths. With these tools, we can learn in detail

    about the inductive bias of architectures, hyperparameters, and optimizers.'
  arxivId: '2106.10165'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - hep-th
  - stat.ML
  authors: Daniel A. Roberts, Sho Yaida, Boris Hanin
  created_at: '2024-12-30T08:27:19.544402'
  issue_number: 523
  issue_url: https://github.com/dmarx/papers-feed/issues/523
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T22:46:13.679Z'
  main_tex_file: null
  published_date: '2021-06-18T15:00:00Z'
  state: open
  title: The Principles of Deep Learning Theory
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2106.10165
'2207.10342':
  abstract: 'Prompted models have demonstrated impressive few-shot learning abilities.

    Repeated interactions at test-time with a single model, or the composition of

    multiple models together, further expands capabilities. These compositions are

    probabilistic models, and may be expressed in the language of graphical models

    with random variables whose values are complex data types such as strings.

    Cases with control flow and dynamic structure require techniques from

    probabilistic programming, which allow implementing disparate model structures

    and inference strategies in a unified language. We formalize several existing

    techniques from this perspective, including scratchpads / chain of thought,

    verifiers, STaR, selection-inference, and tool use. We refer to the resulting

    programs as language model cascades.'
  arxivId: '2207.10342'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael
    Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein,
    Kevin Murphy, Charles Sutton
  created_at: '2024-12-30T08:27:49.537065'
  issue_number: 461
  issue_url: https://github.com/dmarx/papers-feed/issues/461
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T08:48:42.548Z'
  main_tex_file: null
  published_date: '2022-07-21T07:35:18Z'
  state: open
  title: Language Model Cascades
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2207.10342
'2208.11665':
  abstract: 'The Manifold Hypothesis is a widely accepted tenet of Machine Learning
    which

    asserts that nominally high-dimensional data are in fact concentrated near a

    low-dimensional manifold, embedded in high-dimensional space. This phenomenon

    is observed empirically in many real world situations, has led to development

    of a wide range of statistical methods in the last few decades, and has been

    suggested as a key factor in the success of modern AI technologies. We show

    that rich and sometimes intricate manifold structure in data can emerge from a

    generic and remarkably simple statistical model -- the Latent Metric Model --

    via elementary concepts such as latent variables, correlation and stationarity.

    This establishes a general statistical explanation for why the Manifold

    Hypothesis seems to hold in so many situations. Informed by the Latent Metric

    Model we derive procedures to discover and interpret the geometry of

    high-dimensional data, and explore hypotheses about the data generating

    mechanism. These procedures operate under minimal assumptions and make use of

    well known, scaleable graph-analytic algorithms.'
  arxivId: '2208.11665'
  arxiv_tags:
  - stat.ME
  - cs.LG
  - stat.ML
  - 62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30
  authors: Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy
  created_at: '2024-12-30T08:28:07.584219'
  issue_number: 452
  issue_url: https://github.com/dmarx/papers-feed/issues/452
  labels:
  - paper
  - rating:downvote
  last_read: null
  last_visited: '2024-12-29T02:26:31.276Z'
  main_tex_file: null
  published_date: '2022-08-24T17:00:16Z'
  state: open
  title: Statistical exploration of the Manifold Hypothesis
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2208.11665
'2302.10866':
  abstract: 'Recent advances in deep learning have relied heavily on the use of large

    Transformers due to their ability to learn at scale. However, the core building

    block of Transformers, the attention operator, exhibits quadratic cost in

    sequence length, limiting the amount of context accessible. Existing

    subquadratic methods based on low-rank and sparse approximations need to be

    combined with dense attention layers to match Transformers, indicating a gap in

    capability. In this work, we propose Hyena, a subquadratic drop-in replacement

    for attention constructed by interleaving implicitly parametrized long

    convolutions and data-controlled gating. In recall and reasoning tasks on

    sequences of thousands to hundreds of thousands of tokens, Hyena improves

    accuracy by more than 50 points over operators relying on state-spaces and

    other implicit and explicit methods, matching attention-based models. We set a

    new state-of-the-art for dense-attention-free architectures on language

    modeling in standard datasets (WikiText103 and The Pile), reaching Transformer

    quality with a 20% reduction in training compute required at sequence length

    2K. Hyena operators are twice as fast as highly optimized attention at sequence

    length 8K, and 100x faster at sequence length 64K.'
  arxivId: '2302.10866'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen
    Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré
  created_at: '2024-12-30T08:28:22.546796'
  issue_number: 401
  issue_url: https://github.com/dmarx/papers-feed/issues/401
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-28T07:17:27.699Z'
  main_tex_file: null
  published_date: '2023-02-21T18:29:25Z'
  state: open
  title: 'Hyena Hierarchy: Towards Larger Convolutional Language Models'
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2302.10866
'2302.13714':
  abstract: 'In this work, we investigate a challenging problem, which has been considered

    to be an important criterion in designing codewords for DNA computing purposes,

    namely secondary structure avoidance in single-stranded DNA molecules. In

    short, secondary structure refers to the tendency of a single-stranded DNA

    sequence to fold back upon itself, thus becoming inactive in the computation

    process. While some design criteria that reduces the possibility of secondary

    structure formation has been proposed by Milenkovic and Kashyap (2006), the

    main contribution of this work is to provide an explicit construction of DNA

    codes that completely avoid secondary structure of arbitrary stem length.

    Formally, given codeword length n and arbitrary integer m>=2, we provide

    efficient methods to construct DNA codes of length n that avoid secondary

    structure of any stem length more than or equal to m. Particularly, when m = 3,

    our constructions yield a family of DNA codes of rate 1.3031 bits/nt, while the

    highest rate found in the prior art was 1.1609 bits/nt. In addition, for

    m>=3log n + 4, we provide an efficient encoder that incurs only one redundant

    symbol.'
  arxivId: '2302.13714'
  arxiv_tags:
  - cs.IT
  - math.CO
  - math.IT
  authors: Tuan Thanh Nguyen, Kui Cai, Han Mao Kiah, Duc Tu Dao, Kees A. Schouhamer
    Immink
  created_at: '2024-12-30T08:28:25.573770'
  issue_number: 400
  issue_url: https://github.com/dmarx/papers-feed/issues/400
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:25.574539'
  last_visited: '2024-12-28T07:13:40.029Z'
  main_tex_file: null
  published_date: '2023-02-27T12:22:07Z'
  state: open
  title: "On the Design of Codes for DNA Computing: Secondary Structure Avoidance\n\
    \  Codes"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2302.13714
'2307.09288':
  abstract: 'In this work, we develop and release Llama 2, a collection of pretrained
    and

    fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70

    billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for

    dialogue use cases. Our models outperform open-source chat models on most

    benchmarks we tested, and based on our human evaluations for helpfulness and

    safety, may be a suitable substitute for closed-source models. We provide a

    detailed description of our approach to fine-tuning and safety improvements of

    Llama 2-Chat in order to enable the community to build on our work and

    contribute to the responsible development of LLMs.'
  arxivId: '2307.09288'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
  created_at: '2024-12-30T08:28:19.536753'
  issue_number: 434
  issue_url: https://github.com/dmarx/papers-feed/issues/434
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:19.537524'
  last_visited: '2024-12-28T20:46:35.699Z'
  main_tex_file: null
  published_date: '2023-07-18T14:31:57Z'
  state: open
  title: 'Llama 2: Open Foundation and Fine-Tuned Chat Models'
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2307.09288
'2308.06259':
  abstract: 'We present a scalable method to build a high quality instruction following

    language model by automatically labelling human-written text with corresponding

    instructions. Our approach, named instruction backtranslation, starts with a

    language model finetuned on a small amount of seed data, and a given web

    corpus. The seed model is used to construct training examples by generating

    instruction prompts for web documents (self-augmentation), and then selecting

    high quality examples from among these candidates (self-curation). This data is

    then used to finetune a stronger model. Finetuning LLaMa on two iterations of

    our approach yields a model that outperforms all other LLaMa-based models on

    the Alpaca leaderboard not relying on distillation data, demonstrating highly

    effective self-alignment.'
  arxivId: '2308.06259'
  arxiv_tags:
  - cs.CL
  authors: Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer,
    Jason Weston, Mike Lewis
  created_at: '2024-12-30T08:26:46.536724'
  issue_number: 575
  issue_url: https://github.com/dmarx/papers-feed/issues/575
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-30T04:57:10.471Z'
  main_tex_file: null
  published_date: '2023-08-11T17:47:54Z'
  state: open
  title: Self-Alignment with Instruction Backtranslation
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2308.06259
'2309.07965':
  abstract: 'In this work, we introduce the concept of relative Lipschitz saturation,

    along with its key categorical and algebraic properties, and demonstrate how

    such a structure always gives rise to a radicial algebra.'
  arxivId: '2309.07965'
  arxiv_tags:
  - math.AC
  - 13B22
  authors: Thiago da Silva, Guilherme Schultz Netto
  created_at: '2024-12-30T08:28:28.531062'
  issue_number: 398
  issue_url: https://github.com/dmarx/papers-feed/issues/398
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:28.532202'
  last_visited: '2024-12-28T07:12:00.313000+00:00'
  main_tex_file: null
  published_date: '2023-09-14T18:02:12Z'
  state: open
  title: "A survey on relative Lipschitz saturation of algebras and its relation\n\
    \  with radicial algebras"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2309.07965
'2312.17127':
  abstract: "We study semantic models of probabilistic programming languages over\
    \ graphs,\nand establish a connection to graphons from graph theory and combinatorics.\
    \ We\nshow that every well-behaved equational theory for our graph probabilistic\n\
    programming language corresponds to a graphon, and conversely, every graphon\n\
    arises in this way.\n  We provide three constructions for showing that every graphon\
    \ arises from an\nequational theory. The first is an abstract construction, using\
    \ Markov\ncategories and monoidal indeterminates. The second and third are more\
    \ concrete.\nThe second is in terms of traditional measure theoretic probability,\
    \ which\ncovers 'black-and-white' graphons. The third is in terms of probability\
    \ monads\non the nominal sets of Gabbay and Pitts. Specifically, we use a variation\
    \ of\nnominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\\
    'enyi\ngraphons. In this way, we build new models of graph probabilistic programming\n\
    from graphons."
  arxivId: '2312.17127'
  arxiv_tags:
  - cs.PL
  - cs.LO
  - math.PR
  authors: Nathanael L. Ackerman, Cameron E. Freer, Younesse Kaddar, Jacek Karwowski,
    Sean K. Moss, Daniel M. Roy, Sam Staton, Hongseok Yang
  created_at: '2024-12-30T08:27:28.537115'
  issue_number: 509
  issue_url: https://github.com/dmarx/papers-feed/issues/509
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:28.538601'
  last_visited: '2024-12-29T19:52:09.230Z'
  main_tex_file: null
  published_date: '2023-12-28T17:04:50Z'
  state: open
  title: "Probabilistic programming interfaces for random graphs: Markov\n  categories,\
    \ graphons, and nominal sets"
  total_reading_time_seconds: 10
  url: https://arxiv.org/abs/2312.17127
'2401.10774':
  abstract: "Large Language Models (LLMs) employ auto-regressive decoding that requires\n\
    sequential computation, with each step reliant on the previous one's output.\n\
    This creates a bottleneck as each step necessitates moving the full model\nparameters\
    \ from High-Bandwidth Memory (HBM) to the accelerator's cache. While\nmethods\
    \ such as speculative decoding have been suggested to address this issue,\ntheir\
    \ implementation is impeded by the challenges associated with acquiring and\n\
    maintaining a separate draft model. In this paper, we present Medusa, an\nefficient\
    \ method that augments LLM inference by adding extra decoding heads to\npredict\
    \ multiple subsequent tokens in parallel. Using a tree-based attention\nmechanism,\
    \ Medusa constructs multiple candidate continuations and verifies them\nsimultaneously\
    \ in each decoding step. By leveraging parallel processing, Medusa\nsubstantially\
    \ reduces the number of decoding steps required. We present two\nlevels of fine-tuning\
    \ procedures for Medusa to meet the needs of different use\ncases: Medusa-1: Medusa\
    \ is directly fine-tuned on top of a frozen backbone LLM,\nenabling lossless inference\
    \ acceleration. Medusa-2: Medusa is fine-tuned\ntogether with the backbone LLM,\
    \ enabling better prediction accuracy of Medusa\nheads and higher speedup but\
    \ needing a special training recipe that preserves\nthe backbone model's capabilities.\n\
    \  Moreover, we propose several extensions that improve or expand the utility\
    \ of\nMedusa, including a self-distillation to handle situations where no training\n\
    data is available and a typical acceptance scheme to boost the acceptance rate\n\
    while maintaining generation quality. We evaluate Medusa on models of various\n\
    sizes and training procedures. Our experiments demonstrate that Medusa-1 can\n\
    achieve over 2.2x speedup without compromising generation quality, while\nMedusa-2\
    \ further improves the speedup to 2.3-3.6x."
  arxivId: '2401.10774'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming
    Chen, Tri Dao
  created_at: '2024-12-30T08:28:31.552216'
  issue_number: 395
  issue_url: https://github.com/dmarx/papers-feed/issues/395
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:31.553017'
  last_visited: '2024-12-28T07:11:26.727Z'
  main_tex_file: null
  published_date: '2024-01-19T15:48:40Z'
  state: open
  title: "Medusa: Simple LLM Inference Acceleration Framework with Multiple\n  Decoding\
    \ Heads"
  total_reading_time_seconds: 20
  url: https://arxiv.org/abs/2401.10774
'2403.03353':
  abstract: 'This paper introduces a hypothesis space for deep learning that employs
    deep

    neural networks (DNNs). By treating a DNN as a function of two variables, the

    physical variable and parameter variable, we consider the primitive set of the

    DNNs for the parameter variable located in a set of the weight matrices and

    biases determined by a prescribed depth and widths of the DNNs. We then

    complete the linear span of the primitive DNN set in a weak* topology to

    construct a Banach space of functions of the physical variable. We prove that

    the Banach space so constructed is a reproducing kernel Banach space (RKBS) and

    construct its reproducing kernel. We investigate two learning models,

    regularized learning and minimum interpolation problem in the resulting RKBS,

    by establishing representer theorems for solutions of the learning models. The

    representer theorems unfold that solutions of these learning models can be

    expressed as linear combination of a finite number of kernel sessions

    determined by given data and the reproducing kernel.'
  arxivId: '2403.03353'
  arxiv_tags:
  - stat.ML
  - cs.LG
  - math.FA
  authors: Rui Wang, Yuesheng Xu, Mingsong Yan
  created_at: '2024-12-30T08:27:46.542218'
  issue_number: 491
  issue_url: https://github.com/dmarx/papers-feed/issues/491
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:46.543045'
  last_visited: '2024-12-29T10:46:53.922Z'
  main_tex_file: null
  published_date: '2024-03-05T22:42:29Z'
  state: open
  title: Hypothesis Spaces for Deep Learning
  total_reading_time_seconds: 13
  url: https://arxiv.org/abs/2403.03353
'2403.12187':
  abstract: 'Motivated by the abundance of functional data such as time series and
    images,

    there has been a growing interest in integrating such data into neural networks

    and learning maps from function spaces to R (i.e., functionals). In this paper,

    we study the approximation of functionals on reproducing kernel Hilbert spaces

    (RKHS''s) using neural networks. We establish the universality of the

    approximation of functionals on the RKHS''s. Specifically, we derive explicit

    error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev

    kernels. Moreover, we apply our findings to functional regression, proving that

    neural networks can accurately approximate the regression maps in generalized

    functional linear models. Existing works on functional learning require

    integration-type basis function expansions with a set of pre-specified basis

    functions. By leveraging the interpolating orthogonal projections in RKHS''s,

    our proposed network is much simpler in that we use point evaluations to

    replace basis function expansions.'
  arxivId: '2403.12187'
  arxiv_tags:
  - stat.ML
  - cs.LG
  - math.ST
  - stat.TH
  authors: Tian-Yi Zhou, Namjoon Suh, Guang Cheng, Xiaoming Huo
  created_at: '2024-12-30T08:27:40.564686'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2024-12-30T08:27:40.565450'
  last_visited: '2024-12-29T11:02:27.587000+00:00'
  main_tex_file: null
  published_date: '2024-03-18T18:58:23Z'
  state: open
  title: Approximation of RKHS Functionals by Neural Networks
  total_reading_time_seconds: 10
  url: https://arxiv.org/abs/2403.12187
'2405.07987':
  abstract: 'We argue that representations in AI models, particularly deep networks,
    are

    converging. First, we survey many examples of convergence in the literature:

    over time and across multiple domains, the ways by which different neural

    networks represent data are becoming more aligned. Next, we demonstrate

    convergence across data modalities: as vision models and language models get

    larger, they measure distance between datapoints in a more and more alike way.

    We hypothesize that this convergence is driving toward a shared statistical

    model of reality, akin to Plato''s concept of an ideal reality. We term such a

    representation the platonic representation and discuss several possible

    selective pressures toward it. Finally, we discuss the implications of these

    trends, their limitations, and counterexamples to our analysis.'
  arxivId: '2405.07987'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  - cs.NE
  authors: Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola
  created_at: '2024-12-30T08:28:13.570462'
  issue_number: 439
  issue_url: https://github.com/dmarx/papers-feed/issues/439
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:39:54.598Z'
  main_tex_file: null
  published_date: '2024-05-13T17:58:30Z'
  state: open
  title: The Platonic Representation Hypothesis
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2405.07987
'2406.01506':
  abstract: 'The linear representation hypothesis is the informal idea that semantic

    concepts are encoded as linear directions in the representation spaces of large

    language models (LLMs). Previous work has shown how to make this notion precise

    for representing binary concepts that have natural contrasts (e.g., {male,

    female}) as directions in representation space. However, many natural concepts

    do not have natural contrasts (e.g., whether the output is about an animal). In

    this work, we show how to extend the formalization of the linear representation

    hypothesis to represent features (e.g., is_animal) as vectors. This allows us

    to immediately formalize the representation of categorical concepts as

    polytopes in the representation space. Further, we use the formalization to

    prove a relationship between the hierarchical structure of concepts and the

    geometry of their representations. We validate these theoretical results on the

    Gemma and LLaMA-3 large language models, estimating representations for 900+

    hierarchically related concepts using data from WordNet.'
  arxivId: '2406.01506'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.LG
  - stat.ML
  authors: Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch
  created_at: '2024-12-30T08:28:16.542738'
  issue_number: 438
  issue_url: https://github.com/dmarx/papers-feed/issues/438
  labels:
  - paper
  - rating:novote
  last_read: null
  last_visited: '2024-12-29T01:39:53.364Z'
  main_tex_file: null
  published_date: '2024-06-03T16:34:01Z'
  state: open
  title: "The Geometry of Categorical and Hierarchical Concepts in Large Language\n\
    \  Models"
  total_reading_time_seconds: 0
  url: https://arxiv.org/abs/2406.01506
'2407.21787':
  abstract: 'Scaling the amount of compute used to train language models has dramatically

    improved their capabilities. However, when it comes to inference, we often

    limit the amount of compute to only one attempt per problem. Here, we explore

    inference compute as another axis for scaling by increasing the number of

    generated samples. Across multiple tasks and models, we observe that coverage
    -

    the fraction of problems solved by any attempt - scales with the number of

    samples over four orders of magnitude. In domains like coding and formal

    proofs, where all answers can be automatically verified, these increases in

    coverage directly translate into improved performance. When we apply repeated

    sampling to SWE-bench Lite, the fraction of issues solved with

    DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250

    samples, outperforming the single-attempt state-of-the-art of 43% which uses

    more capable frontier models. Moreover, using current API pricing, amplifying

    the cheaper DeepSeek model with five samples is more cost-effective and solves

    more issues than paying a premium for one sample from GPT-4o or Claude 3.5

    Sonnet. Interestingly, the relationship between coverage and the number of

    samples is often log-linear and can be modelled with an exponentiated power

    law, suggesting the existence of inference-time scaling laws. Finally, we find

    that identifying correct samples out of many generations remains an important

    direction for future research in domains without automatic verifiers. When

    solving math word problems from GSM8K and MATH, coverage with Llama-3 models

    grows to over 95% with 10,000 samples. However, common methods to pick correct

    solutions from a sample collection, such as majority voting or reward models,

    plateau beyond several hundred samples and fail to fully scale with the sample

    budget.'
  arxivId: '2407.21787'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le,
    Christopher Ré, Azalia Mirhoseini
  created_at: '2024-12-30T08:27:25.557036'
  issue_number: 513
  issue_url: https://github.com/dmarx/papers-feed/issues/513
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:25.560109'
  last_visited: '2024-12-29T20:31:06.783Z'
  main_tex_file: null
  published_date: '2024-07-31T17:57:25Z'
  state: open
  title: 'Large Language Monkeys: Scaling Inference Compute with Repeated Sampling'
  total_reading_time_seconds: 17
  url: https://arxiv.org/abs/2407.21787
'2408.03314':
  abstract: 'Enabling LLMs to improve their outputs by using more test-time computation
    is

    a critical step towards building generally self-improving agents that can

    operate on open-ended natural language. In this paper, we study the scaling of

    inference-time computation in LLMs, with a focus on answering the question: if

    an LLM is allowed to use a fixed but non-trivial amount of inference-time

    compute, how much can it improve its performance on a challenging prompt?

    Answering this question has implications not only on the achievable performance

    of LLMs, but also on the future of LLM pretraining and how one should tradeoff

    inference-time and pre-training compute. Despite its importance, little

    research attempted to understand the scaling behaviors of various test-time

    inference methods. Moreover, current work largely provides negative results for

    a number of these strategies. In this work, we analyze two primary mechanisms

    to scale test-time computation: (1) searching against dense, process-based

    verifier reward models; and (2) updating the model''s distribution over a

    response adaptively, given the prompt at test time. We find that in both cases,

    the effectiveness of different approaches to scaling test-time compute

    critically varies depending on the difficulty of the prompt. This observation

    motivates applying a "compute-optimal" scaling strategy, which acts to most

    effectively allocate test-time compute adaptively per prompt. Using this

    compute-optimal strategy, we can improve the efficiency of test-time compute

    scaling by more than 4x compared to a best-of-N baseline. Additionally, in a

    FLOPs-matched evaluation, we find that on problems where a smaller base model

    attains somewhat non-trivial success rates, test-time compute can be used to

    outperform a 14x larger model.'
  arxivId: '2408.03314'
  arxiv_tags:
  - cs.LG
  - cs.CL
  authors: Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
  created_at: '2024-12-30T08:27:22.552255'
  issue_number: 515
  issue_url: https://github.com/dmarx/papers-feed/issues/515
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:25.558565'
  last_visited: '2024-12-29T20:31:12.333Z'
  main_tex_file: null
  published_date: '2024-08-06T17:35:05Z'
  state: open
  title: "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling\
    \ Model Parameters"
  total_reading_time_seconds: 39
  url: https://arxiv.org/abs/2408.03314
'2408.04809':
  abstract: 'In this paper, we overview one promising avenue of progress at the

    mathematical foundation of deep learning: the connection between deep networks

    and function approximation by affine splines (continuous piecewise linear

    functions in multiple dimensions). In particular, we will overview work over

    the past decade on understanding certain geometrical properties of a deep

    network''s affine spline mapping, in particular how it tessellates its input

    space. As we will see, the affine spline connection and geometrical viewpoint

    provide a powerful portal through which to view, analyze, and improve the inner

    workings of a deep network.'
  arxivId: '2408.04809'
  arxiv_tags:
  - cs.LG
  - cs.AI
  - cs.CV
  authors: Randall Balestriero, Ahmed Imtiaz Humayun, Richard Baraniuk
  created_at: '2024-12-30T08:27:31.802050'
  issue_number: 500
  issue_url: https://github.com/dmarx/papers-feed/issues/500
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:31.804366'
  last_visited: '2024-12-29T11:11:50.707Z'
  main_tex_file: null
  published_date: '2024-08-09T01:40:12Z'
  state: open
  title: On the Geometry of Deep Learning
  total_reading_time_seconds: 29
  url: https://arxiv.org/abs/2408.04809
'2409.08514':
  abstract: 'Audio restoration has become increasingly significant in modern society,
    not

    only due to the demand for high-quality auditory experiences enabled by

    advanced playback devices, but also because the growing capabilities of

    generative audio models necessitate high-fidelity audio. Typically, audio

    restoration is defined as a task of predicting undistorted audio from damaged

    input, often trained using a GAN framework to balance perception and

    distortion. Since audio degradation is primarily concentrated in mid- and

    high-frequency ranges, especially due to codecs, a key challenge lies in

    designing a generator capable of preserving low-frequency information while

    accurately reconstructing high-quality mid- and high-frequency content.

    Inspired by recent advancements in high-sample-rate music separation, speech

    enhancement, and audio codec models, we propose Apollo, a generative model

    designed for high-sample-rate audio restoration. Apollo employs an explicit

    frequency band split module to model the relationships between different

    frequency bands, allowing for more coherent and higher-quality restored audio.

    Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently

    outperforms existing SR-GAN models across various bit rates and music genres,

    particularly excelling in complex scenarios involving mixtures of multiple

    instruments and vocals. Apollo significantly improves music restoration quality

    while maintaining computational efficiency. The source code for Apollo is

    publicly available at https://github.com/JusperLee/Apollo.'
  arxivId: '2409.08514'
  arxiv_tags:
  - cs.SD
  - cs.AI
  - eess.AS
  authors: Kai Li, Yi Luo
  created_at: '2024-12-30T08:26:43.562979'
  issue_number: 576
  issue_url: https://github.com/dmarx/papers-feed/issues/576
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:26:43.563812'
  last_visited: '2024-12-30T05:02:38.295Z'
  main_tex_file: null
  published_date: '2024-09-13T03:25:34Z'
  state: open
  title: 'Apollo: Band-sequence Modeling for High-Quality Audio Restoration'
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2409.08514
'2412.01023':
  abstract: 'Most real-world datasets consist of a natural hierarchy between classes
    or an

    inherent label structure that is either already available or can be constructed

    cheaply. However, most existing representation learning methods ignore this

    hierarchy, treating labels as permutation invariant. Recent work [Zeng et al.,

    2022] proposes using this structured information explicitly, but the use of

    Euclidean distance may distort the underlying semantic context [Chen et al.,

    2013]. In this work, motivated by the advantage of hyperbolic spaces in

    modeling hierarchical relationships, we propose a novel approach HypStructure:

    a Hyperbolic Structured regularization approach to accurately embed the label

    hierarchy into the learned representations. HypStructure is a

    simple-yet-effective regularizer that consists of a hyperbolic tree-based

    representation loss along with a centering loss, and can be combined with any

    standard task loss to learn hierarchy-informed features. Extensive experiments

    on several large-scale vision benchmarks demonstrate the efficacy of

    HypStructure in reducing distortion and boosting generalization performance

    especially under low dimensional scenarios. For a better understanding of

    structured representation, we perform eigenvalue analysis that links the

    representation geometry to improved Out-of-Distribution (OOD) detection

    performance seen empirically. The code is available at

    \url{https://github.com/uiuctml/HypStructure}.'
  arxivId: '2412.01023'
  arxiv_tags:
  - cs.LG
  - cs.CV
  authors: Aditya Sinha, Siqi Zeng, Makoto Yamada, Han Zhao
  created_at: '2024-12-30T08:28:04.539189'
  issue_number: 440
  issue_url: https://github.com/dmarx/papers-feed/issues/440
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:10.548268'
  last_visited: '2024-12-29T01:41:03.660Z'
  main_tex_file: null
  published_date: '2024-12-02T00:56:44Z'
  state: open
  title: Learning Structured Representations with Hyperbolic Embeddings
  total_reading_time_seconds: 81
  url: https://arxiv.org/abs/2412.01023
'2412.11768':
  abstract: 'In this work, we question the necessity of adaptive gradient methods
    for

    training deep neural networks. SGD-SaI is a simple yet effective enhancement to

    stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning

    rate Scaling at Initialization (SaI) to distinct parameter groups, guided by

    their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning

    rates without relying on adaptive second-order momentum, SGD-SaI helps prevent

    training imbalances from the very first iteration and cuts the optimizer''s

    memory usage by half compared to AdamW. Despite its simplicity and efficiency,

    SGD-SaI consistently matches or outperforms AdamW in training a variety of

    Transformer-based tasks, effectively overcoming a long-standing challenge of

    using SGD for training Transformers. SGD-SaI excels in ImageNet-1K

    classification with Vision Transformers(ViT) and GPT-2 pretraining for large

    language models (LLMs, transformer decoder-only), demonstrating robustness to

    hyperparameter variations and practicality for diverse applications. We further

    tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion

    models, where it consistently outperforms state-of-the-art optimizers. From a

    memory efficiency perspective, SGD-SaI achieves substantial memory savings for

    optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)

    and 25.15 GB for Llama2-7B compared to AdamW in full-precision training

    settings.'
  arxivId: '2412.11768'
  arxiv_tags:
  - cs.LG
  - cs.AI
  authors: Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen
  created_at: '2024-12-30T08:27:16.564411'
  issue_number: 433
  issue_url: https://github.com/dmarx/papers-feed/issues/433
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:28:19.539482'
  last_visited: '2024-12-28T18:49:01.011000+00:00'
  main_tex_file: null
  published_date: '2024-12-16T13:41:37Z'
  state: open
  title: 'No More Adam: Learning Rate Scaling at Initialization is All You Need'
  total_reading_time_seconds: 11
  url: https://arxiv.org/abs/2412.11768
'2412.17558':
  abstract: '\textit{Query Optimization} (QO) refers to techniques aimed at enhancing
    the

    efficiency and quality of Large Language Models (LLMs) in understanding and

    answering queries, especially complex ones in scenarios like

    Retrieval-Augmented Generation (RAG). Specifically, RAG mitigates the

    limitations of LLMs by dynamically retrieving and leveraging up-to-date

    relevant information, which provides a cost-effective solution to the challenge

    of LLMs producing plausible but potentially inaccurate responses. Recently, as

    RAG evolves and incorporates multiple components that influence its

    performance, QO has emerged as a critical element, playing a pivotal role in

    determining the effectiveness of RAG''s retrieval stage in accurately sourcing

    the necessary multiple pieces of evidence to answer queries correctly. In this

    paper, we trace the evolution of QO techniques by summarizing and analyzing

    significant studies. Through an organized framework and categorization, we aim

    to consolidate existing QO techniques in RAG, elucidate their technological

    foundations, and highlight their potential to enhance the versatility and

    applications of LLMs.'
  arxivId: '2412.17558'
  arxiv_tags:
  - cs.CL
  authors: Mingyang Song, Mao Zheng
  created_at: '2024-12-30T08:27:10.552311'
  issue_number: 554
  issue_url: https://github.com/dmarx/papers-feed/issues/554
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:13.538442'
  last_visited: '2024-12-30T04:45:35.389Z'
  main_tex_file: null
  published_date: '2024-12-23T13:26:04Z'
  state: open
  title: A Survey of Query Optimization in Large Language Models
  total_reading_time_seconds: 46
  url: https://arxiv.org/abs/2412.17558
'2412.17758':
  abstract: 'ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily

    due to an evaluation setup that prevents direct comparison of answer choices

    rather than inherent complexity. Although some researchers have quietly shifted

    to a more appropriate scheme over the last year, the implications of this

    change have yet to be widely acknowledged. We highlight this overlooked shift,

    show how similar evaluation practices falsely imply reasoning deficits in other

    benchmarks, and demonstrate that fairer methods dramatically reduce performance

    gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing

    so, we reveal how evaluation shapes perceived difficulty and offer guidelines

    to ensure that multiple-choice evaluations accurately reflect actual model

    capabilities.'
  arxivId: '2412.17758'
  arxiv_tags:
  - cs.CL
  - cs.AI
  authors: Łukasz Borchmann
  created_at: '2024-12-30T08:27:52.551001'
  issue_number: 458
  issue_url: https://github.com/dmarx/papers-feed/issues/458
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:52.551795'
  last_visited: '2024-12-29T08:28:16.461Z'
  main_tex_file: null
  published_date: '2024-12-23T18:14:36Z'
  state: open
  title: 'In Case You Missed It: ARC ''Challenge'' Is Not That Challenging'
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.17758
'2412.17759':
  abstract: 'Multimodal learning, a rapidly evolving field in artificial intelligence,

    seeks to construct more versatile and robust systems by integrating and

    analyzing diverse types of data, including text, images, audio, and video.

    Inspired by the human ability to assimilate information through many senses,

    this method enables applications such as text-to-video conversion, visual

    question answering, and image captioning. Recent developments in datasets that

    support multimodal language models (MLLMs) are highlighted in this overview.

    Large-scale multimodal datasets are essential because they allow for thorough

    testing and training of these models. With an emphasis on their contributions

    to the discipline, the study examines a variety of datasets, including those

    for training, domain-specific tasks, and real-world applications. It also

    emphasizes how crucial benchmark datasets are for assessing models'' performance

    in a range of scenarios, scalability, and applicability. Since multimodal

    learning is always changing, overcoming these obstacles will help AI research

    and applications reach new heights.'
  arxivId: '2412.17759'
  arxiv_tags:
  - cs.AI
  - cs.CV
  - cs.LG
  authors: Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal,
    Ishan Banerjee, Srikant Panda, Tejaswini Kumar
  created_at: '2024-12-30T08:27:13.536066'
  issue_number: 556
  issue_url: https://github.com/dmarx/papers-feed/issues/556
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:13.536863'
  last_visited: '2024-12-30T04:45:50.115Z'
  main_tex_file: null
  published_date: '2024-12-23T18:15:19Z'
  state: open
  title: "Survey of Large Multimodal Model Datasets, Application Categories and\n\
    \  Taxonomy"
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2412.17759
'2412.18082':
  abstract: 'The item cold-start problem is crucial for online recommender systems,
    as the

    success of the cold-start phase determines whether items can transition into

    popular ones. Prompt learning, a powerful technique used in natural language

    processing (NLP) to address zero- or few-shot problems, has been adapted for

    recommender systems to tackle similar challenges. However, existing methods

    typically rely on content-based properties or text descriptions for prompting,

    which we argue may be suboptimal for cold-start recommendations due to 1)

    semantic gaps with recommender tasks, 2) model bias caused by warm-up items

    contribute most of the positive feedback to the model, which is the core of the

    cold-start problem that hinders the recommender quality on cold-start items. We

    propose to leverage high-value positive feedback, termed pinnacle feedback as

    prompt information, to simultaneously resolve the above two problems. We

    experimentally prove that compared to the content description proposed in

    existing works, the positive feedback is more suitable to serve as prompt

    information by bridging the semantic gaps. Besides, we propose item-wise

    personalized prompt networks to encode pinnaclce feedback to relieve the model

    bias by the positive feedback dominance problem. Extensive experiments on four

    real-world datasets demonstrate the superiority of our model over

    state-of-the-art methods. Moreover, PROMO has been successfully deployed on a

    popular short-video sharing platform, a billion-user scale commercial

    short-video application, achieving remarkable performance gains across various

    commercial metrics within cold-start scenarios'
  arxivId: '2412.18082'
  arxiv_tags:
  - cs.IR
  - cs.AI
  authors: Yuezihan Jiang, Gaode Chen, Wenhan Zhang, Jingchi Wang, Yinjie Jiang, Qi
    Zhang, Jingjian Lin, Peng Jiang, Kaigui Bian
  created_at: '2024-12-30T08:26:55.551355'
  issue_number: 570
  issue_url: https://github.com/dmarx/papers-feed/issues/570
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:26:55.552642'
  last_visited: '2024-12-30T04:50:12.821000+00:00'
  main_tex_file: null
  published_date: '2024-12-24T01:38:19Z'
  state: open
  title: Prompt Tuning for Item Cold-start Recommendation
  total_reading_time_seconds: 19
  url: https://arxiv.org/abs/2412.18082
'2412.18168':
  abstract: 'Intuitively, an ideal collaborative filtering (CF) model should learn
    from

    users'' full rankings over all items to make optimal top-K recommendations. Due

    to the absence of such full rankings in practice, most CF models rely on

    pairwise loss functions to approximate full rankings, resulting in an immense

    performance gap. In this paper, we provide a novel analysis using the multiple

    ordinal classification concept to reveal the inevitable gap between a pairwise

    approximation and the ideal case. However, bridging the gap in practice

    encounters two formidable challenges: (1) none of the real-world datasets

    contains full ranking information; (2) there does not exist a loss function

    that is capable of consuming ranking information. To overcome these challenges,

    we propose a pseudo-ranking paradigm (PRP) that addresses the lack of ranking

    information by introducing pseudo-rankings supervised by an original noise

    injection mechanism. Additionally, we put forward a new ranking loss function

    designed to handle ranking information effectively. To ensure our method''s

    robustness against potential inaccuracies in pseudo-rankings, we equip the

    ranking loss function with a gradient-based confidence mechanism to detect and

    mitigate abnormal gradients. Extensive experiments on four real-world datasets

    demonstrate that PRP significantly outperforms state-of-the-art methods.'
  arxivId: '2412.18168'
  arxiv_tags:
  - cs.IR
  authors: Yuhan Zhao, Rui Chen, Li Chen, Shuang Zhang, Qilong Han, Hongtao Song
  created_at: '2024-12-30T08:26:58.544281'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2024-12-30T08:27:02.068052'
  last_visited: '2024-12-30T04:49:25.255000+00:00'
  main_tex_file: null
  published_date: '2024-12-24T05:01:16Z'
  state: open
  title: "From Pairwise to Ranking: Climbing the Ladder to Ideal Collaborative\n \
    \ Filtering with Pseudo-Ranking"
  total_reading_time_seconds: 6
  url: https://arxiv.org/abs/2412.18168
'2412.18431':
  abstract: 'Retrieval-augmented generation systems rely on effective document retrieval

    capabilities. By design, conventional sparse or dense retrievers face

    challenges in multi-hop retrieval scenarios. In this paper, we present GeAR,

    which advances RAG performance through two key innovations: (i) graph

    expansion, which enhances any conventional base retriever, such as BM25, and

    (ii) an agent framework that incorporates graph expansion. Our evaluation

    demonstrates GeAR''s superior retrieval performance on three multi-hop question

    answering datasets. Additionally, our system achieves state-of-the-art results

    with improvements exceeding 10% on the challenging MuSiQue dataset, while

    requiring fewer tokens and iterations compared to other multi-step retrieval

    systems.'
  arxivId: '2412.18431'
  arxiv_tags:
  - cs.CL
  - cs.AI
  - cs.IR
  authors: Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam,
    Damien Graux, Dandan Tu, Zeren Jiang, Ruofei Lai, Yang Ren, Jeff Z. Pan
  created_at: '2024-12-30T08:27:04.570417'
  issue_number: 564
  issue_url: https://github.com/dmarx/papers-feed/issues/564
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:04.571191'
  last_visited: '2024-12-30T04:48:52.907Z'
  main_tex_file: null
  published_date: '2024-12-24T13:45:22Z'
  state: open
  title: 'GeAR: Graph-enhanced Agent for Retrieval-augmented Generation'
  total_reading_time_seconds: 24
  url: https://arxiv.org/abs/2412.18431
'2412.18537':
  abstract: 'Large Language Models (LLMs) demonstrate remarkable capabilities, yet

    struggle with hallucination and outdated knowledge when tasked with complex

    knowledge reasoning, resulting in factually incorrect outputs. Previous studies

    have attempted to mitigate it by retrieving factual knowledge from large-scale

    knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of

    answers. However, this kind of approach often introduces noise and irrelevant

    data, especially in situations with extensive context from multiple knowledge

    aspects. In this way, LLM attention can be potentially mislead from question

    and relevant information. In our study, we introduce an Adaptive Multi-Aspect

    Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge

    including entities, relations, and subgraphs, and converts each piece of

    retrieved text into prompt embeddings. The Amar framework comprises two key

    sub-components: 1) a self-alignment module that aligns commonalities among

    entities, relations, and subgraphs to enhance retrieved text, thereby reducing

    noise interference; 2) a relevance gating module that employs a soft gate to

    learn the relevance score between question and multi-aspect retrieved data, to

    determine which information should be used to enhance LLMs'' output, or even

    filtered altogether. Our method has achieved state-of-the-art performance on

    two common datasets, WebQSP and CWQ, showing a 1.9\% improvement in accuracy

    over its best competitor and a 6.6\% improvement in logical form generation

    over a method that directly uses retrieved text as context prompts. These

    results demonstrate the effectiveness of Amar in improving the reasoning of

    LLMs.'
  arxivId: '2412.18537'
  arxiv_tags:
  - cs.CL
  authors: Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, Zhihong Zhu, Zhi Zheng,
    Xian Wu, Xiangyu Zhao, Tong Xu, Enhong Chen
  created_at: '2024-12-30T08:27:07.547992'
  issue_number: 563
  issue_url: https://github.com/dmarx/papers-feed/issues/563
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:27:07.549197'
  last_visited: '2024-12-30T04:47:23.772000+00:00'
  main_tex_file: null
  published_date: '2024-12-24T16:38:04Z'
  state: open
  title: "Harnessing Large Language Models for Knowledge Graph Question Answering\n\
    \  via Adaptive Multi-Aspect Retrieval-Augmentation"
  total_reading_time_seconds: 4
  url: https://arxiv.org/abs/2412.18537
'2412.18860':
  abstract: 'We introduce a bootstrapping approach to train long-context language
    models

    by exploiting their short-context capabilities only. Our method utilizes a

    simple agent workflow to synthesize diverse long-context instruction tuning

    data, thereby eliminating the necessity for manual data collection and

    annotation. The proposed data synthesis workflow requires only a short-context

    language model, a text retriever, and a document collection, all of which are

    readily accessible within the open-source ecosystem. Subsequently, language

    models are fine-tuned using the synthesized data to extend their context

    lengths. In this manner, we effectively transfer the short-context capabilities

    of language models to long-context scenarios through a bootstrapping process.

    We conduct experiments with the open-source Llama-3 family of models and

    demonstrate that our method can successfully extend the context length to up to

    1M tokens, achieving superior performance across various benchmarks.'
  arxivId: '2412.18860'
  arxiv_tags:
  - cs.CL
  - cs.IR
  authors: Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei
  created_at: '2024-12-30T08:26:49.555274'
  issue_number: 574
  issue_url: https://github.com/dmarx/papers-feed/issues/574
  labels:
  - paper
  - rating:novote
  last_read: '2024-12-30T08:26:49.556588'
  last_visited: '2024-12-30T04:54:02.225000+00:00'
  main_tex_file: null
  published_date: '2024-12-25T10:08:54Z'
  state: open
  title: Bootstrap Your Own Context Length
  total_reading_time_seconds: 29
  url: https://arxiv.org/abs/2412.18860
'2412.18956':
  abstract: 'When you have a question, the most effective way to have the question

    answered is to directly connect with experts on the topic and have a

    conversation with them. Prior to the invention of writing, this was the only

    way. Although effective, this solution exhibits scalability challenges. Writing

    allowed knowledge to be materialized, preserved, and replicated, enabling the

    development of different technologies over the centuries to connect information

    seekers with relevant information. This progression ultimately culminated in

    the ten-blue-links web search paradigm we''re familiar with, just before the

    recent emergence of generative AI. However, we often forget that consuming

    static content is an imperfect solution. With the advent of large language

    models, it has become possible to develop a superior experience by allowing

    users to directly engage with experts. These interactions can of course satisfy

    information needs, but expert models can do so much more. This coming future

    requires reimagining search.'
  arxivId: '2412.18956'
  arxiv_tags:
  - cs.IR
  authors: Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne
  created_at: '2024-12-30T08:26:52.831644'
  issue_number: 572
  issue_url: https://github.com/dmarx/papers-feed/issues/572
  labels:
  - paper
  - rating:downvote
  last_read: '2024-12-30T08:26:52.832952'
  last_visited: '2024-12-30T04:52:50.157000+00:00'
  main_tex_file: null
  published_date: '2024-12-25T18:09:34Z'
  state: open
  title: 'Musings About the Future of Search: A Return to the Past?'
  total_reading_time_seconds: 25
  url: https://arxiv.org/abs/2412.18956
'2412.19442':
  abstract: 'Large Language Models (LLMs) have revolutionized a wide range of domains
    such

    as natural language processing, computer vision, and multi-modal tasks due to

    their ability to comprehend context and perform logical reasoning. However, the

    computational and memory demands of LLMs, particularly during inference, pose

    significant challenges when scaling them to real-world, long-context, and

    real-time applications. Key-Value (KV) cache management has emerged as a

    critical optimization technique for accelerating LLM inference by reducing

    redundant computations and improving memory utilization. This survey provides
    a

    comprehensive overview of KV cache management strategies for LLM acceleration,

    categorizing them into token-level, model-level, and system-level

    optimizations. Token-level strategies include KV cache selection, budget

    allocation, merging, quantization, and low-rank decomposition, while

    model-level optimizations focus on architectural innovations and attention

    mechanisms to enhance KV reuse. System-level approaches address memory

    management, scheduling, and hardware-aware designs to improve efficiency across

    diverse computing environments. Additionally, the survey provides an overview

    of both text and multimodal datasets and benchmarks used to evaluate these

    strategies. By presenting detailed taxonomies and comparative analyses, this

    work aims to offer useful insights for researchers and practitioners to support

    the development of efficient and scalable KV cache management techniques,

    contributing to the practical deployment of LLMs in real-world applications.

    The curated paper list for KV cache management is in:

    \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.'
  arxivId: '2412.19442'
  arxiv_tags:
  - cs.AI
  - cs.DC
  authors: Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen,
    Nicole Hu, Wei Dong, Qing Li, Lei Chen
  created_at: '2024-12-30T08:27:02.066552'
  issue_number: 0
  issue_url: ''
  labels:
  - paper
  last_read: '2024-12-30T08:27:02.067361'
  last_visited: '2024-12-30T04:49:33.842000+00:00'
  main_tex_file: null
  published_date: '2024-12-27T04:17:57Z'
  state: open
  title: "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
  total_reading_time_seconds: 5
  url: https://arxiv.org/abs/2412.19442
