- **Extreme-Token Phenomena**: Refers to three observed phenomena in LLMs: attention sinks, value-state drains, and residual-state peaks.
  
- **Attention Sinks**: Specific tokens (sink tokens) receive disproportionately high attention weights, often the first token in a sequence.

- **Value-State Drains**: Sink tokens exhibit significantly smaller value states compared to other tokens.

- **Residual-State Peaks**: Sink tokens have larger residual state norms than other tokens, particularly in layers excluding the first and last.

- **Active-Dormant Mechanism**: Attention heads can switch between active (sinks) and dormant states based on input domains, influencing attention dynamics.

- **Mutual Reinforcement Mechanism**: Attention sinks and value-state drains reinforce each other, leading to stable phases where query tokens generate similar attention logits for extreme tokens.

- **Bigram-Backcopy (BB) Task**: A simplified task used to study extreme-token phenomena, demonstrating similar behaviors to those in LLMs.

- **Quantitative Properties**: Consistent observations include:
  - Sink-logits concentration: Logits for extreme tokens and non-extreme tokens are nearly identical.
  - Value state norms and residual state norms show predictable patterns across tasks and models.

- **Mitigation Strategies**:
  - Replace SoftMax with ReLU in attention heads to eliminate extreme-token phenomena.
  - Switch from Adam to SGD to remove residual-state peak phenomena.

- **Empirical Validation**: Observations from pretrained models like Llama and OLMo align with theoretical predictions from the BB task, confirming the active-dormant and mutual reinforcement mechanisms.

- **Architectural Modifications**: Suggests that similar strategies used in the BB task could be applied to mitigate extreme-token phenomena in larger LLMs.

- **Notation Summary**:
  - Input sequence: \( H = [h_1, h_2, \ldots, h_n] \in \mathbb{R}^{d \times n} \)
  - Layer normalization: \( \text{LN} \)
  - SoftMax operation: \( \text{SoftMax} \)
  - Pointwise ReLU: \( \text{ReLU} \)

- **Key Figures**: Reference to Figure 1 illustrating the empirical distribution of value states and residual states across layers and heads in Llama-3.

- **Challenges in LLMs**: Extreme-token phenomena complicate inference, quantization, and interpretability, necessitating special treatment for sink tokens during long-context inference.

- **Related Work**: Acknowledge previous studies on attention sinks and their implications for model performance and quantization strategies.