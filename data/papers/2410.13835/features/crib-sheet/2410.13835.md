- **Extreme-Token Phenomena**: Refers to three observed phenomena in LLMs: attention sinks, value-state drains, and residual-state peaks.
  
- **Attention Sinks**: Specific tokens (sink tokens) receive disproportionately high attention weights, often the first token in a sequence.

- **Value-State Drains**: Sink tokens exhibit significantly smaller value states compared to other tokens.

- **Residual-State Peaks**: Sink tokens have larger residual-state norms than other tokens, particularly in layers excluding the first and last.

- **Active-Dormant Mechanism**: Attention heads can switch between active (attracting attention) and dormant (not attracting attention) states based on input domains.

- **Mutual Reinforcement Mechanism**: Attention sinks and value-state drains reinforce each other, leading to stable phases where query tokens generate similar attention logits for extreme tokens.

- **Bigram-Backcopy (BB) Task**: A simplified task used to study extreme-token phenomena, demonstrating similar behaviors to those in larger LLMs.

- **Quantitative Properties**: Consistent observations include:
  - Logit concentration for extreme tokens: \( \Delta \text{logit}_{•,\langle s \rangle} = \text{logit}_{•,\langle s \rangle} - \text{Mean}[\text{logit}_{•,\text{others}}] \)
  - Value state norm: \( \| \text{Val}_{\langle s \rangle} \| \) shows a monotonic decrease.
  - Residual state norm: \( \| \text{Res}_{\langle s \rangle} \| \) exhibits linear growth.

- **Architectural Modifications**: 
  - Replacing SoftMax with ReLU in attention heads mitigates extreme-token phenomena.
  - Switching from Adam to SGD helps eliminate residual-state peaks.

- **Empirical Evidence**: Observations from pretrained LLMs (e.g., Llama, OLMo) support the active-dormant and mutual reinforcement mechanisms identified in the BB task.

- **Challenges in LLMs**: Extreme-token phenomena complicate inference, quantization, and interpretability, necessitating special treatment for sink tokens.

- **Diagrammatic Representation** (if needed):
```mermaid
flowchart TD
    A[Input Sequence H] --> B[Attention Heads]
    B --> C{Active or Dormant?}
    C -->|Active| D[High Attention Weights]
    C -->|Dormant| E[Low Attention Weights]
    D --> F[Attention Sinks]
    E --> G[Value-State Drains]
    F --> H[Residual-State Peaks]
    H --> I[Mutual Reinforcement]
```

- **Key Contributions**: 
  - Identification of mechanisms behind extreme-token phenomena.
  - Proposal of strategies to mitigate these phenomena during pretraining.
  - Empirical validation of theoretical claims through experiments on both simplified models and pretrained LLMs.