\section{Dicussion, Limitations, Conclusion}
\label{sec:discussion}

With \fat, we have demonstrated that new programming techniques and hardware
features such as asynchrony and low-precision can have a dramatic impact on the
efficiency and accuracy of attention.
We are able to speed up attention by 1.5-2.0$\times$ times compared to \faa, and
reduce FP8 numerical error by 2.6$\times$ compared to standard per-tensor quantization.
Some limitations of our work that we hope to address in the future include: optimizing for
LLM inference, integrating a persistent kernel design into the FP8 kernel,\footnote{For our benchmarks, FP16 \fat has a persistent kernel and load balancing strategy, while FP8 \fat does not. This partly explains why FP8 \fat does not perform as well for small sequence length and causal masking compared to the FP8 cuDNN kernels.} and understanding the effects of low-precision attention in
large-scale training.
Though we have focused on Hopper GPUs in this work, we expect that the
techniques developed here will apply to other hardware accelerators.
We hope that a faster and more accurate primitive such as attention will unlock
new applications in long-context tasks.
