\section{Background: Multi-Head Attention and GPU Characteristics}
\label{sec:background}

\subsection{Multi-Head Attention}
\label{subsec:multi_head_attn}

Let $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$ be the query, key and value input sequences associated to a single head, where $N$ is the sequence length and $d$ is the head dimension. Then the attention output $\vO$ is computed as:
\begin{equation*}
  \vS = \alpha \vQ \vK^\top \in \mathbb{R}^{N \times N}, \quad \vP = \softmax(\vS) \in \mathbb{R}^{N \times N}, \quad \vO = \vP\vV \in \mathbb{R}^{N \times d},
\end{equation*}
where $\softmax$ is applied row-wise and one typically sets $\alpha = 1/\sqrt{d}$ as the scaling factor.
In practice, we subtract $\rowmax(\vS)$ from $\vS$ to prevent numerical instability with the exponential function.
For multi-head attention (MHA), each head has its own set of query, key and value projections, and this computation parallelizes across multiple heads and batches to produce the full output tensor.

Now let $\phi$ be a scalar loss function and let $\mathbf{d}(-) = \partial \phi / \partial (-)$ be notation for the gradient.
Given the output gradient $\vdO \in \mathbb{R}^{N \times d}$, we compute $\vdQ$, $\vdK$, and $\vdV$ according to the chain rule as follows:
\iftoggle{arxiv}{
\begin{align*}
  \vdV &= \vP^\top \vdO \in \mathbb{R}^{N \times d} \\
  \vdP &= \vdO \vV^\top \in \mathbb{R}^{N \times N} \\
  \vdS &= \dsoftmax (\vdP) \in \mathbb{R}^{N \times N} \\
  \vdQ &= \alpha \vdS \vK \in \mathbb{R}^{N \times d} \\
  \vdK &= \alpha \vdS^\top \vQ \in \mathbb{R}^{N \times d},
\end{align*}
}{
\begin{align*}
  \vdV &= \vP^\top \vdO \in \mathbb{R}^{N \times d}, \: &\vdP &= \vdO \vV^\top \in \mathbb{R}^{N \times N}, \\
  \vdS &= \dsoftmax (\vdP) \in \mathbb{R}^{N \times N}, \: &\vdQ &= \alpha \vdS \vK \in \mathbb{R}^{N \times d}, \qquad \vdK = \alpha \vdS^\top \vQ \in \mathbb{R}^{N \times d}.
\end{align*}
}
Here, we have that $\mathbf{d}s = (\diag(p) - p p^\top)\mathbf{d}p$ for $p = \softmax(s)$ as a function of a vector $s$, and we write $\dsoftmax(\vdP)$ for this formula applied row-wise.
Finally, this computation again parallelizes across the number of heads and batches for the backward pass of MHA.

\subsection{GPU hardware characteristics and execution model}
\label{subsec:hardware}

We describe the aspects of the GPU's execution model relevant for \fat, with a focus on the NVIDIA Hopper architecture as a concrete instantiation of this model.

\paragraph{Memory hierarchy:} The GPU's memories are organized as a hierarchy of data locales, with capacity inversely related to bandwidth (\cref{tab:gpu-hierarchy})\footnote{\citet{luo2024benchmarking} reports shared memory bandwidth of 128 bytes per clock cycle per SM, and we multiply that by 132 SMs and the boost clock of 1830 MHz.}.
Global memory (GMEM), also known as HBM, is the off-chip DRAM accessible to all streaming multiprocessors (SMs).
Data from GMEM gets transparently cached into an on-chip L2 cache.
Next, each SM contains a small on-chip, programmer-managed highly banked cache called shared memory (SMEM). 
Lastly, there is the register file within each SM.

\paragraph{Thread hierarchy:} The GPU's programming model is organized around logical groupings of execution units called threads.
From the finest to coarsest level, the thread hierarchy is comprised of threads, warps (32 threads), warpgroups (4 contiguous warps), threadblocks (i.e., cooperative thread arrays or CTAs), threadblock clusters (in Hopper), and grids.

These two hierarchies are closely interlinked.
Threads in the same CTA are co-scheduled on the same SM, and CTAs in the same cluster are co-scheduled on the same GPC.
SMEM is directly addressable by all threads within a CTA, whereas each thread has at most 256 registers (RMEM) private to itself.




\begin{table}[h!]
  \small
  \centering
  \caption{Thread-Memory hierarchy for the NVIDIA Hopper H100 SXM5 GPU.}
  \label{tab:gpu-hierarchy}
  \begin{tabular}{|r|l|l|l|}
      \hline
      \textbf{Hardware Level} & \textbf{Parallel Agent} & \textbf{Data Locale} & \textbf{Capacity @ Bandwidth} \\
      \hline
      Chip   & Grid                 & GMEM & 80  GiB @ 3.35 TB/s \\
      GPC    & Threadblock Clusters & L2   & 50  MiB @ ~12 TB/s \\
      SM     & Threadblock (CTA)    & SMEM & 228 KiB per SM, 31TB/s per GPU \\
      Thread & Thread               & RMEM & 256 KiB per SM \\
      \hline
  \end{tabular}
\end{table}



\paragraph{Asynchrony and warp-specialization:}

GPUs are throughput processors that rely on concurrency and asynchrony to hide memory and execution latencies.
For async memory copy between GMEM and SMEM, Hopper has the Tensor Memory Accelerator (TMA) as a dedicated hardware unit \cite[\S7.29]{cuda}.
Furthermore, unlike prior architectures such as Ampere, the Tensor Core of Hopper, exposed via the warpgroup-wide WGMMA instruction \cite[\S9.7.14]{ptx}, is also asynchronous and can source its inputs directly from shared memory.

Hardware support for asynchrony allows for warp-specialized kernels, where the warps of a CTA are divided into producer or consumer roles that only ever issue either data movement or computation.
Generically, this improves the compiler's ability to generate optimal instruction schedules \citep{warp-specialization-2011}.
In addition, Hopper supports the dynamic reallocation of registers between warpgroups via \verb|setmaxnreg| \cite[\S9.7.17.1]{ptx}, so those warps doing MMAs can obtain a larger share of RMEM than those just issuing TMA (for which only a single thread is needed).




\paragraph{Low-precision number formats:}
\label{sec:low-precision-gpu}
Modern GPUs have specialized hardware units for accelerating low-precision computation.
For example, the WGMMA instruction can target the FP8 Tensor Cores on Hopper to deliver 2x the throughput per SM when compared to FP16 or BF16.

However, correctly invoking FP8 WGMMA entails understanding the layout constraints on its operands. 
Given a GEMM call to multiply $A \times B^{\top}$ for an $M\times K$-matrix $A$ and an $N\times K$-matrix $B$, we say that the $A$ or $B$ operand is \emph{mn-major} if it is contiguous in the outer $M$ or $N$ dimension, and \emph{k-major} if is instead contiguous in the inner $K$-dimension.
Then for FP16 WGMMA, both mn-major and k-major input operands are accepted for operands in SMEM, but for FP8 WGMMA, only the k-major format is supported.
Moreover, in situations such as attention where one wants to fuse back-to-back GEMMs in a single kernel, clashing FP32 accumulator and FP8 operand layouts pose an obstacle to invoking dependent FP8 WGMMAs.

In the context of attention, these layout restrictions entail certain modifications to the design of an FP8 algorithm, which we describe in \cref{sec:algofp8}.

\subsection{Standard Attention and Flash Attention}
Following \citet{dao2022flashattention}, we let \textbf{standard attention} denote an implementation of attention on the GPU that materializes the intermediate matrices $\vS$ and $\vP$ to HBM. The main idea of \fa was to leverage a local version of the softmax reduction to avoid these expensive intermediate reads/writes and fuse attention into a single kernel. Local softmax corresponds to lines \ref{code-ws:softmax_start}-\ref{code-ws:softmax_end} of the consumer mainloop in \cref{alg:flash3_wgmma_ws_only} together with the rescalings of blocks of $\vO$. The simple derivation that this procedure indeed computes $\vO$ can be found in \cite[\S 2.3.1]{dao2023flashattention2}.
