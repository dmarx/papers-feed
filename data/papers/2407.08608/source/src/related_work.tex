\section{Related Work}
\label{sec:related_work}

\paragraph{Attention variants and distributed attention}
Ever since attention became popular with the Transformer
architecture~\citep{vaswani2017attention}, there has been a large body of work
on approximating attention to scale it to longer sequences.
These approximation methods can generally be categorized into two classes:
sparse and low-rank.
Sparse attention only computes some entries of the attention matrix ($\mathrm{softmax}(\vQ
\vK^T)$) and assumes that other entries are zero.
Different methods have different ways of choosing which entries should be zero,
either with a fixed pattern~\citep{child2019generating}, with a sliding
window~\citep{beltagy2020longformer}, or with a dynamic pattern through
hashing~\citep{kitaev2020reformer} or routing~\citep{roy2020efficient}.
The low-rank approach instead assumes that the attention matrix has a low-rank
structure, and apply a pointwise nonlinearity to the query and
key~\citep{katharopoulos2020transformers} with random
projection~\citep{choromanski2021rethinking, peng2021random, xiong2021nystromformer}.
One can also combine the sparse and low-rank approximation for better
quality~\citep{zaheer2020bigbird,scatterbrain}.
However, these approximation methods typically do not offer the same model
quality as standard attention~\citep{tay2020efficient}, and so most large-scale
models do not employ these techniques.

There are other variants of attention aimed at reducing the size of the KV cache
to improve inference efficiency. Multi-query attention~\citep{shazeer2019fast} and grouped query
attention~\citep{ainslie2023gqa} tie different heads of $\vK$ and $\vV$, and
multiple query heads interact with the same key and value head.
Multi-head latent attention~\citep{deepseekv2} parameterizes the $\vK$ and $\vV$
as low-rank projections of a shared matrix to further reduce the KV cache size.
However, all of these approaches do not change the core computation
$\mathrm{softmax}(\vQ \vK^T) \vV$ during training and simply change how $\vQ, \vK, \vV$ are
obtained.
As a result, any efficiency or accuracy improvement to the standard attention
computation benefits these methods.

To extend to even longer context, attention computation can be distributed
across multiple GPUs.
Methods such as Ring attention~\citep{liu2023ring,liu2024world} and
variants~\citep{brandon2023striped} can reach a context length of up to 1
million.
They use \fa (or \faa) as a primitive, and so the improvement from \fat would
benefit these distributed attention methods as well.

\paragraph{Alternative architectures}
Motivated by the limitations of attention, a variety of alternative
architectures have been proposed.
They build on the connection between linear
attention~\citep{katharopoulos2020transformers} and recurrent neural networks
(RNNs).
RWKV~\citep{peng2023rwkv}, H3~\citep{dao2023hungry}, MEGA~\citep{ma2023mega},
Retnet~\citep{sun2023retentive}  enhance the expressivity of the simple
cumulative sum in linear attention with more sophisticated recurrences.
Mamba~\citep{gu2023mamba} and xLSTM~\citep{beck2024xlstm} use learnable
weighting for the recurrence and can match the quality of Transformers in
language modeling at small or medium scale.
These approaches can be connected to generalizations of linear attention through
the lens of the structure of the token-mixing matrix~\citep{dao2024transformers}.
These models have started to see some traction, seeing usage in some medium to
large-scale models such as Jamba~\citep{jamba}, Zamba~\citep{zamba},
Megalodon~\citep{ma2024megalodon}, and Mamba2-hybrid~\citep{waleffe2024empirical}.
For the highest quality, these SSM- and RNN-based models still employ
many layers of attention.
We expect that techniques to speed up attention presented in this work will be
useful to speedup these alternative architectures.

\paragraph{Low-precision attention}
Quantization is a promising approach to speed up attention, but they have mostly
focused on reducing the space for KV cache for inference efficiency.
QuIP~\citep{chee2024quip} and QuIP\#\citep{tseng2024quip} use incoherent processing to reduce the quantization,
and we adapted this technique for FP8 \fat.
Recent work suggests that for inference the KV cache is highly compressible down to 4-, 3-, or
even 2-bits~\citep{hooper2024kvquant, liu2024kivi}.
However, quantization during training is still challenging as higher precision
is typically required for stable training.

\paragraph{Hardware-aware Algorithms}
Our work presented in this paper focuses on the micro-architecture
specific tuning to leverage new instruction sets and adopt a natively
asynchronous programming model. There are other orthogonal axes for
hardware-aware algorithm co-design being explored.
A recent example of this is LeanAttention~\citep{sanovar2024-leanattention},
which recognizes the poor GPU occupancy and high memory bandwidth requirements
of the sequential token generation phase as primary bottlenecks for inference
and optimizes it via a smarter load balancing strategy similar to Stream-K
load balancing~\citep{streamk} to achieve nearly peak occupancy.
There is a large literature on optimizing GEMM for specific hardware that employs
many of the same techniques.
As an example, \citet{abdel2016batched} presents a high performance batched GEMM kernel on
K40c Graphics Processing Units (GPU) for both fixed and variable sizes,
proposing specialized GEMM designs
and a comprehensive autotuning process to deliver state-of-the-art 
performance.

