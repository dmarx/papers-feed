\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdelfattah et~al.(2016)Abdelfattah, Haidar, Tomov, and
  Dongarra]{abdel2016batched}
Ahmad Abdelfattah, Azzam Haidar, Stanimire Tomov, and Jack Dongarra.
\newblock Performance, design, and autotuning of batched gemm for gpus.
\newblock pages 21--38, 06 2016.
\newblock ISBN 978-3-319-41320-4.
\newblock \doi{10.1007/978-3-319-41321-1_2}.

\bibitem[AI21(2024)]{jamba}
AI21.
\newblock Introducing jamba: Ai21's groundbreaking ssm-transformer model.
\newblock \emph{AI21 blog}, 2024.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy,
  Lebr{\'o}n, and Sanghai]{ainslie2023gqa}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico
  Lebr{\'o}n, and Sumit Sanghai.
\newblock Gqa: Training generalized multi-query transformer models from
  multi-head checkpoints.
\newblock \emph{arXiv preprint arXiv:2305.13245}, 2023.

\bibitem[Bauer et~al.(2011)Bauer, Cook, and Khailany]{warp-specialization-2011}
Michael Bauer, Henry Cook, and Brucek Khailany.
\newblock Cuda{DMA}: {O}ptimizing {GPU} {M}emory {B}andwidth via {W}arp
  {S}pecialization.
\newblock In \emph{Proceedings of 2011 International Conference for High
  Performance Computing, Networking, Storage and Analysis}, SC '11, New York,
  NY, USA, 2011. Association for Computing Machinery.
\newblock ISBN 9781450307710.
\newblock \doi{10.1145/2063384.2063400}.
\newblock URL \url{https://doi.org/10.1145/2063384.2063400}.

\bibitem[Beck et~al.(2024)Beck, P{\"o}ppel, Spanring, Auer, Prudnikova, Kopp,
  Klambauer, Brandstetter, and Hochreiter]{beck2024xlstm}
Maximilian Beck, Korbinian P{\"o}ppel, Markus Spanring, Andreas Auer,
  Oleksandra Prudnikova, Michael Kopp, G{\"u}nter Klambauer, Johannes
  Brandstetter, and Sepp Hochreiter.
\newblock xlstm: Extended long short-term memory.
\newblock \emph{arXiv preprint arXiv:2405.04517}, 2024.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Bikshandi and Shah(2024)]{colfax_fp8_flashattention_2024}
Ganesh Bikshandi and Jay Shah.
\newblock {D}elivering 1 {PFLOP}/s of {P}erformance with {FP8}
  {F}lash{A}ttention-2, 2024.
\newblock URL
  \url{https://research.colfax-intl.com/adding-fp8-to-flashattention/}.

\bibitem[Brandon et~al.(2023)Brandon, Nrusimha, Qian, Ankner, Jin, Song, and
  Ragan-Kelley]{brandon2023striped}
William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin,
  Zhiye Song, and Jonathan Ragan-Kelley.
\newblock Striped attention: Faster ring attention for causal transformers.
\newblock \emph{arXiv preprint arXiv:2311.09431}, 2023.

\bibitem[Chee et~al.(2024)Chee, Cai, Kuleshov, and De~Sa]{chee2024quip}
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher~M De~Sa.
\newblock Quip: 2-bit quantization of large language models with guarantees.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Chen et~al.(2021)Chen, Dao, Winsor, Song, Rudra, and
  Ré]{scatterbrain}
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré.
\newblock Scatterbrain: Unifying sparse and low-rank attention.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Chen et~al.(2022)Chen, Chen, Li, Chen, Trister, Krishnan, and
  Mahmood]{chen2022scaling}
Richard~J Chen, Chengkuan Chen, Yicong Li, Tiffany~Y Chen, Andrew~D Trister,
  Rahul~G Krishnan, and Faisal Mahmood.
\newblock Scaling vision transformers to gigapixel images via hierarchical
  self-supervised learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16144--16155, 2022.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2021rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2021.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Dao(2023)]{dao2023flashattention2}
Tri Dao.
\newblock Flash{A}ttention-2: Faster {A}ttention with {B}etter {P}arallelism
  and {W}ork {P}artitioning, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.08691}.

\bibitem[Dao and Gu(2024)]{dao2024transformers}
Tri Dao and Albert Gu.
\newblock Transformers are {SSM}s: Generalized models and efficient algorithms
  with structured state space duality.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2024.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with
  {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Dao et~al.(2023)Dao, Fu, Saab, Thomas, Rudra, and
  R{\'e}]{dao2023hungry}
Tri Dao, Daniel~Y Fu, Khaled~K Saab, Armin~W Thomas, Atri Rudra, and
  Christopher R{\'e}.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2023.

\bibitem[DeepSeek-AI(2024)]{deepseekv2}
DeepSeek-AI.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts
  language model.
\newblock \emph{arXiv preprint arXiv:2405.04434}, 2024.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2208llm}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{CoRR abs/2208.07339}, 2022.

\bibitem[Golden et~al.(2024)Golden, Hsia, Sun, Acun, Hosmer, Lee, DeVito,
  Johnson, Wei, Brooks, et~al.]{golden2024flash}
Alicia Golden, Samuel Hsia, Fei Sun, Bilge Acun, Basil Hosmer, Yejin Lee,
  Zachary DeVito, Jeff Johnson, Gu-Yeon Wei, David Brooks, et~al.
\newblock Is flash attention stable?
\newblock \emph{arXiv preprint arXiv:2405.02803}, 2024.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock 2023.

\bibitem[Gulati et~al.(2020)Gulati, Qin, Chiu, Parmar, Zhang, Yu, Han, Wang,
  Zhang, Wu, et~al.]{gulati2020conformer}
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu~Zhang, Jiahui Yu,
  Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et~al.
\newblock Conformer: Convolution-augmented transformer for speech recognition.
\newblock \emph{arXiv preprint arXiv:2005.08100}, 2020.

\bibitem[Guo et~al.(2021)Guo, Ainslie, Uthus, Ontanon, Ni, Sung, and
  Yang]{guo2021longt5}
Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan
  Sung, and Yinfei Yang.
\newblock Longt5: Efficient text-to-text transformer for long sequences.
\newblock \emph{arXiv preprint arXiv:2112.07916}, 2021.

\bibitem[Ho et~al.(2022)Ho, Salimans, Gritsenko, Chan, Norouzi, and
  Fleet]{ho2022video}
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi,
  and David~J Fleet.
\newblock Video diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 8633--8646, 2022.

\bibitem[Hooper et~al.(2024)Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer,
  and Gholami]{hooper2024kvquant}
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael~W Mahoney, Yakun~Sophia
  Shao, Kurt Keutzer, and Amir Gholami.
\newblock Kvquant: Towards 10 million context length llm inference with kv
  cache quantization.
\newblock \emph{arXiv preprint arXiv:2401.18079}, 2024.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2020.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang,
  and Stoica]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu,
  Joseph Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with
  pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems
  Principles}, pages 611--626, 2023.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone,
  Akiki, Li, Chim, et~al.]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
  Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023.

\bibitem[Liu et~al.(2023)Liu, Zaharia, and Abbeel]{liu2023ring}
Hao Liu, Matei Zaharia, and Pieter Abbeel.
\newblock Ring attention with blockwise transformers for near-infinite context.
\newblock \emph{arXiv preprint arXiv:2310.01889}, 2023.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Yan, Zaharia, and
  Abbeel]{liu2024world}
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.
\newblock World model on million-length video and language with ringattention.
\newblock \emph{arXiv preprint arXiv:2402.08268}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Yuan, Jin, Zhong, Xu, Braverman,
  Chen, and Hu]{liu2024kivi}
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir
  Braverman, Beidi Chen, and Xia Hu.
\newblock Kivi: A tuning-free asymmetric 2bit quantization for kv cache.
\newblock \emph{arXiv preprint arXiv:2402.02750}, 2024{\natexlab{b}}.

\bibitem[Luo et~al.(2024)Luo, Fan, Li, Du, Wang, and Chu]{luo2024benchmarking}
Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Qiang Wang, and Xiaowen Chu.
\newblock {B}enchmarking and {D}issecting the {N}vidia {H}opper {GPU}
  {A}rchitecture, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.13499}.

\bibitem[Ma et~al.(2023)Ma, Zhou, Kong, He, Gui, Neubig, May, and
  Zettlemoyer]{ma2023mega}
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig,
  Jonathan May, and Luke Zettlemoyer.
\newblock Mega: Moving average equipped gated attention.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2023.

\bibitem[Ma et~al.(2024)Ma, Yang, Xiong, Chen, Yu, Zhang, May, Zettlemoyer,
  Levy, and Zhou]{ma2024megalodon}
Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang,
  Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou.
\newblock Megalodon: Efficient llm pretraining and inference with unlimited
  context length.
\newblock \emph{arXiv preprint arXiv:2404.08801}, 2024.

\bibitem[Micikevicius et~al.(2022)Micikevicius, Stosic, Burgess, Cornea, Dubey,
  Grisenthwaite, Ha, Heinecke, Judd, Kamalu, et~al.]{micikevicius2022fp8}
Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey,
  Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John
  Kamalu, et~al.
\newblock Fp8 formats for deep learning.
\newblock \emph{arXiv preprint arXiv:2209.05433}, 2022.

\bibitem[NVIDIA(2024)]{cuda}
NVIDIA.
\newblock {CUDA} {P}rogramming {G}uide {V}ersion 12.4, 2024.
\newblock URL
  \url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html}.

\bibitem[Nvidia(2024)]{cudnn9}
Nvidia.
\newblock Accelerating transformers with nvidia cudnn 9.
\newblock \emph{Nvidia blog}, 2024.
\newblock URL
  \url{https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/}.

\bibitem[NVIDIA(2024)]{ptx}
NVIDIA.
\newblock {P}arallel {T}hread {E}xecution {ISA} {V}ersion 8.4, 2024.
\newblock URL \url{https://docs.nvidia.com/cuda/pdf/ptx_isa_8.4.pdf}.

\bibitem[Osama et~al.(2023)Osama, Merrill, Cecka, Garland, and Owens]{streamk}
Muhammad Osama, Duane Merrill, Cris Cecka, Michael Garland, and John~D. Owens.
\newblock Stream-k: Work-centric parallel decomposition for dense matrix-matrix
  multiplication on the gpu.
\newblock In \emph{Proceedings of the 28th ACM SIGPLAN Annual Symposium on
  Principles and Practice of Parallel Programming}, PPoPP '23, pages 429--431,
  New York, NY, USA, 2023. Association for Computing Machinery.
\newblock ISBN 9798400700156.
\newblock \doi{10.1145/3572848.3577479}.
\newblock URL \url{https://doi.org/10.1145/3572848.3577479}.

\bibitem[Peng et~al.(2023{\natexlab{a}})Peng, Alcaide, Anthony, Albalak,
  Arcadinho, Cao, Cheng, Chung, Grella, GV, et~al.]{peng2023rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi
  Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, et~al.
\newblock R{W}{K}{V}: Reinventing {R}{N}{N}s for the {T}ransformer era.
\newblock \emph{arXiv preprint arXiv:2305.13048}, 2023{\natexlab{a}}.

\bibitem[Peng et~al.(2023{\natexlab{b}})Peng, Quesnelle, Fan, and
  Shippole]{peng2023yarn}
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
\newblock Yarn: Efficient context window extension of large language models.
\newblock \emph{arXiv preprint arXiv:2309.00071}, 2023{\natexlab{b}}.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{peng2021random}
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah~A Smith, and
  Lingpeng Kong.
\newblock Random feature attention.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2021.

\bibitem[Rabe and Staats(2021)]{rabe2021self}
Markus~N Rabe and Charles Staats.
\newblock Self-attention does not need $ {O} (n^2) $ memory.
\newblock \emph{arXiv preprint arXiv:2112.05682}, 2021.

\bibitem[Research(2024)]{colfax_cutlass_transpose_2024}
Colfax Research.
\newblock {T}utorial: {M}atrix {T}ranspose in {CUTLASS}, 2024.
\newblock URL
  \url{https://research.colfax-intl.com/tutorial-matrix-transpose-in-cutlass/}.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and Grangier]{roy2020efficient}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing {T}ransformers.
\newblock \emph{arXiv preprint arXiv:2003.05997}, 2020.

\bibitem[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi,
  Liu, Remez, Rapin, et~al.]{roziere2023code}
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
  Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin,
  et~al.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Sanovar et~al.(2024)Sanovar, Bharadwaj, Amant, Rühle, and
  Rajmohan]{sanovar2024-leanattention}
Rya Sanovar, Srikant Bharadwaj, Renee~St. Amant, Victor Rühle, and Saravan
  Rajmohan.
\newblock Lean attention: Hardware-aware scalable attention mechanism for the
  decode-phase of transformers.
\newblock 2024.

\bibitem[Shaham et~al.(2022)Shaham, Segal, Ivgi, Efrat, Yoran, Haviv, Gupta,
  Xiong, Geva, Berant, et~al.]{shaham2022scrolls}
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit
  Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et~al.
\newblock Scrolls: Standardized comparison over long language sequences.
\newblock \emph{arXiv preprint arXiv:2201.03533}, 2022.

\bibitem[Shazeer(2019)]{shazeer2019fast}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv preprint arXiv:1911.02150}, 2019.

\bibitem[Spector et~al.(2024)Spector, Singhal, Arora, and
  R{\'e}]{spector2024thunder}
Benjamin Spector, Aaryan Singhal, Simran Arora, and Christopher R{\'e}, 2024.
\newblock URL \url{https://github.com/HazyResearch/ThunderKittens}.

\bibitem[Sun et~al.(2019)Sun, Liu, Wu, Pei, Lin, Ou, and
  Jiang]{sun2019bert4rec}
Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
\newblock Bert4rec: Sequential recommendation with bidirectional encoder
  representations from transformer.
\newblock In \emph{Proceedings of the 28th ACM international conference on
  information and knowledge management}, pages 1441--1450, 2019.

\bibitem[Sun et~al.(2024)Sun, Chen, Kolter, and Liu]{sun2024massive}
Mingjie Sun, Xinlei Chen, J~Zico Kolter, and Zhuang Liu.
\newblock Massive activations in large language models.
\newblock \emph{arXiv preprint arXiv:2402.17762}, 2024.

\bibitem[Sun et~al.(2023)Sun, Dong, Huang, Ma, Xia, Xue, Wang, and
  Wei]{sun2023retentive}
Yutao Sun, Li~Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong
  Wang, and Furu Wei.
\newblock Retentive network: A successor to transformer for large language
  models.
\newblock \emph{arXiv preprint arXiv:2307.08621}, 2023.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Bahri, and Metzler]{tay2020efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020.

\bibitem[Thakkar et~al.(2023)Thakkar, Ramani, Cecka, Shivam, Lu, Yan, Kosaian,
  Hoemmen, Wu, Kerr, Nicely, Merrill, Blasig, Qiao, Majcher, Springer,
  Hohnerbach, Wang, and Gupta]{Thakkar_CUTLASS_2023}
Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan
  Yan, Jack Kosaian, Mark Hoemmen, Haicheng Wu, Andrew Kerr, Matt Nicely, Duane
  Merrill, Dustyn Blasig, Fengqi Qiao, Piotr Majcher, Paul Springer, Markus
  Hohnerbach, Jin Wang, and Manish Gupta.
\newblock {CUTLASS}, January 2023.
\newblock URL \url{https://github.com/NVIDIA/cutlass}.

\bibitem[Tseng et~al.(2024)Tseng, Chee, Sun, Kuleshov, and
  De~Sa]{tseng2024quip}
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher
  De~Sa.
\newblock Quip\#: Even better llm quantization with hadamard incoherence and
  lattice codebooks.
\newblock \emph{arXiv preprint arXiv:2402.04396}, 2024.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Waleffe et~al.(2024)Waleffe, Byeon, Riach, Norick, Korthikanti, Dao,
  Gu, Hatamizadeh, Singh, Narayanan, et~al.]{waleffe2024empirical}
Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti,
  Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et~al.
\newblock An empirical study of mamba-based language models.
\newblock \emph{arXiv preprint arXiv:2406.07887}, 2024.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and
  Singh]{xiong2021nystromformer}
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung,
  Yin Li, and Vikas Singh.
\newblock Nystr{\"o}mformer: A nyst{\"o}m-based algorithm for approximating
  self-attention.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence. AAAI Conference on Artificial Intelligence}, volume~35, page
  14138, 2021.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and
  Cao]{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
  and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020bigbird}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zyphra(2024)]{zamba}
Zyphra.
\newblock Zyphra unveils zamba: A compact 7b ssm hybrid model.
\newblock \emph{Zyphra blog}, 2024.

\end{thebibliography}
