- **Distributed Representations**: Dense real-valued vectors encoding semantics of linguistic units; crucial for NLP tasks.
  
- **Key Models**:
  - **SkipThought Vectors**: Predicts surrounding sentences using RNNs; sensitive to word order.
  - **ParagraphVector (DBOW & DM)**: Log-linear models predicting words from sentence vectors; DBOW uses sentence vectors alone, while DM combines with word vectors.
  
- **New Objectives**:
  - **Sequential Denoising Autoencoders (SDAEs)**: Trained to recover original sentences from corrupted versions; adaptable to variable-length sentences.
    - Noise function: Deletes words with probability \( p_o \) and swaps bigrams with probability \( p_x \).
  - **FastSent**: Log-linear model predicting adjacent sentences from a bag-of-words representation; computationally efficient.
    - Cost function: \( \sum_{w \in S_{i-1} \cup S_{i+1}} \phi(s_i, v_w) \).

- **Evaluation Metrics**:
  - **Supervised**: Performance on tasks like paraphrase identification and sentiment analysis using logistic regression classifiers.
  - **Unsupervised**: Cosine distance between sentence vectors correlated with human judgments (e.g., SICK dataset).

- **Findings**:
  - Deeper models (e.g., SkipThought) excel in supervised tasks; shallow models (e.g., FastSent) perform better in unsupervised settings.
  - Bag-of-words model using dictionary definitions shows strong performance across both evaluation types.

- **Training Details**:
  - Models trained on the Toronto Books Corpus; representation dimensions tuned from {100, 200, 300, 400, 500}.
  - SDAEs and SkipThought require significant training time (up to two weeks).

- **Applications**: Insights from this study can guide the development of language understanding systems by optimizing representation learning based on task requirements.