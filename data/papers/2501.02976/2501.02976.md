---
author:
- |
  Rui Xie$^{1*}$, Yinhong Liu$^{1*}$, Penghao Zhou$^2$, Chen Zhao$^1$, Jun Zhou$^3$  
  Kai Zhang$^1$, Zhenyu Zhang$^1$, Jian Yang$^{1}$, Zhenheng Yang$^2$, Ying Tai$^{1\dagger}$  
  $^1$Nanjing University, $^2$ByteDance, $^3$Southwest University  
  <https://nju-pcalab.github.io/projects/STAR>
bibliography:
- main.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: |
  STAR: Spatial-Temporal Augmentation with Text-to-Video Models  
  for Real-World Video Super-Resolution
---





# Perception-Distortion Trade-Off

<figure id="fig:bt_ablation">
<span class="image placeholder" data-original-image-src="figure_of_supp/bt_ablation.png" data-original-image-title="" width="1\linewidth"></span>
<figcaption>Ablation on <span class="math inline">\(b(t)\)</span>. Higher hyper-parameter <span class="math inline">\(\beta\)</span> produces results with greater fidelity, while lower <span class="math inline">\(\beta\)</span> emphasizes more perceptual quality.</figcaption>
</figure>

The trade-off between perception and distortion is a widely recognized challenge in the super-resolution domain. Thanks to our *DF Loss*, our method can easily control the model to favor either fidelity or perceptual quality in the generated results. We can adjust the hyper-parameter $\beta$ in the $b(t)$ to achieve this goal. The total loss in our STAR is: $$\mathcal{L}_{total} = \mathcal{L}_{v} + b(t)\mathcal{L}_{DF},$$ The $b(t)$ can be written as follows: $$b(t) = \beta \cdot (1 - \frac{t}{t_{max}}),$$ Where $t$ is the timestep and $\beta$ is the hyper-parameter that adjusts the weight between $\mathcal{L}_v$ and $\mathcal{L}_{DF}$, which we set to 1 by default. From equations (1) and (2), we can observe that a larger $\beta$ increases the weight of the DF loss at each timestep, thereby further enhancing the fidelity of the results. In contrast, a smaller $\beta$ reduces the influence of the DF loss at each timestep, allowing the v-prediction loss to have a greater impact and produce more perceptual results. The $b(t)$ - $t$ curves under different $\beta$ are shown in Figure .

We conduct experiments under these settings to demonstrate the ability to achieve the perception-distortion trade-off. The quantitative results are shown in Table . From Table , we can observe that increasing $\beta$ improves the PSNR and $E_{warp}^*$, leading to better fidelity. Conversely, decreasing $\beta$ reduces the LPIPS score, indicating better perceptual quality.

| $\beta$ | PSNR$\uparrow$ | LPIPS$\downarrow$ | $E_{warp}^*\downarrow$ |
|:-------:|:--------------:|:-----------------:|:----------------------:|
|  0.25   |     23.55      |    **0.1825**     |          2.88          |
|  0.75   |     23.76      |      0.1842       |          2.74          |
|   1.0   |     23.91      |      0.1885       |          2.68          |
|   1.5   |     24.08      |      0.2272       |          2.53          |
|   2.0   |   **24.41**    |      0.3339       |        **2.21**        |

Qualitative comparison under different $\beta$ of $b(t)$.

# More Results

## User Study

To find the human-preferred results between our STAR and other state-of-the-art methods, we conduct a user study that evaluate the results on both real-world and synthetic datasets. Specifically, we use the real-world dataset VideoLQ and the synthetic dataset REDS30 . We select two image-diffusion-model-based methods, Upscale-A-Video and MGLD-VSR ; and one GAN-based method, RealViformer for comparison. We invite 12 evaluators to participate in the user study. For each evaluator, we randomly select 10 videos from each dataset and present four results: one from our STAR and three from the compared methods. The evaluators were asked to choose which result had the best visual quality and temporal consistency. The results of the user study are depicted in Figure , indicating that our STAR is preferred by most human evaluators for both visual quality and temporal consistency.

## Qualitative Comparisons

We provide more visual comparisons on synthetic and real-world datasets in Figure and Figure to further highlight our advantages in spatial quality. These results clearly demonstrate that our method preserves richer details and achieves greater realism. To demonstrate the impact of scaling up with larger text-to-video (T2V) models, we present additional results in Figure . It is evident that scaling up the T2V model further improves the restoration effect, indicating that a large and robust T2V model can serve as a strong base model for video super-resolution.

## Video Demo

We provide a demo video [<span style="color: red">\[STAR-demo.mp4\]</span>](https://youtu.be/hx0zrql-SrU) in the supplementary material, showcasing the temporal and spatial advantages of our proposed STAR more intuitively. This video includes additional results and comparisons on synthetic, real-world, and AIGC videos.
