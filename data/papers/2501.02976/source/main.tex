% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper,table]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage{multirow}
\usepackage{xcolor}
% \usepackage{array}
% \usepackage[accsupp]{axessibility}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\paperID{7929} % *** Enter the Paper ID here
% \def\confName{CVPR}
% \def\confYear{2025}

\renewcommand{\thefootnote}{}
\newcommand{\name}{STAR}
% \newcommand{\name}{$\mathtt{STAR}$}
%%%%%%%%% TITLE - PLEASE UPDATE
\title{STAR: Spatial-Temporal Augmentation with Text-to-Video Models\\ for Real-World Video Super-Resolution}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{
  Rui Xie$^{1*}$, \hspace{0.2cm}
  Yinhong Liu$^{1*}$, \hspace{0.2cm}
  Penghao Zhou$^2$, \hspace{0.2cm}
  Chen Zhao$^1$, \hspace{0.2cm}
  Jun Zhou$^3$ \\
  Kai Zhang$^1$, \hspace{0.2cm}
  Zhenyu Zhang$^1$, \hspace{0.2cm}
  Jian Yang$^{1}$, \hspace{0.2cm}
  Zhenheng Yang$^2$, \hspace{0.2cm}
  Ying Tai$^{1\dagger}$ \\
  $^1$Nanjing University, \hspace{0.2cm}
  $^2$ByteDance, \hspace{0.2cm}
  $^3$Southwest University \\
  {\small \url{https://nju-pcalab.github.io/projects/STAR}}
}


\begin{document}
% \maketitle

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \captionsetup{type=figure}
    \includegraphics[width=\textwidth]{figure/figure1.pdf} \vspace{-6mm}
    \captionof{figure}{Visualization comparisons on both real-world and synthetic low-resolution videos. 
    Compared to the state-of-the-art VSR models~\cite{zhang2024realviformer,zhou2024upscale}, our results demonstrate more natural facial details and better structure of the text. 
    \textbf{(Zoom-in for best view)}}
    \label{teaser}
\end{center}
}]

\input{sec/0_abstract}
\input{sec/1_intro}
\input{sec/2_relatedwork}
\input{sec/3_methodology}
\input{sec/4_experiments}
\input{sec/5_conclusion}
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

\newpage
\appendix

\section{Perception-Distortion Trade-Off}

\begin{figure}[b]
    \centering
    \includegraphics[width=1\linewidth]{figure_of_supp/bt_ablation.png}
    \caption{Ablation on $b(t)$. Higher hyper-parameter $\beta$ produces results with greater fidelity, while lower $\beta$ emphasizes more perceptual quality.}
    \label{fig:bt_ablation}
\end{figure}

The trade-off between perception and distortion \cite{blau2018perception} is a widely recognized challenge in the super-resolution domain. Thanks to our \textit{DF Loss}, our method can easily control the model to favor either fidelity or perceptual quality in the generated results. We can adjust the hyper-parameter $\beta$ in the $b(t)$ to achieve this goal. The total loss in our~\name~is:
\begin{equation}
    \mathcal{L}_{total} = \mathcal{L}_{v} + b(t)\mathcal{L}_{DF},
\end{equation}
The $b(t)$ can be written as follows:
\begin{equation}
    b(t) = \beta \cdot (1 - \frac{t}{t_{max}}),
\end{equation}
Where \( t \) is the timestep and \( \beta \) is the hyper-parameter that adjusts the weight between \( \mathcal{L}_v \) and \( \mathcal{L}_{DF} \), which we set to 1 by default. From equations (1) and (2), we can observe that a larger \( \beta \) increases the weight of the DF loss at each timestep, thereby further enhancing the fidelity of the results. In contrast, a smaller \( \beta \) reduces the influence of the DF loss at each timestep, allowing the v-prediction loss to have a greater impact and produce more perceptual results. The $b(t)$ - $t$ curves under different $\beta$ are shown in Figure \ref{fig:bt_ablation}. 

We conduct experiments under these settings to demonstrate the ability to achieve the perception-distortion trade-off. The quantitative results are shown in Table \ref{tab:beta_ablation}. From Table \ref{tab:beta_ablation}, we can observe that increasing $\beta$ improves the PSNR and $E_{warp}^*$, leading to better fidelity. Conversely, decreasing $\beta$ reduces the LPIPS score, indicating better perceptual quality.


\begin{table}[]
    \centering
    \caption{Qualitative comparison under different $\beta$ of $b(t)$.}
    \begin{tabular}{c|ccc}
    \hline
    $\beta$ & PSNR$\uparrow$ & LPIPS$\downarrow$ & $E_{warp}^*\downarrow$ \\ \hline
       0.25 & 23.55 & \textbf{0.1825} & 2.88  \\
       0.75 & 23.76 & 0.1842 & 2.74  \\
       1.0 & 23.91 & 0.1885 & 2.68  \\
       1.5 & 24.08 & 0.2272 & 2.53  \\
       2.0 & \textbf{24.41} & 0.3339 & \textbf{2.21} \\ \hline
    \end{tabular}
    \label{tab:beta_ablation}
\end{table}


\section{More Results}
\subsection{User Study}
To find the human-preferred results between our~\name~and other state-of-the-art methods, we conduct a user study that evaluate the results on both real-world and synthetic datasets. Specifically, we use the real-world dataset VideoLQ \cite{chan2022investigating} and the synthetic dataset REDS30 \cite{nah2019ntire}. We select two image-diffusion-model-based methods, Upscale-A-Video \cite{zhou2024upscale} and MGLD-VSR \cite{yang2023mgldvsr}; and one GAN-based method, RealViformer \cite{zhang2024realviformer} for comparison. 
We invite 12 evaluators to participate in the user study. For each evaluator, we randomly select 10 videos from each dataset and present four results: one from our~\name~and three from the compared methods. The evaluators were asked to choose which result had the best visual quality and temporal consistency.
The results of the user study are depicted in Figure \ref{fig:user_study}, indicating that our \name\ is preferred by most human evaluators for both visual quality and temporal consistency.

\begin{figure*}[]
    \centering
    \includegraphics[width=\linewidth]{figure_of_supp/user_study.pdf}
    \caption{User study results. Our \name\ is preferred by human evaluators for both visual quality and temporal consistency.}
    \label{fig:user_study}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure_of_supp/synthetic.pdf}
    \caption{Qualitative comparisons on synthetic datasets. Our~\name~generates more detailed and realistic results. \textbf{(Zoom-in for best view)}}
    \label{fig:synthetic_comparison}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure_of_supp/real_world.pdf}
    \caption{Qualitative comparisons on real-world datasets. Our~\name~produces the clearest facial details and the most accurate text structure. \textbf{(Zoom-in for best view)}}
    \label{fig:realworld_comparison}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure_of_supp/scale_up.pdf}
    \caption{Qualitative comparisons on synthetic and real-world datasets with larger T2V models. Scaling up the T2V model enhances detail and realism in video super-resolution results. \textbf{(Zoom-in for best view)}}
    \label{fig:scale_up}
\end{figure*}

\subsection{Qualitative Comparisons}
We provide more visual comparisons on synthetic and real-world datasets in Figure \ref{fig:synthetic_comparison} and Figure \ref{fig:realworld_comparison} to further highlight our advantages in spatial quality. These results clearly demonstrate that our method preserves richer details and achieves greater realism.
To demonstrate the impact of scaling up with larger text-to-video (T2V) models, we present additional results in Figure \ref{fig:scale_up}. It is evident that scaling up the T2V model further improves the restoration effect, indicating that a large and robust T2V model can serve as a strong base model for video super-resolution.


\subsection{Video Demo}
We provide a demo video \href{https://youtu.be/hx0zrql-SrU}{\textcolor{red}{[STAR-demo.mp4]}} in the supplementary material, showcasing the temporal and spatial advantages of our proposed~\name~more intuitively. This video includes additional results and comparisons on synthetic, real-world, and AIGC videos.


\end{document}
