\section{Introduction}
\label{sec:intro}
Real-world video super-resolution (VSR) aims to generate high-resolution (HR) videos with clear details and strong temporal consistency from low-resolution (LR) inputs with unknown degradations. 
Most VSR methods \cite{wang2019edvr, chan2022basicvsr++, jo2018deep, xue2019video} only focus on simple, known degradations like downsampling \cite{fuoli2019efficient, isobe2020video} or camera-related issues~\cite{yang2021realvsr}. 
However, real-world scenarios often involve \textit{unexpected degradations} such as noise, blur, and compression, making it difficult for models to capture both spatial and temporal information needed for high-quality, consistent restoration.

GAN-based methods \cite{zhang2024realviformer, chan2022investigating, wang2021realesrgan, wu2022animesr, yang2021realvsr} are widely used in real-world VSR for improving details through adversarial learning. 
By incorporating optical flow maps, they also improve temporal consistency, yielding smooth motion across frames. However, their limited generative capacity often results in oversmoothing, as illustrated in Figure~\ref{teaser}.
%
Recently, image diffusion models \cite{rombach2022high} have been applied to real-world VSR for realistic video generation. Methods like \cite{zhou2024upscale, chen2024learning, yuan2024inflation, yang2023mgldvsr} incorporate temporal blocks or optical flow maps to improve temporal information capture. However, since these models are primarily trained on image data rather than video data~\cite{nan2024openvid, chen2024panda, wang2023internvid, wang2024vidprom}, simply adding temporal layers often fails to ensure high temporal consistency (see Figure~\ref{fig:temp_consis}). 
%
VEnhancer \cite{he2024venhancer} and LaVie-SR \cite{wang2023lavie} incorporate T2V models for super-resolving AI-generated videos. However, two key challenges still remain: \textit{artifacts introduced by complex degradations} in real-world settings, and \textit{compromised fidelity} due to the strong generative capacity of powerful T2V models (\textit{e.g.}, CogVideoX).

% To fully leverage the T2V prior~\cite{zhang2023i2vgen, yang2024cogvideox} for improve real-world VSR, we introduce\textbf{~\name}, a novel Spatial-Temporal Augmentation for Real-world VSR, achieving realistic spatial details and robust temporal consistency.
To fully leverage the T2V prior~\cite{zhang2023i2vgen, yang2024cogvideox} to enhance practical VSR, we introduce~\name, a novel Spatial-Temporal Augmentation approach for Real-world VSR that achieves realistic spatial details and robust temporal consistency.
%
Specifically,
$1$) To address artifacts, we introduce a Local Information Enhancement Module (LIEM) before global self-attention to evaluate its impact on T2V models for real-world VSR. 
This approach stems from our observation that most T2V models rely solely on a global information extraction module (\textit{i.e.}, global self-attention), whereas capturing local details is crucial for video restoration.
$2$) To improve fidelity, we propose a Dynamic Frequency (DF) Loss, guiding the model to prioritize low- or high-frequency information at different diffusion steps. 
This is based on our observation that during the reverse diffusion process, our model tends to first recover structure and then refine details. This approach decouples fidelity requirements, reduces learning difficulty, and enhances restoration fidelity.


In summary, our main contributions are as follows:

$\bullet$ 
%We are the first to introduce the text-to-video diffusion prior into real-world video super-resolution. Furthermore, our proposed LIEM and DF Loss are adaptable to most T2V models, leading to improved performance in video restoration tasks.
We propose~\name, a Spatio-Temporal quality Augmentation framework for Real-world VSR.
To our best knowledge, we are the first to integrate diverse, powerful text-to-video diffusion priors into real-world VSR, improving both spatial details and temporal consistency.
%We are the first to integrate the text-to-video diffusion prior into real-world video super-resolution. Additionally, our proposed LIEM and DF Loss are adaptable across most T2V models, enhancing performance in video restoration tasks.

$\bullet$ 
%We introduce LIEM to enhance local information and reduce the difficulty of degradation removal, which helps mitigate artifacts. Additionally, we propose DF Loss to guide the model in learning different frequency information across diffusion steps, decoupling fidelity requirements, and ultimately improving fidelity.
We introduce LIEM to enhance local details and ease degradation removal, effectively mitigating artifacts. 
Moreover, we propose DF loss to guide the model in learning frequency-specific information across diffusion steps, decoupling fidelity requirements and ultimately improving overall fidelity.

$\bullet$ Our~\name~achieves the highest clarity (DOVER scores) across all datasets compared to state-of-the-art methods, while maintaining robust temporal consistency.