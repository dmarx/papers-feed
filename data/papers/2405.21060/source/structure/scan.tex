\section{Efficient Algorithms for the Scalar SSM Scan (1-SS Multiplication)}
\label{sec:scan}

In this section we flesh out various algorithms for computing the scalar SSM scan, through the lens of structured matrix decompositions.
The scalar SSM scan is defined as computing the recurrent part of the discrete SSM \eqref{eq:1ss-recurrence},
in the case when $N=1$ (i.e.\ $A$ is a scalar).
This is commonly used to compute SSMs recurrently;
in particular, the case of structured SSMs where $A$ is diagonally structured reduces down to this operation,
such as in the S5~\citep{smith2023s5} and S6~\citep{gu2023mamba} models.

The goal of this section is to support a central theme of this paper that \emph{efficient algorithms for sequence models can be viewed as structured matrix multiplication algorithms}.
The various matrix decomposition ideas we show here are related to ideas used to derive fast SSM algorithms (\cref{sec:efficient}),
as well as directly used as a subroutine.


\subsection{Problem Definition}
Let $a : \mathtt{(D,)}$ and $b : \mathtt{(D,)}$ be sequences of scalars.
The \textbf{scalar SSM scan} is defined as
\begin{equation}%
  \label{eq:ssm-scan}
  h_t = a_t h_{t-1} + b_t
  .
\end{equation}
Here $h_{-1}$ can be an arbitrary value representing the previous \emph{hidden state} to the SSM recurrence;
unless otherwise specified, we assume $h_{-1} = 0$.

We also call equation \eqref{eq:ssm-scan} the \textbf{\texttt{cumprodsum}} (cumulative product sum).
Note that the \texttt{cumprodsum} reduces to the \texttt{cumprod} (cumulative product) when $b = 0$ is the additive identity and it reduces to the \texttt{cumsum} (cumulative sum) when $a=1$ is the multiplicative identity.

Finally, note that in vectorized form we can write
\begin{align*}%
  h &= M b \\
  M &=
  \begin{bmatrix}
    1 & \\
    a_1 & 1 & \\
    a_2a_1 & a_2 & 1 \\
    \vdots & \vdots & \ddots & \ddots \\
    a_{T-1}\dots a_1 & a_{T-1}\dots a_2 & \dots & a_{T-1} & 1 \\
  \end{bmatrix}
\end{align*}
In other words, this is simply the matrix-vector product by a 1-SS matrix $M$.

Therefore we have three ways of viewing this fundamental primitive operation that are all equivalent:
\begin{itemize}
  \item A (scalar) SSM scan.
  \item A \texttt{cumprodsum}.
  \item A 1-SS matrix-vector multiplication .
\end{itemize}

\subsection{Classical Algorithms}

We first describe the two classical ways of computing the SSM scan \eqref{eq:ssm-scan},
previously used by prior work.

\subsubsection{Sequential Recurrence}
\label{sec:scan:recurrence}

The recurrent mode simply computes \eqref{eq:ssm-scan} one timestep $t$ at a time.
From the perspective of 1-SS multiplication, this was also described in \cref{sec:ssm:algorithms:linear}.

\subsubsection{Parallel Associative Scan}
\label{sec:scan:classical:parallel}

Second, an important observation is that this recurrence can be turned into an associative scan~\citep{martin2018parallelizing,smith2023s5}.
This fact is not completely obvious.
For example, S5 defined the correct associative scan operator and then showed associativity of the operator through rote calculation.

A slightly cleaner way to see that this is computable with an associative scan is to turn the multi-term recurrence into a single-term recurrence on a hidden state of size $2$ instead of $1$:
\begin{align*}%
  h_t &= a_t h_{t-1} + b_t
  \\
  \begin{bmatrix} h_t \\ 1 \end{bmatrix}
      &=
  \begin{bmatrix}
    a_t & b_t \\ 0 & 1
  \end{bmatrix}
  \begin{bmatrix} h_{t-1} \\ 1 \end{bmatrix}
  .
\end{align*}
Then computing all the $h_t$ is the same as taking the cumulative products of these $2 \times 2$ matrices.
Since matrix multiplication is associative,
this can be computed with an associative scan.
The associative binary operator is simply matrix multiplication on these particular matrices:
\begin{align*}%
  \begin{bmatrix}
    a_t & b_t \\ 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
    a_s & b_s \\ 0 & 1
  \end{bmatrix}
  =
  \begin{bmatrix}
    a_ta_s & a_tb_s + b_t \\ 0 & 1
  \end{bmatrix}
  .
\end{align*}
Equating the top row yields the same associative scan operator as defined by S5:
\begin{equation}%
  \label{eq:scan:associative-operator}
  (a_t, b_t) \otimes (a_s, b_s) = (a_ta_s, a_tb_s + b_t)
  .
\end{equation}

The reason why associative scans are important is that they can be parallelized using a divide-and-conquer algorithm~\citep{blelloch1990prefix}.
We omit the details of this algorithm, and instead show that the entire associative SSM scan algorithm can be derived from scratch through matrix decompositions (\cref{sec:scan:associative}).

\subsection{Efficient Algorithms via Structured Matrix Decompositions}

We discuss several algorithms for computing the SSM scan, all through the lens of finding structured matrix decompositions of the 1-SS matrix $M$.
These algorithms or computation modes include
\begin{itemize}
  \item A \emph{dilated} mode where information is propagated $1, 2, 4, 8, \dots$ steps at a time.
  \item A \emph{state-passing} mode where information is propagated forward in chunks.
  \item A \emph{fully recurrent} mode that increments one step at a time, which is a special case of the state-passing mode.
  \item A \emph{block decomposition} parallel mode where $M$ is divided into hierarchical blocks.
  \item A \emph{scan} mode where $M$ is divide into equal size blocks and reduced recursively.
\end{itemize}

\subsubsection{Dilated Mode}

This mode factors the 1-SS matrix in a particular way involving increasing ``strides''.
This is best illustrated through a concrete example:
\footnotesize
\begin{align*}%
  M &=
  \begingroup
  \setlength\arraycolsep{2pt}
  \begin{bmatrix}
    a_{0:0} & \\
    a_{1:0} & a_{1:1} & \\
    a_{2:0} & a_{2:1} & a_{2:2} \\
    a_{3:0} & a_{3:1} & a_{3:2} & a_{3:3} \\
    a_{4:0} & a_{4:1} & a_{4:2} & a_{4:3} & a_{4:4} \\
    a_{5:0} & a_{5:1} & a_{5:2} & a_{5:3} & a_{5:4} & a_{5:5} \\
    a_{6:0} & a_{6:1} & a_{6:2} & a_{6:3} & a_{6:4} & a_{6:5} & a_{6:6} \\
    a_{7:0} & a_{7:1} & a_{7:2} & a_{7:3} & a_{7:4} & a_{7:5} & a_{7:6} & a_{7:7} \\
  \end{bmatrix}
  \endgroup
  \\&=
  \begingroup
  \setlength\arraycolsep{2pt}
  \begin{bmatrix}
    a_{0:0} & \\
            & a_{1:1} & \\
            &         & a_{2:2} \\
            &         &         & a_{3:3} \\
    a_{4:0} &         &         &         & a_{4:4} \\
            & a_{5:1} &         &         & & a_{5:5} \\
            &         & a_{6:2} &         & & & a_{6:6} \\
            &         &         & a_{7:3} & & & & a_{7:7} \\
  \end{bmatrix}
  \begin{bmatrix}
    a_{0:0} & \\
            & a_{1:1} & \\
    a_{2:0} &         & a_{2:2} \\
            & a_{3:1} &         & a_{3:3} \\
            &         & a_{4:2} &         & a_{4:4} \\
            &         &         & a_{5:3} &         & a_{5:5} \\
            &         &         &         & a_{6:4} &         & a_{6:6} \\
            &         &         &         &         & a_{7:5} & & a_{7:7} \\
  \end{bmatrix}
  \begin{bmatrix}
    a_{0:0} & \\
    a_{1:0} & a_{1:1} & \\
            & a_{2:1} & a_{2:2} \\
            &         & a_{3:2} & a_{3:3} \\
            &         &         & a_{4:3} & a_{4:4} \\
            &         &         &         & a_{5:4} & a_{5:5} \\
            &         &         &         &         & a_{6:5} & a_{6:6} \\
            &         &         &         &         &         & a_{7:6} & a_{7:7} \\
  \end{bmatrix}
  \endgroup
\end{align*}
\normalsize

Note that this closely resembles the computation of dilated convolutions.

We also note that this factorization shows that 1-SS matrices are a special case of butterfly matrices, another broad and fundamental type of structured matrix \citep{dao2019learning,dao2020kaleidoscope}.

\begin{remark}
  This algoritihm is sometimes described as a ``work-inefficient but more parallelizable'' prefix sum algorithm~\citep{hillis1986data}, becauses it uses $O(T\log(T))$ operations but has half the depth/span as the work-efficient associative scan algorithm.
\end{remark}

\subsubsection{State-Passing (Chunkwise) Mode}

This mode can be viewed as a generalization of the standard recurrent mode where instead of passing forward the recurrent state $h$ one step at a time, we compute the answer on chunks of arbitrary length $k$ and pass the state through the chunk.
This can also be derived from a simple block decomposition of the 1-SS matrix.

\begin{remark}
  While we call this ``state-passing'' to refer to how states are passed from one local segment to another,
  this is related to the ``chunkwise'' algorithms proposed by related models~\citep{sun2023retentive,yang2024gated}.
\end{remark}


Consider computing $h = Mb$ in ``chunks'': for some index $k \in [T]$, we want to compute $h_{0:k}$ or the output up to index $k$, and have a way to reduce the problem to a smaller problem on indices $[k:T]$.


We write $M$ as
\begin{align*}%
  M =
  \begin{bmatrix}
    a_{0:0} & \\
    a_{1:0} & a_{1:1} \\
    \vdots & & \ddots \\
    a_{k-1:0} & \dots & \dots & a_{k-1:k-1} \\
    a_{k:0} & \dots & \dots & a_{k:k-1} & a_{k:k} \\
    \vdots & & & \vdots & \vdots & \ddots \\
    a_{T-1:0} & \dots & \dots & a_{T-1:k-1} & a_{T-1:k} & \dots & a_{T-1:T-1} \\
  \end{bmatrix}
\end{align*}

Let the upper-left triangle be $M_L$, lower-right be $M_R$ (left and right subproblems), and lower-left be $M_C$.
Divide up $b$ into $b_L = b_{0:k}$ and $b_R = b_{k:T}$ in the same way.
Note that
\begin{align*}%
  Mb = \begin{bmatrix} M_L b_L \\ M_R b_R + M_C b_L \end{bmatrix}
\end{align*}
Also, $M_C$ has the rank-1 factorization (this is essentially the defining property of semiseparable matrices)
\begin{align*}%
  M_C =
  \begin{bmatrix} a_{k:k} \\ \vdots \\ a_{T-1:k} \end{bmatrix}
  a_k
  \begin{bmatrix} a_{k-1:0} & \cdots & a_{k-1:k-1} \end{bmatrix}
\end{align*}

Thus
\begin{align*}%
  M_C b_L =
  \begin{bmatrix} a_{k:k} \\ \vdots \\ a_{T-1:k} \end{bmatrix}
  a_k
  \cdot
  (Mb)_{k-1}
  .
\end{align*}
Here we think of $(Mb)_{k-1} = h_{k-1}$ as the ``final state'' of the left chunk,
because the row vector in $M_C$'s factorization is the same as the final row of $M_L$.
Furthermore, note that the column vector in $M_C$'s factorization is the same as the final column of $M_R$.\footnote{Both these facts can be seen from the Woodbury inverse...}
Thus
\begin{align*}%
  M_R b_R + M_C b_L =
  M_R
  \begin{bmatrix} a_k h_{k-1} + b_k \\ b_{k+1} \\ \vdots \\ b_{T-1} \end{bmatrix}
\end{align*}

Finally, we use the observation that $M_L$ and $M_R$ are self-similar to the original matrix $M$; the answers for these two smaller 1-SS matrix multiplications can be performed arbitrarily using any algorithm.
In total, the algorithm proceeds as follows:
\begin{enumerate}
  \item Compute the left half of the answer $h_{0:k}$ using any desired method (i.e.\ any of the methods for 1-SS multiplication from this section).
  \item Compute the final state $h_{k-1}$. %
  \item Increment the state by one step to modify $b_{k}$.
  \item Compute the right half of the answer $h_{k:T}$ using any desired method.
\end{enumerate}
%

In other words, we compute the left subproblem as a black box, pass its final state on to the right problem, and compute the right subproblem as a black box.

The utility of this method comes from more complicated settings, such as in the general $N$-semiseparable case,
and when the input $b$ has an additional ``batch'' dimension (or in other words this is a matrix-matrix instead of matrix-vector multiplication).
In this case, we can use an alternate algorithm for the chunks (corresponding to MM by $M_L$ and $M_R$) that does not materialize the full hidden states $h$.
Instead, we skip the hidden states and directly compute the final state $h_{k-1}$ in an alternate way, then ``pass'' the state to the next chunk.
%


\paragraph{Complexity.}
This method can be very work-efficient because steps 2-3 takes only constant time.
Therefore assuming the two subproblems (steps 1 and 4) are linear time,
the whole method takes linear time.

The downside is that this is also sequential.

\subsubsection{Fully Recurrent Mode}

Note that the fully recurrent mode, where the recurrence is evolved one step at a time \eqref{eq:ssm-scan},
is simply an instantiation of the state-passing mode with chunk size $k=1$.

\subsubsection{(Parallel) Block Decomposition Mode}


This uses the same matrix decomposition as the state-passing mode,
but computes subproblems in a different order that trades off computation for parallelization.

As usual, we write $M$ as
\begin{align*}%
  M =
  \begin{bmatrix}
    1 & \\
    a_1 & 1 & \\
    a_2a_1 & a_2 & 1 \\
    \vdots & \vdots & \ddots & \ddots \\
    a_{T-1}\dots a_1 & a_{T-1}\dots a_2 & \dots & a_{T-1} & 1 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    1 & \\
    -a_1 & 1 & \\
    0 & -a_2 & 1 \\
    \vdots & \vdots & \ddots & \ddots \\
    0 & 0 & \dots & -a_{T-1} & 1 \\
  \end{bmatrix}^{-1}
\end{align*}
The key observation is again that the bottom-left quadrant of $M$ is rank-1.
Aside from inspection, another way to see this is by using the RHS, observing that the bottom-left quadrant of it is a trivial rank-1 matrix (it is all 0 except the top-right corner is $-a_{T/2}$),
and using the Woodbury inversion formula to see that the bottom-left corner of the LHS must also be rank 1.
This also provides a way to deduce the rank-1 factorization,
which can be verified through inspection:
\begin{align*}%
  M_{\text{lower-left-quadrant}}
  &=
  \begin{bmatrix}
    (a_{T/2} \dots a_1) & \dots & a_{T/2} \\
    \vdots & \ddots & \vdots \\
    (a_{T-1} \dots a_{T/2} a_{T/2-1} \dots a_1) & \dots & (a_{T-1} \dots a_{T/2})
  \end{bmatrix}
  \\
  &=
  \begin{bmatrix}
    a_{T/2} \\ \vdots \\ a_{T-1} \dots a_{T/2}
  \end{bmatrix}
  \begin{bmatrix}
    (a_{T/2-1} \dots a_1) & \dots & a_{T/2-1} & 1
  \end{bmatrix}
  .
\end{align*}

A second observation is that \emph{this matrix is self-similar}: any principle submatrix has the same form.
In particular, the top-left and bottom-right quadrants are both 1-SS matrices.

This provides an easy way to perform the matrix multiplication by $M$:
recurse on the two halves (i.e.\ top-left and bottom-right) in parallel,
and then account for the bottom-left submatrix.
This ``combination'' step in the divide-and-conquer algorithm is easy since the submatrix is rank 1.
This leads to a parallel algorithm.

\paragraph{Complexity.}
Like the state-passing algorithm,
this method uses the same block decompositions of the rank-structured semiseparable matrices.
The difference is that we recurse on both subproblems in parallel,
while the state-passing algorithm handles the left and then right subproblems.
This lowers the depth/span of the algorithm from linear to $\log(T)$.
The tradeoff is that the combination step (accounting for the rank-1 bottom-left submatrix) requires linear instead of constant work,
so the total work is $O(T\log(T))$ instead of linear.

Note also that in the recursion,
we can stop at any time and compute the subproblems in any other way.
This is a main idea behind the SSD algorithm (\cref{sec:efficient}),
where we switch to the dual \emph{quadratic attention} formulation on small subproblems.


\subsubsection{Associative Scan Mode}
\label{sec:scan:associative}

The state passing (chunkwise) algorithm has linear work, but also involves sequential operations.

The block matrix reduction and dilated modes are parallelizable: they have $\log(T)$ depth/span. However, they do extra work ($O(T \log(T)$).

As noted in \cref{sec:scan:classical:parallel}, there is an algorithm that achieves both $O(\log T)$ depth and $O(T)$ work by leveraging the associative scan (also called prefix scan) algorithm~\citep{baker1996pade}.
This algorithm is most easily seen from the SSM scan or \texttt{cumprodsum} view, and even then is not obvious: it requires separately deriving an associative operator \eqref{eq:scan:associative-operator},
and then leveraging the parallel/associative/prefix scan algorithm as a black box \citep{blelloch1990prefix}.

Here we show that it is actually possible to derive this parallel scan from leveraging a different matrix decomposition:

\begin{align*}%
M &=
\begin{bNiceArray}{cc|cc|cc|cc}
    a_{0:0} & \\
    a_{1:0} & a_{1:1} & \\
    \hline
    a_{2:0} & a_{2:1} & a_{2:2} \\
    a_{3:0} & a_{3:1} & a_{3:2} & a_{3:3} \\
    \hline
    a_{4:0} & a_{4:1} & a_{4:2} & a_{4:3} & a_{4:4} \\
    a_{5:0} & a_{5:1} & a_{5:2} & a_{5:3} & a_{5:4} & a_{5:5} \\
    \hline
    a_{6:0} & a_{6:1} & a_{6:2} & a_{6:3} & a_{6:4} & a_{6:5} & a_{6:6} \\
    a_{7:0} & a_{7:1} & a_{7:2} & a_{7:3} & a_{7:4} & a_{7:5} & a_{7:6} & a_{7:7} \\
\end{bNiceArray}
\\&=
\begin{bNiceArray}{cc|cc|cc|cc}[cell-space-limits=6pt,columns-width=1.2cm]
    a_{0:0} & \\
    a_{1:0} & a_{1:1} & \\
    \hline
    \Block{2-2}{\begin{bmatrix}a_{2:2}\\a_{3:2}\end{bmatrix}a_{2:1}\begin{bmatrix}a_{1:0}\\a_{1:1}\end{bmatrix}^{\top}} && a_{2:2} \\
                                                                                                                        && a_{3:2} & a_{3:3} \\
    \hline
    \Block{2-2}{\begin{bmatrix}a_{4:4}\\a_{5:4}\end{bmatrix}a_{4:1}\begin{bmatrix}a_{1:0}\\a_{1:1}\end{bmatrix}^{\top}} &&
    \Block{2-2}{\begin{bmatrix}a_{4:4}\\a_{5:4}\end{bmatrix}a_{4:3}\begin{bmatrix}a_{3:2}\\a_{3:3}\end{bmatrix}^{\top}} && a_{4:4} \\
                                                                                                                        &&&& a_{5:4} & a_{5:5} \\
    \hline
    \Block{2-2}{\begin{bmatrix}a_{6:6}\\a_{7:6}\end{bmatrix}a_{6:1}\begin{bmatrix}a_{1:0}\\a_{1:1}\end{bmatrix}^{\top}} &&
    \Block{2-2}{\begin{bmatrix}a_{6:6}\\a_{7:6}\end{bmatrix}a_{6:3}\begin{bmatrix}a_{3:2}\\a_{3:3}\end{bmatrix}^{\top}} &&
    \Block{2-2}{\begin{bmatrix}a_{6:6}\\a_{7:6}\end{bmatrix}a_{6:1}\begin{bmatrix}a_{5:4}\\a_{5:5}\end{bmatrix}^{\top}} && a_{6:6} \\
                                                                                                                        &&&&&& a_{7:6} & a_{7:7} \\
\end{bNiceArray}
\end{align*}

Now we proceed in three stages.

\paragraph{Stage 1.}
First we compute the answers for each of the diagonal blocks in the multiplication $Mb$.
This produces two numbers, but the first element is unchanged.
For example, the second block is going to compute $b_2$ and $a_3 b_2 + b_3$

\paragraph{Stage 2.}
Now consider each of the $2 \times 2$ blocks factored as a rank-1 matrix in the strictly lower triangular part of the matrix.
Note that each of the right side row vectors is the same as the bottom row vector in the diagonal block in its column:
in particular the $[ a_{1:0} \; a_{1:1} ]$, $[ a_{3:2} \; a_{3:3} ]$, and $[ a_{5:4} \; a_{5:5} ]$ rows.

Therefore we already have the answers to these from Stage 1,
which is the second element of all $T/2$ subproblems in Stage 1.
If we call this array of elements $b'$ (of half the size of $b$),
then we need to multiply $b'$ by the 1-SS matrix generated by $a_{3:-1}, a_{3:1}, a_{5:3}, a_{7:5}$.
%

\paragraph{Stage 3.}
Finally, each of the answers to Stage 2 can be broadcast into two final answers by multiplying by the left-side column vectors:
in particular the $[ a_{2:2} \; a_{3:2} ]^{\top}$, $[ a_{4:4} \; a_{5:4} ]^{\top}$, and $[ a_{6:6} \; a_{7:6} ]^{\top}$ vectors.

Note that this can be slightly modified with some off-by-one shifting of the indices.
An equivalent way to view this algorithm is as the three-step matrix factorization

\footnotesize
\begin{align*}%
  M &=
  \begingroup
  \setlength\arraycolsep{2pt}
  \begin{bmatrix}
    a_{0:0} & \\
    a_{1:0} & a_{1:1} & \\
    a_{2:0} & a_{2:1} & a_{2:2} \\
    a_{3:0} & a_{3:1} & a_{3:2} & a_{3:3} \\
    a_{4:0} & a_{4:1} & a_{4:2} & a_{4:3} & a_{4:4} \\
    a_{5:0} & a_{5:1} & a_{5:2} & a_{5:3} & a_{5:4} & a_{5:5} \\
    a_{6:0} & a_{6:1} & a_{6:2} & a_{6:3} & a_{6:4} & a_{6:5} & a_{6:6} \\
    a_{7:0} & a_{7:1} & a_{7:2} & a_{7:3} & a_{7:4} & a_{7:5} & a_{7:6} & a_{7:7} \\
  \end{bmatrix}
  \endgroup
  \\&=
  \begingroup
  \setlength\arraycolsep{2pt}
  \begin{bmatrix}
    a_{0:0} & \\
            & a_{1:1} & \\
            & a_{2:1} & a_{2:2} \\
            &         &         & a_{3:3} \\
            &         &         & a_{4:3} & a_{4:4} \\
            &         &         &         &         & a_{5:5} \\
            &         &         &         &         & a_{6:5} & a_{6:6} \\
            &         &         &         &         &         & & a_{7:7} \\
  \end{bmatrix}
  \begin{bmatrix}
    a_{0:0}  & \\
             & a_{1:1} & \\
             &         & a_{2:2} \\
             & a_{3:1} & & a_{3:3} \\
             &         & &         & a_{4:4} \\
             & a_{5:1} & & a_{5:3} & & a_{5:5} \\
             &         & &         & &         & a_{6:6} \\
             & a_{7:1} & & a_{7:3} & & a_{7:5} & & a_{7:7} \\
  \end{bmatrix}
  \begin{bmatrix}
    a_{0:0} & \\
    a_{1:0} & a_{1:1} & \\
            &         & a_{2:2} \\
            &         & a_{3:2} & a_{3:3} \\
            &         &         &         & a_{4:4} \\
            &         &         &         & a_{5:4} & a_{5:5} \\
            &         &         &         &         &         & a_{6:6} \\
            &         &         &         &         &         & a_{7:6} & a_{7:7} \\
  \end{bmatrix}
  \endgroup
\end{align*}
\normalsize

Note that Stage 1 and Stage 3 require $O(T)$ work, while Stage 2 reduces to a self-similar problem of half the size.
It is easy to check that this requires $O(T)$ total work and $O(\log T)$ depth/span.

\begin{remark}
  In fact, it is possible to see that the computation graph of this algorithm is identical to that of the associative scan algorithm described in \cref{sec:scan:classical:parallel}.
  The key takeaway is that instead of the steps of (1) recognizing that $M$ defines a recurrence (2) observing that the recurrence can be defined with an associative binary operator;
  there is a completely different perspective of simply finding a structured matrix decomposition algorithm for $M$.
\end{remark}


%
