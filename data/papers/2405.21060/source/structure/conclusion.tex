\section{Conclusion}
\label{sec:conclusion}

We proposed a theoretical framework based on well-studied classes of structured
matrices that bridges the conceptual gap between SSMs and attention variants.
This framework yields insights on how recent SSMs (e.g. Mamba) perform as well
as Transformers on language modeling.
Moreover, our theoretical tools provide new ideas to improve SSMs (and
potentially Transformers) by connecting the algorithmic\iftoggle{arxiv}{ and systems}{} advances on
both sides.
As a demonstration, the framework guides our design of a new architecture
(Mamba-2) at the intersection of SSMs and structured attention.




