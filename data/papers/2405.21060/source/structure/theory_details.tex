\section{Theory Details}

\subsection{Extras: Closure Properties of SSMs}
\label{sec:ssm:properties}

%

%

We present here some additional properties of semiseparable matrices to illustrate their flexibility and utility.
This section is not necessary to understand our core results.

\begin{proposition}[Semiseparable Closure Properties]
  \label{prop:ss-closure}
  Semiseparable matrices are closed under several primitive operations.
  \begin{itemize}
    \item \textbf{Addition}: The sum of an $N$-SS and $P$-SS matrix is at most ($N+P$)-SS.
    \item \textbf{Multiplication}: The product of an $N$-SS and $P$-SS matrix is ($N+P$)-SS.
    \item \textbf{Inverse}: The inverse of an $N$-SS matrix is at most $(N+1)$-SS.
  \end{itemize}
\end{proposition}
The addition and multiplication properties are easily seen.
The inverse property has many proofs; one approach follows immediately from the Woodbury inversion identity, which has also featured prominently in the structured SSM literature~\citep{gu2022efficiently}.
%
%


In turn, these imply closure properties of state space models.

For example, the addition property says that summing two parallel SSM models is still an SSM.
The multiplication property says that sequentially composing or chaining two SSMs can still be viewed as an SSM, whose total state size is additive--a somewhat nontrivial fact.

Finally, the inverse property can let us relate SSMs to other types of models. For example,
one can notice that banded matrices are semiseparable, so their inverses are semiseparable.
(In fact, the semiseparable family of structure is often motivated by taking inverses of banded matrices~\citep{vandebril2005bibliography}).
Moreover, the fast recurrence properties of semiseparable matrices can be viewed as a consequence of their inverse being banded.

\begin{remark}
  The fact that 1-SS matrices are simple recurrences \eqref{eq:1ss-recurrence} are equivalent to the fact that the inverse of a 1-SS matrix is a 2-banded matrix:
\begin{align*}
  M =
  \begin{bmatrix}
    1 & \\
    a_1 & 1 & \\
    a_2a_1 & a_2 & 1 \\
    \vdots & \vdots & \ddots & \ddots \\
    a_{T-1}\dots a_1 & a_{T-1}\dots a_2 & \dots & a_{T-1} & 1 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    1 & \\
    -a_1 & 1 & \\
    0 & -a_2 & 1 \\
    \vdots & \vdots & \ddots & \ddots \\
    0 & 0 & \dots & -a_{T-1} & 1 \\
  \end{bmatrix}^{-1}
\end{align*}
Thus $y = Mx \leftrightarrow M^{-1}y = x$, or
\begin{align*}
  \begin{bmatrix}
    1 & \\
    -a_1 & 1 & \\
    0 & -a_2 & 1 \\
    \vdots & \vdots & \ddots & \ddots \\
    0 & 0 & \dots & -a_{T-1} & 1 \\
  \end{bmatrix}
  y = x
  .
\end{align*}
Or elementwise,
\begin{align*}
  & y_t - a_t y_{t-1} = x_t \\
  & y_t = a_t y_{t-1} + x_t
  .
\end{align*}
\end{remark}
Conversely,
we also use these closure results to prove that autoregressive structured attention (under certain assumptions)
must be SSMs,
allowing us to show that more general families of efficient sequence models including attention variants can be reduced to state space models (\cref{sec:theory-details:ssm-sma}).




\subsection{Autoregressive Masked Attention is Semiseparable-Structured Attention}
\label{sec:theory-details:ssm-sma}

We prove \cref{thm:ss-sma} from \cref{sec:ssd:1ss-sma}.
In \cref{sec:structured-attention} we defined structured attention as a broad generalization of masked attention,
where the property of efficiency (i.e.\ a linear-time form for the kernel attention) is abstracted into the efficiency of structured matrix multiplication.
However, beyond computational efficiency, standard linear attention~\citep{katharopoulos2020transformers} also has two important properties.
First, it is \emph{causal}, which is required for settings such as autoregressive modeling.
Moreover, it has \emph{efficient autoregressive generation}.
In other words, the cost of an autoregressive step -- i.e.\ the incremental cost of computing the output $y_T$ upon seeing $x_T$, given that $x_{0:T}$ has already been seen and preprocessed --
requires only constant time.

Here we characterize which instances of SMA have efficient autoregression.

In the framework of SMA,
causality is equivalent to the constraint that the mask $L$ is a \emph{lower-triangular} matrix.

Characterizing the space of $L$ matrices that have efficient autoregression is more difficult.
We will use a narrow technical definition of autoregressive processes, in the spirit of classical definitions from the time series literature (e.g.\ ARIMA processes~\citep{box2015time}).%
\begin{definition}
We define an autoregressive transformation $x \in \R^T \mapsto y \in \R^T$ of order $k$ as one where each output $y_t$ depends only on the current input and last $k$ outputs:
\begin{equation}
  \label{eq:efficient-ar}
  y_t = \mu_t x_t + \ell_{t1} y_{t-1} + \dots + \ell_{tk} y_{t-k}.
\end{equation}
\end{definition}
Note that the case where $L$ is the cumsum matrix is a special case with $k=1$ and thus $y_t = x_t + y_{t_1}$.
With this definition, characterizing the space of efficient autoregressive linear transforms follows from the properties of semiseparable matrices.
\cref{thm:ss-sma-formal} formalizes and proves \cref{thm:ss-sma}.

\begin{theorem}
  \label{thm:ss-sma-formal}
  Let $L \in \R^{T \times T}$ be an efficient autoregressive transformation of order $k$.
  Then $L$ is a state space model of order $k+1$.
\end{theorem}
\begin{proof}
  Let $(x, y)$ be input and output sequences, so that $y = Lx$.
  Rearranging the definition \eqref{eq:efficient-ar},
  \begin{align*}
    y_t - \ell_{t1} y_{t-1} - \dots - \ell_{tk} y_{t-k} = \mu_t x_t
    .
  \end{align*}
  Vectorizing over $t$, this can be expressed as a matrix transformation
  \begin{align*}%
    \begin{bmatrix}
      1 & \\
      -\ell_{t1} & 1 \\
      \vdots & \ddots & \ddots \\
      -\ell_{tk} & \dots & -\ell_{t1} & 1 \\
      \vdots & \ddots & \vdots & \ddots & \ddots \\
      0 & \dots & -\ell_{T-1,k} & \dots & -\ell_{T-1,1} & 1 \\
    \end{bmatrix}
    \begin{bmatrix}
      y_0 \\
      y_1 \\
      \vdots \\
      y_k \\
      \vdots \\
      y_{T-1} \\
    \end{bmatrix}
    =
    \begin{bmatrix}
      \mu_0 \\
      & \mu_1 \\
      & & \ddots \\
      & & & \mu_k \\
      & & & & \ddots \\
      & & & & & \mu_{T-1} \\
    \end{bmatrix}
    \begin{bmatrix}
      x_0 \\
      x_1 \\
      \vdots \\
      x_k \\
      \vdots \\
      x_{T-1} \\
    \end{bmatrix}
    .
  \end{align*}
  The $\mu$ diagonal matrix can be moved to the left and folded into the matrix of $\ell$ coefficients,
  which remains a $k+1$-band lower-triangular matrix.
  But we also have $L^{-1} y = x$, so $L$ is the inverse of this matrix.

  Next, note that $k+1$-band matrices are $k+1$-semiseparable by the rank characterization of semiseparability (\cref{def:semiseparable-rank}).
  By \cref{prop:ss-closure}, the inverse $L$ is therefore at most $k+2$-semiseparable.
  A slightly stronger bound of $k+1$ can be obtained because of the additional structure of banded matrices.
  Finally, the characterization of $L$ as an order-$k+1$ state space model follows from \cref{thm:ssm-sss}.
\end{proof}


In other words, efficient autoregressive attention is \textbf{semiseparable SMA}.
