\section{Background and Overview}
\label{sec:background}


\subsection{Structured State Space Models}


Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, CNNs,
and classical state space models.
They are inspired by a particular continuous system \eqref{eq:ssm}
that maps a 1-dimensional sequence $x \in \R^\mathtt{T} \mapsto y \in \R^\mathtt{T}$ through an implicit latent state \( h \in \R^{\mathtt{(T, N)}} \). %
\iftoggle{arxiv}{

}{}
A general discrete form of structured SSMs takes the form of equation \eqref{eq:ssm}.
\begin{center}
  \vspace*{-1em}
\begin{minipage}[t]{.45\linewidth}
\begin{subequations}
  \label{eq:ssm}
  \begin{align}
  \label{eq:ssm:1}
    h_{t} &= A h_{t-1} + B x_t \\
  \label{eq:ssm:2}
    y_t &= C^{\top} h_t
  \end{align}
\end{subequations}
\end{minipage}
\qquad
\begin{minipage}[t]{.45\linewidth}
\begin{subequations}
  \label{eq:s6}
  \begin{align}
  \label{eq:s6:1}
    h_{t} &= A_t h_{t-1} + B_t x_t \\
  \label{eq:s6:2}
    y_t &= C_t^{\top} h_t
  \end{align}
\end{subequations}
\end{minipage}
\end{center}

where $A \in \R^{\mathtt{(N,N)}}, B \in \R^{\mathtt{(N,1)}}, C \in \R^{\mathtt{(N,1)}}$.
Structured SSMs are so named because the $A$ matrix controlling the temporal dynamics must be \emph{structured} in order to compute this sequence-to-sequence transformation efficiently enough to be used in deep neural networks.
The original structures introduced were diagonal plus low-rank (DPLR) \citep{gu2022efficiently} and diagonal~\citep{gupta2022diagonal,gu2022parameterization,smith2023s5}, which remains the most popular structure.

In this work, we use the term state space model (SSM) to refer to structured SSMs.
There are many flavors of such SSMs, with deep ties to several major paradigms of neural sequence models such as continuous-time, recurrent, and convolutional models~\citep{gu2021combining}.
\iftoggle{arxiv}{
  We provide a brief overview below, and refer to prior work for more context and details~\citep{gu2023thesis,gu2023mamba}.

%


%

\para{Continuous-time Models.}
The original structured SSMs originated as continuous-time maps on functions $x(t) \in \R \mapsto y(t) \in \R$, rather than operating directly on sequences. %
In the continuous-time perspective, in equation \eqref{eq:ssm:1} the matrices $(A, B)$ are not directly learned but generated from underlying
parameters $(\mathring{A}, \mathring{B})$, along with a parameterized step size $\dt$.
The ``continuous parameters'' $(\dt, \mathring{A}, \mathring{B})$ are converted to ``discrete parameters'' $(A, B)$ through fixed formulas $A = f_A(\dt, \mathring{A})$ and $B = f_B(\dt, \mathring{B})$,
where the pair $(f_A, f_B)$ is called a \emph{discretization rule}.


\begin{remark}
  While our main models adopt the same parameterization and discretization step as prior work (see \citet{gu2023mamba} for details), for simplifying exposition and notation we omit it in the rest of this paper.
  We note that prior work on structured SSMs referred to the continuous parameters $(\mathring{A}, \mathring{B})$ and discrete parameters $(A, B)$ as
  $(A, B)$ and $(\bar{A}, \bar{B})$ instead; we have changed notation to simplify the presentation and focus directly on the discrete parameters, which govern the main SSM recurrence.
\end{remark}

\para{Recurrent Models.}

Equations \eqref{eq:ssm} and \eqref{eq:s6} take the form of a recurrence which is linear in its input $x$.
Structured SSMs can therefore be viewed as types of recurrent neural networks (RNNs), where the linearity endows them with additional properties and
allows them to avoid the sequential computation of traditional RNNs.
Conversely, despite this simplification, SSMs are still fully expressive as sequence transformations (in the sense of universal approximation)~\citep{kaul2020linear,orvieto2023resurrecting,wang2023state}.

\para{Convolutional Models.}
When the SSM's dynamics are constant through time as in equation \eqref{eq:ssm}, the model is called \textbf{linear time-invariant (LTI)}.
In this case, they are equivalent to convolutions.
Thus, SSMs can also be viewed as types of CNNs, but where (i) the convolution kernels are implicitly parameterized through the SSM parameters $(A, B, C)$ and
(ii) the convolution kernels are generally global instead of local.
Conversely, through classical signal processing theory all sufficiently well-behaved convolutions can be represented as SSMs.

Commonly, previous LTI SSMs would use the convolutional mode for efficient parallelizable training (where the whole input sequence is seen ahead of time), %
and switched into recurrent mode \eqref{eq:ssm} for efficient autoregressive inference (where the inputs are seen one step at a time).

}{}

\para{Selective State Space Models.}
The form \eqref{eq:s6} where the parameters $(A, B, C)$ can also vary in time was introduced in Mamba as the \textbf{selective SSM}.
Compared to the standard LTI formulation \eqref{eq:ssm}, this model can selectively choose to focus on or ignore inputs at every timestep.
It was shown to perform much better than LTI SSMs on information-dense data such as language,
especially as its state size $\mathtt{N}$ increases allowing for more information capacity.
However, it can only be computed in recurrent instead of convolutional mode, and requires a careful hardware-aware implementation to be efficient.
Even so, it is still less efficient than hardware-friendly models such as CNNs and Transformers because it does not leverage matrix multiplication units, which modern accelerators such as GPUs and TPUs are specialized for.

While \emph{time-invariant} SSMs are closely related to continuous, recurrent, and convolutional sequence models,
they are not directly related to attention.
In this paper, we show a deeper relationship between \emph{selective} SSMs and attention,
and use it to significantly improve the training speed of SSMs while simultaneously allowing for much larger state sizes $\mathtt{N}$.

%






\para{Structured SSMs as Sequence Transformations.}
%

\begin{definition}
  \label{def:sequence-transformation}
We use the term \textbf{sequence transformation} to refer to a parameterized map on sequences $Y = f_{\theta}(X)$ where $X, Y \in \R^{\mathtt{(T,P)}}$ and $\theta$ is an arbitrary collection of parameters.
$\mathtt{T}$ represents the sequence or \emph{time} axis; subscripts index into the first dimension, e.g.\ $X_t, Y_t \in \R^\mathtt{P}$.
\end{definition}
Sequence transformations (e.g.\ SSMs, or self-attention) are the cornerstone of deep sequence models, where they are incorporated into neural network architectures (e.g.\ Transformers).
The SSM in \eqref{eq:ssm} or \eqref{eq:s6} is a sequence transformation with $\mathtt{P}=1$; it can be generalized to $\mathtt{P} > 1$ by simply broadcasting across this dimension (in other words, viewing the input as $\mathtt{P}$ independent sequences and applying the SSM to each).
One can think of $\mathtt{P}$ as a \textbf{head dimension}\iftoggle{arxiv}{, which we will elaborate on in \cref{sec:architecture}}{}.

\begin{definition}
  \label{def:ssm}
  We define the \textbf{SSM operator}
    $\mathsf{SSM}(A, B, C) = \mathsf{SSM}(A_{0:T}, B_{0:T}, C_{0:T})$ as the sequence transformation $X \in \R^{\mathtt{(T,P)}} \mapsto Y \in \R^{\mathtt{(T,P)}}$
    defined by equation \eqref{eq:s6}.
    %
\end{definition}

In SSMs, the $\mathtt{N}$ dimension is a free parameter called the \textbf{state size} or state dimension.
We also call it the \textbf{state expansion factor}, because it expands the size of the input/output by a factor of $N$, with implications for the computational efficiency of these models.


Finally, we remark that many types of sequence transformations, such as attention, can be represented as a single matrix multiplication across the sequence dimension.
\begin{definition}
  \label{def:matrix-transformation}
  We call a sequence transformation $Y = f_\theta(X)$ a \textbf{matrix transformation} if it can be written in the form $Y = M_\theta X$ where $M$ is a matrix depending on the parameters $\theta$.
  We identify the sequence transformation with the matrix $M$, and often drop the dependence on $\theta$ when clear from context.
\end{definition}


\subsection{Attention}
\label{sec:overview:attention}

Attention broadly refers to a type of computation that assigns scores to every pair of positions in a sequence, allowing each element to ``attend'' to the rest.
By far the most common and important variant of attention is softmax self-attention, which can be defined as
\begin{align*}%
  Y = \operatorname*{softmax}(QK^{\top}) \cdot V
\end{align*}
for $Q, K, V \in \R^{\mathtt{(T,P)}}$.
The mechanism of pairwise comparisons (induced by materializing $QK^{\top}$) leads to the characteristic quadratic training cost of attention.

Many variants of attention have been proposed, but all share the underlying core of these attention scores, with various approximations~\citep{tay2022efficient}.
The most important variant for this work is \textbf{linear attention}~\citep{katharopoulos2020transformers}.
Roughly speaking, this family of methods drops the softmax by folding it into a kernel feature map, and uses associativity of matrix multiplication to rewrite
$(QK^{\top}) \cdot V = Q \cdot (K^{\top} V)$.
Moreover, in the important case of causal (autoregressive) attention,
they show that when the causal mask is incorporated into the left-hand side as
$(L \circ QK^{\top}) \cdot V$, where $L$ is the lower-triangular 1's matrix,
then the right-hand side can be expanded as a recurrence.
Several recent and concurrent works
such as RetNet~\citep{sun2023retentive} and GateLoop~\citep{katsch2023gateloop} strengthen this to more general forms of $L$ (\cref{sec:related}).
In this work, our formulation of structured masked attention will strongly generalize these ideas.

\subsection{Structured Matrices}
\label{sec:overview:structured-matrix}

General matrices $M \in \R^{\mathtt{(T,T)}}$ require $\mathtt{T}^2$ parameters to represent and $O(\mathtt{T}^2)$ time to perform basic operations such as matrix-vector multiplication.
\textbf{Structured matrices} are those that
\begin{enumerate}[label=(\roman*)]
  \item can be represented in subquadratic (ideally linear) parameters through a compressed representation, and
  \item have fast algorithms (most importantly matrix multiplication) by operating directly on this compressed representation.
\end{enumerate}
Perhaps the most canonical families of structured matrices are sparse and low-rank matrices.
However, there exist many other families, such as Toeplitz, Cauchy, Vandermonde, and butterfly matrices, which have all been used in machine learning for efficient models~\citep{thomas2018learning,dao2019learning,gu2022parameterization,fu2024monarch}.
Structured matrices are a powerful abstraction for efficient representations and algorithms.
In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms.


\subsection{Overview: Structured State Space Duality}

While this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically.

\paragraph{Recurrent (Linear) Form.}
The state space dual (SSD) layer can be defined as a special case of the selective SSM \eqref{eq:s6}.
The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length.
Compared to the version used in Mamba, SSD has two minor differences:
\begin{itemize}
  \item The structure on $A$ is further simplified from diagonal to \emph{scalar times identity} structure.
    Each $A_t$ can also be identified with just a scalar in this case.
  \item We use a larger head dimension $\mathtt{P}$, compared to $\mathtt{P}=1$ used in Mamba. Typically $\mathtt{P}=\{64,128\}$ is chosen which is similar to conventions for modern Transformers. %
\end{itemize}
Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements.
In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators.

\paragraph{Dual (Quadratic) Form.}

The dual form of SSD is a quadratic computation closely related to attention, defined as
\begin{align*}%
  (L \circ QK^{\top}) \cdot V \qquad
  L_{ij} = \begin{cases}
    a_i \times \dots \times a_{j+1} & i \ge j \\
    0 & i < j
  \end{cases}
\end{align*}
where $a_i$ are input-dependent scalars bounded in $[0, 1]$.

Compared to standard softmax attention, there are two main differences
\begin{itemize}
  \item The softmax is dropped.
  \item The attention matrix is multiplied elementwise-wise by an additional mask matrix $L$.
\end{itemize}
Both of these changes can be viewed as addressing problems in vanilla attention.
For example, the softmax has been recently observed to cause problems in attention scores, such as the ``attention sink'' phenomenon~\citep{xiao2024efficient,darcet2024vision}.
More importantly, the mask matrix $L$ can be viewed as replacing the heuristic positional embeddings of Transformers with a different \emph{data-dependent positional mask} that controls how much information is transfered across time.

More broadly, this form is an instance of our \textbf{structured masked attention} generalization of linear attention, defined in \cref{sec:attention}.


\paragraph{Matrix Form and SSD Algorithm.}

The various forms of SSD are connected through a unified matrix representation,
by showing that SSMs have a matrix transformation form
$Y = MX$ for a matrix $M_\theta \in \R^{\mathtt{(T,T)}}$ that depends on $\theta = (A, B, C)$.
In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix $M$,
and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in $M$.

Going beyond these, \emph{any} algorithm for multiplication by $M$ can be applied.
Our proposed hardware-efficient SSD algorithm (\cref{sec:efficient}) is a new structured matrix multiplication method that involves block decompositions of $M$, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms.
It is relatively simple and easy-to-implement compared to general selective SSMs \citep{gu2023mamba};
\cref{listing} provides a complete implementation in a few lines of code.

\iftoggle{arxiv}{
\cref{fig:roadmap} provides a simple roadmap of the relationships between the concepts presented in this paper.
}{}


\subsection{Notation}

Throughout this paper, we prefer using precise notation that can be mapped to code.

\para{Matrices and Vectors.}
We generally use lower case to denote vectors (i.e.\ tensors with a single axis)
and upper case to denote matrices (i.e.\ tensors with more than one axes).
We do not bold matrices in this work.
Sometimes, if a matrix is tied or repeated along one axis (and hence can also be viewed as a vector),
we may use either upper or lower case for it.\footnote{In this work, this happens only with the $A$ parameter of SSMs.}
$\cdot$ denotes scalar or matrix multiplication while $\circ$ denotes Hadamard (elementwise) multiplication.

\para{Indexing.}
We use Python-style indexing, e.g. $i:j$ refers to the range $(i, i+1, \dots, j-1)$ when $i < j$ and $(i, i-1, \dots, j+1)$ when $i > j$.
For example, for any symbol $v$ we let $v_{j:i}$ for $j \ge i$ denote the sequence $(v_j, \dots, v_{i+1})$.
$[i]$ is equivalent to $0:i = (0, \dots, i-1)$.
For shorthand, we also let $v_{j:i}^\times$ denote the product $v_j \times \dots \times v_{i+1}$.%
\footnote{In some contexts, it is always clear that the notation $a_{i:j}$ or $A_{i:j}$ means $a_{i:j}^\times$, and the superscript is omitted. }

\para{Dimensions.}
To distinguish from matrices and tensors, we often use capital letters in typewriter fonts (e.g. $\mathtt{D}, \mathtt{N}, \mathtt{T})$ to denote dimensions and tensor shapes.
Instead of the traditional notation $M \in \mathbb{R}^{T \times T}$ we frequently use $M \in \mathbb{R}^{\mathtt{(T,T)}}$ to reflect tensor shapes in code.

%

%

\para{Tensor Contractions.}
%
We will heavily rely on \textbf{tensor contraction} or \textbf{einsum} notation both for clarity and as a central tool in stating and proving our results.
We assume the reader to be familiar with this notation,
which is commonly used in modern tensor libraries such as \texttt{numpy}.
For example, we can use $\mathsf{contract}(\mathtt{MN,NK \to MK})$ to denote the matrix-matrix multiplication operator, and in our notation $\mathsf{contract}(\mathtt{MN,NK \to MK})(X, Y)$ (which is equivalent to $X \cdot Y$) can be translated to code as $\mathtt{numpy.einsum('mn,nk\to mk', X, Y)}$.

\iftoggle{arxiv}{
A large glossary of notation is included in \cref{sec:glossary}.
}{}
