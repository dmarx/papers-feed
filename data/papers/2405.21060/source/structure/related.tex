\section{Related Work and Discussion}
\label{sec:related}


The state space duality framework bridges connections between SSMs, structured matrices, and attention.
We discuss in more depth the relations between SSD and these concepts more broadly.
Using ideas from each of the viewpoints, we also suggest some directions that the SSD framework can be extended in future work.

\subsection{State Space Models}
\label{sec:related:ssm}

Structured state space models can be characterized along the axes
\begin{enumerate}[label=(\roman*)]
  \item whether it is time-invariant or time-varying.
  \item the dimensionality of the system.
  \item the structure on the recurrent transitions $A$.
\end{enumerate}
SSD can be described as a selective SSM with SISO dimensions and scalar-identity structure.

\paragraph{Time Variance (Selectivity).}
The original structured SSMs (S4) were linear time-invariant (LTI) systems~\citep{gu2023thesis,gu2022efficiently} motivated by continuous-time online memorization~\citep{gu2020hippo,gu2021combining,gu2023train}.
Many variants of structured SSMs have been proposed~\citep{gupta2022diagonal,gu2022parameterization,smith2023s5,ma2023mega,dao2023hungry}, including several that drop the recurrence and focus on the convolutional representation of LTI SSMs~\citep{li2023makes,poli2023hyena,fu2023simple,qin2023toeplitz}.

SSD is a time-varying structured SSM, also known as a \textbf{selective SSM} introduced in Mamba~\citep{gu2023mamba}.
Selective SSMs are closely related to gating mechanisms of RNNs, including classical RNNs such as the LSTM~\citep{lstm} and GRU~\citep{chung2014empirical} as well as more modern variants such as the QRNN~\citep{bradbury2016quasi}, SRU~\citep{lei2017simple,lei2021attention}, RWKV~\citep{peng2023rwkv}, HGRN~\citep{qin2023hierarchically}, and Griffin~\citep{de2024griffin,botev2024recurrentgemma}.
These RNNs differ in their parameterizations in various ways, most importantly in the lack of a state expansion.

\paragraph{Dimensionality and State Expansion.}
An important characteristic of SSD, shared by previous SSMs in its lineage (S4, H3, Mamba), is that it is a \textbf{single-input single-output (SISO)} system where input channels are processed independently.
This leads to a much larger effective state size of $\mathtt{ND}$ where $\mathtt{N}$ is the SSM state size (also called state expansion factor) and $\mathtt{D}$ is the standard model dimension.
Traditional RNNs either have $\mathtt{N}=1$ or are
multi-input multi-output (MIMO) with dense $B, C$ matrices,
either of which leads to a smaller state.
While MIMO SSMs have been shown to work well in some domains~\citep{smith2023s5,orvieto2023resurrecting,lu2023structured}, Mamba showed that state expansion is crucial for information-dense domains such as language.
One of the main advantages of SSD is allowing for even larger state expansion factors without slowing down the model.
Many subsequent works have since adopted state expansion (\cref{sec:related:concurrent}).


\paragraph{Structure.}
Compared to previous structured SSMs,
the main restriction of SSD is on the expressivity of the state transitions $A_t$.
We note that more general SSMs, such as the case of diagonal $A_t$, have the same theoretical efficiency as SSD, but are less hardware-friendly.
This is because the dual quadratic form loses its attention-like interpretation and becomes more difficult to compute.
Thus compared to Mamba, SSD differs only in a slightly more restrictive form of diagonal $A_t$,
and trades off this expressivity for improved hardware efficiency (and ease of implementation).

We hypothesize that it may be possible to refine our structured matrix algorithms to improve to the general diagonal SSM case as well.

\subsection{Structured Matrices}
\label{sec:related:matrices}

The first viewpoint of the state space duality adopts the viewpoint of these models as \textbf{matrix sequence transformations} or ``matrix mixers'':
sequence transformations (\cref{def:sequence-transformation}) that can be represented as matrix multiplication (by a $\mathtt{T} \times \mathtt{T}$ matrix) along the sequence dimension $\mathtt{T}$.

Several such matrix mixers have been proposed before, where the primary axis of variation is the representation of the matrix.
These include MLP-Mixer~\citep{tolstikhin2021mlp} (unstructured matrix), FNet~\citep{lee2021fnet} (Fourier Transform matrix), M2~\citep{dao2019learning,dao2020kaleidoscope,dao2022monarch,fu2024monarch} (butterfly/monarch matrix),
Toeplitz matrices~\citep{poli2023hyena,qin2023toeplitz}, and even more exotic structures~\citep{de2018two,thomas2018learning}.

An important characterization is that efficient (sub-quadratic) matrix sequence transformations are exactly those which have \emph{structured matrix mixers}.
A core result of the SSD framework is viewing SSMs as matrix mixers with a particular structure -- semiseparable matrices (\cref{sec:ssm}).
The linear vs. quadratic duality then takes the form of structured matrix multiplication vs. naive matrix multiplication.

The structure matrix representation led to our efficient SSD algorithm through block decompositions of particular semiseparable matrices (\cref{sec:efficient}).
We note that semiseparable matrices are well-studied in the scientific computing literature, and incorporating those ideas may be a promising avenue for more improvements to state space models.
We also suggest that focusing on the matrix mixer viewpoint can lead to more fruitful directions for sequence models,
such as designing principled non-causal variants of Mamba, or finding ways to characterize and bridge the gap between softmax attention and sub-quadratic models through analyzing their matrix transformation structure.

%




\subsection{(Linear) Attention}
\label{sec:related:attention}

Compared to standard (causal) attention, SSD has only two main differences.


First, SSD does not use the softmax activation of standard attention~\citep{bahdanau2015neural,vaswani2017attention}, which is what gives attention its quadratic complexity.
When the softmax is dropped, the sequence can be computed with linear scaling through the linear attention framework~\citep{katharopoulos2020transformers}.

Second, SSD multiplies the logits matrix by an input-dependent 1-semiseparable mask.
Thus this mask can be viewed as replacing the softmax in standard attention.

This semiseparable mask can also be viewed as providing positional information.
The elements $a_t$ act as ``gates'' in the RNN sense, or a ``selection'' mechanism (see discussion in Mamba paper),
and their cumulative products $a_{j:i}$ control how much interaction is allowed between positions $i$ and $j$.
Positional embeddings (e.g.\ sinusoidal~\citep{vaswani2017attention}, AliBi~\citep{press2022train}, and RoPE~\citep{su2021roformer}) are an important component of Transformers that are often viewed as heuristics,
and the 1-SS mask of SSD can be seen as a more principled form of relative positional embeddings.
We note that this view was also posited concurrently by GateLoop~\citep{katsch2023gateloop}.



The second viewpoint of state space duality is a special case of our more general structured masked attention (SMA) framework,
where the duality is revealed as different contraction orderings on a simple 4-way tensor contraction.
SMA is a strong generalization of linear attention that is much more general than SSD as well;
other forms of structured masks may lead to more variants of efficient attention with different properties than SSD.

Beside leading to new models, these connections to attention can lead to other directions for understanding SSMs.
For example, we are curious whether the phenomenon of attention sinks~\citep{darcet2024vision,xiao2024efficient} exist for Mamba models,
and more broadly whether interpretability techniques can be transferred to SSMs~\citep{ali2024hidden}.


Finally, many other variants of linear attention have been proposed~\citep{schlag2021linear,peng2021random,choromanski2021rethinking,qin2022devil,qin2022cosformer,zheng2022linear,zhang2024hedgehog,arora2024zoology,arora2024simple} (see \cref{sec:attention:kernel} for descriptions of several of these),
and we expect that many techniques can be transferred to SSMs\iftoggle{arxiv}{ (e.g. \cref{sec:architecture:kernels})}{}.




We emphasize that SSD \textbf{does not generalize standard softmax attention}, or any other transformation on the attention kernel matrix that does not have a finite feature map $\psi$.
Compared to general attention, SSD's advantage is having a controllable state expansion factor $\mathtt{N}$ that compresses the history, compared to quadratic attention's cache of the entire history scaling with sequence length $\mathtt{T} \gg \mathtt{N}$.
Concurrent work has starting studying the tradeoffs of these representations, for example on copying and in-context learning tasks~\citep{akyurek2024context,jelassi2024repeat,grazzi2024mamba,park2024can}.
We note that Mamba-2 significantly improves on Mamba on some of these capabilities (e.g. as demonstrated by MQAR results in \cref{sec:experiments:mqar}), but more remains to be understood.

\subsection{Related Models}
\label{sec:related:concurrent}

We finally highlight a growing body of recent and concurrent work that have developed sequence models very similar to Mamba and Mamba-2.

\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
  \item RetNet~\citep{sun2023retentive} and TransNormerLLM~\citep{qin2023transnormerllm} generalize Linear Attention using decay terms instead of a cumulative sum,
    and propose dual parallel/recurrent algorithms as well as a hybrid ``chunkwise'' mode.
    These algorithms can be seen as an instantiation of SSD where $A_t$ is time-invariant (constant for all $t$);
    in the SMA interpretation, the mask matrix $L$ would be a decay matrix $L_{i,j} = \gamma^{i-j}$.
    These models also differ architecturally in various ways.
    For example, since they were derived from an attention-centric perspective they preserve the multi-head attention (MHA) pattern; since Mamba-2 was derived from an SSM-centric pattern it preserves the multi-value attention (MVA) or multi-expand SSM (MES) pattern, which we show to be better\iftoggle{arxiv}{ (\cref{sec:experiments:ablations})}{}.

  \item GateLoop~\citep{katsch2023gateloop} concurrently proposed using input-dependent decay factors $A_t$, and developed the same dual quadratic form as in SSD which they call a ``surrogate attention'' form.

  \item Gated Linear Attention (GLA)~\citep{yang2024gated} proposed a variant of linear attention with data-dependent gates, along with efficient algorithms to compute a chunkwise mode and hardware-aware implementations.

  \item HGRN~\citep{qin2023hierarchically} introduced an RNN with input-dependent gates, which was improved to incorporate state expansion in HGRN2~\citep{qin2024hgrn2}.

  \item Griffin~\citep{de2024griffin} and RecurrentGemma~\citep{botev2024recurrentgemma} showed that an RNN with input-dependent gating, combined with local attention, can be very competitive with strong modern Transformers.
    Jamba also showed that combining Mamba with a few layers of attention performs very well on language modeling~\citep{lieber2024jamba}.

  \item xLSTM~\citep{beck2024xlstm} improves the xLSTM by adopting the idea of state expansion and other gating, normalization, and stabilization techniques.

  \item RWKV(-4)~\citep{peng2023rwkv} is an RNN based on a different linear attention approximation (the attention-free Transformer~\citep{zhai2021attention}).
    It has recently been improved to the RWKV-5/6 (Eagle and Finch) architectures~\citep{peng2024eagle} by adopting the ideas of selectivity and state expansion.
\end{itemize}


