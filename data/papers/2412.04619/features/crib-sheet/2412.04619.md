- **Key Concepts:**
  - Language models (LMs) often rely on surface-level heuristics early in training, resembling n-gram models.
  - Transition from surface-level heuristics to hierarchical syntactic representations is termed **structural grokking**.
  
- **Data Characteristics:**
  - **Data Diversity:** Variation in syntactic structure influences model behavior; low diversity leads to memorization, while high diversity promotes generalization.
  - **Data Complexity:** Complexity, measured by center-embedded sentences, induces hierarchical rules; simpler data leads to surface-level rules.

- **Generalization Behavior:**
  - Models exhibit stable in-distribution performance but unstable out-of-distribution (OOD) performance due to competition between generalization rules.
  - Hierarchical generalization is favored by training on complex data with center embeddings.

- **Training Dynamics:**
  - Inconsistent OOD performance across random seeds is linked to the competition between different generalization rules.
  - Intermediate levels of data diversity can lead to greater instability than low or high diversity.

- **Experimental Tasks:**
  - **Question Formation (QF) Task:** 
    - Two strategies: linear rule (first auxiliary verb) vs. hierarchical rule (based on syntax tree).
    - OOD accuracy measures hierarchical generalization.
  - **Tense Inflection (TI) Task:**
    - Similar structure; models must identify subjects for correct verb inflection.
    - OOD accuracy also measures hierarchical generalization.

- **Model Architecture:**
  - Decoder-only Transformer with 8 heads, 512-dimensional embeddings.
  - QF models: 6 layers; TI models: 4 layers.
  - Trained on causal language modeling objective for 300K steps using Adam optimizer.

- **Findings:**
  - Data composition critically shapes OOD generalization behavior.
  - Models stabilize in OOD performance only when committing to a specific rule (either surface-level or hierarchical).
  - Less diverse data can lead to a memorization regime without learning systematic rules.

- **References to Prior Work:**
  - Structural grokking parallels classic grokking, where models transition from memorization to generalization.
  - Previous studies highlight the role of data diversity in achieving OOD compositional generalization.

- **Code Availability:**
  - Code for experiments is available at [GitHub Repository](https://github.com/sunnytqin/concept_comp.git).