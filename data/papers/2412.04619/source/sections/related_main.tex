\section{Background}
Our extended literature review in Appendix \ref{appdx:related} expands on the following background overview.


\subsection{Syntax and Hierarchical Generalization}
\label{sec:syntax_related}
% \paragraph{Syntax and Hierarchical Generalization}
\citet{McCoy2018-uv} first used the question formation task to study hierarchical generalization in neural networks, showing that attention mechanisms improved generalization performance in RNNs. Later, \citet{McCoy2020-pj} found that tree-structured architectures consistently induce hierarchical generalization. \citet{Petty2021-pe} and \citet{Mueller2022-rm} further concluded that  transformers tend to generalize linearly. This view was challenged by \citet{Murty2023-xp}, who attributed the failure of prior attempts to insufficient training, demonstrating that decoder-only transformers can generalize hierarchically, but only after in-distribution performance has plateaued. They named this transition from surface-level heuristics to hierarchical generalization structural grokking. Expanding on their findings, \citet{Ahuja2024-ul} showed that models only generalize hierarchically when trained on a language modeling objective. All of this prior work attributed hierarchical inductive bias to model architecture or objective, whereas our study highlights the impact data. 
While previous work observed some inconsistency across seeds \citep{McCoy2018-uv, McCoyUnknown-uy}, we further characterize the specific distributions produced by this inconsistency. 


\subsection{Training Dynamics and Grokking}
During \textit{grokking}, a neural network suddenly generalizes to a test set long after it has overfitted to its training data. \citet{Power2022-hz} first observed this phenomenon in simple arithmetic tasks. 
This classic grokking is different from our main focus---\textit{structural grokking} \citep{Murty2023-xp}. In classic grokking, the model transitions from memorization to generalization, allowing it to achieve non-trivial performance on unseen data from the same distribution as the train set. In structural grokking, a model transitions from the simple linear rule to the hierarchical rule, leading to non-trivial performance on OOD data. However, our findings also relate to classic grokking through our study of data diversity and memorization. 

\citet{Zhu2024-nz} studied the role of data and finds that grokking only occurs when training set is sufficiently large, and thus more diverse. \citet{Berlot-Attwell2023-qx} studied how data diversity leads to OOD compositional generalization in multimodal models and 
\citet{Lubana2024-ed} showed that diversity also induces compositional behaviors late in LM training.
\citet{Liu2022-mj} showed grokking can be induced by forcing a specific weight norm, a measurement of model---not data---complexity. 
\citet{Huang2024-aw} and \citet{Varma2023-iq} have shown that during training, different circuit compete and data and model size can lead to different competition and training dynamics. Circuit competitions also shape other phase transitions, such as transient in-context learning \citep{Park2024-ri}.
% While these works primarily study training dynamics, our findings highlight that this competition can also lead to inconsistent outcomes when training converges.
% We unify these threads in the existing grokking literature by characterizing the unstable regime in both data diversity and data complexity, connecting training stability with consistency under random seeds.


\subsection{Random Variation} 
Although choices like hyperparameters, architecture, and optimizer all shape model outcomes, training remains inherently stochastic. Models are sensitive to random initialization and the order of training examples \cite{Dodge2020-pb}. Several studies \citep{Zhou2020-xt, D-Amour2022-tl, Naik2018-og} have reported significant performance difference across random seeds. \citet{Zhou2020-xt} further observed that on Natural Language Inference (NLI) tasks, OOD instability is observed throughout training. We investigate the source of these training inconsistencies and link them more precisely to characteristics of the training data. 



