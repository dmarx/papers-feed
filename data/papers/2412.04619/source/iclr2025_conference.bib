@misc{wexler1980formal,
  title={Formal principles of language acquisition},
  author={Wexler, Kenneth},
  year={1980},
  publisher={MIT Press}
}

@article{lightfoot1989child,
  title={The child's trigger experience: Degree-0 learnability},
  author={Lightfoot, David},
  journal={Behavioral and brain sciences},
  volume={12},
  number={2},
  pages={321--334},
  year={1989},
  publisher={Cambridge University Press}
}

@article{shah_pitfalls_2020,
	title = {The Pitfalls of Simplicity Bias in Neural Networks},
	url = {http://arxiv.org/abs/2006.07710},
	abstract = {Several works have proposed Simplicity Bias ({SB})---the tendency of standard training procedures such as Stochastic Gradient Descent ({SGD}) to find simple models---to justify why neural networks generalize well [Arpit et al. 2017, Nakkiran et al. 2019, Soudry et al. 2018]. However, the precise notion of simplicity remains vague. Furthermore, previous settings that use {SB} to theoretically justify why neural networks generalize well do not simultaneously capture the non-robustness of neural networks---a widely observed phenomenon in practice [Goodfellow et al. 2014, Jo and Bengio 2017]. We attempt to reconcile {SB} and the superior standard generalization of neural networks with the non-robustness observed in practice by designing datasets that (a) incorporate a precise notion of simplicity, (b) comprise multiple predictive features with varying levels of simplicity, and (c) capture the non-robustness of neural networks trained on real data. Through theory and empirics on these datasets, we make four observations: (i) {SB} of {SGD} and variants can be extreme: neural networks can exclusively rely on the simplest feature and remain invariant to all predictive complex features. (ii) The extreme aspect of {SB} could explain why seemingly benign distribution shifts and small adversarial perturbations significantly degrade model performance. (iii) Contrary to conventional wisdom, {SB} can also hurt generalization on the same data distribution, as {SB} persists even when the simplest feature has less predictive power than the more complex features. (iv) Common approaches to improve generalization and robustness---ensembles and adversarial training---can fail in mitigating {SB} and its pitfalls. Given the role of {SB} in training neural networks, we hope that the proposed datasets and methods serve as an effective testbed to evaluate novel algorithmic approaches aimed at avoiding the pitfalls of {SB}.},
	journaltitle = {{arXiv}:2006.07710 [cs, stat]},
	author = {Shah, Harshay and Tamuly, Kaustav and Raghunathan, Aditi and Jain, Prateek and Netrapalli, Praneeth},
	urldate = {2022-03-07},
	date = {2020-10-28},
	eprinttype = {arxiv},
	eprint = {2006.07710},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/naomisaphra/Zotero/storage/XJNKI2SQ/2006.html:text/html;Shah et al_2020_The Pitfalls of Simplicity Bias in Neural Networks.pdf:/Users/naomisaphra/Zotero/storage/4B24CZS2/Shah et al_2020_The Pitfalls of Simplicity Bias in Neural Networks.pdf:application/pdf},
}
