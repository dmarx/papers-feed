% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{6709} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\dataset: Learning How Things Move in 3D from Internet Stereo Videos}

%%%%%%%%% AUTHORS - PLEASE UPDATE

\author{
Linyi Jin$^{1,2}$\qquad
Richard Tucker$^1$\qquad
Zhengqi Li$^1$\qquad
David Fouhey$^3$\\
Noah Snavely$^{1*}$\qquad
Aleksander Holynski$^{1*}$
\\[0.5em]
$^1$Google DeepMind \ \ \
$^2$University of Michigan \ \ \ 
$^3$New York University \ \ \ 
$^*$equal contribution\\ \\
}

\begin{document}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\vspace{-2em}
 \includegraphics[width=\textwidth]{fig/teaser_v5.pdf}
\captionof{figure}{There is currently no scalable source of data for real-world, ground truth 3D motion paired with video. 
We present a framework for mining such data from existing stereoscopic videos on the Internet, in the form of 3D point clouds with long-range world-space trajectories. Our framework fuses and filters camera poses, dense depth maps, and 2D motion trajectories to produce high-quality, pseudo-metric point clouds with long-term 3D motion trajectories, pictured above, for hundreds of thousands of video clips. We show how this data is useful in learning a model that reasons about both 3D shape and motion in imagery.\\ }
\label{fig:teaser}
}]
\input{sec/0_abstract}    
\input{sec/1_intro}
\input{sec/2_related}
\input{sec/3_data}
\input{sec/4_method}
\input{sec/5_experiment}
\input{sec/6_conclusion}

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
\input{sec/X_suppl}

\end{document}
