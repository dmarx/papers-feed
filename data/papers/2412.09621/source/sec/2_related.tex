\section{Related work}
\label{sec:related}




\noindent \textbf{2D and 3D motion data.} 
There has been tremendous progress on the task of motion estimation from images and videos, and in particular for 2D image-space correspondence estimation. 
Most state-of-the-art methods use neural networks trained on ground truth data to predict these correspondences directly from images. While these approaches require large training datasets, synthetic data from graphics engines~\cite{dosovitskiy2015flownet, mayer2016large, harley2022particle, butler2012naturalistic, sun2021autoflow, greff2021kubric, zheng2023point}  has proven surprisingly effective at generalizing to real-world data, likely because the core task, low-level textural correspondence, is similar between the two domains.

However, the same cannot be said for 3D motion estimation, since predicting both 3D structure and motion is usually more ambiguous and can require specific prior knowledge about the real world and how it moves. To help address this domain gap, a number of real-world datasets have been proposed. The KITTI~\cite{geiger2013vision} and Waymo~\cite{sun2020scalability} datasets include real-world autonomous driving sequences with stereo and motion annotations derived from LiDAR and odometry information, but only focus on the relatively closed domain of street scenes, whereas our data depicts more diverse in-the-wild scenarios. 
A number of annotated smaller-scale datasets, such as TAPVid~\cite{doersch2022tap}, TAPVid3D~\cite{koppula2024tapvid3d}, and Dycheck~\cite{gao2022monocular}, have been proposed, primarily serving as evaluation datasets for benchmarking depth estimation, 3D reconstruction, and 3D motion estimation approaches.
WSVD~\cite{wang2019web} and NVDS~\cite{NVDS} are stereo video datasets that include 
disparity maps derived from optical flow. While their source content is similar, our method provides richer 3D annotations beyond time-independent disparity maps, such as 3D camera parameters and long-term 3D motion tracks.




\bfpar{Static and dynamic scene reconstruction}
The problem of reconstructing a static 3D scene has been studied for decades. Traditional 3D reconstruction methods tackle this problem by first estimating camera parameters via Structure-from-Motion~(SfM)~\cite{snavely2006photo, pollefeys2008detailed, pollefeys2004visual,pollefeys2008detailed, agarwal2011building, schonberger2016structure, sweeney2019structure, holynski2020reducing} or SLAM~\cite{engel2017direct, campos2021orb, mur2015orb, davison2007monoslam}. 
Dense scene geometry can then be estimated through Multi-view Stereo (MVS)~\cite{campbell2008using, jancosek2011multi, furukawa2010towards, furukawa2009accurate, galliani2015massively, schonberger2016pixelwise, yao2018mvsnet, yao2019recurrent} followed by surface reconstruction algorithms~\cite{hoppe1992surface, curless1996volumetric, kazhdan2006poisson}. More recently, deep neural network-based approaches have shown promising results in improving camera localization accuracy or scene reconstruction through intermediate representations such as depth maps~\cite{bloesch2018codeslam, tang2018ba, teed2024deep, teed2021droid, shen2023dytanvo, li2024megasam}, radiance fields~\cite{lin2021barf, Fu_2024_CVPR, park2023camp, gao2024cat3d, shih2024extranerf, weber2024nerfiller}, or 3D scene coordinates~\cite{brachmann2023ace, brachmann2024acezero, leroy2024grounding, wang2024dust3r, zhang2024monst3r}. However, these methods assume the input images to be observations of a static environment, and therefore produce inaccurate geometry and camera poses for dynamic scenes.

Reconstructing dynamic scenes is more challenging since scene and object motions violate the multi-view constraints used to reconstruct static scenes. As a result, many prior works require RGBD input~\cite{bozic2020deepdeform,  newcombe2015dynamicfusion} or only recover sparse geometry~\cite{park20103d, vo2016spatiotemporal, simon2016kronecker}. Several recent works tackle this problem from monocular input through video depth maps~\cite{zhang2021consistent, kopf2021rcvd, zhang2022structure}, time-varying radiance fields~\cite{park2021nerfies,park2021hypernerf,li2021neural, liu2023robust, li2023dynibar, gao2022monocular, lei2024mosca, wang2024shape}, or generative priors~\cite{wu2024cat4d}.

\bfpar{Monocular and stereo depth.}
Recent works on single-view depth prediction have shown strong zero-shot generalization to in-the-wild domains by training deep neural networks on diverse RGBD datasets~\cite{li2018megadepth, li2019learning, ranftl2021vision, yin2021learning, ranftl2020towards, ke2024repurposing, depthanything, yang2024depth, yin2023metric3d, piccinelli2024unidepth}. However, producing \textit{temporally consistent} and \textit{metric} depth from video is still challenging. To tackle this, recent works use test-time optimization~\cite{luo2020consistent, zhang2022structure} or end-to-end learning with temporal attention~\cite{kopf2021rcvd, hu2024depthcrafter, shao2024learning,NVDS}.
On the other hand, stereo images or videos are also popular input modalities for obtaining reliable metric depth maps, and various stereo matching algorithms have been proposed~\cite{birchfield1999depth, hirschmuller2002real, van2002hierarchical, klaus2006segment, sun2003stereo, pang2017cascade, chang2018pyramid, kendall2017end, zhang2019ga, li2023temporally, zhang2023temporalstereo, karaev2023dynamicstereo, jing2024matchstereovideos, wang2025sea}.
Building on these advancements, our method bridges ideas from monocular video depth estimation and stereo video processing. We use a light-weight optimization step and extend them to stereo inputs for more consistent motion estimation in metric space.




	
