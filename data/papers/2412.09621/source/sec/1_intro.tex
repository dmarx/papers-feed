\section{Introduction}
\label{sec:intro}

Simultaneously predicting and understanding geometry and motion---that is, dynamic 3D content---from images is a fundamental building block for computer vision, with applications ranging from robotic interaction and scene reconstruction to novel view synthesis of dynamic scenes. 
While recent work has made remarkable progress in predicting static 3D structure from images~\citep{wang2024dust3r,yang2024depth,bochkovskii2024depth}, modeling real-world 3D motion---people gesturing, balls bouncing, leaves rustling in the wind---remains a critical unsolved challenge for building truly general models of the visual world.

Recent breakthroughs in AI, from 
large language models~\cite{achiam2023gpt,team2023gemini} to 
image generation~\cite{polyak2024movie} and 
static 3D reconstruction~\cite{wang2024dust3r,bochkovskii2024depth,yang2024depth}, 
demonstrate a consistent pattern: large amounts of high-quality, realistic training data and scalable architectures enable dramatic performance improvements. 
In the realm of 3D reasoning, prior works~\cite{li2018megadepth, depthanything, ranftl2020towards, ranftl2021vision, wang2024dust3r} have shown the value of large-scale training data for strong zero-shot generalization within single-view or two-view static scene settings.
But applying this same formula to \emph{dynamic} 3D scenes (i.e. moving 3D structure) requires a corresponding large-scale dataset consisting of diverse visual content paired with corresponding ground-truth 3D motion trajectories. 
Obtaining such data presents unique challenges. While there are synthetic datasets~\cite{zheng2023point,butler2012naturalistic,dosovitskiy2015flownet,greff2021kubric}, these often fail to capture the distribution of real-world content and the nuanced patterns of real-world motion. 
Traditional approaches to gathering real motion data, such as motion capture systems or multi-view camera arrays~\cite{Joo_2015_ICCV,Grauman_2024_CVPR,kirschstein2023nersemble,isik2023humanrf}
are accurate, but difficult to scale and limited in the diversity of scenes they can capture. 

We identify online stereoscopic fisheye videos (often referred to as VR180 videos) as an untapped source of such data. 
These videos, designed to capture immersive VR experiences, provide wide field-of-view stereo imagery with a standardized stereo baseline. We present a pipeline that carefully combines state-of-the-art methods for stereo depth estimation and video tracking along with
structure-from-motion methods optimized for dynamic scenes. By combining our system with careful filtering and quality control, we show that we can extract over 100K video sequences, each containing high-quality 3D point clouds with per-point long-term trajectories (see \Fig{teaser}), as well as all other intermediate quantities: depth maps, camera poses, images, and 2D correspondences. 
We additionally show the utility of the dataset by training \emph{\method}, an extension to \duster that can predict high-quality 3D structure {\it and} motion from challenging image pairs. 

Our contributions include: (1) a framework for obtaining real-world, dynamic, and pseudo-metric 4D reconstructions and camera poses at scale from existing online video; (2) \method, a method that takes a pair of frames from any real-world video, and predicts a pair of 3D point clouds and the corresponding 3D motion trajectories that connect them in time. 
