- **Main Thesis**: Adversarial perturbations do not reliably protect artists from generative AI style mimicry; existing tools provide a false sense of security.
  
- **Key Protection Tools Evaluated**:
  - **Glaze**: Perturbs images to disrupt style mimicry by targeting the encoder in latent diffusion models.
  - **Mist**: Similar to Glaze, also targets the encoder.
  - **Anti-DreamBooth**: Targets the denoiser to maximize prediction error on perturbed images.

- **Robust Mimicry Methods**:
  - **Gaussian Noising**: Simple preprocessing step that adds Gaussian noise to protected images.
  - **DiffPure**: Uses image-to-image models to remove perturbations; requires two models (purifier and mimicry model).
  - **Noisy Upscaling**: Combines Gaussian noise with upscaling to effectively reduce protections without introducing perceptible artifacts.
  - **IMPRESS++**: A white-box method that builds on IMPRESS with a different loss function and negative prompting.

- **User Study Findings**: All existing protections can be easily bypassed, producing results indistinguishable from unprotected artworks.

- **Limitations of Current Protections**:
  - **Brittleness**: Protections do not generalize across different finetuning setups, making them vulnerable to simple circumvention techniques.
  - **Sub-optimal Evaluations**: Previous evaluations of protections used outdated models and did not account for the capabilities of resourceful forgers.
  - **Non-comprehensive Comparisons**: Different evaluation setups hinder direct comparison of protection effectiveness.

- **Threat Model**: 
  - **Forger's Goal**: To produce images that mimic the style of a targeted artist using their protected artworks.
  - **Assumptions**: All artist images available online are protected, and the forger has full control over the finetuning process.

- **Evaluation Protocol**: A unified evaluation protocol was introduced to assess the robustness of protections against various robust mimicry methods.

- **Conclusion**: Adversarial machine learning techniques are insufficient for protecting artists from generative style mimicry; alternative protective measures are urgently needed. 

- **Code Repository**: [GitHub - robust-style-mimicry](https://github.com/ethz-spylab/robust-style-mimicry) for access to code and images used in the study.