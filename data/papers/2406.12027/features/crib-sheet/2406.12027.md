- **Main Thesis**: Adversarial perturbations in style mimicry protections do not reliably safeguard artists from generative AI misuse.
  
- **Key Protections Evaluated**:
  - **Glaze**: Perturbs images to disrupt style mimicry by targeting the encoder in latent diffusion models.
  - **Mist**: Similar to Glaze, also targets the encoder.
  - **Anti-DreamBooth**: Targets the denoiser to maximize prediction error on perturbed images.

- **Robust Mimicry Methods**: 
  - Low-effort techniques (e.g., Gaussian noise addition, different finetuning scripts) can effectively bypass existing protections.
  - Multi-step strategies combining off-the-shelf tools can produce indistinguishable results from unprotected artworks.

- **User Study Findings**: All tested protections were easily bypassed, demonstrating that they provide a false sense of security for artists.

- **Limitations of Current Protections**:
  - Protections do not generalize across different finetuning setups.
  - Existing methods are sub-optimal and fail to account for resourceful forgers.
  - Evaluations of protections are inconsistent and often rely on unreliable automated metrics.

- **Unified Evaluation Protocol**: 
  - Introduced to assess the effectiveness of protections against a variety of robust mimicry methods.
  - Emphasizes the use of a common set of artists and prompts for comprehensive evaluation.

- **Proposed Robust Mimicry Methods**:
  1. **Noisy Upscaling**: Combines Gaussian noise addition with upscaling to remove protections.
  2. **Off-the-shelf Finetuning**: Utilizes popular scripts independent of protection methods.
  3. **Purification Techniques**: Methods like DiffPure can effectively remove perturbations.
  4. **White-box Method**: Inspired by IMPRESS, designed to circumvent protections.

- **Conclusion**: Current adversarial machine learning techniques are insufficient for protecting artists; alternative protective measures are urgently needed. 

- **Code Repository**: [GitHub - robust-style-mimicry](https://github.com/ethz-spylab/robust-style-mimicry) for access to code and images used in the study.