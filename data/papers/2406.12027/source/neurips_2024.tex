\documentclass{article}


    \PassOptionsToPackage{authoryear}{natbib}




    \usepackage[preprint]{neurips_2024}





\usepackage[ruled,vlined]{algorithm2e}
\usepackage{setspace}
\usepackage[utf8]{inputenc} %
\usepackage[hidelinks]{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{amsmath}
\usepackage{mathtools} %
\usepackage{bm}      %
\usepackage{cleveref}
\usepackage{graphicx} %
\usepackage{color}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{makecell}
\usepackage{import}
\usepackage{layouts}
\usepackage{tikz}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{pdfpages}
\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{dsfont}

\captionsetup[table]{skip=10pt}

\definecolor{mycitecolor}{HTML}{395B9E}
\hypersetup{
    colorlinks,
    linkcolor={mycitecolor},
    citecolor={mycitecolor},
    urlcolor={blue!80!black}
}

\usepackage{amssymb}
\newcommand*{\circledbullet}{%
    \mathbin{%
        \ooalign{$\circledcirc$\cr\hidewidth$\bullet$\hidewidth}%
    }%
}

\usetikzlibrary{shapes, patterns}

\definecolor{lightred}{rgb}{0.98, 0.3, 0.3}



\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}



\input{mathcommands}
\newtheorem{definition}{Definition} 

\newcommand{\style}[1]{\mathcal{#1}}
\newcommand{\advstyle}[1]{\style{#1}_{\textrm{adv}}}
\newcommand{\adv}[1]{#1_{\textrm{adv}}}
\newcommand{\model}[1]{\mathrm{#1}}
\newcommand{\advmodel}[1]{\mathrm{#1}_{\textrm{adv}}}
\newcommand{\dataset}[1]{#1}
\newcommand{\token}[1]{#1}
\newcommand{\word}[1]{#1_*}
\newcommand{\loss}[0]{\mathcal{L}}
\newcommand{\enc}[0]{\mathcal{E}_{\vphi}}
\newcommand{\dec}[0]{\mathcal{D}_{\vphi'}}
\newcommand{\denoiser}[0]{\eps_{\vtheta}}
\newcommand{\cfgdenoiser}[0]{\tilde{\eps}_{\vtheta}}
\newcommand{\advdenoiser}[0]{\eps_{\adv{\vtheta}}}
\newcommand{\type}[1]{\mathrm{#1}}
\newcommand{\advimggen}[0]{I'_{\textrm{adv}}}
\newcommand{\advimg}[0]{x_{\textrm{adv}}}
\newcommand{\advimgset}[0]{X_{\textrm{prot}}}
\newcommand{\mistenc}[0]{$\textrm{Mist}_{\phi}$}
\newcommand{\mistden}[0]{$\textrm{Mist}_{\eps}$}
\newcommand{\mist}[0]{$\textrm{Mist}$}
\newcommand{\attacker}[0]{$A$}
\newcommand{\imgsimilarity}[0]{d_{\mathrm{Img}}}
\newcommand{\quality}[0]{d_{\mathrm{Qual}}}
\newcommand{\latentdist}[0]{d_{\mathrm{Lat}}}
\newcommand{\sDelta}{\Delta\!\!\!\!\Delta}
\newcommand{\advperturb}[0]{\vdelta_{\textrm{adv}}}
\newcommand{\negprompt}[0]{P_{\textrm{neg}}}


\newcommand{\leftout}[1]{\color{gray}{#1}\color{black}}
\newcommand{\defense}[1]{\mathcal{D}_{\mathrm{#1}}}
\newcommand{\attack}[0]{\mathcal{P}}
\newcommand{\orig}[0]{\mathcal{O}}
\newcommand{\method}[0]{\mathcal{S}}

\newcommand{\jr}[1]{\textcolor{red}{[Javi says: #1]}}
\newcommand{\ft}[1]{\textcolor{blue}{[Florian says: #1]}}
\newcommand{\robh}[1]{\textcolor{purple}{[Robert says: #1]}}

\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}

\newcommand{\mytriangle}{
  \protect\begin{tikzpicture}[baseline=-0.6ex]
    \protect\node[draw, regular polygon, regular polygon sides=3, minimum size=0.15cm, scale=0.2, fill=blue] {};
  \end{tikzpicture}
}

\newcommand{\mycircle}{
  \protect\begin{tikzpicture}[baseline=-0.6ex]
    \protect\node[circle, draw, minimum size=0.15cm, scale=0.30, fill=green] {};
  \end{tikzpicture}
}

\newcommand{\mydiamond}{
  \protect\begin{tikzpicture}[baseline=-0.6ex]
    \protect\node[diamond, draw, minimum size=0.15cm, scale=0.30, fill=yellow] {};
  \end{tikzpicture}
}


\newcommand{\mystar}{
  \protect\begin{tikzpicture}[baseline=-0.6ex]
    \protect\node[draw, star, star points=5, star point ratio=2.25, minimum size=0.15cm, scale=0.15, fill=red] {};
  \end{tikzpicture}
}

\newcommand{\hexagon}{
  \protect\begin{tikzpicture}[baseline=-0.6ex]
    \protect\node[rectangle, draw, minimum size=0.15cm, scale=0.3, draw, fill=purple] {};
  \end{tikzpicture}
}

\newcommand{\sixstar}{
  \protect\begin{tikzpicture}[baseline=-0.6ex]
    \protect\node[star, star points=6, star point ratio=2.25, draw, minimum size=0.15cm, scale=0.15, fill=orange] {};
  \end{tikzpicture}
}

\newcommand{\dottedline}[1][0.7em]{%
  \tikz[baseline=0ex]\draw[dotted, line width=1pt] (0,0) -- (0,#1);%
}

\newcommand{\solidline}[1][0.7em]{%
  \tikz[baseline=0ex]\draw[solid,line width=1pt] (0,0) -- (0,#1);%
}

\newcommand{\repo}{\url{https://github.com/ethz-spylab/robust-style-mimicry}}

\crefalias{objective}{equation}
\crefname{objective}{objective}{objectives}
\Crefname{objective}{Objective}{Objectives}
\creflabelformat{objective}{#2\textup{(#1)}#3}

\crefalias{routine}{equation}
\crefname{routine}{routine}{routine}
\Crefname{routine}{Routine}{Routines}
\creflabelformat{routine}{#2\textup{(#1)}#3}

\title{Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI
}




\author{%
  Robert H\"onig \\
  ETH Zurich\\
  \And
  Javier Rando \\
  ETH Zurich \\
  \And
  Nicholas Carlini \\
  Google DeepMind \\
  \And
  Florian Tram\`er \\
  ETH Zurich \\
  \And
\texttt{\{robert.hoenig, javier.rando, florian.tramer\}@inf.ethz.ch}
}


\begin{document}






\maketitle


\begin{abstract}
Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles.
In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online.
In this work, we evaluate the effectiveness of popular protections---with millions of downloads---and show they only provide a false sense of security.
We find that low-effort and ``off-the-shelf'' techniques, such as image upscaling, are sufficient to create {robust mimicry methods} that significantly degrade existing protections.
Through a user study, we demonstrate that \emph{all existing protections can be easily bypassed}, leaving artists vulnerable to style mimicry. 
We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions.

\end{abstract}

\section{Introduction}

\emph{Style mimicry} is a popular application of text-to-image generative models. Given a few images from an artist, a model can be finetuned to generate new images in that style (e.g., a spaceship in the style of Van Gogh).
But style mimicry has the potential to cause significant harm if misused. In particular, many contemporary artists worry that others could now produce images that copy their unique art style, and potentially steal away customers~\citep{techreview}.
As a response, several protections have been developed to protect artists from style mimicry~\citep{glaze, antidreambooth, mist}.
These protections add adversarial perturbations to images that artists publish online, in order to inhibit the finetuning process.
These protections have received significant attention from the media---with features in the New York Times~\citep{hill2023tool}, CNN~\citep{cnn} and Scientific American~\citep{scientificamerican}---and have been downloaded over 1M times~\citep{glaze}.

Yet, it is unclear to what extent these tools actually protect artists against style mimicry, especially if someone actively attempts to circumvent them~\citep{radiya2021data}.
In this work, we show that state-of-the-art style protection tools---\emph{Glaze}~\citep{glaze}, \emph{Mist}~\citep{mist} and \emph{Anti-DreamBooth}~\citep{antidreambooth}---are ineffective when faced with simple \emph{robust mimicry methods}. The robust mimicry methods we consider range from low-effort strategies---such as using a different finetuning script, or adding Gaussian noise to the images before training---to multi-step strategies that combine off-the-shelf tools.
We validate our results with a user study, which reveals
that robust mimicry methods can produce results indistinguishable in quality from those obtained from unprotected artworks (see Figure \ref{fig:figure1} for an illustrative example).\blfootnote{Code and images released at: \repo.}

We show that existing protection tools merely provide a false sense of security. 
Our robust mimicry methods do not require the development of new tools or fine-tuneing methods, but only carefully combining standard image processing techniques
\emph{which already existed at the time that these protection tools were first introduced!}. Therefore, we believe that even low-skilled forgers could have easily circumvented these tools since their inception.

Although we evaluate specific protection tools that exist today, the limitations of style mimicry protections are inherent. Artists are necessarily at a disadvantage since they have to act first (i.e., once someone downloads protected art, the protection can no longer be changed). 
To be effective, protective tools face the challenging task of creating perturbations that transfer to \emph{any} finetuning technique, even ones chosen adaptively in the future. A similar conclusion was drawn by Radiya-Dixit \emph{et al.}~\citep{radiya2021data}, who argued that adversarial perturbations cannot protect users from facial recognition systems.
We thus caution that \emph{adversarial machine learning techniques will not be able to reliably protect artists from generative style mimicry}, and urge the development of alternative measures to protect artists.

We disclosed our results to the affected protection tools prior to publication, so that they could determine the best course of action for existing users.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{plots/Figure_1.pdf}
    \caption{Artists are vulnerable to style mimicry from  generative models finetuned on their art. Existing protection tools add small perturbations to published artwork to prevent mimicry \citep{glaze,mist,antidreambooth}. However, these protections fail against \emph{robust mimicry methods}, giving a false sense of security and leaving artists vulnerable.  Artwork by \emph{@nulevoy} (Stas Voloshin), reproduced with permission.}
    \label{fig:figure1}
\end{figure}





\section{Background and Related Work}

\textbf{Text-to-image diffusion models.~} A latent diffusion model consists
of an image autoencoder and a denoiser. The autoencoder is trained to encode and decode images using a lower-dimensional latent space. The denoiser predicts the noise added to latent representations of images
in a diffusion process \citep{diffusion}.
Latent diffusion models can generate images from text prompts by conditioning the denoiser on image captions \citep{stablediffusion}. Popular text-to-image diffusion models include open models such as Stable Diffusion \citep{stablediffusion} and Kandinsky \citep{kandinsky}, as well as closed models like Imagen \citep{imagen} and DALL-E \citep{dalle2, dalle3}.

\textbf{Style mimicry.~} Style mimicry uses generative models to create images matching a target artistic style.
Existing techniques vary in complexity and quality (see \Cref{sec:stylemimicryappendix}). An effective method is to finetune a diffusion model using a few images in the targeted style. Some artists worry that style mimicry can be misused to reproduce their work without permission and steal away customers~\citep{techreview}.

\textbf{Style mimicry protections.~} Several tools have been proposed to prevent unauthorized style mimicry. These tools allow artists to include small perturbations---optimized to disrupt style mimicry techniques---in their images before publishing. The most popular protections are Glaze \citep{glaze} and Mist \citep{mist}. Additionally, Anti-DreamBooth \citep{antidreambooth} was introduced to prevent fake personalized images, but we also find it effective for style mimicry. Both Glaze and Mist target the encoder in latent diffusion models; they perturb images to obtain latent representations that decode to images in a different style  (see \Cref{sec:mistenc}). On the other hand, Anti-DreamBooth targets the denoiser and maximizes the prediction error on the latent representations of the perturbed images (see \Cref{sec:antidb}).


\textbf{Circumventing style mimicry protections.~} %
Although not initially designed for this purpose, adversarial purification \citep{advpur1, advpur2, advpur3} could be used to remove the perturbations introduced by style mimicry protections. DiffPure \citep{diffpure} is the strongest purification method and Mist claims robustness against it. %
Another existing method for purification is upscaling \citep{advupscale}. Similarly, Mist and Glaze claim robustness against upscaling. Section \ref{sec:prot-limitations} highlights flaws in previous evaluations and how a careful application of both methods can effectively remove mimicry protections.


IMPRESS \citep{impress} was the first purification method designed specifically to circumvent style mimicry protections. 
While IMPRESS claims to circumvent Glaze, the authors of Glaze critique the method's evaluation~\citep{glazeresponsetoimpress}, namely the reliance on automated metrics instead of a user study, as well as the method's poor performance on contemporary artists. Our work addresses these limitations by considering simpler and stronger
purification methods, and evaluating them rigorously with a user study and across a variety of historical and contemporary artists.
Our results show that the main idea of IMPRESS is sound, and that very similar robust mimicry methods are effective.


\textbf{Unlearnable examples~.}
Style mimicry protections build upon a line of work that aims to 
make data ``unlearnable'' by machine learning models~\citep{shan2020fawkes, huang2021unlearnable, cherepanova2021lowkey, salman2023raising}.
These methods typically rely on some form of adversarial optimization, inspired by adversarial examples~\citep{szegedy2013intriguing}.
Ultimately, these techniques always fall short of an \emph{adaptive} adversary that enjoys a second-mover advantage: once unlearnable examples have been collected, their protection can no longer be changed, and the adversary can thereafter select a learning method tailored towards breaking the protections~\citep{radiya2021data, fowl2021adversarial, tao2021better}. 



\section{Threat Model} 
\label{sec:attackscenario}

The goal of style mimicry is to produce images, of some chosen content, that mimic the style of a targeted artist.
Since artistic style is challenging to formalize or quantify, we refrain from doing so and define a mimicry attempt as successful if it generates new images that a human observer would qualify as 
possessing the artist's style.

We assume two parties, the \emph{artist} who places art online (e.g., in their portfolio), and a \emph{forger} who performs style mimicry using these images.
The challenge for the forger is that the artist first \emph{protects} their original art collection before releasing it online, using a state-of-the-art protection tool such as Glaze, Mist or Anti-DreamBooth.
We make the conservative assumption that \emph{all} the artist's images available online are protected. If a mimicry method succeeds in this setting, we call it \emph{robust}.

In this work, we consider style forgers who finetune a text-to-image model on an artist's images---the most successful style mimicry method to date \citep{glaze}.
Specifically, the forger finetunes a pretrained model $f$ on protected images $X$ from the artist to obtain a finetuned model $\hat{f}$.
The forger has full control over the protected images and finetuning process, and can arbitrarily modify to maximize the mimicry success. %
Our \emph{robust mimicry methods} combine a number of ``off-the-shelf'' manipulations that allow even low-skilled parties to bypass existing style mimicry protections.
In fact, our most successful methods require only black-box access to a finetuning API for the model $f$, and could thus also be applied to proprietary text-to-image models that expose such an interface. 


\section{Robust Style Mimicry}
We say that a style mimicry method is \emph{robust} if it can emulate an artist's style using only \emph{protected} artwork. 
While methods for robust mimicry have already been proposed, we note a number of limitations in these methods and their evaluation in Section \ref{sec:prot-limitations}.
We then propose our own methods (Section \ref{sec:our-methods}) and evaluation (Section \ref{sec:experimentalsetup}) which address these limitations.

\subsection{Limitations of Prior Robust Mimicry Methods and of Their Evaluations}
\label{sec:prot-limitations}

\paragraph{(1) Some mimicry protections do not generalize across finetuning setups.}\label{sec:glazebrittle} 
Most forgers are inherently ill-intentioned since they ignore artists' genuine requests \emph{not} to use their art for generative AI~\citep{heikkila2022artist}.
A successful protection must thus resist circumvention attempts from a reasonably resourced forger who may try out a variety of tools.
Yet, in preliminary experiments, we found that Glaze~\citep{glaze} performed significantly worse than claimed in the original evaluation, even before actively attempting to circumvent it.
After discussion with the authors of Glaze, we
found small differences between our off-the-shelf finetuning script, and the one used in Glaze's original evaluation (which the authors shared with us).\footnote{The two finetuning scripts mainly differ in the choice of library, model, and hyperparameters. We use a standard HuggingFace script and Stable Diffusion 2.1 (the model evaluated in the Glaze paper).}
These minor differences in finetuning are sufficient to significantly degrade Glaze's protections (see \Cref{fig:glazebad} for qualitative examples).
Since our off-the-shelf finetuning script was not designed to bypass style mimicry protections, these results already hint 
at the superficial and brittle protections that existing tools provide: artists have no control over the finetuning script or hyperparameters a forger would use, so protections must be robust across these choices.

\begin{figure}[]
\centering
\begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/original/0009.jpg}
         \caption{Original artwork}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.39\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/theirs.jpg}
         \caption{Finetuning used in \citep{glaze}.}
     \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.39\textwidth}
        \centering
         \includegraphics[width=\textwidth]{plots/ours.jpg}
         \caption{Our finetuning}
     \end{subfigure}
     
\caption{The protections of Glaze~\citep{glaze} do not generalize across fine-tuning setups. We mimic the style of the contemporary artist @nulevoy from Glaze-protected images by using: (b) the finetuning script provided by Glaze authors; and (c) an alternative \emph{off-the-shelf} finetuning script from HuggingFace.
In both cases, we perform ``naive'' style mimicry with no effort to bypass Glaze's protections.
Glaze protections are successful using finetuning from the original paper, but significantly degrade with our script. Our finetuning is also better for unprotected images (see Appendix \ref{sec:glazebad-unp}).}
\label{fig:glazebad}
\end{figure}

\paragraph{(2) Existing robust mimicry attempts are sub-optimal.} Prior evaluations of protections fail to reflect the capabilities of moderately resourceful forgers, who employ state-of-the-art methods (even off-the-shelf ones). For instance, Mist~\citep{mist} evaluates  against \emph{DiffPure} purifications using an outdated and low-resolution purification model. Using DiffPure with a more recent model, we observe significant improvements. Glaze~\citep{glaze} is not evaluated against any version of DiffPure, but claims protection against \emph{Compressed Upscaling}, which first compresses an image with JPEG and then upscales it with a dedicated model. Yet, we will show that by simply swapping the JPEG compression with Gaussian noising, we create \emph{Noisy Upscaling} as a variant that is highly successful at removing mimicry protections  (see Figure \ref{fig:noisyups} for a comparison between both methods). 

\paragraph{(3) Existing evaluations are non-comprehensive.} Comparing the robustness of prior protections is challenging because the original evaluations use different sets of artists, prompts, and finetuning setups. Moreover, some evaluations rely on automated metrics (e.g., CLIP similarity) which are unreliable for measuring style mimicry~\citep{glaze,glazeresponsetoimpress}. Due to the brittleness of protection methods and the subjectivity of mimicry assessments, we believe a unified evaluation is needed.

\subsection{A Unified and Rigorous Evaluation of Robust Mimicry Methods}
To address the limitations presented in Section~\ref{sec:prot-limitations}, we introduce a unified evaluation protocol to reliably assess how existing protections fare against a variety of simple and natural robust mimicry methods. Our solutions to each of the numbered limitations above are: (1) The attacker uses a popular ``off-the-shelf'' finetuning script for the strongest open-source model that all protections claim to be effective for: Stable Diffusion 2.1. This finetuning script is chosen independently of any of these protections, and we treat it as a black-box. (2) We design four robust mimicry methods, described in Section \ref{sec:our-methods}. We prioritize simplicity and ease of use for low-expertise attackers by combining a variety of off-the-shelf tools. (3) We design and conduct a user study to evaluate each mimicry protection against each robust mimicry method on a common set of artists and prompts.


\subsection{Our Robust Mimicry Methods}
\label{sec:our-methods}


We now describe four robust mimicry methods that we designed to assess the robustness of protections. We primarily prioritize simple methods that only require \emph{preprocessing} protected images. These methods present a higher risk because they are more accessible, do not require technical expertise, and can be used in black-box scenarios (e.g. if finetuning is provided as an API service). For completeness, we further propose one white-box method, inspired by IMPRESS~\citep{impress}.

We note that the methods we propose have been considered (at least in part) in prior work that found them to be \emph{ineffective} against style mimicry protections~\citep{glaze, mist, glazeresponsetoimpress}. Yet, as we noted in Section~\ref{sec:prot-limitations}, these evaluations suffered from a number of limitations. We thus re-evaluate these methods (or slight variants thereof) and will show that they are significantly more successful than previously claimed.

\paragraph{Black-box preprocessing methods.}\hspace{0.3em}

\noindent \ding{70} \emph{Gaussian noising.} As a simple preprocessing step, we add small amounts of Gaussian noise to protected images. This approach can be used ahead of any black-box diffusion model.

\noindent \ding{70} \emph{DiffPure.} We use image-to-image models to remove perturbations introduced by the protections, also called DiffPure \citep{diffpure} (see \Cref{sec:diffpure}). This method is black-box, but requires two different models: the purifier, and the one used for style mimicry. We use Stable Diffusion XL as our purifier.

    \noindent \ding{70} \emph{Noisy Upscaling.} We introduce a simple and effective variant of the two-stage upscaling purification considered in Glaze~\citep{glaze}. Their method first performs JPEG compression (to minimize perturbations) and then uses the Stable Diffusion Upscaler \citep{stablediffusion} (to mitigate degradations in quality). 
    Yet, we find that upscaling actually \emph{magnifies} JPEG compression artifacts instead of removing them. To design a better purification method, we observe that the Upscaler is trained on images augmented with Gaussian noise.  Therefore, we purify a protected image by first applying Gaussian noise and then applying the Upscaler. This Noisy Upscaling method introduces no perceptible artifacts and significantly reduces protections (see Figure \ref{fig:noisyups} for an example and \Cref{sec:noisyupscaling} for details).

\paragraph{White-box methods.}\hspace{0.3em}

\noindent \ding{70} \emph{IMPRESS++.} For completeness, we design a white-box method to assess whether more complex methods can further enhance the robustness of style mimicry. Our method builds on IMPRESS~\citep{impress} but adopts a different loss function and further applies \emph{negative prompting}~\citep{miyake2023negative} and \emph{denoising} to improve the robustness of the sampling procedure (see \Cref{ap:impressplus} and Figure \ref{fig:impress} for details).






\section{Experimental Setup}
\label{sec:experimentalsetup}

\paragraph{Protection tools.}
We evaluate three protection tools---\text{Mist}, \text{Glaze} and \text{Anti-DreamBooth}---against four robust mimicry methods---\text{Gaussian noising}, \text{DiffPure}, \text{Noisy Upscaling} and \text{IMPRESS++}---and a baseline mimicry method.
We refer to a combination of a protection tool and a mimicry method as a \emph{scenario}. We thus analyze fifteen possible scenarios. \Cref{ap:experimentalsetup} describes our experimental setup for style mimicry and protections in detail.


\paragraph{Artists.}
We evaluate each style mimicry scenario on images from 10 different artists, which we selected to maximize style diversity. To address limitations in prior evaluations~\citep{glazeresponsetoimpress}, we use five historical artists
as well as five contemporary artists who are unlikely to be highly represented in the generative model's training set (two of these were also used in Glaze's evaluation).\footnote{Contemporary Artists were selected from \emph{Artstation}. We keep them anonymous throughout this work---and refrain from showcasing their art---except for artists who gave us explicit permission to share their identity and art. We will share all images used in our experiments upon request with researchers.} All details about artist selection are included in \Cref{ap:experimentalsetup}.

\paragraph{Implementation.}
Our mimicry methods finetune Stable Diffusion 2.1 \citep{stablediffusion}, the best open-source model available at the time when the protections we study were introduced.
We use an off-the-shelf finetuning script from HuggingFace
(see \Cref{sec:stylemimicryconf} for details).
We first validate that our style mimicry pipeline is successful on unprotected art using a user study, detailed in Appendix \ref{sec:mimicrysetupvalidation}. For protections, we use the original codebases to reproduce Mist and Anti-Dreambooth. Since Glaze does not have a public codebase (and the authors were unable to share one), we use the released Windows application binary (version 1.1.1) as a black-box.
We set each scheme's hyperparameters to maximize protections.
See \Cref{sec:attackconf} for details on the configuration for each protection.

We perform robust mimicry by finetuning on 18 different images per artist. We then generate images for 10 different prompts. These prompts are designed to cover diverse motifs that the base model, Stable Diffusion 2.1, can successfully generate. See Appendix \ref{ap:prompts} for details about prompt design.

\paragraph{User study.}
To measure the success of each style mimicry scenario, we rely only on human evaluations since previous work found automated metrics (e.g., using CLIP~\citep{radford2021learning}) to be unreliable~\citep{glaze,glazeresponsetoimpress}. Moreover, style protections not only prevent style transfer, but also reduce the overall quality of the generated images (see Figure \ref{fig:examplesmimicry} for examples). We thus design a user study to evaluate image quality and style transfer as independent attributes of the generations.\footnote{The user study was approved by %
our institution's IRB.}







\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{plots/examples.pdf}
    \caption{Examples of robust style mimicry for two different artists: @greg-f (contemporary) and Edvard Munch (historical). Cherry-picked examples with strong protections and successful robust mimicry. We apply Noisy Upscaling for prompts: ``\textit{a shoe}'' and ``\textit{an astronaut riding a horse}''.}
    \label{fig:examplesmimicry}
\end{figure}

Our user study asks annotators on  Amazon Mechanical Turk (MTurk) to compare image pairs, where one image is generated by a robust mimicry method, and the other from a baseline state-of-the-art mimicry method that uses \emph{unprotected} art of the artist. A perfectly robust mimicry method would generate images of quality and style indistinguishable from those generated directly from unprotected art. We perform two separate studies: one assessing image quality (e.g., which image looks ``better'') and another evaluating stylistic transfer (i.e., which image captures the artist's original style better, disregarding potential quality artifacts). Our results show that these two metrics obtain very similar results across all scenarios. \Cref{sec:userstudy} describes our user study and interface in detail.

As noted by the authors of Glaze~\citep{glaze}, the users of platforms like 
MTurk might not have high artistic expertise. However, we believe that the judgment of non-artists is also relevant as they ultimately represent a large fraction of the \emph{consumers} of digital art. Thus, if lay people consider mimicry attempts to be successful, mimicked art could hurt an artist's business. Also, to mitigate potential issues with the quality of annotations~\citep{kennedy2020shape}, we put in place several control mechanisms to filter out low-quality annotations to the best of our abilities (details in Appendix \ref{sec:userstudy}).




\paragraph{Evaluation metric.} We define the \emph{success rate} of a robust mimicry method as the percentage of annotators (5 per comparison) who prefer outputs from the robust mimicry method over those from a baseline method finetuned on \emph{unprotected} art (when judging either style match or overall image quality). Formally, we define the success rate for an artist in a specific scenario as:
\begin{equation}
\label{eq:success}
 \texttt{success rate} = \frac{1}{{\color{black} 10} \cdot {\color{black} 5}} \; \; {\color{black}\sum_{\text{prompt}}^{10}} \hspace{0.5em}{\color{black}\sum_{\text{annotator}}^5} \mathds{1}{[\textit{robust mimicry} \text{ \small{preferred over} } \textit{unprotected mimicry}]}
\end{equation}

A perfectly robust mimicry method would thus obtain a success rate of 50\%, indicating that its outputs are indistinguishable in quality and style from those from the baseline, unprotected method. In contrast, a very successful protection would result in success rates of around 0\% for robust mimicry methods, indicating that mimicry on top of protected images always yields worse outputs.

\section{Results}
\label{sec:results}


In \Cref{fig:resultsmain}, we report the distribution of success rates per artist (N=10) for each scenario. We averaged the quality and stylistic transfer success rates to simplify the analysis (detailed results can be found in Appendix \ref{ap:additionalresults}). Since the forger can try multiple mimicry methods for each prompt, and then decide which one worked best, we also evaluate a ``best-of-4'' method that picks the most successful mimicry method for each generation (according to human evaluators).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{plots/legend.pdf}
    \end{subfigure}
    \hspace*{-3.3cm}
    \begin{subfigure}[t]{\textwidth}
    \resizebox{1.13\linewidth}{!}{\import{plots/main_avg}{plot.pgf}}
    \end{subfigure}
    \caption{Success rate per artist (N=10) on all mimicry scenarios. Box plots represent success rates for most protected, quartiles, median and least protected artists, respectively. Success rates around 50\% indicate that robust mimicry outputs are indistinguishable
    in style and quality from mimicry outputs based on unprotected images. \emph{Best-of-4} selects the most successful method for each prompt.}
    \label{fig:resultsmain}
\end{figure}

\subsection{Main Findings: All Protections are Easily Circumvented} %
We find that all existing protective tools create a false sense of security and leave artists vulnerable to style mimicry. Indeed, our best robust mimicry methods produce images that are, on average, indistinguishable from baseline mimicry attempts using unprotected art.
Since many of our simple mimicry methods only use tools that were available before the protections were released, style forgers may have already circumvented these protections since their inception.

Noisy upscaling is the most effective method for robust mimicry, with a median success rate above 40\% for each protection tool (recall that 50\% success indicates that the robust method is indistinguishable from a mimicry using unprotected images). This method only requires preprocessing images and black-box access to the model via a finetuning API. Other simple preprocessing methods like Gaussian noising or DiffPure also significantly reduce the effectiveness of protections. The more complex white-box method IMPRESS++ does not provide significant advantages. Sample generations for each method are in Appendix \ref{ap:generations}.

A style forger does not have to use a single robust mimicry method, but can test all of them and select the most successful.
This ``best-of-4'' approach always beats the baseline mimicry method over unprotected images (which attempts a single method and not four) for all protections.

Appendix \ref{ap:examples} shows images at each step of the robust mimicry process (i.e., protections, preprocessing, and sampling). Appendix \ref{ap:generations} shows example generations for each protection and mimicry method.
Appendix \ref{ap:additionalresults} has detailed success rates broken down per artist, for both image style and quality. 

\subsection{Analysis}
We now discuss key insights and lessons learned from these results. 


\paragraph{Glaze protections break down without any circumvention attempt.} Results for Glaze without robust mimicry (see ``Naive mimicry'' row in \Cref{fig:resultsmain}) show that the tool's protections are often ineffective.
Without any robustness intervention, 30\% of the images generated with our off-the-shelf finetuning are rated as better than the baseline results using only unprotected images.
This contrasts with Glaze's original evaluation, which claimed a success rate of at most 10\% for robust mimicry.\footnote{The original evaluation in Glaze directly asks annotators whether a mimicry is successful or not, rather than a binary comparison between a robust mimicry and a baseline mimicry as in our setup. \citet{glaze} report that mimicry fails in 4\% of cases for unprotected images, and succeeds in 6\% of cases for protected images. This bounds the success rate for robust mimicry---according to our definition in \Cref{eq:success}---by at most $10\%$.} This difference is likely due to the protection's brittleness to slight changes in the finetuning setup (as we illustrated in Section \ref{sec:glazebrittle}).
With our best robust mimicry method (noisy upscaling) the median success rate across artists rises further to 40\%, and our best-of-4 strategy yields results indistinguishable from the baseline for a majority of artists.






\paragraph{Robust mimicry works for contemporary and historical artists alike.} \citet{glazeresponsetoimpress} note that one of IMPRESS' main limitations is that ``purification has a limited effect when tested on artists that are not well-known historical artists already embedded in original training data''. Yet, we find that our best-performing robust mimicry method---Noisy Upscaling---has a similar success rate for historical artists (42.2\%) and contemporary artists with little representation in the model's training set (43.5\%).

\paragraph{Protections are highly non-uniform across artists.} As we observe from Figure \ref{fig:resultsmain}, the effectiveness of protections varies significantly across artists: the least vulnerable artist (left-most whisker) enjoys much stronger mimicry protections than the median artist or the most vulnerable artist (right-most whisker). We find that robust mimicry is the least successful for artists where the baseline mimicry from unprotected images gives poor results to begin with (cf. results for artist $A_1$ in Appendix~\ref{ap:additionalresults} and Appendix~\ref{sec:mimicrysetupvalidation}).
Yet, since existing tools do not provide artists with a way to \emph{check} how vulnerable they are, these tools still provide a false sense of security for all artists. This highlights an inherent asymmetry between protection tools and mimicry methods: protections should hold for \emph{all} artists alike, while a mimicry method might successfully target only specific artists.


\begin{figure}[t]
    \centering
        \includegraphics[width=0.9\textwidth]{plots/Failure_Modes.pdf}
    \vspace{-0.25em}
    \caption{Randomly selected comparisons where all 5 annotators preferred mimicry from unprotected art over robust mimicry. Both use Noisy Upscaling for robust mimicry.}
    \label{fig:failuremodes}
    \vspace{-0.5em}
\end{figure}

\paragraph{Robust mimicry failures still remove protection artifacts.} We manually checked the cases where all annotators ranked mimicry from unprotected art as better than robust mimicry with Noisy Upscaling. Figure \ref{fig:failuremodes} shows two examples. We find that in many instances, the model fails to mimic the style accurately even from unprotected art. In these cases, robust mimicry is still able to generate clear images that are similar to unprotected mimicry, but neither matches the original style well. 





\section{Discussion and Broader Impact}
\label{sec:discussion}

\paragraph{Adversarial perturbations do not protect artists from style mimicry.} 
Our work is not intended as an exhaustive search for the best robust mimicry method, but as a demonstration of the brittleness of existing protections. 
Because these protections have received significant attention, artists may believe they are effective.
But our experiments show \emph{they are not}.
As we have learned from adversarial ML, whoever acts first (in this case, the artist) is at a fundamental disadvantage \citep{radiya2021data}. 
We urge the community to acknowledge these limitations and think critically when performing future evaluations.

\paragraph{Just like adversarial examples defenses, mimicry protections should be evaluated adaptively.} 
In adversarial settings, where one group wants to prevent another group from achieving some goal, it is necessary to consider ``adaptive attacks'' that are specifically designed to evade the defense \citep{carlini2017adversarial}. 
Unfortunately, as repeatedly seen in the literature on machine learning robustness, even after adaptive attacks were introduced, many evaluations remained flawed and defenses were broken by (stronger) adaptive attacks \citep{tramer2020adaptive}.
We show it is the same with mimicry protections: simple adaptive attacks significantly reduce their effectiveness.
Surprisingly, most protections we study claim robustness against input transformations~\citep{mist,glaze}, but minor modifications were sufficient to circumvent them.

We hope that the literature on style mimicry prevention will learn from the failings of the adversarial example literature:
performing reliable, future-proof evaluations is much harder than proposing a new defense.
Especially when techniques are widely publicized in the popular press, we believe it is necessary to provide users with exceptionally high degrees of confidence in their efficacy.

\paragraph{Protections are broken from day one, and cannot improve over time.}
Our most successful robust style mimicry methods rely solely on techniques that existed before the protections were introduced.
Also, protections applied to online images cannot easily be changed (i.e., even if the image is perturbed again and re-uploaded, the older version may still be available in an internet archive)~\citep{radiya2021data}.
It is thus challenging for a broken protection method to be fixed retroactively. Of course, an artist can apply the new tool to their images going forward, but pre-existing images with weaker protections (or none at all) will significantly boost an attacker's success~\citep{glaze}.

Nevertheless, the Glaze and Mist protection tools recently received significant updates (after we had concluded our user study). Yet, we find that the newest 2.0 versions do not protect against our robust mimicry attempts either (see Appendix \ref{ap:glaze20} and \ref{ap:mist20}).
A future version could explicitly target the methods we studied, but this would not change the fact that all previously protected art would remain vulnerable, and that future attacks could again attempt to adaptively evade the newest protections.
The same holds true for attempts to design similar protections for other data modalities, such as video~\citep{passananti2024disrupting} or audio~\citep{gokul2024poscuda}.

\paragraph{Ethics and broader impact.} The goal of our research is to help artists better decide how to protect their artwork and business. We do not focus on creating the \emph{best} mimicry method, but rather on highlighting limitations in popular perturbation tools---especially since using these tools incurs a cost, as they degrade the quality of published art.
We will disclose our results to the affected protection tools prior to publication, so that they can determine the best course of action for their users.

Further, we argue that having \emph{no} protection tools is preferable to having \emph{insecure} ones. Insecure protections may mislead artists to believe it is safe to release their work, enabling forgery and putting them in a worse situation than if they had been more cautious in the absence of any protection.


With respect to our paper, all the art featured in this paper comes either from historical artists, or from contemporary artists who explicitly permitted us to display their work. We hope our results will inform improved non-technical protections for artists in the era of generative AI.

\paragraph{Limitations and future work.} 
A larger study with more than 10 artists and more annotators may help us better understand the difference in vulnerability across artists.
The protections we study are not designed in awareness of our robust mimicry methods. However, we do not believe this limits the extent to which our general claims hold: artists will always be at a disadvantage if attackers can design adaptive methods to circumvent the protections.





\section*{Acknowledgements}
We thank all the MTurkers that engaged with our tasks, especially those that provided valuable feedback during our preliminary studies to improve the survey. We thank the contemporary artists Stas Voloshin (@nulevoy) and Gregory Fromenteau (@greg-f) for allowing us to display their artwork in this paper. JR is supported by an ETH AI Center doctoral fellowship.

\clearpage
\bibliographystyle{bib}
\bibliography{bib}

\newpage
\clearpage
\appendix

\clearpage
\section{Detailed Art Examples}
\label{ap:examples}

This section illustrates how images look like at every stage of our work. We include (1) original artwork from a contemporary artist (@nulevoy)\footnote{The artist gave explicit permission for the use of their art} as a reference in Figure \ref{fig:proc-original}, (2) the original artwork after applying each of the available protections in Figure \ref{fig:proc-protections}, (3) one image after applying the cross product of all protections and preprocessing methods in Figure \ref{fig:proc-preprocessing}, (4) baseline generations from a model trained on unprotected art in Figure \ref{fig:proc-gen-clean}, and (5) robust mimicry generations for each scenario in Figure \ref{fig:proc-gen-robust}.

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/original/0000.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/original/0001.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/original/0005.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/original/0009.jpg}
     \end{subfigure}
    \caption{4 samples from the original artwork from @nulevoy.}
    \label{fig:proc-original}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/glaze/0000.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/glaze/0001.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/glaze/0005.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/glaze/0009.jpg}
     \end{subfigure}
    \caption{Glaze}
    \vspace{0.5em}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/mist/0000.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/mist/0001.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/mist/0005.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/mist/0009.jpeg}
     \end{subfigure}
    \caption{Mist}
    \vspace{0.5em}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/antidb/0000.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/antidb/0001.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/antidb/0005.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/antidb/0009.jpeg}
     \end{subfigure}
    \caption{Anti-DreamBooth}
    \end{subfigure}
    \caption{Artwork in Figure \ref{fig:proc-original} after applying different protections.}
    \label{fig:proc-protections}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.24\textwidth}
         \centering
         No preprocessing\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/glaze/0009.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         Gaussian Noising\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/preprocess/glaze/gaussian.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         DiffPure\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/preprocess/glaze/diffpure.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         Noisy Upscaling\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/preprocess/glaze/upscale.jpg}
     \end{subfigure}
    \caption{Glaze}
    \vspace{0.5em}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/mist/0009.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/preprocess/mist/gaussian.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/preprocess/mist/diffpure.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/preprocess/mist/upscale.jpg}
     \end{subfigure}
    \caption{Mist}
    \vspace{0.5em}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/antidb/0009.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/preprocess/antidb/gaussian.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/preprocess/antidb/diffpure.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/preprocess/antidb/upscale.jpg}
     \end{subfigure}
    \caption{Anti-DreamBooth}
    \end{subfigure}
    \caption{Artwork used for finetuning after applying preprocessing methods to protected images in Figure \ref{fig:proc-protections}. Each row represents a protection, and each column a preprocessing method. Noisy Upscaling is the most successful preprocessing technique at removing the perturbations introduced by protections.}
    \label{fig:proc-preprocessing}
\end{figure}

\clearpage

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/generations/clean/1.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/generations/clean/2.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/generations/clean/3.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/generations/clean/4.jpeg}
     \end{subfigure}
    \caption{Generations in the style of @nulevoy after finetuning on \emph{unprotected} images. Each generation is sampled with a different seed.}
    \label{fig:proc-gen-clean}
\end{figure}


\vspace{5em}
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.19\textwidth}
         \centering
         Naive mimicry\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/generations/glaze/none.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         Gaussian Noising\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/generations/glaze/gaussian.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         DiffPure\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/generations/glaze/diffpure.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         IMPRESS++\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/generations/glaze/impress.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         Noisy Upscaling\vspace{0.3em}
         \includegraphics[width=\textwidth]{plots/process/generations/glaze/upscale.jpeg}
     \end{subfigure}
    \caption{Glaze}
    \vspace{0.5em}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.19\textwidth}
         \centering
         
         \includegraphics[width=\textwidth]{plots/process/generations/mist/none.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/mist/gaussian.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/mist/diffpure.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/mist/impress.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/mist/upscale.jpeg}
     \end{subfigure}
    \caption{Mist}
    \vspace{0.5em}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/antidb/none.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/antidb/gaussian.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/antidb/diffpure.jpeg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/antidb/impress.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering

         \includegraphics[width=\textwidth]{plots/process/generations/antidb/upscale.jpeg}
     \end{subfigure}
    \caption{Anti-DreamBooth}
    \end{subfigure}
    \caption{Generations in the style of @nulevoy using robust mimicry methods for the prompt ``\textit{an astronaut riding a horse}''. Each row represents which protection was applied to the finetuning data. Each column represents the robust mimicry method used. The first column indicates naive mimicry was applied (i.e. we trained directly on the protected images). Figure \ref{fig:proc-gen-clean} includes sample generations from a model trained on artwork without protections.}
    \label{fig:proc-gen-robust}
\end{figure}

\newpage
\section{Robust Mimicry Generations}
\label{ap:generations}

\begin{figure}[h]
    \centering
    \hspace*{-3.4cm}
    \resizebox{.65\textwidth}{!}{\begin{subfigure}[t]{\textwidth}
    \import{plots/random_samples_none}{plot.pgf}
    \end{subfigure}}
    \caption{Style mimicry for all protections using \emph{naive mimicry}---no robust method is used and we finetune directly on protected images. We randomly chose artists and prompts. Each image pair shows the protected generation and generation from unprotected art.} 
    \label{fig:nodefensesamples}
\end{figure}

\begin{figure}[h]
    \centering
    \hspace*{-3.4cm}
    \resizebox{.77\textwidth}{!}{\begin{subfigure}[t]{\textwidth}
    \import{plots/random_samples_gaussian_noising}{plot.pgf}
    \end{subfigure}}
    \caption{Style mimicry for all protections using \emph{Gaussian Noising}. We randomly chose artists and prompts. Each image pair shows the protected robust generation and generation from unprotected art.} 
    \label{fig:gaussiannoisingsamples}
\end{figure}


\begin{figure}[h]
    \centering
    \hspace*{-3.4cm}
    \resizebox{.77\textwidth}{!}{\begin{subfigure}[t]{\textwidth}
    \import{plots/random_samples_diffpure}{plot.pgf}
    \end{subfigure}}
    \caption{Style mimicry for all protections using \emph{DiffPure}. We randomly chose artists and prompts. Each image pair shows the protected robust generation and generation from unprotected art.} 
    \label{fig:diffpuresamples}
\end{figure}

\begin{figure}[h]
    \centering
    \hspace*{-3.4cm}
    \resizebox{.77\textwidth}{!}{\begin{subfigure}[t]{\textwidth}
    \import{plots/random_samples_nrnd}{plot.pgf}
    \end{subfigure}}
    \caption{Style mimicry for all protections using \emph{IMPRESS++}. We randomly chose artists and prompts. Each image pair shows the protected robust generation and generation from unprotected art.} 
    \label{fig:nrndsamples}
\end{figure}

\begin{figure}[h]
    \centering
    \hspace*{-3.4cm}
    \resizebox{.77\textwidth}{!}{\begin{subfigure}[t]{\textwidth}
    \import{plots/random_samples_noisy_upscaling}{plot.pgf}
    \end{subfigure}}
    \caption{Style mimicry for all protections using \emph{Noisy Upscaling}. We randomly chose artists and prompts. Each image pair shows the protected robust generation and generation from unprotected art.} 
    \label{fig:noisyupsamplingsamples}
\end{figure}

\clearpage

\clearpage
\section{Detailed Results}
\label{ap:additionalresults}

\subsection{Mimicry Quality Versus Style}
This section includes the detailed results from our user study. As mentioned in Section \ref{sec:experimentalsetup}, we ask users to assess quality and stylistic fit separately in our study. Figure \ref{fig:quality} and \ref{fig:style} show the results for each of these evaluations separately (the results in the main body represent the average of the two). Finally, Table \ref{tab:resultsummary} includes numerical results for each scenario.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{plots/legend.pdf}
    \end{subfigure}
    \hspace*{-3.2cm}
    \begin{subfigure}[t]{\textwidth}
    \resizebox{1.13\linewidth}{!}{\import{plots/main_qual}{plot.pgf}}
    \end{subfigure}
    \caption{Quality evaluation. User preference ratings of all style mimicry scenarios but only for the quality question: ``Based on noise, artifacts, detail, prompt fit, and your impression, which image has higher quality?''.}
    \label{fig:quality}
\end{figure}

\begin{figure}[h]
    \centering
    \hspace*{-3.2cm}
    \begin{subfigure}[t]{\textwidth}
    \resizebox{1.13\linewidth}{!}{\import{plots/main_style}{plot.pgf}}
    \end{subfigure}
    \caption{Style evaluation. User preference ratings of all style mimicry scenarios but only for the quality question: ``Overall, ignoring quality, which image better fits the style of the style samples?''.}
    \label{fig:style}
\end{figure}

\begin{table}[h]
\caption{Success rates averaged across artists for all style mimicry scenarios. Higher percentages indicate more successful mimicry, and 50\% would indicate perfect mimicry.}
\begin{subtable}{\textwidth}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrr}
\toprule
Method &  Naive mimicry &  Gaussian noising &  IMPRESS++ &  DiffPure &  Noisy Upscaling &  Best-of-4 \\
Protection  &       &                   &       &           &                  &            \\
\midrule
Anti-DB & 11.6\% &             20.6\% & 32.2\% &     26.6\% &            45.0\% &      56.6\% \\
Glaze   & 22.2\% &             29.6\% & 35.4\% &     32.0\% &            39.4\% &      56.6\% \\
Mist    &  9.0\% &             21.0\% & 37.4\% &     35.8\% &            42.8\% &      62.0\% \\
\bottomrule
\end{tabular}}
\caption{Quality}
\end{subtable}
\begin{subtable}{\textwidth}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrr}
\toprule
Method &  Naive mimicry &  Gaussian noising &  IMPRESS++ &  DiffPure &  Noisy Upscaling &  Best-of-4 \\
Protection  &       &                   &       &           &                  &            \\
\midrule
Anti-DB & 21.8\% &             31.2\% & 28.6\% &     31.0\% &            44.0\% &      52.4\% \\
Glaze   & 30.8\% &             35.4\% & 27.8\% &     37.6\% &            41.6\% &      51.2\% \\
Mist    & 19.4\% &             35.4\% & 31.6\% &     37.4\% &            44.2\% &      53.4\% \\
\bottomrule
\end{tabular}}
\caption{Style}
\end{subtable}
\label{tab:resultsummary}
\end{table}

\newpage

\subsection{Results Broken Down per Artist}
We present next the results obtained for each artist in each scenario. Table \ref{tab:resultsperartist} plots the success rate for each method against each protection for all artists, and Table \ref{tab:resultsperartistfull} includes the detailed success rates.

\begin{table}[h]
\caption{Success rates per artist for style and quality questions, respectively. Each line plot shows, for a given protection and artist, the success rate with Gaussian noising (\hexagon), naive mimicry (\mydiamond), IMPRESS++ (\protect{\mycircle}), DiffPure (~\mystar~), Noisy Upscaling (\mytriangle), and Best-of-4 (~\sixstar~) on a scale from 0\% to 77\%, where the bar $\mid$ demarcates 50\%.}

\label{tab:resultsperartist}
\begin{subtable}{\textwidth}
\centering
\input{tables/resultartisthistqual}
\caption{Quality}
\end{subtable}
\begin{subtable}{\textwidth}
\centering
\input{tables/resultartisthiststyle}
\caption{Style}
\end{subtable}
\end{table}

\begin{table}[h]

\caption{User preference ratings of all style mimicry scenarios $\method \in \sM$ for each artist $A \in \sA$ by name. Each cell states the percentage of votes that prefer an image generated under the corresponding scenario $\method$ and artist $A \in \sA$ over a matching image generated under clean style mimicry. Higher percentages indicate weaker attacks or better defenses.}

\begin{subtable}{\textwidth}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{llrrrrrr}
\toprule
     & Method &  Naive mimicry &  Gaussian noising &  IMPRESS++ &  DiffPure &  Noisy Upscaling &  Best-of-4 \\
Protection & Artist &       &                   &       &           &                  &            \\
\midrule
Anti-DB & $A_1$ &  4\% &              6\% &  8\% &     18\% &            26\% &      30\% \\
     & $A_2$ & 14\% &             48\% & 54\% &     32\% &            50\% &      62\% \\
     & $A_3$ & 10\% &              8\% & 18\% &     16\% &            40\% &      46\% \\
     & $A_4$ & 14\% &             22\% & 20\% &     14\% &            54\% &      70\% \\
     & $A_5$ & 16\% &             16\% & 22\% &     24\% &            54\% &      60\% \\
     & Albrecht Durer &  2\% &             22\% & 32\% &     26\% &            42\% &      70\% \\
     & Alphonse Mucha & 16\% &             22\% & 44\% &     42\% &            60\% &      66\% \\
     & Anna O.-Lebedeva & 38\% &             40\% & 56\% &     40\% &            44\% &      76\% \\
     & Edvard Munch &  2\% &             14\% & 40\% &     40\% &            46\% &      56\% \\
     & Edward Hopper &  0\% &              8\% & 28\% &     14\% &            34\% &      30\% \\ \midrule
Glaze & $A_1$ &  8\% &             20\% & 22\% &     10\% &            12\% &      24\% \\
     & $A_2$ & 12\% &             42\% & 40\% &     28\% &            44\% &      60\% \\
     & $A_3$ & 12\% &             26\% & 18\% &     26\% &            34\% &      52\% \\
     & $A_4$ & 22\% &             20\% & 20\% &     54\% &            54\% &      60\% \\
     & $A_5$ & 18\% &             34\% & 34\% &     24\% &            40\% &      52\% \\
     & Albrecht Durer &  2\% &             16\% & 40\% &     28\% &            26\% &      54\% \\
     & Alphonse Mucha & 40\% &             44\% & 58\% &     42\% &            56\% &      66\% \\
     & Anna O.-Lebedeva & 42\% &             46\% & 54\% &     44\% &            34\% &      70\% \\
     & Edvard Munch & 40\% &             16\% & 42\% &     42\% &            38\% &      62\% \\
     & Edward Hopper & 26\% &             32\% & 26\% &     22\% &            56\% &      66\% \\ \midrule
Mist & $A_1$ &  0\% &              6\% & 20\% &      4\% &            12\% &      28\% \\
     & $A_2$ & 14\% &             50\% & 50\% &     46\% &            48\% &      76\% \\
     & $A_3$ &  0\% &             10\% & 22\% &     24\% &            60\% &      60\% \\
     & $A_4$ &  0\% &             16\% & 24\% &     36\% &            66\% &      70\% \\
     & $A_5$ & 12\% &             22\% & 40\% &     28\% &            50\% &      54\% \\
     & Albrecht Durer & 10\% &             24\% & 28\% &     46\% &            38\% &      60\% \\
     & Alphonse Mucha & 32\% &             18\% & 60\% &     56\% &            54\% &      66\% \\
     & Anna O.-Lebedeva & 20\% &             38\% & 54\% &     50\% &            34\% &      74\% \\
     & Edvard Munch &  2\% &             22\% & 54\% &     44\% &            28\% &      72\% \\
     & Edward Hopper &  0\% &              4\% & 22\% &     24\% &            38\% &      60\% \\
\bottomrule
\end{tabular}}



\caption{Quality}
\end{subtable}
\begin{subtable}{\textwidth}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{llrrrrrr}
\toprule
     & Method &  Naive mimicry &  Gaussian noising &  IMPRESS++ &  DiffPure &  Noisy Upscaling &  Best-of-4 \\
Protection & Artist &       &                   &       &           &                  &            \\
\midrule
Anti-DB & $A_1$ &  0\% &              4\% &  4\% &     10\% &            34\% &      36\% \\
     & $A_2$ & 14\% &             20\% & 40\% &     16\% &            48\% &      54\% \\
     & $A_3$ & 10\% &             14\% & 26\% &     28\% &            42\% &      46\% \\
     & $A_4$ & 36\% &             58\% & 42\% &     56\% &            54\% &      56\% \\
     & $A_5$ &  4\% &              0\% & 10\% &     32\% &            60\% &      66\% \\
     & Albrecht Durer & 20\% &             32\% & 36\% &     28\% &            44\% &      50\% \\
     & Alphonse Mucha & 56\% &             56\% & 42\% &     52\% &            48\% &      58\% \\
     & Anna O.-Lebedeva & 32\% &             50\% & 24\% &     30\% &            28\% &      56\% \\
     & Edvard Munch &  6\% &             30\% & 26\% &     20\% &            46\% &      50\% \\
     & Edward Hopper & 40\% &             48\% & 36\% &     38\% &            36\% &      52\% \\ \midrule
Glaze & $A_1$ &  8\% &             14\% &  8\% &     14\% &            30\% &      34\% \\
     & $A_2$ & 36\% &             42\% & 26\% &     46\% &            44\% &      52\% \\
     & $A_3$ & 24\% &             24\% & 16\% &     40\% &            32\% &      50\% \\
     & $A_4$ & 56\% &             58\% & 32\% &     44\% &            58\% &      66\% \\
     & $A_5$ & 12\% &             18\% & 18\% &     30\% &            32\% &      40\% \\
     & Albrecht Durer & 22\% &             28\% & 26\% &     26\% &            38\% &      38\% \\
     & Alphonse Mucha & 48\% &             54\% & 36\% &     54\% &            52\% &      56\% \\
     & Anna O.-Lebedeva & 26\% &             32\% & 40\% &     38\% &            44\% &      68\% \\
     & Edvard Munch & 38\% &             32\% & 36\% &     40\% &            48\% &      56\% \\
     & Edward Hopper & 38\% &             52\% & 40\% &     44\% &            38\% &      52\% \\ \midrule
Mist & $A_1$ &  0\% &              6\% &  4\% &      0\% &            22\% &      18\% \\
     & $A_2$ &  6\% &             38\% & 44\% &     42\% &            64\% &      72\% \\
     & $A_3$ &  6\% &             28\% & 26\% &     36\% &            34\% &      44\% \\
     & $A_4$ & 36\% &             58\% & 46\% &     52\% &            48\% &      54\% \\
     & $A_5$ &  4\% &             14\% & 18\% &     26\% &            58\% &      56\% \\
     & Albrecht Durer & 28\% &             32\% & 24\% &     36\% &            50\% &      60\% \\
     & Alphonse Mucha & 34\% &             50\% & 34\% &     50\% &            48\% &      64\% \\
     & Anna O.-Lebedeva & 32\% &             48\% & 44\% &     56\% &            38\% &      64\% \\
     & Edvard Munch & 10\% &             38\% & 36\% &     40\% &            42\% &      64\% \\
     & Edward Hopper & 38\% &             42\% & 40\% &     36\% &            38\% &      38\% \\
\bottomrule
\end{tabular}}

\caption{Style}
\end{subtable}


\label{tab:resultsperartistfull}
\end{table}

\clearpage

\subsection{Inter-Annotator Agreement}

\begin{figure}[h]
    \centering
    \import{plots/interagreement}{plot.pgf}
    \caption{Inter-annotator agreement for generations from robust mimicry with Noisy Upscaling and generations from models finetuned on protected art directly (naive mimicry). We plot the percentage of comparisons for which the preferred option was selected by 3, 4 or 5 annotators, respectively. The graph shows a higher consensus for naive mimicry, since the differences are clearer, and more variance for robust mimicry.}
    \label{fig:voteagreement}
\end{figure}

\section{Differences with Glaze Finetuning}
\label{sec:glazebad-unp}
In Section \ref{sec:prot-limitations} and Figure \ref{fig:glazebad}, we discussed the brittleness of Glaze protections against small changes in the finetuning script. We also found our finetuning setup to be better at baseline style mimicry from unprotected art (see Figure \ref{fig:glazebad-unp}).

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/process/original/0009.jpg}
         \caption{Original artwork}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.39\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/theirs-unp.jpg}
         \caption{Glaze finetuning}
     \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.39\textwidth}
        \centering
         \includegraphics[width=\textwidth]{plots/ours-unp.jpg}
         \caption{Our finetuning}
     \end{subfigure}

\caption{The finetuning script shared by Glaze authors produce substantially worse mimicry even from unprotected art. We apply both finetuning scripts directly on unprotected art from @nulevoy. The main reason behind this difference might be that the script uses Stable Diffusion 1.5, instead of version 2.1 as reported in their paper.}
    \label{fig:glazebad-unp}
\end{figure}


\clearpage

\section{Findings on Glaze 2.0}
\label{ap:glaze20}

After concluding our user study, Glaze \citep{glaze} released an updated version of their tool (v2.0). According to the official release, ``This new version significantly improved Glaze robustness against the newest AI models''. Although we could not run the entire user study with the latest protections, we reproduced some of our experiments to verify if protections were more robust under robust mimicry. We believe this comparison is fair to Glaze since we are using newer models---such as Stable Diffusion XL for upscaling. These models, although released before Glaze 1.1.1, may not have been considered in the tool’s design and are now explicitly accounted for.


The official release specifically mentions ``Significantly improved robustness against Stable Diffusion 1, 2, SDXL, especially for smooth surface art (e.g. anime, cartoon)''. Therefore, we decided to test this new tool with the contemporary artist \emph{\@nulevoy}, who draws in a cartoon style and gave us permission to display their artwork. As with the previous version, we only have access to the publicly available Windows application that uses unknown parameters. We protect the images using the ``highest'' protection option. Our main findings are:

\begin{enumerate}
    \item Glaze v2.0 introduces more visible perturbations uniformly over the images. See Figure \ref{fig:glaze20prot}.
    \item Glaze v2.0 does not improve protection under robust mimicry. Noisy Upscaling still achieves almost perfect style mimicry. See Figure \ref{fig:glaze20mimicry}.
    \item Noisy Upscaling is able to to remove visible perturbations during preprocessing as before. See Figure \ref{fig:glaze20preproc}.
\end{enumerate}

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/Glaze11.pdf}
         \caption{Glaze v1.1.1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/Glaze20.pdf}
         \caption{Glaze v2.0}
     \end{subfigure}
    \caption{Comparison of perturbations by Glaze v1.1.1 and v2.0 on artwork from \emph{@nulevoy}.}
    \label{fig:glaze20prot}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/Glaze11RobustGen.pdf}
         \caption{Robust style mimicry on Glaze v1.1.1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/Glaze20RobustGen.pdf}
         \caption{Robust style mimicry on Glaze v2.0}
     \end{subfigure}
    \caption{Comparison of robust style mimicry (Noisy Upscaling) on artwork from \emph{@nulevoy} protected with both versions of Glaze. Images in \Cref{fig:proc-original} serve as a reference for the artistic style.}
    \label{fig:glaze20mimicry}
\end{figure}

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/nulevoy-original.pdf}
         \caption{Original artwork}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/Glaze20Upscaled.pdf}
         \caption{Protected images after Noisy Upscaling}
     \end{subfigure}
    \caption{Original artwork from \emph{@nulevoy} and the resulting images after applying Noisy Upscaling to artwork protected with Glaze v2.0. See protected images in Figure \ref{fig:glaze20prot}.}
    \label{fig:glaze20preproc}
\end{figure}

\section{Findings on Mist v2}
\label{ap:mist20}

After responsibly disclosing our work to defense developers, authors from Mist brought to our attention the recent release of their latest Mist v2 with improved resilience~\citep{zheng2023understanding}. As we did with Glaze v2.0 (see Section \ref{ap:glaze20}), we reproduced some of our experiments with the latest protections to verify the success of robust mimicry. Their original implementation still uses the outdated version 1.5 of Stable Diffusion. We change to SD 2.1 to match our previous experiments\footnote{Both models share the same encoder for which protections are optimized.}.

Our findings, as we saw with Glaze v2.0, highlight that improved protections are still not effective against low-effort robust mimicry. More specifically, the latest version of Mist:
\begin{enumerate}
    \item introduces visible perturbations over the images. See Figure \ref{fig:mist20prot}
    \item does not improve protections against robust mimicry. See Figure \ref{fig:mist20mimicry}
    \item creates protection that are easily removable with Noisy Upscaling. See Figure \ref{fig:mist20preproc}.
\end{enumerate}

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/protectedmist10.pdf}
         \caption{Mist v1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/protectedmist20.pdf}
         \caption{Mist v2}
     \end{subfigure}
    \caption{Comparison of perturbations introduced by Mist v1 and v2 on artwork from \emph{@nulevoy}.}
    \label{fig:mist20prot}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/mist10.pdf}
         \caption{Robust style mimicry on Mist v1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/mist20.pdf}
         \caption{Robust style mimicry on Mist v2}
     \end{subfigure}
    \caption{Comparison of robust style mimicry (Noisy Upscaling) on artwork from \emph{@nulevoy} protected with both versions of Mist. Images in \Cref{fig:proc-original} serve as a reference for the artistic style.}
    \label{fig:mist20mimicry}
\end{figure}

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/nulevoy-original.pdf}
         \caption{Original artwork}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/mist20upscale.pdf}
         \caption{Protected images after Noisy Upscaling}
     \end{subfigure}
    \caption{Original artwork from \emph{@nulevoy} and the resulting images after applying Noisy Upscaling to artwork protected with Mist v2. See protected images in Figure \ref{fig:mist20prot}.}
    \label{fig:mist20preproc}
\end{figure}

\newpage
\section{Methods for Style Mimicry}
\label{sec:stylemimicryappendix}

This section summarizes the existing methods that a style forger can use to perform style mimicry. Our work only considers \emph{finetuning} since it is reported to be the most effective \citep{glaze}.

\subsection{Prompting}
\label{sec:prompting}

Well-known artistic styles contained in the training data (e.g. Van Gogh) can be mimicked by prompting a text-to-image model with a description of the style or the name of the artist.
For example, a prompt can be augmented with $\textrm{`` painted in a cubistic style''}$ $\textrm{`` painted by van Gogh''}$ to mimic those styles, respectively. Prompting is easy to apply and does not require changes to the model. However, it fails to mimic styles that are not sufficiently represented in the training data of model---often from the most vulnerable artists.

\subsection{Img2Img}
\label{sec:mimicimg2img}

Img2Img creates an updated version of an image with guidance from a prompt. For this, Img2Img processes image $x$ with $t$ timesteps of a diffusion process to obtain the diffused image $x_t$. Then, Img2Img uses the model with guidance from prompt $P$ to reverse the diffusion process into the output image variation $x_P$.
Analogous to prompting, a prompt suffices to transfer a well-known style, but Img2Img also fails for unknown styles.

\subsection{Textual Inversion}

Textual inversion \citep{textualinversion} optimizes the embedding of some $n$ new tokens $\vt = \bracket{\token{t}_1,\ldots, \token{t}_n}$ that are appended to image prompts $P$ so that generations closely mimic the style of a given set of images. The tokens are optimized via gradient descent on the model training loss so that $P + \vt$ generates images that mimic the target style. Textual inversion requires white-box access to the target model, but enables the mimicry of unknown styles.

\subsection{Finetuning}
\label{sec:finetuning}

Finetuning updates the weights of a pretrained text-to-image model to introduce a new functionality. In this case, finetuning allows a forger to ``teach'' the generative model an unknown style using a set of images in the target style and their captions (e.g. \emph{an astronaut riding a horse}). First, all captions are augmented with some special word, like the name of the artist, to create prompts $P_x = C_x + \textrm{``by }\word{w}\textrm{''}$. Then, the model weights are updated to minimize the reconstruction loss of the given images following the augmented prompts. At inference time, the forger can append $\textrm{``by }\word{w}\textrm{''}$ to any prompt to obtain art in the target style

The authors of Glaze identify this finetuning setup as the strongest style mimicry method \citep{glaze}. We validate the success of our style mimicry with a user study detailed in Appendix \ref{sec:mimicrysetupvalidation}


\section{Existing Style Mimicry Protections}
\label{sec:attackappendix}

\paragraph{Naming convention.} Depending on the context, style mimicry protections may be viewed either as attacks or as the targets of attacks. In an artistic setting, artists see style mimicry as an attack and utilize methods like Glaze as a defense. Conversely, in the context of adversarial robustness, Glaze can be seen as an attack against style mimicry methods through adversarial perturbations. The research community has not reached a consensus on terminology: Glaze's authors consider style mimicry an attack and label Glaze as a defense, while the authors of Mist and Anti-DreamBooth describe their approaches as attacks. In our work, we distance ourselves from the attack/defense terminology and instead refer to these mechanisms as protections, and to the party performing mimicry as the ``style forger''.

Existing protections can either target the encoder or the decoder of text-to-image models. We classify them accordingly.

\subsection{Encoder Protections}
\label{sec:encattack}
Encoder protections include adversarial perturbations in the images $\sI$ so that the encoder $\enc$ of the model maps images to latent representations that, when reconstructed, recover images in a different style. Concretely, an encoder protection
first defines a target latent representation $\vt_x \in \type{Latent}$ for each image $x \in \sI$ that is different to its own style. For instance, the target latent representation for Edvard Munch could be Vincent Van Gogh. Then, protection $\attack$ optimizes the objective
\begin{equation}
\label[objective]{obj:encattack}
\begin{gathered}
\min_{\vdelta_x} \f{\latentdist}{\f{\enc}{x + \vdelta_x}, \vt_x} \\
\text{subject to} \quad \f{\imgsimilarity}{x + \vdelta_x, x} \leq p.
\end{gathered}
\end{equation}

\textbf{Glaze}\label{sec:glaze} \citep{glaze} is an instance of an encoder protection. Glaze first selects an adversarial target style $\advstyle{S}$ that style mimicry should learn instead of the style $\style{S}$ to be protected. Then, Glaze uses Img2Img style transfer to create a variation $x_{\advstyle{S}}$ in style $\advstyle{S}$ of each image $x \in \sI$. The latent representation of variation $x_{\advstyle{S}}$ is used
as the target latent representation $\vt_x$ for each image $x \in \sI$.

Glaze selects the target style $\advstyle{S}$ from a pre-defined set of 50 styles $\adv{\sS}$. First, Glaze computes the distance between the mean CLIP embedding of the images $\sI$ and the prompt $P_{S'}$ corresponding to each style $S' \in \adv{\sS}$. Then, Glaze randomly samples target style $\advstyle{S}$ from the 50 to the 75 percentile of target styles $\adv{\sS}$ sorted by distance. 

Glaze implements \Cref{obj:encattack} with the penalty method \citep{penaltymethod} as
\begin{equation}
    \label[objective]{obj:glaze}
    \min_{\vdelta_x} \norm{2}{\f{\enc}{x + \vdelta_x}, \vt_x}^2 + \alpha \cdot \f{\mathrm{max}}{\f{\mathrm{LPIPS}}{x + \vdelta_x, x} - p, 0}
\end{equation}

where LPIPS \citep{LPIPS} is a choice for metric $\imgsimilarity$ that aims to measure user-perceived image distortion. Glaze then optimizes \Cref{obj:glaze} with the Adam \citep{adam} optimizer.

\textbf{\mistenc{}}\label{sec:mistenc} \citep{mist} is a different encoder protection from the Mist project\footnote{Mist project also contains a denoiser attack that we fail to reproduce as a robust protection.}. \mistenc{} optimizes perturbations with PGD to minimize the squared $L_2$-induced distance between the latent representation of the artists' images and some unrelated target image.

In their original work, Mist is only evaluated against DreamBooth, Style Transfer, and Textual Inversion, but not against finetuning. Also, the original Mist work refers to \mistenc{} as Mist operating in \textit{textural mode}.











\subsection{Denoiser Protections}
\label{sec:denattack}
Denoiser protections use the prediction error of the denoiser $\denoiser$ as a proxy of the quality of style mimicry, making it a feasible target for adversarial optimization. Current Denoiser protections, such as Mist \citep{mist} and Anti-DreamBooth \citep{antidreambooth} assume that poorly reconstructed images will fail to mimic style



\textbf{Anti-DreamBooth}\label{sec:antidb} \citep{antidreambooth} uses the prediction error of the denoiser $\advdenoiser$ as a proxy for the mimicry quality, where denoiser $\advdenoiser$ corresponds to the denoiser from a finetuned model trained on images with the style to be protected. Since perturbations maximizing the error with the pretrained decoder can be easily circunvented with finetuning, Anti-DreamBooth uses a technique they refer to as \textit{Alternating Surrogate and Perturbation Learning} (ASPL). The intuition behind ASPL is trying to simulate finetuning on the art and maximizing the error during finetuning. For this purpose, they interleave finetuning steps with perturbation optimization steps.

\section{Robust Mimicry Methods}
\label{sec:defenseappendix}

This section details the robust mimicry methods we use in our work. These methods are not aimed at maximizing performance. Instead, they demonstrate how various "off-the-shelf" and low-effort techniques can significantly weaken style mimicry protections. 

Formally, given protected images {\color{red}$X$} and a pretrained text-to-image model {\color{blue}$f$}, we define a general robust mimicry pipeline that finetunes a model ${\color{blue}\hat{f}}$ and then produces an image {\color{red}$Z$} for a given {\color{brown}\emph{prompt}} as follows (a successful method may not require modifications in all stages):
\begin{align*}
{\color{blue}\hat{f}} \gets \texttt{Finetune}({\color{blue}f}; \texttt{PreProcess}({\color{red}X})) \\
{\color{red}Z} \gets \texttt{PostProcess}(\texttt{Sample}({\color{blue}\hat{f}}, {\color{brown}\mathrm{``prompt"}})).
\end{align*}


\subsection{DiffPure}
\label{sec:diffpure}
DiffPure \citep{diffpure} uses image generation diffusion models to adversarially purify images $\advimgset$. DiffPure processes
each image $\advimg \in \advimgset$ with $t$ timesteps of a diffusion
process to obtain the diffused image $\advimg^{t} = \sqrt{\alpha_t} \cdot \advimg + \sqrt{1 - \alpha_t} \cdot \veps$, where $\alpha$ is the noise schedule of the diffusion process and noise $\veps$ is sampled from $\f{\mathcal{N}}{0, \mI}$. Then, DiffPure constructs the purified image $\f{\mathrm{DiffPure}}{\advimg}$ by applying reverse diffusion to image $\advimg^{t}$ for $t$ timesteps with an image generation diffusion model $\model{DM}$. \citeauthor{diffpure} prove that under certain idealized conditions, DiffPure is likely to weaken adversarial
perturbations in image $\advimg$.

If the text-to-image model $\model{M}$ supports unconditional image generation,
then we can use model $\model{M}$ for the reverse diffusion process. For example, Stable Diffusion \citep{stablediffusion} generates images unconditionally
when the prompt $P$ equals the empty string. Under these conditions, Img2Img is equivalent to DiffPure. Therefore, in the context of defenses for style mimicry,
we refer to Img2Img applied with an empty prompt $P$ as \textit{unconditional DiffPure}, and to Img2Img applied with a non-empty prompt $P$ as \textit{conditional DiffPure}.




\subsection{Noisy Upscaling}
\label{sec:noisyupscaling}

Upscaling increases the resolution of an image by predicting new pixels that enhance the level of detail. Upscaling images can purify adversarially perturbed images \citep{advupscale}. However, we discover that applying upscaling directly on protected images fails to remove the perturbations. 

We define \textit{Noisy Upscaling} as a way to address the shortcomings of upscaling. Noisy Upscaling
first applies Gaussian noising and then upscales the noisy image.
Noisy Upscaling has a more profound effect than the sum of its parts: Gaussian noising only adds noise
to an image $\advimg$, but does not remove the adversarial perturbation $\vdelta_x$. Similarly, we observe upscaling to roughly preserve perturbation $\vdelta_x$. In contrast, $\f{\mathrm{NoisyUpscale}}{\advimg}$ shows neither visually perceptible noise, nor adversarial perturbations. Figure \ref{fig:noisyups} illustrates the improvements. We explain these phenomena as follows.

First, we use the Stable Diffusion Upscaler ($\mathrm{Upscale}_{\text{SD}}$), which is trained on noise-augmented images and accepts the corresponding noise level $L$ as a class-conditioning label. We can therefore condition $\mathrm{Upscale}_{\text{SD}}$ on the noise level $L_{\sigma^2}$, corresponding to the variance $\sigma^2$ used by $\mathrm{GaussianNoising}$, to remove the noise that $\mathrm{GaussianNoising}$ adds.

Second, we note that upscaling has shown success against adversarial perturbations for classifiers \citep{advupscale}, but not against adversarial perturbations for generative models \citep{mist, glaze}. 




\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.195\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/noisyups/0001_nulevoy_clean.jpg}
         \caption{Original artwork}
     \end{subfigure}
     \begin{subfigure}[b]{0.195\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/noisyups/0001_nulevoy_protected.jpg}
         \caption{Protected artwork}
     \end{subfigure}
     \begin{subfigure}[b]{0.195\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/noisyups/0001_nulevoy_upscaled.jpg}
         \caption{Upscaling}
     \end{subfigure}
     \begin{subfigure}[b]{0.195\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/noisyups/0001_nulevoy_upscaled_compressed.jpg}
         \caption{Compr. Upscaling}
     \end{subfigure}
     \begin{subfigure}[b]{0.195\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/noisyups/0001_nulevoy_upscaled_noisy.jpg}
         \caption{\emph{Noisy Upscaling}}
     \end{subfigure}
    \caption{Illustration of Noisy Upscaling on a random image from @nulevoy. Unlike naive upscaling and Compressed Upscaling, Noisy Upscaling removes protections while preserving the details in the original artwork.}
    \label{fig:noisyups}
\end{figure}


\subsection{IMPRESS++}
\label{ap:impressplus}

We enhance the IMPRESS algorithm \citep{impress}. We change the loss of the reverse encoding optimization from patch similarity to $l_\infty$ and include two additional steps: negative prompting and post-processing. All in all, IMPRESS++ first preprocesses protected images with Gaussian noise and reverse encoder optimization, then samples using negative prompting and finally post-processes the generated images with DiffPure to remove noise.

\paragraph{Reverse encoder optimization.}
\textit{Reverse encoder optimization} is a preprocessing defense against encoder protections.
It adds additional perturbations $\sDelta'$ to images $\advimgset$ so that the latent representation $\vt_{\advimg'} = \f{\enc}{\advimg'}$ of each protected image
$\advimg' = \advimg + \vdelta_{\advimg}$ satisfies 
\begin{equation}
    \label{eq:revencoptlat}
    \f{\dec}{\vt_{\advimg'}} \approx \advimg'
\end{equation}
and each perturbation $\vdelta_{\advimg} \in \sDelta'$ satisfies
\begin{equation}
    \label{eq:revencoptimg}
    \f{\imgsimilarity}{\advimg + \vdelta_{\advimg}, \advimg} \leq p.
\end{equation}
If \Cref{eq:revencoptlat} holds, then style mimicry finetuning learns the
style of images $\advimgset'$. 
In addition, the combination of \Cref{eq:revencoptimg} with the image similarity constraint $\f{\imgsimilarity}{x + \vdelta_x, x} \leq p$ in \Cref{obj:encattack} ensures that the defended images $\advimgset'$ look similar to the original images $\sI$. Therefore, style mimicry finetuning on images $\advimgset'$ should learn a style similar to style $\style{S}$.

Reverse encoder optimization aims to achieve \Cref{eq:revencoptlat} and \Cref{eq:revencoptimg} by optimizing the objective
\begin{equation}
\label[objective]{obj:revopt}
\begin{gathered}
\min_{\vdelta_{\advimg}} \f{\latentdist}{\f{\enc}{\advimg + \vdelta_{\advimg}}, \f{\enc}{\advimg}} \\
\text{subject to} \quad \f{\imgsimilarity}{\advimg + \vdelta_{\advimg}, \advimg} \leq p
\end{gathered}
\end{equation}
with PGD.

\paragraph{Negative prompting.}
\label{sec:negprompt}
Negative prompting \citep{miyake2023negative} is a technique to guide image generation of a diffusion-based text-to-image model $\model{M}$ away from a prompt $\negprompt$. To this end, negative prompting manipulates the
classifier-free guidance \citep{classifierfreeguidance}, which computes the denoiser output of model $\model{M}$ as
\begin{equation}
    \label{eq:cfg}
    \f{\cfgdenoiser}{\vz, t, P} = \brace{1+w}\cdot\f{\denoiser}{\vz, t, P} - w\cdot\f{\denoiser}{\vz, t, \text{``''}}
\end{equation}
where parameter $w$ controls the guidance strength.
Negative prompting simply substitutes the empty string ``'' with $\negprompt$ to obtain
\begin{equation}
    \label{eq:negprompt}
    \f{\cfgdenoiser}{\vz, t, P} = \brace{1+w}\cdot\f{\denoiser}{\vz, t, P} - w\cdot\f{\denoiser}{\vz, t, \negprompt}.
\end{equation}

We design a routine for $\defense{In_F}$ that leverages negative prompting to guide model $\model{M}$ away from adversarial generations.
To this end, we first apply Textual Inversion with adversarial images $\advimgset$ to encode the style of adversarial generations $\advstyle{S}$ into a special word $\word{w}$. We then set prompt $\negprompt = $ ``art by $\word{w}$\!''. 

Naive negative prompting offers no strength control. Too little strength may fail to guide model $\model{M}$ away from the adversarial style $\advstyle{S}$. Too much strength may guide towards the style opposite to style $\advstyle{S}$ in the latent space of model $\model{M}$, which is not necessarily the desired style $\style{S}$. We use negative prompt weights \citep{negativepromptweighting} to control the strength of negative prompting. The negative prompt weights technique introduces the strength
control parameter $c$ to interpolate between \Cref{eq:cfg} and \Cref{eq:negprompt} as
\begin{equation}
    \label{eq:negpromptinterp}
    \f{\cfgdenoiser}{\vz, t, P} = \brace{1+w}\cdot\f{\denoiser}{\vz, t, P} - w\cdot\brace{\brace{1+c}\cdot\f{\denoiser}{\vz, t, \negprompt} - c\cdot\f{\denoiser}{\vz, t, \text{``''}}}.
\end{equation}

Figure \ref{fig:impress} illustrates the improvements introduced by each additional step.

\begin{figure}[h]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/impressplus/antidb_nulevoy_revopt.jpeg}
    \caption{Original\\IMPRESS}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/impressplus/antidb_nulevoy_impress_negprompt.jpeg}
    \caption{IMPRESS +\\negative prompting}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/impressplus/antidb_nulevoy_impress_negprompt_img2img.jpeg}
    \caption{\emph{IMPRESS++}. IMPRESS +\\negative prompting + denoising}
    \end{subfigure}
    \hfill
    \caption{Improvements of each additional step in IMPRESS++ over the original IMPRESS~\citep{impress}. Negative prompting improves image consistency and denoising reduces artifacts in generated images.}
    \label{fig:impress}
\end{figure}


\section{Experimental Setup}
\label{ap:experimentalsetup}

This section describes our general experimental setup and specifies the settings and hyperparameters of the methods we use. When possible, we use default values from the machine learning literature. For implementation details see our official repository: \repo


\subsection{Style Mimicry Experimental Details}
\label{sec:stylemimicryconf}

As described in \Cref{sec:attackscenario}, our threat model
considers style mimicry with a latent diffusion text-to-image model $\model{M}$
that is finetuned on a set of images $\sI$ in a style $\style{S}$. This section specifies our choices for model $\model{M}$, images $\sI$, style $\style{S}$, the hyperparameters for finetuning $\model{M}$, and the hyperparameters for generating images with the finetuned model. Where possible, we
try to replicate the style mimicry setup used by \citeauthor{glaze}
to evaluate Glaze, and highlight any differences.

\paragraph{Model} We use Stable Diffusion version 2.1 \citep{sd21}, the same model used to optimize the protections we evaluate \citep{glaze,mist,antidreambooth}. %

\paragraph{Dataset.} We collate 10 image sets $\cbrace{\sI^{A} : A \in \sA}$ from 10 different artists $\sA$.
Each image set $\sI^{A}$ contains 18 images that we choose manually to follow
a consistent style $\style{S}_A$. We select the artists $\sA$ from contemporary and historical artists: We select 5 contemporary artists from ArtStation\footnote{\url{www.artstation.com}} and 5 historical artists from the WikiArt dataset \citep{wikiart}. We found 2 of the 4 artists used by Glaze and included them in our evaluation. We manually select the remaining 8 artists to cover a broad variety of styles. 
Glaze additionally verified that the images of the contemporary artists in their evaluation are not included in the training dataset of the model $\model{M}$.  Unfortunately, the LAION-5B dataset \citep{laion} used to train SD 2.1 was taken offline \citep{laionoffline}, so we are unable to perform this verification. Instead, we verify for each contemporary artist $A \in \sA$ that SD 2.1 is unable to mimic the style $\style{S}_{A}$ by 
manually inspecting SD 2.1 generations for prompts of the form
``An \{object\} by \{artist\}''. We center-crop each image $x$ to $512 \times 512$ pixels and generate a caption $C_x$ for $x$ with the BLIP-2 model \citep{blip2}.

\paragraph{Finetuning hyperparameters.} Glaze does not specify which finetuning script they use, but they claim to ``follow the same training parameters as \citep{stablediffusion}. We use $5\cdot10^{-6}$ learning rate and batch size of 32.'' This batch size misfits their small finetuning image sets that contain no more than 34 images. Moreover, the finetuning code that \citeauthor{glaze} kindly sent us upon request
uses DreamBooth finetuning with Stable Diffusion 1.5, instead of version 2.1 as described in their work.

In light of these discrepancies, and assuming that mimicry protections should be agnostic to the finetuning setup used, we use an ``off-the-shelf'' HuggingFace finetuning script for Stable Diffusion \citep{diffusers} and manually tune hyperparameters for optimal style mimicry before protections are applied. Concretely, we use $2{,}000$ training steps, batch size $4$, learning rate $5\cdot10^{-6}$, and set the remaining hyperparameters to their default values. We pair each image $x$ with the prompt
$P_x = C_x +$`` by $\word{w}$'', where $\word{w} =$ ``nulevoy''\footnote{@nulevoy is the first ArtStation artist that we experimented with. In our experiments, we found ``nulevoy'' a suitable choice for the special word $\word{w}$ and use it for all artists. We check that all of nulevoy's images are published after
the release date of LAION-5B to ensure that SD 2.1 has no prior knowledge about nulevoy's style.}.

\paragraph{Generation hyperparameters} We use the DPM-Solver++(2M) 
 Karras \citep{dpmpp, karras} scheduler for 50 steps to generate images
 of size $768 \times 768$. This scheduler generates images with slightly higher quality than the PNDM \citep{pndm} scheduler used by Glaze.

\subsection{Protections Experimental Details}
\label{sec:attackconf}
We evaluate three different protections: Mist \citep{mist}, Glaze \citep{glaze}, and Anti-DreamBooth \citep{antidreambooth}. For a fair comparison, we fix the perturbation budget $p$ for each adversarial perturbation $\vdelta_x$ created by Mist and Anti-DreamBooth to $p = 8 / 255$, which is the same budget that \citeauthor{mist} use to evaluate Mist. It is not possible to evaluate Glaze with exactly this perturbation budget, for three reasons: First, Glaze uses LPIPS for the image similarity measure $\imgsimilarity$, which does not bound the $L_{\infty}$ norm.
Second, Glaze implements the metric $\imgsimilarity$ as a soft bound in \Cref{obj:glaze}, which offers no hard bound guarantees. Third, Glaze is closed-source software
whose perturbation budget control only offers the settings
\texttt{Default}, \texttt{Medium}, and \texttt{High}. Upon request, the Glaze authors refused to share a codebase where we could control the hyperparameters. Therefore, we evaluate Glaze through their official public tool with
the setting \texttt{High} to evaluate our protections under the highest protections. In our evaluation, we perceive images processed
with Glaze to be equally or less perturbed than images processed with Mist and Anti-DreamBooth.

Next, we describe specific hyperparameters we use to reproduce each of the protections.

\subsubsection{Anti-DreamBooth}
\label{sec:antidreamboothexp}
\citeauthor{antidreambooth} implement Anti-DreamBooth against
DreamBooth finetuning. We adapt their implementation to our vanilla finetuning for style mimicry, using the same hyperparameters where
possible: We set the number of iterations to $N = 50$,
the PGD perturbation budget to $p = 8 / 255$, the PGD step size to $\alpha = 5 \cdot 10^{-3}$, and the number of PGD steps per ASPL iteration to $N_\mathrm{PGD} = 6$. We minimize the loss
$\loss_{\mathrm{Finetune}}$ with the vanilla finetuning setup
in \Cref{sec:stylemimicryconf} for $300$ training steps.

\subsubsection{\mistenc{}}
\label{sec:mistencexp}
We replicate the evaluation that \citeauthor{misttechnical} use to evaluate
\mistenc{} against Stable Diffusion. We set the PGD perturbation budget to $p = 8 / 255$, the number of PGD iterations
to $N_\mathrm{PGD} = 100$, the PGD step size to $\alpha = 1 / 255$, and the target image to $T = \text{Target\_Mist}$ shown in \Cref{fig:targetmist}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\linewidth]{media/Target_Mist.png}
\caption{The Mist target image $\text{Target\_Mist}$. $\text{Target\_Mist}$ is the default target image in the reference Mist implementation and one of the successful target images evaluated by \citeauthor{misttechnical}.}
\label{fig:targetmist}
\end{figure}

\subsubsection{Glaze}
\label{sec:glazeexp}
The Glaze authors were unable to share a codebase upon request. We thus use their publicly released Windows application binary. We use the latest available version of Glaze, v1.1.1. We set \texttt{Intensity} to \texttt{High} and \texttt{Render Quality} to \texttt{Slowest}, to obtain the strongest protections. Appendix \ref{ap:glaze20} includes qualitative results on an updated version released after we concluded our user study.

\subsection{Robust Mimicry Methods Experimental Details}

\subsubsection{Gaussian noising}
\label{sec:gaussiannoisingexp}
We manually tune the Gaussian noising strength to $\sigma_2 = 0.05$. 

\subsubsection{DiffPure}
\label{sec:diffpureexp}
We use conditional DiffPure with the best-performing publicly available image generation diffusion model, Stable Diffusion XL 1.0 (SDXL) \citep{sdxl}. We implement conditional DiffPure using the HuggingFace \texttt{AutoPipelineForImage2Image} pipeline. We use classifier-free guidance scale $\text{\texttt{guidance\_scale}} = 7.5$ with prompt $P = C_x$ for image $x$.
We manually tune the number of diffusion timesteps $t$ via the $\text{\texttt{strength}}$ pipeline argument to $\text{\texttt{strength}} = 0.2$.

\subsubsection{IMPRESS++}

\paragraph{Reverse Optimization}\label{sec:revoptexp} Like \mistenc, we set the PGD perturbation budget to $p = 8 / 255$ and the PGD step size to $\alpha = 1/255$. We manually tune the number of PGD iterations to $N_{\mathrm{PGD}} = 400$.

\paragraph{Noisy Upscaling}\label{sec:noisyupscalingexp} We manually tune the Gaussian noising strength to
$\sigma = 0.1$. We then use the Stable Diffusion Upscaler \footnote{\url{www.huggingface.co/stabilityai/stable-diffusion-x4-upscaler}} with the maximum denoising strength $L$.\footnote{We inadvertently set the denoising strength to $L = 320$ instead of the actual maximum denoising strength $L = 350$. We observe no qualitative difference in the generated images.}.

We note that the Stable Diffusion Upscaler is trained on
diffused images of the form $x_{\alpha} = \sqrt{\alpha} \cdot x + \sqrt{1 - \alpha} \cdot \f{\mathcal{N}}{0, \mI}$. In contrast,
noisy upscaling noises images additively, that is, without the factor $\sqrt{\alpha}$. However, we note that for $\sqrt{1 - \alpha} = \sigma = 0.1$, we have $\sqrt{\alpha} = 0.995 \approx 1$. In practice,
we observe no qualitative difference in the generated images.

\paragraph{Negative Prompting}\label{sec:negpromptexp} We manually tune the negative prompting strength to $c = 0.5$. We use the Stable Diffusion web UI \footnote{\url{https://github.com/AUTOMATIC1111/stable-diffusion-webui}} to apply Textual Inversion on
the adversarial images $\advimgset$. We follow the Textual Inversion
setup used by \citeauthor{mist} to evaluate Mist and set the length of the token vector $\vt$ to $n=8$, the embedding initialization text
to ``style *'', the learning rate to $\gamma = 0.005$, the batch size
to $1$, and the number of training steps to $500$.

\paragraph{$\text{DiffPure}_{\text{post}}$}\label{sec:diffpurepostexp} To make IMPRESS++ work under a single-model availability, we apply $\text{DiffPure}_{\text{post}}$ with the same model that we use for image generation, SD 2.1. We implement $\text{DiffPure}_{\text{post}}$ using the HuggingFace \texttt{AutoPipelineForImage2Image} pipeline. We use the classifier-free guidance scale $\text{\texttt{guidance\_scale}} = 7.5$ with prompt $P = C_x + \text{``, artistic''}$ for image $x$.
We manually tune the number of diffusion timesteps $t$ via the $\text{\texttt{strength}}$ pipeline argument to the value $\text{\texttt{strength}} = 0.2$.

\section{User Study}
\label{sec:userstudy}

This user study was approved by our institution's IRB.

\paragraph{Design.}

Our user study asks annotators to compare outputs from one robust mimicry method against a baseline where images are generated from a model trained on the original art without protections---for a fixed set of prompts $\sP$. 

We present participants with both generations and a gallery with original art in the target style.
We ask participants to decide which image is better in terms of style and quality, separately. For this, we ask them two different questions:
\begin{enumerate}
    \item Based on noise, artifacts, detail, prompt fit, and your impression, which image has higher quality?
    \item Overall, ignoring quality, which image better fits the style of the style samples?
\end{enumerate}

For each comparison, we collect data from 5 users. We randomize several aspects of our study to minimize user bias. We randomly select the order of robust mimicry and baseline generations. Second, we randomly shuffle the order of all image comparisons to prevent all images from the same mimicry method to appear consecutively. Finally, we also randomly sample the seeds that models use to generate images to prevent repeating the same baseline image across different comparisons.

\paragraph{Differences with Glaze's user study.} Our study does not exactly replicate the design of Glaze's user study for two reasons. First, the Glaze study provided annotators with four AI-generated images and four original images, asking if the generated images successfully mimicked the original artwork. This evaluation fails to account for the commonly encountered scenario where current models are incapable of reliably mimicking an artist's style even from unprotected art. Second, we believe the relative assessment recorded in our study (``Which of these two mimicry attempts is more successful?'') is easier for humans than the absolute assessment used in the Glaze study (``Is this mimicry attempt successful'').


\paragraph{Prompts.}\label{ap:prompts}

We curate a small dataset of 10 prompts $\sP$. We design the prompts to satisfy two criteria:
\begin{enumerate}
    \item \textit{The prompts should cover diverse motifs with varying complexity.} This  ensures that we can detect if a scenario compromised the prompt-following capabilities of a style mimicry model.
    \item \textit{The prompts should only include prompts for which our finetuning base model $\model{M}$, SD 2.1, can successfully generate a matching image.}  This reduces the impact of potential human bias against common defects of SD 2.1.
\end{enumerate}
To satisfy criterion 1 and increase variety, we instruct ChatGPT to generate prompt suggestions for four different categories:
\begin{enumerate}
    \item \textit{Simple prompts} with template ``a \{subject\}''.
    \item \textit{Two-entity prompts} with template ``a \{subject\} \{ditransitive verb\} a \{object\}''.
    \item \textit{Entity-attribute prompts} with template ``a \{adjective\} \{subject\}''.
    \item \textit{Entity-scene prompts} with template ``a \{subject\} in a \{scene\}''.
\end{enumerate}
The chat we used to generate our prompts can be accessed at \url{https://chatgpt.com/share/ea3d1290-f137-4131-baca-2fa1c92b3859}. To satisfy criterion 2, we generate images with SD 2.1 on prompts suggested by ChatGPT and manually filter out
prompts with defect generations (e.g. a horse with 6 legs). We populate the final set of prompts $\sP$ with 4
simple prompts, 2 two-entity prompts, 2 entity-attribute prompts,
and 2 entity-scene prompts (see \Cref{fig:prompts}).

\begin{figure}[]
\centering
\begin{lstlisting}[language=Python]
prompts = [
    # simple prompts
    "a mountain",
    "a piano",
    "a shoe",
    "a candle",
    # two-entity prompts
    "a astronaut riding a horse",
    "a shoe with a plant growing inside",
    # entity-attribute prompts
    "a feathered car",
    "a golden apple",
    # entity-scene prompts
    "a castle in the jungle",
    "a village in a thunderstorm",
]
\end{lstlisting}
\caption{Our set of prompts. We manually wrote the prompts ``a astronaut riding a horse'' and ``a village in a thunderstorm''. ChatGPT wrote the remaining prompts.}
\label{fig:prompts}
\end{figure}


\paragraph{Quality control.}

We first run a pilot study where we directly ask users to answer the previous questions about style and quality. This study resulted in very low-quality responses that are barely better than random choice. We enhanced the study to introduce several quality control measures to improve response quality and filter out low-quality annotations:
\begin{enumerate}
    \item We limit our study to desktop users so that images are sufficiently large to perceive artifacts introduced by protections.
    \item We precede the questions we use for our study with four dummy questions about
    the noise, artifacts, detail, and prompt matching of the images. The dummy questions force annotators to pay attention and gather information useful to answer the target questions.
    \item We precede our study with a \emph{training session} that shows for question 1, 2, and each of the four dummy questions an image pair with a clear, objective answer. The training session helps users to understand the study questions. We introduced this stage after gathering valuable feedback for annotators.
    \item We add \emph{control comparisons} to detect annotators who did not understand the tasks or were answering randomly. We generated several images from the baseline model trained on the original art. For each of these images, we created two ablations. For question 1 (quality), we include Gaussian noise to degrade its quality but preserve the same information. For question 2 (style), we apply Img2Img to remove the artist style and map the image back to photorealism using the prompt \textit{``high quality photo, award winning''}. We randomly include control comparisons between the original generations and these ablations, and we only accept labels from users who answered correctly at least 80\% of the control questions. 
\end{enumerate}

\paragraph{Execution.} We execute our study on Amazon Mechanical Turk (MTurk). We design and evaluate an MTurk Human Intelligence Task (HIT) for each artist $A \in \sA$, shown in \Cref{fig:mturkui}. Each HIT includes image pair comparisons for a single artist $A$ under all scenarios $\method \in \sM$, as well 10 quality control image pairs, 10 style control image pairs, and 6 training image pairs. We generate an image pair for each of the 10 prompts and each of 15 scenarios, for a total of $10 \cdot 15 + 10 + 10 + 6 = 176$ image pairs per HIT. We estimate study participants to 
spend 5 minutes on the training image pairs and 30 seconds per remaining image pair, so 90 minutes in total. We compensate
study participants at a rate of $\$16 / \text{hour}$, so $\$24$ per HIT.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{media/mturk_ui_2.png}
    \caption{The interface of our user study.}
    \label{fig:mturkui}
\end{figure}

\subsection{Style Mimicry Setup Validation}
\label{sec:mimicrysetupvalidation}
We execute an additional user study to validate that our style mimicry setup in \Cref{sec:stylemimicryappendix} successfully mimics style from
unprotected images.

For each prompt $P \in \sP$ and artist $A \in \sA$, our validation study uses the baseline model trained on uprotected art to generate one image. Inspired by the evaluation by Glaze \citep{glaze}, we ask participants to evaluate the style mimicry success by answering the question:
\begin{enumerate}
  \item[] How successfully does the style of the image mimic the style of the style samples? Ignore the content and only focus on the style.
\end{enumerate}
To answer this question, we show a participant the image $x_A^\orig{}$ and the images $\sI^{\sA}$ that serve as style samples.
The participant can answer the question on a 5-point Likert scale with options  
\begin{enumerate}
    \item Not successful at all
    \item Not very successful
    \item Somewhat successful
    \item Successful
    \item Very successful
\end{enumerate}

We also execute the style mimicry validation study on MTurk. We design and evaluate a single HIT for all questions, shown in \Cref{fig:mturkuival}. We estimate study participants to spend 15 seconds on each question, and to spend 1 minute to familiarize themselves with a new style, so 35 minutes in total.
We compensate study participants at a rate of \$18/hour, so \$10.50 per HIT.

We find that style mimicry is successful in over 70\% of the comparisons. Results are detailed in Figure \ref{fig:mimicrysucces}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\textwidth}{\import{plots/style_mimicry_success}{plot.pgf}}
    \end{subfigure}
    \caption{User ratings of clean style mimicry success. Each bar indicates the percentage of votes for the corresponding success level for clean style mimicry generations. \Cref{fig:mimicrysuccesperartist} breaks the ratings down by artist.}
    \label{fig:mimicrysucces}
\end{figure}

\begin{figure}[t]
    \centering
    \hspace*{-1.5cm}
    \resizebox{0.6\linewidth}{!}{\begin{subfigure}[t]{\textwidth}{\import{plots/style_mimicry_success_allartists}{plot.pgf}}
    \end{subfigure}}
    \caption{User ratings of clean style mimicry success. Each bar indicates the percentage of votes for the corresponding success level over all clean style mimicry generations for the corresponding artist.}
    \label{fig:mimicrysuccesperartist}
\end{figure}

\begin{figure}[t]
    \centering
    \vspace{-2em}
    \includegraphics[width=\linewidth]{media/mturk_ui_val.png}
    \caption{The interface of our style mimicry setup validation study.}
    \label{fig:mturkuival}
\end{figure}

\clearpage
\section{Compute Resources}
\Cref{tab:computeresources} reports the compute resources for our experiments.

\begin{table}[h!]
\centering
\scriptsize
\caption{Compute resources for our experiments. \emph{Execution time per image / (artist)} reports the execution time of the method to compute a single image, or the combined execution time for all samples of an artist, if the method operates on all samples of an artist at once.
$\dagger$ Google Cloud
$\ddagger$ IMPRESS++ requires an additional 2 seconds per image generation.}
\begin{tabular}{@{}lllrrrr@{}}
\toprule
Method & GPU & CPU & Memory & Storage & \begin{tabular}[c]{@{}l@{}}Execution time per\\ image / (artist)\end{tabular} & Overall execution time \\ \midrule
Finetuning & RTX A6000 & EPYC 7742 & 5 GB & 5 GB & (40 minutes) & 100 hours \\
Image generation & RTX A6000 & EPYC 7742 & 5 GB & 5 GB & 15 seconds & 13 hours \\
Anti-DB & RTX A6000 & EPYC 7742 & 5 GB & 10 GB & 29 minutes & 88 hours \\
Glaze & T4 & 16 vCPUs on GCP$^\dagger$ & 5 GB & 5 GB & 4 minutes & 12 hours \\
Mist & RTX A6000 & EPYC 7742 & 5 GB & 5 GB & 18 seconds & 54 minutes \\
Gaussian noising & None & EPYC 7742 & 0 GB & 0 GB & 143 milliseconds & 26 seconds \\
IMPRESS++ & RTX A6000 & EPYC 7742 & 5 GB & 5 GB & (27 minutes)$^\ddagger$ & 370 minutes$^\ddagger$ \\
DiffPure & RTX A6000 & EPYC 7742 & 7 GB & 7 GB & 48 seconds & 144 minutes \\
Noisy Upscaling & RTX A6000 & EPYC 7742 & 3.5 GB & 3.5 GB & 217 seconds & 651 minutes \\ \bottomrule
\end{tabular}
\label{tab:computeresources}
\end{table}


\end{document}
