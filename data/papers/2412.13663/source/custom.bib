% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{DBLP:journals/corr/abs-2404-06395,
  author       = {Shengding Hu and
                  Yuge Tu and
                  Xu Han and
                  Chaoqun He and
                  Ganqu Cui and
                  Xiang Long and
                  Zhi Zheng and
                  Yewei Fang and
                  Yuxiang Huang and
                  Weilin Zhao and
                  Xinrong Zhang and
                  Zhen Leng Thai and
                  Kai Zhang and
                  Chongyi Wang and
                  Yuan Yao and
                  Chenyang Zhao and
                  Jie Zhou and
                  Jie Cai and
                  Zhongwu Zhai and
                  Ning Ding and
                  Chao Jia and
                  Guoyang Zeng and
                  Dahai Li and
                  Zhiyuan Liu and
                  Maosong Sun},
  title        = {MiniCPM: Unveiling the Potential of Small Language Models with Scalable
                  Training Strategies},
  journal      = {CoRR},
  volume       = {abs/2404.06395},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.06395},
  doi          = {10.48550/ARXIV.2404.06395},
  eprinttype    = {arXiv},
  eprint       = {2404.06395},
  timestamp    = {Mon, 29 Jul 2024 16:18:16 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-06395.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
nguyen2021do,
title={Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth},
author={Thao Nguyen and Maithra Raghu and Simon Kornblith},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=KJNcAkY8tY4}
}

@article{DBLP:journals/corr/abs-1910-08475,
  author       = {Jordan T. Ash and
                  Ryan P. Adams},
  title        = {On the Difficulty of Warm-Starting Neural Network Training},
  journal      = {CoRR},
  volume       = {abs/1910.08475},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.08475},
  eprinttype    = {arXiv},
  eprint       = {1910.08475},
  timestamp    = {Tue, 22 Oct 2019 18:17:16 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-08475.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2405-18392,
  author       = {Alexander H{\"{a}}gele and
                  Elie Bakouch and
                  Atli Kosson and
                  Loubna Ben Allal and
                  Leandro von Werra and
                  Martin Jaggi},
  title        = {Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations},
  journal      = {CoRR},
  volume       = {abs/2405.18392},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.18392},
  doi          = {10.48550/ARXIV.2405.18392},
  eprinttype    = {arXiv},
  eprint       = {2405.18392},
  timestamp    = {Fri, 21 Jun 2024 22:39:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-18392.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/cvpr/Zhai0HB22,
  author       = {Xiaohua Zhai and
                  Alexander Kolesnikov and
                  Neil Houlsby and
                  Lucas Beyer},
  title        = {Scaling Vision Transformers},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
  pages        = {1204--1213},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/CVPR52688.2022.01179},
  doi          = {10.1109/CVPR52688.2022.01179},
  timestamp    = {Tue, 04 Oct 2022 17:56:08 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/Zhai0HB22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/XiongYHZZXZLWL20,
  author       = {Ruibin Xiong and
                  Yunchang Yang and
                  Di He and
                  Kai Zheng and
                  Shuxin Zheng and
                  Chen Xing and
                  Huishuai Zhang and
                  Yanyan Lan and
                  Liwei Wang and
                  Tie{-}Yan Liu},
  title        = {On Layer Normalization in the Transformer Architecture},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {10524--10533},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/xiong20b.html},
  timestamp    = {Fri, 10 Nov 2023 21:09:38 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/XiongYHZZXZLWL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{mambaoutai,
  title={Passing the Torch: Training a Mamba Model for Smooth Handover},
  author={Hallström, Oskar and Taghadouini, Said and Thiriet, Clément and Chaffin, Antoine},
  url={https://www.lighton.ai/blog/lighton-s-blog-4/passing-the-torch-training-a-mamba-model-for-smooth-handover-54},
  year={2024}
}
@inproceedings{DBLP:conf/acl/WangYHYMW24,
  author       = {Liang Wang and
                  Nan Yang and
                  Xiaolong Huang and
                  Linjun Yang and
                  Rangan Majumder and
                  Furu Wei},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {Improving Text Embeddings with Large Language Models},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {11897--11916},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.642},
  doi          = {10.18653/V1/2024.ACL-LONG.642},
  timestamp    = {Tue, 24 Sep 2024 10:55:48 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/WangYHYMW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@misc{xing2018walksgd,
      title={A Walk with SGD}, 
      author={Chen Xing and Devansh Arpit and Christos Tsirigotis and Yoshua Bengio},
      year={2018},
      eprint={1802.08770},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.08770}, 
}

@article{DBLP:journals/corr/abs-2404-05961,
  author       = {Parishad BehnamGhader and
                  Vaibhav Adlakha and
                  Marius Mosbach and
                  Dzmitry Bahdanau and
                  Nicolas Chapados and
                  Siva Reddy},
  title        = {LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},
  journal      = {CoRR},
  volume       = {abs/2404.05961},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.05961},
  doi          = {10.48550/ARXIV.2404.05961},
  eprinttype    = {arXiv},
  eprint       = {2404.05961},
  timestamp    = {Wed, 15 May 2024 08:47:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-05961.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{llmtradeoffs,
  title={The economic trade-offs of large language models: A case study},
  author={Howell, Kristen and Christian, Gwen and Fomitchov, Pavel and Kehat, Gitit and Marzulla, Julianne and Rolston, Leanne and Tredup, Jadin and Zimmerman, Ilana and Selfridge, Ethan and Bradley, Joseph},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)},
  pages={248--267},
  year={2023}
}

@inproceedings{DBLP:conf/emnlp/KarpukhinOMLWEC20,
  author       = {Vladimir Karpukhin and
                  Barlas Oguz and
                  Sewon Min and
                  Patrick S. H. Lewis and
                  Ledell Wu and
                  Sergey Edunov and
                  Danqi Chen and
                  Wen{-}tau Yih},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {Dense Passage Retrieval for Open-Domain Question Answering},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {6769--6781},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.550},
  doi          = {10.18653/V1/2020.EMNLP-MAIN.550},
  timestamp    = {Tue, 20 Aug 2024 07:54:43 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/KarpukhinOMLWEC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/sigir/KhattabZ20,
  author       = {Omar Khattab and
                  Matei Zaharia},
  editor       = {Jimmy X. Huang and
                  Yi Chang and
                  Xueqi Cheng and
                  Jaap Kamps and
                  Vanessa Murdock and
                  Ji{-}Rong Wen and
                  Yiqun Liu},
  title        = {ColBERT: Efficient and Effective Passage Search via Contextualized
                  Late Interaction over {BERT}},
  booktitle    = {Proceedings of the 43rd International {ACM} {SIGIR} conference on
                  research and development in Information Retrieval, {SIGIR} 2020, Virtual
                  Event, China, July 25-30, 2020},
  pages        = {39--48},
  publisher    = {{ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1145/3397271.3401075},
  doi          = {10.1145/3397271.3401075},
  timestamp    = {Mon, 05 Feb 2024 20:27:55 +0100},
  biburl       = {https://dblp.org/rec/conf/sigir/KhattabZ20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/naacl/SanthanamKSPZ22,
  author       = {Keshav Santhanam and
                  Omar Khattab and
                  Jon Saad{-}Falcon and
                  Christopher Potts and
                  Matei Zaharia},
  editor       = {Marine Carpuat and
                  Marie{-}Catherine de Marneffe and
                  Iv{\'{a}}n Vladimir Meza Ru{\'{\i}}z},
  title        = {ColBERTv2: Effective and Efficient Retrieval via Lightweight Late
                  Interaction},
  booktitle    = {Proceedings of the 2022 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL} 2022, Seattle, WA, United States, July 10-15, 2022},
  pages        = {3715--3734},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.naacl-main.272},
  doi          = {10.18653/V1/2022.NAACL-MAIN.272},
  timestamp    = {Mon, 01 Aug 2022 16:28:04 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/SanthanamKSPZ22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/ChenXZLLL24,
  author       = {Jianlyu Chen and
                  Shitao Xiao and
                  Peitian Zhang and
                  Kun Luo and
                  Defu Lian and
                  Zheng Liu},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity
                  Text Embeddings Through Self-Knowledge Distillation},
  booktitle    = {Findings of the Association for Computational Linguistics, {ACL} 2024,
                  Bangkok, Thailand and virtual meeting, August 11-16, 2024},
  pages        = {2318--2335},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.findings-acl.137},
  doi          = {10.18653/V1/2024.FINDINGS-ACL.137},
  timestamp    = {Tue, 24 Sep 2024 10:55:45 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ChenXZLLL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bert,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4171--4186},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1423},
  doi          = {10.18653/V1/N19-1423},
  timestamp    = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{unpadding,
  title={Boosting distributed training performance of the unpadded bert model},
  author={Zeng, Jinle and Li, Min and Wu, Zhihua and Liu, Jiaqi and Liu, Yuang and Yu, Dianhai and Ma, Yanjun},
  journal={arXiv preprint arXiv:2208.08124},
  year={2022}
}

@article{disablelayernormbias,
  title={Understanding and improving layer normalization},
  author={Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{layernorm,
  title={Layer normalization},
  author={Lei Ba, Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={ArXiv e-prints},
  pages={arXiv--1607},
  year={2016}
}

@inproceedings{FA2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023},
}

@article{FA,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@inproceedings{debertav3,
  author       = {Pengcheng He and
                  Jianfeng Gao and
                  Weizhu Chen},
  title        = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with
                  Gradient-Disentangled Embedding Sharing},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=sE7-XhLxHA},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HeGC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{adversarialprompt1,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei W and Ippolito, Daphne and Tramer, Florian and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{adversarialprompt,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{adversarialprompt2,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei W and Ippolito, Daphne and Tramer, Florian and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{phi15,
      title={Textbooks Are All You Need II: phi-1.5 technical report}, 
      author={Yuanzhi Li and Sébastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},
      year={2023},
      eprint={2309.05463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.05463}, 
}

@article{phi2,
  title={Phi-2: The surprising power of small language models},
  author={Javaheripi, Mojan and Bubeck, S{\'e}bastien and Abdin, Marah and Aneja, Jyoti and Bubeck, Sebastien and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Del Giorno, Allie and Eldan, Ronen and Gopi, Sivakanth and others},
  journal={Microsoft Research Blog},
  volume={1},
  number={3},
  pages={3},
  year={2023}
}

@article{phi3,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@inproceedings{gte,
  author       = {Xin Zhang and
                  Yanzhao Zhang and
                  Dingkun Long and
                  Wen Xie and
                  Ziqi Dai and
                  Jialong Tang and
                  Huan Lin and
                  Baosong Yang and
                  Pengjun Xie and
                  Fei Huang and
                  Meishan Zhang and
                  Wenjie Li and
                  Min Zhang},
  editor       = {Franck Dernoncourt and
                  Daniel Preotiuc{-}Pietro and
                  Anastasia Shimorina},
  title        = {mGTE: Generalized Long-Context Text Representation and Reranking Models
                  for Multilingual Text Retrieval},
  booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural
                  Language Processing: {EMNLP} 2024 - Industry Track, Miami, Florida,
                  USA, November 12-16, 2024},
  pages        = {1393--1412},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.emnlp-industry.103},
  timestamp    = {Thu, 21 Nov 2024 17:01:46 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/ZhangZLXDTLYXHZ24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{nomic,
  author       = {Zach Nussbaum and
                  John X. Morris and
                  Brandon Duderstadt and
                  Andriy Mulyar},
  title        = {Nomic Embed: Training a Reproducible Long Context Text Embedder},
  journal      = {CoRR},
  volume       = {abs/2402.01613},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.01613},
  doi          = {10.48550/ARXIV.2402.01613},
  eprinttype    = {arXiv},
  eprint       = {2402.01613},
  timestamp    = {Fri, 09 Feb 2024 12:18:48 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-01613.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{roberta,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 14 Dec 2023 18:03:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{efficientT5,
  title={Scale Efficiently: Insights from Pretraining and Finetuning Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},
  booktitle={International Conference on Learning Representations (ICLR) 22},
  year={2022}
}

@article{codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@article{coir,
  title={Coir: A comprehensive benchmark for code information retrieval models},
  author={Li, Xiangyang and Dong, Kuicai and Lee, Yi Quan and Xia, Wei and Yin, Yichun and Zhang, Hao and Liu, Yong and Wang, Yasheng and Tang, Ruiming},
  journal={arXiv preprint arXiv:2407.02883},
  year={2024}
}

@article{gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Gemma, Team and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{olmo,
  title={Olmo: Accelerating the science of language models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  journal={arXiv preprint arXiv:2402.00838},
  year={2024}
}

@inproceedings{neox,
  title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  author={Black, Sidney and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  booktitle={Proceedings of BigScience Episode\# 5--Workshop on Challenges \& Perspectives in Creating Large Language Models},
  pages={95--136},
  year={2022}
}

@article{silu,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

@article{ruler,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
  year={2024},
  journal={arXiv preprint arXiv:2404.06654},
}

@inproceedings{MNLI,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={1112--1122},
  year={2018}
}

@article{beavertails,
  title   = {BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset},
  author  = {Jiaming Ji and Mickel Liu and Juntao Dai and Xuehai Pan and Chi Zhang and Ce Bian and Chi Zhang and Ruiyang Sun and Yizhou Wang and Yaodong Yang},
  journal = {arXiv preprint arXiv:2307.04657},
  year    = {2023}
}

@misc{wildjailbreak,
      title={WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models}, 
      author={Liwei Jiang and Kavel Rao and Seungju Han and Allyson Ettinger and Faeze Brahman and Sachin Kumar and Niloofar Mireshghallah and Ximing Lu and Maarten Sap and Yejin Choi and Nouha Dziri},
      year={2024},
      eprint={2406.18510},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18510}, 
}


@inproceedings{pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@misc{karpathy,
  title={The most dramatic optimization to nanoGPT so far (~25\% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64).},
  author={Karpathy, Andrej},
  year={2023},
  publisher={Twitter},
  url = {https://x.com/karpathy/status/1621578354024677377}
}

@inproceedings{gliner,
  title={GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer},
  author={Zaratiana, Urchade and Tomeh, Nadi and Holat, Pierre and Charnois, Thierry},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={5364--5376},
  year={2024}
}

@misc{setfit,
  doi = {10.48550/ARXIV.2209.11055},
  url = {https://arxiv.org/abs/2209.11055},
  author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Few-Shot Learning Without Prompts},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{rag,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{shazeerglu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Gemma, Team and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@misc{jacolbertv25,
      title={JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art Japanese Retrievers with Constrained Resources}, 
      author={Benjamin Clavié},
      year={2024},
      eprint={2407.20750},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.20750}, 
}

@article{megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{mosaic,
  author       = {Jacob Portes and
                  Alexander Trott and
                  Sam Havens and
                  Daniel King and
                  Abhinav Venigalla and
                  Moin Nadeem and
                  Nikhil Sardana and
                  Daya Khudia and
                  Jonathan Frankle},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {MosaicBERT: {A} Bidirectional Encoder Optimized for Fast Pretraining},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/095a6917768712b7ccc61acbeecad1d8-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:19 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/PortesTHKVNSKF23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2004-05150,
  author       = {Iz Beltagy and
                  Matthew E. Peters and
                  Arman Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {CoRR},
  volume       = {abs/2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150},
  eprinttype    = {arXiv},
  eprint       = {2004.05150},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Attention is All you Need},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {5998--6008},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/ijon/SuALPBL24,
  author       = {Jianlin Su and
                  Murtadha H. M. Ahmed and
                  Yu Lu and
                  Shengfeng Pan and
                  Wen Bo and
                  Yunfeng Liu},
  title        = {RoFormer: Enhanced transformer with Rotary Position Embedding},
  journal      = {Neurocomputing},
  volume       = {568},
  pages        = {127063},
  year         = {2024},
  url          = {https://doi.org/10.1016/j.neucom.2023.127063},
  doi          = {10.1016/J.NEUCOM.2023.127063},
  timestamp    = {Fri, 26 Jan 2024 07:56:41 +0100},
  biburl       = {https://dblp.org/rec/journals/ijon/SuALPBL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{glu,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05202},
  eprinttype    = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@inproceedings{evor,
  title={EvoR: Evolving Retrieval for Code Generation},
  author={Su, Hongjin and Jiang, Shuyang and Lai, Yuhang and Wu, Haoyuan and Shi, Boao and Liu, Che and Liu, Qian and Yu, Tao},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={2538--2554},
  year={2024}
}

@misc{composer,
    author = {Mosaic ML Team, The},
    title = {composer},
    year = {2021},
    howpublished = {\url{https://github.com/mosaicml/composer/}},
}

@article{qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{niah,
  title={{Needle In A Haystack} - Pressure Testing {LLM}s},
   author={Kamradt, Gregory},
   year={2023},
   journal ={Github},  
   url={https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main},
}

@article{vespalongcolbert,
  title={Announcing Vespa Long-Context {ColBERT}},
   author={Bergum, Jo Kristian},
   year={2024},
   journal ={Vespa Blog},  
   url={https://blog.vespa.ai/announcing-long-context-colbert-in-vespa/},
}

@article{llmsurvey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}


@inproceedings{electra,
  author       = {Kevin Clark and
                  Minh{-}Thang Luong and
                  Quoc V. Le and
                  Christopher D. Manning},
  title        = {{ELECTRA:} Pre-training Text Encoders as Discriminators Rather Than
                  Generators},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=r1xMH1BtvB},
  timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ClarkLLM20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{crammingbert,
  author       = {Jonas Geiping and
                  Tom Goldstein},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Cramming: Training a Language Model on a single {GPU} in one day},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {11117--11143},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/geiping23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/GeipingG23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{alibi,
  author       = {Ofir Press and
                  Noah A. Smith and
                  Mike Lewis},
  title        = {Train Short, Test Long: Attention with Linear Biases Enables Input
                  Length Extrapolation},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=R8sQPpGCv0},
  timestamp    = {Tue, 27 Dec 2022 12:44:40 +0100},
  biburl       = {https://dblp.org/rec/conf/iclr/PressSL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{berticl,
  author       = {David Samuel},
  title        = {BERTs are Generative In-Context Learners},
  journal      = {CoRR},
  volume       = {abs/2406.04823},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.04823},
  doi          = {10.48550/ARXIV.2406.04823},
  eprinttype    = {arXiv},
  eprint       = {2406.04823},
  timestamp    = {Sat, 13 Jul 2024 22:06:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-04823.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lora,
  author       = {Yuhui Xu and
                  Lingxi Xie and
                  Xiaotao Gu and
                  Xin Chen and
                  Heng Chang and
                  Hengheng Zhang and
                  Zhengsu Chen and
                  Xiaopeng Zhang and
                  Qi Tian},
  title        = {QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language
                  Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=WvFoJccpo8},
  timestamp    = {Fri, 23 Aug 2024 13:54:08 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/XuXG0CZC0024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{palm,
  author       = {Aakanksha Chowdhery and
                  Sharan Narang and
                  Jacob Devlin and
                  Maarten Bosma and
                  Gaurav Mishra and
                  Adam Roberts and
                  Paul Barham and
                  Hyung Won Chung and
                  Charles Sutton and
                  Sebastian Gehrmann and
                  Parker Schuh and
                  Kensen Shi and
                  Sasha Tsvyashchenko and
                  Joshua Maynez and
                  Abhishek Rao and
                  Parker Barnes and
                  Yi Tay and
                  Noam Shazeer and
                  Vinodkumar Prabhakaran and
                  Emily Reif and
                  Nan Du and
                  Ben Hutchinson and
                  Reiner Pope and
                  James Bradbury and
                  Jacob Austin and
                  Michael Isard and
                  Guy Gur{-}Ari and
                  Pengcheng Yin and
                  Toju Duke and
                  Anselm Levskaya and
                  Sanjay Ghemawat and
                  Sunipa Dev and
                  Henryk Michalewski and
                  Xavier Garcia and
                  Vedant Misra and
                  Kevin Robinson and
                  Liam Fedus and
                  Denny Zhou and
                  Daphne Ippolito and
                  David Luan and
                  Hyeontaek Lim and
                  Barret Zoph and
                  Alexander Spiridonov and
                  Ryan Sepassi and
                  David Dohan and
                  Shivani Agrawal and
                  Mark Omernick and
                  Andrew M. Dai and
                  Thanumalayan Sankaranarayana Pillai and
                  Marie Pellat and
                  Aitor Lewkowycz and
                  Erica Moreira and
                  Rewon Child and
                  Oleksandr Polozov and
                  Katherine Lee and
                  Zongwei Zhou and
                  Xuezhi Wang and
                  Brennan Saeta and
                  Mark Diaz and
                  Orhan Firat and
                  Michele Catasta and
                  Jason Wei and
                  Kathy Meier{-}Hellstern and
                  Douglas Eck and
                  Jeff Dean and
                  Slav Petrov and
                  Noah Fiedel},
  title        = {PaLM: Scaling Language Modeling with Pathways},
  journal      = {J. Mach. Learn. Res.},
  volume       = {24},
  pages        = {240:1--240:113},
  year         = {2023},
  url          = {https://jmlr.org/papers/v24/22-1144.html},
  timestamp    = {Wed, 11 Sep 2024 14:41:28 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{BEIR,
  author       = {Nandan Thakur and
                  Nils Reimers and
                  Andreas R{\"{u}}ckl{\'{e}} and
                  Abhishek Srivastava and
                  Iryna Gurevych},
  editor       = {Joaquin Vanschoren and
                  Sai{-}Kit Yeung},
  title        = {{BEIR:} {A} Heterogeneous Benchmark for Zero-shot Evaluation of Information
                  Retrieval Models},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html},
  timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/Thakur0RSG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wsdscalinglaw,
  author       = {Alexander H{\"{a}}gele and
                  Elie Bakouch and
                  Atli Kosson and
                  Loubna Ben Allal and
                  Leandro von Werra and
                  Martin Jaggi},
  title        = {Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations},
  journal      = {CoRR},
  volume       = {abs/2405.18392},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.18392},
  doi          = {10.48550/ARXIV.2405.18392},
  eprinttype    = {arXiv},
  eprint       = {2405.18392},
  timestamp    = {Fri, 21 Jun 2024 22:39:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-18392.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{codesigning,
      title={The Case for Co-Designing Model Architectures with Hardware}, 
      author={Quentin Anthony and Jacob Hatef and Deepak Narayanan and Stella Biderman and Stas Bekman and Junqi Yin and Aamir Shafi and Hari Subramoni and Dhabaleswar Panda},
      year={2024},
      eprint={2401.14489},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.14489}, 
}

@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}

@misc{PyLate,
  title={PyLate: Flexible Training and Retrieval for Late Interaction Models},
  author={Chaffin, Antoine and Sourty, Raphaël},
  url={https://github.com/lightonai/pylate},
  year={2024}
}

@InProceedings{pmlr-v70-dauphin17a,
  title = 	 {Language Modeling with Gated Convolutional Networks},
  author =       {Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {933--941},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/dauphin17a.html},
  abstract = 	 {The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al. (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.}
}

@misc{Dayma_DALLE_Mini_2021,
      author = {Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and L\^{e} Kh\u{a}c, Ph\'{u}c and Melas, Luke and Ghosh, Ritobrata},
      doi = {10.5281/zenodo.5146400},
      month = {7},
      title = {DALL·E Mini},
      url = {https://github.com/borisdayma/dalle-mini},
      year = {2021}
}

@inproceedings{treccovid,
  title={TREC-COVID: constructing a pandemic information retrieval test collection},
  author={Voorhees, Ellen and Alam, Tasmeer and Bedrick, Steven and Demner-Fushman, Dina and Hersh, William R and Lo, Kyle and Roberts, Kirk and Soboroff, Ian and Wang, Lucy Lu},
  booktitle={ACM SIGIR Forum},
  volume={54},
  pages={1--12},
  year={2021},
  organization={ACM New York, NY, USA}
}

@article{RepresentationDP,
  title={Representation Degeneration Problem in Training Natural Language Generation Models},
  author={Jun Gao and Di He and Xu Tan and Tao Qin and Liwei Wang and Tie-Yan Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.12009},
  url={https://api.semanticscholar.org/CorpusID:59317065}
}

@article{llmcodesurvey,
  title={A Survey on Large Language Models for Code Generation},
  author={Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
  journal={arXiv preprint arXiv:2406.00515},
  year={2024}
}

@inproceedings{normstab,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@inproceedings{hardnegs,
  title={Hard negative examples are hard, but useful},
  author={Xuan, Hong and Stylianou, Abby and Liu, Xiaotong and Pless, Robert},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XIV 16},
  pages={126--142},
  year={2020},
  organization={Springer}
}

@misc{fineweb-edu,
      title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale}, 
      author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
      year={2024},
      eprint={2406.17557},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17557}, 
}

@misc{mobilellm,
      title={MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases}, 
      author={Zechun Liu and Changsheng Zhao and Forrest Iandola and Chen Lai and Yuandong Tian and Igor Fedorov and Yunyang Xiong and Ernie Chang and Yangyang Shi and Raghuraman Krishnamoorthi and Liangzhen Lai and Vikas Chandra},
      year={2024},
      eprint={2402.14905},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.14905}, 
}

@Misc{xformers,
  author =       {Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza and Luca Wehrstedt and Jeremy Reizenstein and Grigory Sizov},
  title =        {xFormers: A modular and hackable Transformer modelling library},
  howpublished = {\url{https://github.com/facebookresearch/xformers}},
  year =         {2022}
}

@misc{bge,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{e5,
  title={Text embeddings by weakly-supervised contrastive pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

@misc{1minussqrt,
      title={Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations}, 
      author={Alexander Hägele and Elie Bakouch and Atli Kosson and Loubna Ben Allal and Leandro Von Werra and Martin Jaggi},
      year={2024},
      eprint={2405.18392},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.18392}, 
}

@inproceedings{contrastivelearning,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@article{msmarco,
  title={Ms marco: A human generated machine reading comprehension dataset},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}

@misc{improvinglowcomputelanguage,
      title={Improving Low Compute Language Modeling with In-Domain Embedding Initialisation}, 
      author={Charles Welch and Rada Mihalcea and Jonathan K. Kummerfeld},
      year={2020},
      eprint={2009.14109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.14109}, 
}

@misc{mask15maskedlanguage,
      title={Should You Mask 15\% in Masked Language Modeling?}, 
      author={Alexander Wettig and Tianyu Gao and Zexuan Zhong and Danqi Chen},
      year={2023},
      eprint={2202.08005},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.08005}, 
}

@inproceedings{academicbudget,
    title = "How to Train {BERT} with an Academic Budget",
    author = "Izsak, Peter  and
      Berchansky, Moshe  and
      Levy, Omer",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.831",
    doi = "10.18653/v1/2021.emnlp-main.831",
    pages = "10644--10652",
    abstract = "While large language models a la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are competitive with BERT-base on GLUE tasks at a fraction of the original pretraining cost.",
}

@article{flash-attention-3,
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}


@article{debertabaseresults,
  author       = {Rushi Qiang and
                  Ruiyi Zhang and
                  Pengtao Xie},
  title        = {BiLoRA: {A} Bi-level Optimization Framework for Overfitting-Resilient
                  Low-Rank Adaptation of Large Pre-trained Models},
  journal      = {CoRR},
  volume       = {abs/2403.13037},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.13037},
  doi          = {10.48550/ARXIV.2403.13037},
  eprinttype    = {arXiv},
  eprint       = {2403.13037},
  timestamp    = {Mon, 08 Apr 2024 18:24:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-13037.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gpt3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{stableladamw,
      title={Stable and low-precision training for large-scale vision-language models}, 
      author={Mitchell Wortsman and Tim Dettmers and Luke Zettlemoyer and Ari Morcos and Ali Farhadi and Ludwig Schmidt},
      year={2023},
      eprint={2304.13013},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.13013}, 
}

@inproceedings{adamw,
    title={Decoupled Weight Decay Regularization},
    author={Ilya Loshchilov and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@InProceedings{adafactor,
  title = 	 {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author =       {Shazeer, Noam and Stern, Mitchell},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4596--4604},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/shazeer18a/shazeer18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/shazeer18a.html},
  abstract = 	 {In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.}
}

@misc{dataengineering,
      title={Data Engineering for Scaling Language Models to 128K Context}, 
      author={Yao Fu and Rameswar Panda and Xinyao Niu and Xiang Yue and Hannaneh Hajishirzi and Yoon Kim and Hao Peng},
      year={2024},
      eprint={2402.10171},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.10171}, 
}

@misc{prolong,
      title={How to Train Long-Context Language Models (Effectively)}, 
      author={Tianyu Gao and Alexander Wettig and Howard Yen and Danqi Chen},
      year={2024},
      eprint={2410.02660},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02660}, 
}

@InProceedings{Radford2018ImprovingLU,
  author = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskeve},
  title  = {Improving Language Understanding by Generative Pre-Training},
  booktitle="OpenAI Tech Report",
  year   = {2018},
}


@Article{Radford2019,
  author  = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal = {OpenAI Blog},
  title   = {Language Models are Unsupervised Multitask Learners},
  year    = {2019},
}


@inproceedings{react,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {ReAct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=WE\_vluYUL-X},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/YaoZYDSN023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{toolformer,
  author       = {Timo Schick and
                  Jane Dwivedi{-}Yu and
                  Roberto Dess{\`{\i}} and
                  Roberta Raileanu and
                  Maria Lomeli and
                  Eric Hambro and
                  Luke Zettlemoyer and
                  Nicola Cancedda and
                  Thomas Scialom},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/SchickDDRLHZCS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sst2,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 14 Dec 2023 18:03:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1807-03748,
  author       = {A{\"{a}}ron van den Oord and
                  Yazhe Li and
                  Oriol Vinyals},
  title        = {Representation Learning with Contrastive Predictive Coding},
  journal      = {CoRR},
  volume       = {abs/1807.03748},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.03748},
  eprinttype    = {arXiv},
  eprint       = {1807.03748},
  timestamp    = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{ansel2024pytorch,
  title={PyTorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation},
  author={Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and others},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  volume={2},
  pages={929--947},
  year={2024}
}


@misc{longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.05150}, 
}

@misc{gopher,
      title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.11446}, 
}

@misc{seqpack,
      title={Efficient Sequence Packing without Cross-contamination: Accelerating Large Language Models without Impacting Performance}, 
      author={Mario Michael Krell and Matej Kosec and Sergio P. Perez and Andrew Fitzgibbon},
      year={2022},
      eprint={2107.02027},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2107.02027}, 
}

@misc{optimi,
  title={{optimī}: Fast, Modern, Memory Efficient, and Low Precision PyTorch Optimizers},
  author={Warner, Benjamin},
  url={https://github.com/warner-benjamin/optimi},
  year={2023}
}