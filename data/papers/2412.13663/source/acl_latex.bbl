\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Ansel et~al.(2024)Ansel, Yang, He, Gimelshein, Jain, Voznesensky, Bao, Bell, Berard, Burovski et~al.}]{ansel2024pytorch}
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et~al. 2024.
\newblock Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation.
\newblock In \emph{Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems}, volume~2, pages 929--947.

\bibitem[{Anthony et~al.(2024)Anthony, Hatef, Narayanan, Biderman, Bekman, Yin, Shafi, Subramoni, and Panda}]{codesigning}
Quentin Anthony, Jacob Hatef, Deepak Narayanan, Stella Biderman, Stas Bekman, Junqi Yin, Aamir Shafi, Hari Subramoni, and Dhabaleswar Panda. 2024.
\newblock \href {https://arxiv.org/abs/2401.14489} {The case for co-designing model architectures with hardware}.
\newblock \emph{Preprint}, arXiv:2401.14489.

\bibitem[{Ash and Adams(2019)}]{DBLP:journals/corr/abs-1910-08475}
Jordan~T. Ash and Ryan~P. Adams. 2019.
\newblock \href {https://arxiv.org/abs/1910.08475} {On the difficulty of warm-starting neural network training}.
\newblock \emph{CoRR}, abs/1910.08475.

\bibitem[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang et~al.}]{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al. 2023.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}.

\bibitem[{Bajaj et~al.(2016)Bajaj, Campos, Craswell, Deng, Gao, Liu, Majumder, McNamara, Mitra, Nguyen et~al.}]{msmarco}
Payal Bajaj, Daniel Campos, Nick Craswell, Li~Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et~al. 2016.
\newblock Ms marco: A human generated machine reading comprehension dataset.
\newblock \emph{arXiv preprint arXiv:1611.09268}.

\bibitem[{Beltagy et~al.(2020)Beltagy, Peters, and Cohan}]{longformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan. 2020.
\newblock \href {https://arxiv.org/abs/2004.05150} {Longformer: The long-document transformer}.
\newblock \emph{Preprint}, arXiv:2004.05150.

\bibitem[{Bergum(2024)}]{vespalongcolbert}
Jo~Kristian Bergum. 2024.
\newblock \href {https://blog.vespa.ai/announcing-long-context-colbert-in-vespa/} {Announcing vespa long-context {ColBERT}}.
\newblock \emph{Vespa Blog}.

\bibitem[{Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff et~al.}]{pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al. 2023.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In \emph{International Conference on Machine Learning}, pages 2397--2430. PMLR.

\bibitem[{Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang et~al.}]{neox}
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al. 2022.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock In \emph{Proceedings of BigScience Episode\# 5--Workshop on Challenges \& Perspectives in Creating Large Language Models}, pages 95--136.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.
\newblock \href {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html} {Language models are few-shot learners}.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}.

\bibitem[{Chaffin and Sourty(2024)}]{PyLate}
Antoine Chaffin and Raphaël Sourty. 2024.
\newblock \href {https://github.com/lightonai/pylate} {Pylate: Flexible training and retrieval for late interaction models}.

\bibitem[{Chen et~al.(2024)Chen, Xiao, Zhang, Luo, Lian, and Liu}]{DBLP:conf/acl/ChenXZLLL24}
Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.
\newblock \href {https://doi.org/10.18653/V1/2024.FINDINGS-ACL.137} {M3-embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge distillation}.
\newblock In \emph{Findings of the Association for Computational Linguistics, {ACL} 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024}, pages 2318--2335. Association for Computational Linguistics.

\bibitem[{Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur{-}Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier{-}Hellstern, Eck, Dean, Petrov, and Fiedel}]{palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur{-}Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier{-}Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
  2023.
\newblock \href {https://jmlr.org/papers/v24/22-1144.html} {Palm: Scaling language modeling with pathways}.
\newblock \emph{J. Mach. Learn. Res.}, 24:240:1--240:113.

\bibitem[{Clark et~al.(2020)Clark, Luong, Le, and Manning}]{electra}
Kevin Clark, Minh{-}Thang Luong, Quoc~V. Le, and Christopher~D. Manning. 2020.
\newblock \href {https://openreview.net/forum?id=r1xMH1BtvB} {{ELECTRA:} pre-training text encoders as discriminators rather than generators}.
\newblock In \emph{8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net.

\bibitem[{Clavié(2024)}]{jacolbertv25}
Benjamin Clavié. 2024.
\newblock \href {https://arxiv.org/abs/2407.20750} {Jacolbertv2.5: Optimising multi-vector retrievers to create state-of-the-art japanese retrievers with constrained resources}.
\newblock \emph{Preprint}, arXiv:2407.20750.

\bibitem[{Dao(2023)}]{FA2}
Tri Dao. 2023.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}}]{FA}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}. 2022.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:16344--16359.

\bibitem[{Dauphin et~al.(2017)Dauphin, Fan, Auli, and Grangier}]{pmlr-v70-dauphin17a}
Yann~N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017.
\newblock \href {https://proceedings.mlr.press/v70/dauphin17a.html} {Language modeling with gated convolutional networks}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning}, volume~70 of \emph{Proceedings of Machine Learning Research}, pages 933--941. PMLR.

\bibitem[{Dayma et~al.(2021)Dayma, Patil, Cuenca, Saifullah, Abraham, L\^{e}~Kh\u{a}c, Melas, and Ghosh}]{Dayma_DALLE_Mini_2021}
Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Ph\'{u}c L\^{e}~Kh\u{a}c, Luke Melas, and Ritobrata Ghosh. 2021.
\newblock \href {https://doi.org/10.5281/zenodo.5146400} {Dall·e mini}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {https://doi.org/10.18653/V1/N19-1423} {{BERT:} pre-training of deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)}, pages 4171--4186. Association for Computational Linguistics.

\bibitem[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{llama3}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}.

\bibitem[{Fu et~al.(2024)Fu, Panda, Niu, Yue, Hajishirzi, Kim, and Peng}]{dataengineering}
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. 2024.
\newblock \href {https://arxiv.org/abs/2402.10171} {Data engineering for scaling language models to 128k context}.
\newblock \emph{Preprint}, arXiv:2402.10171.

\bibitem[{Gao et~al.(2019)Gao, He, Tan, Qin, Wang, and Liu}]{RepresentationDP}
Jun Gao, Di~He, Xu~Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019.
\newblock \href {https://api.semanticscholar.org/CorpusID:59317065} {Representation degeneration problem in training natural language generation models}.
\newblock \emph{ArXiv}, abs/1907.12009.

\bibitem[{Gao et~al.(2024)Gao, Wettig, Yen, and Chen}]{prolong}
Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2024.
\newblock \href {https://arxiv.org/abs/2410.02660} {How to train long-context language models (effectively)}.
\newblock \emph{Preprint}, arXiv:2410.02660.

\bibitem[{Geiping and Goldstein(2023)}]{crammingbert}
Jonas Geiping and Tom Goldstein. 2023.
\newblock \href {https://proceedings.mlr.press/v202/geiping23a.html} {Cramming: Training a language model on a single {GPU} in one day}.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 11117--11143. {PMLR}.

\bibitem[{Gemma et~al.(2024)Gemma, Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e} et~al.}]{gemma2}
Team Gemma, Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al. 2024.
\newblock Gemma 2: Improving open language models at a practical size.
\newblock \emph{arXiv preprint arXiv:2408.00118}.

\bibitem[{Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang et~al.}]{olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al. 2024.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{arXiv preprint arXiv:2402.00838}.

\bibitem[{H{\"{a}}gele et~al.(2024)H{\"{a}}gele, Bakouch, Kosson, Allal, von Werra, and Jaggi}]{DBLP:journals/corr/abs-2405-18392}
Alexander H{\"{a}}gele, Elie Bakouch, Atli Kosson, Loubna~Ben Allal, Leandro von Werra, and Martin Jaggi. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2405.18392} {Scaling laws and compute-optimal training beyond fixed training durations}.
\newblock \emph{CoRR}, abs/2405.18392.

\bibitem[{Hallström et~al.(2024)Hallström, Taghadouini, Thiriet, and Chaffin}]{mambaoutai}
Oskar Hallström, Said Taghadouini, Clément Thiriet, and Antoine Chaffin. 2024.
\newblock \href {https://www.lighton.ai/blog/lighton-s-blog-4/passing-the-torch-training-a-mamba-model-for-smooth-handover-54} {Passing the torch: Training a mamba model for smooth handover}.

\bibitem[{He et~al.(2023)He, Gao, and Chen}]{debertav3}
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.
\newblock \href {https://openreview.net/forum?id=sE7-XhLxHA} {Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing}.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net.

\bibitem[{Hendrycks and Gimpel(2016)}]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel. 2016.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}.

\bibitem[{Hsieh et~al.(2024)Hsieh, Sun, Kriman, Acharya, Rekesh, Jia, Zhang, and Ginsburg}]{ruler}
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024.
\newblock Ruler: What's the real context size of your long-context language models?
\newblock \emph{arXiv preprint arXiv:2404.06654}.

\bibitem[{Hu et~al.(2024)Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, Zhang, Thai, Zhang, Wang, Yao, Zhao, Zhou, Cai, Zhai, Ding, Jia, Zeng, Li, Liu, and Sun}]{DBLP:journals/corr/abs-2404-06395}
Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen~Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2404.06395} {Minicpm: Unveiling the potential of small language models with scalable training strategies}.
\newblock \emph{CoRR}, abs/2404.06395.

\bibitem[{Husain et~al.(2019)Husain, Wu, Gazit, Allamanis, and Brockschmidt}]{codesearchnet}
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019.
\newblock Codesearchnet challenge: Evaluating the state of semantic code search.
\newblock \emph{arXiv preprint arXiv:1909.09436}.

\bibitem[{Hägele et~al.(2024)Hägele, Bakouch, Kosson, Allal, Werra, and Jaggi}]{1minussqrt}
Alexander Hägele, Elie Bakouch, Atli Kosson, Loubna~Ben Allal, Leandro~Von Werra, and Martin Jaggi. 2024.
\newblock \href {https://arxiv.org/abs/2405.18392} {Scaling laws and compute-optimal training beyond fixed training durations}.
\newblock \emph{Preprint}, arXiv:2405.18392.

\bibitem[{Izsak et~al.(2021)Izsak, Berchansky, and Levy}]{academicbudget}
Peter Izsak, Moshe Berchansky, and Omer Levy. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.831} {How to train {BERT} with an academic budget}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 10644--10652, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[{Javaheripi et~al.(2023)Javaheripi, Bubeck, Abdin, Aneja, Bubeck, Mendes, Chen, Del~Giorno, Eldan, Gopi et~al.}]{phi2}
Mojan Javaheripi, S{\'e}bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C{\'e}sar~Teodoro Mendes, Weizhu Chen, Allie Del~Giorno, Ronen Eldan, Sivakanth Gopi, et~al. 2023.
\newblock Phi-2: The surprising power of small language models.
\newblock \emph{Microsoft Research Blog}, 1(3):3.

\bibitem[{Ji et~al.(2023)Ji, Liu, Dai, Pan, Zhang, Bian, Zhang, Sun, Wang, and Yang}]{beavertails}
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce~Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023.
\newblock Beavertails: Towards improved safety alignment of llm via a human-preference dataset.
\newblock \emph{arXiv preprint arXiv:2307.04657}.

\bibitem[{Jiang et~al.(2024{\natexlab{a}})Jiang, Wang, Shen, Kim, and Kim}]{llmcodesurvey}
Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024{\natexlab{a}}.
\newblock A survey on large language models for code generation.
\newblock \emph{arXiv preprint arXiv:2406.00515}.

\bibitem[{Jiang et~al.(2024{\natexlab{b}})Jiang, Rao, Han, Ettinger, Brahman, Kumar, Mireshghallah, Lu, Sap, Choi, and Dziri}]{wildjailbreak}
Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, and Nouha Dziri. 2024{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2406.18510} {Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models}.
\newblock \emph{Preprint}, arXiv:2406.18510.

\bibitem[{Kamradt(2023)}]{niah}
Gregory Kamradt. 2023.
\newblock \href {https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main} {{Needle In A Haystack} - pressure testing {LLM}s}.
\newblock \emph{Github}.

\bibitem[{Karpathy(2023)}]{karpathy}
Andrej Karpathy. 2023.
\newblock \href {https://x.com/karpathy/status/1621578354024677377} {The most dramatic optimization to nanogpt so far (~25\% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64).}

\bibitem[{Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen, and Yih}]{DBLP:conf/emnlp/KarpukhinOMLWEC20}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.~H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen{-}tau Yih. 2020.
\newblock \href {https://doi.org/10.18653/V1/2020.EMNLP-MAIN.550} {Dense passage retrieval for open-domain question answering}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2020, Online, November 16-20, 2020}, pages 6769--6781. Association for Computational Linguistics.

\bibitem[{Khattab and Zaharia(2020)}]{DBLP:conf/sigir/KhattabZ20}
Omar Khattab and Matei Zaharia. 2020.
\newblock \href {https://doi.org/10.1145/3397271.3401075} {Colbert: Efficient and effective passage search via contextualized late interaction over {BERT}}.
\newblock In \emph{Proceedings of the 43rd International {ACM} {SIGIR} conference on research and development in Information Retrieval, {SIGIR} 2020, Virtual Event, China, July 25-30, 2020}, pages 39--48. {ACM}.

\bibitem[{Krell et~al.(2022)Krell, Kosec, Perez, and Fitzgibbon}]{seqpack}
Mario~Michael Krell, Matej Kosec, Sergio~P. Perez, and Andrew Fitzgibbon. 2022.
\newblock \href {https://arxiv.org/abs/2107.02027} {Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance}.
\newblock \emph{Preprint}, arXiv:2107.02027.

\bibitem[{Lefaudeux et~al.(2022)Lefaudeux, Massa, Liskovich, Xiong, Caggiano, Naren, Xu, Hu, Tintore, Zhang, Labatut, Haziza, Wehrstedt, Reizenstein, and Sizov}]{xformers}
Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. 2022.
\newblock xformers: A modular and hackable transformer modelling library.
\newblock \url{https://github.com/facebookresearch/xformers}.

\bibitem[{Lei~Ba et~al.(2016)Lei~Ba, Kiros, and Hinton}]{layernorm}
Jimmy Lei~Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton. 2016.
\newblock Layer normalization.
\newblock \emph{ArXiv e-prints}, pages arXiv--1607.

\bibitem[{Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel et~al.}]{rag}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al. 2020.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33:9459--9474.

\bibitem[{Li et~al.(2024)Li, Dong, Lee, Xia, Yin, Zhang, Liu, Wang, and Tang}]{coir}
Xiangyang Li, Kuicai Dong, Yi~Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. 2024.
\newblock Coir: A comprehensive benchmark for code information retrieval models.
\newblock \emph{arXiv preprint arXiv:2407.02883}.

\bibitem[{Li et~al.(2023)Li, Bubeck, Eldan, Giorno, Gunasekar, and Lee}]{phi15}
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie~Del Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023.
\newblock \href {https://arxiv.org/abs/2309.05463} {Textbooks are all you need ii: phi-1.5 technical report}.
\newblock \emph{Preprint}, arXiv:2309.05463.

\bibitem[{Liu et~al.(2019{\natexlab{a}})Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/1907.11692} {Roberta: {A} robustly optimized {BERT} pretraining approach}.
\newblock \emph{CoRR}, abs/1907.11692.

\bibitem[{Liu et~al.(2019{\natexlab{b}})Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{sst2}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/1907.11692} {Roberta: {A} robustly optimized {BERT} pretraining approach}.
\newblock \emph{CoRR}, abs/1907.11692.

\bibitem[{Liu et~al.(2024)Liu, Zhao, Iandola, Lai, Tian, Fedorov, Xiong, Chang, Shi, Krishnamoorthi, Lai, and Chandra}]{mobilellm}
Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, and Vikas Chandra. 2024.
\newblock \href {https://arxiv.org/abs/2402.14905} {Mobilellm: Optimizing sub-billion parameter language models for on-device use cases}.
\newblock \emph{Preprint}, arXiv:2402.14905.

\bibitem[{Loshchilov and Hutter(2019)}]{adamw}
Ilya Loshchilov and Frank Hutter. 2019.
\newblock \href {https://openreview.net/forum?id=Bkg6RiCqY7} {Decoupled weight decay regularization}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Mosaic ML~Team(2021)}]{composer}
The Mosaic ML~Team. 2021.
\newblock composer.
\newblock \url{https://github.com/mosaicml/composer/}.

\bibitem[{Nguyen et~al.(2021)Nguyen, Raghu, and Kornblith}]{nguyen2021do}
Thao Nguyen, Maithra Raghu, and Simon Kornblith. 2021.
\newblock \href {https://openreview.net/forum?id=KJNcAkY8tY4} {Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Nussbaum et~al.(2024)Nussbaum, Morris, Duderstadt, and Mulyar}]{nomic}
Zach Nussbaum, John~X. Morris, Brandon Duderstadt, and Andriy Mulyar. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2402.01613} {Nomic embed: Training a reproducible long context text embedder}.
\newblock \emph{CoRR}, abs/2402.01613.

\bibitem[{Penedo et~al.(2024)Penedo, Kydlíček, allal, Lozhkov, Mitchell, Raffel, Werra, and Wolf}]{fineweb-edu}
Guilherme Penedo, Hynek Kydlíček, Loubna~Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro~Von Werra, and Thomas Wolf. 2024.
\newblock \href {https://arxiv.org/abs/2406.17557} {The fineweb datasets: Decanting the web for the finest text data at scale}.
\newblock \emph{Preprint}, arXiv:2406.17557.

\bibitem[{Portes et~al.(2023)Portes, Trott, Havens, King, Venigalla, Nadeem, Sardana, Khudia, and Frankle}]{mosaic}
Jacob Portes, Alexander Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. 2023.
\newblock \href {http://papers.nips.cc/paper\_files/paper/2023/hash/095a6917768712b7ccc61acbeecad1d8-Abstract-Conference.html} {Mosaicbert: {A} bidirectional encoder optimized for fast pretraining}.
\newblock In \emph{Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}.

\bibitem[{Qiang et~al.(2024)Qiang, Zhang, and Xie}]{debertabaseresults}
Rushi Qiang, Ruiyi Zhang, and Pengtao Xie. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2403.13037} {Bilora: {A} bi-level optimization framework for overfitting-resilient low-rank adaptation of large pre-trained models}.
\newblock \emph{CoRR}, abs/2403.13037.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, and Sutskeve}]{Radford2018ImprovingLU}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskeve. 2018.
\newblock Improving language understanding by generative pre-training.
\newblock In \emph{OpenAI Tech Report}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever}]{Radford2019}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}.

\bibitem[{Rae et~al.(2022)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving}]{gopher}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris Jones,
  James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed~Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2022.
\newblock \href {https://arxiv.org/abs/2112.11446} {Scaling language models: Methods, analysis \& insights from training gopher}.
\newblock \emph{Preprint}, arXiv:2112.11446.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}]{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21(140):1--67.

\bibitem[{Reimers and Gurevych(2019)}]{reimers-2019-sentence-bert}
Nils Reimers and Iryna Gurevych. 2019.
\newblock \href {https://arxiv.org/abs/1908.10084} {Sentence-bert: Sentence embeddings using siamese bert-networks}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics.

\bibitem[{Samuel(2024)}]{berticl}
David Samuel. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2406.04823} {Berts are generative in-context learners}.
\newblock \emph{CoRR}, abs/2406.04823.

\bibitem[{Santhanam et~al.(2022)Santhanam, Khattab, Saad{-}Falcon, Potts, and Zaharia}]{DBLP:conf/naacl/SanthanamKSPZ22}
Keshav Santhanam, Omar Khattab, Jon Saad{-}Falcon, Christopher Potts, and Matei Zaharia. 2022.
\newblock \href {https://doi.org/10.18653/V1/2022.NAACL-MAIN.272} {Colbertv2: Effective and efficient retrieval via lightweight late interaction}.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL} 2022, Seattle, WA, United States, July 10-15, 2022}, pages 3715--3734. Association for Computational Linguistics.

\bibitem[{Schick et~al.(2023)Schick, Dwivedi{-}Yu, Dess{\`{\i}}, Raileanu, Lomeli, Hambro, Zettlemoyer, Cancedda, and Scialom}]{toolformer}
Timo Schick, Jane Dwivedi{-}Yu, Roberto Dess{\`{\i}}, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.
\newblock \href {http://papers.nips.cc/paper\_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html} {Toolformer: Language models can teach themselves to use tools}.
\newblock In \emph{Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}.

\bibitem[{Shah et~al.(2024)Shah, Bikshandi, Zhang, Thakkar, Ramani, and Dao}]{flash-attention-3}
Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. 2024.
\newblock Flashattention-3: Fast and accurate attention with asynchrony and low-precision.
\newblock \emph{arXiv preprint arXiv:2407.08608}.

\bibitem[{Shazeer(2020)}]{shazeerglu}
Noam Shazeer. 2020.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}.

\bibitem[{Shazeer and Stern(2018)}]{adafactor}
Noam Shazeer and Mitchell Stern. 2018.
\newblock \href {https://proceedings.mlr.press/v80/shazeer18a.html} {Adafactor: Adaptive learning rates with sublinear memory cost}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pages 4596--4604. PMLR.

\bibitem[{Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro}]{megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}.

\bibitem[{Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu}]{DBLP:journals/ijon/SuALPBL24}
Jianlin Su, Murtadha H.~M. Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.
\newblock \href {https://doi.org/10.1016/J.NEUCOM.2023.127063} {Roformer: Enhanced transformer with rotary position embedding}.
\newblock \emph{Neurocomputing}, 568:127063.

\bibitem[{Tay et~al.(2022)Tay, Dehghani, Rao, Fedus, Abnar, Chung, Narang, Yogatama, Vaswani, and Metzler}]{efficientT5}
Yi~Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung~Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. 2022.
\newblock Scale efficiently: Insights from pretraining and finetuning transformers.
\newblock In \emph{International Conference on Learning Representations (ICLR) 22}.

\bibitem[{Thakur et~al.(2021)Thakur, Reimers, R{\"{u}}ckl{\'{e}}, Srivastava, and Gurevych}]{BEIR}
Nandan Thakur, Nils Reimers, Andreas R{\"{u}}ckl{\'{e}}, Abhishek Srivastava, and Iryna Gurevych. 2021.
\newblock \href {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html} {{BEIR:} {A} heterogeneous benchmark for zero-shot evaluation of information retrieval models}.
\newblock In \emph{Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual}.

\bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Tunstall et~al.(2022)Tunstall, Reimers, Jo, Bates, Korat, Wasserblat, and Pereg}]{setfit}
Lewis Tunstall, Nils Reimers, Unso Eun~Seo Jo, Luke Bates, Daniel Korat, Moshe Wasserblat, and Oren Pereg. 2022.
\newblock \href {https://doi.org/10.48550/ARXIV.2209.11055} {Efficient few-shot learning without prompts}.
\newblock \emph{arXiv preprint}.

\bibitem[{van~den Oord et~al.(2018)van~den Oord, Li, and Vinyals}]{DBLP:journals/corr/abs-1807-03748}
A{\"{a}}ron van~den Oord, Yazhe Li, and Oriol Vinyals. 2018.
\newblock \href {https://arxiv.org/abs/1807.03748} {Representation learning with contrastive predictive coding}.
\newblock \emph{CoRR}, abs/1807.03748.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}]{DBLP:conf/nips/VaswaniSPUJGKP17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock \href {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html} {Attention is all you need}.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pages 5998--6008.

\bibitem[{Voorhees et~al.(2021)Voorhees, Alam, Bedrick, Demner-Fushman, Hersh, Lo, Roberts, Soboroff, and Wang}]{treccovid}
Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William~R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy~Lu Wang. 2021.
\newblock Trec-covid: constructing a pandemic information retrieval test collection.
\newblock In \emph{ACM SIGIR Forum}, volume~54, pages 1--12. ACM New York, NY, USA.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang-etal-2018-glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018.
\newblock \href {https://doi.org/10.18653/v1/W18-5446} {{GLUE}: A multi-task benchmark and analysis platform for natural language understanding}.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}}, pages 353--355, Brussels, Belgium. Association for Computational Linguistics.

\bibitem[{Wang et~al.(2022)Wang, Yang, Huang, Jiao, Yang, Jiang, Majumder, and Wei}]{e5}
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022.
\newblock Text embeddings by weakly-supervised contrastive pre-training.
\newblock \emph{arXiv preprint arXiv:2212.03533}.

\bibitem[{Warner(2023)}]{optimi}
Benjamin Warner. 2023.
\newblock \href {https://github.com/warner-benjamin/optimi} {{optimī}: Fast, modern, memory efficient, and low precision pytorch optimizers}.

\bibitem[{Welch et~al.(2020)Welch, Mihalcea, and Kummerfeld}]{improvinglowcomputelanguage}
Charles Welch, Rada Mihalcea, and Jonathan~K. Kummerfeld. 2020.
\newblock \href {https://arxiv.org/abs/2009.14109} {Improving low compute language modeling with in-domain embedding initialisation}.
\newblock \emph{Preprint}, arXiv:2009.14109.

\bibitem[{Wettig et~al.(2023)Wettig, Gao, Zhong, and Chen}]{mask15maskedlanguage}
Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. 2023.
\newblock \href {https://arxiv.org/abs/2202.08005} {Should you mask 15\% in masked language modeling?}
\newblock \emph{Preprint}, arXiv:2202.08005.

\bibitem[{Williams et~al.(2018)Williams, Nangia, and Bowman}]{MNLI}
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.
\newblock A broad-coverage challenge corpus for sentence understanding through inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pages 1112--1122.

\bibitem[{Wortsman et~al.(2023)Wortsman, Dettmers, Zettlemoyer, Morcos, Farhadi, and Schmidt}]{stableladamw}
Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. 2023.
\newblock \href {https://arxiv.org/abs/2304.13013} {Stable and low-precision training for large-scale vision-language models}.
\newblock \emph{Preprint}, arXiv:2304.13013.

\bibitem[{Xiao et~al.(2023)Xiao, Liu, Zhang, and Muennighoff}]{bge}
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.
\newblock \href {https://arxiv.org/abs/2309.07597} {C-pack: Packaged resources to advance general chinese embedding}.
\newblock \emph{Preprint}, arXiv:2309.07597.

\bibitem[{Xing et~al.(2018)Xing, Arpit, Tsirigotis, and Bengio}]{xing2018walksgd}
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. 2018.
\newblock \href {https://arxiv.org/abs/1802.08770} {A walk with sgd}.
\newblock \emph{Preprint}, arXiv:1802.08770.

\bibitem[{Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu}]{DBLP:conf/icml/XiongYHZZXZLWL20}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie{-}Yan Liu. 2020.
\newblock \href {http://proceedings.mlr.press/v119/xiong20b.html} {On layer normalization in the transformer architecture}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of \emph{Proceedings of Machine Learning Research}, pages 10524--10533. {PMLR}.

\bibitem[{Xu et~al.(2019)Xu, Sun, Zhang, Zhao, and Lin}]{disablelayernormbias}
Jingjing Xu, Xu~Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. 2019.
\newblock Understanding and improving layer normalization.
\newblock \emph{Advances in neural information processing systems}, 32.

\bibitem[{Xuan et~al.(2020)Xuan, Stylianou, Liu, and Pless}]{hardnegs}
Hong Xuan, Abby Stylianou, Xiaotong Liu, and Robert Pless. 2020.
\newblock Hard negative examples are hard, but useful.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XIV 16}, pages 126--142. Springer.

\bibitem[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang et~al.}]{qwen2}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et~al. 2024.
\newblock Qwen2 technical report.
\newblock \emph{arXiv preprint arXiv:2407.10671}.

\bibitem[{Yao et~al.(2023)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao}]{react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik~R. Narasimhan, and Yuan Cao. 2023.
\newblock \href {https://openreview.net/forum?id=WE\_vluYUL-X} {React: Synergizing reasoning and acting in language models}.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net.

\bibitem[{Zaratiana et~al.(2024)Zaratiana, Tomeh, Holat, and Charnois}]{gliner}
Urchade Zaratiana, Nadi Tomeh, Pierre Holat, and Thierry Charnois. 2024.
\newblock Gliner: Generalist model for named entity recognition using bidirectional transformer.
\newblock In \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 5364--5376.

\bibitem[{Zeng et~al.(2022)Zeng, Li, Wu, Liu, Liu, Yu, and Ma}]{unpadding}
Jinle Zeng, Min Li, Zhihua Wu, Jiaqi Liu, Yuang Liu, Dianhai Yu, and Yanjun Ma. 2022.
\newblock Boosting distributed training performance of the unpadded bert model.
\newblock \emph{arXiv preprint arXiv:2208.08124}.

\bibitem[{Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and Beyer}]{DBLP:conf/cvpr/Zhai0HB22}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2022.
\newblock \href {https://doi.org/10.1109/CVPR52688.2022.01179} {Scaling vision transformers}.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pages 1204--1213. {IEEE}.

\bibitem[{Zhang et~al.(2024)Zhang, Zhang, Long, Xie, Dai, Tang, Lin, Yang, Xie, Huang, Zhang, Li, and Zhang}]{gte}
Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. 2024.
\newblock \href {https://aclanthology.org/2024.emnlp-industry.103} {mgte: Generalized long-context text representation and reranking models for multilingual text retrieval}.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: {EMNLP} 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024}, pages 1393--1412. Association for Computational Linguistics.

\bibitem[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et~al.}]{llmsurvey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 2023.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}.

\end{thebibliography}
