\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2023)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou]{related_work}
Aky{\"u}rek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=0g0X4H8yN4I}.

\bibitem[Amos \& Kolter(2017)Amos and Kolter]{amos_optnet_2017}
Amos, B. and Kolter, J.~Z.
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, Shillingford, and de~Freitas]{andrychowicz_learning_2016}
Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M.~W., Pfau, D., Schaul, T.,
  Shillingford, B., and de~Freitas, N.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  2016.

\bibitem[Ba et~al.(2016)Ba, Hinton, Mnih, Leibo, and
  Ionescu]{hinton_fast_weights}
Ba, J., Hinton, G.~E., Mnih, V., Leibo, J.~Z., and Ionescu, C.
\newblock Using fast weights to attend to the recent past.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, 2016.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{bai_deep_2019}
Bai, S., Kolter, J.~Z., and Koltun, V.
\newblock Deep equilibrium models.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Bengio et~al.(1990)Bengio, Bengio, and Cloutier]{bengio_learning_1990}
Bengio, Y., Bengio, S., and Cloutier, J.
\newblock Learning a synaptic learning rule.
\newblock Technical report, Université de Montréal, Département
  d'Informatique et de Recherche opérationnelle, 1990.

\bibitem[Benzing et~al.(2022)Benzing, Schug, Meier, von Oswald, Akram, Zucchet,
  Aitchison, and Steger]{permute2}
Benzing, F., Schug, S., Meier, R., von Oswald, J., Akram, Y., Zucchet, N.,
  Aitchison, L., and Steger, A.
\newblock Random initialisations performing above chance and how to find them.
\newblock \emph{OPT2022: 14th Annual Workshop on Optimization for Machine
  Learning}, 2022.

\bibitem[Bertinetto et~al.(2019)Bertinetto, Henriques, Torr, and
  Vedaldi]{bertinetto_meta-learning_2019}
Bertinetto, L., Henriques, J.~F., Torr, P. H.~S., and Vedaldi, A.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In \emph{International {Conference} on {Learning} {Representations}},
  2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{transformers_few_shot}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
  Zagoruyko]{https://doi.org/10.48550/arxiv.2005.12872}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,
  S.
\newblock End-to-end object detection with transformers.
\newblock In \emph{Computer Vision -- ECCV 2020}. Springer International
  Publishing, 2020.

\bibitem[Chalmers(1991)]{chalmers_evolution_1991}
Chalmers, D.~J.
\newblock The evolution of learning: an experiment in genetic connectionism.
\newblock In Touretzky, D.~S., Elman, J.~L., Sejnowski, T.~J., and Hinton,
  G.~E. (eds.), \emph{Connectionist {Models}}, pp.\  81--90. Morgan Kaufmann,
  1991.

\bibitem[Chan et~al.(2022{\natexlab{a}})Chan, Dasgupta, Kim, Kumaran, Lampinen,
  and Hill]{Trans_generalize_differently_weights}
Chan, S. C.~Y., Dasgupta, I., Kim, J., Kumaran, D., Lampinen, A.~K., and Hill,
  F.
\newblock Transformers generalize differently from information stored in
  context vs in weights.
\newblock \emph{arXiv preprint arXiv:2210.05675}, 2022{\natexlab{a}}.

\bibitem[Chan et~al.(2022{\natexlab{b}})Chan, Santoro, Lampinen, Wang, Singh,
  Richemond, McClelland, and Hill]{data_dis_in_context}
Chan, S. C.~Y., Santoro, A., Lampinen, A.~K., Wang, J.~X., Singh, A.,
  Richemond, P.~H., McClelland, J., and Hill, F.
\newblock Data distributional properties drive emergent in-context learning in
  transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{choromanski2021rethinking}
Choromanski, K.~M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos,
  T., Hawkins, P., Davis, J.~Q., Mohiuddin, A., Kaiser, L., Belanger, D.~B.,
  Colwell, L.~J., and Weller, A.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Ua6zuk0WRH}.

\bibitem[Dai et~al.(2023)Dai, Sun, Dong, Hao, Ma, Sui, and Wei]{dai2023why}
Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F.
\newblock Why can {GPT} learn in-context? language models implicitly perform
  gradient descent as meta-optimizers.
\newblock In \emph{ICLR 2023 Workshop on Mathematical and Empirical
  Understanding of Foundation Models}, 2023.
\newblock URL \url{https://openreview.net/forum?id=fzbHRjAd8U}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{transformers_vision}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Entezari et~al.(2021)Entezari, Sedghi, Saukh, and Neyshabur]{permute1}
Entezari, R., Sedghi, H., Saukh, O., and Neyshabur, B.
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.06296}, 2021.

\bibitem[Finn \& Levine(2018)Finn and Levine]{finn2018metalearning_universal}
Finn, C. and Levine, S.
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HyjC5yWCW}.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn_model-agnostic_2017}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2017.

\bibitem[Flennerhag et~al.(2020)Flennerhag, Rusu, Pascanu, Visin, Yin, and
  Hadsell]{flennerhag2020metalearning}
Flennerhag, S., Rusu, A.~A., Pascanu, R., Visin, F., Yin, H., and Hadsell, R.
\newblock Meta-learning with warped gradient descent.
\newblock In \emph{{International Conference on Learning Representations}},
  2020.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{simple_case_study}
Garg, S., Tsipras, D., Liang, P., and Valiant, G.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=flNZJ2eOet}.

\bibitem[Gordon et~al.(2019)Gordon, Bronskill, Bauer, Nowozin, and
  Turner]{DBLP:conf/iclr/GordonBBNT19}
Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R.
\newblock Meta-learning probabilistic inference for prediction.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HkxStoC5F7}.

\bibitem[Gould et~al.(2021)Gould, Hartley, and Campbell]{gould_deep_2021}
Gould, S., Hartley, R., and Campbell, D.~J.
\newblock Deep declarative networks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[Gulati et~al.(2020)Gulati, Qin, Chiu, Parmar, Zhang, Yu, Han, Wang,
  Zhang, Wu, and Pang]{convormer}
Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang,
  S., Zhang, Z., Wu, Y., and Pang, R.
\newblock Conformer: Convolution-augmented transformer for speech recognition.
\newblock \emph{arXiv preprint arXiv:2005.08100}, 2020.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{gelu}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Hinton \& Plaut(1987)Hinton and Plaut]{Hinton1987UsingFW}
Hinton, G.~E. and Plaut, D.~C.
\newblock Using fast weights to deblur old memories.
\newblock 1987.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{meta_hochreiter}
Hochreiter, S., Younger, A.~S., and Conwell, P.~R.
\newblock Learning to learn using gradient descent.
\newblock In Dorffner, G., Bischof, H., and Hornik, K. (eds.), \emph{Artificial
  Neural Networks --- ICANN 2001}, pp.\  87--94, Berlin, Heidelberg, 2001.
  Springer Berlin Heidelberg.
\newblock ISBN 978-3-540-44668-2.

\bibitem[Hubinger et~al.(2019)Hubinger, van Merwijk, Mikulik, Skalse, and
  Garrabrant]{mesa}
Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S.
\newblock Risks from learned optimization in advanced machine learning systems.
\newblock \emph{arXiv [cs.AI]}, Jun 2019.
\newblock URL \url{http://arxiv.org/abs/1906.01820}.

\bibitem[Irie et~al.(2021)Irie, Schlag, Csordás, and
  Schmidhuber]{irie2021going}
Irie, K., Schlag, I., Csordás, R., and Schmidhuber, J.
\newblock Going beyond linear transformers with recurrent fast weight
  programmers.
\newblock \emph{CoRR}, abs/2106.06295, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.06295}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization, 2014.

\bibitem[Kirsch \& Schmidhuber(2021)Kirsch and Schmidhuber]{kirsch2021meta}
Kirsch, L. and Schmidhuber, J.
\newblock Meta learning backpropagation and improving it.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=hhU9TEvB6AF}.

\bibitem[Kirsch et~al.(2022)Kirsch, Harrison, Sohl-Dickstein, and
  Metz]{kirsch2022generalpurpose}
Kirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L.
\newblock General-purpose in-context learning by meta-learning transformers.
\newblock In \emph{Sixth Workshop on Meta-Learning at the Conference on Neural
  Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=t6tA-KB4dO}.

\bibitem[Lee et~al.(2019)Lee, Maji, Ravichandran, and Soatto]{meta_opt_net}
Lee, K., Maji, S., Ravichandran, A., and Soatto, S.
\newblock Meta-learning with differentiable convex optimization.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2019.

\bibitem[Lee \& Choi(2018)Lee and Choi]{lee2018gradient}
Lee, Y. and Choi, S.
\newblock Gradient-based meta-learning with learned layerwise metric and
  subspace.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2018.

\bibitem[Li et~al.(2017)Li, Zhou, Chen, and Li]{DBLP:journals/corr/LiZCL17}
Li, Z., Zhou, F., Chen, F., and Li, H.
\newblock {Meta-SGD}: Learning to learn quickly for few shot learning.
\newblock \emph{arXiv preprint arXiv:1707.09835}, 2017.

\bibitem[Liu et~al.(2021)Liu, Yuan, Fu, Jiang, Hayashi, and
  Neubig]{pre_train_prompt}
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{arXiv preprint arXiv:2107.13586}, 2021.

\bibitem[Nadaraya(1964)]{nadaraya1964estimating}
Nadaraya, E.~A.
\newblock On estimating regression.
\newblock \emph{Theory of Probability \& its Applications}, 9\penalty0
  (1):\penalty0 141--142, 1964.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez,
  Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{induction_heads}
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T.,
  Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D.,
  Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J.,
  Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J.,
  McCandlish, S., and Olah, C.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Park \& Oliva(2019)Park and Oliva]{DBLP:conf/nips/ParkO19}
Park, E. and Oliva, J.~B.
\newblock Meta-curvature.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and
  Misra]{grocking}
Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock abs/2201.02177, 2022.

\bibitem[Raghu et~al.(2020)Raghu, Raghu, Bengio, and Vinyals]{raghu_rapid_2020}
Raghu, A., Raghu, M., Bengio, S., and Vinyals, O.
\newblock Rapid learning or feature reuse? {Towards} understanding the
  effectiveness of {MAML}.
\newblock In \emph{{International Conference on Learning Representations}},
  2020.

\bibitem[Ramsauer et~al.(2020)Ramsauer, Schäfl, Lehner, Seidl, Widrich, Adler,
  Gruber, Holzleitner, Pavlović, Sandve, Greiff, Kreil, Kopp, Klambauer,
  Brandstetter, and Hochreiter]{hopfield}
Ramsauer, H., Schäfl, B., Lehner, J., Seidl, P., Widrich, M., Adler, T.,
  Gruber, L., Holzleitner, M., Pavlović, M., Sandve, G.~K., Greiff, V., Kreil,
  D., Kopp, M., Klambauer, G., Brandstetter, J., and Hochreiter, S.
\newblock Hopfield networks is all you need.
\newblock \emph{arXiv preprint arXiv:2008.02217}, 2020.

\bibitem[Rusu et~al.(2019)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero, and
  Hadsell]{rusu2019metalearning}
Rusu, A.~A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S.,
  and Hadsell, R.
\newblock Meta-learning with latent embedding optimization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and
  Schmidhuber]{linear_transformers_fast_weight}
Schlag, I., Irie, K., and Schmidhuber, J.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In \emph{ICML}, 2021.

\bibitem[Schmidhuber(1987)]{schmidhuber_evolutionary_1987}
Schmidhuber, J.
\newblock \emph{Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock Diploma thesis, Institut für Informatik, Technische Universität
  München, 1987.

\bibitem[Schmidhuber(1992)]{fast_weights}
Schmidhuber, J.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock \emph{Neural Computation}, 4\penalty0 (1):\penalty0 131--139, 1992.
\newblock \doi{10.1162/neco.1992.4.1.131}.

\bibitem[Thrun \& Pratt(1998)Thrun and Pratt]{thrun_learning_1998}
Thrun, S. and Pratt, L.
\newblock \emph{Learning to learn}.
\newblock Springer US, 1998.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformers}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need, 2017.

\bibitem[von Oswald et~al.(2021)von Oswald, Zhao, Kobayashi, Schug, Caccia,
  Zucchet, and Sacramento]{von_oswald_learning_2021}
von Oswald, J., Zhao, D., Kobayashi, S., Schug, S., Caccia, M., Zucchet, N.,
  and Sacramento, J.
\newblock Learning where to learn: {Gradient} sparsity in meta and continual
  learning.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  2021.

\bibitem[Watson(1964)]{watson1964smooth}
Watson, G.~S.
\newblock Smooth regression analysis.
\newblock \emph{Sankhy{\=a}: The Indian Journal of Statistics, Series A}, pp.\
  359--372, 1964.

\bibitem[Widrow \& Hoff(1960)Widrow and Hoff]{widrow:switching}
Widrow, B. and Hoff, M.~E.
\newblock Adaptive switching circuits.
\newblock In \emph{1960 {IRE} {WESCON} Convention Record, Part 4}, pp.\
  96--104, New York, 1960. {IRE}.

\bibitem[Yun et~al.(2019)Yun, Jeong, Kim, Kang, and Kim]{NEURIPS2019_9d63484a}
Yun, S., Jeong, M., Kim, R., Kang, J., and Kim, H.~J.
\newblock Graph transformer networks.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\' Alch\'{e}-Buc,
  F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems}, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Lipton, Li, and Smola]{zhang2021dive}
Zhang, A., Lipton, Z.~C., Li, M., and Smola, A.~J.
\newblock Dive into deep learning.
\newblock \emph{arXiv preprint arXiv:2106.11342}, 2021.

\bibitem[Zhao et~al.(2020)Zhao, Kobayashi, Sacramento, and von
  Oswald]{zhao_meta_learning_hypernetworks}
Zhao, D., Kobayashi, S., Sacramento, J., and von Oswald, J.
\newblock Meta-learning via hypernetworks.
\newblock In \emph{{NeurIPS Workshop on Meta-Learning}}, 2020.

\bibitem[Zhmoginov et~al.(2022)Zhmoginov, Sandler, and
  Vladymyrov]{hypertransformer}
Zhmoginov, A., Sandler, M., and Vladymyrov, M.
\newblock {H}yper{T}ransformer: Model generation for supervised and
  semi-supervised few-shot learning.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  27075--27098. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/zhmoginov22a.html}.

\bibitem[Zucchet \& Sacramento(2022)Zucchet and
  Sacramento]{zucchet_beyond_2022}
Zucchet, N. and Sacramento, J.
\newblock Beyond backpropagation: bilevel optimization through implicit
  differentiation and equilibrium propagation.
\newblock \emph{Neural Computation}, 34\penalty0 (12), December 2022.

\end{thebibliography}
