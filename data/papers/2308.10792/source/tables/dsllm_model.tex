\begin{table*}[t]
\centering
\small
\scalebox{0.85}{
\begin{threeparttable}
\begin{tabular}{lllll}
% p{5cm}p{2cm}p{5cm}p{3cm}
\toprule
 \multirow{1}{*}{\bf Domain Type} &  {\bf Domain-specific Instruction} & \multicolumn{2}{c}{\bf Base Model} & \multirow{2}{*}{\bf Trainset Size}\\
& {\bf Fine-tuned LLMs} & {\bf Model Name} & {\bf \# Params} \\\midrule
 \multirow{1}{*}{Dialogue} & \multirow{1}{*}{{InstructDial}~\citep{Gupta2022InstructDialIZ}\tnotex{id:1}}  & T0~\citep{sanh2021multitask} & 3B & \multirow{2}{*}{-} \\
Classification & {{LINGUIST}~\citep{Rosenbaum2022LINGUISTLM}}  & AlexaTM~\citep{soltan2022alexatm} & 5B &  13K \\
Information extraction & {InstructUIE}~\citep{Wang2023InstructUIEMI}\tnotex{id:2} & FlanT5~\citep{Chung2022ScalingIL} & 11B & 1.0M \\

Sentiment analysis & IT-MTL~\citep{Varia2022InstructionTF}\tnotex{id:3} &  T5~\citep{Raffel2019ExploringTL} & 220M & - \\
\midrule
\multirow{3}{*}{\shortstack{ Writing}}   & {Writing-Alpaca-7B}~\citep{Zhang2023MultiTaskIT}\tnotex{id:4} & LLaMA~\citep{Touvron2023LLaMAOA} & 7B & - \\

& \multirow{1}{*}
{{CoEdIT}~\citep{Raheja2023CoEdITTE}\tnotex{id:5}}  & FlanT5~\citep{Chung2022ScalingIL} & 11B & \\

& \multirow{1}{*}
{{CoPoet}~\citep{Chakrabarty2022HelpMW}\tnotex{id:6}}  & T5~\citep{Raffel2019ExploringTL} & 11B & \\
\midrule 
\multirow{3}{*}{\shortstack{ Medical}}   & {Radiology-GPT}~\citep{Liu2023RadiologyGPTAL}\tnotex{id:7} & Alpaca~\citep{taori2023alpaca} & 7B &  122K \\

 & {ChatDoctor}~\citep{Li2023ChatDoctorAM}\tnotex{id:8} & LLaMA~\citep{Touvron2023LLaMAOA} & 7B &  100K \\
 & {ChatGLM-Med}~\citep{ChatGLM-Med}\tnotex{id:9} & ChatGLM~\citep{du2022glm} & 6B & - \\
\midrule
Arithmetic& {Goat}~\citep{liu2023goat}\tnotex{id:10}  & LLaMA~\citep{Touvron2023LLaMAOA} & 7B & 1.0M \\

Code & {WizardCoder}~\citep{luo2023wizardcoder}\tnotex{id:11}  & StarCoder~\citep{li2023starcoder} & 15B &  78K \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\begin{multicols}{2}
\begin{tablenotes}
\item[1] \label{id:1} {$^1$ https://github.com/prakharguptaz/Instructdial} 
\item[2] \label{id:2} {$^2$ https://github.com/BeyonderXX/InstructUIE}
\item[3] \label{id:3} {$^3$ https://github.com/amazon-science/instruction-tuning-for-absa}
\item[4] \label{id:4} {$^4$ https://github.com/facebookresearch/EditEval}
\item[5] \label{id:5} {$^5$ https://github.com/vipulraheja/coedit}
\item[6] \label{id:6} {$^6$ https://github.com/vishakhpk/creative-instructions}
\item[7] \label{id:7} {$^7$ https://huggingface.co/spaces/allen-eric/radiology-gpt}
\item[8] \label{id:8} {$^8$ https://github.com/Kent0n-Li/ChatDoctor}
\item[9] \label{id:9} {$^9$ https://github.com/SCIR-HI/Med-ChatGLM}
\item[10] \label{id:10} {$^{10}$ https://github.com/liutiedong/goat}
\item[11] \label{id:11} {$^{11}$ https://github.com/nlpxucan/WizardLM}
\end{tablenotes}
\end{multicols}
\caption{An overview of domain-specific instruction fine-tuned LLMs. }
\label{tab:mmllms_model_table}
\end{table*}