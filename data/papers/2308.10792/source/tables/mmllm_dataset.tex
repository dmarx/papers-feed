\begin{table*}[t]
\centering
\small
\scalebox{0.85}{
\begin{threeparttable}
\begin{tabular}{llll}
% p{5cm}p{2cm}p{5cm}p{3cm}
\toprule
\multirow{2}{*}{\bf Multi-modality Instruction Fine-tuning Dataset} &  \multicolumn{2}{c}{\bf Modalities} & \multirow{2}{*}{\bf \# Tasks} \\
& {\bf Modality Pair} & {\bf \# Instance} & \\\midrule
{MUL-TIINSTRUCT}~\citep{Xu2022MultiInstructIM}\tnotex{id:1} & Image-Text & 5k to 5M per task & 62 \\
{PMC-VQA}~\citep{Zhang2023PMCVQAVI}\tnotex{id:2} & Image-Text & 227k & 2 \\
\multirow{2}{*}{{LAMM}~\citep{Yin2023LAMMLM}\tnotex{id:3}} & Image-Text & 186k & 9 \\
& Point Cloud-Text & 10k & 3 \\
{Vision-Flan}~\citep{xu2024vision}\tnotex{id:4} & Multi-Pairs & Over 1M & 200+ \\
{ALLAVA}~\citep{chen2024allava}\tnotex{id:5} & Image-Text & 1.4M & 2 \\
{ShareGPT4V}~\citep{chen2023sharegpt4v}\tnotex{id:6} & Image-Text & 1.2M & 2 \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\begin{multicols}{2}
\begin{tablenotes}
\item[1] \label{id:1} {$^1$ https://github.com/VT-NLP/MultiInstruct} 
\item[2] \label{id:2} {$^2$ https://github.com/xiaoman-zhang/PMC-VQA}
\item[3] \label{id:3} {$^3$ https://github.com/OpenLAMM/LAMM}
\item[4] \label{id:4} {$^4$ https://vision-flan.github.io/}
\item[5] \label{id:5} {$^5$ https://github.com/FreedomIntelligence/ALLaVA}
\item[6] \label{id:6} {$^6$ https://sharegpt4v.github.io/}
\end{tablenotes}
\end{multicols}
\caption{An overview of multi-modality instruction fine-tuning datasets.}
\label{tab:mmllms_dataset_table}
\end{table*}