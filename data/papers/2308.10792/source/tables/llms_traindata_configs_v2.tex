\begin{table*}[t]
\centering
\small
%\scalebox{0.75}{
\begin{adjustbox}{max width=0.9\textwidth}
\begin{threeparttable}
%\scalebox{0.75}{
\begin{tabular}{p{3.8cm}|lccccc}
\toprule 
{\bf Type} & {\bf Dataset Name} & {\bf \# of Instances} & {\bf \# of Lang} & {\bf Construction} & {\bf Open-source} \\\midrule

% & {\bf Corpus Name} & {\bf \# of Instructions} & {\bf \# of Tasks} & {\bf # of Lang} & {\bf Construction} & {\bf Open-source}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\multirow{13}{*}{\bf Human-Crafted} & UnifiedQA~\citep{khashabi2020unifiedqa}\tnotex{id:1} & 750K & En & human-crafted & Yes\\
%& T0~\citep{sanh2021multitask} &  {\bf \# of Instructions} & {\bf \# of Tasks} & {\bf # of Lang} & {\bf Construction} & {\bf Open-source}\\
& UnifiedSKG~\citep{Xie2022UnifiedSKGUA}\tnotex{id:3} & 0.8M & En & human-crafted & Yes\\
& Natural Instructions~\citep{honovich2022unnatural}\tnotex{id:4} & 193K & En  & human-crafted & Yes \\
& Super-Natural Instructions~\citep{supernaturalinstructions}\tnotex{id:5} & 5M & 55 Lang & human-crafted & Yes \\
& P3~\citep{sanh2021multitask}\tnotex{id:6} & 12M & En & human-crafted & Yes \\
& xP3~\citep{muennighoff2022crosslingual}\tnotex{id:7} & 81M & 46 Lang & human-crafted & Yes \\
& Flan 2021~\citep{longpre2023flan}\tnotex{id:8} & 4.4M & En & human-crafted & Yes \\
& COIG~\citep{Zhang2023ChineseOI}\tnotex{id:9} & - & - & - & Yes \\
&  InstructGPT~\citep{ouyang2022training} & 13K & Multi & human-crafted & No \\
& Dolly~\citep{conover2023free}\tnotex{id:16} & 15K & En & human-crafted & Yes \\
& LIMA~\citep{Zhou2023LIMALI}\tnotex{id:18} & 1K & En & human-crafted & Yes \\
& ChatGPT~\citep{chatgpt} & - & Multi & human-crafted & No \\
& OpenAssistant~\citep{kopf2023openassistant}\tnotex{id:20} & 161,443 & Multi & human-crafted & Yes \\
\midrule 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\multirow{20}{*}{\shortstack{\bf Synthetic Data \\ \bf (Distillation)}} & OIG~\citep{2023oig}\tnotex{id:2} & 43M & En & ChatGPT (No technique reports) & Yes \\
& Unnatural Instructions~\citep{honovich2022unnatural}\tnotex{id:10} & 240K & En & InstructGPT-Generated & Yes \\
& InstructWild~\citep{instructionwild}\tnotex{id:12} & 104K & - & ChatGPT-Generated & Yes\\
& Evol-Instruct / WizardLM~\citep{xu2023wizardlm}\tnotex{id:13} & 52K & En & ChatGPT-generated & Yes \\
& Alpaca~\citep{taori2023alpaca}\tnotex{id:14}  & 52K & En & InstructGPT-generated & Yes \\
& LogiCoT~\citep{Liu2023LogiCoTLC}\tnotex{id:15} & - & En & GPT-4-Generated & Yes \\
& GPT-4-LLM~\citep{peng2023instruction}\tnotex{id:17} & 52K & En\&Zh & GPT-4-Generated & Yes \\
& Vicuna~\citep{chiang2023vicuna} & 70K & En & Real User-ChatGPT Conversations & No \\
& Baize v1~\citep{DatabricksBlog2023DollyV2}\tnotex{id:21} & 111.5K & En & ChatGPT-Generated & Yes \\
& UltraChat~\citep{ding2023enhancing}\tnotex{id:22} & 675K & En\&Zh & GPT 3/4-Generated & Yes \\
& Guanaco~\citep{Guanaco}\tnotex{id:19} & 534,530 & Multi & GPT (Unknown Version)-Generated & Yes \\
& Orca~\citep{mukherjee2023orca}\tnotex{id:23} & ~1.5M & En & GPT 3.5/4-Generated & Yes \\
& ShareGPT\tnotex{id:24} & 90K & Multi & Real User-ChatGPT Conversations & Yes \\
& WildChat\tnotex{id:25} & 150K & Multi & Real User-ChatGPT Conversations & Yes \\
& WizardCoder~\citep{luo2023wizardcoder} & - & Code & LLaMa 2-Generated & No \\
& Magicoder~\citep{wei2023magicoder}\tnotex{id:26} & 75K/110K & Code & GPT-3.5-Generated & Yes \\
& WaveCoder~\citep{yu2023wavecoder} & - & Code & GPT 4-Generated & No \\
& Phi-1~\citep{gunasekar2023textbooks}\tnotex{id:27} & 6B Tokens & Code Q and A & GPT-3.5-Generated & Yes \\
& Phi-1.5~\citep{li2023textbooks} & - & Code Q and A & GPT-3.5-Generated & No \\
& Nectar~\citep{zhu2023starling}\tnotex{id:28} & ~183K & En & GPT 4-Generated & Yes \\

\midrule 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\multirow{3}{*}{\shortstack{\bf Synthetic Data \\ \bf (Self-Improvement)}} & Self-Instruct~\citep{wang2022self}\tnotex{id:11} & 52K & En & InstructGPT-Generated & Yes \\
& Instruction Backtranslation~\citep{li2023self} & 502K  & En & LLaMa-Generated & No \\
& SPIN~\citep{chen2024self}\tnotex{id:29} & 49.8K & En & Zephyr-Generated & Yes \\

\bottomrule
\end{tabular}
\end{threeparttable}
\end{adjustbox}
\begin{multicols}{2}
\begin{tablenotes}
\item[1] \label{id:1} {$^1$ https://github.com/allenai/unifiedqa} 
\item[2] \label{id:2} {$^2$ https://github.com/LAION-AI/Open-Instruction-Generalist}
\item[3] \label{id:3} {$^3$ https://github.com/hkunlp/unifiedskg}
\item[4] \label{id:4} {$^4$ https://github.com/allenai/natural-instructions-v1}
\item[5] \label{id:5} {$^5$ https://github.com/allenai/natural-instructions}
\item[6] \label{id:6} {$^6$ https://huggingface.co/datasets/bigscience/P3} 
\item[7] \label{id:7} {$^7$ https://github.com/bigscience-workshop/xmtf}
\item[8] \label{id:8} {$^8$ https://github.com/google-research/FLAN}
\item[9] \label{id:9} {$^9$ https://github.com/BAAI-Zlab/COIG}
\item[10] \label{id:10} {$^{10}$ https://github.com/orhonovich/unnatural-instructions}
\item[11] \label{id:11} {$^{11}$ https://github.com/yizhongw/self-instruct}
\item[12] \label{id:12} {$^{12}$ https://github.com/XueFuzhao/InstructionWild}
\item[13] \label{id:13} {$^{13}$ https://github.com/nlpxucan/evol-instruct} 
\item[14] \label{id:14} {$^{14}$ https://github.com/tatsu-lab/stanford\_alpaca}
\item[15] \label{id:15} {$^{15}$  https://github.com/csitfun/LogiCoT}
\item[16] \label{id:16} {$^{16}$ https://huggingface.co/datasets/databricks/databricks-dolly-15k}
\item[17] \label{id:17} {$^{17}$ https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM}
\item[18] \label{id:18} {$^{18}$ https://huggingface.co/datasets/GAIR/lima}
\item[19] \label{id:19} {$^{19}$ https://huggingface.co/datasets/JosephusCheung/GuanacoDataset}
\item[20] \label{id:20} {$^{20}$ https://github.com/LAION-AI/Open-Assistant}
\item[21] \label{id:21} {$^{21}$ https://github.com/project-baize/baize-chatbot}
\item[22] \label{id:22} {$^{22}$ https://github.com/thunlp/UltraChat\#data}
\item[23] \label{id:23} {$^{23}$ https://huggingface.co/datasets/Open-Orca/OpenOrca}
\item[24] \label{id:24} {$^{24}$ https://huggingface.co/datasets/RyokoAI/ShareGPT52K}
\item[25] \label{id:25} {$^{25}$ https://huggingface.co/datasets/allenai/WildChat}
\item[26] \label{id:26} {$^{26}$ https://github.com/ise-uiuc/magicoder?tab=readme-ov-file\#-dataset}
\item[27] \label{id:27} {$^{27}$ https://huggingface.co/microsoft/phi-1}
\item[28] \label{id:28} {$^{28}$ https://huggingface.co/datasets/berkeley-nest/Nectar}
\item[29] \label{id:29} {$^{29}$ https://github.com/uclaml/SPIN?tab=readme-ov-file\#Data}
\end{tablenotes}
\end{multicols}
%}
\caption{An overview of instruction tuning datasets.}
\label{tab:llms_traindata_table}
\end{table*}