\begin{table*}[t]
\centering
\small
\begin{adjustbox}{max width=0.9\textwidth}
\begin{threeparttable}
\begin{tabular}{lclccc}
% p{5cm}p{2cm}p{5cm}p{3cm}
\toprule 
\multirow{2}{*}{\bf Instruction fine-tuned LLMs} & \multirow{2}{*}{\bf \# Params} & \multirow{2}{*}{\bf Base Model} & \multicolumn{3}{c}{\bf Fine-tuning Trainset} \\
& & & {\bf Self-build} & {\bf Dataset Name} & {\bf Size} \\\midrule
Instruct-GPT~\citep{ouyang2022training} & 176B & GPT-3~\citep{Brown2020LanguageMA} & Yes & - & - \\ 
BLOOMZ~\citep{muennighoff2022crosslingual}\tnotex{id:1} & 176B & BLOOM~\citep{Scao2022BLOOMA1} & No & xP3 & -  \\ 
% xP3~\citep{muennighoff2022crosslingual} 
FLAN-T5~\citep{Chung2022ScalingIL}\tnotex{id:2} & 11B & T5~\citep{Raffel2019ExploringTL} & No & FLAN 2021 & - \\ 
% FLAN~\citep{longpre2023flan}
Alpaca~\citep{taori2023alpaca}\tnotex{id:3} & 7B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & - & 52K  \\ 
Vicuna~\citep{chiang2023vicuna}\tnotex{id:4} & 13B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & - & 70K  \\ 
GPT-4-LLM~\citep{peng2023instruction}\tnotex{id:5} & 7B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & - & 52K \\ 
Claude~\citep{bai2022constitutional} & - & - & Yes & - & - \\ 
WizardLM~\citep{xu2023wizardlm}\tnotex{id:6} & 7B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & Evol-Instruct & 70K  \\ 
ChatGLM2~\citep{du2022glm}\tnotex{id:7}& 6B & GLM~\citep{du2022glm} & Yes & - & 1.1 Tokens \\ 
LIMA~\citep{Zhou2023LIMALI}& 65B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & - & 1K  \\ 
%  GPT-4~\citep{OpenAI2023GPT4TR}: also a multi-modality models& - & - & - \\ 
OPT-IML~\citep{Iyer2022OPTIMLSL}\tnotex{id:8}& 175B & OPT~\citep{Zhang2022OPTOP} & No & - & - \\ 
Dolly 2.0~\citep{conover2023free}\tnotex{id:9} & 12B & Pythia~\citep{Biderman2023PythiaAS} & No & - & 15K  \\ 
% Pythia~\citep{Biderman2023PythiaAS}& - & - & - \\ 
Falcon-Instruct~\citep{falcon40b}\tnotex{id:10}& 40B & Falcon~\citep{almazrouei2023falcon} & No & - & - \\ 
Guanaco~\citep{Guanaco}\tnotex{id:11} & 7B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & - & 586K \\ 
Minotaur~\citep{minotaur}\tnotex{id:12}& 15B & Starcoder Plus~\citep{li2023starcoder} & No & - & -  \\ 
Nous-Hermes~\citep{nous-hermes}\tnotex{id:13}& 13B & LLaMA~\citep{Touvron2023LLaMAOA} & No & - & 300K+ \\ 
%  Dromedary~\citep{Sun2023PrincipleDrivenSO}& - & - & - \\ 
%  FedIT~\citep{Zhang2023TowardsBT}& - & - & - \\ 
TÃœLU~\citep{Wang2023HowFC}\tnotex{id:14} & 6.7B & OPT~\citep{Zhang2022OPTOP} & No & Mixed   & - \\ 
YuLan-Chat~\citep{YuLan-Chat}\tnotex{id:15}& 13B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & - & 250K  \\ 
MOSS~\citep{moss}\tnotex{id:16} & 16B & - & Yes & - & -  \\ 
Airoboros~\citep{airoboros}\tnotex{id:17} & 13B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & - & -  \\ 
UltraLM~\citep{ding2023enhancing}\tnotex{id:18}& 13B & LLaMA~\citep{Touvron2023LLaMAOA} & Yes & - & - \\ 
\bottomrule
\end{tabular}
\end{threeparttable}
\end{adjustbox}
\begin{multicols}{2}
\begin{tablenotes}
\item[1] \label{id:1} {$^1$ https://huggingface.co/bigscience/bloomz} 
\item[2] \label{id:2} {$^2$ https://huggingface.co/google/flan-t5-xxl}
\item[3] \label{id:3} {$^3$ https://github.com/tatsu-lab/stanford\_alpaca}
\item[4] \label{id:4} {$^4$ https://github.com/lm-sys/FastChat}
\item[5] \label{id:5} {$^5$ https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM}
\item[6] \label{id:6} {$^6$ https://github.com/nlpxucan/WizardLM} 
\item[7] \label{id:7} {$^7$ https://github.com/THUDM/ChatGLM2-6B}
\item[8] \label{id:8} {$^8$ https://huggingface.co/facebook/opt-iml-30b}
\item[9] \label{id:9} {$^9$ https://github.com/databrickslabs/dolly}
\item[10] \label{id:10} {$^{10}$ https://huggingface.co/tiiuae/falcon-40b-instruct}
\item[11] \label{id:11} {$^{11}$ https://huggingface.co/JosephusCheung/Guanaco}
\item[12] \label{id:12} {$^{12}$ https://huggingface.co/openaccess-ai-collective/minotaur-15b}
\item[13] \label{id:13} {$^{13}$ https://huggingface.co/NousResearch/Nous-Hermes-13b } 
\item[14] \label{id:14} {$^{14}$ https://github.com/allenai/open-instruct}
\item[15] \label{id:15} {$^{15}$ https://github.com/RUC-GSAI/YuLan-Chat}
\item[16] \label{id:16} {$^{16}$ https://github.com/OpenLMLab/MOSS}
\item[17] \label{id:17} {$^{17}$ https://github.com/jondurbin/airoboros}
\item[18] \label{id:18} {$^{18}$ https://github.com/thunlp/UltraChat}

\end{tablenotes}
\end{multicols}
\caption{An overview of LLMs tuned on IT datasets.}
\label{tab:llms_table}
\end{table*}