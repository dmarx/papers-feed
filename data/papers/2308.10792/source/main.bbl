\begin{thebibliography}{193}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Adlakha et~al.(2023)Adlakha, BehnamGhader, Lu, Meade, and
  Reddy}]{Adlakha2023EvaluatingCA}
Vaibhav Adlakha, Parishad BehnamGhader, Xing~Han Lu, Nicholas Meade, and Siva
  Reddy. 2023.
\newblock Evaluating correctness and faithfulness of instruction-following
  models for question answering.
\newblock \emph{ArXiv}, abs/2307.16877.

\bibitem[{Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei,
  Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski,
  Barreira, Vinyals, Zisserman, and
  Simonyan}]{DBLP:conf/nips/AlayracDLMBHLMM22}
Jean{-}Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
  Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
  Samangooei, Marianne Monteiro, Jacob~L. Menick, Sebastian Borgeaud, Andy
  Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo
  Barreira, Oriol Vinyals, Andrew Zisserman, and Kar{\'{e}}n Simonyan. 2022.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In \emph{NeurIPS}.

\bibitem[{Almazrouei et~al.(2023{\natexlab{a}})Almazrouei, Alobeidli, Alshamsi,
  Cappelli, Cojocaru, Debbah, Goffinet, Heslow, Launay, Malartic, Noune,
  Pannier, and Penedo}]{falcon40b}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
  Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien
  Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme
  Penedo. 2023{\natexlab{a}}.
\newblock {Falcon-40B}: an open large language model with state-of-the-art
  performance.

\bibitem[{Almazrouei et~al.(2023{\natexlab{b}})Almazrouei, Alobeidli, Alshamsi,
  Cappelli, Cojocaru, Debbah, Goffinet, Heslow, Launay, Malartic
  et~al.}]{almazrouei2023falcon}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
  Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien
  Launay, Quentin Malartic, et~al. 2023{\natexlab{b}}.
\newblock Falcon-40b: an open large language model with state-of-the-art
  performance.

\bibitem[{Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu,
  Marathe, Bitton, Gadre, Jitsev et~al.}]{anas_awadalla_2023_7733589}
Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong
  Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, et~al. 2023.
\newblock Openflamingo.

\bibitem[{Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma,
  Kim, Bari, F{\'e}vry, Alyafeai, Dey, Santilli, Sun, Ben-David, Xu, Chhablani,
  Wang, Fries, Al-shaibani, Sharma, Thakker, Almubarak, Tang, Jiang, and
  Rush}]{Bach2022PromptSourceAI}
Stephen~H. Bach, Victor Sanh, Zheng~Xin Yong, Albert Webson, Colin Raffel,
  Nihal~V. Nayak, Abheesht Sharma, Taewoon Kim, M~Saiful Bari, Thibault
  F{\'e}vry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik
  Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason~Alan Fries, Maged~S.
  Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang,
  Mike Tian-Jian Jiang, and Alexander~M. Rush. 2022.
\newblock Promptsource: An integrated development environment and repository
  for natural language prompts.
\newblock \emph{ArXiv}, abs/2202.01279.

\bibitem[{Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and
  Zhou}]{bai2023qwen}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang
  Lin, Chang Zhou, and Jingren Zhou. 2023.
\newblock Qwen-vl: A versatile vision-language model for understanding,
  localization, text reading, and beyond.

\bibitem[{Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,
  DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
  2022{\natexlab{a}}.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}.

\bibitem[{Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,
  Jones, Chen, Goldie, Mirhoseini, McKinnon et~al.}]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al. 2022{\natexlab{b}}.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}.

\bibitem[{Bain et~al.(2021)Bain, Nagrani, Varol, and
  Zisserman}]{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman. 2021.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In \emph{IEEE International Conference on Computer Vision}.

\bibitem[{Bar-Tal et~al.(2022)Bar-Tal, Ofri-Amar, Fridman, Kasten, and
  Dekel}]{bar2022text2live}
Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel.
  2022.
\newblock Text2live: Text-driven layered image and video editing.
\newblock In \emph{European Conference on Computer Vision}, pages 707--723.
  Springer.

\bibitem[{Baumgartner et~al.(2020)Baumgartner, Zannettou, Keegan, Squire, and
  Blackburn}]{Baumgartner2020ThePR}
Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy
  Blackburn. 2020.
\newblock The pushshift reddit dataset.
\newblock In \emph{International Conference on Web and Social Media}.

\bibitem[{Beeching et~al.(2023)Beeching, Han, Lambert, Rajani, Sanseviero,
  Tunstall, and Wolf}]{beeching2023open}
Edward Beeching, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero,
  Lewis Tunstall, and Thomas Wolf. 2023.
\newblock Open llm leaderboard.
\newblock \emph{Hugging Face}.

\bibitem[{Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien,
  Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der
  Wal}]{Biderman2023PythiaAS}
Stella~Rose Biderman, Hailey Schoelkopf, Quentin~G. Anthony, Herbie Bradley,
  Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit,
  USVSN~Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar
  van~der Wal. 2023.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock \emph{ArXiv}, abs/2304.01373.

\bibitem[{Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang,
  and Weinbach}]{Black2022GPTNeoX20BAO}
Sid Black, Stella~Rose Biderman, Eric Hallahan, Quentin~G. Anthony, Leo Gao,
  Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
  Michael~Martin Pieler, USVSN~Sai Prashanth, Shivanshu Purohit, Laria
  Reynolds, Jonathan Tow, Benqi Wang, and Samuel Weinbach. 2022.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock \emph{ArXiv}, abs/2204.06745.

\bibitem[{Brooks et~al.(2022)Brooks, Holynski, and
  Efros}]{Brooks2022InstructPix2PixLT}
Tim Brooks, Aleksander Holynski, and Alexei~A. Efros. 2022.
\newblock Instructpix2pix: Learning to follow image editing instructions.
\newblock \emph{ArXiv}, abs/2211.09800.

\bibitem[{Brown et~al.(2020{\natexlab{a}})Brown, Mann, Ryder, Subbiah, Kaplan,
  Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020{\natexlab{a}}.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:1877--1901.

\bibitem[{Brown et~al.(2020{\natexlab{b}})Brown, Mann, Ryder, Subbiah, Kaplan,
  Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{Brown2020LanguageMA}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T.~J.
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeff Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei. 2020{\natexlab{b}}.
\newblock Language models are few-shot learners.
\newblock \emph{ArXiv}, abs/2005.14165.

\bibitem[{Chakrabarty et~al.(2022)Chakrabarty, Padmakumar, and
  He}]{Chakrabarty2022HelpMW}
Tuhin Chakrabarty, Vishakh Padmakumar, and Hengxing He. 2022.
\newblock Help me write a poem - instruction tuning as a vehicle for
  collaborative poetry writing.
\newblock \emph{ArXiv}, abs/2210.13669.

\bibitem[{Chaudhary(2023)}]{chaudhary2023code}
Sahil Chaudhary. 2023.
\newblock Code alpaca: An instruction-following llama model for code
  generation.

\bibitem[{Chen et~al.(2024{\natexlab{a}})Chen, Chen, Zhang, Chen, Wu, Zhang,
  Chen, Li, Wan, and Wang}]{chen2024allava}
Guiming~Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi
  Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang.
  2024{\natexlab{a}}.
\newblock Allava: Harnessing gpt4v-synthesized data for a lite vision-language
  model.
\newblock \emph{arXiv preprint arXiv:2402.11684}.

\bibitem[{Chen et~al.(2023{\natexlab{a}})Chen, Li, Dong, Zhang, He, Wang, Zhao,
  and Lin}]{chen2023sharegpt4v}
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao,
  and Dahua Lin. 2023{\natexlab{a}}.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock \emph{arXiv preprint arXiv:2311.12793}.

\bibitem[{Chen et~al.(2021{\natexlab{a}})Chen, Tworek, Jun, Yuan, Pinto,
  Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al. 2021{\natexlab{a}}.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}.

\bibitem[{Chen et~al.(2021{\natexlab{b}})Chen, Tworek, Jun, Yuan, Ponde,
  Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf,
  Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter,
  Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss,
  Nichol, Babuschkin, Balaji, Jain, Carr, Leike, Achiam, Misra, Morikawa,
  Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei,
  McCandlish, Sutskever, and Zaremba}]{Chen2021EvaluatingLL}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan,
  Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul
  Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela
  Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
  Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
  Felipe~Petroski Such, David~W. Cummings, Matthias Plappert, Fotios Chantzis,
  Elizabeth Barnes, Ariel Herbert-Voss, William~H. Guss, Alex Nichol, Igor
  Babuschkin, S.~Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua
  Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew~M. Knight, Miles
  Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,
  Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021{\natexlab{b}}.
\newblock Evaluating large language models trained on code.
\newblock \emph{ArXiv}, abs/2107.03374.

\bibitem[{Chen et~al.(2023{\natexlab{b}})Chen, Xu, Yan, Zhang, Huang, Si, and
  Zhang}]{Chen2023DistinguishBA}
Qianglong Chen, Guohai Xu, Mingshi Yan, Ji~Zhang, Fei Huang, Luo Si, and Yin
  Zhang. 2023{\natexlab{b}}.
\newblock Distinguish before answer: Generating contrastive explanation as
  knowledge for commonsense question answering.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}.

\bibitem[{Chen et~al.(2024{\natexlab{b}})Chen, Deng, Yuan, Ji, and
  Gu}]{chen2024self}
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu.
  2024{\natexlab{b}}.
\newblock Self-play fine-tuning converts weak language models to strong
  language models.
\newblock \emph{arXiv preprint arXiv:2401.01335}.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez et~al.}]{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al. 2023.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock \emph{See https://vicuna. lmsys. org (accessed 14 April 2023)}.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph,
  Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat,
  Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, D{\'i}az, Firat,
  Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and
  Fiedel}]{Chowdhery2022PaLMSL}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc{\'i}a,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark D{\'i}az, Orhan Firat, Michele Catasta, Jason Wei,
  Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah
  Fiedel. 2022.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{ArXiv}, abs/2204.02311.

\bibitem[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Valter, Narang,
  Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, hsin Chi, Dean, Devlin, Roberts,
  Zhou, Le, and Wei}]{Chung2022ScalingIL}
Hyung~Won Chung, Le~Hou, S.~Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric
  Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
  Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
  Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams~Wei Yu, Vincent
  Zhao, Yanping Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~Huai hsin
  Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason
  Wei. 2022.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{ArXiv}, abs/2210.11416.

\bibitem[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova}]{Clark2019BoolQET}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova. 2019.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock \emph{ArXiv}, abs/1905.10044.

\bibitem[{Clark et~al.(2020)Clark, Choi, Collins, Garrette, Kwiatkowski,
  Nikolaev, and Palomaki}]{Clark2020TyDiQA}
J.~Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly
  Nikolaev, and Jennimaria Palomaki. 2020.
\newblock Tydi qa: A benchmark for information-seeking question answering in
  typologically diverse languages.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:454--470.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord}]{Clark2018ThinkYH}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord. 2018.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{ArXiv}, abs/1803.05457.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{ArXiv}, abs/2110.14168.

\bibitem[{Collective(2023)}]{minotaur}
OpenAccess~AI Collective. 2023.
\newblock \emph{software:
  huggingface.co/openaccess-ai-collective/minotaur-15b}.

\bibitem[{Conover et~al.(2023{\natexlab{a}})Conover, Hayes, Mathur, Meng, Xie,
  Wan, Shah, Ghodsi, Wendell, Zaharia et~al.}]{conover2023free}
Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan,
  Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, et~al.
  2023{\natexlab{a}}.
\newblock Free dolly: Introducing the world’s first truly open
  instruction-tuned llm.

\bibitem[{Conover et~al.(2023{\natexlab{b}})Conover, Hayes, Mathur, Xie, Wan,
  Shah, Ghodsi, Wendell, Zaharia, and Xin}]{DatabricksBlog2023DollyV2}
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali
  Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023{\natexlab{b}}.
\newblock Free dolly: Introducing the world's first truly open
  instruction-tuned llm.

\bibitem[{Coucke et~al.(2018)Coucke, Saade, Ball, Bluche, Caulier, Leroy,
  Doumouro, Gisselbrecht, Caltagirone, Lavril
  et~al.}]{DBLP:journals/corr/abs-1805-10190}
Alice Coucke, Alaa Saade, Adrien Ball, Th{\'e}odore Bluche, Alexandre Caulier,
  David Leroy, Cl{\'e}ment Doumouro, Thibault Gisselbrecht, Francesco
  Caltagirone, Thibaut Lavril, et~al. 2018.
\newblock Snips voice platform: an embedded spoken language understanding
  system for private-by-design voice interfaces.
\newblock \emph{arXiv preprint arXiv:1805.10190}.

\bibitem[{Dai et~al.(2023)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and
  Hoi}]{Dai2023InstructBLIPTG}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao,
  Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.
\newblock Instructblip: Towards general-purpose vision-language models with
  instruction tuning.
\newblock \emph{ArXiv}, abs/2305.06500.

\bibitem[{Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}}]{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}. 2022.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with
  {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer}]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Ding et~al.(2023{\natexlab{a}})Ding, Chen, Xu, Qin, Zheng, Hu, Liu,
  Sun, and Zhou}]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
  Liu, Maosong Sun, and Bowen Zhou. 2023{\natexlab{a}}.
\newblock Enhancing chat language models by scaling high-quality instructional
  conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}.

\bibitem[{Ding et~al.(2023{\natexlab{b}})Ding, Qin, Yang, Wei, Yang, Su, Hu,
  Chen, Chan, Chen, Yi, Zhao, Wang, Liu, Zheng, Chen, Liu, Tang, Li, and
  Sun}]{Ding2023ParameterefficientFO}
Ning Ding, Yujia Qin, Guang Yang, Fu~Wei, Zonghan Yang, Yusheng Su, Shengding
  Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang,
  Zhiyuan Liu, Haitao Zheng, Jianfei Chen, Y.~Liu, Jie Tang, Juanzi Li, and
  Maosong Sun. 2023{\natexlab{b}}.
\newblock Parameter-efficient fine-tuning of large-scale pre-trained language
  models.
\newblock \emph{Nature Machine Intelligence}, 5:220--235.

\bibitem[{Du et~al.(2022)Du, Qian, Liu, Ding, Qiu, Yang, and Tang}]{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang. 2022.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 320--335.

\bibitem[{Dubois et~al.(2024)Dubois, Galambosi, Liang, and
  Hashimoto}]{dubois2024length}
Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto.
  2024.
\newblock Length-controlled alpacaeval: A simple way to debias automatic
  evaluators.
\newblock \emph{arXiv preprint arXiv:2404.04475}.

\bibitem[{Durbin(2023)}]{airoboros}
Jon Durbin. 2023.
\newblock Airoboros.
\newblock \emph{software: github.com/jondurbin/airoboros}.

\bibitem[{Dwivedi-Yu et~al.(2022)Dwivedi-Yu, Schick, Jiang, Lomeli, Lewis,
  Izacard, Grave, Riedel, and Petroni}]{dwivediyu2022editeval}
Jane Dwivedi-Yu, Timo Schick, Zhengbao Jiang, Maria Lomeli, Patrick Lewis,
  Gautier Izacard, Edouard Grave, Sebastian Riedel, and Fabio Petroni. 2022.
\newblock \href {http://arxiv.org/abs/2209.13331} {Editeval: An
  instruction-based benchmark for text improvements}.

\bibitem[{Fedus et~al.(2021)Fedus, Zoph, and Shazeer}]{Fedus2021SwitchTS}
William Fedus, Barret Zoph, and Noam~M. Shazeer. 2021.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{J. Mach. Learn. Res.}, 23:120:1--120:39.

\bibitem[{Gao et~al.(2023{\natexlab{a}})Gao, Zhao, Yu, and
  Xu}]{Gao2023ExploringTF}
Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023{\natexlab{a}}.
\newblock Exploring the feasibility of chatgpt for event extraction.
\newblock \emph{ArXiv}, abs/2303.03836.

\bibitem[{Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff et~al.}]{gao2021framework}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  et~al. 2021.
\newblock A framework for few-shot language model evaluation.
\newblock \emph{Version v0. 0.1. Sept}.

\bibitem[{Gao et~al.(2023{\natexlab{b}})Gao, Yen, Yu, and
  Chen}]{gao2023enabling}
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023{\natexlab{b}}.
\newblock Enabling large language models to generate text with citations.
\newblock \emph{arXiv preprint arXiv:2305.14627}.

\bibitem[{Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith}]{Gehman2020RealToxicityPromptsEN}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
  2020.
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models.
\newblock \emph{ArXiv}, abs/2009.11462.

\bibitem[{Girdhar et~al.(2023)Girdhar, El-Nouby, Liu, Singh, Alwala, Joulin,
  and Misra}]{girdhar2023imagebind}
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan~Vasudev
  Alwala, Armand Joulin, and Ishan Misra. 2023.
\newblock Imagebind: One embedding space to bind them all.
\newblock In \emph{CVPR}.

\bibitem[{Gong et~al.(2023)Gong, Lyu, Zhang, Wang, Zheng, Zhao, Liu, Zhang,
  Luo, and Chen}]{Gong2023MultiModalGPTAV}
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qianmengke Zhao,
  Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023.
\newblock Multimodal-gpt: A vision and language model for dialogue with humans.
\newblock \emph{ArXiv}, abs/2305.04790.

\bibitem[{Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel,
  Levine, and Song}]{gudibande2023false}
Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter
  Abbeel, Sergey Levine, and Dawn Song. 2023.
\newblock The false promise of imitating proprietary llms.
\newblock \emph{arXiv preprint arXiv:2305.15717}.

\bibitem[{Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno,
  Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi
  et~al.}]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie
  Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
  de~Rosa, Olli Saarikivi, et~al. 2023.
\newblock Textbooks are all you need.
\newblock \emph{arXiv preprint arXiv:2306.11644}.

\bibitem[{Gupta et~al.(2023)Gupta, Sawant, Mishra, Nakamura, Mitra, Mashetty,
  and Baral}]{gupta2023instruction}
Himanshu Gupta, Saurabh~Arjun Sawant, Swaroop Mishra, Mutsumi Nakamura, Arindam
  Mitra, Santosh Mashetty, and Chitta Baral. 2023.
\newblock Instruction tuned models are quick learners.
\newblock \emph{arXiv preprint arXiv:2306.05539}.

\bibitem[{Gupta et~al.(2022)Gupta, Jiao, Yeh, Mehri, Esk{\'e}nazi, and
  Bigham}]{Gupta2022InstructDialIZ}
Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Esk{\'e}nazi, and
  Jeffrey~P. Bigham. 2022.
\newblock Instructdial: Improving zero and few-shot generalization in dialogue
  through instruction tuning.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}.

\bibitem[{Hendrycks et~al.(2020{\natexlab{a}})Hendrycks, Burns, Basart, Zou,
  Mazeika, Song, and Steinhardt}]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt. 2020{\natexlab{a}}.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}.

\bibitem[{Hendrycks et~al.(2020{\natexlab{b}})Hendrycks, Burns, Basart, Zou,
  Mazeika, Song, and Steinhardt}]{Hendrycks2020MeasuringMM}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
  Dawn~Xiaodong Song, and Jacob Steinhardt. 2020{\natexlab{b}}.
\newblock Measuring massive multitask language understanding.
\newblock \emph{ArXiv}, abs/2009.03300.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart,
  Tang, Song, and Steinhardt}]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
  Tang, Dawn Song, and Jacob Steinhardt. 2021.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{arXiv preprint arXiv:2103.03874}.

\bibitem[{Honovich et~al.(2022)Honovich, Scialom, Levy, and
  Schick}]{honovich2022unnatural}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022.
\newblock Unnatural instructions: Tuning language models with (almost) no human
  labor.
\newblock \emph{arXiv preprint arXiv:2212.09689}.

\bibitem[{Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  de~Laroussilhe, Gesmundo, Attariyan, and
  Gelly}]{DBLP:conf/icml/HoulsbyGJMLGAG19}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  de~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97, pages 2790--2799. {PMLR}.

\bibitem[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen}]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen. 2021.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}.

\bibitem[{Huang and Chang(2022)}]{huang2022towards}
Jie Huang and Kevin Chen-Chuan Chang. 2022.
\newblock Towards reasoning in large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2212.10403}.

\bibitem[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang,
  Lei, Fu, Sun, and He}]{huang2023ceval}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su,
  Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and
  Junxian He. 2023.
\newblock C-eval: A multi-level multi-discipline chinese evaluation suite for
  foundation models.
\newblock \emph{arXiv preprint arXiv:2305.08322}.

\bibitem[{Islamovic()}]{stabilityStabilityLaunches}
Anel Islamovic.
\newblock {S}tability {A}{I} {L}aunches the {F}irst of its {S}table{L}{M}
  {S}uite of {L}anguage {M}odels — {S}tability {A}{I} --- stability.ai.
\newblock
  \url{https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models}.
\newblock [Accessed 09-Jun-2023].

\bibitem[{Ivison et~al.(2022)Ivison, Bhagia, Wang, Hajishirzi, and
  Peters}]{Ivison2022HINTHI}
Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, and
  Matthew~E. Peters. 2022.
\newblock Hint: Hypernetwork instruction tuning for efficient zero-shot
  generalisation.
\newblock \emph{ArXiv}, abs/2212.10315.

\bibitem[{Iyer et~al.(2022)Iyer, Lin, Pasunuru, Mihaylov, Simig, Yu, Shuster,
  Wang, Liu, Koura, Li, O'Horo, Pereyra, Wang, Dewan, Celikyilmaz, Zettlemoyer,
  and Stoyanov}]{Iyer2022OPTIMLSL}
Srinivas Iyer, Xiaojuan Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig,
  Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit~Singh Koura, Xian Li,
  Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli
  Celikyilmaz, Luke Zettlemoyer, and Veselin Stoyanov. 2022.
\newblock Opt-iml: Scaling language model instruction meta learning through the
  lens of generalization.
\newblock \emph{ArXiv}, abs/2212.12017.

\bibitem[{JosephusCheung(2021)}]{Guanaco}
JosephusCheung. 2021.
\newblock Guanaco: Generative universal assistant for natural-language adaptive
  context-aware omnilingual outputs.

\bibitem[{Khashabi et~al.(2020)Khashabi, Min, Khot, Sabharwal, Tafjord, Clark,
  and Hajishirzi}]{khashabi2020unifiedqa}
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord,
  Peter Clark, and Hannaneh Hajishirzi. 2020.
\newblock Unifiedqa: Crossing format boundaries with a single qa system.
\newblock \emph{arXiv preprint arXiv:2005.00700}.

\bibitem[{K{\"o}pf et~al.(2023)K{\"o}pf, Kilcher, von R{\"u}tte, Anagnostidis,
  Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi et~al.}]{kopf2023openassistant}
Andreas K{\"o}pf, Yannic Kilcher, Dimitri von R{\"u}tte, Sotiris Anagnostidis,
  Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver
  Stanley, Rich{\'a}rd Nagyfi, et~al. 2023.
\newblock Openassistant conversations--democratizing large language model
  alignment.
\newblock \emph{arXiv preprint arXiv:2304.07327}.

\bibitem[{Kung and Peng(2023)}]{Kung2023DoMR}
Po-Nien Kung and Nanyun Peng. 2023.
\newblock Do models really learn to follow instructions? an empirical study of
  instruction tuning.
\newblock \emph{ArXiv}, abs/2305.11383.

\bibitem[{LAION.ai(2023)}]{2023oig}
LAION.ai. 2023.
\newblock Oig: the open instruction generalist dataset.

\bibitem[{Lau et~al.(2018)Lau, Gayen, Ben~Abacha, and
  Demner-Fushman}]{lau2018dataset}
Jason~J Lau, Soumya Gayen, Asma Ben~Abacha, and Dina Demner-Fushman. 2018.
\newblock A dataset of clinically generated visual questions and answers about
  radiology images.
\newblock \emph{Scientific data}, 5(1):1--10.

\bibitem[{Lee et~al.(2022)Lee, Liang, and Yang}]{Lee2022CoAuthorDA}
Mina Lee, Percy Liang, and Qian Yang. 2022.
\newblock Coauthor: Designing a human-ai collaborative writing dataset for
  exploring language model capabilities.
\newblock \emph{Proceedings of the 2022 CHI Conference on Human Factors in
  Computing Systems}.

\bibitem[{Lester et~al.(2021)Lester, Al-Rfou, and
  Constant}]{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}.

\bibitem[{Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, rahman Mohamed,
  Levy, Stoyanov, and Zettlemoyer}]{lewis-etal-2020-bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel rahman
  Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Fang, Yang, Wang, Ye, Zhao, and
  Zhang}]{Li2023EvaluatingCI}
Bo~Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun
  Zhang. 2023{\natexlab{a}}.
\newblock Evaluating chatgpt's information extraction capabilities: An
  assessment of performance, explainability, calibration, and faithfulness.
\newblock \emph{ArXiv}, abs/2304.11633.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Zhang, Chen, Wang, Yang, and
  Liu}]{Li2023OtterAM}
Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
  2023{\natexlab{b}}.
\newblock Otter: A multi-modal model with in-context instruction tuning.
\newblock \emph{ArXiv}, abs/2305.03726.

\bibitem[{Li et~al.(2023{\natexlab{c}})Li, Hammoud, Itani, Khizbullin, and
  Ghanem}]{li2023camel}
Guohao Li, Hasan Abed Al~Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and
  Bernard Ghanem. 2023{\natexlab{c}}.
\newblock Camel: Communicative agents for "mind" exploration of large scale
  language model society.

\bibitem[{Li et~al.(2023{\natexlab{d}})Li, Li, Savarese, and Hoi}]{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023{\natexlab{d}}.
\newblock {BLIP-2:} bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock In \emph{ICML}.

\bibitem[{Li et~al.(2023{\natexlab{e}})Li, He, Wang, Li, Wang, Luo, Wang, Wang,
  and Qiao}]{2023videochat}
Kunchang Li, Yinan He, Yi~Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,
  Limin Wang, and Yu~Qiao. 2023{\natexlab{e}}.
\newblock Videochat: Chat-centric video understanding.
\newblock \emph{arXiv preprint arXiv:2305.06355}.

\bibitem[{Li et~al.(2023{\natexlab{f}})Li, Allal, Zi, Muennighoff, Kocetkov,
  Mou, Marone, Akiki, Li, Chim et~al.}]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
  Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
  2023{\natexlab{f}}.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}.

\bibitem[{Li et~al.(2023{\natexlab{g}})Li, Yu, Zhou, Schick, Zettlemoyer, Levy,
  Weston, and Lewis}]{li2023self}
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy,
  Jason Weston, and Mike Lewis. 2023{\natexlab{g}}.
\newblock Self-alignment with instruction backtranslation.
\newblock \emph{arXiv preprint arXiv:2308.06259}.

\bibitem[{Li et~al.(2023{\natexlab{h}})Li, Zhang, Dubois, Taori, Gulrajani,
  Guestrin, Liang, and Hashimoto}]{alpaca_eval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023{\natexlab{h}}.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \emph{GitHub repository}.

\bibitem[{Li et~al.(2023{\natexlab{i}})Li, Bubeck, Eldan, Del~Giorno,
  Gunasekar, and Lee}]{li2023textbooks}
Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya
  Gunasekar, and Yin~Tat Lee. 2023{\natexlab{i}}.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock \emph{arXiv preprint arXiv:2309.05463}.

\bibitem[{Li et~al.(2023{\natexlab{j}})Li, Li, Zhang, Dan, and
  Zhang}]{Li2023ChatDoctorAM}
Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang.
  2023{\natexlab{j}}.
\newblock Chatdoctor: A medical chat model fine-tuned on llama model using
  medical domain knowledge.
\newblock \emph{ArXiv}, abs/2303.14070.

\bibitem[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, Newman, Yuan, Yan, Zhang, Cosgrove, Manning,
  R'e, Acosta-Navas, Hudson, Zelikman, Durmus, Ladhak, Rong, Ren, Yao, Wang,
  Santhanam, Orr, Zheng, Yuksekgonul, Suzgun, Kim, Guha, Chatterji, Khattab,
  Henderson, Huang, Chi, Xie, Santurkar, Ganguli, Hashimoto, Icard, Zhang,
  Chaudhary, Wang, Li, Mai, Zhang, and Koreeda}]{Liang2022HolisticEO}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  Benjamin Newman, Binhang Yuan, Bobby Yan, Ce~Zhang, Christian Cosgrove,
  Christopher~D. Manning, Christopher R'e, Diana Acosta-Navas, Drew~A. Hudson,
  E.~Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao,
  Jue Wang, Keshav Santhanam, Laurel~J. Orr, Lucia Zheng, Mert Yuksekgonul,
  Mirac Suzgun, Nathan~S. Kim, Neel Guha, Niladri~S. Chatterji, Omar Khattab,
  Peter Henderson, Qian Huang, Ryan Chi, Sang~Michael Xie, Shibani Santurkar,
  Surya Ganguli, Tatsunori Hashimoto, Thomas~F. Icard, Tianyi Zhang, Vishrav
  Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta
  Koreeda. 2022.
\newblock Holistic evaluation of language models.
\newblock \emph{Annals of the New York Academy of Sciences}.

\bibitem[{Lin et~al.(2024)Lin, Deng, Chandu, Brahman, Ravichander, Pyatkin,
  Dziri, Bras, and Choi}]{lin2024wildbench}
Bill~Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha
  Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan~Le Bras, and Yejin Choi.
  2024.
\newblock Wildbench: Benchmarking llms with challenging tasks from real users
  in the wild.
\newblock \emph{arXiv preprint arXiv:2406.04770}.

\bibitem[{Lin et~al.(2023{\natexlab{a}})Lin, Ravichander, Lu, Dziri, Sclar,
  Chandu, Bhagavatula, and Choi}]{lin2023unlocking}
Bill~Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar,
  Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023{\natexlab{a}}.
\newblock The unlocking spell on base llms: Rethinking alignment via in-context
  learning.

\bibitem[{Lin et~al.(2022)Lin, Tan, Miller, Tian, and
  Ren}]{lin2022unsupervised}
Bill~Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren. 2022.
\newblock Unsupervised cross-task generalization via retrieval augmentation.
\newblock \emph{ArXiv}, abs/2204.07937.

\bibitem[{Lin et~al.(2021)Lin, Hilton, and Evans}]{Lin2021TruthfulQAMH}
Stephanie~C. Lin, Jacob Hilton, and Owain Evans. 2021.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}.

\bibitem[{Lin et~al.(2023{\natexlab{b}})Lin, Zhao, Zhang, Wu, Zhang, Wang, and
  Xie}]{lin2023pmc}
Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya~Zhang, Yanfeng Wang,
  and Weidi Xie. 2023{\natexlab{b}}.
\newblock Pmc-clip: Contrastive language-image pre-training using biomedical
  documents.
\newblock \emph{arXiv preprint arXiv:2303.07240}.

\bibitem[{Liu et~al.(2021{\natexlab{a}})Liu, Zhan, Xu, Ma, Yang, and
  Wu}]{liu2021slake}
Bo~Liu, Li-Ming Zhan, Li~Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.
  2021{\natexlab{a}}.
\newblock Slake: A semantically-labeled knowledge-enhanced dataset for medical
  visual question answering.
\newblock In \emph{2021 IEEE 18th International Symposium on Biomedical Imaging
  (ISBI)}, pages 1650--1654. IEEE.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Teng, Cui, Zhang, Zhou, and
  Zhang}]{Liu2023LogiCoTLC}
Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang.
  2023{\natexlab{a}}.
\newblock Logicot: Logical chain-of-thought instruction-tuning data collection
  with gpt-4.
\newblock \emph{ArXiv}, abs/2305.12147.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee}]{Liu2023VisualIT}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. 2023{\natexlab{b}}.
\newblock Visual instruction tuning.
\newblock \emph{ArXiv}, abs/2304.08485.

\bibitem[{Liu et~al.(2021{\natexlab{b}})Liu, Shen, Zhang, Dolan, Carin, and
  Chen}]{liu2021makes}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu
  Chen. 2021{\natexlab{b}}.
\newblock What makes good in-context examples for gpt-$3 $?
\newblock \emph{arXiv preprint arXiv:2101.06804}.

\bibitem[{Liu and Low(2023)}]{liu2023goat}
Tiedong Liu and Bryan Kian~Hsiang Low. 2023.
\newblock Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks.
\newblock \emph{arXiv preprint arXiv:2305.14201}.

\bibitem[{Liu et~al.(2023{\natexlab{c}})Liu, Zhong, Li, Yang, Ju, Wu, Ma, Shu,
  Chen, Kim, Dai, Zhao, Zhu, Liu, Liu, Shen, Li, Li, and
  Liu}]{Liu2023RadiologyGPTAL}
Zheng Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma,
  Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Dajiang Zhu, Jun
  Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li, and Tianming Liu.
  2023{\natexlab{c}}.
\newblock Radiology-gpt: A large language model for radiology.

\bibitem[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei et~al.}]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al. 2023.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}.

\bibitem[{Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and
  Jiang}]{luo2023wizardcoder}
Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang
  Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.
\newblock Wizardcoder: Empowering code large language models with
  evol-instruct.

\bibitem[{Lv et~al.(2023)Lv, Yang, Liu, jie Gao, Guo, and Qiu}]{Lv2023FullPF}
Kai Lv, Yuqing Yang, Tengxiao Liu, Qi~jie Gao, Qipeng Guo, and Xipeng Qiu.
  2023.
\newblock Full parameter fine-tuning for large language models with limited
  resources.

\bibitem[{Meng et~al.(2022)Meng, He, Song, Song, Wu, Zhu, and
  Ermon}]{meng2022sdedit}
Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and
  Stefano Ermon. 2022.
\newblock {SDE}dit: Guided image synthesis and editing with stochastic
  differential equations.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Mishra et~al.(2021)Mishra, Khashabi, Baral, and
  Hajishirzi}]{mishra2021cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock \emph{arXiv preprint arXiv:2104.08773}.

\bibitem[{Mitra et~al.(2023)Mitra, Del~Corro, Mahajan, Codas, Simoes, Agarwal,
  Chen, Razdaibiedina, Jones, Aggarwal et~al.}]{mitra2023orca}
Arindam Mitra, Luciano Del~Corro, Shweti Mahajan, Andres Codas, Clarisse
  Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti
  Aggarwal, et~al. 2023.
\newblock Orca 2: Teaching small language models how to reason.
\newblock \emph{arXiv preprint arXiv:2311.11045}.

\bibitem[{Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts,
  Biderman, Scao, Bari, Shen, Yong, Schoelkopf
  et~al.}]{muennighoff2022crosslingual}
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella
  Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
  Schoelkopf, et~al. 2022.
\newblock Crosslingual generalization through multitask finetuning.
\newblock \emph{arXiv preprint arXiv:2211.01786}.

\bibitem[{Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi,
  and Awadallah}]{mukherjee2023orca}
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid
  Palangi, and Ahmed Awadallah. 2023.
\newblock Orca: Progressive learning from complex explanation traces of gpt-4.
\newblock \emph{arXiv preprint arXiv:2306.02707}.

\bibitem[{Ning et~al.(2023)Ning, Xie, Chen, Song, Yuan, Tian, Ye, and
  Yuan}]{Ning2023AlbumSW}
Munan Ning, Yujia Xie, Dongdong Chen, Zeyin Song, Lu~Yuan, Yonghong Tian,
  Qixiang Ye, and Liuliang Yuan. 2023.
\newblock Album storytelling with iterative story-aware captioning and large
  language models.
\newblock \emph{ArXiv}, abs/2305.12943.

\bibitem[{NousResearch(2023)}]{nous-hermes}
NousResearch. 2023.
\newblock \emph{software: huggingface.co/NousResearch/Nous-Hermes-13b}.

\bibitem[{OpenAI(2022)}]{chatgpt}
OpenAI. 2022.
\newblock Introducing chatgpt.
\newblock \emph{Blog post openai.com/blog/chatgpt}.

\bibitem[{OpenAI(2023)}]{OpenAI2023GPT4TR}
OpenAI. 2023.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
  2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:27730--27744.

\bibitem[{Overwijk et~al.(2022)Overwijk, Xiong, Liu, VandenBerg, and
  Callan}]{overwijk2022clueweb22}
Arnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron VandenBerg, and Jamie Callan.
  2022.
\newblock Clueweb22: 10 billion web documents with visual and semantic
  information.
\newblock \emph{arXiv preprint arXiv:2211.15848}.

\bibitem[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli,
  Alobeidli, Pannier, Almazrouei, and Launay}]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
  Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
  and Julien Launay. 2023.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora
  with web data, and web data only.
\newblock \emph{arXiv preprint arXiv:2306.01116}.

\bibitem[{Peng et~al.(2023)Peng, Li, He, Galley, and Gao}]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}.

\bibitem[{Qian et~al.(2022)Qian, Dong, Shen, Wei, and
  Chen}]{qian2022controllable}
Jing Qian, Li~Dong, Yelong Shen, Furu Wei, and Weizhu Chen. 2022.
\newblock Controllable natural language generation with contrastive prefixes.
\newblock \emph{arXiv preprint arXiv:2202.13257}.

\bibitem[{Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and
  Sutskever}]{Radford2021LearningTV}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever. 2021.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1(8):9.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al. 2021.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}.

\bibitem[{Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{Raffel2019ExploringTL}
Colin Raffel, Noam~M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
  Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2019.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{ArXiv}, abs/1910.10683.

\bibitem[{Raheja et~al.(2023)Raheja, Kumar, Koo, and Kang}]{Raheja2023CoEdITTE}
Vipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop Kang. 2023.
\newblock Coedit: Text editing by task-specific instruction tuning.
\newblock \emph{ArXiv}, abs/2305.09857.

\bibitem[{Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and
  He}]{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.
\newblock Deepspeed: System optimizations enable training deep learning models
  with over 100 billion parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 3505--3506.

\bibitem[{Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer}]{Rombach_2022_CVPR}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\"orn
  Ommer. 2022.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 10684--10695.

\bibitem[{Rosenbaum et~al.(2022)Rosenbaum, Soltan, Hamza, Versley, and
  Boese}]{Rosenbaum2022LINGUISTLM}
Andrew Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese.
  2022.
\newblock Linguist: Language model instruction tuning to generate annotated
  utterances for intent classification and slot tagging.
\newblock In \emph{International Conference on Computational Linguistics}.

\bibitem[{Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Scao, Raja et~al.}]{sanh2021multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika,
  Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja,
  et~al. 2021.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock \emph{arXiv preprint arXiv:2110.08207}.

\bibitem[{Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili'c, Hesslow,
  Castagn'e, Luccioni, Yvon, Gall{\'e}, Tow, Rush, Biderman, Webson,
  Ammanamanchi, Wang, Sagot, Muennighoff, del Moral, Ruwase, Bawden, Bekman,
  McMillan-Major, Beltagy, Nguyen, Saulnier, Tan, Suarez, Sanh, Laurenccon,
  Jernite, Launay, Mitchell, Raffel, Gokaslan, Simhi, Etxabe, Aji, Alfassy,
  Rogers, Nitzav, Xu, Mou, Emezue, Klamm, Leong, van Strien, Adelani, Radev,
  Ponferrada, Levkovizh, Kim, Natan, Toni, Dupont, Kruszewski, Pistilli,
  ElSahar, Benyamina, Tran, Yu, Abdulmumin, Johnson, Gonzalez-Dios, de~la Rosa,
  Chim, Dodge, Zhu, Chang, Frohberg, Tobing, Bhattacharjee, Almubarak, Chen,
  Lo, von Werra, Weber, Phan, Allal, Tanguy, Dey, Mu{\~n}oz, Masoud, Grandury,
  vSavsko, Huang, Coavoux, Singh, Jiang, Vu, Jauhar, Ghaleb, Subramani,
  Kassner, Khamis, Nguyen, Espejel, de~Gibert, Villegas, Henderson, Colombo,
  Amuok, Lhoest, Harliman, Bommasani, L'opez, Ribeiro, Osei, Pyysalo, Nagel,
  Bose, Muhammad, Sharma, Longpre, Nikpoor, Silberberg, Pai, Zink, Torrent,
  Schick, Thrush, Danchev, Nikoulina, Laippala, Lepercq, Prabhu, Alyafeai,
  Talat, Raja, Heinzerling, Si, Salesky, Mielke, Lee, Sharma, Santilli,
  Chaffin, Stiegler, Datta, Szczechla, Chhablani, Wang, Pandey, Strobelt,
  Fries, Rozen, Gao, Sutawika, Bari, Al-shaibani, Manica, Nayak, Teehan,
  Albanie, Shen, Ben-David, Bach, Kim, Bers, F{\'e}vry, Neeraj, Thakker,
  Raunak, Tang, Yong, Sun, Brody, Uri, Tojarieh, Roberts, Chung, Tae, Phang,
  Press, Li, Narayanan, Bourfoune, Casper, Rasley, Ryabinin, Mishra, Zhang,
  Shoeybi, Peyrounette, Patry, Tazi, Sanseviero, von Platen, Cornette,
  Lavall'ee, Lacroix, Rajbhandari, Gandhi, Smith, Requena, Patil, Dettmers,
  Baruwa, Singh, Cheveleva, Ligozat, Subramonian, N'ev'eol, Lovering, Garrette,
  Tunuguntla, Reiter, Taktasheva, Voloshina, Bogdanov, Winata, Schoelkopf,
  Kalo, Novikova, Forde, Tang, Kasai, Kawamura, Hazan, Carpuat, Clinciu, Kim,
  Cheng, Serikov, Antverg, van~der Wal, Zhang, Zhang, Gehrmann, Mirkin, Pais,
  Shavrina, Scialom, Yun, Limisiewicz, Rieser, Protasov, Mikhailov,
  Pruksachatkun, Belinkov, Bamberger, Kasner, Rueda, Pestana, Feizpour, Khan,
  Faranak, Santos, Hevia, Unldreaj, Aghagol, Abdollahi, Tammour, HajiHosseini,
  Behroozi, Ajibade, Saxena, Ferrandis, Contractor, Lansky, David, Kiela,
  Nguyen, Tan, Baylor, Ozoani, Mirza, Ononiwu, Rezanejad, Jones, Bhattacharya,
  Solaiman, Sedenko, Nejadgholi, Passmore, Seltzer, Sanz, Fort, Dutra,
  Samagaio, Elbadri, Mieskes, Gerchick, Akinlolu, McKenna, Qiu, Ghauri,
  Burynok, Abrar, Rajani, Elkott, Fahmy, Samuel, An, Kromann, Hao, Alizadeh,
  Shubber, Wang, Roy, Viguier, Le, Oyebade, Le, Yang, Nguyen, Kashyap,
  Palasciano, Callahan, Shukla, Miranda-Escalada, Singh, Beilharz, Wang,
  de~Brito, Zhou, Jain, Xu, Fourrier, Perin'an, Molano, Yu, Manjavacas, Barth,
  Fuhrimann, Altay, Bayrak, Burns, Vrabec, Bello, Dash, Kang, Giorgi, Golde,
  Posada, Sivaraman, Bulchandani, Liu, Shinzato, de~Bykhovetz, Takeuchi,
  P{\`a}mies, Castillo, Nezhurina, Sanger, Samwald, Cullan, Weinberg, Wolf,
  Mihaljcic, Liu, Freidank, Kang, Seelam, Dahlberg, Broad, Muellner, Fung,
  Haller, Chandrasekhar, Eisenberg, Martin, Canalli, Su, Su, Cahyawijaya,
  Garda, Deshmukh, Mishra, Kiblawi, Ott, Sang-aroonsiri, Kumar, Schweter,
  Bharati, Laud, Gigant, Kainuma, Kusa, Labrak, Bajaj, Venkatraman, Xu, Xu,
  chao Xu, Tan, Xie, Ye, Bras, Belkada, and Wolf}]{Scao2022BLOOMA1}
Teven~Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana
  Ili'c, Daniel Hesslow, Roman Castagn'e, Alexandra~Sasha Luccioni, Franccois
  Yvon, Matthias Gall{\'e}, Jonathan Tow, Alexander~M. Rush, Stella~Rose
  Biderman, Albert Webson, Pawan~Sasanka Ammanamanchi, Thomas Wang, Beno{\^i}t
  Sagot, Niklas Muennighoff, Albert~Villanova del Moral, Olatunji Ruwase,
  Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz~Beltagy, Huu Nguyen,
  Lucile Saulnier, Samson Tan, Pedro~Ortiz Suarez, Victor Sanh, Hugo
  Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel,
  Aaron Gokaslan, Adi Simhi, Aitor~Soroa Etxabe, Alham~Fikri Aji, Amit Alfassy,
  Anna Rogers, Ariel~Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris~C.
  Emezue, Christopher Klamm, Colin Leong, Daniel~Alexander van Strien,
  David~Ifeoluwa Adelani, Dragomir~R. Radev, Eduardo~Gonz'alez Ponferrada,
  Efrat Levkovizh, Ethan Kim, Eyal~Bar Natan, Francesco~De Toni, G{\'e}rard
  Dupont, Germ{\'a}n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina,
  Hieu~Trung Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar
  Gonzalez-Dios, Javier de~la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan
  Chang, Jorg Frohberg, Josephine~L. Tobing, Joydeep Bhattacharjee, Khalid
  Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan,
  Loubna~Ben Allal, Ludovic Tanguy, Manan Dey, Manuel~Romero Mu{\~n}oz, Maraim
  Masoud, Mar'ia Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank
  Singh, Mike Tian-Jian Jiang, Minh~Chien Vu, Mohammad~Ali Jauhar, Mustafa
  Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen,
  Omar Espejel, Ona de~Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo,
  Priscilla~A. Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto
  L'opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik
  Bose, Shamsuddeen~Hassan Muhammad, Shanya Sharma, S.~Longpre, Somaieh
  Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago~Timponi Torrent,
  Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika
  Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun
  Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina~J.
  Mielke, Wilson~Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin,
  Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han
  Wang, Harshit Pandey, Hendrik Strobelt, Jason~Alan Fries, Jos Rozen, Leo Gao,
  Lintang Sutawika, M~Saiful Bari, Maged~S. Al-shaibani, Matteo Manica,
  Nihal~V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David,
  Stephen~H. Bach, Taewoon Kim, Tali Bers, Thibault F{\'e}vry, Trishala Neeraj,
  Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng~Xin Yong, Zhiqing Sun, Shaked
  Brody, Y~Uri, Hadar Tojarieh, Adam Roberts, Hyung~Won Chung, Jaesung Tae,
  Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune,
  Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang,
  Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar
  Sanseviero, Patrick von Platen, Pierre Cornette, Pierre~Franccois Lavall'ee,
  R{\'e}mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith,
  St{\'e}phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
  Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur'elie
  N'ev'eol, Charles Lovering, Daniel~H Garrette, Deepak~R. Tunuguntla, Ehud
  Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta~Indra
  Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova,
  Jessica~Zosa Forde, Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan,
  Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer
  Antverg, Oskar van~der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
  Shachar Mirkin, S.~Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun,
  Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada
  Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice
  Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda
  Santa~Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
  Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi,
  Benjamin~Olusola Ajibade, Bharat~Kumar Saxena, Carlos~Mu{\~n}oz Ferrandis,
  Danish Contractor, David~M. Lansky, Davis David, Douwe Kiela, Duong~Anh
  Nguyen, Edward Tan, Emily Baylor, Ezinwanne Ozoani, Fatim~T Mirza, Frankline
  Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman,
  Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua Seltzer, Julio~Bonis
  Sanz, Karen Fort, L{\'i}via~Macedo Dutra, Mairon Samagaio, Maraim Elbadri,
  Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu,
  M.~K.~K. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott,
  Nourhan Fahmy, Olanrewaju Samuel, Ran An, R.~P. Kromann, Ryan Hao, Samira
  Alizadeh, Sarmad Shubber, Silas~L. Wang, Sourav Roy, Sylvain Viguier,
  Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen~Hai Le, Yoyo Yang, Zachary~Kyle
  Nguyen, Abhinav~Ramesh Kashyap, A.~Palasciano, Alison Callahan, Anima Shukla,
  Antonio Miranda-Escalada, Ayush~Kumar Singh, Benjamin Beilharz, Bo~Wang, Caio
  Matheus~Fonseca de~Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl{\'e}mentine
  Fourrier, Daniel~Le'on Perin'an, Daniel Molano, Dian Yu, Enrique Manjavacas,
  Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully
  Burns, Helena~U. Vrabec, Iman~I.B. Bello, Isha Dash, Ji~Soo Kang, John
  Giorgi, Jonas Golde, Jose~David Posada, Karthi Sivaraman, Lokesh Bulchandani,
  Lu~Liu, Luisa Shinzato, Madeleine~Hahn de~Bykhovetz, Maiko Takeuchi, Marc
  P{\`a}mies, Mar{\'i}a~Andrea Castillo, Marianna Nezhurina, Mario Sanger,
  Matthias Samwald, Michael Cullan, Michael Weinberg, M~Wolf, Mina Mihaljcic,
  Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg,
  Nicholas~Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller,
  R.~Chandrasekhar, R.~Eisenberg, Robert Martin, Rodrigo~L. Canalli, Rosaline
  Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok~S Deshmukh, Shubhanshu
  Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan
  Schweter, Sushil~Pratap Bharati, T.~A. Laud, Th'eo Gigant, Tomoya Kainuma,
  Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y.~Venkatraman, Yifan Xu, Ying
  Xu, Yun chao Xu, Zhee~Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes
  Belkada, and Thomas Wolf. 2022.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{ArXiv}, abs/2211.05100.

\bibitem[{Schick and Sch{\"{u}}tze(2021)}]{DBLP:conf/eacl/SchickS21}
Timo Schick and Hinrich Sch{\"{u}}tze. 2021.
\newblock Exploiting cloze-questions for few-shot text classification and
  natural language inference.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pages
  255--269.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov}]{Schulman2017ProximalPO}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
  2017.
\newblock Proximal policy optimization algorithms.
\newblock \emph{ArXiv}, abs/1707.06347.

\bibitem[{Shi et~al.(2022)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung,
  Tay, Ruder, Zhou, Das, and Wei}]{Shi2022LanguageMA}
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush
  Vosoughi, Hyung~Won Chung, Yi~Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,
  and Jason Wei. 2022.
\newblock Language models are multilingual chain-of-thought reasoners.
\newblock \emph{ArXiv}, abs/2210.03057.

\bibitem[{Soltan et~al.(2022)Soltan, Ananthakrishnan, FitzGerald, Gupta, Hamza,
  Khan, Peris, Rawls, Rosenbaum, Rumshisky et~al.}]{soltan2022alexatm}
Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael
  Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna
  Rumshisky, et~al. 2022.
\newblock Alexatm 20b: Few-shot learning using a large-scale multilingual
  seq2seq model.
\newblock \emph{arXiv preprint arXiv:2208.01448}.

\bibitem[{Srivastava et~al.(2022{\natexlab{a}})Srivastava, Rastogi, Rao, Shoeb,
  Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso
  et~al.}]{Srivastava2022BeyondTI}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al. 2022{\natexlab{a}}.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}.

\bibitem[{Srivastava et~al.(2022{\natexlab{b}})Srivastava, Rastogi, Rao, Shoeb,
  Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso
  et~al.}]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al. 2022{\natexlab{b}}.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}.

\bibitem[{Sun et~al.(2023{\natexlab{a}})Sun, Cai, Chen, Ren, Chen, de~Rijke,
  and Ren}]{Sun2023AnsweringAQ}
Weiwei Sun, Hengyi Cai, Hongshen Chen, Pengjie Ren, Zhumin Chen, Maarten
  de~Rijke, and Zhaochun Ren. 2023{\natexlab{a}}.
\newblock Answering ambiguous questions via iterative prompting.
\newblock \emph{ArXiv}, abs/2307.03897.

\bibitem[{Sun et~al.(2023{\natexlab{b}})Sun, Dong, Li, Wan, Wang, Zhang, Li,
  Cheng, Lyu, Wu et~al.}]{sun2023pushing}
Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan, Shuhe Wang, Tianwei Zhang,
  Jiwei Li, Fei Cheng, Lingjuan Lyu, Fei Wu, et~al. 2023{\natexlab{b}}.
\newblock Pushing the limits of chatgpt on nlp tasks.
\newblock \emph{arXiv preprint arXiv:2306.09719}.

\bibitem[{Sun et~al.(2023{\natexlab{c}})Sun, Li, Li, Wu, Guo, Zhang, and
  Wang}]{sun2023text}
Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and
  Guoyin Wang. 2023{\natexlab{c}}.
\newblock Text classification via large language models.
\newblock \emph{arXiv preprint arXiv:2305.08377}.

\bibitem[{Suzgun et~al.(2022{\natexlab{a}})Suzgun, Scales, Sch{\"a}rli,
  Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou
  et~al.}]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
  2022{\natexlab{a}}.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve
  them.
\newblock \emph{arXiv preprint arXiv:2210.09261}.

\bibitem[{Suzgun et~al.(2022{\natexlab{b}})Suzgun, Scales, Scharli, Gehrmann,
  Tay, Chung, Chowdhery, Le, hsin Chi, Zhou, and Wei}]{Suzgun2022ChallengingBT}
Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V. Le, Ed~Huai hsin Chi, Denny
  Zhou, and Jason Wei. 2022{\natexlab{b}}.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve
  them.
\newblock \emph{ArXiv}, abs/2210.09261.

\bibitem[{Taori et~al.(2023{\natexlab{a}})Taori, Gulrajani, Zhang, Dubois, Li,
  Guestrin, Liang, and Hashimoto}]{taori2023alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B Hashimoto. 2023{\natexlab{a}}.
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock \emph{Stanford Center for Research on Foundation Models.
  https://crfm. stanford. edu/2023/03/13/alpaca. html}, 3(6):7.

\bibitem[{Taori et~al.(2023{\natexlab{b}})Taori, Gulrajani, Zhang, Dubois, Li,
  Guestrin, Liang, and Hashimoto}]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023{\natexlab{b}}.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Tay et~al.(2022)Tay, Dehghani, Tran, Garcia, Wei, Wang, Chung, Bahri,
  Schuster, Zheng et~al.}]{tay2022ul2}
Yi~Tay, Mostafa Dehghani, Vinh~Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang,
  Hyung~Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et~al. 2022.
\newblock Ul2: Unifying language learning paradigms.

\bibitem[{Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du et~al.}]{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al. 2022.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}.

\bibitem[{Tianxiang and Xipeng(2023)}]{moss}
Sun Tianxiang and Qiu Xipeng. 2023.
\newblock Moss.
\newblock \emph{Blog post txsun1997.github.io/blogs/moss.html}.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin,
  Grave, and Lample}]{Touvron2023LLaMAOA}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample. 2023{\natexlab{a}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ArXiv}, abs/2302.13971.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar
  et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al. 2023{\natexlab{b}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Varia et~al.(2022)Varia, Wang, Halder, Vacareanu, Ballesteros,
  Benajiba, John, Anubhai, Muresan, and Roth}]{Varia2022InstructionTF}
Siddharth Varia, Shuai Wang, Kishaloy Halder, Robert Vacareanu, Miguel
  Ballesteros, Yassine Benajiba, Neha~Ann John, Rishita Anubhai, Smaranda
  Muresan, and Dan Roth. 2022.
\newblock Instruction tuning for few-shot aspect-based sentiment analysis.
\newblock \emph{ArXiv}, abs/2210.06629.

\bibitem[{Wan et~al.(2023)Wan, Cheng, Mao, Liu, Song, Li, and
  Kurohashi}]{wan2023gpt}
Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and
  Sadao Kurohashi. 2023.
\newblock Gpt-re: In-context learning for relation extraction using large
  language models.
\newblock \emph{arXiv preprint arXiv:2305.02105}.

\bibitem[{Wang et~al.(2023{\natexlab{a}})Wang, Liu, Zhao, Qin, and
  Liu}]{ChatGLM-Med}
Haochun Wang, Chi Liu, Sendong Zhao, Bing Qin, and Ting Liu.
  2023{\natexlab{a}}.
\newblock Chatglm-med.
\newblock https://github.com/SCIR-HI/Med-ChatGLM.

\bibitem[{Wang et~al.(2022{\natexlab{a}})Wang, Yang, Men, Lin, Bai, Li, Ma,
  Zhou, Zhou, and Yang}]{DBLP:conf/icml/WangYMLBLMZZY22}
Peng Wang, An~Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
  Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022{\natexlab{a}}.
\newblock Ofa: Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock In \emph{International Conference on Machine Learning}, pages
  23318--23340. PMLR.

\bibitem[{Wang et~al.(2023{\natexlab{b}})Wang, Sun, Li, Ouyang, Wu, Zhang, Li,
  and Wang}]{wang2023gpt}
Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang,
  Jiwei Li, and Guoyin Wang. 2023{\natexlab{b}}.
\newblock Gpt-ner: Named entity recognition via large language models.
\newblock \emph{arXiv preprint arXiv:2304.10428}.

\bibitem[{Wang et~al.(2023{\natexlab{c}})Wang, Zhou, Zu, Xia, Chen, Zhang,
  Zheng, Ye, Zhang, Gui, Kang, Yang, Li, and Du}]{Wang2023InstructUIEMI}
Xiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen, Yuan Zhang, Rui Zheng,
  Junjie Ye, Qi~Zhang, Tao Gui, Jihua Kang, J.~Yang, Siyuan Li, and Chunsai Du.
  2023{\natexlab{c}}.
\newblock Instructuie: Multi-task instruction tuning for unified information
  extraction.
\newblock \emph{ArXiv}, abs/2304.08085.

\bibitem[{Wang et~al.(2022{\natexlab{b}})Wang, Wei, Schuurmans, Le, hsin Chi,
  and Zhou}]{Wang2022SelfConsistencyIC}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Huai hsin Chi, and Denny
  Zhou. 2022{\natexlab{b}}.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock \emph{ArXiv}, abs/2203.11171.

\bibitem[{Wang et~al.(2023{\natexlab{d}})Wang, Ivison, Dasigi, Hessel, Khot,
  Chandu, Wadden, MacMillan, Smith, Beltagy, and Hajishirzi}]{Wang2023HowFC}
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,
  Khyathi~Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah~A. Smith,
  Iz~Beltagy, and Hanna Hajishirzi. 2023{\natexlab{d}}.
\newblock How far can camels go? exploring the state of instruction tuning on
  open resources.
\newblock \emph{ArXiv}, abs/2306.04751.

\bibitem[{Wang et~al.(2022{\natexlab{c}})Wang, Kordi, Mishra, Liu, Smith,
  Khashabi, and Hajishirzi}]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi. 2022{\natexlab{c}}.
\newblock Self-instruct: Aligning language model with self generated
  instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}.

\bibitem[{Wang et~al.(2022{\natexlab{d}})Wang, Mishra, Alipoormolabashi, Kordi,
  Mirzaei, Arunkumar, Ashok, Dhanasekaran, Naik, Stap, Pathak, Karamanolakis,
  Lai, Purohit, Mondal, Anderson, Kuznia, Doshi, Patel, Pal, Moradshahi,
  Parmar, Purohit, Varshney, Kaza, Verma, Puri, Karia, Sampat, Doshi, Mishra,
  Reddy, Patro, Dixit, Shen, Baral, Choi, Smith, Hajishirzi, and
  Khashabi}]{Wang2022SuperNaturalInstructionsGV}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva
  Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi~Gary Lai,
  Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,
  Maitreya Patel, Kuntal~Kumar Pal, M.~Moradshahi, Mihir Parmar, Mirali
  Purohit, Neeraj Varshney, Phani~Rohitha Kaza, Pulkit Verma, Ravsehaj~Singh
  Puri, Rushang Karia, Shailaja~Keyur Sampat, Savan Doshi, Siddharth~Deepak
  Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral,
  Yejin Choi, Noah~A. Smith, Hanna Hajishirzi, and Daniel Khashabi.
  2022{\natexlab{d}}.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}.

\bibitem[{Wang et~al.(2022{\natexlab{e}})Wang, Mishra, Alipoormolabashi, Kordi,
  Mirzaei, Arunkumar, Ashok, Dhanasekaran, Naik, Stap et~al.}]{wang2022super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva
  Naik, David Stap, et~al. 2022{\natexlab{e}}.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks.
\newblock \emph{arXiv preprint arXiv:2204.07705}.

\bibitem[{Wang et~al.(2022{\natexlab{f}})Wang, Mishra, Alipoormolabashi, Kordi,
  Mirzaei, Arunkumar, Ashok, Dhanasekaran, Naik, Stap
  et~al.}]{supernaturalinstructions}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva
  Naik, David Stap, et~al. 2022{\natexlab{f}}.
\newblock Super-naturalinstructions:generalization via declarative instructions
  on 1600+ tasks.
\newblock In \emph{EMNLP}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, hsin Chi, Xia, Le, and
  Zhou}]{Wei2022ChainOT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Huai hsin Chi,
  F.~Xia, Quoc Le, and Denny Zhou. 2022.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{ArXiv}, abs/2201.11903.

\bibitem[{Wei et~al.(2023{\natexlab{a}})Wei, Cui, Cheng, Wang, Zhang, Huang,
  Xie, Xu, Chen, Zhang et~al.}]{wei2023zero}
Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun
  Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et~al. 2023{\natexlab{a}}.
\newblock Zero-shot information extraction via chatting with chatgpt.
\newblock \emph{arXiv preprint arXiv:2302.10205}.

\bibitem[{Wei et~al.(2023{\natexlab{b}})Wei, Wang, Liu, Ding, and
  Zhang}]{wei2023magicoder}
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.
  2023{\natexlab{b}}.
\newblock Magicoder: Source code is all you need.
\newblock \emph{arXiv preprint arXiv:2312.02120}.

\bibitem[{Wiegreffe et~al.(2021)Wiegreffe, Hessel, Swayamdipta, Riedl, and
  Choi}]{wiegreffe2021reframing}
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi.
  2021.
\newblock Reframing human-ai collaboration for generating free-text
  explanations.
\newblock \emph{arXiv preprint arXiv:2112.08674}.

\bibitem[{Xie et~al.(2022)Xie, Wu, Shi, Zhong, Scholak, Yasunaga, Wu, Zhong,
  Yin, Wang, Zhong, Wang, Li, Boyle, Ni, Yao, Radev, Xiong, Kong, Zhang, Smith,
  Zettlemoyer, and Yu}]{Xie2022UnifiedSKGUA}
Tianbao Xie, Chen~Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro
  Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida~I. Wang, Victor
  Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao,
  Dragomir~R. Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah~A. Smith,
  Luke Zettlemoyer, and Tao Yu. 2022.
\newblock Unifiedskg: Unifying and multi-tasking structured knowledge grounding
  with text-to-text language models.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}.

\bibitem[{Xu et~al.(2023{\natexlab{a}})Xu, Sun, Zheng, Geng, Zhao, Feng, Tao,
  and Jiang}]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, and Daxin Jiang. 2023{\natexlab{a}}.
\newblock Wizardlm: Empowering large language models to follow complex
  instructions.

\bibitem[{Xu et~al.(2023{\natexlab{b}})Xu, Guo, Duan, and
  McAuley}]{xu2023baize}
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023{\natexlab{b}}.
\newblock Baize: An open-source chat model with parameter-efficient tuning on
  self-chat data.
\newblock \emph{arXiv preprint arXiv:2304.01196}.

\bibitem[{Xu et~al.(2023{\natexlab{c}})Xu, Guo, Duan, and
  McAuley}]{Xu2023BaizeAO}
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023{\natexlab{c}}.
\newblock Baize: An open-source chat model with parameter-efficient tuning on
  self-chat data.
\newblock \emph{ArXiv}, abs/2304.01196.

\bibitem[{Xu et~al.(2020)Xu, Haider, and Mansour}]{xu2020endtoend}
Weijia Xu, Batool Haider, and Saab Mansour. 2020.
\newblock End-to-end slot alignment and recognition for cross-lingual nlu.
\newblock \emph{arXiv preprint arXiv:2004.14353}.

\bibitem[{Xu et~al.(2024)Xu, Feng, Shao, Ashby, Shen, Jin, Cheng, Wang, and
  Huang}]{xu2024vision}
Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di~Jin, Yu~Cheng,
  Qifan Wang, and Lifu Huang. 2024.
\newblock Vision-flan: Scaling human-labeled tasks in visual instruction
  tuning.
\newblock \emph{arXiv preprint arXiv:2402.11690}.

\bibitem[{Xu et~al.(2022)Xu, Shen, and Huang}]{Xu2022MultiInstructIM}
Zhiyang Xu, Ying Shen, and Lifu Huang. 2022.
\newblock Multiinstruct: Improving multi-modal zero-shot learning via
  instruction tuning.
\newblock \emph{ArXiv}, abs/2212.10773.

\bibitem[{Xue et~al.(2023)Xue, Jain, Shah, Zheng, and You}]{instructionwild}
Fuzhao Xue, Kabir Jain, Mahir~Hitesh Shah, Zangwei Zheng, and Yang You. 2023.
\newblock Instruction in the wild: A user-based instruction dataset.
\newblock https://github.com/XueFuzhao/InstructionWild.

\bibitem[{Yang et~al.(2023{\natexlab{a}})Yang, Jin, Tang, Han, Feng, Jiang,
  Yin, and Hu}]{yang2023harnessing}
Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming
  Jiang, Bing Yin, and Xia Hu. 2023{\natexlab{a}}.
\newblock Harnessing the power of llms in practice: A survey on chatgpt and
  beyond.
\newblock \emph{arXiv preprint arXiv:2304.13712}.

\bibitem[{Yang et~al.(2022{\natexlab{a}})Yang, Peng, Tian, and
  Klein}]{yang2022re3}
Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022{\natexlab{a}}.
\newblock Re3: Generating longer stories with recursive reprompting and
  revision.
\newblock \emph{arXiv preprint arXiv:2210.06774}.

\bibitem[{Yang et~al.(2022{\natexlab{b}})Yang, Liu, Lei, Yang, Xue, Chen, and
  Xie}]{Yang2022TailorAP}
Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, Boxing
  Chen, and Jun Xie. 2022{\natexlab{b}}.
\newblock Tailor: A prompt-based approach to attribute-based controlled text
  generation.
\newblock \emph{ArXiv}, abs/2204.13362.

\bibitem[{Yang et~al.(2023{\natexlab{b}})Yang, Li, Lin, Wang, Lin, Liu, and
  Wang}]{yang2023dawn}
Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng
  Liu, and Lijuan Wang. 2023{\natexlab{b}}.
\newblock The dawn of lmms: Preliminary explorations with gpt-4v (ision).
\newblock \emph{arXiv preprint arXiv:2309.17421}, 9(1):1.

\bibitem[{Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and
  Narasimhan}]{Yao2023TreeOT}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L. Griffiths, Yuan
  Cao, and Karthik Narasimhan. 2023.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock \emph{ArXiv}, abs/2305.10601.

\bibitem[{Yin et~al.(2023)Yin, Wang, Cao, Shi, Liu, Li, Sheng, Bai, Huang,
  Wang, Ouyang, and Shao}]{Yin2023LAMMLM}
Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li,
  Lu~Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Wanli Ouyang, and Jing Shao.
  2023.
\newblock Lamm: Language-assisted multi-modal instruction-tuning dataset,
  framework, and benchmark.
\newblock \emph{ArXiv}, abs/2306.06687.

\bibitem[{Yu et~al.(2023)Yu, Zhang, Shang, Huang, Xu, Zhao, Hu, and
  Yin}]{yu2023wavecoder}
Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao,
  Wenxiang Hu, and Qiufeng Yin. 2023.
\newblock Wavecoder: Widespread and versatile enhanced instruction tuning with
  refined data generation.
\newblock \emph{arXiv preprint arXiv:2312.14187}.

\bibitem[{YuLan-Chat-Team(2023)}]{YuLan-Chat}
YuLan-Chat-Team. 2023.
\newblock Yulan-chat: An open-source bilingual chatbot.
\newblock https://github.com/RUC-GSAI/YuLan-Chat.

\bibitem[{Zaken et~al.(2022)Zaken, Goldberg, and
  Ravfogel}]{DBLP:conf/acl/ZakenGR22}
Elad~Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.
\newblock Bitfit: Simple parameter-efficient fine-tuning for transformer-based
  masked language-models.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers), {ACL} 2022, Dublin,
  Ireland, May 22-27, 2022}, pages 1--9. Association for Computational
  Linguistics.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, Shi, Liu, Yuan, Li, Dong, Shu,
  Li, Wang, Lin, Huang, and Fu}]{Zhang2023ChineseOI}
Ge~Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu~Shu,
  Zhaoqun Li, Zekun Wang, Chenghua Lin, Wen-Fen Huang, and Jie Fu.
  2023{\natexlab{a}}.
\newblock Chinese open instruction generalist: A preliminary release.
\newblock \emph{ArXiv}, abs/2304.07987.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, Li, and
  Bing}]{damonlpsg2023videollama}
Hang Zhang, Xin Li, and Lidong Bing. 2023{\natexlab{b}}.
\newblock Video-llama: An instruction-tuned audio-visual language model for
  video understanding.
\newblock \emph{arXiv preprint arXiv:2306.02858}.

\bibitem[{Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen,
  Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura,
  Sridhar, Wang, and Zettlemoyer}]{Zhang2022OPTOP}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona~T. Diab, Xian Li, Xi~Victoria Lin, Todor
  Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh
  Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022{\natexlab{a}}.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{ArXiv}, abs/2205.01068.

\bibitem[{Zhang et~al.(2023{\natexlab{c}})Zhang, Wu, Zhao, Lin, Zhang, Wang,
  and Xie}]{Zhang2023PMCVQAVI}
Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya~Zhang, Yanfeng Wang,
  and Weidi Xie. 2023{\natexlab{c}}.
\newblock Pmc-vqa: Visual instruction tuning for medical visual question
  answering.
\newblock \emph{ArXiv}, abs/2305.10415.

\bibitem[{Zhang et~al.(2022{\natexlab{b}})Zhang, Sun, Zhou, He, Yin, Wang,
  Sheng, Qiao, Shao, and Liu}]{zhang2022bamboo}
Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang,
  Lu~Sheng, Yu~Qiao, Jing Shao, and Ziwei Liu. 2022{\natexlab{b}}.
\newblock Bamboo: Building mega-scale vision dataset continually with
  human-machine synergy.
\newblock \emph{arXiv preprint arXiv:2203.07845}.

\bibitem[{Zhang et~al.(2023{\natexlab{d}})Zhang, Cui, Cai, Huang, Fang, and
  Bi}]{Zhang2023MultiTaskIT}
Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, and Wei Bi.
  2023{\natexlab{d}}.
\newblock Multi-task instruction tuning of llama for specific scenarios: A
  preliminary study on writing assistance.
\newblock \emph{ArXiv}, abs/2305.13225.

\bibitem[{Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh}]{Zhao2021CalibrateBU}
Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,
  Dong et~al.}]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
  Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 2023.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}.

\bibitem[{Zhao et~al.(2024)Zhao, Ren, Hessel, Cardie, Choi, and
  Deng}]{zhao2024wildchat}
Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian
  Deng. 2024.
\newblock wildchat: 570k chatgpt interaction logs in the wild.

\bibitem[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing et~al.}]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 2023.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:46595--46623.

\bibitem[{Zheng et~al.(2024)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing et~al.}]{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 2024.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Zhou et~al.(2023{\natexlab{a}})Zhou, Liu, Xu, Iyer, Sun, Mao, Ma,
  Efrat, Yu, Yu, Zhang, Ghosh, Lewis, Zettlemoyer, and Levy}]{Zhou2023LIMALI}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, L.~Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke
  Zettlemoyer, and Omer Levy. 2023{\natexlab{a}}.
\newblock Lima: Less is more for alignment.
\newblock \emph{ArXiv}, abs/2305.11206.

\bibitem[{Zhou et~al.(2023{\natexlab{b}})Zhou, Lu, Mishra, Brahma, Basu, Luan,
  Zhou, and Hou}]{zhou2023instruction}
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu,
  Yi~Luan, Denny Zhou, and Le~Hou. 2023{\natexlab{b}}.
\newblock Instruction-following evaluation for large language models.
\newblock \emph{arXiv preprint arXiv:2311.07911}.

\bibitem[{Zhu et~al.(2023{\natexlab{a}})Zhu, Frick, Wu, Zhu, and
  Jiao}]{zhu2023starling}
Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao.
  2023{\natexlab{a}}.
\newblock Starling-7b: Improving llm helpfulness \& harmlessness with rlaif.

\bibitem[{Zhu et~al.(2023{\natexlab{b}})Zhu, Chen, Shen, Li, and
  Elhoseiny}]{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
  2023{\natexlab{b}}.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced
  large language models.
\newblock \emph{arXiv preprint arXiv:2304.10592}.

\end{thebibliography}
