\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdelfattah et~al.(2021)Abdelfattah, Mehrotra, Dudziak, and Lane]{abdelfattahzero}
Abdelfattah, M.~S., Mehrotra, A., Dudziak, {\L}., and Lane, N.~D.
\newblock Zero-cost proxies for lightweight nas.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Banerjee \& Lavie(2005)Banerjee and Lavie]{banerjee2005meteor}
Banerjee, S. and Lavie, A.
\newblock Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.
\newblock In \emph{Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization}, pp.\  65--72, 2005.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and Anandkumar]{bernstein2018signsgdcompressedoptimisationnonconvex}
Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A.
\newblock signsgd: Compressed optimisation for non-convex problems, 2018.
\newblock URL \url{https://arxiv.org/abs/1802.04434}.

\bibitem[Beyer et~al.(2022{\natexlab{a}})Beyer, Zhai, and Kolesnikov]{beyer2022betterplainvitbaselines}
Beyer, L., Zhai, X., and Kolesnikov, A.
\newblock Better plain vit baselines for imagenet-1k, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2205.01580}.

\bibitem[Beyer et~al.(2022{\natexlab{b}})Beyer, Zhai, and Kolesnikov]{big_vision}
Beyer, L., Zhai, X., and Kolesnikov, A.
\newblock Big vision.
\newblock \url{https://github.com/google-research/big_vision}, 2022{\natexlab{b}}.

\bibitem[Bhardwaj et~al.(2021)Bhardwaj, Li, and Marculescu]{bhardwaj2021doestopologyinfluencegradient}
Bhardwaj, K., Li, G., and Marculescu, R.
\newblock How does topology influence gradient propagation and model performance of deep networks with densenet-type skip connections?, 2021.
\newblock URL \url{https://arxiv.org/abs/1910.00780}.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and Hutter]{chrabaszcz2017downsampled}
Chrabaszcz, P., Loshchilov, I., and Hutter, F.
\newblock A downsampled variant of imagenet as an alternative to the cifar datasets, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dettmers \& Zettlemoyer(2023)Dettmers and Zettlemoyer]{dettmers2023case}
Dettmers, T. and Zettlemoyer, L.
\newblock The case for 4-bit precision: k-bit inference scaling laws.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7750--7774. PMLR, 2023.

\bibitem[Dettmers et~al.(2021)Dettmers, Lewis, Shleifer, and Zettlemoyer]{DBLP:journals/corr/abs-2110-02861}
Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L.
\newblock 8-bit optimizers via block-wise quantization.
\newblock \emph{CoRR}, abs/2110.02861, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.02861}.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022llm}
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Doddington(2002)]{doddington2002automatic}
Doddington, G.
\newblock Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.
\newblock In \emph{Proceedings of the second international conference on Human Language Technology Research}, pp.\  138--145, 2002.

\bibitem[Dong et~al.(2021)Dong, Liu, Musial, and Gabrys]{dong2021nats}
Dong, X., Liu, L., Musial, K., and Gabrys, B.
\newblock {NATS-Bench}: Benchmarking nas algorithms for architecture topology and size.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, 2021.
\newblock \doi{10.1109/TPAMI.2021.3054824}.
\newblock \mbox{doi}:\url{10.1109/TPAMI.2021.3054824}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021imageworth16x16words}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale, 2021.
\newblock URL \url{https://arxiv.org/abs/2010.11929}.

\bibitem[Dozat(2016)]{dozat.2016}
Dozat, T.
\newblock Incorporating {Nesterov Momentum into Adam}.
\newblock In \emph{Proceedings of the 4th International Conference on Learning Representations}, pp.\  1--4, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{JMLR:v12:duchi11a}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (61):\penalty0 2121--2159, 2011.
\newblock URL \url{http://jmlr.org/papers/v12/duchi11a.html}.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and Carbin]{frankle2020pruning}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Pruning neural networks at initialization: Why are we missing the mark?
\newblock \emph{arXiv preprint arXiv:2009.08576}, 2020.

\bibitem[Ghiasi et~al.(2023)Ghiasi, Shafahi, and Ardekani]{ghiasi2023improvingrobustnessadaptiveweight}
Ghiasi, A., Shafahi, A., and Ardekani, R.
\newblock Improving robustness with adaptive weight decay, 2023.
\newblock URL \url{https://arxiv.org/abs/2210.00094}.

\bibitem[Ghorbani et~al.(2022)Ghorbani, Suo, Cardoze, Dahl, Cohen, Gilmer, Agarwal, Krishnan, Medapati, and Nado]{cohen2024adaptivegradientmethodsedge}
Ghorbani, B., Suo, D., Cardoze, D., Dahl, G., Cohen, J., Gilmer, J., Agarwal, N., Krishnan, S., Medapati, S., and Nado, Z.
\newblock Adaptive gradient methods at the edge of stability.
\newblock 2022.
\newblock URL \url{https://arxiv.org/abs/2207.14484}.

\bibitem[Gokaslan \& Cohen(2019)Gokaslan and Cohen]{Gokaslan2019OpenWeb}
Gokaslan, A. and Cohen, V.
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem[Graves(2014)]{graves2014generatingsequencesrecurrentneural}
Graves, A.
\newblock Generating sequences with recurrent neural networks, 2014.
\newblock URL \url{https://arxiv.org/abs/1308.0850}.

\bibitem[Gu et~al.(2024)Gu, Wang, Wu, Shi, Chen, Fan, Xiao, Zhao, Chang, Wu, et~al.]{gu2024mix}
Gu, Y., Wang, X., Wu, J.~Z., Shi, Y., Chen, Y., Fan, Z., Xiao, W., Zhao, R., Chang, S., Wu, W., et~al.
\newblock Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[He et~al.(2015{\natexlab{a}})He, Zhang, Ren, and Sun]{he2015deepresiduallearningimage}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition, 2015{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1512.03385}.

\bibitem[He et~al.(2015{\natexlab{b}})He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  1026--1034, 2015{\natexlab{b}}.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012neural}
Hinton, G., Srivastava, N., and Swersky, K.
\newblock Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.
\newblock \emph{Cited on}, 14\penalty0 (8):\penalty0 2, 2012.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Kumar(2017)]{kumar2017weight}
Kumar, S.~K.
\newblock On weight initialization in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1704.08863}, 2017.

\bibitem[Kunstner et~al.(2023)Kunstner, Chen, Lavington, and Schmidt]{kunstner2023noise}
Kunstner, F., Chen, J., Lavington, J.~W., and Schmidt, M.
\newblock Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be.
\newblock \emph{arXiv preprint arXiv:2304.13960}, 2023.

\bibitem[Lee et~al.(2018)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.~H.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock \emph{arXiv preprint arXiv:1810.02340}, 2018.

\bibitem[Lei et~al.(2023)Lei, Xu, Zhang, He, and Mallick]{lei2023balanceessenceacceleratingsparse}
Lei, B., Xu, D., Zhang, R., He, S., and Mallick, B.~K.
\newblock Balance is essence: Accelerating sparse training via adaptive gradient correction, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.03573}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Chen, and Zhu]{li2023memoryefficientoptimizers4bit}
Li, B., Chen, J., and Zhu, J.
\newblock Memory efficient optimizers with 4-bit states, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2309.01507}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Yang, Bhardwaj, and Marculescu]{lizico}
Li, G., Yang, Y., Bhardwaj, K., and Marculescu, R.
\newblock Zico: Zero-shot nas via inverse coefficient of variation on gradients.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2024)Li, Rakhlin, and Jadbabaie]{liconvergence23}
Li, H., Rakhlin, A., and Jadbabaie, A.
\newblock Convergence of adam under relaxed assumptions.
\newblock In \emph{Proceedings of the 37th International Conference on Neural Information Processing Systems}, NIPS '23, Red Hook, NY, USA, 2024. Curran Associates Inc.

\bibitem[Lin(2004)]{lin2004rouge}
Lin, C.-Y.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pp.\  74--81, 2004.

\bibitem[Liu et~al.(2019)Liu, Jiang, He, Chen, Liu, Gao, and Han]{liu2019variance}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock \emph{arXiv preprint arXiv:1908.03265}, 2019.

\bibitem[Liu et~al.(2021)Liu, Jiang, He, Chen, Liu, Gao, and Han]{liu2021variance}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond, 2021.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{loshchilov2019decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization, 2019.

\bibitem[Mishchenko \& Defazio(2023)Mishchenko and Defazio]{mishchenko2023prodigy}
Mishchenko, K. and Defazio, A.
\newblock Prodigy: An expeditiously adaptive parameter-free learner.
\newblock \emph{arXiv preprint arXiv:2306.06101}, 2023.

\bibitem[Nesterov(1983)]{Nes83}
Nesterov, Y.~E.
\newblock A method of solving a convex programming problem with convergence rate $o(1/k^2)$.
\newblock \emph{Doklady Akademii Nauk SSSR}, 269\penalty0 (3):\penalty0 543--547, 1983.
\newblock URL \url{http://mi.mathnet.ru/dan46009}.
\newblock MathNet: \url{http://mi.mathnet.ru/dan46009}, MathSciNet: \url{http://mathscinet.ams.org/mathscinet-getitem?mr=0701288}, zbMATH: \url{https://zbmath.org/?q=an:0535.90071}.

\bibitem[Novikova et~al.(2017)Novikova, Du{\v{s}}ek, and Rieser]{novikova2017e2e}
Novikova, J., Du{\v{s}}ek, O., and Rieser, V.
\newblock The e2e dataset: New challenges for end-to-end generation.
\newblock \emph{arXiv preprint arXiv:1706.09254}, 2017.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pp.\  311--318, 2002.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022highresolutionimagesynthesislatent}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models, 2022.
\newblock URL \url{https://arxiv.org/abs/2112.10752}.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{shazeer2018adafactor}
Shazeer, N. and Stern, M.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{International Conference on Machine Learning}, pp.\  4596--4604. PMLR, 2018.

\bibitem[Steiner et~al.(2022)Steiner, Kolesnikov, Zhai, Wightman, Uszkoreit, and Beyer]{steiner2022trainvitdataaugmentation}
Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., and Beyer, L.
\newblock How to train your vit? data, augmentation, and regularization in vision transformers, 2022.
\newblock URL \url{https://arxiv.org/abs/2106.10270}.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and Ganguli]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving synaptic flow.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6377--6389, 2020.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Vaswani(2017)]{vaswani2017attention}
Vaswani, A.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Vedantam et~al.(2015)Vedantam, Lawrence~Zitnick, and Parikh]{vedantam2015cider}
Vedantam, R., Lawrence~Zitnick, C., and Parikh, D.
\newblock Cider: Consensus-based image description evaluation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  4566--4575, 2015.

\bibitem[Ward et~al.(2020)Ward, Wu, and Bottou]{ward2020adagrad}
Ward, R., Wu, X., and Bottou, L.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (219):\penalty0 1--30, 2020.

\bibitem[Xiang et~al.(2023)Xiang, Hunter, Xu, Dudziak, and Wen]{xiang2023exploiting}
Xiang, L., Hunter, R., Xu, M., Dudziak, {\L}., and Wen, H.
\newblock Exploiting network compressibility and topology in zero-cost nas.
\newblock In \emph{AutoML Conference 2023}, 2023.

\bibitem[Xiao et~al.(2021)Xiao, Singh, Mintun, Darrell, Dollár, and Girshick]{xiao2021earlyconvolutionshelptransformers}
Xiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., and Girshick, R.
\newblock Early convolutions help transformers see better, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.14881}.

\bibitem[Zeiler(2012)]{zeiler2012adadeltaadaptivelearningrate}
Zeiler, M.~D.
\newblock Adadelta: An adaptive learning rate method, 2012.
\newblock URL \url{https://arxiv.org/abs/1212.5701}.

\bibitem[Zhang \& Pilanci(2024)Zhang and Pilanci]{zhang2024riemannian}
Zhang, F. and Pilanci, M.
\newblock Riemannian preconditioned lora for fine-tuning foundation models.
\newblock \emph{arXiv preprint arXiv:2402.02347}, 2024.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Chen, Ding, Li, Sun, and Luo]{zhang2024transformers}
Zhang, Y., Chen, C., Ding, T., Li, Z., Sun, R., and Luo, Z.-Q.
\newblock Why transformers need adam: A hessian perspective.
\newblock \emph{arXiv preprint arXiv:2402.16788}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Chen, Li, Ding, Wu, Ye, Luo, and Sun]{zhang2024adamminiusefewerlearning}
Zhang, Y., Chen, C., Li, Z., Ding, T., Wu, C., Ye, Y., Luo, Z.-Q., and Sun, R.
\newblock Adam-mini: Use fewer learning rates to gain more, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2406.16793}.

\end{thebibliography}
