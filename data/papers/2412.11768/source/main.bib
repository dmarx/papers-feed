@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@misc{
zhang2020why,
title={Why {\{}ADAM{\}} Beats {\{}SGD{\}} for Attention Models	},
author={Jingzhao Zhang and Sai Praneeth Karimireddy and Andreas Veit and Seungyeon Kim and Sashank J Reddi and Sanjiv Kumar and Suvrit Sra},
year={2020},
url={https://openreview.net/forum?id=SJx37TEtDH}
}

@inproceedings{liconvergence23,
    author = {Li, Haochuan and Rakhlin, Alexander and Jadbabaie, Ali},
    title = {Convergence of adam under relaxed assumptions},
    year = {2024},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
    articleno = {2272},
    numpages = {31},
    location = {New Orleans, LA, USA},
    series = {NIPS '23}
}

@inproceedings{j.2018on,
    title={On the Convergence of Adam and Beyond},
    author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=ryQu7f-RZ},
}

@misc{ruder2017overview,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@Book{GoodBengCour16,
  Title                    = {Deep Learning},
  Author                   = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},

  Address                  = {Cambridge, MA, USA},
  Note                     = {\url{http://www.deeplearningbook.org}}
}
@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{keskar2017largebatch,
      title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, 
      author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
      year={2017},
      eprint={1609.04836},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zhang2018yellowfin,
      title={YellowFin and the Art of Momentum Tuning}, 
      author={Jian Zhang and Ioannis Mitliagkas},
      year={2018},
      eprint={1706.03471},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{liu2021variance,
      title={On the Variance of the Adaptive Learning Rate and Beyond}, 
      author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
      year={2021},
      eprint={1908.03265},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dong2021nats,
  title   = {{NATS-Bench}: Benchmarking NAS Algorithms for Architecture Topology and Size},
  author  = {Dong, Xuanyi and Liu, Lu and Musial, Katarzyna and Gabrys, Bogdan},
  doi     = {10.1109/TPAMI.2021.3054824},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year    = {2021},
  note    = {\mbox{doi}:\url{10.1109/TPAMI.2021.3054824}}
}
@misc{chrabaszcz2017downsampled,
      title={A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets}, 
      author={Patryk Chrabaszcz and Ilya Loshchilov and Frank Hutter},
      year={2017},
      eprint={1707.08819},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}

@article{merrill2023tale,
  title={A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks},
  author={Merrill, William and Tsilivis, Nikolaos and Shukla, Aman},
  journal={arXiv preprint arXiv:2303.11873},
  year={2023}
}

@article{varma2023explaining,
  title={Explaining grokking through circuit efficiency},
  author={Varma, Vikrant and Shah, Rohin and Kenton, Zachary and Kram{\'a}r, J{\'a}nos and Kumar, Ramana},
  journal={arXiv preprint arXiv:2309.02390},
  year={2023}
}

@article{pearce2023machine,
  title={Do machine learning models memorize or generalize},
  author={Pearce, Adam and Ghandeharioun, Asma and Hussein, Nada and Thain, Nithum and Wattenberg, Martin and Dixon, Lucas},
  journal={People+ AI Research},
  year={2023}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{zhuang2020adabelief,
  title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},
  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18795--18806},
  year={2020}
}

@article{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{xie2024overlooked,
  title={On the overlooked pitfalls of weight decay and how to mitigate them: A gradient-norm perspective},
  author={Xie, Zeke and Xu, Zhiqiang and Zhang, Jingzhao and Sato, Issei and Sugiyama, Masashi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{xiang2023exploiting,
  title={Exploiting Network Compressibility and Topology in Zero-Cost NAS},
  author={Xiang, Lichuan and Hunter, Rosco and Xu, Minghao and Dudziak, {\L}ukasz and Wen, Hongkai},
  booktitle={AutoML Conference 2023},
  year={2023}
}

@inproceedings{sun2023unleashing,
  title={Unleashing the Power of Gradient Signal-to-Noise Ratio for Zero-Shot NAS},
  author={Sun, Zihao and Sun, Yu and Yang, Longxing and Lu, Shun and Mei, Jilin and Zhao, Wenxiao and Hu, Yu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5763--5773},
  year={2023}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{krogh1991simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John},
  journal={Advances in neural information processing systems},
  volume={4},
  year={1991}
}

@article{hochreiter1994simplifying,
  title={Simplifying neural nets by discovering flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={7},
  year={1994}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={arXiv preprint arXiv:1912.02178},
  year={2019}
}

@article{hanson1988comparing,
  title={Comparing biases for minimal network construction with back-propagation},
  author={Hanson, Stephen and Pratt, Lorien},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@book{hutter2019automated,
  title={Automated machine learning: methods, systems, challenges},
  author={Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year={2019},
  publisher={Springer Nature}
}

@article{imambi2021pytorch,
  title={PyTorch},
  author={Imambi, Sagar and Prakash, Kolla Bhanu and Kanagachidambaresan, GR},
  journal={Programming with TensorFlow: Solution for Edge Computing Applications},
  pages={87--104},
  year={2021},
  publisher={Springer}
}

@inproceedings{liang2019evolutionary,
  title={Evolutionary neural automl for deep learning},
  author={Liang, Jason and Meyerson, Elliot and Hodjat, Babak and Fink, Dan and Mutch, Karl and Miikkulainen, Risto},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={401--409},
  year={2019}
}

@inproceedings{sivaprasad2020optimizer,
  title={Optimizer benchmarking needs to account for hyperparameter tuning},
  author={Sivaprasad, Prabhu Teja and Mai, Florian and Vogels, Thijs and Jaggi, Martin and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={9036--9045},
  year={2020},
  organization={PMLR}
}

@article{dong2020autohas,
  title={Autohas: Differentiable hyper-parameter and architecture search},
  author={Dong, Xuanyi and Tan, Mingxing and Yu, Adams Wei and Peng, Daiyi and Gabrys, Bogdan and Le, Quoc V},
  journal={arXiv preprint arXiv:2006.03656},
  volume={4},
  number={5},
  year={2020}
}

@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}

@misc{tan2021efficientnetv2,
      title={EfficientNetV2: Smaller Models and Faster Training}, 
      author={Mingxing Tan and Quoc V. Le},
      year={2021},
      eprint={2104.00298},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{simonyan2015deep,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{huang2018densely,
      title={Densely Connected Convolutional Networks}, 
      author={Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
      year={2018},
      eprint={1608.06993},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{szegedy2014going,
      title={Going Deeper with Convolutions}, 
      author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
      year={2014},
      eprint={1409.4842},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@Techreport{krizhevsky2009learning,
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
 address = {Toronto, Ontario},
 institution = {University of Toronto},
 number = {0},
 publisher = {Technical report, University of Toronto},
 title = {Learning multiple layers of features from tiny images},
 year = {2009},
 title_with_no_special_chars = {Learning multiple layers of features from tiny images},
 url = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}
}

@misc{madry2019deep,
      title={Towards Deep Learning Models Resistant to Adversarial Attacks}, 
      author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
      year={2019},
      eprint={1706.06083},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}

@article{keskar2017large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Nocedal, Jorge and Tang, Ping and Mudigere, Dushyanth and Smelyanskiy, Mikhail},
  journal={arXiv preprint arXiv:1609.04836},
  year={2017}
}

@article{chaudhari2019entropy,
  title={Entropy-SGD: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@inproceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6389--6399},
  year={2018}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N and Topin, Nicholay},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={464--472},
  year={2017},
  organization={IEEE}
}

@article{li2019towards,
  title={Towards understanding the dynamics of the effective learning rate in deep learning},
  author={Li, Lei and Su, Hang and Ji, Shuiwang},
  journal={arXiv preprint arXiv:1909.09188},
  year={2019}
}

@article{lewkowycz2020large,
  title={Large learning rates facilitate convergence in deep learning},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}

@inproceedings{smith2018disciplined,
  title={A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay},
  author={Smith, Leslie N and Kindermans, Pieter-Jan and Ying, Clifford},
  booktitle={arXiv preprint arXiv:1803.09820},
  year={2018}
}

@article{jastrzebski2020break,
  title={The break-even point on the optimization trajectories of deep neural networks},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zakariya and Arpit, Devansh and Ballas, Nicolas and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2002.09572},
  year={2020}
}

@article{wu2024implicit,
  title={Implicit bias of gradient descent for logistic regression at the edge of stability},
  author={Wu, Jingfeng and Braverman, Vladimir and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{hayou2024lora+,
  title={Lora+: Efficient low rank adaptation of large models},
  author={Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  journal={arXiv preprint arXiv:2402.12354},
  year={2024}
}

@article{zhang2024riemannian,
  title={Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models},
  author={Zhang, Fangzhao and Pilanci, Mert},
  journal={arXiv preprint arXiv:2402.02347},
  year={2024}
}

@article{wang2024EfficientTrain_pp,
        title = {EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training},
       author = {Wang, Yulin and Yue, Yang and Lu, Rui and Han, Yizeng and Song, Shiji and Huang, Gao},
      journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
         year = {2024},
          doi = {10.1109/TPAMI.2024.3401036}
}

@article{zhang2024transformers,
  title={Why transformers need adam: A hessian perspective},
  author={Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2402.16788},
  year={2024}
}

@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@article{novikova2017e2e,
  title={The E2E dataset: New challenges for end-to-end generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  journal={arXiv preprint arXiv:1706.09254},
  year={2017}
}

@article{gu2024mix,
  title={Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models},
  author={Gu, Yuchao and Wang, Xintao and Wu, Jay Zhangjie and Shi, Yujun and Chen, Yunpeng and Fan, Zihan and Xiao, Wuyou and Zhao, Rui and Chang, Shuning and Wu, Weijia and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@article{kunstner2023noise,
  title={Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be},
  author={Kunstner, Frederik and Chen, Jacques and Lavington, Jonathan Wilder and Schmidt, Mark},
  journal={arXiv preprint arXiv:2304.13960},
  year={2023}
}

@misc{beyer2022betterplainvitbaselines,
      title={Better plain ViT baselines for ImageNet-1k}, 
      author={Lucas Beyer and Xiaohua Zhai and Alexander Kolesnikov},
      year={2022},
      eprint={2205.01580},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.01580}, 
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@misc{zhang2024adamminiusefewerlearning,
      title={Adam-mini: Use Fewer Learning Rates To Gain More}, 
      author={Yushun Zhang and Congliang Chen and Ziniu Li and Tian Ding and Chenwei Wu and Yinyu Ye and Zhi-Quan Luo and Ruoyu Sun},
      year={2024},
      eprint={2406.16793},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.16793}, 
}

@inproceedings{pan2022toward,
  title={Toward Understanding Why Adam Converges Faster Than SGD for Transformers},
  year={2023},
  author={Pan, Yan and Li, Yuanzhi},
  booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{mishchenko2023prodigy,
  title={Prodigy: An expeditiously adaptive parameter-free learner},
  author={Mishchenko, Konstantin and Defazio, Aaron},
  journal={arXiv preprint arXiv:2306.06101},
  year={2023}
}

@inproceedings{lizico,
  title={ZiCo: Zero-shot NAS via inverse Coefficient of Variation on Gradients},
  author={Li, Guihong and Yang, Yuedong and Bhardwaj, Kartikeya and Marculescu, Radu},
  year={2023},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@inproceedings{wangpicking,
  title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  booktitle={International Conference on Learning Representations}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6377--6389},
  year={2020}
}

@inproceedings{abdelfattahzero,
  title={Zero-Cost Proxies for Lightweight NAS},
  author={Abdelfattah, Mohamed S and Mehrotra, Abhinav and Dudziak, {\L}ukasz and Lane, Nicholas Donald},
  year={2021},
  booktitle={International Conference on Learning Representations}
}

@misc{big_vision,
  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},
  title = {Big Vision},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/google-research/big_vision}}
}

@misc{steiner2022trainvitdataaugmentation,
      title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers}, 
      author={Andreas Steiner and Alexander Kolesnikov and Xiaohua Zhai and Ross Wightman and Jakob Uszkoreit and Lucas Beyer},
      year={2022},
      eprint={2106.10270},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2106.10270}, 
}

@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{rombach2022highresolutionimagesynthesislatent,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.10752}, 
}

@misc{zhou2021theoreticallyunderstandingsgdgeneralizes,
      title={Towards Theoretically Understanding Why SGD Generalizes Better Than ADAM in Deep Learning}, 
      author={Pan Zhou and Jiashi Feng and Chao Ma and Caiming Xiong and Steven Hoi and Weinan E},
      year={2021},
      eprint={2010.05627},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.05627}, 
}


@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@misc{zhao2024deconstructingmakesgoodoptimizer,
      title={Deconstructing What Makes a Good Optimizer for Language Models}, 
      author={Rosie Zhao and Depen Morwani and David Brandfonbrener and Nikhil Vyas and Sham Kakade},
      year={2024},
      eprint={2407.07972},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.07972}, 
}

@misc{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@misc{sun2019optimizationdeeplearningtheory,
      title={Optimization for deep learning: theory and algorithms}, 
      author={Ruoyu Sun},
      year={2019},
      eprint={1912.08957},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.08957}, 
}


@techreport{collobert2004large,
  title={Large scale machine learning},
  author={Collobert, Ronan},
  year={2004},
  institution={Universit{\'e} de Paris VI}
}

@article{JMLR:v12:duchi11a,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}

@misc{graves2014generatingsequencesrecurrentneural,
      title={Generating Sequences With Recurrent Neural Networks}, 
      author={Alex Graves},
      year={2014},
      eprint={1308.0850},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1308.0850}, 
}

@misc{zeiler2012adadeltaadaptivelearningrate,
      title={ADADELTA: An Adaptive Learning Rate Method}, 
      author={Matthew D. Zeiler},
      year={2012},
      eprint={1212.5701},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1212.5701}, 
}


@inproceedings{dozat.2016,
	title        = {Incorporating {Nesterov Momentum into Adam}},
	author       = {Dozat, Timothy},
	booktitle    = {Proceedings of the 4th International Conference on Learning Representations},
	pages        = {1--4},
	year         = 2016
}

@misc{xie2022adaptiveinertiadisentanglingeffects,
      title={Adaptive Inertia: Disentangling the Effects of Adaptive Learning Rate and Momentum}, 
      author={Zeke Xie and Xinrui Wang and Huishuai Zhang and Issei Sato and Masashi Sugiyama},
      year={2022},
      eprint={2006.15815},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.15815}, 
}

@misc{wang2024provableadaptivityadamnonuniform,
      title={Provable Adaptivity of Adam under Non-uniform Smoothness}, 
      author={Bohan Wang and Yushun Zhang and Huishuai Zhang and Qi Meng and Ruoyu Sun and Zhi-Ming Ma and Tie-Yan Liu and Zhi-Quan Luo and Wei Chen},
      year={2024},
      eprint={2208.09900},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.09900}, 
}

@misc{crawshaw2022robustnessunboundedsmoothnessgeneralized,
      title={Robustness to Unbounded Smoothness of Generalized SignSGD}, 
      author={Michael Crawshaw and Mingrui Liu and Francesco Orabona and Wei Zhang and Zhenxun Zhuang},
      year={2022},
      eprint={2208.11195},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.11195}, 
}

@misc{ghiasi2023improvingrobustnessadaptiveweight,
      title={Improving Robustness with Adaptive Weight Decay}, 
      author={Amin Ghiasi and Ali Shafahi and Reza Ardekani},
      year={2023},
      eprint={2210.00094},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.00094}, 
}

@article{ward2020adagrad,
  title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={219},
  pages={1--30},
  year={2020}
}

@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  pages={2},
  year={2012}
}



@misc{bernstein2018signsgdcompressedoptimisationnonconvex,
      title={signSGD: Compressed Optimisation for Non-Convex Problems}, 
      author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2018},
      eprint={1802.04434},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.04434}, 
}

@misc{wu2022dissectinghessianunderstandingcommon,
      title={Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks}, 
      author={Yikai Wu and Xingyu Zhu and Chenwei Wu and Annie Wang and Rong Ge},
      year={2022},
      eprint={2010.04261},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.04261}, 
}

@misc{liu2024sophiascalablestochasticsecondorder,
      title={Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training}, 
      author={Hong Liu and Zhiyuan Li and David Hall and Percy Liang and Tengyu Ma},
      year={2024},
      eprint={2305.14342},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14342}, 
}

@misc{xiao2021earlyconvolutionshelptransformers,
      title={Early Convolutions Help Transformers See Better}, 
      author={Tete Xiao and Mannat Singh and Eric Mintun and Trevor Darrell and Piotr DollÃ¡r and Ross Girshick},
      year={2021},
      eprint={2106.14881},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2106.14881}, 
}

@misc{li2023memoryefficientoptimizers4bit,
      title={Memory Efficient Optimizers with 4-bit States}, 
      author={Bingrui Li and Jianfei Chen and Jun Zhu},
      year={2023},
      eprint={2309.01507},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.01507}, 
}

@article{DBLP:journals/corr/abs-2110-02861,
  author       = {Tim Dettmers and
                  Mike Lewis and
                  Sam Shleifer and
                  Luke Zettlemoyer},
  title        = {8-bit Optimizers via Block-wise Quantization},
  journal      = {CoRR},
  volume       = {abs/2110.02861},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.02861},
  eprinttype    = {arXiv},
  eprint       = {2110.02861},
  timestamp    = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-02861.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@inproceedings{dettmers2023case,
  title={The case for 4-bit precision: k-bit inference scaling laws},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={7750--7774},
  year={2023},
  organization={PMLR}
}

@misc{zhang2020adaptivemethodsgoodattention,
      title={Why are Adaptive Methods Good for Attention Models?}, 
      author={Jingzhao Zhang and Sai Praneeth Karimireddy and Andreas Veit and Seungyeon Kim and Sashank J Reddi and Sanjiv Kumar and Suvrit Sra},
      year={2020},
      eprint={1912.03194},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1912.03194}, 
}

@misc{kunstner2023noisemainfactorgap,
      title={Noise Is Not the Main Factor Behind the Gap Between SGD and Adam on Transformers, but Sign Descent Might Be}, 
      author={Frederik Kunstner and Jacques Chen and Jonathan Wilder Lavington and Mark Schmidt},
      year={2023},
      eprint={2304.13960},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.13960}, 
}

@inproceedings{chen2020tenas,
  title={Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective},
  author={Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@misc{battash2023revisitingnoisemodelstochastic,
      title={Revisiting the Noise Model of Stochastic Gradient Descent}, 
      author={Barak Battash and Ofir Lindenbaum},
      year={2023},
      eprint={2303.02749},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.02749}, 
}

@misc{evci2022gradientflowsparseneural,
      title={Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win}, 
      author={Utku Evci and Yani A. Ioannou and Cem Keskin and Yann Dauphin},
      year={2022},
      eprint={2010.03533},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.03533}, 
}

@misc{bhardwaj2021doestopologyinfluencegradient,
      title={How does topology influence gradient propagation and model performance of deep networks with DenseNet-type skip connections?}, 
      author={Kartikeya Bhardwaj and Guihong Li and Radu Marculescu},
      year={2021},
      eprint={1910.00780},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1910.00780}, 
}

@misc{lei2023balanceessenceacceleratingsparse,
      title={Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction}, 
      author={Bowen Lei and Dongkuan Xu and Ruqi Zhang and Shuren He and Bani K. Mallick},
      year={2023},
      eprint={2301.03573},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.03573}, 
}

@article{kumar2017weight,
  title={On weight initialization in deep neural networks},
  author={Kumar, Siddharth Krishna},
  journal={arXiv preprint arXiv:1704.08863},
  year={2017}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{doddington2002automatic,
  title={Automatic evaluation of machine translation quality using n-gram co-occurrence statistics},
  author={Doddington, George},
  booktitle={Proceedings of the second international conference on Human Language Technology Research},
  pages={138--145},
  year={2002}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@inproceedings{cohen2024adaptivegradientmethodsedge,
title	= {Adaptive Gradient Methods at the Edge of Stability},
author	= {Behrooz Ghorbani and Daniel Suo and David Cardoze and George Dahl and Jeremy Cohen and Justin Gilmer and Naman Agarwal and Shankar Krishnan and Sourabh Medapati and Zack Nado},
year	= {2022},
URL	= {https://arxiv.org/abs/2207.14484}
}

@article{Nes83,
  author    = {Yu. E. Nesterov},
  title     = {A method of solving a convex programming problem with convergence rate $O(1/k^2)$},
  journal   = {Doklady Akademii Nauk SSSR},
  year      = {1983},
  volume    = {269},
  number    = {3},
  pages     = {543--547},
  url       = {http://mi.mathnet.ru/dan46009},
  note      = {MathNet: \url{http://mi.mathnet.ru/dan46009}, MathSciNet: \url{http://mathscinet.ams.org/mathscinet-getitem?mr=0701288}, zbMATH: \url{https://zbmath.org/?q=an:0535.90071}}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{frankle2020pruning,
  title={Pruning neural networks at initialization: Why are we missing the mark?},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:2009.08576},
  year={2020}
}

@article{lee2018snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}