\section{Related Work}
\label{sec:related_work}


\textbf{Adaptive Gradient Methods:}Stochastic gradient descent (SGD) is an efficient optimization method commonly used in deep learning, but it \textbf{struggles with tasks that have non-stationary objectives or involve very noisy and/or sparse gradients}~\cite{kingma2014adam}, often requiring extensive hyperparameter tuning.
To improve upon these limitations, adaptive gradient methods were developed to continuously and dynamically adjust learning rates for individual parameters throughout the training process~\cite{JMLR:v12:duchi11a, graves2014generatingsequencesrecurrentneural, zeiler2012adadeltaadaptivelearningrate}, with the Adam optimizer becoming particularly popular. Adam combines features from AdaGrad~\cite{ward2020adagrad}, which effectively manages sparse gradients, and RMSProp~\cite{hinton2012neural}, which is suitable for online and non-stationary tasks, allowing it to outperform SGD in many cases with less tuning effort.
However, Adam has its own challenges, leading to the creation of enhancements such as AdamW~\cite{loshchilov2019decoupled}, which introduces decoupled weight decay for better generalization, and adaptations~\cite{dozat.2016} that incorporate Nesterov momentum for faster convergence. To address early training noise, warm-up phases and Rectified Adam~\cite{liu2021variance} have been proposed. Additionally, Adaptive Weight Decay~\cite{ghiasi2023improvingrobustnessadaptiveweight} further improves convergence, while ~\cite{mishchenko2023prodigy} introduced a dynamic component for automatic learning rate adjustments within the Adam framework.


\textbf{Adam in Transformer Realm:} Transformers~\cite{vaswani2017attention} have become essential in modern deep learning, particularly in natural language processing. While the Adam~\cite{kingma2014adam} optimizer generally outperforms Stochastic Gradient Descent (SGD) in training Transformer architectures~\cite{xiao2021earlyconvolutionshelptransformers}, it has a significant downside: as model sizes grow, Adam's memory requirements, which are twice that of SGD due to first and second-order momentum storage~\cite{kingma2014adam}, become a concern.
To mitigate this overhead, researchers have explored methods like sign-based optimization~\cite{bernstein2018signsgdcompressedoptimisationnonconvex, kunstner2023noise} and low-precision quantization~\cite{li2023memoryefficientoptimizers4bit, DBLP:journals/corr/abs-2110-02861, dettmers2022llm, dettmers2023case}, although these can compromise performance. Studies have shown that Adam’s adaptive learning rates based on gradient norm history contribute to its performance advantage~\cite{zhang2024transformers}, whereas SGD lacks this capability. However, finding the right learning rate scale for SGD to match Adam’s performance remains unresolved.
Adam's insights, rooted in RMSprop~\cite{hinton2012neural}, suggest that \textbf{a global learning rate should be adjusted according to local gains}. Researchers have developed block-wise dynamic learning rates that perform comparably to Adam with reduced memory use~\cite{zhang2024adamminiusefewerlearning}. Similar trends are seen in parameter-efficient fine-tuning, emphasizing the importance of \textbf{local gains} for learning rate adjustments~\cite{zhang2024riemannian}.
Furthermore, theoretical analyses have raised doubts about the necessity of adaptive gradient methods. While Adam offers practical benefits, research~\cite{liconvergence23} indicates that the convergence rates of Adam and SGD are not significantly different.


\textbf{Gradient at Initialization:} 
Recent research has highlighted the importance of gradient patterns at initialization, demonstrating a strong correlation between these early signals and a model’s eventual performance. Pruning at Initialization (PaI) methods, inspired by the lottery ticket hypothesis~\cite{frankle2018lottery}, leverage this principle by identifying high-potential subnetworks before training begins. These techniques typically remove parameters associated with the lowest gradients or the weakest early learning responses~\cite{tanaka2020pruning, frankle2020pruning, lee2018snip}, emphasizing how initial gradient-based criteria can guide the formation of effective, sparse architectures.

From a gradient sparsity perspective, PaI methods effectively preserve the essential characteristics of the full network’s gradient distribution. The resulting subnetworks maintain similar gradient variance and overall gradient magnitude by masking out parameters tied to minimal gradient or learning response. This careful selection ensures that the pruned models exhibit performance levels on par with their unpruned counterparts despite operating with significantly fewer parameters.


A similar observation has also been revealed in Zero-Cost NAS studies~\cite{abdelfattahzero, lizico, xiang2023exploiting}, which aim to predict the performance of untrained networks by analyzing gradient patterns, finding that gradient score rankings—such as the gradient sum—correlate more strongly with architectural structures than with data batches or initialization parameters. Research by ~\cite{bhardwaj2021doestopologyinfluencegradient} highlights that gradient flow patterns are inherently linked to a network's architecture. Additionally, studies~\cite{lizico, xiang2023exploiting} show that gradient sparsity, measured by mean and variance, is closely related to convergence rates and generalization ability. They emphasize calculating gradient sparsity block-wise due to the diverse distributions of gradients across parameter blocks. Moreover, \cite{lei2023balanceessenceacceleratingsparse} suggests that a balanced training procedure with low-variance gradients enhances sparse training.






