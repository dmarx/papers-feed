\section{Conclusion}
In summary, our results demonstrate that simply applying selective learning rate scaling at initialization (SGD-SaI) can unlock performance comparable to—if not better than—leading adaptive gradient methods like AdamW, all while retaining the simplicity and efficiency of SGDM. By leveraging g-SNR to guide parameter group scaling, SGD-SaI not only mitigates early training imbalances but also substantially reduces optimizer memory overhead, enabling more resource-efficient model training. Its robustness across a wide range of Transformer-based tasks, including ImageNet classification with ViT, GPT-2 pretraining, LoRA fine-tuning, and diffusion modelling, underscores its versatility and practicality. 

% Taken together, these findings offer a compelling alternative to conventional adaptive methods, paving the way for more efficient, memory-friendly, and high-performing optimization strategies in deep learning.


\section{Limitation}
While SGD-SaI demonstrates promising results across various Transformer-based tasks, our study is constrained by limited computational resources, preventing us from conducting large-scale pre-training on more extensive models such as Llama-2-7B. This remains an avenue for future research. However, to address the efficiency challenges of training larger models, we have performed detailed profiling of GPU memory usage and optimizer step speed on these architectures. These preliminary analyses indicate the potential scalability of SGD-SaI, but comprehensive evaluations on larger-scale models are necessary to establish its effectiveness and efficiency in such settings fully. Moreover, our methods ensure a steady and stable update during training, allowing the model to converge better in a given task with sufficient training steps. Thus, we might observe that the convergence speed is relatively lower than Adam's in the early stage of training; as our primary focus is to investigate the effectiveness of the SaI approach, we left the acceleration of convergence speed in future work.



% \section{Limitation}
% While computational constraints prevented us from pre-training large language models (LLMs) such as GPT2-1.5B or Llama-7B from scratch, we comprehensively evaluated SGD-SaI's effectiveness in reducing GPU memory usage while improving optimization speed and accuracy. We validated our method's optimization capabilities across multiple domains, including Vision Transformers (ViT) on ImageNet-1K and fine-tuning tasks for both LLMs and Diffusion Models (DMs). Our code will be made publicly available to support large-scale training in the LLM and DM communities. Future work will focus on analyzing SGD-SaI's performance in large-scale pre-training scenarios for LLMs and DMs

% Due to computational resource limitations, we are unable to pre-train large language models (LLMs) such as GPT2-1.5B or Llama-7B from scratch. However, we have thoroughly investigated how SGD-SaI helps reduce GPU memory usage while enhancing optimization speed and accuracy. Additionally, we validated its optimization capabilities by applying it to the transformer architecture in Vision Transformers (ViT) on the ImageNet-1K dataset, as well as fine-tuning tasks for both LLMs and Diffusion Models (DMs). We aim to release the code to the community to support large-scale training in the LLMs and DMs domains. We will try to the analysis of large-scale pre-training performance for LLMs and DMs in future studies.