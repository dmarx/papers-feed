%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
% \onecolumn

% \clearpage
% \setcounter{page}{1}
% \maketitlesupplementary

\section{More Experiments Details}

\subsection{Details for ViT Experiments}\label{sec:sup:vit_gs_details}
In this section, we will list list the settings of the experiment regarding to Section~\ref{sec:exp:pretrain_vit}.

\textbf{Hyperparameter Settings:} We start by following the settings in~\cite{beyer2022betterplainvitbaselines, steiner2022trainvitdataaugmentation, big_vision};  Specifically, we include Nesterov-SGD as a baseline, offering better performance than naive SGD. All optimizers are tested using a grid search within the same hyperparameter ranges: learning rate \( lr \in \{0.1, 0.01, 0.001, 0.0001\} \) and weight decay \( wd \in \{0.01, 0.001, 0.0001\} \).


\subsection{Details for CNN Experiments}\label{see:sup:cnn_gs_details}
In this section, we will list the settings of the experiment regarding to Section~\ref{sec:exp:pretrain_cnn}.

\textbf{Models and Datasets:}  We follow the same settings and train some CNN-based architectures proposed in NATS-Benchmark ~\cite{dong2021nats}. We test the optimizers on CIFAR-10/CIFAR-100 ~\cite{krizhevsky2009learning} and ImageNet16-120 ~\cite{chrabaszcz2017downsampled}. Based on the NATS-Benchmark work, we test different sizes of architectures. Here, we select ten architectures with top-10 validation accuracy and one architecture with bottom-1 validation accuracy in terms of different datasets and training epochs to present. 
% For a reasonable and fair comparison, we choose 2 of 3 reported epochs, which are training of a total of 12 epochs and 90 epochs.

\textbf{Hyperparameter Settings:} The optimal learning rate and weight decay are chosen by performing the grid search. The learning rate and weight decay are selected from $\eta \in \{0.1, 0.01, 0.001\}$ and $\lambda \in \{0.5, 0.05, 0.005, 0.0005\}$, respectively. We use the same cosine annealing scheduler on three datasets without learning rate warmup. We use the same data augmentation methods and set the batch size to 256 for all datasets. The experiments are designed to run for full training without early stopping. There is no linear scaling on the initial weight decay either since we are doing the grid search within a feasible range. The seed is only {777} which is the same seed reported by NATS-Benchmark on Size Search Space. The original NATS-Benchmark were produced using SGD with a fixed learning rate 0.1 and weight decay 0.0005 and the default setting of the Nesterov Momentum. For fair comparison, we apply the same grid search policy for SGD, as the baseline with or without Nesterov Momentum. 

\textbf{Results:} The performance of various architectures has been represented as histograms showing top-1 accuracy on different datasets. Fig.~\ref{fig:fig_sss_performance_distribution_histogram} demonstrates that our method outperforms other optimizers in terms of evaluation accuracies within the same hyperparameter search space.


\subsection{Extra Results for ResNet18 on CIFAR10}


As a classic model of the CNNs, we also conduct the grid search on ResNet18 as an extended experiment. 

\textbf{Models and Datasets:} We follow the similar setting in the Section~\ref{sec:exp:pretrain_cnn}. We particularly choose the CIFAR-10~\cite{krizhevsky2009learning} as the dataset we test on. We test on the classic ResNet18 model.

\textbf{Hyperparameter Settings:} Since we are focusing on a single model with one dataset—unlike the NAST-Benchmark CNN experiments discussed in Section~\ref{sec:exp:pretrain_cnn}—we are scaling up our search by exploring a wider range of learning rates and weight decays. The learning rates are chosen from the set \(\eta \in \{0.1, 0.01, 0.001, 0.0001\}\) and the weight decays from \(\lambda \in \{0.5, 0.05, 0.005, 0.0005, 0.00005\}\). We will repeat our grid search three times using three different random seeds. The random seeds used for the experiments are \{42, 888, 999\}.
We opted for a step learning rate scheduler rather than a cosine annealing scheduler to test our method's resilience to different learning rate scheduling policies. The learning rate will decrease by a factor of 10 every 80 epochs, with a total of 200 epochs for training.
Our data augmentation methods remain consistent, with a batch size set to 128. These experiments will run for the entire training duration without early stopping, and there will be no linear scaling applied to the initial weight decay, as we are conducting a grid search within a reasonable range. 
The distribution of accuracies averaged over the three seeds for each hyperparameter combination is depicted in Fig.\ref{fig:fig_vit_cnn_performance_3d_surface}. The best performance of each optimizer, along with the optimal learning rate and weight decay, is annotated with red numbers on the graph. For simplicity, we are only testing the Stochastic Gradient Descent with Momentum (SGDM) as the baseline. The momentum is set to the default value of 0.9, consistent with both the Adam(W) optimizer and our method to ensure fair comparison.

\textbf{Results: } The grid search results are shown in the Fig.~\ref{fig:fig_vit_cnn_performance_3d_surface}. Not only does our method converge better (ours 95.36\% v.s. SGDM 95.26\%), but it also demonstrates greater resilience to changes in hyperparameters. This means the performance is less likely to downgrade compared to SGDM.

All the experiments in this paper were conducted using various types of GPUs, including NVIDIA GeForce RTX 3090, NVIDIA A100 PCIe 40GB, and NVIDIA A100 80GB. To ensure consistent experimental conditions, each experiment was conducted using only one GPU type.


% SGD Pseudocode
\begin{figure}[t]
\begin{algorithm}[H]
    \caption{SGD}\label{algorithm:sgd}
    \footnotesize
    \begin{algorithmic}[1]
        \Require $t$ (step), $\eta$ (lr), $\theta^i$ (i-th params), $L(\theta)$ (loss function), $\lambda$ (weight decay), $\mu$ (momentum), $\tau$ (dampening),  $maximize$ 
        
        \Repeat
        \For{$t\gets 1$}
            \State $g^i_t \gets \nabla L(\theta^i_{t-1})$
            \State 
            \State \text{/* do weight decay */}
            \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            \State
            \State \text{/* do momentum */}
            \If{$\mu \neq 0$ \textbf{and} $t > 1$}
                \State {$m^i_t \gets \mu m^i_{t-1} + (1 - \tau) g^i_t$}
            \Else
                \State {$m^i_t \gets g^i_t$}
            \EndIf
            \State
            \If{$maximize$}
                \State $\theta^i_t \gets \theta^i_{t-1} + \eta m^i_t$
            \Else
                \State $\theta^i_t \gets \theta^i_{t-1} - \eta m^i_t$
            \EndIf
        \EndFor
        \Until {epochs end}
    \end{algorithmic}
\end{algorithm}
% \caption{Pseudocode for Adam-mini.}
% \label{fig:adammini_pseudocode}
\end{figure}


% Adam Pseudocode
\begin{figure}[t]
\begin{algorithm}[H]
    \caption{Adam}\label{algorithm:adam}
    \begin{algorithmic}[1]
        \Require $t$ (step), $\eta$ (lr), $\theta^i$ (i-th params), $L(\theta)$ (loss function), $\lambda$ (weight decay), $\beta_1, \beta_2$ (betas),  $maximize$ 
        
        \Repeat
        \For{$t\gets 1$}
            \If{$maximize$}
                \State $g^i_t \gets -\nabla L(\theta^i_{t-1})$
            \Else
                \State $g^i_t \gets \nabla L(\theta^i_{t-1})$
            \EndIf
            \State 
            \State \text{/* do weight decay */}
            \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            \State
            \State \text{/* do momentum */}
            \If{$t > 1$}
                \State {$m^i_t \gets \beta_1 m^i_{t-1} + \text{$(1-\beta_1)$} g^i_t $}
                \State $v^i_t \gets \beta_2 v^i_{t-1} + (1-\beta_2) (g^i_t)^2 $
            \Else
                \State \text{$m^i_t \gets (1-\beta_1) g^i_t $}; $v^i_t \gets (1-\beta_2) (g^i_t)^2 $
            \EndIf
            \State{$\hat{m^i_t} \gets \frac{m^i_t}{1-\beta_1^t}$};{$\hat{v^i_t} \gets \frac{v^i_t}{1-\beta_2^t}$}
            \State
            
            % \State{\text{/* do weight decay */}}
            % % \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            % \State \text{$\theta^i_t \gets \theta^i_{t-1} - \lambda \eta \theta^i_{t-1}$}
            % \State
            \State $\theta^i_t \gets \theta^i_{t-1} - \text{$ \frac{\eta}{\sqrt{\hat{v^i_t}} + \epsilon}m^i_t $} $

        \EndFor
        \Until {epochs end}
    \end{algorithmic}
\end{algorithm}
% \caption{Pseudocode for Adam.}
% \label{fig:adam_pseudocode}
\end{figure}

\begin{figure}[t]
\begin{algorithm}[H]
    \caption{AdamW}\label{algorithm:adamw}
    \begin{algorithmic}[1]
        \Require $t$ (step), $\eta$ (lr), $\theta^i$ (i-th params), $L(\theta)$ (loss function), $\lambda$ (weight decay), $\beta_1, \beta_2$ (betas),  $maximize$ 
        
        \Repeat
        \For{$t\gets 1$}
            \If{$maximize$}
                \State $g^i_t \gets -\nabla L(\theta^i_{t-1})$
            \Else
                \State $g^i_t \gets \nabla L(\theta^i_{t-1})$
            \EndIf
            % \State 
            % \State \text{/* do weight decay */}
            % \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            \State \text{/* do momentum */}
            \If{$t > 1$}
                \State {$m^i_t \gets \beta_1 m^i_{t-1} + \text{$(1-\beta_1)$} g^i_t $}
                \State $v^i_t \gets \beta_2 v^i_{t-1} + (1-\beta_2) (g^i_t)^2 $
            \Else
                \State \text{$m^i_t \gets (1-\beta_1) g^i_t $}; $v^i_t \gets (1-\beta_2) (g^i_t)^2 $
            \EndIf
            \State{$\hat{m^i_t} \gets \frac{m^i_t}{1-\beta_1^t}$};{$\hat{v^i_t} \gets \frac{v^i_t}{1-\beta_2^t}$}
            \State
            
            \State{\text{/* do weight decay */}}
            % \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            \State \text{$\theta^i_t \gets \theta^i_{t-1} - \lambda \eta \theta^i_{t-1}$}
            \State
            \State $\theta^i_t \gets \theta^i_{t-1} - \text{$ \frac{\eta}{\sqrt{\hat{v^i_t}} + \epsilon}m^i_t $} $

        \EndFor
        \Until {epochs end}
    \end{algorithmic}
\end{algorithm}
% \caption{Pseudocode for AdamW.}
% \label{fig:adamw_pseudocode}
\end{figure}

\section{Optimizer Analysis} \label{sec:sup:optimizer_breakdown}
% \subsection{Adam, AdamW, Prodigy, SGD with the pseudocode and memory lower boundary analysis}
This section provides a supplementary analysis for Section~\ref{sec:results}. We will detail the optimizers and empirically estimate the lower boundary of the state tensors in memory.

\subsection{Break Down SGD}
% SGD is faster than adaptive gradient methods due to its simplicity. The difference between adaptive gradient methods and SGD is that SGD utilizes fixed learning rate, while other adaptive methods continuously scales the fixed learning rate accordingly for each block, either on the element level (Adam(W)) or on the block level (Adam-mini). 

% The advantages of SGD can be listed as follows: a. runtime efficiency: has a fast and efficient iteration runtime for each optimization step; b. memory efficiency: requires only one time of the gradients when using momentum, and requires no extra memory overhead if no momentum applied.

Stochastic Gradient Descent (SGD) is faster than adaptive gradient methods primarily due to its simplicity. The key difference between SGD and these adaptive methods is that SGD uses a fixed learning rate, while adaptive methods adjust the learning rate dynamically for each parameter at each step. This adjustment can be done at the element level, as seen in Adam(W), or at the block level, like in Adam-mini.

The advantages of SGD can be summarized as follows: 
a. Runtime efficiency: It offers a fast and efficient iteration time for each optimization step.  
b. Memory efficiency: When using momentum, SGD requires only one instance of the gradients~\ref{algorithm:sgd} and incurs no additional memory overhead when momentum is not applied.

\subsection{Break Down Adam(W)}
The main difference between Adam and AdamW lies in how they apply weight decay. AdamW applies a direct penalty to the weights themselves, which is known as Decoupled Weight Decay, whereas Adam applies the penalty to the gradients at the outset, utilizing L2 Regularization. 
Both algorithms enhance adaptability by scaling the learning rate for each parameter individually. In every optimization step, the scaling ratios are recalculated by dividing the first-order moment by the square root of the second-order moment, both of which are maintained and updated in state tensors.
As illustrated in Alog.~\ref{algorithm:adam} (line 17 for Adam) and Alog.~\ref{algorithm:adamw} (line 14 for AdamW), the first-order moment $m$ and the second-order moment $v$ are stored in GPU memory throughout the entire training process.

Generally, the estimated minimum memory requirement for the state tensors in both Adam and AdamW is approximately twice the size of the gradient tensors. This is because $m$ and $v$ share the same shape as the corresponding gradient tensors.

\subsection{Break Down Adam-mini}

Adam-mini is a variant of the Adam optimizer. As shown in the Alog.~\ref{algorithm:adam_mini}, Adam-mini redesigns the adaptive update rules by using the mean of the squared gradients instead of the original squared gradients in most layers except for the embedding layer. This version of Adam reduces the number of learning rates to the number of blocks in each layer while keeping the update rules unchanged in the embedding layers. 
Therefore, the reduction in memory usage is influenced by the proportion of non-embedding parameters in the model. This limitation not only restricts the potential for memory savings but also incurs additional computational costs due to the extra operations (see Alog.~\ref{algorithm:adam_mini} Line 15, 16) needed when calculating the new $v$ compared to the original Adam algorithm.

Regarding the lower boundary of the state tensor memory, as mentioned in \cite{zhang2024adamminiusefewerlearning}, Adam-mini can reduce the memory used for the Adam optimizer's $v$ by at least 90\%. This results in a memory cost savings of approximately 45\% to 50\% compared to the original Adam. 

% Adam-mini
\begin{figure}[t]
\begin{algorithm}[H]
    \caption{Adam-mini}\label{algorithm:adam_mini}
    \begin{algorithmic}[1]
        \Require $t$ (step), $\eta$ (lr), $\theta^i$ (i-th params), $L(\theta)$ (loss function), $\lambda$ (weight decay), $\beta_1, \beta_2$ (betas),  $maximize$ 
        
        \Repeat
        \For{$t\gets 1$}
            \If{$maximize$}
                \State $g^i_t \gets -\nabla L(\theta^i_{t-1})$
            \Else
                \State $g^i_t \gets \nabla L(\theta^i_{t-1})$
            \EndIf
            % \State 
            % \State \text{/* do weight decay */}
            % \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            \State
            \State \text{/* do momentum */}
            \If{$t > 1$}
                \State {$m^i_t \gets \beta_1 m^i_{t-1} + \text{$(1-\beta_1)$} g^i_t $}
                \If{$\theta^i_{t-1} \in embedding\_layer$}
                    \State $v^i_t \gets \beta_2 v^i_{t-1} + (1-\beta_2) (g^i_t)^2 $
                \Else
                    \State Divide $\theta^i_{t-1}$ into Q,K heads if needed.
                    \State $v^i_t \gets \beta_2 v^i_{t-1} + (1-\beta_2) Mean( (g^i_t)^2 ) $
                \EndIf
            \Else
                \State \text{$m^i_t \gets (1-\beta_1) g^i_t $}; $v^i_t \gets (1-\beta_2) (g^i_t)^2 $
            \EndIf
            \State{$\hat{m^i_t} \gets \frac{m^i_t}{1-\beta_1^t}$};{$\hat{v^i_t} \gets \frac{v^i_t}{1-\beta_2^t}$}
            \State
            
            \State{\text{/* do weight decay */}}
            % \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            \State \text{$\theta^i_t \gets \theta^i_{t-1} - \lambda \eta \theta^i_{t-1}$}
            \State
            \State $\theta^i_t \gets \theta^i_{t-1} - \text{$ \frac{\eta}{\sqrt{\hat{v^i_t}} + \epsilon}m^i_t $} $

        \EndFor
        \Until {epochs end}
    \end{algorithmic}
\end{algorithm}
% \caption{Pseudocode for Adam-mini.}
% \label{fig:adammini_pseudocode}
\end{figure}


\subsection{Break Down Prodigy}

Prodigy is one of the most popular variants of the Adam optimizer, offering a new approach to calculating the step size. It alleviates the need for extensive learning rate tuning. While most of Adam's update rules remain unchanged, Prodigy introduces a new scaling ratio, denoted as $d$ (for D-Adaption), which adaptively adjusts the learning rate. To update the scaling ratio $d$ for each optimization step, Prodigy requires the maintenance of two additional tensors: the initial weight value $x_0$ and the denominator $s$, both of which share the same shape as the gradient.

As a result, the lower boundary for estimating the memory required by Prodigy's state tensor is approximately four times the size of the gradient by default. The majority of the memory for the tensor state is occupied by four tensors: $m, v, x_0, s$ (Algo.~\ref{algorithm:prodigy} Line 11, 12, 17, 18). Consequently, Prodigy can be very memory-intensive when applied to large models with billions of parameters.


% Adam-mini
\begin{figure}[t]
\begin{algorithm}[H]
    \caption{Prodigy}\label{algorithm:prodigy}
    \begin{algorithmic}[1]
        \Require $t$ (step), $\eta$ (lr, default 1 with cosine annealing), $\theta^i$ (i-th params), $L(\theta)$ (loss function), $\lambda$ (weight decay), $\beta_1, \beta_2$ (betas),  $maximize$, $d_0 > 0$ (default $1e^{-6}$), $x_0$
        
        \Repeat
        \For{$t\gets 1$}
            \If{$maximize$}
                \State $g^i_t \gets -\nabla L(\theta^i_{t-1})$
            \Else
                \State $g^i_t \gets \nabla L(\theta^i_{t-1})$
            \EndIf
            % \State 
            % \State \text{/* do weight decay */}
            % \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            \State
            \State \text{/* do momentum */}
            \If{$t > 1$}
                \State {$m^i_t \gets \beta_1 m^i_{t-1} + \text{$(1-\beta_1)$} d_t g^i_t $}
                \State $v^i_t \gets \beta_2 v^i_{t-1} + (1-\beta_2) d_t^2 (g^i_t)^2 $
                
            \Else
                \State \text{$m^i_t \gets (1-\beta_1) d_t g^i_t $}; $v^i_t \gets (1-\beta_2) d_t^2 (g^i_t)^2 $
                \State $r_{t-1} = 0$; $s_{t-1} = 0$
            \EndIf

            \State{$r_t = \sqrt{\beta_2} r_{t-1} + (1-\sqrt{\beta_2}) \eta d_t^2 \langle g^i_t, x_0-x_t \rangle$}

            \State{$s_t = \sqrt{\beta_2} s_{t-1} + (1-\sqrt{\beta_2}) \eta d_t^2 g^i_t $}
        
            \State{$ \hat{d_{t+1}} = \frac{r_k}{\|s_t\|_1} $}

            \State{$ d_{t+1} = max(d_k, \hat{d_{t+1}})$}
            \State
            
            \State{\text{/* do weight decay */}}
            % \State \text{$g^i_t \gets g^i_t + \lambda \theta^i_{t-1}$}
            \State \text{$\theta^i_t \gets \theta^i_{t-1} - \lambda \eta \theta^i_{t-1}$}
            \State
            \State $\theta^i_t \gets \theta^i_{t-1} - \text{$ \frac{\eta d_t}{\sqrt{v^i_t} + d_t \epsilon}m^i_t $} $

        \EndFor
        \Until {epochs end}
    \end{algorithmic}
\end{algorithm}
% \caption{Pseudocode for Prodigy.}
% \label{fig:prodigy_pseudocode}
\end{figure}



\section{Profiling Results on "g-SNR Calculation" Stage}

As discussed in Section~\ref{sec:sup:optimizer_breakdown}, our method requires calculating the scale ratio during the initial step of the optimization process, which we refer to as the "g-SNR Calculation" stage. This procedure may take extra time because it involves calculating both the gradient norm and its standard deviation. As shown in Table~\ref{table:rtx3090_warmup_result} and Table~\ref{table:a100_warmup_result}, although square root operations are generally considered more computationally intensive than standard float addition and multiplication, the time taken for the "g-SNR Calculation" is still relatively small. Since this calculation is performed only once during each training procedure by design, its duration is negligible compared to the overall training iterations. Therefore, our method remains efficient for optimization in the long run.





\begin{table}[!htbp]
\centering
\small % Reduce font size for CVPR two-column format
\setlength{\tabcolsep}{6pt} % Reduce column padding
\renewcommand{\arraystretch}{1.3} % Adjust row height
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l|l|l}
\hline
Model                      & Method           & Iter Times (ms) & g-SNR Calc (ms) \\ \hline
\multirow{2}{*}{ViT-S/16}  & SGDM             & 12.2 $\pm$ 2.9  & 0                 \\
                           & SGD-SaI (ours) & 13.7 $\pm$ 3.8  & 14.5              \\ \hline
\multirow{2}{*}{ViT-H/14}  & SGDM             & 48.6 $\pm$ 7.8  & 0                 \\
                           & SGD-SaI (ours) & 65.5 $\pm$ 10.0 & 43.3              \\ \hline
\multirow{2}{*}{GPT2-1.5B} & SGDM             & 287.4 $\pm$ 0.9 & 0                 \\
                           & SGD-SaI (ours) & 340.1 $\pm$ 1.1 & 267.6             \\ \hline
\end{tabular}
}
\caption{RTX 3090 Profile Results. The results here are based on a single NVIDIA GeForce RTX 3090 GPU. The trials were conducted over 20 iterations, recording the time taken for each optimization step, which is referred to as the "iteration time" column. We compared the time taken for the g-SNR calculation stage and found that it takes an equal amount of time or less than an optimization step. However, since this calculation is only performed once, it is considered tolerable. }
\label{table:rtx3090_warmup_result}
\end{table}



\begin{table}[!htbp]
\centering
\small % Reduce font size for CVPR two-column format
\setlength{\tabcolsep}{6pt} % Reduce column padding
\renewcommand{\arraystretch}{1.3} % Adjust row height
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l|l|l}
\hline
Model                      & Method           & Iter Times (ms) & g-SNR Calc (ms) \\ \hline
\multirow{2}{*}{ViT-S/16}  & SGDM             & 7.1 $\pm$ 0.0   & 0                 \\
                           & SGD-SaI (ours) & 13.0 $\pm$ 0.1  & 13.7              \\ \hline
\multirow{2}{*}{ViT-H/14}  & SGDM             & 51.9 $\pm$ 10.7 & 0                 \\
                           & SGD-SaI (ours) & 63.1 $\pm$ 7.8  & 106.0             \\ \hline
\multirow{2}{*}{GPT2-1.5B} & SGDM             & 353.2 $\pm$ 0.9 & 0                 \\
                           & SGD-SaI (ours) & 392.8 $\pm$ 0.2 & 353.8             \\ \hline
\end{tabular}
}
\caption{A100 PCIe 40GB Profile Results. It follows the same setting as Table~\ref{table:rtx3090_warmup_result}, expect for the GPU type.}
\label{table:a100_warmup_result}
\end{table}



\section{Algorithm Overview}

In this section, we will provide an overview of how SGD-SaI operates. As a non-adaptive gradient method, SGD-SaI calculates the preconditioned scaling factor based on the g-SNR values before applying the first batch of data in the optimization step. These scaling factors vary across different partitions, as they are closely linked to the architectures being utilized. However, once established, they remain constant throughout the entire training process.

While Adam-mini is memory-efficient, its complex partition rules and repetitive local gain recalculation result in significant computational costs. The improvement in throughput compared to Adam(W) is primarily due to the ability to reduce memory usage, allowing for larger batch sizes and enabling more data to be processed in parallel. In contrast, we not only reduce the memory footprint but also eliminate the entire adaptive local gain calculation, achieving a significant breakthrough in both memory and computational efficiency.
