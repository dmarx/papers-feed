- **SGD-SaI Overview**: A novel optimization method that enhances SGD by applying learning rate Scaling at Initialization (SaI) based on gradient signal-to-noise ratios (g-SNR).

- **g-SNR Definition**: Gradient signal-to-noise ratio (g-SNR) quantifies the relationship between the gradient's norm and its variance, providing a measure to adjust learning rates effectively.

- **Memory Efficiency**: SGD-SaI reduces memory usage by half compared to AdamW, achieving substantial savings (e.g., 5.93 GB for GPT-2 with 1.5B parameters).

- **Performance Comparison**: SGD-SaI consistently matches or outperforms AdamW across various tasks, including ImageNet-1K classification with Vision Transformers and GPT-2 pretraining.

- **Robustness to Hyperparameters**: Demonstrated robustness to hyperparameter variations, making it practical for diverse applications, including LoRA fine-tuning for LLMs and diffusion models.

- **Key Contributions**:
  - Challenges the necessity of adaptive gradient methods.
  - Proposes a constant g-SNR value to replace second-order momentum, reducing memory and computation costs.
  - Empirical analysis of g-SNR characteristics during training.

- **SGD Update Rule**: 
  \[
  \theta_{t+1} = \theta_t - \eta_t D_t
  \]
  where \(D_t = g_t\) for SGD.

- **SGDM Update Rule**: 
  \[
  m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
  \]
  \[
  D_t = m_t
  \]

- **Adam Update Rule**: 
  \[
  v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
  \]
  \[
  D_t = \alpha_t m_t, \quad \alpha_t = \frac{1}{\sqrt{v_t} + \epsilon}
  \]

- **Gradient Initialization Insights**: Early gradient patterns correlate strongly with model performance, influencing the effectiveness of training strategies.

- **Notational Summary**:
  - \( \theta \in \mathbb{R}^d \): Parameters of the neural network.
  - \( L(\theta) \): Loss function to minimize.
  - \( \eta > 0 \): Global learning rate.
  - \( \lambda \geq 0 \): Weight decay coefficient.
  - \( \mu \): Momentum coefficient.
  - \( g_t = \nabla L(\theta_t) \): Gradient at step \(t\).

- **Empirical Findings**: SGD-SaI shows consistent improvements over state-of-the-art optimizers in various tasks, addressing the long-standing challenge of using SGD for training transformer architectures.