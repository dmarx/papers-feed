- **SGD-SaI Overview**: A novel optimization method that enhances SGD by applying learning rate Scaling at Initialization (SaI) based on gradient signal-to-noise ratios (g-SNR) to distinct parameter groups.

- **g-SNR Definition**: Gradient signal-to-noise ratio (g-SNR) quantifies the relationship between a gradient's norm and its variance, providing a measure to adjust learning rates effectively.

- **Memory Efficiency**: SGD-SaI reduces memory usage by half compared to AdamW, achieving substantial savings (5.93 GB for GPT-2 and 25.15 GB for Llama2-7B) in full-precision training settings.

- **Performance Comparison**: SGD-SaI consistently matches or outperforms AdamW across various tasks, including ImageNet-1K classification with Vision Transformers and GPT-2 pretraining for large language models.

- **Robustness to Hyperparameters**: Demonstrated robustness to hyperparameter variations, making it practical for diverse applications, including LoRA fine-tuning for LLMs and diffusion models.

- **Training Dynamics**: Addresses training imbalances from the first iteration by adjusting learning rates based on g-SNR, facilitating stable training progress without adaptive gradient overhead.

- **Key Notations**:
  - Parameters: \( \theta \in \mathbb{R}^d \)
  - Loss function: \( L(\theta) \)
  - Global learning rate: \( \eta > 0 \)
  - Weight decay coefficient: \( \lambda \geq 0 \)
  - Momentum coefficient: \( \mu \)
  - Gradient: \( g^{(i)}_t = \nabla_{\theta^{(i)}} L(\theta) \)

- **SGD Update Rule**: 
  \[
  \theta_{t+1} = \theta_t - \eta_t D_t
  \]
  where \( D_t = g_t \) for standard SGD.

- **SGDM Update Rule**: 
  \[
  m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
  \]
  \[
  D_t = m_t
  \]

- **Adam Update Rule**: 
  \[
  v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
  \]
  \[
  D_t = \alpha_t m_t, \quad \alpha_t = \frac{1}{\sqrt{v_t} + \epsilon}
  \]

- **Empirical Analysis**: Investigated g-SNR distribution across ViT parameters, showing temporal consistency and correlation with architectural characteristics.

- **Conclusion**: SGD-SaI effectively overcomes the limitations of SGD in training Transformer architectures, providing a simpler, more efficient alternative to adaptive methods like Adam.