- **Inference Compute Scaling**: Repeated sampling as a method to scale inference compute, improving reasoning performance across tasks.
  
- **Key Metrics**:
  - **Coverage**: Fraction of problems solved by any generated sample; increases with the number of samples.
  - **Precision**: Ability to identify correct samples from generated candidates.

- **Performance Improvement**: 
  - DeepSeek-Coder-V2-Instruct: Coverage increases from 15.9% (1 sample) to 56% (250 samples) on SWE-bench Lite, surpassing the state-of-the-art (43%).

- **Scaling Laws**: Coverage vs. number of samples often follows a log-linear relationship, modeled by an exponentiated power law.

- **Task Categories**:
  - **GSM8K**: Grade-school math problems.
  - **MATH**: More complex math word problems.
  - **MiniF2F-MATH**: Formalized math problems using Lean4.
  - **CodeContests**: Competitive programming problems with hidden test cases.
  - **SWE-bench Lite**: Real-world GitHub issues requiring code modifications.

- **Sample Generation Process**:
  1. Generate multiple candidate solutions from an LLM.
  2. Use domain-specific verifiers (e.g., unit tests) to select the final answer.

- **Coverage Calculation**: 
  - Formula for pass@k:
  \[
  pass@k = 1 - \frac{N - C_i}{N} \quad \text{for } k \leq N
  \]
  where \(C_i\) is the number of correct samples.

- **Cost-Effectiveness**: 
  - Repeated sampling can minimize inference costs; e.g., sampling from a cheaper model can outperform single samples from more expensive models.

- **Verification Challenges**: 
  - In domains without automatic verifiers, common methods (majority voting, reward models) plateau, failing to scale with sample budget.

- **Empirical Findings**:
  - Coverage increases significantly with repeated sampling (e.g., Gemma-2B: 0.02% to 7.1% on CodeContests with 10,000 samples).
  - Smaller models can show sharper increases in coverage with repeated sampling.

- **Figures and Results**:
  - Figure 1: Overview of repeated sampling process.
  - Figure 2: Coverage improvement across tasks with increased samples.
  - Figure 3: Coverage trends across different model sizes and families.

- **Conclusion**: 
  - Repeated sampling is a viable strategy for enhancing LLM performance, particularly in tasks with automatic verification, while also being cost-effective.