@misc{singh2024human,
      title={Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models}, 
      author={Avi Singh and John D. Co-Reyes and Rishabh Agarwal and Ankesh Anand and Piyush Patil and Xavier Garcia and Peter J. Liu and James Harrison and Jaehoon Lee and Kelvin Xu and Aaron Parisi and Abhishek Kumar and Alex Alemi and Alex Rizkowsky and Azade Nova and Ben Adlam and Bernd Bohnet and Gamaleldin Elsayed and Hanie Sedghi and Igor Mordatch and Isabelle Simpson and Izzeddin Gur and Jasper Snoek and Jeffrey Pennington and Jiri Hron and Kathleen Kenealy and Kevin Swersky and Kshiteej Mahajan and Laura Culp and Lechao Xiao and Maxwell L. Bileschi and Noah Constant and Roman Novak and Rosanne Liu and Tris Warkentin and Yundi Qian and Yamini Bansal and Ethan Dyer and Behnam Neyshabur and Jascha Sohl-Dickstein and Noah Fiedel},
      year={2024},
      eprint={2312.06585},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{azerbayev2024llemmaopenlanguagemodel,
      title={Llemma: An Open Language Model For Mathematics}, 
      author={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
      year={2024},
      eprint={2310.10631},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.10631}, 
}

@misc{ning2024skeletonofthoughtpromptingllmsefficient,
      title={Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation}, 
      author={Xuefei Ning and Zinan Lin and Zixuan Zhou and Zifu Wang and Huazhong Yang and Yu Wang},
      year={2024},
      eprint={2307.15337},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.15337}, 
}

@misc{tian2024selfimprovementllmsimaginationsearching,
      title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing}, 
      author={Ye Tian and Baolin Peng and Linfeng Song and Lifeng Jin and Dian Yu and Haitao Mi and Dong Yu},
      year={2024},
      eprint={2404.12253},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.12253}, 
}

@article{Besta_2024,
   title={Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
   volume={38},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v38i16.29720},
   DOI={10.1609/aaai.v38i16.29720},
   number={16},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
   year={2024},
   month=mar, pages={17682–17690} }


@misc{yao2023treethoughtsdeliberateproblem,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10601}, 
}

@misc{shao2024scalingretrievalbasedlanguagemodels,
      title={Scaling Retrieval-Based Language Models with a Trillion-Token Datastore}, 
      author={Rulin Shao and Jacqueline He and Akari Asai and Weijia Shi and Tim Dettmers and Sewon Min and Luke Zettlemoyer and Pang Wei Koh},
      year={2024},
      eprint={2407.12854},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12854}, 
}


@misc{jones2021scalingscalinglawsboard,
      title={Scaling Scaling Laws with Board Games}, 
      author={Andy L. Jones},
      year={2021},
      eprint={2104.03113},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.03113}, 
}

@misc{hestness2017deeplearningscalingpredictable,
      title={Deep Learning Scaling is Predictable, Empirically}, 
      author={Joel Hestness and Sharan Narang and Newsha Ardalani and Gregory Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou},
      year={2017},
      eprint={1712.00409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.00409}, 
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@article{zheng2021minif2f,
  title={MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
  author={Zheng, Kunhao and Han, Jesse Michael and Polu, Stanislas},
  journal={arXiv preprint arXiv:2109.00110},
  year={2021}
}

@misc{lewkowycz2022solvingquantitativereasoningproblems,
      title={Solving Quantitative Reasoning Problems with Language Models}, 
      author={Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and Henryk Michalewski and Vinay Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
      year={2022},
      eprint={2206.14858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.14858}, 
}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@misc{jiang2024selfincorrect,
      title={SELF-[IN]CORRECT: LLMs Struggle with Refining Self-Generated Responses}, 
      author={Dongwei Jiang and Jingyu Zhang and Orion Weller and Nathaniel Weir and Benjamin Van Durme and Daniel Khashabi},
      year={2024},
      eprint={2404.04298},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{huang2024large,
      title={Large Language Models Cannot Self-Correct Reasoning Yet}, 
      author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
      year={2024},
      eprint={2310.01798},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{holtzman2020curious,
      title={The Curious Case of Neural Text Degeneration}, 
      author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
      year={2020},
      eprint={1904.09751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{Li_2022,
   title={Competition-level code generation with AlphaCode},
   volume={378},
   ISSN={1095-9203},
   url={http://dx.doi.org/10.1126/science.abq1158},
   DOI={10.1126/science.abq1158},
   number={6624},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and Hubert, Thomas and Choy, Peter and de Masson d’Autume, Cyprien and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Sutherland Robson, Esme and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
   year={2022},
   month=dec, pages={1092–1097} }

@misc{hendrycks2021measuringapps,
      title={Measuring Coding Challenge Competence With APPS}, 
      author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2105.09938},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lightman2023lets,
      title={Let's Verify Step by Step}, 
      author={Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
      year={2023},
      eprint={2305.20050},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yuan2024selfrewarding,
      title={Self-Rewarding Language Models}, 
      author={Weizhe Yuan and Richard Yuanzhe Pang and Kyunghyun Cho and Xian Li and Sainbayar Sukhbaatar and Jing Xu and Jason Weston},
      year={2024},
      eprint={2401.10020},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023selfconsistency,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{silver2017mastering,
      title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
      author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
      year={2017},
      eprint={1712.01815},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@Article{Trinh2024,
author={Trinh, Trieu H.
and Wu, Yuhuai
and Le, Quoc V.
and He, He
and Luong, Thang},
title={Solving olympiad geometry without human demonstrations},
journal={Nature},
year={2024},
month={Jan},
day={01},
volume={625},
number={7995},
pages={476-482},
abstract={Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1--4, owing to their reputed difficulty among the world's best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.},
issn={1476-4687},
doi={10.1038/s41586-023-06747-5},
url={https://doi.org/10.1038/s41586-023-06747-5}
}



@misc{tajwar2024preference,
      title={Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data}, 
      author={Fahim Tajwar and Anikait Singh and Archit Sharma and Rafael Rafailov and Jeff Schneider and Tengyang Xie and Stefano Ermon and Chelsea Finn and Aviral Kumar},
      year={2024},
      eprint={2404.14367},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yuan2023scaling,
      title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models}, 
      author={Zheng Yuan and Hongyi Yuan and Chengpeng Li and Guanting Dong and Keming Lu and Chuanqi Tan and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.01825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{tian2024selfimprovement,
      title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing}, 
      author={Ye Tian and Baolin Peng and Linfeng Song and Lifeng Jin and Dian Yu and Haitao Mi and Dong Yu},
      year={2024},
      eprint={2404.12253},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zelikman2022star,
      title={STaR: Bootstrapping Reasoning With Reasoning}, 
      author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman},
      year={2022},
      eprint={2203.14465},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hendrycks2021measuringmath,
      title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2103.03874},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhao2023pytorch,
      title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, 
      author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
      year={2023},
      eprint={2304.11277},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@online{gpt4o,
title={Hello GPT-4o},
year={2024},
url={https://openai.com/index/hello-gpt-4o/},
urldata={2024-22-05},
}

@online{sonnet35,
    title={Claude 3.5 Sonnet},
    year={2024},
    url={https://www.anthropic.com/news/claude-3-5-sonnet},

}

@online{metallama,
  title = {Meta Llama 3},
  year = {2024},
  url = {https://llama.meta.com/llama3/},
}


@misc{yuan2023scaling,
      title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models}, 
      author={Zheng Yuan and Hongyi Yuan and Chengpeng Li and Guanting Dong and Keming Lu and Chuanqi Tan and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.01825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hwang2024selfexplore,
      title={Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards}, 
      author={Hyeonbin Hwang and Doyoung Kim and Seungone Kim and Seonghyeon Ye and Minjoon Seo},
      year={2024},
      eprint={2404.10346},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2024alphamath,
      title={AlphaMath Almost Zero: process Supervision without process}, 
      author={Guoxin Chen and Minpeng Liao and Chengxi Li and Kai Fan},
      year={2024},
      eprint={2405.03553},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hosseini2024vstar,
      title={V-STaR: Training Verifiers for Self-Taught Reasoners}, 
      author={Arian Hosseini and Xingdi Yuan and Nikolay Malkin and Aaron Courville and Alessandro Sordoni and Rishabh Agarwal},
      year={2024},
      eprint={2402.06457},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{juravsky2024hydragen,
  title={Hydragen: High-Throughput LLM Inference with Shared Prefixes},
  author={Juravsky, Jordan and Brown, Bradley and Ehrlich, Ryan and Fu, Daniel Y and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2402.05099},
  year={2024}
}

@misc{bai2022constitutional,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rafailov2023direct,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{pluribus,
author = {Brown, Noam and Bakhtin, Anton and Lerer, Adam and Gong, Qucheng},
title = {Combining deep reinforcement learning and search for imperfect-information games},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1431},
numpages = {13},
location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {NIPS '20}
}

@article{deepblue,
author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
title = {Deep Blue},
year = {2002},
issue_date = {January 2002},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {134},
number = {1–2},
issn = {0004-3702},
url = {https://doi.org/10.1016/S0004-3702(01)00129-1},
doi = {10.1016/S0004-3702(01)00129-1},
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: a single-chip chess search engine, a massively parallel system with multiple levels of parallelism, a strong emphasis on search extensions, a complex evaluation function, and effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decision behind Deep Blue. Copyright 2001 Elsevier B.V.},
journal = {Artif. Intell.},
month = {jan},
pages = {57–83},
numpages = {27},
keywords = {selective search, search extensions, parallel search, game tree search, evaluation function, computer chess}
}

@misc{jimenez2024swebenchlanguagemodelsresolve,
      title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?}, 
      author={Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},
      year={2024},
      eprint={2310.06770},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06770}, 
}

@misc{deepseekai2024deepseekv2strongeconomicalefficient,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI and others},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.04434}, 
}

@misc{moatless,
    author={Albert Örwall},
    title={Moatless Tools},
    publisher={Github},
    howpublished = {\url{https://github.com/aorwall/moatless-tools/tree/a1017b78e3e69e7d205b1a3faa83a7d19fce3fa6}},
    year={2024},
}

@misc{arc_gpt4o,
    author={Ryan Greenblatt},
    title={Geting 50% (SoTA) on ARC-AGI with GPT-4o},
    howpublished = {\url{https://www.lesswrong.com/posts/Rdwui3wHxCeKb7feK/getting-50-sota-on-arc-agi-with-gpt-4o}},
    year={2024}

}

@misc{gemmateam2024gemmaopenmodelsbased,
      title={Gemma: Open Models Based on Gemini Research and Technology}, 
      author={Gemma Team and others},
      year={2024},
      eprint={2403.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08295}, 
}


@misc{zhang2024probingmultiturnplanningcapabilities,
      title={Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games}, 
      author={Yizhe Zhang and Jiarui Lu and Navdeep Jaitly},
      year={2024},
      eprint={2310.01468},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.01468}, 
}

@misc{zelikman2024quietstarlanguagemodelsteach,
      title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking}, 
      author={Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman},
      year={2024},
      eprint={2403.09629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09629}, 
}

@misc{wang2024mixtureofagentsenhanceslargelanguage,
      title={Mixture-of-Agents Enhances Large Language Model Capabilities}, 
      author={Junlin Wang and Jue Wang and Ben Athiwaratkun and Ce Zhang and James Zou},
      year={2024},
      eprint={2406.04692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.04692}, 
}

@misc{song2024goodbadgreedyevaluation,
      title={The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism}, 
      author={Yifan Song and Guoyin Wang and Sujian Li and Bill Yuchen Lin},
      year={2024},
      eprint={2407.10457},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10457}, 
}

@misc{biderman2023pythiasuiteanalyzinglarge,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01373}, 
}


@misc{ArmoR,
      title={Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts}, 
      author={Haoxiang Wang and Wei Xiong and Tengyang Xie and Han Zhao and Tong Zhang},
      year={2024},
      eprint={2406.12845},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.12845}, 
}

@misc{lambert2024rewardbenchevaluatingrewardmodels,
      title={RewardBench: Evaluating Reward Models for Language Modeling}, 
      author={Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2403.13787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.13787}, 
}

@misc{chen2021evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}

@misc{chollet2019measureintelligence,
      title={On the Measure of Intelligence}, 
      author={François Chollet},
      year={2019},
      eprint={1911.01547},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1911.01547}, 
}

@misc{geminiteam2024geminifamilyhighlycapable,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and others},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{voyage,
  title = {Voyage AI},
  url = {https://www.voyageai.com/},
  year = {2024},
}

@misc{aide,
    title = {Aide.dev},
    url = {https://aide.dev/},
    year = {2024}
}


@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}


@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}


@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}


@misc{dehghani2022efficiencymisnomer,
      title={The Efficiency Misnomer}, 
      author={Mostafa Dehghani and Anurag Arnab and Lucas Beyer and Ashish Vaswani and Yi Tay},
      year={2022},
      eprint={2110.12894},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.12894}, 
}


@misc{christiano2017deepreinforcementlearninghuman,
      title={Deep reinforcement learning from human preferences}, 
      author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
      year={2017},
      eprint={1706.03741},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1706.03741}, 
}


@misc{zheng2024sglangefficientexecutionstructured,
      title={SGLang: Efficient Execution of Structured Language Model Programs}, 
      author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      year={2024},
      eprint={2312.07104},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.07104}, 
}


@misc{athiwaratkun2024bifurcatedattentionacceleratingmassively,
      title={Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs}, 
      author={Ben Athiwaratkun and Sujan Kumar Gonugondla and Sanjay Krishna Gouda and Haifeng Qian and Hantian Ding and Qing Sun and Jun Wang and Jiacheng Guo and Liangfu Chen and Parminder Bhatia and Ramesh Nallapati and Sudipta Sengupta and Bing Xiang},
      year={2024},
      eprint={2403.08845},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.08845}, 
}


@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and others},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{kulal2019spocsearchbasedpseudocodecode,
      title={SPoC: Search-based Pseudocode to Code}, 
      author={Sumith Kulal and Panupong Pasupat and Kartik Chandra and Mina Lee and Oded Padon and Alex Aiken and Percy Liang},
      year={2019},
      eprint={1906.04908},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.04908}, 
}

@misc{graves2012sequencetransductionrecurrentneural,
      title={Sequence Transduction with Recurrent Neural Networks}, 
      author={Alex Graves},
      year={2012},
      eprint={1211.3711},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1211.3711}, 
}

@misc{boulangerlewandowski2012highdimensionalsequencetransduction,
      title={High-dimensional sequence transduction}, 
      author={Nicolas Boulanger-Lewandowski and Yoshua Bengio and Pascal Vincent},
      year={2012},
      eprint={1212.1936},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1212.1936}, 
}


@misc{irvine2023rewardingchatbotsrealworldengagement,
      title={Rewarding Chatbots for Real-World Engagement with Millions of Users}, 
      author={Robert Irvine and Douglas Boubert and Vyas Raina and Adian Liusie and Ziyi Zhu and Vineet Mudupalli and Aliaksei Korshuk and Zongyi Liu and Fritz Cremer and Valentin Assassi and Christie-Carol Beauchamp and Xiaoding Lu and Thomas Rialan and William Beauchamp},
      year={2023},
      eprint={2303.06135},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.06135}, 
}

@misc{yao2022reactsynergizingreasoningacting,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2022},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03629}, 
}

@misc{madaan2023selfrefineiterativerefinementselffeedback,
      title={Self-Refine: Iterative Refinement with Self-Feedback}, 
      author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
      year={2023},
      eprint={2303.17651},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.17651}, 
}

@misc{bai2022constitutionalaiharmlessnessai,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.08073}, 
}

@article{Trinh2024alphageometry,
  author = {Trinh, Trieu H. and Wu, Yuhuai and Le, Quoc V. and He, He and Luong, Thang},
  year = {2024},
  title = {Solving olympiad geometry without human demonstrations},
  journal = {Nature},
  volume = {625},
  number = {7995},
  pages = {476--482},
  doi = {10.1038/s41586-023-06747-5},
  issn = {1476-4687},
  url = {https://doi.org/10.1038/s41586-023-06747-5}
}

@misc{chen2024llmcallsneedscaling,
      title={Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems}, 
      author={Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis and Ion Stoica and Matei Zaharia and James Zou},
      year={2024},
      eprint={2403.02419},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.02419}, 
}

@misc{ong2024routellmlearningroutellms,
      title={RouteLLM: Learning to Route LLMs with Preference Data}, 
      author={Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
      year={2024},
      eprint={2406.18665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.18665}, 
}

@misc{wan2024knowledgefusionlargelanguage,
      title={Knowledge Fusion of Large Language Models}, 
      author={Fanqi Wan and Xinting Huang and Deng Cai and Xiaojun Quan and Wei Bi and Shuming Shi},
      year={2024},
      eprint={2401.10491},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.10491}, 
}

@misc{jiang2023llmblenderensemblinglargelanguage,
      title={LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion}, 
      author={Dongfu Jiang and Xiang Ren and Bill Yuchen Lin},
      year={2023},
      eprint={2306.02561},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.02561}, 
}

@misc{rozière2023codellamaopenfoundation,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2023},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.12950}, 
}

@misc{nguyen2024consistentpredictionlikelycorrect,
      title={When is the consistent prediction likely to be a correct prediction?}, 
      author={Alex Nguyen and Dheeraj Mekala and Chengyu Dong and Jingbo Shang},
      year={2024},
      eprint={2407.05778},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.05778}, 
}

@misc{hassid2024largerbetterimprovedllm,
      title={The Larger the Better? Improved LLM Code-Generation via Budget Reallocation}, 
      author={Michael Hassid and Tal Remez and Jonas Gehring and Roy Schwartz and Yossi Adi},
      year={2024},
      eprint={2404.00725},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2404.00725}, 
}

@misc{davis2024networksnetworkscomplexityclass,
      title={Networks of Networks: Complexity Class Principles Applied to Compound AI Systems Design}, 
      author={Jared Quincy Davis and Boris Hanin and Lingjiao Chen and Peter Bailis and Ion Stoica and Matei Zaharia},
      year={2024},
      eprint={2407.16831},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.16831}, 
}

@misc{kang2024mindstarenhancingmathreasoning,
      title={MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time}, 
      author={Jikun Kang and Xin Zhe Li and Xi Chen and Amirreza Kazemi and Qianyi Sun and Boxing Chen and Dong Li and Xu He and Quan He and Feng Wen and Jianye Hao and Jun Yao},
      year={2024},
      eprint={2405.16265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16265}, 
}

@misc{wang2024mathshepherdverifyreinforcellms,
      title={Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations}, 
      author={Peiyi Wang and Lei Li and Zhihong Shao and R. X. Xu and Damai Dai and Yifei Li and Deli Chen and Y. Wu and Zhifang Sui},
      year={2024},
      eprint={2312.08935},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.08935}, 
}