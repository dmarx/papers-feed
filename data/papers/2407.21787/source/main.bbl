\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[aid(2024)]{aide}
Aide.dev, 2024.
\newblock URL \url{https://aide.dev/}.

\bibitem[gpt(2024)]{gpt4o}
Hello gpt-4o, 2024.
\newblock URL \url{https://openai.com/index/hello-gpt-4o/}.

\bibitem[met(2024)]{metallama}
Meta llama 3, 2024.
\newblock URL \url{https://llama.meta.com/llama3/}.

\bibitem[son(2024)]{sonnet35}
Claude 3.5 sonnet, 2024.
\newblock URL \url{https://www.anthropic.com/news/claude-3-5-sonnet}.

\bibitem[voy(2024)]{voyage}
Voyage ai, 2024.
\newblock URL \url{https://www.voyageai.com/}.

\bibitem[Athiwaratkun et~al.(2024)Athiwaratkun, Gonugondla, Gouda, Qian, Ding, Sun, Wang, Guo, Chen, Bhatia, Nallapati, Sengupta, and Xiang]{athiwaratkun2024bifurcatedattentionacceleratingmassively}
Ben Athiwaratkun, Sujan~Kumar Gonugondla, Sanjay~Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Jiacheng Guo, Liangfu Chen, Parminder Bhatia, Ramesh Nallapati, Sudipta Sengupta, and Bing Xiang.
\newblock Bifurcated attention: Accelerating massively parallel decoding with shared prefixes in llms, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.08845}.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, and Kaplan]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.
\newblock Constitutional ai: Harmlessness from ai feedback, 2022.

\bibitem[Besta et~al.(2024)Besta, Blach, Kubicek, Gerstenberger, Podstawski, Gianinazzi, Gajda, Lehmann, Niewiadomski, Nyczyk, and Hoefler]{Besta_2024}
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler.
\newblock Graph of thoughts: Solving elaborate problems with large language models.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 38\penalty0 (16):\penalty0 17682–17690, March 2024.
\newblock ISSN 2159-5399.
\newblock \doi{10.1609/aaai.v38i16.29720}.
\newblock URL \url{http://dx.doi.org/10.1609/aaai.v38i16.29720}.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal]{biderman2023pythiasuiteanalyzinglarge}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der Wal.
\newblock Pythia: A suite for analyzing large language models across training and scaling, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.01373}.

\bibitem[Brown et~al.(2020{\natexlab{a}})Brown, Bakhtin, Lerer, and Gong]{pluribus}
Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong.
\newblock Combining deep reinforcement learning and search for imperfect-information games.
\newblock In \emph{Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS '20, Red Hook, NY, USA, 2020{\natexlab{a}}. Curran Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Brown et~al.(2020{\natexlab{b}})Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020languagemodelsfewshotlearners}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Campbell et~al.(2002)Campbell, Hoane, and Hsu]{deepblue}
Murray Campbell, A.~Joseph Hoane, and Feng-hsiung Hsu.
\newblock Deep blue.
\newblock \emph{Artif. Intell.}, 134\penalty0 (1–2):\penalty0 57–83, jan 2002.
\newblock ISSN 0004-3702.
\newblock \doi{10.1016/S0004-3702(01)00129-1}.
\newblock URL \url{https://doi.org/10.1016/S0004-3702(01)00129-1}.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Liao, Li, and Fan]{chen2024alphamath}
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan.
\newblock Alphamath almost zero: process supervision without process, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Davis, Hanin, Bailis, Stoica, Zaharia, and Zou]{chen2024llmcallsneedscaling}
Lingjiao Chen, Jared~Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou.
\newblock Are more llm calls all you need? towards scaling laws of compound inference systems, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2403.02419}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluatinglargelanguagemodels}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Chollet(2019)]{chollet2019measureintelligence}
François Chollet.
\newblock On the measure of intelligence, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.01547}.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deepreinforcementlearninghuman}
Paul Christiano, Jan Leike, Tom~B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.
\newblock Deep reinforcement learning from human preferences, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.03741}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems, 2021.

\bibitem[Davis et~al.(2024)Davis, Hanin, Chen, Bailis, Stoica, and Zaharia]{davis2024networksnetworkscomplexityclass}
Jared~Quincy Davis, Boris Hanin, Lingjiao Chen, Peter Bailis, Ion Stoica, and Matei Zaharia.
\newblock Networks of networks: Complexity class principles applied to compound ai systems design, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.16831}.

\bibitem[DeepSeek-AI et~al.(2024)]{deepseekai2024deepseekv2strongeconomicalefficient}
DeepSeek-AI et~al.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.04434}.

\bibitem[Dehghani et~al.(2022)Dehghani, Arnab, Beyer, Vaswani, and Tay]{dehghani2022efficiencymisnomer}
Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi~Tay.
\newblock The efficiency misnomer, 2022.
\newblock URL \url{https://arxiv.org/abs/2110.12894}.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Greenblatt(2024)]{arc_gpt4o}
Ryan Greenblatt.
\newblock Geting 50% (sota) on arc-agi with gpt-4o.
\newblock \url{https://www.lesswrong.com/posts/Rdwui3wHxCeKb7feK/getting-50-sota-on-arc-agi-with-gpt-4o}, 2024.

\bibitem[Hassid et~al.(2024)Hassid, Remez, Gehring, Schwartz, and Adi]{hassid2024largerbetterimprovedllm}
Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, and Yossi Adi.
\newblock The larger the better? improved llm code-generation via budget reallocation, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.00725}.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, and Steinhardt]{hendrycks2021measuringapps}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt.
\newblock Measuring coding challenge competence with apps, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycks2021measuringmath}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset, 2021{\natexlab{b}}.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou]{hestness2017deeplearningscalingpredictable}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically, 2017.
\newblock URL \url{https://arxiv.org/abs/1712.00409}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre]{hoffmann2022trainingcomputeoptimallargelanguage}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.15556}.

\bibitem[Hosseini et~al.(2024)Hosseini, Yuan, Malkin, Courville, Sordoni, and Agarwal]{hosseini2024vstar}
Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal.
\newblock V-star: Training verifiers for self-taught reasoners, 2024.

\bibitem[Irvine et~al.(2023)Irvine, Boubert, Raina, Liusie, Zhu, Mudupalli, Korshuk, Liu, Cremer, Assassi, Beauchamp, Lu, Rialan, and Beauchamp]{irvine2023rewardingchatbotsrealworldengagement}
Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, and William Beauchamp.
\newblock Rewarding chatbots for real-world engagement with millions of users, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.06135}.

\bibitem[Jiang et~al.(2023)Jiang, Ren, and Lin]{jiang2023llmblenderensemblinglargelanguage}
Dongfu Jiang, Xiang Ren, and Bill~Yuchen Lin.
\newblock Llm-blender: Ensembling large language models with pairwise ranking and generative fusion, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.02561}.

\bibitem[Jimenez et~al.(2024)Jimenez, Yang, Wettig, Yao, Pei, Press, and Narasimhan]{jimenez2024swebenchlanguagemodelsresolve}
Carlos~E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan.
\newblock Swe-bench: Can language models resolve real-world github issues?, 2024.
\newblock URL \url{https://arxiv.org/abs/2310.06770}.

\bibitem[Jones(2021)]{jones2021scalingscalinglawsboard}
Andy~L. Jones.
\newblock Scaling scaling laws with board games, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.03113}.

\bibitem[Juravsky et~al.(2024)Juravsky, Brown, Ehrlich, Fu, R{\'e}, and Mirhoseini]{juravsky2024hydragen}
Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel~Y Fu, Christopher R{\'e}, and Azalia Mirhoseini.
\newblock Hydragen: High-throughput llm inference with shared prefixes.
\newblock \emph{arXiv preprint arXiv:2402.05099}, 2024.

\bibitem[Kang et~al.(2024)Kang, Li, Chen, Kazemi, Sun, Chen, Li, He, He, Wen, Hao, and Yao]{kang2024mindstarenhancingmathreasoning}
Jikun Kang, Xin~Zhe Li, Xi~Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu~He, Quan He, Feng Wen, Jianye Hao, and Jun Yao.
\newblock Mindstar: Enhancing math reasoning in pre-trained llms at inference time, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.16265}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models, 2020.

\bibitem[Kulal et~al.(2019)Kulal, Pasupat, Chandra, Lee, Padon, Aiken, and Liang]{kulal2019spocsearchbasedpseudocodecode}
Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy Liang.
\newblock Spoc: Search-based pseudocode to code, 2019.
\newblock URL \url{https://arxiv.org/abs/1906.04908}.

\bibitem[Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu, Dziri, Kumar, Zick, Choi, Smith, and Hajishirzi]{lambert2024rewardbenchevaluatingrewardmodels}
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ~Miranda, Bill~Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah~A. Smith, and Hannaneh Hajishirzi.
\newblock Rewardbench: Evaluating reward models for language modeling, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.13787}.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, Wu, Neyshabur, Gur-Ari, and Misra]{lewkowycz2022solvingquantitativereasoningproblems}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.
\newblock Solving quantitative reasoning problems with language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2206.14858}.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, Hubert, Choy, de~Masson~d’Autume, Babuschkin, Chen, Huang, Welbl, Gowal, Cherepanov, Molloy, Mankowitz, Sutherland~Robson, Kohli, de~Freitas, Kavukcuoglu, and Vinyals]{Li_2022}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago, Thomas Hubert, Peter Choy, Cyprien de~Masson~d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel~J. Mankowitz, Esme Sutherland~Robson, Pushmeet Kohli, Nando de~Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092–1097, December 2022.
\newblock ISSN 1095-9203.
\newblock \doi{10.1126/science.abq1158}.
\newblock URL \url{http://dx.doi.org/10.1126/science.abq1158}.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023lets}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step, 2023.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, Gupta, Majumder, Hermann, Welleck, Yazdanbakhsh, and Clark]{madaan2023selfrefineiterativerefinementselffeedback}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa~Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.
\newblock Self-refine: Iterative refinement with self-feedback, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.17651}.

\bibitem[Nguyen et~al.(2024)Nguyen, Mekala, Dong, and Shang]{nguyen2024consistentpredictionlikelycorrect}
Alex Nguyen, Dheeraj Mekala, Chengyu Dong, and Jingbo Shang.
\newblock When is the consistent prediction likely to be a correct prediction?, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.05778}.

\bibitem[Ong et~al.(2024)Ong, Almahairi, Wu, Chiang, Wu, Gonzalez, Kadous, and Stoica]{ong2024routellmlearningroutellms}
Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph~E. Gonzalez, M~Waleed Kadous, and Ion Stoica.
\newblock Routellm: Learning to route llms with preference data, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.18665}.

\bibitem[OpenAI et~al.(2024)]{openai2024gpt4technicalreport}
OpenAI et~al.
\newblock Gpt-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rozière et~al.(2023)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Sauvestre, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier, Scialom, and Synnaeve]{rozière2023codellamaopenfoundation}
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian~Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.
\newblock Code llama: Open foundation models for code, 2023.
\newblock URL \url{https://arxiv.org/abs/2308.12950}.

\bibitem[Shao et~al.(2024)Shao, He, Asai, Shi, Dettmers, Min, Zettlemoyer, and Koh]{shao2024scalingretrievalbasedlanguagemodels}
Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang~Wei Koh.
\newblock Scaling retrieval-based language models with a trillion-token datastore, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.12854}.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and Hassabis]{silver2017mastering}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis.
\newblock Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017.

\bibitem[Song et~al.(2024)Song, Wang, Li, and Lin]{song2024goodbadgreedyevaluation}
Yifan Song, Guoyin Wang, Sujian Li, and Bill~Yuchen Lin.
\newblock The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.10457}.

\bibitem[Team et~al.(2024{\natexlab{a}})]{geminiteam2024geminifamilyhighlycapable}
Gemini Team et~al.
\newblock Gemini: A family of highly capable multimodal models, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2312.11805}.

\bibitem[Team et~al.(2024{\natexlab{b}})]{gemmateam2024gemmaopenmodelsbased}
Gemma Team et~al.
\newblock Gemma: Open models based on gemini research and technology, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2403.08295}.

\bibitem[Tian et~al.(2024)Tian, Peng, Song, Jin, Yu, Mi, and Yu]{tian2024selfimprovementllmsimaginationsearching}
Ye~Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu.
\newblock Toward self-improvement of llms via imagination, searching, and criticizing, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.12253}.

\bibitem[Trinh et~al.(2024)Trinh, Wu, Le, He, and Luong]{Trinh2024alphageometry}
Trieu~H. Trinh, Yuhuai Wu, Quoc~V. Le, He~He, and Thang Luong.
\newblock Solving olympiad geometry without human demonstrations.
\newblock \emph{Nature}, 625\penalty0 (7995):\penalty0 476--482, 2024.
\newblock ISSN 1476-4687.
\newblock \doi{10.1038/s41586-023-06747-5}.
\newblock URL \url{https://doi.org/10.1038/s41586-023-06747-5}.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy, Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett, Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng, Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris, Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0 Contributors}]{2020SciPy-NMeth}
Pauli Virtanen, Ralf Gommers, Travis~E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St{\'e}fan~J. {van der Walt}, Matthew Brett, Joshua Wilson, K.~Jarrod Millman, Nikolay Mayorov, Andrew R.~J. Nelson, Eric Jones, Robert Kern, Eric Larson, C~J Carey, {\.I}lhan Polat, Yu~Feng, Eric~W. Moore, Jake {VanderPlas}, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E.~A. Quintero, Charles~R. Harris, Anne~M. Archibald, Ant{\^o}nio~H. Ribeiro, Fabian Pedregosa, Paul {van Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.
\newblock \doi{10.1038/s41592-019-0686-2}.

\bibitem[Wan et~al.(2024)Wan, Huang, Cai, Quan, Bi, and Shi]{wan2024knowledgefusionlargelanguage}
Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi.
\newblock Knowledge fusion of large language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.10491}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Xiong, Xie, Zhao, and Zhang]{ArmoR}
Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang.
\newblock Interpretable preferences via multi-objective reward modeling and mixture-of-experts, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2406.12845}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Wang, Athiwaratkun, Zhang, and Zou]{wang2024mixtureofagentsenhanceslargelanguage}
Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce~Zhang, and James Zou.
\newblock Mixture-of-agents enhances large language model capabilities, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2406.04692}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Li, Shao, Xu, Dai, Li, Chen, Wu, and Sui]{wang2024mathshepherdverifyreinforcellms}
Peiyi Wang, Lei Li, Zhihong Shao, R.~X. Xu, Damai Dai, Yifei Li, Deli Chen, Y.~Wu, and Zhifang Sui.
\newblock Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2312.08935}.

\bibitem[Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models, 2023.

\bibitem[Wei et~al.(2023)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou]{wei2023chainofthought}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models, 2023.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022reactsynergizingreasoningacting}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.03629}.

\bibitem[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023treethoughtsdeliberateproblem}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L. Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.10601}.

\bibitem[Zelikman et~al.(2024)Zelikman, Harik, Shao, Jayasiri, Haber, and Goodman]{zelikman2024quietstarlanguagemodelsteach}
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah~D. Goodman.
\newblock Quiet-star: Language models can teach themselves to think before speaking, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.09629}.

\bibitem[Zheng et~al.(2021)Zheng, Han, and Polu]{zheng2021minif2f}
Kunhao Zheng, Jesse~Michael Han, and Stanislas Polu.
\newblock Minif2f: a cross-system benchmark for formal olympiad-level mathematics.
\newblock \emph{arXiv preprint arXiv:2109.00110}, 2021.

\bibitem[Zheng et~al.(2024)Zheng, Yin, Xie, Sun, Huang, Yu, Cao, Kozyrakis, Stoica, Gonzalez, Barrett, and Sheng]{zheng2024sglangefficientexecutionstructured}
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody~Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph~E. Gonzalez, Clark Barrett, and Ying Sheng.
\newblock Sglang: Efficient execution of structured language model programs, 2024.
\newblock URL \url{https://arxiv.org/abs/2312.07104}.

\bibitem[Örwall(2024)]{moatless}
Albert Örwall.
\newblock Moatless tools.
\newblock \url{https://github.com/aorwall/moatless-tools/tree/a1017b78e3e69e7d205b1a3faa83a7d19fce3fa6}, 2024.

\end{thebibliography}
