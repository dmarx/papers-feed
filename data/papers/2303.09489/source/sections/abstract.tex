%
\begin{abstract}
Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. 
%
State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient \emph{sequence modeling}. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express  autoregressive time series processes.
% , and note they require equally long inputs to forecast long horizons.
% , and note they rely on equally long input sequences to forecast long horizons. 
%
We thus introduce \textsc{SpaceTime}, a new state-space time series architecture that improves all three criteria. 
For expressivity, we propose a new SSM parameterization based on the \textit{companion matrix}---a canonical representation for discrete-time processes---which enables \ourmethod{}'s SSM layers to learn desirable autoregressive processes.
% and also perform popular time series data preprocessing steps via simple weight initializations.   
For long horizon forecasting, we introduce a ``closed-loop'' variation of the companion SSM, which enables \ourmethod{} to predict many future time-steps by generating its own layer-wise inputs.
% that allows \textsc{SpaceTime} to predict long future 
% giving \textsc{SpaceTime} the ability to continue forecasting long horizons in the absence of an input. 
For efficient training and inference, we introduce an algorithm  
% is able to efficeintly learn over long sequencing by introducing an inference algorithm, 
that reduces the memory and compute of a forward pass with the companion matrix. With sequence length $\ell$ and state-space size $d$, we go from $\tilde{O}(d \ell)$ na\"ively to $\tilde{O}(d + \ell)$. 
%
In experiments, 
% we find
% empirically validate that \textsc{SpaceTime}’s 
% expressiveness, forecasting, and efficiency 
% contributions lead to improved time series modeling. 
%
our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. 
%
Furthermore, we find \textsc{SpaceTime} (1) fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73\% and 80\% relative wall-clock time over Transformers and LSTMs.
% when training on time series with 2,000 time-steps.

\end{abstract}
%


% Time series modeling is a well-established and challenging problem, which often requires methods that (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. 
% State-space models (SSMs) are classic models for time series, and prior works combine SSMs with deep learning layers for efficient \emph{sequence modeling}. However, we find fundamental limitations with these prior approaches for time series: we prov their SSM representations cannot express simple autoregressive time series processes, and note they require equally long inputs to forecast long horizons.
% We thus introduce \textsc{SpaceTime}, a new state-space time series deep learning architecture. 
% To improve expressivity, we propose a new SSM parameterization based on the \textit{companion matrix}---a canonical representation for discrete-time processes---which now enables \ourmethod{}'s SSM layers to learn desirable autoregressive processes.
% To forecast long horizons, we introduce a ``closed-loop'' variation of the companion SSM, which enables SSM-based models to predict many future time-steps without equally long input sequences.
% To efficiently train and perform inference, we finally derive a new algorithm  
% that reduces the time and space complexity of a forward pass with the companion matrix. With sequence length $L$ and state-space size $D$, we go from $\tilde{O}(DL)$ na\"ively to $\tilde{O}(D + L)$. 
% In experiments, we empirically validate that \textsc{SpaceTime}’s 
% contributions lead to improved time series modeling. 
% \textsc{SpaceTime} obtains state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 12 / 16 Informer forecasting tasks. 
% Furthermore, we find \textsc{SpaceTime} models (1) fit AR($p$) processes that prior deep SSMs fail on, (2) forecast notably more accurately on longer horizons than prior state-of-the-art, and (3) speed up training on 2000-time-step sequences by 66.7\% and 85.7\% relative wall-clock time over LSTMs and Transformers.