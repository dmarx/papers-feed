% Discuss preliminaries for time series modeling? 
% \section{Method: State-Space Time-Series Modeling with \ourmethod{}}

\section{Method: \ourmethod{}}
\label{sec:method_spacetime}
%

We now present \ourmethod{}, a deep architecture that uses structured state-spaces for more effective time-series modeling.
%
\ourmethod{} is a standard multi-layer encoder-decoder sequence model, built as a stack of repeated layers that each parametrize multiple SSMs. We designate the last layer as the ``decoder'', and prior layers as ``encoder'' layers. 
Each encoder layer processes an input time series sample as a sequence-to-sequence map. The decoder layer then takes the encoded sequence representation as input and outputs a prediction (for classification) or sequence (for forecasting). 

Below we expand on our contributions that allow \ourmethod{} to improve expressivity, long-horizon forecasting, and efficiency of time series modeling.
%
In Sec.~\ref{sec:expressive_ssm_layer}, we present our key building block, a layer that parametrizes the \emph{companion matrix} SSM (companion SSM) for expressive autoregressive modeling. 
% We first discuss how the companion SSM provides a highly expressive model for time series, proving that it can represent multiple fundamental processes. We then go over how we represent and compute multiple SSMs in each \ourmethodunit{}. We finally show how the companion SSM's expressiveness allows a \ourmethodunit{} to perform various standard time series data preprocessing operations simply as a forward pass via specific weight initializations.
%
In Sec.~\ref{sec:forecasting_ssm}, we introduce a specific instantiation of the companion SSM to flexibly forecast long horizons. 
% We show how (i) computing an SSM's outputs as a recurrence and (ii) explicitly training an SSM to predict future time-steps of its \emph{layer-specific} input sequences enables \ourmethod{}'s decoder to dynamically forecast arbitrary horizon lengths in a multi-layer network.
% time complexity
%
In Sec.~\ref{sec:efficient_algorithm}, we provide an efficient inference algorithm that allows \ourmethod{} to train and predict over long sequences in sub-quadratic time and space complexity. 
%
% Together these contributions make \ourmethod{} a highly modular architecture, where simply via specific weight initializations, we can efficiently compute multiple autoregressive outputs and perform built-in data preprocessing in a single forward pass.

\begin{figure}[!t]
  \centering
%   \includegraphics[width=1\textwidth]{_ICLR2023_paper/figures/figure_system_v1.pdf}
    \includegraphics[width=1\textwidth]{_ICLR2023_paper/figures/space_time_architecture_v2_flipped.pdf}
%   \caption{\small \textbf{\ourmethod{} architecture overview}. The left figure displays the \ourmethod{} architecture, which is a stack on SSM layers. The middle figure expands on an SSM layer, which consists of $h$ SSMs, each of which convolves the previous layer's output sequence with the SSM kernel $\cK_L(\zA,B,C)$. On the right, we expand on the SSM kernel which is a krylov function, where the state matrix $A$ is the expressive companion matrix. \KS{update caption}}
  \caption{ \textbf{\ourmethod{} architecture and components}. \textbf{(Left):} Each \ourmethodunit{} carries weights that model multiple companion SSMs, followed optionally by a nonlinear FFN. The SSMs are learned in parallel (1) and computed as a single matrix multiplication (2). 
  %
  \textbf{(Right):} We stack these layers into a \ourmethod{} network, where
  earlier layers compute SSMs as convolutions for fast sequence-to-sequence modeling and data preprocessing, while a decoder layer computes SSMs as recurrences for dynamic forecasting.}
  \label{fig:arch_overview} 
\end{figure}

\input{sections/subsections_method/companion_architecture}
\input{sections/subsections_method/forecasting}
\input{sections/subsections_method/algorithm}


