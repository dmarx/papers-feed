% Discuss preliminaries for time series modeling? 
\section{Preliminaries}
%
% Should also discuss two kinds of forecasting?  
% \begin{itemize}
%     \item Iterated multi-step forecasting (IMS): Recursive and direct multi-step forecasting: the best of both worlds, volume 19 
%     \item Direct multi-step (DMS): Direct multi-step estimation and forecasting.
% \end{itemize}
% \subsection{Problem Setting}

\header{Problem setting} We evaluate effective time series modeling with
% In this work, we evaluate effective time series modeling with accurate 
classification and forecasting tasks. For both tasks, we are given input sequences of $\ell$ ``look-back'' or ``lag'' time series samples $\boldsymbol{u}_{t - \ell: t - 1} = (u_{t - \ell}, \ldots, u_{t - 1}) \in \mathbb{R}^{\ell \times m}$ for sample feature size $m$. 
%
For classification, we aim to classify the sequence as the true class $y$ out of  possible classes $\mathcal{Y}$. 
For forecasting, we aim to correctly predict $H$ future time-steps over a ``horizon'' $\boldsymbol{y}_{t, t + h - 1} = (u_{t}, \ldots, u_{t + h - 1}) \in \mathbb{R}^{h \times m}$.
% To do so, we require methods that are both expressive and efficient.

% \subsection{State-Space Models for Time Series}
\header{State-space models for time series} 
We build on the discrete-time state-space model (SSM), which maps observed inputs $u_k$ to hidden states $x_k$, before projecting back to observed outputs $y_k$
\begin{align}
    x_{k+1} &= \zA x_k + \zB u_k  \label{eq:discrete_ssm_state} \\
    y_k &= \zC x_k + \zD u_k \label{eq:discrete_ssm_output}
\end{align}
where $\zA\in\R^{d\times d}$, $\zB\in\R^{d \times m}$, $\zC\in\R^{m' \times d}$, and $\zD\in\R^{m' \times m}$. 
% where $\zA\in\R^{d\times d}$, $\zB\in\R^{d \times m}$, $\zC\in\R^{m \times d}$, and $\zD\in\R^{m \times m}$. 
%
% The same relationship specified by $A, B, C, D$ holds for all samples in the input sequence $\boldsymbol{u}$ and output sequence $\boldsymbol{y}$. 
% To model time series in the \emph{single} SSM setting, because data is typically given as a single sequence, we treat $\boldsymbol{u}$ and $\boldsymbol{y}$ as copies of the same time series sequence. 
% To model time series in the \emph{single} SSM setting, we treat $\boldsymbol{u}$ and $\boldsymbol{y}$ as copies of the same time series sequence, such that
For now,  
% standard linear dynamical systems conventions, 
we stick to \emph{single-input single-output} conventions where $m, m' = 1$, and let $\zD = 0$. 
%
To model time series in the single SSM setting, we treat $\boldsymbol{u}$ and $\boldsymbol{y}$ as copies of the same process, such that  
% \st{}
% Matrices $A, B, C$ thus govern how the time series evolves over time as
% % We use the conventional linear dynamical systems (LDS) notation also adopted in prior work~\citep{Brogan:226422, gu2021combining}. 
\begin{equation}
    y_{k + 1} = u_{k + 1} = \zC(\zA x_k + \zB u_k)
\label{eq:input_output_ts_equal}
\end{equation}
We can thus learn a time series SSM by treating $\zA, \zB, \zC$ as black-box parameters in a neural net layer, \ie{} by updating $\zA, \zB, \zC$ via gradient descent \st{} with input $u_k$ and state $x_k$ at time-step $k$, following (\ref{eq:input_output_ts_equal}) predicts $\hat{y}_{k + 1}$ that matches the next time-step sample $y_{k + 1} = u_{k + 1}$.
%
This SSM framework and modeling setup is similar to prior works~\citep{gu2021combining, gu2021efficiently}, which adopt a similar interpretation of inputs and outputs being derived from the ``same'' process, \eg{} for language modeling. Here we study and improve this framework for time series modeling.
%
As extensions, in Sec.~\ref{sec:expressive_ssm_with_companion} we show how (\ref{eq:discrete_ssm_state}) and (\ref{eq:discrete_ssm_output}) express univariate time series with the right $\zA$ representation.
% and generalize to multivariate time series in Sec.~\ref{sec:method_architecture_overview}. 
%
In Sec.~\ref{sec:method_spacetime_layer} we discuss the multi-layer setting, where layer-specific $\boldsymbol{u}$ and $\boldsymbol{y}$ now differ, and we only model first layer inputs and last layer outputs as copies of the same time series process.
% In Sec.~\ref{todo}, we show how we learn a time series SSM by treating $\zA, B, C$ as black-box parameters in a linear neural network layer, \ie{} by updating $\zA, B, C$ via gradient descent \st{} with input $u_k$ and state $x_k$ at time-step $k$, (\ref{eq:input_output_ts_equal}) results in prediction $\hat{y}_{k + 1}$ that matches the next time-step sample $y_{k + 1} = u_{k + 1}$. 

% predicting future samples from past samples, and training the SSM with a regression objective between the predicted and ground-truth outputs.
% via supervised regression between predicted outputs $\hat{\boldsymbol{y}} = \text{SSM}(\zu)$ and $\zy$.

% While \cite{gu2021combining} also model the continuous version of (\ref{eq:discrete_ssm_state}, \ref{eq:discrete_ssm_output}), we stick with the discrete SSM due to its relative simplicity, alignment with how time series data is often a discrete sequence, and expressive power.
% %
% In the next section, we expand on this last point. We introduce our specific formulation of $\zA$ as the companion matrix, and show how this enables learning expressive SSMs for a wide range of time series processes (which are not all learnable via prior continuous SSMs). 

% \header{Expressiveness}


% \subsection{Core Challenges for Effective Time Series Modeling}

% \header{Expressiveness}
% \MZ{
% Describe how we need to be able to capture higher-order dependencies. Need large enough model dimension size to do this. For example, DLinear does lag input size times prediction size. 
% }

% \header{Efficiency}
% \MZ{
% Discuss how we want to get to $O(D + L)$, but the naive solution is $O(DL)$. Describe why this is important for time-series (in order to actually learn higher-order and long-range dependencies, we need large enough dimension size for the model, and need to be able to process long enough sequence, with reasonable time-frame and memory size.

% For example, DLinear does lag input size times prediction size. This is not great, because you end up with a very big model where model parameters scale with the size of the input sequence and the horizon.  

% It's also not very robust to different timesteps? But we are more robust? 


% }