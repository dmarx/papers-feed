\section{Proofs and Theoretical Discussion}\label{appendix:theory}
%

\subsection{Expressivity Results}\label{appendix:companion_expressivity}
%
%
% \begin{prop}\label{prop:ssm_companion_expressiveness}
%     An SSM with a companion state matrix can represent
%     \begin{itemize}[leftmargin=0.4in]
%         \item[i.] \fcircle[fill=green!30]{3pt}~~ARIMA \citep{box1970time}
%         \item[ii.] \fcircle[fill=red!30]{3pt}~~ Exponential smoothing
%         \item[iii.] \fcircle[fill=blue!30]{3pt}~~Controllable LTI systems \citep{chen1984linear}
%     \end{itemize}
% \end{prop}
\begin{p1}
An SSM with a companion state matrix can represent
    \begin{itemize}[leftmargin=0.4in]
        \item[i.] \fcircle[fill=green!30]{3pt}~~ARIMA \citep{box1970time}
        \item[ii.] \fcircle[fill=red!30]{3pt}~~ Exponential smoothing
        \item[iii.] \fcircle[fill=blue!30]{3pt}~~Controllable LTI systems \citep{chen1984linear}
        % \item[i.] \fcircle[fill=green!30]{3pt}~~ARIMA \citep{box1970time}
        % \item[ii.] \fcircle[fill=red!30]{3pt}~~ Exponential smoothing
        % \item[iii.] \fcircle[fill=blue!30]{3pt}~~Controllable LTI systems \citep{chen1984linear}
    \end{itemize}
\end{p1}
%
\begin{proof}[Proof of Proposition~\ref{prop:ssm_companion_expressiveness_td_bluf}]
%
We show each case separately. We either provide a set of algebraic manipulations to obtain the desired model from a companion SSM, or alternatively invoke standard results from signal processing and system theory.

% To obtain a second matrix $\hat A$ in companion form from $A$, note that
% %
% \[
%     \begin{aligned}
    
%     \end{aligned}}
% \]

%  The characteristic polynomial of $A$ is $\lambda^d + \sum_{k=0}^{d-1} a_k \lambda^k$. 
 
\textit{i.} \fcircle[fill=green!30]{3pt}~~ We start with a standard $\text{ARMA}(p,q)$ model
%
\[
y_k = u_k + \sum_{i=1}^q \theta_i u_{k-i} + \sum_{i=1}^p \phi_i y_{k-i} p_i
\]
%
We consider two cases: 

\paragraph{Case (1): Outputs $y$ are a shifted (lag--1) version of the inputs $u$}
%
\begin{equation}\label{eq:arma1}
    \begin{aligned}
        y_{k+1} &= y_k + \sum_{i=1}^q \theta_i y_{k-i} + \sum_{i=1}^p \phi_i y_{k-i+1} p_i \\
        &= (1 + \phi_1 y_k) + \sum_{i=1}^{q} (\theta_i + \phi_{i+1}) y_{k-i} + \sum_{i=q+1}^{p} \theta_i y_{k-i}
    \end{aligned}
\end{equation}
%
where, without loss of generality, we have assumed that $p > q$ for notational convenience. The autoregressive system \eqref{eq:arma1} is equivalent to

\[
%
\begin{bmatrix}\zA & \zB \\ \zC & \zD\end{bmatrix} = \begin{bmatrix}0 & 0 & \dots & 0 & 0  & 1 \\ 1 & 0 & \dots & 0 & 0 & 0 \\ \vdots & \vdots & \dots & \vdots & \vdots & \vdots \\ 0 & 0 & \dots & 0 & 0 & 0 \\ 0 & 0 & \dots & 1 & 0 & 0 \\ (1 + \phi_1) & (\theta_1 + \phi_{2}) & \dots & \theta_{d-1} & \theta_d & 0 \end{bmatrix}.
%
\]

in state-space form, with $x\in\R^{d}$ and $d = \max(p, q)$. Note that the state-space formulation is not unique.

\paragraph{Case (2): Outputs $y$ are ``shaped noise''.}
%
The ARMA(p,q) formulation (classically) defines inputs $u$ as white noise samples\footnote{Other formulations with forecast residuals are also common.}, $\forall k: p(u_k)$ is a normal distribution with mean zero and some variance. In this case, we can decompose the output as follows:
%
\[
\begin{aligned}
    y^{\text{ar}}_k &= \sum_{i=1}^p \phi_i y_{k-i} p_i &&&& y^{\text{ma}}_k = u_k + \sum_{i=1}^q \theta_i u_{k-i}
\end{aligned}
\]

such that $y_k =y^{\text{ar}}_k + y^{\text{ma}}_k$. The resulting state-space models are: 
%
\[
%
\begin{bmatrix}\zA^{\text{ar}} & \zB^{\text{ar}} \\ \zC^{\text{ar}} & \zD^{\text{ar}}\end{bmatrix} = \begin{bmatrix}0 & 0 & \dots & 0 & 0  & 1 \\ 1 & 0 & \dots & 0 & 0 & 0 \\ \vdots & \vdots & \dots & \vdots & \vdots & \vdots \\ 0 & 0 & \dots & 0 & 0 & 0 \\ 0 & 0 & \dots & 1 & 0 & 0 \\ \phi_1 & \phi_2 & \dots & \phi_{p-1} & \phi_p & 0 \end{bmatrix}.
%
\]
and 
%
\[
%
\begin{bmatrix}\zA^{\text{ma}} & \zB^{\text{ma}} \\ \zC^{\text{ma}} & \zD^{\text{ma}}\end{bmatrix} = \begin{bmatrix}0 & 0 & \dots & 0 & 0  & 1 \\ 1 & 0 & \dots & 0 & 0 & 0 \\ \vdots & \vdots & \dots & \vdots & \vdots & \vdots \\ 0 & 0 & \dots & 0 & 0 & 0 \\ 0 & 0 & \dots & 1 & 0 & 0 \\ \theta_1 & \theta_2 & \dots & \theta_{q-1} & \theta_q & 1 \end{bmatrix}.
%
\]
Note that $\zA^{\text{ar}} \in \R^{p \times p}, \zA^{\text{ma}} \in \R^{q \times q}$. More generally, our method can represent any ARMA process as the sum of two \ourmethod~\textit{heads}: one taking as input the time series itself, and one the driving signal $u$.
%

\paragraph{ARIMA} $\text{ARIMA}$ processes are $\text{ARMA}(p,q)$ applied to differenced time series. For example, first-order differencing $y_k = u_k - u_{k-1}$. Differencing corresponds to high--pass filtering of the signal $y$, and can be thus be realized via a convolution \citep{strang1996wavelets}. 

Any digital filter that can be expressed as a difference equation admits a state--space representation in companion form \citep{oppenheim1999discrete}, and hence can be learned by \ourmethod{}.


\textit{ii.} \fcircle[fill=red!30]{3pt}~~ Simple exponential smoothing (SES) \citep{brown1959statistical}
%
\begin{equation}\label{eq:ses}
    y_k = \alpha y_{k-1} + \alpha (1 - \alpha) y_{k-2} + \dots + \alpha(1 - \alpha)^{p-1} y_{k-p}
\end{equation}
is an AR process with a parametrization involving a single scalar $0 < \alpha < 1$ and can thus be represented in companion form as shown above. 

% Double exponential smoothing, extends \eqref{eq:ses} with a trend estimate $\hat{u}_k$
% %
% \begin{equation}\label{eq:holt}
%     \begin{aligned}
%         \hat{u}_k &= \beta ( y_k - y_{k-1} ) + (1 - \beta) \hat{u}_{k-1}  \\
%         y_{k+1} &= \alpha y_k + \alpha (1 - \alpha) y_{k-1} + \dots + \alpha(1 - \alpha)^{p} y_{k-p} + \hat{u}_k 
%     \end{aligned}
% \end{equation}
% yielding a system of coupled difference equations. The trend update equation is ARMA, and the entire system can be represented with two \ourmethod{} heads. 

% Other exponential smoothing models, such as Holt--Winters \citep{winters1960forecasting} also fit in the \ourmethod{} framework.
% As we will discuss show in more generality [cit opp], high--pass filters are not the only class of digital filters that can be represented via a state--space model.


\textit{iii.} \fcircle[fill=blue!30]{3pt}~~ Let $(\zA, \zB, \zC)$ be any controllable linear system. Controllability corresponds to invertibility of the Krylov matrix \cite[Thm 6.1, p145]{chen1984linear}
\[
\cK(\zA, \zB) = [\zB, \zA \zB, \dots, \zA^{d-1}\zB],~~\quad \cK(\zA, \zB)\in\R^{d \times d}.
\]
%
From ${\tt rank}(\cK) = d$, it follows that there exists a $\boldsymbol{a}\in\R^d$
\[
a_0 \zB + a_1 \zA \zB + \dots + a_{d-1}\zA^{d-1} \zB + \zA^d \zB = 0.
\]

Thus

\[
    \begin{aligned}
    \zA \cK &= [\zA \zB, \zA^2 \zB, \dots, \zA^d \zB]\\
    &= [\underbrace{\zA\zB, \zA^2 \zB, \dots, \zA^{d-1}\zB}_{\text{column left shift of}~\cK},~~ \underbrace{ - (a_0 \zB + a_1 \zA b + \dots + a_{d-1}\zA^{d-1} \zB)}_{\text{linear combination, columns of $\cK$}}] \\
    &= \cK (\zS^f - \boldsymbol{a} \boldsymbol{e}_{d-1}^\top )
    \end{aligned}
\]
%
where $\boldsymbol{G} = (\zS^f - \boldsymbol{a} \boldsymbol{e}_{d-1}^\top)$ is a companion matrix. 

\[
\zA \cK = \cK \boldsymbol{G} \iff \boldsymbol{G}= \cK ^{-1}\zA \cK. 
\]

Therefore $\boldsymbol{G}$ is similar to $\zA$. We can then construct a companion form state space $(\boldsymbol{G}, \zB, \zC, \zD)$ from $\zA$ using the relation above.

\end{proof}

\begin{p2}
    No class of continuous-time LSSL SSMs can represent the noiseless $\text{AR}(p)$ process.  
\end{p2}
%
\begin{proof}[Proof of Proposition~\ref{prop:continuous_ssm_no_ar_td_bluf}]
%
Recall from Sec.~\ref{sec:expressive_ssm_with_companion} that a noiseless $\text{AR}(p)$ process is defined by
\begin{align}
    y_{t} 
    &= \sum_{i = 1}^p \phi_i y_{t - i} = \phi_1y_{t - 1} + \ldots + \phi_p y_{t - p} \label{eq:appendix_ar_p}
\end{align}
with coefficients $\phi_1, \ldots, \phi_p$. This is represented by the SSM
\begin{align}
    x_{t + 1} &= \zS x_{t} + \zB u_{t} \\
    y_{t } &= \zC x_{t} + \zD u_{t} 
\end{align}
when $\zS \in \mathbb{R}^{p \times p}$ is the shift matrix, $\zB \in \mathbb{R}^{p \times 1}$ is the first basis vector $e_1$, $\zC \in \mathbb{R}^{1 \times p}$ is a vector of coefficients $\phi_1, \ldots, \phi_p$, and $\zD = 0$, \ie{}
\begin{align}
    \zS = 
    \begin{bmatrix}
    0  & 0  & \ldots & 0 & 0 \\
    1 & 0  &\ldots & 0 & 0 \\ 
    0 & 1 & \ldots & 0 & 0 \\
    \vdots &    & \ddots  & \vdots  & \vdots \\
    0 & 0  & \ldots & 1 & 0 \\
    \end{bmatrix},
    \;\;
    \zB = 
    \begin{bmatrix}
    1 & 0 & \ldots & 0
    \end{bmatrix}^T, 
    \;\; 
    \zC = 
    \begin{bmatrix}
    \phi_1 & \ldots & \phi_p
    \end{bmatrix}
\label{eq:shift_matrix_example}
\end{align}
We prove by contradiction that a continuous-time LSSL SSM cannot represent such a process. Consider the following solutions to a continuous-time system and a system \eqref{eq:appendix_ar_p}, both in autonomous form
\[
\begin{aligned}
    x^{\tt cont}_{t + 1} = e^{\zA} x_t \quad\quad x^{\tt disc}_{t + 1} = \zS x_t.
\end{aligned}
\]

It follows 
\[
\begin{aligned}
x^{\tt cont}_{t + 1} = x^{\tt disc}_{t + 1} &\iff e^{\zA} = \zS \\ 
&\iff \zA = \log{(\zS)}.
\end{aligned}
\]
we have reached a contradiction by \citep[Theorem 1]{culver1966existence}, as $\zS$ is singular by definition and thus its matrix logarithm does not exist.


\end{proof}


\subsection{Efficiency Results}~\label{appendix:efficiency_proofs}


We first prove that~\cref{alg:output_filter_computation} yields the correct
output filter $\zF^y$.
We then analyze its time complexity, showing that it takes time
$O(\ell \log \ell + d \log d)$ for sequence length $\ell$ and state dimension $d$.

\begin{theorem}
  \cref{alg:output_filter_computation} returns the filter $\zF^y = (\zC \zB, \dots, \zC\zA^{\ell-1}\zB)$.
\end{theorem}

\begin{proof}
  We follow the outline of the proof in \cref{sec:efficient_algorithm}.
  Instead of computing $\zF^y$ directly, we compute its spectrum (its discrete Fourier transform):
\begin{equation*}
  \tilde{\zF}^y[m] := {\cal F}(\zF^y) = \sum_{j=0}^{\ell-1} \zC \zA^j \omega^{mj} \zB = \zC(\zI - \zA^\ell) (\zI - \zA \omega^m)^{-1} \zB = \tilde{\zC} (\zI - \zA \omega^m)^{-1} \zB,
  \quad m = 0, 1, \dots, \ell-1.
\end{equation*}
where $\omega = \exp(-2\pi i / \ell)$ is the $\ell$-th root of unity.

This reduces to computing the quadratic form of the resolvent $(\zI - \zA \omega^m)^{-1}$
on the roots of unity (the powers of $\omega$).
Since $\zA$ is a companion matrix, we can write $\zA$ as a shift matrix plus a
rank-1 matrix, $\zA = \zS + a e_d^T$, where $e_d$ is the $d$-th basis vector
$[0, \dots, 0, 1]$ and the shift matrix $\zS$ is:
\begin{equation*}
  \zS = \begin{bmatrix}
    0 & 0 & \dots & 0 & 0 \\
    1 & 0 & \dots & 0 & 0 \\
    0 & 1 & \dots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \dots & 1 & 0 \\
  \end{bmatrix}.
\end{equation*}


Thus Woodbury's matrix identity (i.e., Sherman--Morrison formula) yields:
\begin{align*}
  (\zI - \zA \omega^m)^{-1}
  &= (\zI - \omega^m\zS - \omega^m a e_d^\top)^{-1} \\
  &= (\zI - \omega^m\zS)^{-1} + \frac{(\zI - \omega^m\zS)^{-1} \omega^m a e_d^\top (\zI - \omega^m\zS)^{-1}}{1 - \omega^m e_d^\top (\zI - \omega^m\zS)^{-1} a}.
\end{align*}
This is the resolvent of the shift matrix $(\zI - \omega^m \zS)^{-1}$, with a rank-1
correction.
Hence
\begin{equation}
  \label{eq:woodbury}
  \tilde{\zF}^y = \tilde{\zC} (\zI - \omega^m\zS)^{-1} \zB + \frac{\tilde{\zC}(\zI - \omega^m\zS)^{-1} a e_d^\top (\zI - \omega^m\zS)^{-1} \zB}{\omega^{-m} - e_d^\top (\zI - \omega^m\zS)^{-1} a}.
\end{equation}

We now need to derive how to compute the quadratic form of a resolvent of the
shift matrix efficiently.
Fortunately the resolvent of the shift matrix has a very special structure that
closely relates to the Fourier transform.
We show analytically that:
\begin{equation*}
  (\zI - \omega^m \zS)^{-1} = \begin{bmatrix}
    1 & 0 & \dots & 0 & 0 \\
    \omega^m & 1 & \dots & 0 & 0 \\
    \omega^{2m} & \omega^{m} & \dots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    \omega^{(d-1)m} & \omega^{(d-2)m} & \dots & \omega^m & 1 \\
  \end{bmatrix}.
\end{equation*}
It is easy to verify by multiplying this matrix with $\zI - \omega^m \zS$ to see if
we obtain the identity matrix. Recall that multiplying with $\zS$ on the left
just shifts all the columns down by one index.
Therefore:
\begin{align*}
  & \begin{bmatrix}
    1 & 0 & \dots & 0 & 0 \\
    \omega^m & 1 & \dots & 0 & 0 \\
    \omega^{2m} & \omega^{m} & \dots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    \omega^{(d-1)m} & \omega^{(d-2)m} & \dots & \omega^m & 1 \\
  \end{bmatrix}
  (\zI - \omega^m \zS) \\
    =&   \begin{bmatrix}
         1 & 0 & \dots & 0 & 0 \\
         \omega^m & 1 & \dots & 0 & 0 \\
         \omega^{2m} & \omega^{m} & \dots & 0 & 0 \\
         \vdots & \vdots & \ddots & \vdots & \vdots \\
         \omega^{(d-1)m} & \omega^{(d-2)m} & \dots & \omega^m & 1 \\
       \end{bmatrix}
    - \omega^m
  \zS \begin{bmatrix}
         1 & 0 & \dots & 0 & 0 \\
         \omega^m & 1 & \dots & 0 & 0 \\
         \omega^{2m} & \omega^{m} & \dots & 0 & 0 \\
         \vdots & \vdots & \ddots & \vdots & \vdots \\
         \omega^{(d-1)m} & \omega^{(d-2)m} & \dots & \omega^m & 1 \\
       \end{bmatrix} \\
             =&   \begin{bmatrix}
             1 & 0 & \dots & 0 & 0 \\
             \omega^m & 1 & \dots & 0 & 0 \\
             \omega^{2m} & \omega^{m} & \dots & 0 & 0 \\
             \vdots & \vdots & \ddots & \vdots & \vdots \\
             \omega^{(d-1)m} & \omega^{(d-2)m} & \dots & \omega^m & 1 \\
           \end{bmatrix}
        - \omega^m
        \begin{bmatrix}
              0 & 0 & \dots & 0 & 0 \\
              1 & 0 & \dots & 0 & 0 \\
              \omega^{m} & 1 & \dots & 0 & 0 \\
              \vdots & \vdots & \ddots & \vdots & \vdots \\
              \omega^{(d-2)m} & \omega^{(d-3)m} & \dots & 1 & 0 \\
            \end{bmatrix} \\
                  =&   \begin{bmatrix}
             1 & 0 & \dots & 0 & 0 \\
             \omega^m & 1 & \dots & 0 & 0 \\
             \omega^{2m} & \omega^{m} & \dots & 0 & 0 \\
             \vdots & \vdots & \ddots & \vdots & \vdots \\
             \omega^{(d-1)m} & \omega^{(d-2)m} & \dots & \omega^m & 1 \\
           \end{bmatrix}
        -
        \begin{bmatrix}
          0 & 0 & \dots & 0 & 0 \\
          \omega^m & 0 & \dots & 0 & 0 \\
          \omega^{2m} & \omega^m & \dots & 0 & 0 \\
          \vdots & \vdots & \ddots & \vdots & \vdots \\
          \omega^{(d-1)m} & \omega^{(d-2)m} & \dots & \omega & 0 \\
        \end{bmatrix} \\
          =& \zI.
\end{align*}
Thus the resolvent of the shift matrix indeed has the form of a lower-triangular
matrix containing the roots of unity.

Now that we have the analytic formula of the resolvent, we can derive its
quadratic form, given some vectors $u, v \in \mathbb{R}^d$.
Substituting in, we have
\begin{align*}
  u^T (\zI - \omega^m \zS)^{-1} v
  &= u_1 v_1 + u_2 v_1 \omega^m + u_2 v_2 + u_3 v_1 \omega^{2m} + u_3 v_2 \omega^m + u_3 v_1 + \dots.
\end{align*}
Grouping terms by powers of $\omega$, we see that we want to compute
$u_1 v_1 + u_2 v_2 + \dots + u_d v_d$, then
$u_2v_1 + u_3v_2 + \dots + u_d v_{d-1}$, and so on.
The term corresponding to $\omega^{km}$ is exactly the $k$-th element of the linear
convolution $u \ast v$.
Define $q = u \ast v$, then $u^T(\zI - \omega^m \zS)^{-1} v$ is just the Fourier
transform of $u \ast v$.
To deal with the case where $d > \ell$, we note that the powers of roots of unity
will repeat, so we just need to extend the output of $u \ast v$ to be multiples of
$\ell$, then split them into chunk of size $\ell$, then sum them up and take the
length-$\ell$ Fourier transform.
This is exactly the procedure $\mathrm{quad}(u, v)$ defined in~\cref{alg:output_filter_computation}.

Once we have derived the quadratic form of the resolvent $(\zI - \omega^m \zS)^{-1}$,
simply plugging it into the Woodbury's matrix identity (\cref{eq:woodbury})
yields~\cref{alg:output_filter_computation}.

\end{proof}

We analyze the algorithm's complexity.
\begin{theorem}
  \cref{alg:output_filter_computation} has time complexity
  $O(\ell \log \ell + d \log d)$ for sequence length $\ell$ and state dimension $d$.
\end{theorem}

\begin{proof}
  We see that computing the quadratic form of the resolvent
  $(\zI - \omega^m \zS)^{-1}$ involves a linear convolution of size $d$ and a Fourier
  transform of size $\ell$.
  The linear convolution can be done by performing an FFT of size $2d$ on both
  inputs, multiply them pointwise, then take the inverse FFT of size $2d$.
  This has time complexity $O(d \log d)$.
  The Fourier transform of size $\ell$ has time complexity $O(\ell \log \ell)$.
  %
  The whole algorithm needs to compute four such quadratic forms, hence it takes
  time $O(\ell \log \ell + d \log d)$.
\end{proof}


\textbf{Remark.} We see that the algorithm easily extends to the case where the matrix $\zA$ is a companion matrix plus low-rank matrix (of some rank $k$).
We can write $\zA$ as the sum of the shift matrix and a rank-$(k+1)$ matrix (since $\zA$ itself is the sum of a shift matrix and a rank-1 matrix).
Using the same strategy, we can use the Woodbury's matrix identity for the rank-$(k+1)$ case.
The running time will then scale as $O(k (\ell \log \ell + d \log d))$.


\subsection{Companion Matrix Stability}

\paragraph{Normalizing companion parameters for bounded gradients}
%
\begin{prop}[Bounded \ourmethod{} Gradients]
%
Given $s$, the norm of the gradient of a \ourmethod{} layer is bounded for all $k < s$ if 
\[\sum_{i=0}^{d-1} |\boldsymbol{a}_i| = 1
\]
%
\end{prop}

\begin{proof}
Without loss of generality, we assume $x_0 = 0$. Since the solution at time $s$ is 
\[
y_s = \zC \sum_{i-1}^{s-1} \zA^{s-i-1} \zB u_i 
\]
we compute the gradient w.r.t $u_k$ as
\begin{equation}
    \begin{aligned}
    \frac{\dd y_s}{\dd u_k} = \zC \zA^{s-k-1} \zB. 
    \end{aligned}
\end{equation}

%

The largest eigenvalue of $\zA$
\[
\begin{aligned}
    \max\{{\tt eig}(\zA)\} 
    % &= \\ 
    &\leq \max\big\{1, \sum_{i=0}^{d-1}  |\boldsymbol{a}_i|\big\}\quad\quad \text{Corollary of Gershgorin \citep[Theorem 1]{hirst1997bounding}} \\
&= 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \text{using $\sum_i|\boldsymbol{a}_i| = 1$}
\end{aligned}
\]

is $1$, which implies convergence of the operator $\zC \zA^{s-k-1} \zB$. Thus, the gradients are bounded. 

\end{proof}

We use the proposition above to ensure gradient boundedness in \ourmethod{} layers by normalizing $\boldsymbol{a}$ every forward pass. 



% SCRATCH
% Such models parameterize the continuous-time state-space model, which is defined as
% \begin{align}
% x'(t) &= \zA x(t) + \zB u(t) \label{eq:continuous_ssm_state}\\
% y(t) &= \zC x(t) + \zD u(t)  \label{eq:continuous_ssm_output}
% \end{align}
% Here $x$ and $y$ are continuous-time processes, and $x(t), y(t)$ denote the values of $x$ and $y$ sampled at time $t$ respectively~\tempcite{}. 




% Assume that Eq.~\ref{eq:continuous_ssm_state} and Eq.~\ref{eq:continuous_ssm_output} together can represent the AR(p) process. Then it follows that Eq.~\ref{eq:continuous_ssm_output}'s solution should express Eq.~\ref{eq:appendix_ar_p} with initial state $x(0)$ and inputs 
% \[u(0), \ldots, u(1) = u_t - p, \ldots, u_t\]
% We solve for Eq.~\ref{eq:continuous_ssm_output} as follows. Noting that $x'(t) &= \zA x(t) + \zB u(t)$ is an ordinary differential equation, we first solve for $x(t)$ with standard integrating factors. Multiplying both sides by $e^{-\zA t}$, it follows from Eq.~\ref{eq:continuous_ssm_state} that
% \[
%     e^{-\zA t} x'(t) - e^{-\zA t}\zA x(t) &= e^{-\zA t} \zB u(t) \\
% \]
% \[
% \Rightarrow \frac{\partial}{\partial t} e^{-\zA t} x(t) &= e^{-\zA t} \zB u(t)
% \]
% Integrating both sides from $t = 0$ to $s$, by the fundamental theorem of calculus we get
% \[
% e^{-\zA s}x(s) - e^{-\zA 0} x(0) = \int_{0}^s e^{-\zA t} \zB u(t) d t
% \]
% So it follows that
% \[
% x(s) = e^{\zA s} x(0) + \int_0^s e^{\zA(s - t)} \zB u(t) dt \\
% \]
% and 
% \[
% y(s) = \zC e^{\zA s} x(0) + \int_0^s \zC e^{\zA(s - t)} \zB u(t) dt \\
% \]