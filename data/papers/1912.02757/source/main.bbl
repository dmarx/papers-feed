\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[MacKay(1992)]{mackay1992bayesian}
David~JC MacKay.
\newblock \emph{Bayesian methods for adaptive models}.
\newblock PhD thesis, California Institute of Technology, 1992.

\bibitem[Neal(1996)]{Neal96}
Radford~M. Neal.
\newblock \emph{{Bayesian Learning for Neural Networks}}.
\newblock Springer-Verlag New York, Inc., 1996.

\bibitem[Welling and Teh(2011)]{Welling2011}
Max Welling and Yee~Whye Teh.
\newblock {Bayesian Learning via Stochastic Gradient Langevin Dynamics}.
\newblock In \emph{ICML}, 2011.

\bibitem[Springenberg et~al.(2016)Springenberg, Klein, Falkner, and
  Hutter]{springenberg2016bayesian}
Jost~Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter.
\newblock Bayesian optimization with robust {B}ayesian neural networks.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Graves(2011)]{graves}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{NeurIPS}, 2011.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{BBB}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{ICML}, 2015.

\bibitem[Louizos and Welling(2017)]{louizos2017multiplicative}
Christos Louizos and Max Welling.
\newblock {Multiplicative Normalizing Flows for Variational Bayesian Neural
  Networks}.
\newblock In \emph{ICML}, 2017.

\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{flipout}
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse.
\newblock Flipout: Efficient pseudo-independent weight perturbations on
  mini-batches.
\newblock In \emph{ICLR}, 2018.

\bibitem[Gal and Ghahramani(2016)]{mcdropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{ICML}, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 2014.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Kendall and Gal(2017)]{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock {What uncertainties do we need in Bayesian deep learning for computer
  vision?}
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Breiman(1996)]{bagging}
Leo Breiman.
\newblock Bagging predictors.
\newblock \emph{Machine learning}, 1996.

\bibitem[Lee et~al.(2015)Lee, Purushwalkam, Cogswell, Crandall, and
  Batra]{mheads}
Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv
  Batra.
\newblock Why {M} heads are better than one: Training a diverse ensemble of
  deep networks.
\newblock \emph{arXiv preprint arXiv:1511.06314}, 2015.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D~Sculley, Sebastian
  Nowozin, Joshua~V Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? {E}valuating predictive
  uncertainty under dataset shift.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Gustafsson et~al.(2019)Gustafsson, Danelljan, and
  Sch{\"o}n]{gustafsson2019evaluating}
Fredrik~K Gustafsson, Martin Danelljan, and Thomas~B Sch{\"o}n.
\newblock Evaluating scalable {B}ayesian deep learning methods for robust
  computer vision.
\newblock \emph{arXiv preprint arXiv:1906.01620}, 2019.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P Vetrov, and
  Andrew~G Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of {DNNs}.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred~A Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock \emph{arXiv preprint arXiv:1803.00885}, 2018.

\bibitem[Fort and Jastrzebski(2019)]{fort2019large}
Stanislav Fort and Stanislaw Jastrzebski.
\newblock Large scale structure of neural network loss landscapes.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Goodfellow and Vinyals(2014)]{Goodfellow2014QualitativelyCN}
Ian~J. Goodfellow and Oriol Vinyals.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{CoRR}, abs/1412.6544, 2014.

\bibitem[Li et~al.(2018)Li, Farkhoor, Liu, and Yosinski]{li2018measuring}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock In \emph{ICLR}, 2018.

\bibitem[Fort and Scherlis(2019)]{fort2019goldilocks}
Stanislav Fort and Adam Scherlis.
\newblock The {G}oldilocks zone: Towards better understanding of neural network
  loss landscapes.
\newblock In \emph{AAAI}, 2019.

\bibitem[Krizhevsky(2009)]{cifar10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR}, 2009.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016{\natexlab{a}}.

\bibitem[Kingma and Ba(2015)]{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, 2016{\natexlab{b}}.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2018benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{ICLR}, 2019.

\bibitem[Brier(1950)]{brier1950verification}
Glenn~W Brier.
\newblock Verification of forecasts expressed in terms of probability.
\newblock \emph{Monthly weather review}, 1950.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{svhn}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock {Reading Digits in Natural Images with Unsupervised Feature
  Learning}.
\newblock In \emph{NeurIPS Workshop on Deep Learning and Unsupervised Feature
  Learning}, 2011.

\bibitem[Maaten and Hinton(2008)]{maaten2008visualizing}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock {Visualizing data using t-SNE}.
\newblock \emph{JMLR}, 2008.

\bibitem[Zhang et~al.(2020)Zhang, Li, Zhang, Chen, and
  Wilson]{Zhang2020Cyclical}
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew~Gordon Wilson.
\newblock Cyclical stochastic gradient mcmc for bayesian deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkeS1RVtPS}.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{maddox2019simple}
Wesley~J Maddox, Pavel Izmailov, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13132--13143, 2019.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{UAI}, 2018.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{mandt2017stochastic}
Stephan Mandt, Matthew~D Hoffman, and David~M Blei.
\newblock Stochastic gradient descent as approximate {B}ayesian inference.
\newblock \emph{JMLR}, 2017.

\end{thebibliography}
