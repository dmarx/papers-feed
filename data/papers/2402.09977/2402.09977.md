---
abstract: |
  Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.
author:
- |
  Leonidas Gee  
  Expert.ai, Italy  
  `lgee@expert.ai`  
  Andrea Zugarini  
  Expert.ai, Italy  
  `azugarini@expert.ai`  
  Leonardo Rigutini  
  Expert.ai, Italy  
  and  
  University of Siena  
  `lrigutini@expert.ai`  
    
  Paolo Torroni  
  Department of Computer Science and  
  Engineering,  
  University of Bologna, Italy  
  `paolo.torroni@unibo.it`  
bibliography:
- anthology.bib
- custom.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Fast Vocabulary Transfer for Language Model Compression
---





# Introduction

In the last few years, many NLP applications have been relying more and more on large pre-trained Language Models (LM) . Because larger LMs, on average, exhibit higher accuracy, a common trend has been to increase the model’s size. Some LMs like GPT-3 and BLOOM[^1] have reached hundreds of billion parameters. However, these models’ superior performance comes at the cost of a steep increase in computational footprint, both for development and for inference, ultimately hampering their adoption in real-world business use-cases. Besides models that only a few hi-tech giants can afford, like GPT-3, even smaller LMs with hundreds of million parameters could be too expensive or infeasible for certain products. For one thing, despite being tremendously cheaper than their bigger cousins, fine-tuning, deploying and maintaining large numbers of such models (one for each downstream task) soon becomes too expensive. Furthermore, latency and/or hardware requirements may limit their applicability to specific use-cases. For all these reasons, significant efforts – in both academic and industry-driven research – are oriented towards the designing of solutions to drastically reduce the costs of LMs.

Recently, several attempts have been made to make these models smaller, faster and cheaper, while retaining most of their original performance . Notably, Knowledge Distillation (KD) is a teacher-student framework, whereby the teacher consists of a pre-trained large model and the student of a smaller one. The teacher-student framework requires that both the teacher and the student estimate the same probability distribution. While the outcome is a smaller model, yet, this procedure constrains the student to operate with the same vocabulary as the teacher in the context of Language Modeling.

In this work, we explore a method for further reducing an LM’s size by compressing its vocabulary through the training of a tokenizer in the downstream task domain. The tokenizer  is a crucial part of modern LMs. In particular, moving from word to subword-level, the tokenization solves two problems: vocabulary explosion and unknown words. Moreover, the capability to tokenize text effectively in any domain is key for the massive adoption of pre-trained general-purpose LMs fine-tuned on downstream tasks. Indeed, tokenizers are still able to process out-of-distribution texts at the cost of producing frequent word splits into multiple tokens.

However, the language varies significantly in vertical domains or, more generally, in different topics. Hence, ad-hoc tokenizers, trained on the domain statistics, may perform a more efficient tokenization, reducing on average the length of the tokenized sequences. This is important since compact and meaningful inputs could reduce computational costs, while improving performance. Indeed, memory and time complexity of attention layers grows quadratically with respect to the sequence length . Furthermore, a vertical tokenizer may require a smaller vocabulary, which also affects the size of the embedding matrix, hence further reducing the model’s size.

Following this intuition, we propose a Vocabulary Transfer (VT) technique to adapt LMs to in-domain, smaller tokenizers, in order to further compress and accelerate them. This technique is complementary to the aforementioned model compression methods and independent of the type of tokenizer. As a matter of fact, we apply it in combination with KD.

Our experiments show that VT achieves an inference speed-up between x1.07 and x1.40, depending on the downstream task, with a limited performance drop, and that a combination of VT with KD yields an overall reduction up to x2.76.

The paper is organized as follows. After reviewing related works in Section , we present the methodology in Section , we then outline the experiments in Section  and draw our conclusions in Section .

# Related Works

The goal of Model Compression is to shrink and optimize neural architectures, while retaining most of their initial performance. Research on LM compression has been carried out following a variety of approaches like quantization , pruning  knowledge distillation , and combinations thereof .

A most popular distillation approach in NLP was proposed by . The obtained model, called DistilBERT, is a smaller version of BERT, with the same architecture but half the layers, trained to imitate the full output distribution of the teacher (a pre-trained BERT model). DistilBERT has a 40% smaller size than BERT and retains 97% of its language understanding capabilities. This enables a 60% inference-time speedup. Further compression was achieved by  by adding transformer-layer, prediction-layer and embedding-layer distillation. The resulting model, TinyBERT, is 10 times smaller than BERT, with only four layers and reduced embeddings sizes. Related methods were proposed , achieving similar compression rates. All these works focus on the distillation of general-purpose language models. investigated the interaction between KD and Domain Adaptation.

Little focus has been devoted thus far to the role of tokenization in the context of model compression. Even in domain adaptation , the vocabulary was kept the same. Both the versatility of the subword-level tokenization, and the constraints imposed by the teacher-student framework (same output distribution), discouraged such investigations. Recently, presented an approach for transferring the vocabulary of an LM into a new vocabulary learned from new domain, with the purpose of boosting the performance of the fine-tuned model. To the best of our knowledge, we are the first to study VT in the scope of model compression.

# Vocabulary Transfer

Let us consider a LM, trained on a general-purpose domain $\mathcal{D}_{gen}$ and associated with a vocabulary ${\cal V}_{gen}$. Such a vocabulary is used by the LM’s tokenizer in order to produce an encoding of the input string via an embedding matrix $E_{gen}$ defined on ${\cal V}_{gen}$. More specifically, a tokenizer is a function that maps a textual string into a sequence of symbols of a given vocabulary ${\cal V}$. Let $\mathcal{T}$ be a tokenizer associated with a vocabulary ${\cal V}$ and a string $\mbox{\boldmath $s$}$, we have $\mathcal{T}: \mbox{\boldmath $s$}\rightarrow (\mbox{\boldmath $t$}_1, \ldots, \mbox{\boldmath $t$}_n), \mbox{\boldmath $t$}_i \in {\cal V},  \forall i=1,\ldots,n$. Hence, the vocabulary of the tokenizer determines how words in a text are split, whether as words, subwords, or even characters. These symbols, which define the LM’s vocabulary, are statistically determined by training the tokenizer to learn the distribution of a dataset.

Now, let us consider a vertical domain $\mathcal{D}_{in}$, also referred as *in-domain*. For the reasons discussed earlier, a vocabulary ${\cal V}_{in}$ specialized on $\mathcal{D}_{in}$ itself better fits the language distribution than ${\cal V}_{gen}$. Unfortunately, with a new vocabulary, embedding representations associated with the tokens of ${\cal V}_{gen}$ would be lost. Thus, VT aims to initialize ${\cal V}_{in}$ by re-using most of the information learned from the LM pre-trained on $\mathcal{D}_{gen}$. Once the new tokenizer $\mathcal{T}_{in}$ has been trained on the in-domain dataset $\mathcal{D}_{in}$ using a given vocabulary size, $\mathcal{T}_{in}$ will be different from the LM’s tokenizer $\mathcal{T}_{gen}$. However, the two tokenizers’ vocabularies ${\cal V}_{gen}$ and ${\cal V}_{in}$ may still have a large portion of their symbols in common. Our objective is to transfer most of the information from ${\cal V}_{gen}$ into ${\cal V}_{in}$. To this end, we first define a mapping between each symbol in ${\cal V}_{in}$ and a set of symbols in ${\cal V}_{gen}$. Then, we define an assignment criterion, based on the mapping, to obtain the embeddings for the tokens of $\mathcal{T}_{in}$.

One such criterion, called Vocabulary Initialization with Partial Inheritance (VIPI), was defined by . Whenever a token is in ${\cal V}_{in}$ but not in ${\cal V}_{gen}$, VIPI calculates all the partitions of the new token with tokens from ${\cal V}_{gen}$, then takes the minimal partitions and finally averages them to obtain an embedding for the new token. Differently, we define a simplified implementation of VIPI called FVT for Fast Vocabulary Transfer. Instead of calculating all tokenizations, FVT uses a straightforward assignment mechanism, whereby each token $t_i \in {\cal V}_{in}$ is partitioned using $\mathcal{T}_{gen}$. If $t_i$ belongs to both vocabularies, $t_i \in {\cal V}_{in} \cap {\cal V}_{gen}$, then $\mathcal{T}_{gen}{(t_i)}=t_i$ and the in-domain LM embedding $E_{in}(t_i)$ is the same as the embedding in the general LM: $$E_{in}(t_i) = E_{gen}(t_i).\label{eq:base_vt}$$ If instead $t_i \in {\cal V}_{in} \setminus {\cal V}_{gen}$, then the in-domain embedding is the average of the embeddings associated with the tokens produced by $\mathcal{T}_{gen}$:

$$E_{in}(t_i) = \frac{1}{{|\mathcal{T}_{gen}(t_i)|}} \cdot \sum_{t_j \in \mathcal{T}_{gen}(t_i) } E_{gen}(t_j). \label{eq:fvt}$$ Please notice that Equation  is a generalization of Equation . Indeed, in case $t_i \in {\cal V}_{in} \cap  {\cal V}_{gen}$, Equation  falls back to Equation .

Once embeddings are initialized with FVT, we adjust the model’s weights by training it with MLM on the in-domain data before fine-tuning it on the downstream task. MLM eases adaptation and has already been found to be beneficial in . We observed this trend as well during preliminary experiments, therefore we kept such a tuning stage in all our experiments.

<figure id="fig:vt_examples">
<p></p>
<figcaption>Example of different tokenizations using a pre-trained or an adapted tokenizer. In the latter case, domain-specific words are not broken down into multiple word pieces.</figcaption>
</figure>

<div id="tab:seq_len">

| **Dataset** | $\mathcal{T}_{gen}$ | $\mathcal{T}_{100}$ | $\mathcal{T}_{75}$ | $\mathcal{T}_{50}$ | $\mathcal{T}_{25}$ |
|:-----------:|:-------------------:|:-------------------:|:------------------:|:------------------:|:------------------:|
|   **ADE**   |         31          |         21          |         22         |         23         |         26         |
| **LEDGAR**  |         155         |         131         |        131         |        132         |        135         |
| **CoNLL03** |         19          |         17          |         17         |         18         |         20         |

Average sequence length on the three datasets with different tokenizers. $\mathcal{T}_{gen}$ is the generic tokenizer (BERT cased), the same in each corpus, while $\mathcal{T}_{\%}$ are the tokenizers trained in the vertical domain itself, where ${\%}$ indicates the percentage of the original vocabulary size that has been set for training it.

</div>

As a baseline model, we also implement a method called Partial Vocabulary Transfer (PVT), whereby only the tokens belonging to both vocabularies $t_i \in {\cal V}_{in} \cap  {\cal V}_{gen}$ are initialized with pre-trained embeddings, while unseen new tokens are randomly initialized.

<div id="tab:base_f1_results">

|       **Transfer**        |  **ADE**  | **LEDGAR** | **CoNLL03** |
|:-------------------------:|:---------:|:----------:|:-----------:|
|    $\mathcal{T}_{gen}$    | **90.80** |   80.93    |  **89.43**  |
| $\mathcal{T}_{100}$ + FVT |   90.77   |   80.60    |    87.87    |
| $\mathcal{T}_{75}$ + FVT  |   90.40   |   80.93    |    87.90    |
| $\mathcal{T}_{50}$ + FVT  |   90.07   |   80.93    |    86.87    |
| $\mathcal{T}_{25}$ + FVT  |   90.27   | **81.03**  |    86.17    |
| $\mathcal{T}_{100}$ + PVT |   82.57   |   80.07    |    84.53    |
| $\mathcal{T}_{75}$ + PVT  |   82.47   |   80.33    |    84.63    |
| $\mathcal{T}_{50}$ + PVT  |   83.07   |   80.23    |    84.43    |
| $\mathcal{T}_{25}$ + PVT  |   83.57   |   80.20    |    83.47    |

F1 results on the three benchmarks. A pre-trained language model fine-tuned on the task ($\mathcal{T}_{gen}$) is compared with models having differently sized in-domain tokenizers ($\mathcal{T}_{100},\mathcal{T}_{75},\mathcal{T}_{50},\mathcal{T}_{25}$) adapted by transferring information with FVT or PVT.

</div>

## Distillation

VT can be combined with other model compression methods like quantization, pruning and KD. For some of the methods, the combination is trivial, since they have no impact on the vocabulary. KD, however, requires the vocabularies of the student and teacher to be aligned. Hence, its integration with VT is non-trivial. Accordingly, we set up a KD procedure with VT, in order to determine the effects of applying both VT and KD to an LM.

Our distillation consists of two steps. In the first step, we replicate the distillation process used in for DistilBERT, in which the number of layers of the encoder is halved and a triple loss-function is applied: a distillation loss, a MLM loss, and a cosine embedding loss. However, unlike the original setup, we do not remove the token-type embeddings and pooler. Inspired by , after distilling the student on ${\cal D}_{gen}$, we further distil the student using ${\cal D}_{in}$. However, instead of adapting the teacher before the second distillation, we simply distil the student a second time on the in-domain dataset. Finally, we apply VT using either FVT or PVT and fine-tune the student model on the in-domain datasets.

Our choice of applying VT after KD is based on findings by , that different input embedding spaces will produce different output embedding spaces. This difference in spaces is not conducive to knowledge transfer during distillation. Hence, if VT were to be applied first to the student, its input embedding space would differ greatly from that of the pre-trained teacher during distillation.

# Experiments

In the experiments we measure the impact of FVT on three main KPIs: quality (F1 score), size of the models and speedup in inference.

<div id="tab:distill_f1_results">

|                           |           |            |             |
|:-------------------------:|:---------:|:----------:|:-----------:|
|     **Distillation**      |           |            |             |
|       **Transfer**        |  **ADE**  | **LEDGAR** | **CoNLL03** |
|    $\mathcal{T}_{gen}$    | **90.47** |   78.37    |  **86.90**  |
| $\mathcal{T}_{100}$ + FVT |   89.47   |   78.33    |    84.63    |
| $\mathcal{T}_{75}$ + FVT  |   88.57   |   78.90    |    84.23    |
| $\mathcal{T}_{50}$ + FVT  |   88.43   | **79.30**  |    83.80    |
| $\mathcal{T}_{25}$ + FVT  |   88.23   |   78.10    |    83.13    |
| $\mathcal{T}_{100}$ + PVT |   79.13   |   76.97    |    81.13    |
| $\mathcal{T}_{75}$ + PVT  |   78.87   |   76.93    |    81.40    |
| $\mathcal{T}_{50}$ + PVT  |   76.30   |   77.37    |    81.63    |
| $\mathcal{T}_{25}$ + PVT  |   77.90   |   77.33    |    79.50    |

F1 results on the three benchmarks. A distilled language model fine-tuned on the task ($\mathcal{T}_{gen}$) is compared with models having differently sized in-domain tokenizers ($\mathcal{T}_{100},\mathcal{T}_{75},\mathcal{T}_{50},\mathcal{T}_{25}$) adapted by transferring information with FVT or PVT.

</div>

## Experimental Setup

We consider for all our experiments the pre-trained cased version of $\text{BERT}_{base}$  as our pre-trained language model. Its tokenizer is composed of 28996 wordpieces. We then define four vocabulary sizes for retraining our tokenizers. Specifically, we take the original vocabulary size and define it as a vocabulary size of 100%. We subsequently reduce this size to 75%, 50%, and 25%. From now on, we will refer to such tokenizers as $\mathcal{T}_{100},\mathcal{T}_{75},\mathcal{T}_{50},\mathcal{T}_{25}$ respectively, while the original vocabulary will be called $\mathcal{T}_{gen}$.

Models are fine-tuned for 10 epochs with early stopping on the downstream task. We set the initial learning rate to $3\cdot10^{-5}$ and batch size to 64 for each task. The sequence length is set to 64 for ADE and CoNLL03 and 128 for LEDGAR. Each configuration is repeated 3 times with different random initializations. MLM is performed for one epoch.

## Datasets

To best assess the effectiveness of VT, we apply it on three different tasks from three heterogeneous linguistic domains: medical (ADE), legal (LEDGAR) and news (CoNLL03). Table  reports the dataset statistics.

#### ADE.

The Adverse Drug Events (ADE) corpus is a binary sentence classification dataset in the medical domain. This domain is particularly suitable for investigating the benefits of VT, since documents are characterized by the presence of frequent technical terms, such as drug and disease names, that are usually rare in common language. Domain-specific words are usually split into multiple tokens, yielding longer sequences and breaking the semantics of a word into multiple pieces. An example is shown in Figure .

#### LEDGAR.

LEDGAR  is a document classification corpus of legal provisions in contracts from the US Securities and Exchange Commission (SEC). The dataset is annotated with 100 different mutually-exclusive labels. It is also part of LexGLUE , a benchmark for legal language understanding.

#### CoNLL03.

CoNLL03  is a popular Named Entity Recognition (NER) benchmark. It is made of news stories from the Reuters corpus. We chose this corpus because, differently from ADE and LEDGAR, the news domain typically uses a more standard language, hence we expect its distribution to differ less from the one captured by a general-purpose tokenizers in the web. Statistics in Table  confirms this hypothesis. We can observe that the sequence compression gain obtained with domain-specific tokenizers is less significant with respect to LEDGAR and ADE.

<div id="tab:data_stats">

| **Dataset** | **Train** | **Validation** | **Test** |
|:-----------:|:---------:|:--------------:|:--------:|
|     ADE     |   16716   |      3344      |   836    |
|   LEDGAR    |   60000   |     10000      |  10000   |
|   CoNLL03   |   14042   |      3251      |   3454   |

Number of examples of each dataset.

</div>

<figure id="fig:2d_size_f1f1">
<span class="image placeholder" data-original-image-src="images/results_ADE.png" data-original-image-title="" width="\columnwidth"></span>
<figcaption>F1-score vs model size of VT with or without KD on ADE. VT and KD together can further compress a LM’s size in exchange for a limited performance drop. FVT is better than PVT. A smaller vocabulary size does not always imply a lower performance.</figcaption>
</figure>

## Results

We report an extensive evaluation of FVT on different setups and perspectives.

#### In-domain Tokenization.

By retraining the tokenizer on the in-domain dataset, the average number of tokens per sequence decreases since the learned distribution reduces the number of word splits, as shown in Table . In the medical domain, which is particularly specialized, we notice a remarkable 32% reduction of the average number of tokens per sequence. We expect this to yield a noticeable impact on inference time speedup. Furthermore, we can notice in Figure  that the sequence length distribution shifts to the left for the learned tokenizers. It can also be observed that by reducing the vocabulary size of the *in-domain* tokenizer, the sequence length distribution will begin to shift back to the right. Indeed, with fewer tokens in its vocabulary, the tokenizer will need to break down words more frequently into subwords.

#### Vocabulary Transfer.

From the results shown in Tables  and , we note a few interesting findings. First, FVT vectors initialization method consistently outperforms the baseline PVT, which confirms the positive contribution of Equation . Second, transferring vocabulary with FVT causes limited drops in performance, especially in LEDGAR (the largest one), where F1 slightly increases despite a 75% vocabulary reduction. As a matter of fact, the effects of FVT on model performance do not have a steadily decreasing trend as might be expected when reducing the vocabulary size, as also evident from Figure . In some cases, somewhat surprisingly, reducing the vocabulary size yields better model performance. In other cases, a 50% vocabulary size reduction yields better results than a full scale reduction or no reduction. Hence, vocabulary size should be considered as a hyperparameter.

#### Vocabulary Transfer and Distillation.

The results summarized in Table  clearly indicate that KD is complementary to VT: there is no harm in applying them together, in terms of performance on the downstream task. Crucially, this guarantees a full exploitation of FVT in the scope of language model compression.

#### Compression and Efficiency.

After showcasing that VT has limited impact on performance, we analyze and discuss its effects on efficiency and model compression. Table  reports the relative F1 drop on the downstream task with respect to the original LM ($\Delta$F1), the relative reduction in model size ($\Delta$Size) and the speedup gained by FVT alone and by FVT combined with KD for varying vocabulary sizes. Either way, FVT achieves a remarkable 15%+ reduction with respect to BERT’s learnable parameters, with almost no loss in F1.

Furthermore, the reduced input length enabled by in-domain tokenization brings a reduction in inference time. The more a language is specialized, the higher is the speedup with in-domain tokenizers. This is also confirmed by the experiments, where the major benefits are obtained on the medical domain, with a x1.40 speedup. In CoNLL03 instead where language is much less specialized, speedup reduces and even disappears with $\mathcal{T}_{25}$. Distillation further pushes compression and speedup in any benchmark and setup, up to about 55% (of which 15% due to VT) and x2.75 respectively.

In summary, depending on the application needs, VT enables a strategic trade-off between compression rate, inference speed and accuracy.

# Conclusion

The viability and success of industrial NLP applications often hinges on a delicate trade-off between computational requirements, responsiveness and output quality. Hence, language model compression methods are an active area of research whose practical ramifications are self-evident. One of the factors that greatly contribute to a model’s inference speed and memory footprint is vocabulary size. VT has been recently proposed for improving performance, but never so far in the scope of model compression. In this work, we run an extensive experimental study on the application of a lightweight method for VT, called FVT. An analysis conducted on various downstream tasks, application domains, vocabulary sizes and on its possible combination with knowledge distillation indicates that FVT enables a strategic trade-off between compression rate, inference speed and accuracy, especially, but not only, in more specialized domains. Importantly, FVT appears to be orthogonal to other model compression methods.

In the future, we plan to fully integrate Vocabulary Transfer within Knowledge Distillation during the learning process in order to maximize the information transfer.

[^1]: <https://bigscience.huggingface.co/blog/bloom>
