\begin{thebibliography}{25}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:1877--1901.

\bibitem[{Chalkidis et~al.(2022)Chalkidis, Jana, Hartung, Bommarito,
  Androutsopoulos, Katz, and Aletras}]{chalkidis-etal-2022-lexglue}
Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion
  Androutsopoulos, Daniel Katz, and Nikolaos Aletras. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.297} {{L}ex{GLUE}: A
  benchmark dataset for legal language understanding in {E}nglish}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 4310--4330,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Gordon and Duh(2020)}]{gordon-duh-2020-distill}
Mitchell Gordon and Kevin Duh. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.ngt-1.12} {Distill, adapt,
  distill: Training small, in-domain models for neural machine translation}.
\newblock In \emph{Proceedings of the Fourth Workshop on Neural Generation and
  Translation}, pages 110--118, Online. Association for Computational
  Linguistics.

\bibitem[{Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan}]{gupta2015deep}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
  2015.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{International conference on machine learning}, pages
  1737--1746. PMLR.

\bibitem[{Gurulingappa et~al.(2012)Gurulingappa, Rajput, Roberts, Fluck,
  Hofmann-Apitius, and Toldo}]{GURULINGAPPA2012885}
Harsha Gurulingappa, Abdul~Mateen Rajput, Angus Roberts, Juliane Fluck, Martin
  Hofmann-Apitius, and Luca Toldo. 2012.
\newblock \href {https://doi.org/https://doi.org/10.1016/j.jbi.2012.04.008}
  {Development of a benchmark corpus to support the automatic extraction of
  drug-related adverse effects from medical case reports}.
\newblock \emph{Journal of Biomedical Informatics}, 45(5):885 -- 892.
\newblock Text Mining and Natural Language Processing in Pharmacogenomics.

\bibitem[{He et~al.(2020)He, Liu, Gao, and Chen}]{he2020deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020.
\newblock {DeBERTa}: Decoding-enhanced {BERT} with disentangled attention.
\newblock \emph{arXiv preprint arXiv:2006.03654}.

\bibitem[{Hinton et~al.(2015)Hinton, Vinyals, Dean
  et~al.}]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al. 2015.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2(7).

\bibitem[{Jiao et~al.(2020)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu}]{jiao2020tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu. 2020.
\newblock {TinyBERT}: Distilling {BERT} for natural language understanding.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 4163--4174.

\bibitem[{Kim and Hassan(2020)}]{kim-hassan-2020-fastformers}
Young~Jin Kim and Hany Hassan. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.sustainlp-1.20}
  {{F}ast{F}ormers: Highly efficient transformer models for natural language
  understanding}.
\newblock In \emph{Proceedings of SustaiNLP: Workshop on Simple and Efficient
  Natural Language Processing}, pages 149--158, Online. Association for
  Computational Linguistics.

\bibitem[{Kudo and Richardson(2018)}]{kudo2018sentencepiece}
Taku Kudo and John Richardson. 2018.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Michel et~al.(2019)Michel, Levy, and Neubig}]{sixteenheads}
Paul Michel, Omer Levy, and Graham Neubig. 2019.
\newblock Are sixteen heads really better than one?
\newblock \emph{Advances in neural information processing systems}, 32.

\bibitem[{Polino et~al.(2018)Polino, Pascanu, and Alistarh}]{polino2018model}
Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
\newblock Model compression via distillation and quantization.
\newblock \emph{arXiv preprint arXiv:1802.05668}.

\bibitem[{Samenko et~al.(2021)Samenko, Tikhonov, Kozlovskii, and
  Yamshchikov}]{samenko2021fine}
Igor Samenko, Alexey Tikhonov, Borislav Kozlovskii, and Ivan~P Yamshchikov.
  2021.
\newblock Fine-tuning transformers: Vocabulary transfer.
\newblock \emph{arXiv preprint arXiv:2112.14569}.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf}]{distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
\newblock {DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper
  and lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}.

\bibitem[{Schuster and Nakajima(2012)}]{schuster2012japanese}
Mike Schuster and Kaisuke Nakajima. 2012.
\newblock Japanese and korean voice search.
\newblock In \emph{2012 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 5149--5152. IEEE.

\bibitem[{Sennrich et~al.(2016)Sennrich, Haddow, and
  Birch}]{sennrich2016neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1715--1725.

\bibitem[{Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and
  Keutzer}]{quantization}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W Mahoney, and Kurt Keutzer. 2020.
\newblock {Q-BERT}: Hessian based ultra low precision quantization of {BERT}.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8815--8821.

\bibitem[{Sun et~al.(2020)Sun, Yu, Song, Liu, Yang, and
  Zhou}]{sun2020mobilebert}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
  2020.
\newblock {MobileBERT}: a compact task-agnostic {BERT} for resource-limited
  devices.
\newblock \emph{arXiv preprint arXiv:2004.02984}.

\bibitem[{Tjong Kim~Sang and
  De~Meulder(2003)}]{tjong-kim-sang-de-meulder-2003-introduction}
Erik~F. Tjong Kim~Sang and Fien De~Meulder. 2003.
\newblock \href {https://aclanthology.org/W03-0419} {Introduction to the
  {C}o{NLL}-2003 shared task: Language-independent named entity recognition}.
\newblock In \emph{Proceedings of the Seventh Conference on Natural Language
  Learning at {HLT}-{NAACL} 2003}, pages 142--147.

\bibitem[{Tuggener et~al.(2020)Tuggener, von D{\"a}niken, Peetz, and
  Cieliebak}]{tuggener2020ledgar}
Don Tuggener, Pius von D{\"a}niken, Thomas Peetz, and Mark Cieliebak. 2020.
\newblock Ledgar: A large-scale multi-label corpus for text classification of
  legal provisions in contracts.
\newblock In \emph{Proceedings of the 12th Language Resources and Evaluation
  Conference}, pages 1235--1241.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Wang et~al.(2020)Wang, Wei, Dong, Bao, Yang, and
  Zhou}]{wang2020minilm}
Wenhui Wang, Furu Wei, Li~Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020.
\newblock Minilm: Deep self-attention distillation for task-agnostic
  compression of pre-trained transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:5776--5788.

\bibitem[{Zhu and Gupta(2017)}]{prunenoprune}
Michael Zhu and Suyog Gupta. 2017.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{arXiv preprint arXiv:1710.01878}.

\end{thebibliography}
