- **Vocabulary Transfer (VT) Overview**: A method to adapt language models (LMs) to in-domain tokenizers, reducing model size and inference time while maintaining performance.
  
- **Key Equations**:
  - **Embedding Initialization**:
    - For tokens in both vocabularies:  
      \( E_{in}(t_i) = E_{gen}(t_i) \)  
    - For new tokens:  
      \( E_{in}(t_i) = \frac{1}{|T_{gen}(t_i)|} \sum_{t_j \in T_{gen}(t_i)} E_{gen}(t_j) \)

- **Fast Vocabulary Transfer (FVT)**: A simplified implementation of Vocabulary Initialization with Partial Inheritance (VIPI) that uses a straightforward assignment mechanism for embedding initialization.

- **Performance Metrics**: Evaluated on three KPIs:
  - **Quality**: Measured using F1 score.
  - **Model Size**: Reduction in parameters due to vocabulary compression.
  - **Inference Speedup**: Achieved speedup between 1.07x and 1.40x depending on the task.

- **Combination with Knowledge Distillation (KD)**: 
  - Distillation process involves a triple loss function: distillation loss, masked language modeling (MLM) loss, and cosine embedding loss.
  - VT is applied after KD to ensure alignment of input embedding spaces.

- **Experimental Setup**:
  - Pre-trained model: BERT base with 28,996 wordpieces.
  - Vocabulary sizes tested: 100%, 75%, 50%, and 25%.
  - Fine-tuning for 10 epochs with early stopping, learning rate of \(3 \times 10^{-5}\), and batch size of 64.

- **Datasets Used**:
  - **ADE**: Adverse Drug Events corpus for medical domain.
  - **LEDGAR**: Legal document classification dataset.
  - **CoNLL03**: Named Entity Recognition benchmark from news articles.

- **Results Summary**:
  - In-domain tokenization leads to a significant reduction in the average number of tokens per sequence, with a 32% reduction observed in the medical domain.
  - The learned tokenizers shift the sequence length distribution to the left, indicating fewer splits and more compact representations.

- **Conclusion**: VT effectively reduces model size and inference time with minimal performance loss, demonstrating its potential as a complementary technique in model compression strategies.