- **Vocabulary Transfer (VT) Overview**: A method to adapt language models (LMs) to in-domain tokenizers, reducing model size and inference time while maintaining performance.
  
- **Key Equations**:
  - **Tokenization Function**: \( T : s \rightarrow (t_1, \ldots, t_n), t_i \in V \)
  - **Embedding Initialization**:
    - For common tokens: \( E_{in}(t_i) = E_{gen}(t_i) \) (Equation 1)
    - For new tokens: 
      \[
      E_{in}(t_i) = \frac{1}{|T_{gen}(t_i)|} \sum_{t_j \in T_{gen}(t_i)} E_{gen}(t_j) \quad (Equation 2)
      \]

- **Fast Vocabulary Transfer (FVT)**: A simplified implementation of Vocabulary Initialization with Partial Inheritance (VIPI) that uses a straightforward assignment mechanism for embedding initialization.

- **Model Compression Techniques**: VT can be combined with other methods like Knowledge Distillation (KD), quantization, and pruning to enhance model efficiency.

- **Knowledge Distillation (KD) Process**:
  - Two-step distillation: 
    1. Distill on general domain \( D_{gen} \) using a triple loss function (distillation loss, MLM loss, cosine embedding loss).
    2. Distill on in-domain dataset \( D_{in} \) without adapting the teacher model.

- **Performance Metrics**: Evaluate FVT based on:
  - Quality (F1 score)
  - Model size reduction
  - Inference speedup (achieved speedup between 1.07x and 1.40x).

- **Experimental Setup**:
  - Pre-trained model: BERT base with 28,996 wordpieces.
  - Vocabulary sizes tested: 100%, 75%, 50%, and 25%.
  - Fine-tuning for 10 epochs with early stopping; learning rate set to \( 3 \times 10^{-5} \).

- **Datasets Used**:
  - **ADE**: Adverse Drug Events corpus for medical domain.
  - **LEDGAR**: Legal document classification dataset.
  - **CoNLL03**: Named Entity Recognition benchmark from news stories.

- **Results Summary**:
  - In-domain tokenization leads to a significant reduction in average tokens per sequence (up to 32% in medical domain).
  - Sequence length distribution shifts left with learned tokenizers, indicating improved efficiency.

- **Conclusion**: VT is effective for model compression, yielding significant reductions in model size and inference time with minimal performance loss, especially when combined with KD.