\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\section{Limitations}
\label{sec:limitations}
Our DiT-based method for RGBA generation incurs quadratic computational costs due to sequence expansion. However, our method achieves an optimal balance between generation and alignment when trained with a limited dataset. Numerous studies~\cite{wang2020linformer, ding2023longnet, zhu2021long} have addressed the computational overhead of long sequences, with many optimizations reducing complexity to a linear scale. To enhance the efficiency of our method, we plan to incorporate these optimizations in future work. Additionally, our performance is influenced by the generative priors provided by the chosen T2V model, which affects the quality and consistency of our outputs.

\section{Comparisons with Video Matting}
\label{sec:comparisons}
We compare our method with video matting methods BiMatting~\cite{qin2023bimatting} and Robust Video Matting (RVM)~\cite{lin2022robust}, as well as the image matting method Matte-Anything~\cite{yao2024matte}. From the results, it is evident that most methods, trained on the VideoMatte240k~\cite{lin2021real} dataset, struggle to produce valid outputs for non-human objects, often resulting in empty results. Even image matting methods trained on large-scale datasets fail to handle certain visual effects correctly. Results are shown in the attached HTML source files.

\section{Data Preprocessing}
\label{sec:data-preprocessing}
\noindent\textbf{Color Decontamination}. In our method, we preprocess the training data by applying a color decontamination step to enhance the quality of the RGBA video generation. Color contamination typically occurs when there is an undesired blending of foreground and background colors, especially along the edges of an object, due to imperfect alpha masks. This blending causes color bleeding, where the foreground and background colors mix, resulting in lower quality RGBA frames with inaccurate color representation. To address this issue, we refine the alpha mask using parameters such as gain ($\gamma=1.1$) and choke ($\chi=0.5$) to adjust the sharpness and influence of the mask edges. The decontaminated RGB values are then computed as follows:

\[
\text{RGB}_{\text{decon}} = \text{RGB} \times (1 - \text{mask}_{\text{refined}}) + \text{mask}_{\text{refined}} \times \text{Background}
\]

This equation ensures that unwanted color contamination is minimized, providing a more precise distinction between foreground and background regions. By performing this preprocessing step, we generate high-quality training data that significantly improves the performance of our RGBA video generation model.

\noindent\textbf{Background Blurring}. Unlike typical training strategies in video matting methods, where objects are composited with complex backgrounds to increase the difficulty of the task, our goal is to support joint generation of alpha and RGB channels while ensuring alignment between them. Instead of emphasizing complex matting, we focus on generating consistent and high-quality output by compositing objects with simple, static backgrounds that match the black areas in the alpha channel. Specifically, we apply a large Gaussian blur kernel of size 201 to the first frame to create a blurred background and blend each subsequent frame with this static background. This approach helps simplify the training conditions, allowing the model to better align the RGB and alpha components while maintaining high-quality output.

\section{Optical Flow Difference}
\label{sec:optical-flow}
To evaluate the alignment between the RGB and alpha channels in generated videos, we introduce a metric based on optical flow difference. Optical flow measures the apparent motion of objects between consecutive frames, and comparing the optical flow fields of RGB and alpha frames provides insight into the consistency of motion across these modalities. Specifically, we use the Farneback method (\texttt{cv::calcOpticalFlowFarneback}) to compute the optical flow for both RGB and alpha frames, and then calculate the average Euclidean distance between their flow vectors as a measure of misalignment. This approach quantifies the degree to which the RGB and alpha channels align in terms of motion.

\noindent\textbf{Pseudo Code Overview}:
\begin{enumerate}
    \item \textbf{Load consecutive RGB and alpha frames} from the input video.
    \item \textbf{Convert the frames to grayscale} for optical flow computation, as optical flow is typically calculated on intensity values.
    \item \textbf{Compute optical flow using the Farneback method} (\texttt{cv::calcOpticalFlowFarneback}) for both the RGB and alpha frames.
    \item \textbf{Calculate the Euclidean distance} between the RGB and alpha flow vectors for each pixel.
    \item \textbf{Average the differences} across all pixels and frames to obtain the final optical flow difference.
\end{enumerate}

The average optical flow difference provides a quantitative metric for evaluating the alignment between RGB and alpha channels, helping to ensure that both modalities exhibit consistent motion.

\section{Video Results}
\label{sec:video-results}
For all video results shown in the main paper, please see the attached HTML source files.

\section{Additional Visual Results}
\label{sec:additional-visual-results}
In addition to the video results in the main paper, we provide more generated results in the supplementary files, including various objects and visual effects. Please find the corresponding results in the supplementary files.

