@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

%--------------------------------------------------------------------------
% intro - background - rgb video generation
@article{chen2023pixart,
  title={Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis},
  author={Chen, Junsong and Yu, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yue and Wang, Zhongdao and Kwok, James and Luo, Ping and Lu, Huchuan and others},
  journal={arXiv preprint arXiv:2310.00426},
  year={2023}
}


@article{he2022latent,
  title={Latent video diffusion models for high-fidelity video generation with arbitrary lengths},
  author={He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  journal={arXiv preprint arXiv:2211.13221},
  year={2022}
}

@article{guo2023animatediff,
  title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Wang, Yaohui and Qiao, Yu and Lin, Dahua and Dai, Bo},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023}
}

@article{wang2023modelscope,
  title={Modelscope text-to-video technical report},
  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},
  journal={arXiv preprint arXiv:2308.06571},
  year={2023}
}

@article{wang2024videocomposer,
  title={Videocomposer: Compositional video synthesis with motion controllability},
  author={Wang, Xiang and Yuan, Hangjie and Zhang, Shiwei and Chen, Dayou and Wang, Jiuniu and Zhang, Yingya and Shen, Yujun and Zhao, Deli and Zhou, Jingren},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{chen2023videocrafter1,
  title={Videocrafter1: Open diffusion models for high-quality video generation},
  author={Chen, Haoxin and Xia, Menghan and He, Yingqing and Zhang, Yong and Cun, Xiaodong and Yang, Shaoshu and Xing, Jinbo and Liu, Yaofang and Chen, Qifeng and Wang, Xintao and others},
  journal={arXiv preprint arXiv:2310.19512},
  year={2023}
}

@misc{cerspense2023zeroscope,
  author = {cerspense},
  title = {zeroscope\_v2},
  year = {2023},
  howpublished = {\url{https://huggingface.co/cerspense/zeroscope_v2_576w}},
  note = {Accessed: 2023-02-03}
}

@article{zhang2023show,
  title={Show-1: Marrying pixel and latent diffusion models for text-to-video generation},
  author={Zhang, David Junhao and Wu, Jay Zhangjie and Liu, Jia-Wei and Zhao, Rui and Ran, Lingmin and Gu, Yuchao and Gao, Difei and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2309.15818},
  year={2023}
}

@article{zhang2024moonshot,
  title={Moonshot: Towards controllable video generation and editing with multimodal conditions},
  author={Zhang, David Junhao and Li, Dongxu and Le, Hung and Shou, Mike Zheng and Xiong, Caiming and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2401.01827},
  year={2024}
}

@article{chen2024videocrafter2,
  title={Videocrafter2: Overcoming data limitations for high-quality video diffusion models},
  author={Chen, Haoxin and Zhang, Yong and Cun, Xiaodong and Xia, Menghan and Wang, Xintao and Weng, Chao and Shan, Ying},
  journal={arXiv preprint arXiv:2401.09047},
  year={2024}
}

@article{ma2024latte,
  title={Latte: Latent diffusion transformer for video generation},
  author={Ma, Xin and Wang, Yaohui and Jia, Gengyun and Chen, Xinyuan and Liu, Ziwei and Li, Yuan-Fang and Chen, Cunjian and Qiao, Yu},
  journal={arXiv preprint arXiv:2401.03048},
  year={2024}
}

@article{sora2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}

@article{yang2024cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@software{opensoraplan,
  author       = {PKU-Yuan Lab and Tuzhan AI etc.},
  title        = {Open-Sora-Plan},
  month        = apr,
  year         = 2024,
  publisher    = {GitHub},
  doi          = {10.5281/zenodo.10948109},
  url          = {https://doi.org/10.5281/zenodo.10948109}
}

@software{opensora,
  author = {Zangwei Zheng and Xiangyu Peng and Tianji Yang and Chenhui Shen and Shenggui Li and Hongxin Liu and Yukun Zhou and Tianyi Li and Yang You},
  title = {Open-Sora: Democratizing Efficient Video Production for All},
  month = {March},
  year = {2024},
  url = {https://github.com/hpcaitech/Open-Sora}
}

@misc{genmo2024mochi,
      title={Mochi},
      author={Genmo Team},
      year={2024}
}



%--------------------------------------------------------------------------
% intro - background - rgb video generation - applications
% video editing
@article{geyer2023tokenflow,
  title={Tokenflow: Consistent diffusion features for consistent video editing},
  author={Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
  journal={arXiv preprint arXiv:2307.10373},
  year={2023}
}

@inproceedings{yang2023rerender,
  title={Rerender a video: Zero-shot text-guided video-to-video translation},
  author={Yang, Shuai and Zhou, Yifan and Liu, Ziwei and Loy, Chen Change},
  booktitle={SIGGRAPH Asia 2023 Conference Papers},
  pages={1--11},
  year={2023}
}

@inproceedings{qi2023fatezero,
  title={Fatezero: Fusing attentions for zero-shot text-based video editing},
  author={Qi, Chenyang and Cun, Xiaodong and Zhang, Yong and Lei, Chenyang and Wang, Xintao and Shan, Ying and Chen, Qifeng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15932--15942},
  year={2023}
}
@inproceedings{liu2024video,
  title={Video-p2p: Video editing with cross-attention control},
  author={Liu, Shaoteng and Zhang, Yuechen and Li, Wenbo and Lin, Zhe and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8599--8608},
  year={2024}
}

@inproceedings{wu2023tune,
  title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7623--7633},
  year={2023}
}
@article{chen2023control,
  title={Control-a-video: Controllable text-to-video generation with diffusion models},
  author={Chen, Weifeng and Ji, Yatai and Wu, Jie and Wu, Hefeng and Xie, Pan and Li, Jiashi and Xia, Xin and Xiao, Xuefeng and Lin, Liang},
  journal={arXiv preprint arXiv:2305.13840},
  year={2023}
}

% image animation
@article{blattmann2023stable,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}

@article{niu2024mofa,
  title={MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model},
  author={Niu, Muyao and Cun, Xiaodong and Wang, Xintao and Zhang, Yong and Shan, Ying and Zheng, Yinqiang},
  journal={arXiv preprint arXiv:2405.20222},
  year={2024}
}

@article{guo2024liveportrait,
  title={LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control},
  author={Guo, Jianzhu and Zhang, Dingyun and Liu, Xiaoqiang and Zhong, Zhizhou and Zhang, Yuan and Wan, Pengfei and Zhang, Di},
  journal={arXiv preprint arXiv:2407.03168},
  year={2024}
}

@article{guo2023i2v,
  title={I2v-adapter: A general image-to-video adapter for video diffusion models},
  author={Guo, Xun and Zheng, Mingwu and Hou, Liang and Gao, Yuan and Deng, Yufan and Ma, Chongyang and Hu, Weiming and Zha, Zhengjun and Huang, Haibin and Wan, Pengfei and others},
  journal={arXiv preprint arXiv:2312.16693},
  year={2023}
}


% motion customization

@article{he2024cameractrl,
  title={Cameractrl: Enabling camera control for text-to-video generation},
  author={He, Hao and Xu, Yinghao and Guo, Yuwei and Wetzstein, Gordon and Dai, Bo and Li, Hongsheng and Yang, Ceyuan},
  journal={arXiv preprint arXiv:2404.02101},
  year={2024}
}

@inproceedings{wang2024motionctrl,
  title={Motionctrl: A unified and flexible motion controller for video generation},
  author={Wang, Zhouxia and Yuan, Ziyang and Wang, Xintao and Li, Yaowei and Chen, Tianshui and Xia, Menghan and Luo, Ping and Shan, Ying},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--11},
  year={2024}
}

@article{ling2024motionclone,
  title={MotionClone: Training-Free Motion Cloning for Controllable Video Generation},
  author={Ling, Pengyang and Bu, Jiazi and Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Wu, Tong and Chen, Huaian and Wang, Jiaqi and Jin, Yi},
  journal={arXiv preprint arXiv:2406.05338},
  year={2024}
}

@article{jeong2024dreammotion,
  title={DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing},
  author={Jeong, Hyeonho and Chang, Jinho and Park, Geon Yeong and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2403.12002},
  year={2024}
}

@article{ma2023trailblazer,
  title={TrailBlazer: Trajectory Control for Diffusion-Based Video Generation},
  author={Ma, Wan-Duo Kurt and Lewis, John P and Kleijn, W Bastiaan},
  journal={arXiv preprint arXiv:2401.00896},
  year={2023}
}

@article{yin2023dragnuwa,
  title={Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory},
  author={Yin, Shengming and Wu, Chenfei and Liang, Jian and Shi, Jie and Li, Houqiang and Ming, Gong and Duan, Nan},
  journal={arXiv preprint arXiv:2308.08089},
  year={2023}
}

@article{wang2024motion,
  title={Motion Inversion for Video Customization},
  author={Wang, Luozhou and Shen, Guibao and Liang, Yixun and Tao, Xin and Wan, Pengfei and Zhang, Di and Li, Yijun and Chen, Yingcong},
  journal={arXiv preprint arXiv:2403.20193},
  year={2024}
}

%--------------------------------------------------------------------------
% intro - research gap - rgba data
@article{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25278--25294},
  year={2022}
}

@inproceedings{lin2021real,
  title={Real-time high-resolution background matting},
  author={Lin, Shanchuan and Ryabtsev, Andrey and Sengupta, Soumyadip and Curless, Brian L and Seitz, Steven M and Kemelmacher-Shlizerman, Ira},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8762--8771},
  year={2021}
}

@inproceedings{tudosiu2024mulan,
  title={MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation},
  author={Tudosiu, Petru-Daniel and Yang, Yongxin and Zhang, Shifeng and Chen, Fei and McDonagh, Steven and Lampouras, Gerasimos and Iacobacci, Ignacio and Parisot, Sarah},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22413--22422},
  year={2024}
}


%--------------------------------------------------------------------------
% intro - research gap - prediction wo prior
@article{chen2022pp,
  title={PP-matting: high-accuracy natural image matting},
  author={Chen, Guowei and Liu, Yi and Wang, Jian and Peng, Juncai and Hao, Yuying and Chu, Lutao and Tang, Shiyu and Wu, Zewu and Chen, Zeyu and Yu, Zhiliang and others},
  journal={arXiv preprint arXiv:2204.09433},
  year={2022}
}

@inproceedings{li2024matting,
  title={Matting anything},
  author={Li, Jiachen and Jain, Jitesh and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1775--1785},
  year={2024}
}

@article{yao2024vitmatte,
  title={Vitmatte: Boosting image matting with pre-trained plain vision transformers},
  author={Yao, Jingfeng and Wang, Xinggang and Yang, Shusheng and Wang, Baoyuan},
  journal={Information Fusion},
  volume={103},
  pages={102091},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{wang2024matting,
  title={Matting by Generation},
  author={Wang, Zhixiang and Li, Baiang and Wang, Jian and Liu, Yu-Lun and Gu, Jinwei and Chuang, Yung-Yu and Satoh, Shin'Ichi},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--11},
  year={2024}
}

@inproceedings{lin2023omnimatterf,
  title={OmnimatteRF: Robust Omnimatte with 3D Background Modeling},
  author={Lin, Geng and Gao, Chen and Huang, Jia-Bin and Kim, Changil and Wang, Yipeng and Zwicker, Matthias and Saraf, Ayush},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={23471--23480},
  year={2023}
}

@inproceedings{lin2022robust,
  title={Robust high-resolution video matting with temporal guidance},
  author={Lin, Shanchuan and Yang, Linjie and Saleemi, Imran and Sengupta, Soumyadip},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={238--247},
  year={2022}
}

@article{yao2024matte,
  title={Matte anything: Interactive natural image matting with segment anything model},
  author={Yao, Jingfeng and Wang, Xinggang and Ye, Lang and Liu, Wenyu},
  journal={Image and Vision Computing},
  volume={147},
  pages={105067},
  year={2024},
  publisher={Elsevier}
}

@article{qin2023bimatting,
  title={BiMatting: Efficient video matting via binarization},
  author={Qin, Haotong and Ke, Lei and Ma, Xudong and Danelljan, Martin and Tai, Yu-Wing and Tang, Chi-Keung and Liu, Xianglong and Yu, Fisher},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={43307--43321},
  year={2023}
}
%--------------------------------------------------------------------------
% intro - research gap - prediction with prior
@inproceedings{ke2024repurposing,
  title={Repurposing diffusion-based image generators for monocular depth estimation},
  author={Ke, Bingxin and Obukhov, Anton and Huang, Shengyu and Metzger, Nando and Daudt, Rodrigo Caye and Schindler, Konrad},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9492--9502},
  year={2024}
}

@article{he2024lotus,
  title={Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction},
  author={He, Jing and Li, Haodong and Yin, Wei and Liang, Yixun and Li, Leheng and Zhou, Kaiqiang and Liu, Hongbo and Liu, Bingbing and Chen, Ying-Cong},
  journal={arXiv preprint arXiv:2409.18124},
  year={2024}
}

@inproceedings{yang2024depth,
  title={Depth anything: Unleashing the power of large-scale unlabeled data},
  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10371--10381},
  year={2024}
}



%--------------------------------------------------------------------------
% intro - research gap - generation with prior

@inproceedings{long2024wonder3d,
  title={Wonder3d: Single image to 3d using cross-domain diffusion},
  author={Long, Xiaoxiao and Guo, Yuan-Chen and Lin, Cheng and Liu, Yuan and Dou, Zhiyang and Liu, Lingjie and Ma, Yuexin and Zhang, Song-Hai and Habermann, Marc and Theobalt, Christian and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9970--9980},
  year={2024}
}

@inproceedings{zeng2024rgb,
  title={RGB\(\leftrightarrow\)X: Image decomposition and synthesis using material-and lighting-aware diffusion models},
  author={Zeng, Zheng and Deschaintre, Valentin and Georgiev, Iliyan and Hold-Geoffroy, Yannick and Hu, Yiwei and Luan, Fujun and Yan, Ling-Qi and Ha{\v{s}}an, Milo{\v{s}}},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--11},
  year={2024}
}

@article{zhang2024transparent,
  title={Transparent image layer diffusion using latent transparency},
  author={Zhang, Lvmin and Agrawala, Maneesh},
  journal={arXiv preprint arXiv:2402.17113},
  year={2024}
}

@inproceedings{bao2023one,
  title={One transformer fits all distributions in multi-modal diffusion at scale},
  author={Bao, Fan and Nie, Shen and Xue, Kaiwen and Li, Chongxuan and Pu, Shi and Wang, Yaole and Yue, Gang and Cao, Yue and Su, Hang and Zhu, Jun},
  booktitle={International Conference on Machine Learning},
  pages={1692--1717},
  year={2023},
  organization={PMLR}
}

@inproceedings{luo2024intrinsicdiffusion,
  title={Intrinsicdiffusion: Joint intrinsic layers from latent diffusion models},
  author={Luo, Jundan and Ceylan, Duygu and Yoon, Jae Shin and Zhao, Nanxuan and Philip, Julien and Fr{\"u}hst{\"u}ck, Anna and Li, Wenbin and Richardt, Christian and Wang, Tuanfeng},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--11},
  year={2024}
}
@misc{he2024lucidfusion,
      title={LucidFusion: Generating 3D Gaussians with Arbitrary Unposed Images}, 
      author={Hao He and Yixun Liang and Luozhou Wang and Yuanhao Cai and Xinli Xu and Hao-Xiang Guo and Xiang Wen and Yingcong Chen},
      year={2024},
      eprint={2410.15636},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.15636}, 
}

@misc{yang2023defect,
      title={Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics}, 
      author={Shuai Yang and Zhifei Chen and Pengguang Chen and Xi Fang and Shu Liu and Yingcong Chen},
      year={2023},
      eprint={2310.17316},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

%--------------------------------------------------------------------------
% intro - our solution - framework
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}






%--------------------------------------------------------------------------
% related work - rgb image generation

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{betker2023dalle3,
  title={Improving image generation with better captions},
  author={Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and others},
  journal={Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf},
  volume={2},
  number={3},
  pages={8},
  year={2023}
}

@article{saharia2022photorealistic,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36479--36494},
  year={2022}
}

@article{ramesh2022dalle2,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@article{nichol2021glide,
  title={Glide: Towards photorealistic image generation and editing with text-guided diffusion models},
  author={Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  journal={arXiv preprint arXiv:2112.10741},
  year={2021}
}

%--------------------------------------------------------------------------
% method
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{liu2022flow,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}


%--------------------------------------------------------------------------
% experiments - video matte 240k

@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2408.00714},
  url={https://arxiv.org/abs/2408.00714},
  year={2024}
}

@article{chen2023sharegpt4v,
  title={ShareGPT4V: Improving Large Multi-Modal Models with Better Captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv preprint arXiv:2311.12793},
  year={2023}
}

@article{unterthiner2019fvd,
  title={FVD: A new metric for video generation},
  author={Unterthiner, Thomas and van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Rapha{\"e}l and Michalski, Marcin and Gelly, Sylvain},
  year={2019}
}

@article{horn1981determining,
  title={Determining optical flow},
  author={Horn, Berthold KP and Schunck, Brian G},
  journal={Artificial intelligence},
  volume={17},
  number={1-3},
  pages={185--203},
  year={1981},
  publisher={Elsevier}
}

@inproceedings{farneback2003two,
  title={Two-frame motion estimation based on polynomial expansion},
  author={Farneb{\"a}ck, Gunnar},
  booktitle={Proceedings of the Scandinavian Conference on Image Analysis (SCIA)},
  pages={363--370},
  year={2003},
  organization={Springer}
}

%--------------------------------------------------------------------------
% discussions 
@article{vainer2024collaborative,
  title={Collaborative Control for Geometry-Conditioned PBR Image Generation},
  author={Vainer, Shimon and Boss, Mark and Parger, Mathias and Kutsy, Konstantin and De Nigris, Dante and Rowles, Ciara and Perony, Nicolas and Donn{\'e}, Simon},
  journal={arXiv preprint arXiv:2402.05919},
  year={2024}
}


%--------------------------------------------------------------------------
% supplementary 
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{ding2023longnet,
  title={LongNet: Scaling Transformers to 1,000,000,000 Tokens 2023},
  author={Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu},
  journal={arXiv preprint arXiv:2307.02486},
  year={2023}
}

@article{zhu2021long,
  title={Long-short transformer: Efficient transformers for language and vision},
  author={Zhu, Chen and Ping, Wei and Xiao, Chaowei and Shoeybi, Mohammad and Goldstein, Tom and Anandkumar, Anima and Catanzaro, Bryan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={17723--17736},
  year={2021}
}