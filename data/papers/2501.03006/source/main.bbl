\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bao et~al.(2023)Bao, Nie, Xue, Li, Pu, Wang, Yue, Cao, Su, and Zhu]{bao2023one}
Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu.
\newblock One transformer fits all distributions in multi-modal diffusion at scale.
\newblock In \emph{International Conference on Machine Learning}, pages 1692--1717. PMLR, 2023.

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, et~al.]{blattmann2023stable}
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et~al.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Brooks et~al.(2024)Brooks, Peebles, Holmes, DePue, Guo, Jing, Schnurr, Taylor, Luhman, Luhman, Ng, Wang, and Ramesh]{sora2024}
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
\newblock Video generation models as world simulators.
\newblock 2024.

\bibitem[cerspense(2023)]{cerspense2023zeroscope}
cerspense.
\newblock zeroscope\_v2.
\newblock \url{https://huggingface.co/cerspense/zeroscope_v2_576w}, 2023.
\newblock Accessed: 2023-02-03.

\bibitem[Chen et~al.(2022)Chen, Liu, Wang, Peng, Hao, Chu, Tang, Wu, Chen, Yu, et~al.]{chen2022pp}
Guowei Chen, Yi Liu, Jian Wang, Juncai Peng, Yuying Hao, Lutao Chu, Shiyu Tang, Zewu Wu, Zeyu Chen, Zhiliang Yu, et~al.
\newblock Pp-matting: high-accuracy natural image matting.
\newblock \emph{arXiv preprint arXiv:2204.09433}, 2022.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Xia, He, Zhang, Cun, Yang, Xing, Liu, Chen, Wang, et~al.]{chen2023videocrafter1}
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et~al.
\newblock Videocrafter1: Open diffusion models for high-quality video generation.
\newblock \emph{arXiv preprint arXiv:2310.19512}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2024)Chen, Zhang, Cun, Xia, Wang, Weng, and Shan]{chen2024videocrafter2}
Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.
\newblock Videocrafter2: Overcoming data limitations for high-quality video diffusion models.
\newblock \emph{arXiv preprint arXiv:2401.09047}, 2024.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Yu, Ge, Yao, Xie, Wu, Wang, Kwok, Luo, Lu, et~al.]{chen2023pixart}
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et~al.
\newblock Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
\newblock \emph{arXiv preprint arXiv:2310.00426}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Li, Dong, Zhang, He, Wang, Zhao, and Lin]{chen2023sharegpt4v}
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock \emph{arXiv preprint arXiv:2311.12793}, 2023{\natexlab{c}}.

\bibitem[Chen et~al.(2023{\natexlab{d}})Chen, Ji, Wu, Wu, Xie, Li, Xia, Xiao, and Lin]{chen2023control}
Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin.
\newblock Control-a-video: Controllable text-to-video generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2305.13840}, 2023{\natexlab{d}}.

\bibitem[Ding et~al.(2023)Ding, Ma, Dong, Zhang, Huang, Wang, Zheng, and Wei]{ding2023longnet}
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei.
\newblock Longnet: Scaling transformers to 1,000,000,000 tokens 2023.
\newblock \emph{arXiv preprint arXiv:2307.02486}, 2023.

\bibitem[Farneb{\"a}ck(2003)]{farneback2003two}
Gunnar Farneb{\"a}ck.
\newblock Two-frame motion estimation based on polynomial expansion.
\newblock In \emph{Proceedings of the Scandinavian Conference on Image Analysis (SCIA)}, pages 363--370. Springer, 2003.

\bibitem[Geyer et~al.(2023)Geyer, Bar-Tal, Bagon, and Dekel]{geyer2023tokenflow}
Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.
\newblock Tokenflow: Consistent diffusion features for consistent video editing.
\newblock \emph{arXiv preprint arXiv:2307.10373}, 2023.

\bibitem[Guo et~al.(2024)Guo, Zhang, Liu, Zhong, Zhang, Wan, and Zhang]{guo2024liveportrait}
Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang.
\newblock Liveportrait: Efficient portrait animation with stitching and retargeting control.
\newblock \emph{arXiv preprint arXiv:2407.03168}, 2024.

\bibitem[Guo et~al.(2023{\natexlab{a}})Guo, Zheng, Hou, Gao, Deng, Ma, Hu, Zha, Huang, Wan, et~al.]{guo2023i2v}
Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin Huang, Pengfei Wan, et~al.
\newblock I2v-adapter: A general image-to-video adapter for video diffusion models.
\newblock \emph{arXiv preprint arXiv:2312.16693}, 2023{\natexlab{a}}.

\bibitem[Guo et~al.(2023{\natexlab{b}})Guo, Yang, Rao, Wang, Qiao, Lin, and Dai]{guo2023animatediff}
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai.
\newblock Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.
\newblock \emph{arXiv preprint arXiv:2307.04725}, 2023{\natexlab{b}}.

\bibitem[He et~al.(2024{\natexlab{a}})He, Liang, Wang, Cai, Xu, Guo, Wen, and Chen]{he2024lucidfusion}
Hao He, Yixun Liang, Luozhou Wang, Yuanhao Cai, Xinli Xu, Hao-Xiang Guo, Xiang Wen, and Yingcong Chen.
\newblock Lucidfusion: Generating 3d gaussians with arbitrary unposed images, 2024{\natexlab{a}}.

\bibitem[He et~al.(2024{\natexlab{b}})He, Xu, Guo, Wetzstein, Dai, Li, and Yang]{he2024cameractrl}
Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang.
\newblock Cameractrl: Enabling camera control for text-to-video generation.
\newblock \emph{arXiv preprint arXiv:2404.02101}, 2024{\natexlab{b}}.

\bibitem[He et~al.(2024{\natexlab{c}})He, Li, Yin, Liang, Li, Zhou, Liu, Liu, and Chen]{he2024lotus}
Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Liu, Bingbing Liu, and Ying-Cong Chen.
\newblock Lotus: Diffusion-based visual foundation model for high-quality dense prediction.
\newblock \emph{arXiv preprint arXiv:2409.18124}, 2024{\natexlab{c}}.

\bibitem[He et~al.(2022)He, Yang, Zhang, Shan, and Chen]{he2022latent}
Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen.
\newblock Latent video diffusion models for high-fidelity video generation with arbitrary lengths.
\newblock \emph{arXiv preprint arXiv:2211.13221}, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Horn and Schunck(1981)]{horn1981determining}
Berthold~KP Horn and Brian~G Schunck.
\newblock Determining optical flow.
\newblock \emph{Artificial intelligence}, 17\penalty0 (1-3):\penalty0 185--203, 1981.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jeong et~al.(2024)Jeong, Chang, Park, and Ye]{jeong2024dreammotion}
Hyeonho Jeong, Jinho Chang, Geon~Yeong Park, and Jong~Chul Ye.
\newblock Dreammotion: Space-time self-similarity score distillation for zero-shot video editing.
\newblock \emph{arXiv preprint arXiv:2403.12002}, 2024.

\bibitem[Ke et~al.(2024)Ke, Obukhov, Huang, Metzger, Daudt, and Schindler]{ke2024repurposing}
Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo~Caye Daudt, and Konrad Schindler.
\newblock Repurposing diffusion-based image generators for monocular depth estimation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9492--9502, 2024.

\bibitem[Lab and etc.(2024)]{opensoraplan}
PKU-Yuan Lab and Tuzhan~AI etc.
\newblock Open-sora-plan, 2024.

\bibitem[Li et~al.(2024)Li, Jain, and Shi]{li2024matting}
Jiachen Li, Jitesh Jain, and Humphrey Shi.
\newblock Matting anything.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1775--1785, 2024.

\bibitem[Lin et~al.(2023)Lin, Gao, Huang, Kim, Wang, Zwicker, and Saraf]{lin2023omnimatterf}
Geng Lin, Chen Gao, Jia-Bin Huang, Changil Kim, Yipeng Wang, Matthias Zwicker, and Ayush Saraf.
\newblock Omnimatterf: Robust omnimatte with 3d background modeling.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 23471--23480, 2023.

\bibitem[Lin et~al.(2021)Lin, Ryabtsev, Sengupta, Curless, Seitz, and Kemelmacher-Shlizerman]{lin2021real}
Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian~L Curless, Steven~M Seitz, and Ira Kemelmacher-Shlizerman.
\newblock Real-time high-resolution background matting.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 8762--8771, 2021.

\bibitem[Lin et~al.(2022)Lin, Yang, Saleemi, and Sengupta]{lin2022robust}
Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip Sengupta.
\newblock Robust high-resolution video matting with temporal guidance.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 238--247, 2022.

\bibitem[Ling et~al.(2024)Ling, Bu, Zhang, Dong, Zang, Wu, Chen, Wang, and Jin]{ling2024motionclone}
Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin.
\newblock Motionclone: Training-free motion cloning for controllable video generation.
\newblock \emph{arXiv preprint arXiv:2406.05338}, 2024.

\bibitem[Liu et~al.(2024)Liu, Zhang, Li, Lin, and Jia]{liu2024video}
Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia.
\newblock Video-p2p: Video editing with cross-attention control.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 8599--8608, 2024.

\bibitem[Liu et~al.(2022)Liu, Gong, and Liu]{liu2022flow}
Xingchao Liu, Chengyue Gong, and Qiang Liu.
\newblock Flow straight and fast: Learning to generate and transfer data with rectified flow.
\newblock \emph{arXiv preprint arXiv:2209.03003}, 2022.

\bibitem[Long et~al.(2024)Long, Guo, Lin, Liu, Dou, Liu, Ma, Zhang, Habermann, Theobalt, et~al.]{long2024wonder3d}
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et~al.
\newblock Wonder3d: Single image to 3d using cross-domain diffusion.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9970--9980, 2024.

\bibitem[Luo et~al.(2024)Luo, Ceylan, Yoon, Zhao, Philip, Fr{\"u}hst{\"u}ck, Li, Richardt, and Wang]{luo2024intrinsicdiffusion}
Jundan Luo, Duygu Ceylan, Jae~Shin Yoon, Nanxuan Zhao, Julien Philip, Anna Fr{\"u}hst{\"u}ck, Wenbin Li, Christian Richardt, and Tuanfeng Wang.
\newblock Intrinsicdiffusion: Joint intrinsic layers from latent diffusion models.
\newblock In \emph{ACM SIGGRAPH 2024 Conference Papers}, pages 1--11, 2024.

\bibitem[Ma et~al.(2023)Ma, Lewis, and Kleijn]{ma2023trailblazer}
Wan-Duo~Kurt Ma, John~P Lewis, and W~Bastiaan Kleijn.
\newblock Trailblazer: Trajectory control for diffusion-based video generation.
\newblock \emph{arXiv preprint arXiv:2401.00896}, 2023.

\bibitem[Ma et~al.(2024)Ma, Wang, Jia, Chen, Liu, Li, Chen, and Qiao]{ma2024latte}
Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao.
\newblock Latte: Latent diffusion transformer for video generation.
\newblock \emph{arXiv preprint arXiv:2401.03048}, 2024.

\bibitem[Niu et~al.(2024)Niu, Cun, Wang, Zhang, Shan, and Zheng]{niu2024mofa}
Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng.
\newblock Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model.
\newblock \emph{arXiv preprint arXiv:2405.20222}, 2024.

\bibitem[Qi et~al.(2023)Qi, Cun, Zhang, Lei, Wang, Shan, and Chen]{qi2023fatezero}
Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen.
\newblock Fatezero: Fusing attentions for zero-shot text-based video editing.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15932--15942, 2023.

\bibitem[Qin et~al.(2023)Qin, Ke, Ma, Danelljan, Tai, Tang, Liu, and Yu]{qin2023bimatting}
Haotong Qin, Lei Ke, Xudong Ma, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Xianglong Liu, and Fisher Yu.
\newblock Bimatting: Efficient video matting via binarization.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 43307--43321, 2023.

\bibitem[Ravi et~al.(2024)Ravi, Gabeur, Hu, Hu, Ryali, Ma, Khedr, R{\"a}dle, Rolland, Gustafson, Mintun, Pan, Alwala, Carion, Wu, Girshick, Doll{\'a}r, and Feichtenhofer]{ravi2024sam2}
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R{\"a}dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan~Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Doll{\'a}r, and Christoph Feichtenhofer.
\newblock Sam 2: Segment anything in images and videos.
\newblock \emph{arXiv preprint arXiv:2408.00714}, 2024.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Team(2024)]{genmo2024mochi}
Genmo Team.
\newblock Mochi, 2024.

\bibitem[Unterthiner et~al.(2019)Unterthiner, van Steenkiste, Kurach, Marinier, Michalski, and Gelly]{unterthiner2019fvd}
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Rapha{\"e}l Marinier, Marcin Michalski, and Sylvain Gelly.
\newblock Fvd: A new metric for video generation.
\newblock 2019.

\bibitem[Vainer et~al.(2024)Vainer, Boss, Parger, Kutsy, De~Nigris, Rowles, Perony, and Donn{\'e}]{vainer2024collaborative}
Shimon Vainer, Mark Boss, Mathias Parger, Konstantin Kutsy, Dante De~Nigris, Ciara Rowles, Nicolas Perony, and Simon Donn{\'e}.
\newblock Collaborative control for geometry-conditioned pbr image generation.
\newblock \emph{arXiv preprint arXiv:2402.05919}, 2024.

\bibitem[Wang et~al.(2023)Wang, Yuan, Chen, Zhang, Wang, and Zhang]{wang2023modelscope}
Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.
\newblock Modelscope text-to-video technical report.
\newblock \emph{arXiv preprint arXiv:2308.06571}, 2023.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Shen, Liang, Tao, Wan, Zhang, Li, and Chen]{wang2024motion}
Luozhou Wang, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, and Yingcong Chen.
\newblock Motion inversion for video customization.
\newblock \emph{arXiv preprint arXiv:2403.20193}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Yuan, Zhang, Chen, Wang, Zhang, Shen, Zhao, and Zhou]{wang2024videocomposer}
Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou.
\newblock Videocomposer: Compositional video synthesis with motion controllability.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Li, Wang, Liu, Gu, Chuang, and Satoh]{wang2024matting}
Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, and Shin'Ichi Satoh.
\newblock Matting by generation.
\newblock In \emph{ACM SIGGRAPH 2024 Conference Papers}, pages 1--11, 2024{\natexlab{c}}.

\bibitem[Wang et~al.(2024{\natexlab{d}})Wang, Yuan, Wang, Li, Chen, Xia, Luo, and Shan]{wang2024motionctrl}
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan.
\newblock Motionctrl: A unified and flexible motion controller for video generation.
\newblock In \emph{ACM SIGGRAPH 2024 Conference Papers}, pages 1--11, 2024{\natexlab{d}}.

\bibitem[Wu et~al.(2023)Wu, Ge, Wang, Lei, Gu, Shi, Hsu, Shan, Qie, and Shou]{wu2023tune}
Jay~Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan~Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike~Zheng Shou.
\newblock Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 7623--7633, 2023.

\bibitem[Yang et~al.(2024{\natexlab{a}})Yang, Kang, Huang, Xu, Feng, and Zhao]{yang2024depth}
Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.
\newblock Depth anything: Unleashing the power of large-scale unlabeled data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10371--10381, 2024{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Chen, Chen, Fang, Liu, and Chen]{yang2023defect}
Shuai Yang, Zhifei Chen, Pengguang Chen, Xi Fang, Shu Liu, and Yingcong Chen.
\newblock Defect spectrum: A granular look of large-scale defect datasets with rich semantics, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Zhou, Liu, and Loy]{yang2023rerender}
Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen~Change Loy.
\newblock Rerender a video: Zero-shot text-guided video-to-video translation.
\newblock In \emph{SIGGRAPH Asia 2023 Conference Papers}, pages 1--11, 2023{\natexlab{b}}.

\bibitem[Yang et~al.(2024{\natexlab{b}})Yang, Teng, Zheng, Ding, Huang, Xu, Yang, Hong, Zhang, Feng, et~al.]{yang2024cogvideox}
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et~al.
\newblock Cogvideox: Text-to-video diffusion models with an expert transformer.
\newblock \emph{arXiv preprint arXiv:2408.06072}, 2024{\natexlab{b}}.

\bibitem[Yao et~al.(2024{\natexlab{a}})Yao, Wang, Yang, and Wang]{yao2024vitmatte}
Jingfeng Yao, Xinggang Wang, Shusheng Yang, and Baoyuan Wang.
\newblock Vitmatte: Boosting image matting with pre-trained plain vision transformers.
\newblock \emph{Information Fusion}, 103:\penalty0 102091, 2024{\natexlab{a}}.

\bibitem[Yao et~al.(2024{\natexlab{b}})Yao, Wang, Ye, and Liu]{yao2024matte}
Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu.
\newblock Matte anything: Interactive natural image matting with segment anything model.
\newblock \emph{Image and Vision Computing}, 147:\penalty0 105067, 2024{\natexlab{b}}.

\bibitem[Yin et~al.(2023)Yin, Wu, Liang, Shi, Li, Ming, and Duan]{yin2023dragnuwa}
Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan.
\newblock Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory.
\newblock \emph{arXiv preprint arXiv:2308.08089}, 2023.

\bibitem[Zeng et~al.(2024)Zeng, Deschaintre, Georgiev, Hold-Geoffroy, Hu, Luan, Yan, and Ha{\v{s}}an]{zeng2024rgb}
Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Milo{\v{s}} Ha{\v{s}}an.
\newblock Rgb\(\leftrightarrow\)x: Image decomposition and synthesis using material-and lighting-aware diffusion models.
\newblock In \emph{ACM SIGGRAPH 2024 Conference Papers}, pages 1--11, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Wu, Liu, Zhao, Ran, Gu, Gao, and Shou]{zhang2023show}
David~Junhao Zhang, Jay~Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike~Zheng Shou.
\newblock Show-1: Marrying pixel and latent diffusion models for text-to-video generation.
\newblock \emph{arXiv preprint arXiv:2309.15818}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Li, Le, Shou, Xiong, and Sahoo]{zhang2024moonshot}
David~Junhao Zhang, Dongxu Li, Hung Le, Mike~Zheng Shou, Caiming Xiong, and Doyen Sahoo.
\newblock Moonshot: Towards controllable video generation and editing with multimodal conditions.
\newblock \emph{arXiv preprint arXiv:2401.01827}, 2024.

\bibitem[Zhang and Agrawala(2024)]{zhang2024transparent}
Lvmin Zhang and Maneesh Agrawala.
\newblock Transparent image layer diffusion using latent transparency.
\newblock \emph{arXiv preprint arXiv:2402.17113}, 2024.

\bibitem[Zheng et~al.(2024)Zheng, Peng, Yang, Shen, Li, Liu, Zhou, Li, and You]{opensora}
Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You.
\newblock Open-sora: Democratizing efficient video production for all, 2024.

\bibitem[Zhu et~al.(2021)Zhu, Ping, Xiao, Shoeybi, Goldstein, Anandkumar, and Catanzaro]{zhu2021long}
Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro.
\newblock Long-short transformer: Efficient transformers for language and vision.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 17723--17736, 2021.

\end{thebibliography}
