\section{Related Work}
\label{sec:related}                             

\noindent{\textbf{Text-to-Video Generation.}}~Early video generation models were primarily based on Unet-based latent diffusion models (LDMs) extended from text-to-image models like Stable Diffusion~\cite{rombach2022high}. For example, AnimateDiff~\cite{guo2023animatediff} introduced a temporal attention module to improve temporal consistency across frames. Subsequent video generation models~\cite{wang2023modelscope, cerspense2023zeroscope, chen2023videocrafter1, chen2024videocrafter2, zhang2024moonshot, zhang2023show} adopted an alternating approach with 2D spatial and 1D temporal attention, including works like ModelScope, VideoCrafter, Moonshot, and Show-1. 

With advancements in large language models (LLMs) and the introduction of Sora~\cite{sora2024}, attention shifted from Unet architectures to transformer-based architectures (DiT). DiT-based video generation models, such as Latte~\cite{ma2024latte} and OpenSora~\cite{opensora}, extended the DiT text-to-image (T2I) model~\cite{chen2023pixart} and maintained the 2D and 1D alternating attention approach, achieving promising results. Recently, DiT-based video generation has rapidly progressed, achieving further improvements in quality. Several methods~\cite{yang2024cogvideox, opensoraplan, genmo2024mochi} have moved away from the 2D and 1D alternating approach, instead treating video frames as a single long sequence with 3D positional embeddings for encoding. These approaches also prepend text tokens—processed through a text encoder—to the video sequence, creating a streamlined network that relies solely on full self-attention and feed-forward layers. Our method builds upon these recent open-source transformer-based video generation models.


\vspace{0.5em}
\noindent{\textbf{Video Matting.}}~A straightforward approach for RGBA video generation is to extract the alpha channel from generated RGB content, as done with traditional green screen keying or learning-based video matting expert models~\cite{lin2023omnimatterf, lin2021real, lin2022robust}. OmnimatteRF~\cite{lin2023omnimatterf} introduces a video matting method that combines dynamic 2D foreground layers with a 3D background model, enabling more realistic scene reconstruction for real-world videos. Robust Video Matting (RVM)~\cite{lin2022robust} proposes a real-time, high-quality human video matting method with a recurrent architecture for improved temporal coherence, achieving state-of-the-art results without auxiliary inputs. Another work presents a high-speed, high-resolution background replacement technique with precise alpha matte extraction, supported by the VideoMatte240K and PhotoMatte13K/85 datasets~\cite{lin2021real}. Additionally, many image matting methods~\cite{chen2022pp, li2024matting, yao2024vitmatte, wang2024matting} can be applied for frame-by-frame matting.


Further, several works~\cite{he2024lotus, yang2024depth, ke2024repurposing} in image depth estimation adapt pretrained generation models for prediction tasks, achieving strong performance that often surpasses traditional, scratch-trained expert models. Marigold~\cite{ke2024repurposing} modifies architectures to create image-conditioned generation models, while Lotus~\cite{he2024lotus} explores the role of the diffusion process in this context. Although there is currently no dedicated approach for video matting within video generation models, we replicate and extend these methods to evaluate their performance, allowing us to highlight the limitations of prediction-based pipelines for RGBA generation.

\vspace{0.5em}
\noindent{\textbf{Generation beyond RGB.}}~Another category of methods~\cite{zhang2024transparent, long2024wonder3d, bao2023one, luo2024intrinsicdiffusion, zeng2024rgb, he2024lucidfusion, yang2023defect} explores expanding generation models to simultaneously generate additional channels, though they are not specifically designed for RGBA video generation. 
For instance, LayerDiffusion~\cite{zhang2024transparent} modifies the VAE in latent diffusion models to decode alpha channels. However, VAEs typically lack the semantic understanding required for precise alpha generation, limiting their effectiveness in complex visual scenarios where texture and contour details are critical. 
In contrast, other approaches~\cite{long2024wonder3d, bao2023one, luo2024intrinsicdiffusion, zeng2024rgb} modify the denoising model directly to enable joint generation. Wonder3D~\cite{long2024wonder3d} uses a domain embedding to control the model’s generation modality, while methods like IntrinsicDiffusion~\cite{luo2024intrinsicdiffusion} and RGB\(\leftrightarrow\)X~\cite{zeng2024rgb} adapt the UNet’s input and output layers to jointly produce intrinsic modalities. However, all these methods are designed for image tasks and rely on UNet architectures. When applied to video generation, they face limitations in quality and diversity due to the scarcity of RGBA video data.
