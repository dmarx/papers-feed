\section{Conclusion}
\label{sec:conclusion}
In this work, we present a novel approach for Text-to-RGBA video generation, extending RGB generation models to support RGBA output with minimal modification and high fidelity. By leveraging transformer-based DiT models and optimizing attention mechanisms specific to RGBA generation, our method effectively balances the preservation of RGB quality with the accurate generation of alpha channels. Our approach demonstrates that targeted modifications—such as the addition of alpha tokens, reinitialization of positional embeddings, and selective LoRA fine-tuning—can yield complex and high-quality RGBA outputs even with limited data. Extensive experimental results validate our framework, showing its versatility and robustness across diverse scenarios. Looking forward, we aim to explore further optimizations to reduce computational costs and enhance model scalability.