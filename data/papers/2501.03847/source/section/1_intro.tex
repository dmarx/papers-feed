\vspace{-12pt}
\section{Introduction}

% video generation
The development of diffusion generative models~\cite{rombach2022high,ho2020denoising,blattmann2023stable,brooks2024video,lin2024open,opensora} enables high-quality video generation from text prompts or a starting image. Recent emerging models, e.g. Sora~\cite{brooks2024video}, CogVideo-X~\cite{yang2024cogvideox}, Keling~\cite{keling}, and Hunyuan~\cite{kong2024hunyuanvideo}, have shown impressive video generation ability with strong temporal consistency and appealing visual effects, which becomes a promising tool for artists to create stunning videos using just few images or text prompts. These advancements show strong potential to revolutionize the advertising, film, robotics, and game industries, becoming fundamental elements for various generative AI-based applications.

% camera/motion/object control, all are limited to a specific type of control 
A major challenge in video generation lies in achieving versatile and precise control to align seamlessly with users' creative visions. While recent methods have introduced strategies to integrate control into the video generation process~\cite{wang2024motionctrl,he2024cameractrl,polyak2024movie,he2024id,yuan2024identity,wang2024boximator,huang2023fine,guo2024sparsectrl,namekata2024sg,ma2024trailblazer,ma2024follow}, they predominantly focus on specific control types, relying on specialized architectures that lack adaptability to emerging control requirements. Furthermore, these approaches are generally limited to high-level adjustments—such as camera movements or maintaining identity—falling short when it comes to enabling fine-grained modifications, like precisely raising an avatar's left hand.

We argue that achieving versatile and precise video generation control fundamentally requires 3D control signals in the diffusion model. 
Videos are 2D renderings of dynamic 3D content. 
In a traditional Computer Graphics (CG)- based video-making pipeline, we can effectively control all aspects of a video in detail by manipulating the underlying 3D representations, such as meshes or particles. 
However, existing video control methods solely apply 2D control signals on rendered pixels, lacking the 3D awareness in the video generation process and thus struggling to achieve versatile and fine-grained controls.
Thus, to this end, we present a novel 3D-aware video diffusion method, called \textbf{Diffusion as Shader} (\methodname) in this paper, which utilizes 3D control signals to enable diverse and precise control tasks within a unified architecture. 

Specifically, as shown in Figure~\ref{fig:teaser} (a), \methodname is an image-to-video diffusion model that takes a 3D tracking video as the 3D control signals for various control tasks. 
% \methodname takes an input image and a 3D tracking video as a condition to generate a video that takes the input image as the first frame and follows the motion given by the 3D tracking video. 
The 3D tracking video contains the motion trajectories of 3D points whose colors are defined by their coordinates in the camera coordinate system of the first frame. In this way, the 3D tracking video represents the underlying 3D motion of this video. The video diffusion model acts like a shader to compute shaded appearances on the dynamic 3D points to generate the video. Thus, we call our model \textit{Diffusion as Shader}.

% An additional advantage of utilizing the 3D tracking videos in \methodname is the improved consistency in the generation process. 
Using 3D tracking videos as control signals offers a significant advantage over depth videos with enhanced temporal consistency. While a straightforward approach to incorporating 3D control into video diffusion models involves using depth maps as control signals, depth maps only define the structural properties of the underlying 3D content without explicitly linking frames across time. In contrast, 3D tracking videos provide a consistent association between frames, as identical 3D points maintain the same colors across the video. These color anchors ensure consistent appearances for the same 3D points, thereby significantly improving temporal coherence in the generated videos. Our experiments demonstrate that even when a 3D region temporarily disappears and later reappears, \methodname effectively preserves the appearance consistency of that region, thanks to the temporal consistency enabled by the tracking video.

% our method: 3D flow can be created from both videos or animated meshes.
By leveraging 3D tracking videos, \methodname enables versatile video generation controls, encompassing but not limited to the following video control tasks.
\begin{enumerate}[itemsep=0pt,leftmargin=0.5cm]
    \vspace{-12pt}
    \item \textit{Animating meshes to videos}. Using advanced 3D tools like Blender, we can design animated 3D meshes based on predefined templates. These animated meshes are transformed into 3D tracking videos to guide high-quality video generation (Figure~\ref{fig:teaser} (b)).
    \item \textit{Motion transfer}. Starting with an input video, we employ a 3D tracker~\cite{xiao2024spatialtracker} to generate a corresponding 3D tracking video. Next, the depth-to-image Flux model~\cite{flux} is used to modify the style or content of the first frame. Based on the updated first frame and the 3D tracking video, \methodname generates a new video that replicates the motion patterns of the original while reflecting the new style or content (Figure~\ref{fig:teaser} (c)).
    \item \textit{Camera control}. To enable precise camera control, depth maps are estimated to extract 3D points~\cite{bochkovskii2024depth}. These 3D points are then projected onto a specified camera path to create a 3D tracking video, which guides the generation of videos with customized camera movements (Figure~\ref{fig:teaser} (d)).
    \item \textit{Object manipulation}. By integrating object segmentation techniques~\cite{kirillov2023segment} with a monocular depth estimator~\cite{bochkovskii2024depth}, the 3D points of specific objects can be extracted and manipulated. These modified 3D points are used to construct a 3D tracking video, which guides the creation of videos for object manipulation (Figure~\ref{fig:teaser} (e)).
    % \item \textit{Physics-aware video generation}. To achieve physics-aware video generation, Blender's physics simulation tools are utilized to produce physically accurate motions. These motions are converted into 3D tracking videos, which serve as control signals for \methodname to generate videos that adhere to the principles of physics (Figure~\ref{fig:teaser} (f)).
\end{enumerate}

% additional advantage: temporal consistency

% , as shown in Figure~\ref{fig:teaser} (x). \ly{we need to demo at least one example and show it in the teaser.}

% Results: 4 types of control
Due to the 3D awareness of \methodname, \methodname is data-efficient. Finetuning with less than 10k videos on 8 H800 GPUs for 3 days already gives the powerful control ability to \methodname, which is demonstrated by various control tasks. 
% The image-to-video results show that controlling with the 3D tracking video is more robust and consistent than simply using depth maps as control signals in CogVideoX~\cite{yang2024cogvideox}. Then, 
We compare \methodname with baseline methods on camera control~\cite{wang2024motionctrl,he2024cameractrl} and motion transfer~\cite{geyer2023tokenflow}, which demonstrates that \methodname achieves significantly improved performances in these two controlling tasks than baselines. 
For the remaining two tasks, i.e. mesh-to-video and object manipulation, we provide extensive qualitative results to show the superior generation quality of our method.

