\section{Introduction}

% video generation
The development of diffusion generative models~\cite{rombach2022high,ho2020denoising,blattmann2023stable,brooks2024video,lin2024open,opensora} enables high-quality video generation from text prompts or a starting image. Recent emerging models, e.g. Sora~\cite{brooks2024video}, CogVideo-X~\cite{yang2024cogvideox}, Keling~\cite{keling}, and Hunyuan~\cite{kong2024hunyuanvideo}, have shown impressive video generation ability with strong temporal consistency and appealing visual effects, which becomes a promising tool for artists to create stunning videos using just few images or text prompts. These advancements show strong potential to revolutionize the advertising, film, robotics, and game industries, becoming fundamental elements for various generative AI-based applications.

% camera/motion/object control, all are limited to a specific type of control 
An ongoing and significant challenge in video generation is achieving \ZY{versatile as well as }precise control over the generation process to align seamlessly with users' creative intentions. Current approaches demand extensive and often cumbersome prompt engineering to produce desired visual outcomes. This process is not only time-intensive but also computationally expensive, as generating a single video using diffusion models can take several minutes, even on high-end GPUs. Furthermore, the effectiveness of these prompting strategies varies widely across models, lacking consistency or logical structure, and instead heavily depends on trial-and-error and user expertise.


Recent advancements have introduced strategies to incorporate control in video generation, but these methods primarily focus on single control types using specialized architectures, lacking the flexibility to adapt to new control demands. For example, \cite{wang2024motionctrl,he2024cameractrl} utilize camera or ray embeddings to control camera poses, enabling videos to follow predefined camera trajectories. Similarly, \cite{polyak2024movie,he2024id,yuan2024identity} employ human images as prompts to generate videos that retain specified human identities. Additionally, \cite{wang2024boximator,huang2023fine,guo2024sparsectrl,namekata2024sg,ma2024trailblazer,ma2024follow} focus on manipulating objects to achieve specific movements within generated videos. Despite these advances, existing approaches address only isolated control tasks, leaving unanswered the critical question of how to design a generalizable control framework for diverse applications. Moreover, these methods provide only high-level controls—such as camera movements or overall identity preservation—while precise, fine-grained adjustments, like precisely raising an avatar's left hand, remain beyond their capabilities.

% observation: 3D space
% todo: what is general motion/control
% In this paper, we introduce a versatile video generation control method, called \textbf{Diffusion as Shader} (\methodname), that supports diverse control tasks and fine-grained controlling of the generated videos in a unified architecture. 
% The key idea of \methodname is to utilize dynamic 3D representations as the additional control signals in the video generation.
% Our motivation stems from the traditional Computer Graphics (CG) animation pipeline to render videos, in which we can effectively control the rendered videos by manipulating the underlying 3D representations like meshes or particles. 
% As shown in Figure~\ref{fig:teaser} (a), \methodname follows this idea to incorporate a 3D tracking video as a condition for video generation and manipulates the 3D tracking video to support diverse control tasks and precisely control motions.


In this paper, we present a novel video generation control method, \textbf{Diffusion as Shader} (\methodname), designed to enable diverse and precise control tasks within a unified architecture. \methodname introduces a novel approach by leveraging dynamic 3D representations as additional control signals, allowing fine-grained video manipulation. Our key idea is inspired by the traditional Computer Graphics (CG) animation pipeline, where rendered videos are meticulously controlled by adjusting 3D elements such as meshes or particles. 
\methodname adapts this principle to diffusion-based video generation. As illustrated in Figure~\ref{fig:teaser} (a), our method seamlessly integrates 3D tracking videos as conditional inputs, allowing for unparalleled versatility in controlling video content. By manipulating these 3D representations, \methodname enables precise motion control and supports a wide scope of control tasks, bridging the traditional CG pipelines and the advanced video generation technologies.

% In this paper, we introduce a new method, called \textbf{Diffusion as Shader} (\methodname), to address the above problem by applying 3D tracking as the control signal in the video generation process. 
% Our motivation stems from the traditional Computer Graphics (CG) animation pipeline to render videos, which effectively controls the contents of rendered videos by manipulating the underlying 3D representations like meshes or particles. DAS follows this pipeline to represent the underlying 3D representations of the generated video with a 3D tracking video and uses this 3D tracking video as the control signal in the video generation. Then, by manipulating the 3D tracking video, we effectively and accurately control the video generation process.

Specifically, as shown in Figure~\ref{fig:teaser} (a), \methodname is an image-to-video diffusion model. \methodname takes an input image and a 3D tracking video as a condition to generate a video that takes the input image as the first frame and follows the motion given by the 3D tracking video. The 3D tracking video contains the motion trajectories of 3D points and the colors of the 3D tracking video represent the coordinates of these 3D points in the first frame. In this way, the 3D tracking video represents the underlying 3D motion of this video. The video diffusion model acts like a shader to compute shaded appearances on the dynamic 3D points to generate the video. Thus, we call our model \textit{Diffusion as Shader}.

% our method: 3D flow can be created from both videos or animated meshes.
\methodname allows various video generation controls, which include but are not limited to the following 5 types.
\begin{enumerate}
    \item \textit{Animating meshes to videos}. By using advanced 3D tools like Blender, we can create a simple animated 3D mesh with given templates and transform the animated mesh into a 3D tracking video to guide the generation of a high-quality video. (Figure~\ref{fig:teaser} (b)).
    \item \textit{Motion retargeting}. Given a video, we apply a 3D tracker~\cite{xiao2024spatialtracker} to get a 3D tracking video and then we apply the depth-to-image flux model~\cite{flux} to change the style or contents of the first frame. Then, based on the re-generated first frame and the tracking video, \methodname generates a new video with the same motion pattern as the original video. (Figure~\ref{fig:teaser} (c)).
    \item \textit{Camera control}. To allow camera control, we estimate depth maps to get 3D points~\cite{bochkovskii2024depth} and construct the 3D tracking video by projecting 3D points onto the given cameras to generate a video. (Figure~\ref{fig:teaser} (d)).
    \item \textit{Object manipulation}. By integrating object segmentation methods~\cite{kirillov2023segment} and a monocular depth estimator~\cite{bochkovskii2024depth}, we get the 3D points of a specific object. Then, we create a 3D tracking video by manipulating these 3D points and generate the object manipulation video from the 3D tracking video. (Figure~\ref{fig:teaser} (e)).
    \item \textit{Physics-aware video generation}. To allow physics-aware video generation, we utilize the physics simulation of Blender to generate physically correct motions, which are converted to 3D tracking videos to guide \methodname for physics-aware video generation. (Figure~\ref{fig:teaser} (f)).
\end{enumerate}

% additional advantage: temporal consistency
An additional advantage of utilizing the 3D tracking videos in \methodname is the improved consistency in the generation process. Because the same 3D point always has the same color in the 3D tracking videos. The colors of the 3D tracking video act like anchors to generate consistent appearances for the same 3D points, which greatly improves the temporal consistency of the generated videos. In experiments, we demonstrate that even when a 3D region disappears and re-appears in a video, \methodname still effectively maintains the appearance consistency of this 3D region thanks to the temporal consistency brought by the tracking video.
% , as shown in Figure~\ref{fig:teaser} (x). \ly{we need to demo at least one example and show it in the teaser.}

% Results: 4 types of control
Due to the 3D awareness of \methodname, \methodname is data- and computation-efficient. Finetuning with less than 10k videos on 8 H800 GPUs for 3 days already gives the powerful control ability to \methodname, which is demonstrated by various control tasks. 
The image-to-video results show that controlling with the 3D tracking video is more robust and consistent than simply using depth maps as control signals in CogVideoX~\cite{yang2024cogvideox}. Then, we compare \methodname with baseline methods on camera control~\cite{wang2024motionctrl,he2024cameractrl} and motion retargeting~\cite{geyer2023tokenflow}, which demonstrates that \methodname achieves significantly improved performances in these two controlling tasks than baselines. For the remaining three tasks, i.e. mesh-to-video, physics-aware generation, and object manipulation, we provide extensive qualitative results to show the superior generation quality of our method.

