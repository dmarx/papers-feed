

\section{Limitations and Conclusions}

\textbf{Limitations and future works}. 
% Our method may fail to correctly generate a video with incorrect 3D tracking points as shown in Figure~\ref{fig:tracking1}. Improving the accuracy of 3D trackers could improve the generation results for motion transfer in the future.
Though \methodname achieves control over the video generation process in most cases, it still suffers from multiple failure cases mainly caused by incorrect 3Dtracking videos.
The first failure case is that the input image should be compatible with the 3D tracking videos. Otherwise, the generated videos would be implausible as shown in Figure~\ref{fig:tracking1} (top). Another failure case is that for regions without 3D tracking points, the generated contents may be out-of-control and produce some unnatural results (Figure~\ref{fig:tracking1} (bottom)).
For future works, we currently rely on provided animated meshes or existing videos to get high-quality 3D tracking videos and a promising direction is to learn to generate these 3D tracking videos with a new diffusion model.

\noindent\textbf{Conclusions}. In this paper, we introduce Diffusion as Shader (\methodname) for controllable video generation. 
The key idea of \methodname is to adopt the 3D tracking videos as 3D control signals for video generation. The 3D tracking videos are constructed from colored dynamic 3D points which represent the underlying 3D motion of the video. Then, diffusion models are applied to generate a video following the motion of the 3D tracking video. We demonstrate that the 3D tracking videos not only improve the temporal consistency of the generated videos but also enable versatile control of the video content, including mesh-to-video generation, camera control, motion transfer, and object manipulation.
% \methodname is a 3D-aware video diffusion model that utilizes 3D tracking videos as 3D control signals. 
