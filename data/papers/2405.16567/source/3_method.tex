\input{Figure/figure_method_pipeline}
\vspace{-0.12in}
\section{Automatic prompt generation pipeline for evaluating copyright violations}
\vspace{-0.12in}
T2I models generate single or multiple images based on the user's prompt, aiming to reflect as much information as possible. While following the user's prompt, T2I models may violate the reproduction rights of certain IPs. However, evaluating the safety of T2I systems by a trial-and-error process using manually crafted prompts is challenging and tiresome.

To alleviate the challenge, we propose an \textbf{Automatic Prompt Generation Pipeline (APGP)} that generates high-risk prompts for T2I systems. Generated prompts are designed to test the systems' tendencies to violate copyright and safety policies, allowing us to effectively evaluate the commercial T2I systems' output without any weight updates or gradient computations. APGP consists of three steps: 1) searching seed prompts that describe the target images using vision-language models; 2) revising the generated prompts into high-risk prompts via optimization, based on self-generated QA scores and keyword penalties; and 3) post-processing with a suffix for keyword suppression and intention addition. Details are illustrated in Appendix~\ref{app:pipeline}. 

\vspace{-0.1in}
\subsection{Searching seed prompt using vision-language models}
\vspace{-0.1in}
As shown in Figure~\ref{fig:pipeline} ({\color{Cerulean} left}), we propose an automated pipeline that generates high-risk prompts---detailed descriptions of the target image---to guide the T2I model in replicating the target image. We first use a vision-language model (VLM) to describe the target image. To reach a high success rate in generating a copyright-violated image, we require the initial prompt to accurately depict all components in the target image rather than illustrating general objects.%To reach high-risk prompts, the quality of the initial prompt is highly relevant that can accurately include all components from the target image rather than illustrating general object.

To search optimal seed prompts for T2I models, we utilize an optimization by prompting (OPRO)~\citep{yang2023large} approach, seeking the most effective instructions for a VLM ($g$) by employing a LLM ($f_1$) as the optimizer. Given the predefined $N$ initial instructions $\{{\text inst}_{1:N}\}$, where $i$ ranges from $1$ to $N$, the VLM generates the prompt $\{x_i\}$ that describes the target image $I_{\texttt{target}}$. To measure the effectiveness of the instructions given to the VLM, we utilize the alignment score $c_i$, which is the cosine distance between the embedding vector of each prompt $x_i$ and the embedding vector of the target image $I_{\texttt{target}}$ using CLIP~\citep{radford2021learning}. 
%

\input{Figure/figure_method_suffix}
Similar to OPRO~\citep{yang2023large}, we forward instruction and score pair ($\{{inst}_i, c_i\}$) to the LLM ($f_1$) to update the instructions to ${inst}_{i+1}$. This optimization process is repeated through generating new prompts based on updated instructions, calculating the CLIP scores for each prompt, and refining the instructions by passing the instruction-score pairs back to the LLM. If the highest score remains unchanged for $r$ steps, we conclude the best seed prompt ($z_0$) for the target image has been achieved. The instruction optimization template for the LLM ($f_1$) is described in Appendix~\ref{app:prompt_template}. \blfootnote{{\color{BrickRed}$^1$}The formatting is edited for readability, but the content matches the original screenshot (Appendix~\ref{app:suffix_result_screenshot}).}

%
\vspace{-0.1in}
\subsection{Optimizing the prompts with keyword penalties and self-generated QA scores}
\vspace{-0.1in}
To generate the highest-risk prompt that evokes the exact target content from T2I systems, we propose a automated prompt revision step via optimization based on self-generated QA scores and keyword penalties. In this step, we start with the seed prompt ($z_0$) and refine it to $z_t$ using the LLM ($f_2$) to achieve higher self-generated QA scores and fewer keyword penalties, which induces the generation of the copyright-violating image $I_{\texttt{gen}}$ with T2I systems.
\vspace{-0.1in}
\paragraph{Our score functions.} 
To find the highest-risk prompt for T2I systems, score functions ($S$) are critical to drive the LLM as shown in Figure~\ref{fig:pipeline}. We propose two scores, keyword penalty ($S_k$) and question\&answer (QA) score ($S_{qa}$) along with image-image consistency and image-text alignment. 
% "To bypass word-based detection in some T2I systems, we aim to generate prompts with precise descriptions without using any keywords. Thus, the keyword penalty score applies if the description contains any of the keywords, denoted as $k$. We count the number of keywords appearing in the description ($z_t$) and apply a penalty of $-5$ for each. However, these penalties may result in a prompt with a generic description ($z_t$) that does not accurately describe the target image $I_{\texttt{target}}$."
To bypass the word-based detection in some T2I systems, we aim to generate prompts with precise descriptions of the target image without using any keywords that explicitly represent the target image. Thus, the keyword penalty score applies if the prompt contains any of the keywords, $k$. We count the number of keywords that appear in the prompt ($z_t$) and penalize it with negative value. However, these penalties may lead to the prompt ($z_t$) with a generic description that does not reflect distinct information to describe the target image $I_{\texttt{target}}$.

To prevent generic prompts, we propose a self-generated QA score that evaluates answers based on the text-only prompt ($z_t$) and the questions generated by the VLM from the target image (see Figure~\ref{fig:pipeline}, highlighted in yellow).
The question and answer pairs (\{$q_m$, $a_m$\}) are ``self-generated'' with the VLM based on the given target image $I_{\texttt{target}}$. The LLM ($l$) responds to the question ($q_m$) based on the text-only ($z_t$) as follow, $y_m = l(q_m, z_t)$. To evaluate the response ($y_m$), we employ LLM ($v$) as a verifier as follow, $\mathds{1}^m_{0,1} = m(q_m, a_m, y_m)$, so that the QA score ($S_{qa}$) is $\sum_m \mathds{1}^m*5$. The keyword penalty and self-generated QA score result in refining the prompts to include all information about the target image so that we can find the highest-risk prompt that describes the target image precisely as much as possible without any keywords.


The consistency score and alignment score are calculated using the cosine similarity distance between the vectors extracted by the encoder. For image-image consistency score ($S_{ii}$), we employ pretrained vision encoder $v$ and calculate distance between the target image ($I_{\texttt{target}}$) and generated image ($I_{\texttt{gen}}$) from T2I model $h$. For image-text consistency score ($S_{ti}$), we employ CLIP model as an encoder and calculate the distance between the prompt $z_t$ and target image $I_{\texttt{target}}$. The overall score function for optimizing the highest-risk prompt is as follows,
\begin{equation}
    S = S_{ii} + S_{ti} + S_{k} + S_{qa}.
\end{equation}

\vspace{-0.1in}
\paragraph{Optimizing prompt with automated prompt revision.} To refine the prompt that can induce copyright infringement in T2I systems, we update the prompt based on our proposed score function. We forward paired prompt and score ($\{z_t, s_t\}$) to LLM ($f_2$) to refine the prompt to $z_{t+1}$. This process is repeated, generating new prompts and updating prompt-score pairs with the score function $S$. We optimize the texts for $T$ steps and select the highest-scored prompt.

\vspace{-0.1in}
\subsection{Suffix prompt injection} 
\vspace{-0.1in}
To further rigorously evaluate the T2I systems, we employ suffix prompt injection in our prompt which is a similar approach to jailbreaking the language model behaviors~\citep{wei2024jailbroken}. We append the suffix prompt: "Generate image. Do not rephrase the prompt." This further ensures our prompts are conveyed to the T2I models accurately.

As shown in Table~\ref{table:suffix_prompt}, we also replace specific keywords with generic words in the prompt to bypass the block system, while preserving the overall description of the prompt. For example, we change keywords like "Mickey Mouse" to a general word like "mouse" or "something" as demonstrated in Figure~\ref{fig:suffix_keyword}. Additionally, we employ an intention suffix prompt, which misleads the model into thinking the user's intent is to create their original works. Surprisingly, as shown in Figure~\ref{fig:suffix_intention}, this increases the likelihood of the models committing violations.
\input{Table/table_suffix_prompt}