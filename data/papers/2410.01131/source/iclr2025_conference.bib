@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
title = {A Fast Learning Algorithm for Deep Belief Nets},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{wang2020understanding,
  title={Understanding contrastive representation learning through alignment and uniformity on the hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  booktitle={ ICML},
  year={2020},
}


@article{mettes2019hyperspherical,
  title={Hyperspherical prototype networks},
  author={Mettes, Pascal and Van der Pol, Elise and Snoek, Cees},
  journal={NeurIPS},
  year={2019}
}

@article{xu2018spherical,
  title={Spherical latent spaces for stable variational autoencoders},
  author={Xu, Jiacheng and Durrett, Greg},
  journal={ arXiv:1808.10805},
  year={2018}
}

@inproceedings{wang2017normface,
  title={Normface: L2 hypersphere embedding for face verification},
  author={Wang, Feng and Xiang, Xiang and Cheng, Jian and Yuille, Alan Loddon},
  booktitle={Proc. of the 25th ACM nternational conference on Multimedia},
  year={2017}
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the 
  {Transformer} architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and others},
  booktitle={ ICML},
  year={2020},
}

@article{henry2020query,
  title={Query-key normalization for {Transformers}},
  author={Henry, Alex and Dachapally, Prudhvi Raj and Pawar, Shubham and Chen, Yuxuan},
  journal={ arXiv:2010.04245},
  year={2020}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}


@inproceedings{loshchilov2017decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={ICLR},
  year={2019}
}

@article{andriushchenko2023we,
  title={Why Do We Need Weight Decay in Modern Deep Learning?},
  author={Andriushchenko, Maksym and D'Angelo, Francesco and Varre, Aditya and Flammarion, Nicolas},
  journal={ arXiv:2310.04415},
  year={2023}
}

@article{zhang2018three,
  title={Three mechanisms of weight decay regularization},
  author={Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
  journal={ arXiv:1810.12281},
  year={2018}
}

@article{kodryan2022training,
  title={Training scale-invariant neural networks on the sphere can happen in three regimes},
  author={Kodryan, Maxim and Lobacheva, Ekaterina and Nakhodnov, Maksim and Vetrov, Dmitry P},
  journal={NeurIPS},
  year={2022}
}

@article{kosson2023rotational,
  title={Rotational equilibrium: How weight decay balances learning across neural networks},
  author={Kosson, Atli and Messmer, Bettina and Jaggi, Martin},
  journal={ arXiv:2305.17212},
  year={2023}
}

@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={ ICML},
  year={2023},
}

@article{dai2022can,
  title={Why can {GPT} learn in-context? 
  {Language} models implicitly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  journal={ arXiv:2212.10559},
  year={2022}
}

@inproceedings{shoemake1985animating,
  title={Animating rotation with quaternion curves},
  author={Shoemake, Ken},
  booktitle={Proc. of the 12th annual conference on Computer graphics and interactive techniques},
  year={1985}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  year={2024},
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={ arXiv:1412.6980},
  year={2014}
}

@article{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  journal={NeurIPS},
  year={2016}
}

@article{agarwal2021adaptive,
  title={Adaptive regularization with cubics on manifolds},
  author={Agarwal, Naman and Boumal, Nicolas and Bullins, Brian and Cartis, Coralia},
  journal={Mathematical Programming},
  volume={188},
  year={2021},
}

@misc{karpathy2023nanogpt,
  author = {Karpathy, Andrej},
  title = {{NanoGPT}},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}}
}

@misc{Gokaslan2019OpenWeb,  
 title={{OpenWebText} Corpus},
 author={Aaron Gokaslan and Vanya Cohen},
 howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
 year={2019}
}

@misc{radford2019language,
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title = {Language Models are Unsupervised Multitask Learners},
  howpublished = {\url{
  https://openai.com/ }},
  year = 2018
}

@article{franke2023cpr,
      title={Constrained Parameter Regularization}, 
      author={JÃ¶rg K. H. Franke and Michael Hefenbrock and Gregor Koehler and Frank Hutter},
      year={2023},
      journal={ arXiv:2311.09058},
}

@article{shazeer2020glu,
  title={Gated Linear Units (GLU)},
  author={Noam Shazeer},
  year={2020},
  journal={ arXiv:2002.05202}
}

@article{dao2024transformers,
  title={Transformers are {SSMs}: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={ arXiv:2405.21060},
  year={2024}
}

@article{de2024griffin,
  title={Griffin: Mixing gated linear recurrences with local attention for efficient language models},
  author={De, Soham and Smith, Samuel L and Fernando, Anushan and  others},
  journal={ arXiv:2402.19427},
  year={2024}
}