\begin{table}[h]
% \caption{Ablations on the removal of normalization in nGPT. Removing QK normalization involves taking out the normalization terms \textbf{Norm} in equations~\ref{eq:qkscaling1} and~\ref{eq:qkscaling2}. Similarly, removing transformation input normalization means replacing the normalizations in equations~\ref{eq:ATTNnew2} and~\ref{eq:MLPnew2}, and instead applying normalization to $h$ before the LERP operation.}
\caption{Ablations on the removal of QK normalization in nGPT which involves taking out the normalization terms \textbf{Norm} in equations~\ref{eq:qkscaling1} and~\ref{eq:qkscaling2}.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccc}
\toprule
& \textbf{ARC-E} & \textbf{HellaSwag} & \textbf{WinoGrande} & \textbf{WSC273} & \textbf{LAMBADA} & \textbf{Avg. Acc (\%) $\uparrow$} & \textbf{Valid. Loss $\downarrow$}\\
\midrule
Baseline nGPT & 53.07 & 46.03 & 55.96 & 67.03 & 50.13 & 54.44 & 2.252 \\
Baseline nGPT - QK norm & 52.65 & 46.07 & 56.27 & 68.86 & 49.68 & \textcolor{Green}{54.71} & \textcolor{Red}{+0.12\%} \\
% - Transformation Input norm & 51.09 & 45.71 & 56.12 & 67.03 & 51.21 & \textcolor{Red}{54.23} & \textcolor{Red}{+0.09\%} \\
\bottomrule
\end{tabular}}
\label{ablation-norm}
\end{table}