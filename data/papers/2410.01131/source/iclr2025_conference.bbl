\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Boumal, Bullins, and Cartis]{agarwal2021adaptive}
Naman Agarwal, Nicolas Boumal, Brian Bullins, and Coralia Cartis.
\newblock Adaptive regularization with cubics on manifolds.
\newblock \emph{Mathematical Programming}, 188, 2021.

\bibitem[Andriushchenko et~al.(2023)Andriushchenko, D'Angelo, Varre, and Flammarion]{andriushchenko2023we}
Maksym Andriushchenko, Francesco D'Angelo, Aditya Varre, and Nicolas Flammarion.
\newblock Why do we need weight decay in modern deep learning?
\newblock \emph{arXiv:2310.04415}, 2023.

\bibitem[Dai et~al.(2022)Dai, Sun, Dong, Hao, Ma, Sui, and Wei]{dai2022can}
Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei.
\newblock Why can {GPT} learn in-context? {Language} models implicitly perform gradient descent as meta-optimizers.
\newblock \emph{arXiv:2212.10559}, 2022.

\bibitem[Dao \& Gu(2024)Dao and Gu]{dao2024transformers}
Tri Dao and Albert Gu.
\newblock Transformers are {SSMs}: Generalized models and efficient algorithms through structured state space duality.
\newblock \emph{arXiv:2405.21060}, 2024.

\bibitem[De et~al.(2024)De, Smith, Fernando, et~al.]{de2024griffin}
Soham De, Samuel~L Smith, Anushan Fernando, et~al.
\newblock Griffin: Mixing gated linear recurrences with local attention for efficient language models.
\newblock \emph{arXiv:2402.19427}, 2024.

\bibitem[Franke et~al.(2023)Franke, Hefenbrock, Koehler, and Hutter]{franke2023cpr}
JÃ¶rg K.~H. Franke, Michael Hefenbrock, Gregor Koehler, and Frank Hutter.
\newblock Constrained parameter regularization.
\newblock \emph{arXiv:2311.09058}, 2023.

\bibitem[Gokaslan \& Cohen(2019)Gokaslan and Cohen]{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen.
\newblock {OpenWebText} corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem[Henry et~al.(2020)Henry, Dachapally, Pawar, and Chen]{henry2020query}
Alex Henry, Prudhvi~Raj Dachapally, Shubham Pawar, and Yuxuan Chen.
\newblock Query-key normalization for {Transformers}.
\newblock \emph{arXiv:2010.04245}, 2020.

\bibitem[Karpathy(2023)]{karpathy2023nanogpt}
Andrej Karpathy.
\newblock {NanoGPT}.
\newblock \url{https://github.com/karpathy/nanoGPT}, 2023.

\bibitem[Kingma(2014)]{kingma2014adam}
Diederik~P Kingma.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv:1412.6980}, 2014.

\bibitem[Kodryan et~al.(2022)Kodryan, Lobacheva, Nakhodnov, and Vetrov]{kodryan2022training}
Maxim Kodryan, Ekaterina Lobacheva, Maksim Nakhodnov, and Dmitry~P Vetrov.
\newblock Training scale-invariant neural networks on the sphere can happen in three regimes.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Kosson et~al.(2023)Kosson, Messmer, and Jaggi]{kosson2023rotational}
Atli Kosson, Bettina Messmer, and Martin Jaggi.
\newblock Rotational equilibrium: How weight decay balances learning across neural networks.
\newblock \emph{arXiv:2305.17212}, 2023.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Mettes et~al.(2019)Mettes, Van~der Pol, and Snoek]{mettes2019hyperspherical}
Pascal Mettes, Elise Van~der Pol, and Cees Snoek.
\newblock Hyperspherical prototype networks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Radford et~al.(2018)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \url{ https://openai.com/ }, 2018.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate training of deep neural networks.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Gated linear units (glu).
\newblock \emph{arXiv:2002.05202}, 2020.

\bibitem[Shoemake(1985)]{shoemake1985animating}
Ken Shoemake.
\newblock Animating rotation with quaternion curves.
\newblock In \emph{Proc. of the 12th annual conference on Computer graphics and interactive techniques}, 1985.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 2024.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{ICML}, 2023.

\bibitem[Wang et~al.(2017)Wang, Xiang, Cheng, and Yuille]{wang2017normface}
Feng Wang, Xiang Xiang, Jian Cheng, and Alan~Loddon Yuille.
\newblock Normface: L2 hypersphere embedding for face verification.
\newblock In \emph{Proc. of the 25th ACM nternational conference on Multimedia}, 2017.

\bibitem[Wang \& Isola(2020)Wang and Isola]{wang2020understanding}
Tongzhou Wang and Phillip Isola.
\newblock Understanding contrastive representation learning through alignment and uniformity on the hypersphere.
\newblock In \emph{ICML}, 2020.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, et~al.]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, et~al.
\newblock On layer normalization in the {Transformer} architecture.
\newblock In \emph{ICML}, 2020.

\bibitem[Xu \& Durrett(2018)Xu and Durrett]{xu2018spherical}
Jiacheng Xu and Greg Durrett.
\newblock Spherical latent spaces for stable variational autoencoders.
\newblock \emph{arXiv:1808.10805}, 2018.

\end{thebibliography}
