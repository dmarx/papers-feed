
\documentclass{article} % For LaTeX2e
\PassOptionsToPackage{table,dvipsnames}{xcolor}
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{placeins}
\usepackage{graphicx}
\usepackage{caption} 


\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{booktabs}
%\usepackage[table]{xcolor}
\definecolor{Gray}{gray}{0.95}
\definecolor{Green}{rgb}{0.3, 0.73, 0.09}
\definecolor{Red}{rgb}{0.82, 0.1, 0.26}

\title{nGPT: Normalized Transformer with Representation Learning on the Hypersphere}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun \& Boris Ginsburg \\
NVIDIA\\
\texttt{\{iloshchilov,chsieh,simengs,bginsburg\}@nvidia.com} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

 \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.
\end{abstract}

\section{Introduction}
The Transformer architecture  \citep{vaswani2017attention} is the foundation  for most of modern   language models. An enormous number of modifications to this architecture have been proposed to improve training stability,  inference costs, context length, robustness, etc. It has been noted that the application of various normalization techniques is beneficial \citep{salimans2016weight}, leading to experiments with adding normalization layers such as LayerNorm and RMSNorm in nearly every possible position within the network \citep{xiong2020layer}. Another approach to the model  normalization is through controlling the norm of weights using weight decay \citep{loshchilov2017decoupled}.  Recent studies  \citep{andriushchenko2023we} suggest reevaluating the role of weight decay and taking a closer look at rotations rather than focusing solely on vector norms \citep{kodryan2022training,kosson2023rotational}. \citet{franke2023cpr} suggested to enforce an upper bound on $L_2$ norm of parameter groups. There is growing evidence that representation learning on the hypersphere is associated with more stable training, greater embedding space separability, and better performance on downstream tasks \citep{wang2020understanding}. Recent studies also suggest that transformers implicitly perform gradient descent as meta-optimizers \citep{von2023transformers, dai2022can}. 

We propose to unify various findings and observations made in the field under a new perspective of the normalized Transformer. Our key contributions are as follows:
\begin{description}
    \item \textbf{Optimization of network parameters on the hypersphere} We propose to normalize all vectors forming the embedding dimensions of network matrices to lie on a unit norm hypersphere. This allows us to view matrix-vector multiplications as dot products representing cosine similarities bounded in [-1,1]. The  normalization renders weight decay unnecessary.
    \item \textbf{Normalized Transformer as a variable-metric optimizer on the hypersphere} The normalized Transformer itself performs a multi-step optimization (two steps per layer) on a hypersphere, where each step of the attention and MLP updates is controlled by {eigen} learning ratesâ€”the diagonal elements of a learnable variable-metric matrix. For each token $t_i$ in the input sequence, the optimization path of the normalized Transformer  begins at a point on the hypersphere corresponding to its input embedding vector and moves to a point on the hypersphere that best predicts the embedding vector of the next token $t_{i+1}$. 
    \item \textbf{Faster convergence} We demonstrate that the normalized Transformer reduces the number of training steps required to achieve the same accuracy by a factor of 4 to 20.%is up to 4-20 times faster on common pre-training tasks. 
\end{description}

\begin{table}[b]
 \caption{Transformer vs. Normalized Transformer.}
 \label{table_summary}
    \centering
    \small
    \begin{tabular}{p{0.30\textwidth} p{0.60\textwidth}}
    \toprule
    \textbf{Transformer} & \textbf{Normalized Transformer} \\
    \midrule
    $\vh_{\text{A}} \leftarrow \text{ATTN}(\text{RMSNorm}(\vh))$ & 
    $\vh_{\text{A}} \leftarrow \text{Norm}(\text{ATTN}(\vh))$ \\
    \midrule
    $\vh \leftarrow \vh + \vh_{\text{A}}$ &
    $\vh \leftarrow \text{Norm}(\vh + \bm{\alpha}_{\text{A}} (\vh_{\text{A}}-\vh))$\\
    \midrule
    $\vh_{\text{M}} \leftarrow \text{MLP}(\text{RMSNorm}(\vh))$ &
    $\vh_{\text{M}} \leftarrow \text{Norm}(\text{MLP}(\vh))$ \\
    \midrule
    $\vh \leftarrow \vh + \vh_{\text{M}}$ &
    $\vh \leftarrow \text{Norm}(\vh + \bm{\alpha}_{\text{M}} (\vh_{\text{M}}-\vh))$\\
    \midrule
    \text{Final:} $\vh \leftarrow \text{RMSNorm}(\vh)$ & \\
    \midrule
    \parbox{0.30\textwidth}{All parameters of matrices and embeddings are unconstrained.} & 
    \parbox{0.60\textwidth}{After each batch pass, all matrices and embeddings are normalized along their embedding dimension. The hidden state updates are controlled by learnable vectors of eigen learning rates $\bm{\alpha}_{\text{A}}$ and $\bm{\alpha}_{\text{M}}$.} \\
    \bottomrule
 \end{tabular}
\end{table}




\section{Evolution of the Transformer: From GPT to nGPT}

This section outlines the baseline Transformer and the modifications necessary to derive its normalized version. We illustrate these changes for Transformer decoder with self-attention only. The extension to encoder-decoder and cross-attention is straightforward. A summary of these changes is given in Table \ref{table_summary}, with details in Section \ref{section_summary}. 

\subsection{Token Embeddings and Output Logits}
% \subsubsection{Baseline Transformer}
The decoder-only Transformer  is 
trained to predict 
token $t_i$ using previous tokens 
input sequence $\vx=(t_1, t_2, \dots, t_{i-1}) $. 
% performs the transformation  $\hat{\vy}\leftarrow \mathcal{M}(\vx)$, where based on the input sequence $\vx=(t_1, t_2, \dots, t_T)$ with $T$ tokens,  $\mathcal{M}(\vx)$ is trained to predict $\hat{\vy}$ which approximates the target sequence $\vy=(t_2, t_3, \dots, t_{T+1})$, i.e., $\vx$ is shifted one position to the left. 
%Each input token  $t_i$ from vocabulary of size $V$ is 
%represented by embeddings in  $\mathbb{R}^{d_{\text{model}}}$. 
% corresponding to input token in a 
% Transformer uses   $\mathbb{R}^{d_{\text{model}}}$, where $d_{\text{model}}$ is the embedding dimensionality.
For each input token $t_i$, we retrieve its corresponding embedding in  $\mathbb{R}^{d_{\text{model}}}$ from a learnable embedding matrix $\mE_{\text{input}} \in \mathbb{R}^{V \times d_{\text{model}}}$ with vocabulary of size $V$. Similarly, the target output sequence $\vy$ is represented using a learnable embedding matrix $\mE_{\text{output}} \in \mathbb{R}^{V \times d_{\text{model}}}$. Notably, since both $\mE_{\text{input}}$ and $\mE_{\text{output}}$ are learnable (unless they are tied to be equivalent), any token $t_i$ can have different embeddings in the input and output sequences. 
% \subsubsection{Normalized Transformer}
To measure token similarity during model training, the dot product between the corresponding embedding vectors is used. However, the norms of embedding vectors in the original Transformer are unconstrained, which can lead to inaccurate similarity estimation. 
To improve the accuracy of similarity estimation, we propose to normalize the embedding vectors stored in $\mE_{\text{input}}$ and $\mE_{\text{output}}$ after each step of the training algorithm. 

% \subsection{Loss function}
% \subsection{Output Logits}
% \subsubsection{Baseline Transformer}

The next token prediction  
% % $\hat{\vy}\leftarrow \mathcal{M}(\vx)$ 
% is constrained by the requirement that, to predict any token $\hat{y}_j$ at position $j$, $\mathcal{M}(\vx)$ can only utilize information from the first $j-1$ tokens. This
is enforced by causal masking (see Section \ref{section_selfattention}) to ensure that  no future tokens are considered. This allows the model to compute the prediction error for all $T$ tokens in parallel during training, while preserving the autoregressive nature of the task. 
After the Transformer processes the sequence $(\vx_1, \dots, \vx_{i-1})$, it produces an output vector $\vh_i \in \mathbb{R}^{d_{\text{model}}}$ for each $i$-th position in the predicted sequence. The logits $\vz_i \in \mathbb{R}^V$, representing the unnormalized probabilities for each token in the vocabulary, are computed using the output embedding matrix $\mE_{\text{output}}$:
   \begin{equation}
    \vz_i = \mE_{\text{output}} \vh_i 
    \label{eq:logit}
    \end{equation}
The logits $\vz_i$ are passed through a softmax function to convert them into probabilities:
\begin{equation}
P(y_i | \vx_1, \dots, \vx_{i-1}) = \frac{\exp(z_{i,y_i})}{\sum_{v=1}^{V} \exp(z_{i,v})}
\label{eq:prob}
\end{equation}
Here, $z_{i,y_i}$ is the logit corresponding to the correct token $y_i$, and the denominator normalizes the logits into a probability distribution over the  vocabulary. During inference, the prediction $\hat{\vy}_i$ is obtained by selecting the token with the highest probability.
% The total loss $\mathcal{L}$ is calculated as the average cross-entropy loss across all tokens in the sequence:
% \begin{equation}
% \mathcal{L} = \frac{1}{T} \sum_{j=1}^{T} \mathcal{L}_j = - \frac{1}{T} \sum_{j=1}^{T} \log P(y_j | x_1, \dots, x_{j-1})
% \label{eq:loss}
% \end{equation}
% \subsubsection{Normalized Transformer}
Since all nGPT embeddings are normalized, the logits $\vz \in \mathbb{R}^V$ in \eqref{eq:logit} represent dot products bounded in the range $[-1,1]$. This limits the confidence (temperature) of the probability distribution generated by the softmax in \eqref{eq:prob}. 
To adjust this during training, we introduce a trainable scaling parameter $\vs_{z}\in \mathbb{R}^V$ that scales the logits element-wise:
\begin{align}
    \vz \leftarrow \vz \vs_{z}\label{eq:sscalingnew}
\end{align}

\subsection{Layers and Blocks}

\subsubsection{Baseline Transformer}
$L$ layers of transformations are applied to the hidden state $\vh$, consisting of alternating the self-attention (ATTN) and multi-layer perceptron (MLP) blocks:
\begin{align}
    \vh &\leftarrow \vh + \text{ATTN}(\text{RMSNorm}(\vh)) \label{eq:ATTN} \\ 
    \vh &\leftarrow \vh + \text{MLP}(\text{RMSNorm}(\vh)),  \label{eq:MLP}
\end{align}
where RMSNorm($\vh$) is one of several possible normalizations. It is used  first to normalize each embedding to a norm of $\sqrt{d_{\text{model}}}$, then scales each dimension by a learnable vector of $d_{\text{model}}$ factors, typically initialized to 1.
Since the transformation block outputs are added to $\vh$, the token embedding norms can vary significantly. To address this, normalization is also applied after the final layer.

\subsubsection{Normalized Transformer}

%To maintain the norm of $h$, we propose to normalize the outputs of the transformation blocks to ensure that their recombinations with $h$ stay on the hypersphere. 
For any points $\va$ and $\vb$ on the surface of a hypersphere in $\mathbb{R}^{d_{\text{model}}}$, SLERP (Spherical Linear Interpolation) by \citet{shoemake1985animating} computes an  interpolation along the geodesic (shortest path): 
\begin{align}
\text{SLERP}(\va, \vb; \alpha) = \frac{\sin((1 - \alpha) \theta)}{\sin(\theta)} \va + \frac{\sin(\alpha \theta)}{\sin(\theta)} \vb \label{eq:slerp}
\end{align}
where $\theta = \arccos(\va \cdot \vb)$ is the angle between the points $\va$ and $\vb$, and $\alpha \in [0, 1]$ is the interpolation parameter, with $\alpha = 0$ returning $\va$ and $\alpha = 1$ returning $\vb$. Our experiments suggest that SLERP can be approximated by simple linear interpolation (LERP):
\begin{align}
\text{LERP}(\va, \vb; w) = (1 - \alpha)  \va + \alpha \vb \label{eq:lerp}
\end{align}
Let us rewrite this equation as an update equation in nGPT:
\begin{align}
\va \leftarrow \va + {\alpha} (\vb - \va)  \label{eq:geberalupdate}
\end{align}
where $\va$ is $\vh$, and, $\vb$ is the point suggested by the attention or MLP block. Then, for the gradient $\vg=\va-\vb$, a more general form involving a variable matrix $\mB\in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$ becomes:
%\begin{align}
%\va \leftarrow \va - {\alpha} \vg(\va) 
%\end{align}
%or in its more general variant involving a variable matrix $\mB\in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$ as 
\begin{align}
\va \leftarrow \va - \bm{\alpha} \mB \vg
\end{align}
In quasi-Newton methods, $\mB$ approximates the inverse Hessian matrix  $\mH^{-1}$.% whose diagonal elements are positive values. 
 When $\mB$ is diagonal with non-negative elements, $\alpha \mB$ becomes a vector $\bm{\alpha} \in \mathbb{R}_{\geq 0}^{d_{\text{model}}}$ 
 whose elements correspond to the diagonal of $\mB$ times the learning rate $\alpha$. We denote $\bm{\alpha}$ as \textbf{eigen }learning rates (from the German word eigen, meaning "own," referring to the internal structure of the Transformer). We provide some notes in Appendix \ref{appendix:elrs}. 
%In quasi-Newton methods, $\mB$ approximates the inverse Hessian matrix  $\mH^{-1}(\va)$ whose diagonal elements are positive values  if the function is locally convex near $\va$  which is the assumption of the Newton method ($\mH(\va)$ needs to be positive definite). However, the diagonal of $\mH^{-1}(\va)$ can have negative values if the objective function is non-convex and has saddle points. While quasi-Newton methods like BFGS try to ensure (e.g., via regularization) that $\mB$ is positive-definite even on non-convex problem, some Riemannian optimization methods can exploit negative curvature of $\mH^{-1}(\va)$ \citep{agarwal2021adaptive}. In our case, we have three dinstinct situations: 
%\begin{description}
%    \item \textbf{Gradient descent update} When $\mB$ is set to identity, we perform gradient descent update.
%    \item \textbf{Convex variable-metric update} When the diagonal of $\mB$ is constrained to have non-negative entries, we perform a variable-metric step assuming the function is locally convex. 
%    \item \textbf{Non-convex variable-metric update} When the diagonal of $\mB$ is unconstrained, we perform a variable-metric step which accepts that locally the function can be non-convex. 
%\end{description}
%In the normalized GPT, we do not constrain the diagonal entries of $\mB$, thus, allowing the training process to deal with non-convexity of the transformation blocks. When $\mB$ is diagonal, $\alpha \mB$ becomes a vector $\bm{\alpha}\in \mathbb{R}^{d_{\text{model}}}$ whose elements correspond to the diagonal of $\mB$ times the learning rate. We denote $\bm{\alpha}$ as \textbf{eigen }learning rates (from the German word eigen, meaning "own," referring to the internal structure of the Transformer). In contrast to the regular learning rates, eigen learning rates can be negative as a result of negative entries of the inverse Hessian. 
Following equation \ref{eq:geberalupdate}, the update equations for the attention and MLP blocks are as follows: 
\begin{align}
    \vh &\leftarrow \text{Norm}(\,\vh + \bm{\alpha_{\text{A}}}(\vh_{\text{A}}-\vh)\,) \label{eq:ATTNnew2} \\ 
    \vh &\leftarrow \text{Norm}(\,\vh + \bm{\alpha_{\text{M}}}(\vh_{\text{M}}-\vh)\,),  \label{eq:MLPnew2}
\end{align}
where $\bm{\alpha_{\text{A}}}\in \mathbb{R}_{\geq 0}^{d_{\text{model}}}$ and $\bm{\alpha_{\text{M}}}\in \mathbb{R}_{\geq 0}^{d_{\text{model}}}$ are learnable parameters applied to the normalized outputs of the attention and MLP blocks $\vh_{\text{A}}=\text{Norm}(\text{ATTN}(\vh))$ and $\vh_{\text{M}}=\text{Norm}(\text{MLP}(\vh))$, respectively. The function $\text{Norm}(\vx)$ normalizes any vector $\vx$ to have unit norm, and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. The normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold. Appendix \ref{appendix:riem} discusses the extension of our update equations in the context Riemannian optimization. 
In contrast to the baseline Transformer, no additional normalization is required after the final layer, as the embeddings are already normalized by the proposed scheme.



\subsection{Self-Attention Block}
\label{section_selfattention}

\subsubsection{Baseline Transformer}

 The attention  mechanism is a key component of the Transformer. It allows each token to attend to every other token in the sequence, enabling the model to capture long-range dependencies. The block typically starts with a normalization of the input hidden state $\vh$ using RMSNorm to deal with fluctuating norms of embeddings. Then, the normalized $\vh$ is projected into three separate vectors - the query $\vq$, the key $\vk$, and the value $\vv$:
\begin{align}
     \vq \leftarrow \vh \mW_q, \vk \leftarrow \vh \mW_k, \vv \leftarrow \vh \mW_v\label{eq:QKV}
\end{align}
where $\mW_q, \mW_k, \mW_v \in \mathbb{R}^{d_{\text{model}} \times d_k}$ are learned projection matrices, and $d_k$ is the dimensionality of the query/key vectors. To incorporate positional information, we apply Rotary Position Embeddings (RoPE) by \citet{su2024roformer} to both the query and key vectors. The attention scores are computed by taking the dot product of the query and key vectors, scaling them by $\frac{1}{\sqrt{d_k}}$, then applying a softmax function to obtain attention weights, and finally computing a weighted sum of the value vectors $\vv$:
\begin{align}
     \text{Attention}(\vq, \vk, \vv) \leftarrow \text{softmax}\left( \frac{\vq\vk^\top}{\sqrt{d_k}} + \mM\right)\vv, \label{eq:attQKV}
\end{align}
where $\mM$ is a matrix that prevents attending to future tokens by setting the corresponding entries to $-\infty$. Specifically, $\text{M}_{i,j} = 0$ if $j \leq i$ and $\text{M}_{i,j} = -\infty$ if $j > i$.

 In practice, $n_{\text{heads}}$ attention heads are used where for each $i$-th head, separate linear projections $\mW_q^i, \mW_k^i, \mW_v^i$ are applied, and the attention mechanism is computed independently for each head:
\begin{align}
     \vh_{\text{A}}  \leftarrow \text{Concat}(\text{head}_1, \dots, \text{head}_{n_{\text{heads}}}) \mW_o \label{eq:multihead}
\end{align}
where $\text{head}_i = \text{Attention}(\vq^i, \vk^i, \vv^i)$ and $ \mW_O \in \mathbb{R}^{n_{\text{heads}} \times d_k \times d_{\text{model}}}$ is a learned projection matrix, where $d_k$ is typically set to $d_{\text{model}} / n_{\text{heads}}$.

\subsubsection{Normalized Transformer}

The matrix-vector multiplication of $\mW_{q}\in \mathbb{R}^{  d_{\text{model}} \times d_k}$ of the $i$-th head\footnote{All the following equations are defined per head but we omit $i$-th head index for the sake of readability.}  and $\vh\in \mathbb{R}^{d_{\text{model}}}$ can be viewed as a dot product between the columns of $\mW_{q}$ and $\vh$. %:
%\begin{align}
%\vq = \mW_{q} \vh = \begin{bmatrix}
%(\mW_{q})_1 \cdot \vh \\
%(\mW_{q})_2 \cdot \vh \\
%\vdots \\
%(\mW_{q})_{d_k} \cdot \vh
%\end{bmatrix} \in \mathbb{R}^{d_k} \label{eq:Wqh}
%\end{align}
In the baseline Transformer, all matrices, including  $\mW_{q}$ are unconstrained, leading to unbounded values in $\vq$. We propose to normalize $\mW_{q}$, $\mW_{k}$, $\mW_{v}$ and $\mW_{o}$ along their embedding dimension so that the computed dot products with $\vh$ can be interpreted as cosine similarity between unit norm vectors bounded in $[-1,1]$. Thus, all attention matrices can be viewed as collections of normalized embedding vectors to be compared with. 

While each element of $\vq$ and $\vk$ is now bounded, the norms of these two vectors can still vary. Moreover,  injection of positional information by RoPE further distorts $\vq$ and $\vk$. We propose to additionally normalize $\vq$ and $\vk$, ensuring that the dot product of every query and key is under control: 
\begin{align}
    \vq &\leftarrow \text{Norm}(\vq) \vs_{qk}  \label{eq:qkscaling1} \\ 
    \vk &\leftarrow \text{Norm}(\vk) \vs_{qk}, \label{eq:qkscaling2}
\end{align}
where $\vs_{qk}\in \mathbb{R}^{d_{\text{k}}}$ is a vector\footnote{There is no need for separate scaling factors for $\vq$ and $\vk$ as the scaling would simply be applied element-wise when computing the dot product between  $\vq$ and $\vk$.}  of trainable scaling factors for the $i$-th head. 

In the original Transformer, the softmax scaling factor $1/\sqrt{d_k}$ in \eqref{eq:attQKV} is introduced to account for the expected variance  of $d_k$ in the dot product of non-normalized query and key vectors. In the normalized Transformer, the expected variance of the dot product between normalized query and key vectors is $1/d_k$. To restore a variance of 1, the softmax scaling factor should instead be  $\sqrt{d_k}$. If the softmax scaling factor is set to 1, this is equivalent to initializing the scaling factors $\vs_{qk}$ at $d^{1/4}_k$. 

\subsection{MLP Block}

\subsubsection{Baseline Transformer}

The input hidden state $\vh$ of the MLP block is first normalized using RMSNorm and then passed through two separate linear projections, producing two intermediate vectors (we omit bias terms):
\begin{align}
    \vu \leftarrow \vh \mW_u, \quad \bm{\nu} \leftarrow \vh \mW_\nu \label{eq:mlpUV}
\end{align}
where $\mW_u, \mW_\nu \in \mathbb{R}^{d_{\text{model}} \times d_{\text{MLP}}}$ are the learned weight matrices.  The intermediate vectors $\vu$ and $\bm{\nu}$ are combined using a gated activation function called SwiGLU defined by \citet{shazeer2020glu} as:
\begin{align}
    \text{SwiGLU}(\vu, \bm{\nu}) \leftarrow \vu \cdot \text{SiLU}(\bm{\nu}) \label{eq:mlpGating}
\end{align}
where $\text{SiLU}(\bm{\nu}) = \bm{\nu} \cdot \sigma(\bm{\nu})$, and $\sigma(\bm{\nu})$ is the sigmoid function. The result of the gated activation is then passed through a final linear transformation $\mW_{o\text{MLP}}  \in \mathbb{R}^{d_{\text{MLP}} \times d_{\text{model}}} $:
\begin{align}
    \vh_{\text{M}} \leftarrow \text{SwiGLU}(\vu, \bm{\nu}) \mW_{o\text{MLP}} \label{eq:mlpO}
\end{align}

\subsubsection{Normalized Transformer}

We propose to normalize matrices $\mW_u$ and $\mW_\nu$ along the embedding dimension so that the $\vu$ and $\bm{\nu}$ vectors represent the cosine similarity between $\vh$ and vectors stored in $\mW_u$ and $\mW_\nu$, respectively. 
To control their impact, we introduce scaling factors  $\vs_{u}\in\mathbb{R}^{d_{\text{MLP}}}$ and $\vs_{\nu}\in\mathbb{R}^{d_{\text{MLP}}}$: 
\begin{align}
    \vu &\leftarrow \vu \vs_{u}, \label{eq:mlpscaling1} \\ 
    \bm{\nu} &\leftarrow \bm{\nu} \vs_{\nu} \sqrt{d_{model}}, \label{eq:mlpscaling2}
\end{align}
where the rescaling of $\bm{\nu}$ by $\sqrt{d_{model}}$ is needed to benefit from the non-linearity of $\text{SiLU}$ (see the Appendix \ref{appendix:mlprescaling}). The output of the MLP block is invariant to rescaling of $\vu$ by a scalar. %Therefore, for numerical purposes, $\sqrt{d_{model}}$ can be used to rescale not only $\vv$ but also $\vu$. 

\subsection{Effective learning rates in Adam}
The core of the Adam algorithm by \citet{kingma2014adam} is as follows: 
\begin{align}
    \vm &\leftarrow \beta_1 \vm + (1 - \beta_1) \vg  \notag \\ 
    \vv &\leftarrow \beta_2 \vv + (1 - \beta_2) \vg^2  \notag \\ 
    \bm{\theta} &\leftarrow \bm{\theta} - \alpha \vm / (\sqrt{\vv} + \epsilon) , \label{eq:qkscaling}
\end{align}
where $\bm{\theta}$ is the parameter vector, $\vg$ is the batch gradient, $\vm$ is the momentum, $\vv$ is the estimate of the per-element gradient amplitudes, $\alpha$ is the scheduled learning rate, $\epsilon$ is a small constant, and $\beta_1<\beta_2$ are momentum factors close to 1. We cite the text of the original Adam paper using our notation: \textit{In more common scenarios, we will have that} $ \frac{m}{\sqrt{v}} \approx \pm 1 $ \textit{since} $\left| {\mathbb{E}[g]}/{\sqrt{\mathbb{E}[g^2]}} \right| \leq 1 $. \textit{The effective magnitude of the steps taken in parameter space at each timestep is approximately bounded by the stepsize setting} $\alpha$. Thus, $\alpha$ controls the effective step-size in the search space, while the ratio $\frac{m}{\sqrt{v}}$ can temporarily increase (respectively, decrease) the step-size if the current amplitude of per-parameter momentum is greater (respectively, smaller) than its estimated value over longer time horizon. 
Consider an example where  $\theta_i = 0.01$ and the global learning rate is 0.001. If the gradient amplitude remains stable (i.e., $ \frac{m_i}{\sqrt{v_i}} \approx 1 $), it would take $\frac{0.02-0.01}{0.001}=10$ steps to double $\theta_i$. However, if $\theta_i=1.0$, it would take $\frac{2.0-1.0}{0.001}=1000$  steps to double. Even if the gradient's amplitude is larger in the second case, the number of steps would only decrease if ${m_i}>\sqrt{v}$. 


In nGPT, for any trainable vector of scaling parameters such as $\vs_a$, we use two scalars $\vs_{a,init}$ and $\vs_{a,scale}$. When initializing $\vs_a$ as a trainable parameter, its initial value is set to $\vs_{a,scale}$. However, during the forward pass we restore its actual value by multiplying $ s_{a,init}/s_{a,scale}$. This allows us to control the effective learning rate for $\vs_{a}$ by adjusting $\vs_{a,scale}$, while keeping the global learning rate unchanged. For example, setting $\vs_{a,init}=1$ and $\vs_{a,scale}=1/\sqrt{d_{model}}$ ensures that this parameter is updated with the same effective learning rate as other normalized parameters in the network.

\subsection{Summary of modifications}
\label{section_summary}
The recipe to convert the baseline Transformer into the normalized Transformer is as follows: 

\begin{enumerate}
    \item Remove all normalization layers such as RMSNorm or LayerNorm. 
    \item After each training step, normalize all matrices ($\mE_{\text{input}}$, $\mE_{\text{output}}$, $\mW_q$, $\mW_k$, $\mW_v$, $\mW_o$, $\mW_u$, $\mW_\nu$ and $\mW_{o\text{MLP}}$) along their embedding dimension. 
    \item Replace the update equations \ref{eq:ATTN} and \ref{eq:MLP} by equations \ref{eq:ATTNnew2} and \ref{eq:MLPnew2}, where $\bm{\alpha}_{\text{A}}$ (and also $\bm{\alpha}_{\text{M}}$) is treated with $\bm{\alpha}_{\text{A},init}=0.05$ (in order of $1/n_{layers}$) and $\bm{\alpha}_{\text{A},scale}=1/\sqrt{d_{model}}$.
    \item Change the softmax scaling factor in attention from $1/\sqrt{d_k}$ to $\sqrt{d_k}$. Implement the rescaling and normalization (normalization here is optional) of $\vq$ and $\vk$  as in equations \ref{eq:qkscaling1} and \ref{eq:qkscaling2}, where $\vs_{qk}$ is treated with $\vs_{qk,init}=1$ and $\vs_{qk,scale}=1/\sqrt{d_{model}}$. 
    \item Implement the rescaling of the intermediate state of the MLP block using equations \ref{eq:mlpscaling1} and \ref{eq:mlpscaling2}, where $\vs_{u}$ (and also $\vs_{\nu}$) is treated with $\vs_{u,init}=1$ and $\vs_{u,scale}=1$
    \item Implement the rescaling of logits using equation \ref{eq:sscalingnew}, where $\vs_z$ is treated with $\vs_{z,init}=1$ and $\vs_{z,scale}=1/\sqrt{d_{model}}$.
    \item Remove weight decay and learning rate warmup. 
\end{enumerate}

\section{Experiments}
\label{section_experiments}
We train both the baseline Transformer (GPT) and the normalized Transformer (nGPT) on the OpenWebText dataset \citep{Gokaslan2019OpenWeb} and evaluate them on a set of standard downstream tasks. We experiment with models containing  0.5B and 1B parameters, including the embeddings. For both GPT and nGPT, we report results using the best initial learning rate settings (see Appendix \ref{appendix:init_learning}). A detailed description of the setup and hyperparameters is in Appendix \ref{appendix:expsetup}.

\subsection{Acceleration of training}
\begin{figure*}[h]
     \centering
    \includegraphics[width=0.34\textwidth]{conv1B.eps} % Path to your .eps file :-)
    \caption{Validation loss during training of 1B GPT and nGPT with 4k context length.}
    \label{figure_conv1B}
\end{figure*}


Figure \ref{figure_conv1B} presents the  validation loss during the training of GPT and nGPT models with 1 billion parameters and a sample length of 4k tokens. After 20k iterations, nGPT achieves the same validation loss that GPT reaches only after 200k iterations (approximately 400 billion tokens), demonstrating a 10x speedup in terms of iterations and tokens used.\footnote{ While the time per step of nGPT is higher (80\% - for 4k, and 60\% - for 8k context respectively), it can be reduced after code optimization. Also the overhead is less significant for larger networks (see Appendix \ref{appendix:timecost}).} 

\begin{figure*}[h]
\begin{center}
    \includegraphics[width=1.0\textwidth]{loss.eps} 
\caption{
Final validation loss (\textbf{y-axis}) for training runs with different computation budgets in tokens (\textbf{x-axis}). The training of 0.5B and 1B nGPT models is about 4x, 10x and 20x faster (in terms of tokens) on 1k, 4k and 8k context lengths, respectively.}
\label{figure_scalingLoss}
%\vspace*{-0.25cm}
\end{center}
\end{figure*}

Figure \ref{figure_scalingLoss} illustrates how the performance gap between nGPT and GPT scales across three axes: total token budget, context length, and network size. Training the 0.5B and 1B nGPT models is approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively. 


Figure \ref{figure_scalingTasks4k} shows a similar pattern across downstream tasks, confirming that the acceleration is not only reflected in perplexity but also in task performance. Figures \ref{figure_scalingTasks1k} and \ref{figure_scalingTasks8k} in the Appendix provide results for 1k and 8k context lengths. We observe some saturation for the longest runs of nGPT, suggesting that the model capacity is nearly reached for this number of trainable model parameters.


\FloatBarrier % Ensures that the next figure starts on the same page if possible

\begin{figure*}[!t]
\begin{center}
    \includegraphics[width=1.0\textwidth]{4Ktasks.eps} 
\caption{\label{fig:f1} Models trained with 4k context length. Final performance (\textbf{y-axis}) on a set of downstream tasks and their average value (\textbf{Bottom-Right}) for different computation budgets in tokens (\textbf{x-axis}).}
\label{figure_scalingTasks4k}
%\vspace*{-0.25cm}
\end{center}
\end{figure*}

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.85\textwidth]{embeds.eps} 
\caption{\label{fig_embeddings} \textbf{Left}: Distribution of norms of vectors  from  input (\textbf{Top line}) and output (\textbf{Bottom line}) embedding matrices. \textbf{Middle}: Distribution of eigenvalues divided by its median value. \textbf{Right}: Pairwise distribution of dot products between embeddings. Models are trained for 100k iterations.}
%\vspace*{-0.25cm}
\end{center}
\end{figure*}

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.85\textwidth]{condnums.eps} 
\caption{\label{fig_cond} Median condition numbers  for attention and MLP matrices at different layer depth (24 and 36 layers for 0.5B and 1B models, respectively).  Models are trained for 100k iterations.}
%\vspace*{-0.25cm}
\end{center}
\end{figure*}


\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.85\textwidth]{newparamsmean.eps} 
\caption{\label{fig_newparams} (\textbf{Left}): Eigen learning rates of Attention and MLP blocks. (\textbf{Middle}): Scaling factors applied to the intermediate states of MLP. (\textbf{Right}): Scaling factors applied before the QK dot product; distribution of per-vector scalings applied to logits. Models are trained for 100k iterations.}  
%\vspace*{-0.25cm}
\end{center}
\end{figure*}

% \section{Experiments}
% \label{section_experiments}






 

\subsection{Inspection of network parameters}

Figure \ref{fig_embeddings} shows that, while nGPT maintains a fixed norm for embeddings (by design), GPT exhibits significant variation. The distribution of eigenvalues, computed from the covariance matrix of embeddings and normalized by their median, reveals that GPTâ€™s input embeddings have a higher condition number, especially in the 1B model. The distribution of pairwise dot products between embeddings indicates that even in nGPT, embeddings are not uniformly distributed across the hypersphere (where the dot product would approach 0), but instead form clustersâ€”possibly reflecting natural patterns in language data. Dot products in GPT tend to have higher values due to its embeddings forming a hyper-ellipsoid, as suggested by the spread of vector norms. The ill-conditioned nature of GPT's input embeddings could lead to computational issues involving these embeddings.

Figure \ref{fig_cond} shows the median condition numbers (across heads) for attention and MLP matrices at different layer depthsâ€”24 layers for the 0.5B model and 36 layers for the 1B model. GPT models exhibit significantly higher condition numbers in their attention matrices compared to nGPT.
A closer inspection of these matrices (the 3rd and 4th layers are in Figure \ref{fig_qkLayer3} and Figure \ref{fig_qkLayer4} of Appendix) suggests that they degenerate into lower-rank matrices, potentially reducing the learning capacity of these blocks. One could argue that the elevated condition numbers are influenced by the norms of the vectors in these matrices. Our post-training normalization of  these matrices is depicted by the dotted lines in Figure \ref{fig_conddenorm} of the Appendix. While the adjusted condition numbers are reduced, they remain higher than those for nGPT, indicating potential rank deficiency. The need for such normalization highlights one of the issues that nGPT is specifically designed to address. 


An important contribution of this work is the decoupling of predictions made by the Attention and MLP blocks from their impact on the hidden state  $\vh$. These contributions are controlled by the eigen learning rates $\bm{\alpha}_{\text{A}}$ and $\bm{\alpha}_{\text{M}}$. Their interpretation is straightforward: if $\bm{\alpha}_{\text{A},i}$ for an embedding dimension $i\in\mathbb{R}^{d_{\text{model}}}$ is 0.2, then the update follows  $h_i\leftarrow (1-0.2)h_i + 0.2 h_{\text{A},i}$. Thus, they directly quantify the contribution of $h_{\text{A},i}$ into $\vh$. Figure \ref{fig_newparams} shows the average absolute values of  $\vh_{\text{A}}$ and $\vh_{\text{M}}$ at each layer. Notably, the network learns to take only modest steps (20\%-30\%) in the direction suggested by $\vh_{\text{A}}$ and $\vh_{\text{M}}$. The average magnitude of $\bm{\alpha}_{\text{A}}$ decreases from 0.25 in the 0.5B network (24 layers) to 0.20 in the 1B network (36 layers). Meanwhile, $\bm{\alpha}_{\text{M}}$ decreases from 0.37 to 0.32, possibly because MLP blocks have more parameters, making their suggestions more precise. 

The scaling factors $\vs_u$, $\vs_{\nu}$ and $\vs_{qk}$ remain relatively stable across layers. The value of  $\vs_{\nu}$ can be interpreted as a measure of the non-linearity of the SiLU function, which behaves like ReLU for large $\vs_{\nu}$ and approximates a linear unit for values near 0 (see also Appendix \ref{appendix:mlprescaling}). The distribution of $\vs_z$ is primarily characterized by its mean, which influences the temperature of the softmax during cross-entropy calculations. The introduced scaling factors $\vs_{qk},\vs_u,\vs_{\nu}$ and $\vs_z$ seem to compensate for the removal of magnitude information when normalizing matrices and embeddings. 

\subsection{Ablation studies}

Appendix \ref{appendix:ablations} summarizes numerous ablation experiments. An important finding is that having fixed (non-learnable) values for $\vs_{qk},\vs_u,\vs_{\nu}$ and a single global learnable value for $\vs_z$ leads to only a slight degradation in accuracy. Therefore, our presented general case can be simplified and become easier to interpret. Appendix \ref{appendix:contextextra} demonstrates that nGPT can handle longer contexts 
without requiring any modifications to RoPE. 

\section{Related Work}
\label{section_relatedwork}

\citet{wang2020understanding} provides a comprehensive overview of the arguments for representation learning on the hypersphere. Spherical representations are associated with more stable training in the latent space of variational autoencoders \citep{xu2018spherical} and in embeddings used for face verification \citep{wang2017normface}. Notably, when embeddings are well clustered, they tend to be linearly separable from the rest of the embedding space \citep{wang2020understanding}. \citet{mettes2019hyperspherical} demonstrated that classification and regression can be unified by placing prototype embeddings uniformly on a hypersphere, allowing for separation with large margins a priori. \citet{wang2020understanding} found a strong empirical correlation between downstream task performance and both the alignment (closeness) and uniformity of embeddings on the hypersphere.

Since all embeddings in nGPT lie on the hypersphere, any update that causes the hidden state $\vh$ to deviate from the manifold is followed by a normalization step. This normalization can be interpreted as a retraction in the context of Riemannian optimization. One might attempt to approximate nGPT's update in GPT by applying RMSNorm both at the beginning and end of the block \citep{xiong2020layer}. However, this approach does not guarantee a fixed norm for the hidden state, nor does it ensure that the recombination approximates SLERP or LERP. 

The normalization in equations \ref{eq:qkscaling1} and \ref{eq:qkscaling2} closely resembles the QK normalization by \citet{henry2020query}. In nGPT, this process can be viewed as restoring $\vq$ and $\vk$ of the $i$-th head to a $(d_{\text{model}}/n_{\text{heads}})$-dimensional hypersphere after the projection of $\vh$ by $\mW_q$ and $\mW_k$, respectively. Since $\vh$ and the embedding dimensions of $\mW_q$ and $\mW_k$ are already normalized, the norms of $\vq$ and $\vk$ are also comparable, making their normalization potentially unnecessary. We explored this scenario (i.e., omitting the normalization of $\vq$ and $\vk$) in our ablation studies (see Appendix \ref{appendix:ablations}), where the results showed only a slight degradation. The performance drop was comparable to the computational time savings per step. Therefore, removing the normalization of $\vq$ and $\vk$ in nGPT is a viable option. 

%\section{Limitations}
%\label{section_limitations}

\section{Discussion and Conclusion}
\label{section_discussionconlclusion}
This work builds on numerous key findings and observations made in the field which directly \citep{wang2020understanding,xu2018spherical,wang2017normface} and indirectly \citep{salimans2016weight, franke2023cpr, kodryan2022training,kosson2023rotational} support representation learning on the hypersphere. One of our main contributions is the normalization of the embedding dimensions of all matrices, ensuring they reside on the same hypersphere. Crucially, we observed that such normalization alone would constrain the inputs of non-linear units, and, thus, the scaling factors for these units should be introduced. 

In line with recent studies suggesting that transformers implicitly perform gradient descent as meta-optimizers \citep{von2023transformers, dai2022can}, we explicitly demonstrate how this process occurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii) this information is multiplied by eigen learning rates to adjust the hidden state, and iii) the commonly used normalization can be interpreted as a retraction step in Riemannian optimization, projecting the point back onto the hypersphere. We believe we are the first to decouple the eigen learning rates from the rest of the network, recognizing them as trainable parameters that can be interpreted as the diagonal elements of a variable-metric matrix. In other words, the normalized Transformer functions as a variable-metric optimizer, searching for output solutions using data-driven gradient information estimated in its attention and MLP blocks. 

The spherical representation provides valuable insights into the internals of nGPT, enabling the collection and analysis of statistics about its normalized components. Most importantly, it allows for the application of mathematical techniques specifically designed for dealing with hyperspheres. We believe that the reported acceleration, by a factor from 4 to 20, is only the first step towards uncovering new algorithms and architectures that could emerge from nGPT. Future work should explore scaling nGPT to larger network sizes, real-world datasets, and a broader range of tasks.
For instance, the extension of nGPT to encoder-decoder and hybrid architectures \citep{dao2024transformers, de2024griffin} is straightforward. 

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix
\section{Appendix}

\subsection{Rescaling in the MLP block of the normalized Transformer}
\label{appendix:mlprescaling}

When computing SwiGLU using \eqref{eq:mlpGating}, each element $x$ of the vector $\vv$ is an input to SiLU: 

\begin{align}
    \text{SiLU}(x) = x \cdot \sigma(x) = x \cdot \frac{1}{1 + e^{-x}}
, \label{eq:silu}
\end{align}

where $\sigma(x)$ is sigmoid. For $x$ with large magnitude, $\text{SiLU}(x)$ approximates $\text{ReLU}(x)$: when $x \to -\infty$, $\text{SiLU}(x) \to 0$, and when $x \to \infty$, $\text{SiLU}(x) \approx x$. The minimum of $\text{SiLU}(x_{min})\approx-0.278$ is located at $x_{min}\approx-1.278$. While the elements of $\vv$ represent dot products of $d_{model}$-dimensional vectors and are bounded in $[-1,1]$,  their expected absolute value (when they are random) is $\mathbb{E}[|\cos(\theta)|] = \frac{2}{\pi} \cdot \frac{1}{\sqrt{d_{model}}}\approx\frac{0.7979}{\sqrt{d_{model}}}$. Thus, we should rescale $x$ by ${\sqrt{d_{model}}}$, otherwise, for very small $x$, we end up with $\text{SiLU}(x)\approx x/2$. An alternative view is to note that since the variance of each of the normalized vectors ($\vh$ and a vector from $\mW_v$) is  ${1/d_{model}}$, the variance of 1 (suitable for the sigmoid part of SiLU) can be restored by rescaling of $x$ by ${\sqrt{d_{model}}}$. Based on these two views, we rescale $\vv$ by $\sqrt{d_{model}}$ to benefit from the non-linearity of $\text{SiLU}$. 

\subsection{Eigen learning rates}
\label{appendix:elrs}

In the main text of the paper, we defined eigen learning rates as positive ($\bm{\alpha}\leftarrow|\bm{\alpha}|$ is used during the forward pass). However, when they are not constrained to be positive, we obtain experimental results which are the same (up to numerical difference). This surprising observation can be explained as follows. Both the attention and MLP blocks have transformation matrices at their outputs. When the search is unconstrained, it is sufficient for Adam to flip (select) the sign of the $i$-th row of the output transformation matrix $\mW_o$ to change the sign of the corresponding $i$-th coordinate in $\vh_{\text{A}}$. Thus, the transformation calculated as $\bm{\alpha_{\text{A}}}\mW_o$ is the same as $\bm{\alpha'_{\text{A}}}\mW'_o$, where 
$\bm{\alpha}'_{\text{A},i}=-\bm{\alpha}_{\text{A},i}$
and $\mW'_{o,(i,:)}=-\mW_{o,(i,:)}$. In other words, when unconstrained, we can arrive at exactly the same transformation by flipping the signs in both $\bm{\alpha}$ and $\mW_o$, which cancel each other. For simplicity and clearer interpretation, we suggest constraining $\bm{\alpha}$ to be positive in the main paper.

There is, however, another interesting scenario when eigen learning rate could become negative. 
In quasi-Newton methods, $\mB$ approximates the inverse Hessian matrix  $\mH^{-1}$ whose diagonal elements are positive values if the function is locally convex  which is the assumption of the Newton method ($\mH$ needs to be positive definite). However, the diagonal of $\mH^{-1}$ can have negative values if the objective function is non-convex and has saddle points. While quasi-Newton methods like BFGS aim to ensure (e.g., via regularization) that $\mB$ is positive-definite even on non-convex problem, some Riemannian optimization methods can exploit negative curvature of $\mH^{-1}$ \citep{agarwal2021adaptive}. When the diagonal of $\mB$ is unconstrained, we perform a variable-metric step, acknowledging that the function may be non-convex locally. We did not mention this in the main paper because, as noted earlier, the results with both constrained and unconstrained $\bm{\alpha}$ are essentially the same.


\subsection{Riemannian optimization}
\label{appendix:riem}



If $\vh-\vh_{\text{A}}$ is viewed as the gradient $\vg$ in the Euclidean space, then, aligning with the requirements of Riemannian optimization, the projection of $\vg$ onto the tangent space of the hypersphere is

\begin{align}
    \vg_{\text{proj}} &\leftarrow \vh(\vh^T\vh_{\text{A}}) - \vh_{\text{A}}  \label{eq:gproj}
\end{align}

The projection is equivalent to $\vg$ when the dot product $\vh^T\vh_{\text{A}}$ is 1. Depending on the alignment between $\vh$ and $\vh_{\text{A}}$, the projected gradient varies between $\vh-\vh_{\text{A}}$ (when the vectors are aligned) and $-\vh_{\text{A}}$ (when the vectors are orthogonal). The Riemannian variable-metric update then reads as: 

\begin{align}
    \vh &\leftarrow \text{Norm}(\,\vh - \mB_{\text{A}}(\vh(\vh^T\vh_{\text{A}}) - \vh_{\text{A}})\,) \label{eq:ATTNnew3} \\ 
    \vh &\leftarrow \text{Norm}(\,\vh - \mB_{\text{M}}(\vh(\vh^T\vh_{\text{M}}) - \vh_{\text{M}})\,)  \label{eq:MLPnew3}
\end{align}

The normalization by Norm can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold. 

Our experimental results suggest that the impact of $\vh^T\vh_{\text{M}}$ is negligible. Therefore, all experiments in the paper are based on equations \ref{eq:ATTNnew2} and \ref{eq:MLPnew2}.

\subsection{Time cost per step}
\label{appendix:timecost}

The time cost per step for nGPT is approximately 80\% higher with 4k context length, and 60\% higher with 8k context length. 
This overhead is not only due to nGPT having 6 normalization steps (2 of them are applied for $\vq$ and $\vk$) per layer instead of 2, but also because nGPT's normalizations are not yet fully optimized, unlike GPT, where normalization layers are fused with other operations. Training on larger networks is expected to further reduce this performance gap, as the number of layers (and thus the number of normalizations) increases only modestly with the number of network parameters.  
Appendix \ref{appendix:ablations} shows that we can remove the normalization of $\vq$ and $\vk$ with a minor negative impact on results. 

\subsection{Experimental setup}
\label{appendix:expsetup}

In all experiments, we use OpenWebText \citep{Gokaslan2019OpenWeb} dataset which, according to experiments of \cite{karpathy2023nanogpt},  represents a good approximation of the OpenAI's internal dataset used to train GPT-2 models. We are well aware that this dataset is not of the highest quality. However, we believe that it is suitable for academic research, and, should improve the comparability of our findings with other research papers. 

We trained our models using 64 A100 GPUs distributed across 8 nodes (8 GPUs per node).  Global batch size is 512. We use the LLaMA-2 tokenizer  with 32k tokens. We use the same setup for the 0.5B and 1.0B parameter models. 


\begin{table}[t]
\caption{Model Parameters for GPT and nGPT}
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{ l c c }
\toprule
\textbf{Model Parameter}        & \textbf{0.5B Models} & \textbf{1.0B Models} \\ 
\midrule
Number of Layers ($n_{\text{layers}}$)   & 24                 & 36                 \\ 
Model Dimension ($d_{\text{model}}$)     & 1024               & 1280               \\ 
Number of Attention Heads ($n_{\text{heads}}$) & 16            & 20                 \\ 
Key Dimension ($d_k$)                     & $d_{\text{model}}/n_{\text{heads}}$  & $d_{\text{model}}/n_{\text{heads}}$  \\

MLP Dimension ($d_{\text{MLP}}$)         & $4d_{\text{model}}$ & $4d_{\text{model}}$ \\ 
Parameters in GPT                         & 468.2M             & 1025.7M            \\ 
Parameters in nGPT                        & 468.4M             & 1026.1M            \\ 
\bottomrule
\end{tabular}}
\label{table_model_params}
\end{table}



\begin{table}[t]
\centering
\caption{Optimization Parameters for GPT and nGPT}
\begin{tabular}{ l c c }
\toprule
\textbf{Optimization Parameter}       & \textbf{GPT}                    & \textbf{nGPT}                    \\ 
\midrule
Optimizer                             & AdamW                            & Adam (AdamW with weight decay 0.0) \\ 
Weight Decay                          & 0.1                              & 0.0                               \\ 
Number of Warmup Steps                & 2000                             & 0                                 \\ 
Learning Rate Schedule                & Cosine Annealing                 & Cosine Annealing                  \\ 
Initial Learning Rate                   & problem-specific                                & problem-specific    \\
Final Learning Rate                   & 0                                & 0                                 \\ 
\bottomrule
\end{tabular}
\label{table_optimization_params}
\end{table}

All matrix parameters are initialized by sampling from a zero-mean normal distribution with a standard deviation of $0.02$ for GPT and $1/\sqrt{d_{\text{model}}}$ for nGPT. The standard deviation for the output matrices was scaled by a factor of $\sqrt{2 \times n_{\text{layer}}}$, as suggested by \cite{radford2019language}. The initialization of matrix parameters is not important for nGPT because they are normalized afterwards. The base of RoPE is 10000. The initialization of the additional parameters introduced in nGPT is  described in Section \ref{section_summary}.

\subsection{Selection of the initial learning}
\label{appendix:init_learning}

The initial learning rate is the only hyperparameter we tune for both GPT and nGPT. Figure \ref{figure_lrs} demonstrates our trials to select the most suitable initial learning rate for GPT and nGPT. Our first experiments started with the 0.5B model and  1k context length. After observing the general trend of these curves, we reduced the number of experiments and followed the trends to minimize the total compute used. Apart from estimating the optimal settings for the initial learning rate, our general observation is that the optimal learning rates for GPT and nGPT tend to be similar for the same values of the validation loss. Longer runs are usually associated with the possibility of achieving lower values of the validation loss, and this typically requires lower initial learning rates.

One artifact we observed is the increasing sensitivity of the 1B nGPT model on 8k context length. We found that the smoothness of the hyperparameter curves can be restored (see the dotted lines with squares) by increasing $\bm{\alpha}_{\text{A,init}}$ and $\bm{\alpha}_{\text{M,init}}$ from their default value of  
$1/\sqrt{d_{model}}$ to 0.1. This change decreases the effective learning rate of Adam on these variables by a factor of $(1/\sqrt{d_{model}})/0.1\approx 3$. As a result, the eigen learning rates are learned by Adam at a slower rate. 


\begin{figure*}[h]
\begin{center}
    \includegraphics[width=0.8\textwidth]{figlrs.eps} 
\caption{ Final validation loss values for different initial learning rates for the 0.5B models (\textbf{Top}) and 1B models (\textbf{Bottom}). GPT is denoted by solid lines with circles, while nGPT is represented by the dotted lines with stars. A specific case with a different setup, denoted by the dotted lines with squares (\textbf{Bottom Right}), is discussed in the text.}
\label{figure_lrs}
%\vspace*{-0.25cm}
\end{center}
\end{figure*}

\clearpage
\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.80\textwidth]{1Ktasks.eps} 
\caption{\label{fig:f2} Models trained with 1k context length. Final performance (\textbf{y-axis}) on a set of downstream tasks and their average value (\textbf{Bottom-Right}) for different computation budgets in tokens (\textbf{x-axis}).}
\label{figure_scalingTasks1k}
%\vspace*{-0.25cm}
\end{center}
\end{figure*}

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.80\textwidth]{8Ktasks.eps} 
\caption{\label{fig:f3} Models trained with 8k context length. Final performance (\textbf{y-axis}) on a set of downstream tasks and their average value (\textbf{Bottom-Right}) for different computation budgets in tokens (\textbf{x-axis}).}
\label{figure_scalingTasks8k}
%\vspace*{-0.25cm}
\end{center}
\end{figure*}
\clearpage
\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.8\textwidth]{condnums_denorm.eps} 
\caption{\label{fig_conddenorm} Median condition numbers measured for attention and MLP matrices at different layer depth (24 and 36 layers for 0.5B and 1B networks, respectively). The dotted lines are for the case when GPT's matrices are renormalized after training. Models are trained for 100k iterations.}
%\vspace*{-0.25cm}
\end{center}
\end{figure*}



\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.6\textwidth]{qkvL2.eps} 
\caption{\label{fig_qkLayer3} \textbf{Left}: Distribution of the norms of vectors forming the complete attention matrices (not per head).
\textbf{Right}: Distribution of singular values for all heads. The results are shown for the 3rd layer.}  
%\vspace*{-0.25cm}
\end{center}
\end{figure*}

\clearpage
\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.6\textwidth]{qkvL3.eps} 
\caption{\label{fig_qkLayer4} \textbf{Left}: Distribution of the norms of vectors forming the complete attention matrices (not per head).
\textbf{Right}: Distribution of singular values for all heads. The results are shown for the 4th layer.} 
%\vspace*{-0.25cm}
\end{center}
\end{figure*}

%\begin{figure*}[t]
%\begin{center}
%    \includegraphics[width=1.0\textwidth]{condnums_denorm.eps} 
%\caption{\label{fig:f} Layer\#4}
%%\vspace*{-0.25cm}
%\end{center}
%\end{figure*}
\subsection{Length Extrapolation Ability}
\label{appendix:contextextra}
We investigate the length extrapolation ability of nGPT by evaluating its perplexity on the PG19 dataset, as shown in Figure~\ref{figure_pg19}. In standard GPTs, perplexity tends to increase dramatically when tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity range even at extrapolated lengths. This result demonstrates that nGPT can handle longer contexts without requiring any modifications to RoPE, providing a clear advantage over standard GPTs in terms of length extrapolation.

\begin{figure*}[h] % Adjust the width as needed
    \centering
    \includegraphics[width=0.5\textwidth]{pg19-ppl.eps} % Path to your .eps file
    \caption{PG19 perplexity from 1K to 32K.}
    \label{figure_pg19}
\end{figure*}

\clearpage 
\subsection{Ablation studies}
\label{appendix:ablations}
We perform a series of ablations in nGPT, focusing on the selection of scaling factors $\vs_{qk}$, $\vs_{u}$ ($\vs_{v}$), $\vs_{z}$, as well as evaluating the necessity of normalization. For the scaling factors, we first examine the impact of different initializations of these parameters. Additionally, we explore whether these factors can be simplified to a learnable scalar or a fixed value. For normalization, we analyze whether the removal of QK-normalization is a feasible alternative. To ensure a fair comparison, all our ablation models have size 0.5B and context length 1k and are trained with learning rate $1 \times 10^{-3}$ for 100k iterations ($\sim$52B tokens).

Table~\ref{ablation-init} presents various combinations of $\vs_{init}$ and $\vs_{scale}$ for each scaling factor, as well as the downstream task accuracy and validation loss. Additionally, we report the mean value of each scaling factor distribution to show the final converged scaling value after training. For $\vs_{qk}$, we observe that the mean value (Mean($\vs$)) is relatievly stable with value around 1 across most initialization settings, except when using smaller $\vs_{init}$ and larger $\vs_{scale}$, which results in a Mean($\vs$) value less than 1. For $\vs_{u}$ and $\vs_{v}$, changes in initialization lead to significant shifts in Mean($\vs$), accompanied by increases in final validation loss. For $\vs_{z}$, we see a substantial degradation in both accuracy and loss under certain initializations. %Overall, these results suggest that careful tuning of the hyperparameters for $\vs_u$, $\vs_v$, and $\vs_z$ may be necessary to achieve optimal performance. 
Overall, these results suggest that the baseline hyperparameter settings are robust but additional tuning of $\vs_u$, $\vs_v$, and $\vs_z$ could potentially further improve performance. 

In Table~\ref{ablation-scalar}, we modify each learnable per-element vector of the scaling factors $\vs_{qk}$, $\vs_{u}$ ($\vs_{v}$), $\vs_{z}$, and the eigen learning rates $\bm{\alpha}_{A}$, $\bm{\alpha}_{M}$ to a single learned scalar or a fixed value. This ablation helps us determine whether these tuning parameters can be simplified in nGPT. From the table, we observe that most changes result in only negligible degradation ($\leq0.3\%$) in validation loss, and  some even lead to slight improvements in accuracy. Notably, replacing the per-element vector $\vs_{qk}$ with a single scalar has minimal impact on the mean value, while the mean value of $\vs_{u}$, $\vs_{v}$, $\vs_{z}$, $\alpha_{A}$, and $\alpha_{M}$ show a slight increase. Furthermore, even when fixing $\vs_{qk}$, $\vs_{u}$, and $\vs_{v}$, the model still achieves comparable accuracy and validation loss to the baseline using the per-element vectors.

In Table~\ref{ablation-norm}, we investigate the impact of removing certain normalization operations in nGPT to potentially reduce training time per step. We remove QK normalization by eliminating the normalization terms \textbf{Norm} in equations~\ref{eq:qkscaling1} and~\ref{eq:qkscaling2}. The results show that this modification lead to similar accuracy and loss compared to the baseline, indicating that they are effective options for reducing computational overhead without compromising performance.
 \clearpage
\input{ablation_init}
\input{ablation_scalar}
\input{ablation_norm}


\clearpage
\subsection{Analysis of scaling parameters}
\label{appendix:scalinganalysis}
In nGPT, we introduce a total six trainable parameters: the eigen learning rates $\bm{\alpha}_A$ and $\bm{\alpha}_M$, along with the scaling factors $\vs_{qk}$, $\vs_u$, $\vs_v$ and $\vs_z$. Since these parameters are updated using gradients, we are interested in understanding their learned distributions after training. In Figure~\ref{analysis-scaling}, we present the histograms of these saved weights combining across layers and analyze their distributions under different context lengths, model sizes, learning rates, and numbers of training tokens. 

For $\bm{\alpha}_A$ and $\bm{\alpha}_M$, we observe that their distributions remain stable across various learning rates. However, increasing the context length or the number of training tokens tends to shift the distributions to the right, indicating that the hidden state $\vh$ requires more transformation from the Attention and MLP blocks to handle the larger inputs. Additionally, the mean values for these parameters tend to be smaller in larger models because we have more layers to update the hidden state. 

Regarding the scaling factors, $\vs_{qk}$ is quite stable under all the conditions, suggesting that a fixed value, similar to the softmax scaling factor in original transformer, may be sufficient. Interestingly, we find that $\vs_{qk}$ has a high density close to zero, indicating the sparsity of attention in nGPT. For $\vs_u$ and $\vs_v$, their distributions shift left as the context length increases, but shift right as the model size or the number of training tokens increases. Furthermore, these two factors become flatter when using larger learning rate. Lastly, for $\vs_{z}$, we see higher mean values for longer context lengths, larger model sizes, and more training tokens, suggesting that the model may learn to use a lower temperature to make the final word distribution sharper.
\input{analysis}

\vspace{0.25cm}
\end{document}
