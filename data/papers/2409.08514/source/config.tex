\subsection{Datasets}
We trained and tested Apollo on the combined MUSDB18-HQ \cite{rafii2019musdb18} and MoisesDB \cite{pereira2023moisesdb} datasets. By integrating these two datasets, we leveraged their rich diversity and comprehensive musical resources to evaluate Apollo's restoration performance across different music genres more thoroughly. During the data preprocessing stage, inspired by music separation techniques \cite{uhlich2024sound,li2024subnetwork}, we employed a Source Activity Detector (SAD) to remove silent regions from the tracks, retaining only the significant portions for training. Throughout training, we implemented real-time data augmentation by randomly mixing tracks from different songs. Specifically, we randomly selected between 1 and 8 stems from 11 available tracks and extracted 3-second clips from each selected stem. These clips were then randomly scaled in energy within a range of [-10, 10] dB relative to their original levels. All selected stem clips were summed together to generate simulated music. Subsequently, we simulated dynamic bitrate scenarios by applying MP3 codecs with bitrates of [24000, 32000, 48000, 64000, 96000, 128000] to generate the compressed music. To ensure all samples were on the same scale, we rescaled both the target audio and the encoded audio based on the maximum absolute value.

\subsection{Hyperparameters}
For the proposed Apollo model, the Short-Time Fourier Transform (STFT) window length was set to 20 ms with a hop size of 10 ms, using a Hanning window. The bandwidth for frequency band segmentation was set to 160 Hz, and the feature dimension $N$ was set to 256. The Band Sequence modeling module was stacked $B = 6$ times. In the discriminator network, the STFT window sizes were configured with a multi-scale setup, including $[32, 64, 128, 256, 512, 1024, 2048]$. For the optimizer, both the generator and discriminator utilize the AdamW optimizer \cite{loshchilov2017decoupled}. The generator's initial learning rate was set to 0.001, with a weight decay of 0.01, while the discriminator's initial learning rate was set to 0.0001, with the same weight decay of 0.01. The learning rate decayed by 0.98 every two epochs, and gradient clipping with a maximum norm of 5 was employed to prevent gradient explosion. Additionally, we implemented an early stopping mechanism to prevent overfitting: training was terminated if the validation loss did not decrease for 20 consecutive epochs. All models were trained on 8 Nvidia RTX 4090 GPUs.

\subsection{Evaluation metrics}
In all experiments, we used the Scale-Invariant Signal-to-Noise Ratio (SI-SNR) \cite{le2019sdr}, Signal-to-Distortion Ratio (SDR) \cite{vincent2006performance}, and Virtual Speech Quality Objective Listener (VISQOL) \cite{hines2015visqol} to evaluate the quality of the reconstructed audio. To assess the model's efficiency, we reported the time consumption per second of audio processed by Apollo and SR-GAN (Real-Time Factor, RTF). RTF is calculated by processing 1-second audio tracks sampled at 44.1 kHz on both CPU and GPU, and the average value is taken after running 1000 iterations. Additionally, we measured the model size by reporting the number of parameters using the open-source tool PyTorch-OpCounter\footnote{\url{https://github.com/Lyken17/pytorch-OpCounter}}.