\section{Related Work}

Srivastava et al. \cite{srivastava2022beyond} observed that while accuracy at a particular task can empirically appear sharp and unpredictable, cross entropy does not; the authors then hypothesized that emergent abilities may be partially attributed to the metric.
%writing: ``[Emergent abilities frequently appear] on tasks that have brittle or narrow metrics for success, emphasizing the importance of engineering graded metrics that can capture subthreshold improvements. Our results suggest that breakthrough performance can also occur on tasks that involve multistep reasoning. One possible explanation for the breakthrough phenomenon on multistep tasks is that the probability of success on the task scales like the product of the success probabilities on each step.‚Äù
Our paper converts their discussion into precise predictions, then quantitatively tests the predictions to reveal that: metric choice is likely wholly responsible for emergent abilities; well-known and widely-used metrics (including ones already used by \cite{srivastava2022beyond}) capture graded improvements; emergent abilities do not appear only for tasks involving multiple steps, and indeed appear most commonly on the discontinuous Multiple Choice Grade; metric choice can be used to induce emergent abilities in a novel domain (vision) in diverse architectures and tasks.

Caballero et al. \cite{caballero2022broken} explain emergence by assuming a piece-wise power law functional form; under this view, emergent abilities are real, caused by a change in the governing power law. In contrast, our work suggests that emergent abilities are induced by the researcher, even under a single power law. Michaud et al. \cite{michaud2023quantization} posit that emergent abilities may be real under strong data assumptions.

\section{Discussion}

Our paper presents an alternative explanation for claimed emergent abilities of large language models. For a fixed task and a fixed model family, the researcher can choose a metric to create an emergent ability or choose a metric to ablate an emergent ability. Ergo, \textit{emergent abilities may be creations of the researcher's choices, not a fundamental property of the model family on the specific task.} We emphasize that nothing in this paper should be interpreted as claiming that large language models \textit{cannot} display emergent abilities; rather, our message is that previously claimed emergent abilities in \cite{brown2020language, ganguli2022predictability,srivastava2022beyond,wei2022emergent} might likely be a mirage induced by researcher analyses.

Our paper has several implications. Firstly, a task and a metric are distinct and meaningful choices when constructing a benchmark. Secondly, when choosing metric(s), one should consider the metric's effect on the per-token error rate and adapt their measuring process accordingly, e.g., if one chooses accuracy, one should make sure to have sufficient data to accurately measure accuracy to avoid the risk of drawing invalid scientific conclusions.
%Thirdly, what metrics \textit{should} one choose? If the goal is to measure how useful a model's outputs are to humans, then harsh metrics like Accuracy or Multiple Choice Grade may diverge from human preferences.
%For instance, suppose that Model A places 5\% probability mass on a Yes/No question's correct answer, and Model B places 40\% probability mass on the correct answer; under Multiple Choice Grade, these two models score equivalently: 0. To offer one real-world anecdote, while learning how to use BIG-Bench, the authors accidentally discovered within BIG-Bench a question ``Q: What is 4 plus 5?" and a model's answer ``The sum of 4 and 5 is 9" that was scored as 0 because regex is used to extract the first occurring integer.
%Consequently, determining to what extent common NLP metrics correlate with human preferences should be a priority to avoid overfitting to NLP metrics.
Thirdly, when making claims about capabilities of large models, including proper controls is critical. In this particular setting, emergent abilities claims are possibly infected by a failure to control for multiple comparisons. In BIG-Bench alone, there are $\geq$ 220 tasks, $\sim 40$ metrics per task, $\sim10$ model families, for a total of $\sim 10^6$ task-metric-model family triplets, meaning probability that \textit{no} task-metric-model family triplet exhibits an emergent ability by random chance  might be small.
Fourthly, scientific progress can be hampered when models and their outputs are not made public for independent scientific investigation.

% TODO: Decide whether to keep this or move to appendix. Our findings reveal that metrics exhibiting apparent emergence disproportionately penalize smaller-scale models. 
% Monitoring alternative metrics unveil consistent, predictable alterations as the model scales. 
% Consequently, small-scale experimentation remains valuable, provided appropriate metrics are employed to avoid undue penalization. 
% Specifically, GPT-4 development incorporated small-scaling experimentation alongside scaling laws [1].


% Commented out for NeurIPS

% \section{Contributions}

% RS conceived of the research direction, collected data, ran experiments and analyzed results. SK supervised and guided the project. BM also provided guidance.

% \section{Acknowledgements}

% We thank our colleagues Max Lamparth, Mikail Khona, Kateryna Pistunova, Victor Lecomte, and Zane Durante for discussing our findings with us and providing much appreciated feedback.
