\documentclass{article}
% \usepackage{neurips_2023}
\usepackage[preprint]{neurips_2023}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage[mathscr]{euscript}
\usepackage[numbers]{natbib} % has a nice set of citation styles and commands
\usepackage{mathtools} % amsmath with fixes and additions
\usepackage{bbm}
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
% \usepackage{algorithm2e}
\usepackage{boxedminipage}
\usepackage{alltt}
\usepackage{bm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{lineno}         % Enables line numbers
\usepackage{lipsum}
\geometry{
% a4paper,
 total={150mm,200mm},
 left=30mm,right = 30mm,
 top=30mm,bottom =30mm,
}

\usepackage{graphicx}

% \linenumbers

\DeclareMathOperator{\defeq}{\stackrel{\text{def}}{=}}
\DeclareMathOperator{\assumedeq}{\stackrel{\text{assumed}}{=}}
\DeclareMathOperator{\Tr}{\text{Tr}}
\DeclarePairedDelimiter{\nint}\lfloor\rceil

% Emergent Abilities Don't Exist
% 
\title{Are Emergent Abilities of Large Language Models a Mirage?}
\author[]{Rylan Schaeffer}
\author[]{Brando Miranda}
\author[]{Sanmi Koyejo}

\affil[]{Computer Science, Stanford University}


% \date{Winter 2023}

\begin{document}

\maketitle

% 2023/05/16 Abstract
% Recent work claims that large language models display \textit{emergent abilities}, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their \textit{sharpness}, transitioning seemingly instantaneously from not present to present, and their \textit{unpredictability}, appearing at seemingly unforeseeable model scales.
%     Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not.
%     Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale.
%     We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how similar metric decisions suggest apparent emergent abilities on vision tasks in diverse deep network architectures (convolutional, autoencoder, transformers).
%     In all three analyses, we find strong supporting evidence that emergent abilities may not be a fundamental property of scaling AI models.

\begin{abstract}
Recent work claims that large language models display \textit{emergent abilities}, abilities not present in smaller-scale models that are present in larger-scale models.
What makes emergent abilities intriguing is two-fold: their \textit{sharpness}, transitioning seemingly instantaneously from not present to present, and their \textit{unpredictability}, appearing at seemingly unforeseeable model scales.
Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due the researcherâ€™s choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance.
We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks.
Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.
\end{abstract}

\input{01_introduction}
\input{02_analytical_model.tex}
\input{03_gpt}
\input{04_emergent_paper}
\input{05_toy_networks}
\input{06_discussion}

\clearpage
% \bibliographystyle{plainnat}
\bibliographystyle{plain}
\bibliography{references}

\clearpage
\input{appendix}

\end{document}
