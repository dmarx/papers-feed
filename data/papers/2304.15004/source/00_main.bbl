\begin{thebibliography}{10}

\bibitem{anderson1972more}
Philip~W Anderson.
\newblock More is different: broken symmetry and the nature of the hierarchical
  structure of science.
\newblock {\em Science}, 177(4047):393--396, 1972.

\bibitem{brier1950verification}
Glenn~W Brier et~al.
\newblock Verification of forecasts expressed in terms of probability.
\newblock {\em Monthly weather review}, 78(1):1--3, 1950.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{caballero2022broken}
Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger.
\newblock Broken neural scaling laws.
\newblock {\em arXiv preprint arXiv:2210.14891}, 2022.

\bibitem{chan2022data}
Stephanie~CY Chan, Adam Santoro, Andrew~Kyle Lampinen, Jane~X Wang, Aaditya~K
  Singh, Pierre~Harvey Richemond, James McClelland, and Felix Hill.
\newblock Data distributional properties drive emergent in-context learning in
  transformers.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{clark2022unified}
Aidan Clark, Diego De~Las~Casas, Aurelia Guy, Arthur Mensch, Michela Paganini,
  Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian
  Borgeaud, et~al.
\newblock Unified scaling laws for routed language models.
\newblock In {\em International Conference on Machine Learning}, pages
  4057--4086. PMLR, 2022.

\bibitem{ganguli2022predictability}
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna
  Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et~al.
\newblock Predictability and surprise in large generative models.
\newblock In {\em 2022 ACM Conference on Fairness, Accountability, and
  Transparency}, pages 1747--1764, 2022.

\bibitem{gordon2021data}
Mitchell~A Gordon, Kevin Duh, and Jared Kaplan.
\newblock Data and parameter scaling laws for neural machine translation.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 5915--5922, 2021.

\bibitem{hendrycks2022emergent}
Dan Hendrycks.
\newblock Detecting emergent behavior.
\newblock 2022.

\bibitem{henighan2020scaling}
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob
  Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock {\em arXiv preprint arXiv:2010.14701}, 2020.

\bibitem{hernandez2021scaling}
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
\newblock Scaling laws for transfer.
\newblock {\em arXiv preprint arXiv:2102.01293}, 2021.

\bibitem{hestness2017deep}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock {\em arXiv preprint arXiv:1712.00409}, 2017.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{jones2021scaling}
Andy~L Jones.
\newblock Scaling scaling laws with board games.
\newblock {\em arXiv preprint arXiv:2104.03113}, 2021.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{krakovna2022sharp1}
Victoria Krakovna, Vikrant Varma, Ramana Kumar, and Mary Phuong.
\newblock Refining the sharp left turn threat model, part 1: claims and
  mechanisms.
\newblock 2022.

\bibitem{krakovna2022sharp2}
Victoria Krakovna, Vikrant Varma, Ramana Kumar, and Mary Phuong.
\newblock Refining the sharp left turn threat model, part 2: applying alignment
  techniques.
\newblock 2022.

\bibitem{krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{lake2015human}
Brenden~M Lake, Ruslan Salakhutdinov, and Joshua~B Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 350(6266):1332--1338, 2015.

\bibitem{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In {\em Text summarization branches out}, pages 74--81, 2004.

\bibitem{lowe2022instruct}
Ryan Lowe and Jan Leike.
\newblock Aligning language models to follow instructions.
\newblock 2022.

\bibitem{michaud2023quantization}
Eric~J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark.
\newblock The quantization model of neural scaling, 2023.

\bibitem{neumann2022scaling}
Oren Neumann and Claudius Gros.
\newblock Scaling laws for a multi-agent reinforcement learning model.
\newblock {\em arXiv preprint arXiv:2210.00849}, 2022.

\bibitem{rosenfeld2019constructive}
Jonathan~S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock {\em arXiv preprint arXiv:1909.12673}, 2019.

\bibitem{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{steinhardt2022future}
Jacob Steinhardt.
\newblock Future ml systems will be qualitatively different.
\newblock 2022.

\bibitem{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wei2022bigbench}
Jason Wei.
\newblock 137 emergent abilities of large language models.
\newblock 2022.

\bibitem{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock {\em arXiv preprint arXiv:2206.07682}, 2022.

\bibitem{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12104--12113, 2022.

\end{thebibliography}
