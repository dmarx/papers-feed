---
abstract: |
  Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings. Code: [`github.com/DonkeyShot21/cassle`](https://github.com/DonkeyShot21/cassle).
author:
- |
  Enrico Fini[^1]  $^{\textcolor{azure(colorwheel)}{1,2}}$ Victor G. Turrisi da Costa  $^{\textcolor{azure(colorwheel)}{1}}$ Xavier Alameda-Pineda$^{\textcolor{azure(colorwheel)}{2}}$  
  Elisa Ricci$^{\textcolor{azure(colorwheel)}{1,3}}$ Karteek Alahari$^{\textcolor{azure(colorwheel)}{2}}$ Julien Mairal$^{\textcolor{azure(colorwheel)}{2}}$  
  $^{\textcolor{azure(colorwheel)}{1}}$ University of Trento $^{\textcolor{azure(colorwheel)}{2}}$ Inria[^2] $^{\textcolor{azure(colorwheel)}{3}}$ Fondazione Bruno Kessler
bibliography:
- bib.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: Self-Supervised Models are Continual Learners
---





 

# Conclusion

In this work, we study Continual Self-Supervised Learning (CSSL), the problem of learning a set of tasks without labels continually. We make two important contributions for the SSL and CL communities: (i) we present CaSSLe, a simple and effective framework for CSSL that shows how SSL methods and losses can be seamlessly reused to learn continually, and (ii) we perform a comprehensive analysis of CSSL, leading to the emergence of interesting properties of SSL methods.

**Limitations.** Although CaSSLe shows exciting performance, it has some limitations. First, it is applicable in settings where task boundaries are provided. Second, our framework increases the amount of computational resources needed for training by roughly 30%, both in terms of memory and time. Finally, CaSSLe does not perform clustering, meaning that it is unable to directly learn a mapping from data to latent classes, and thus needs either a linear classifier trained with supervision, or some clustering algorithm.

**Broader impact.** The capabilities of supervised CL agents are bounded by the need for human-produced annotations. CSSL models can potentially improve without the need for human supervision. This facilitates the creation of powerful AIs that may be used for malicious purposes such as discrimination and surveillance. Also, since in CSSL the data is supposed to come from a non-curated stream, the model may be affected by biases in the data. This is problematic because biases are then be transferred to downstream tasks.

**Acknowledgements.** This work was supported by the European Institute of Innovation & Technology (EIT) and the H2020 EU project SPRING, funded by the European Commission under GA 871245. It was carried out under the “Vision and Learning joint Laboratory" between FBK and UNITN. Karteek Alahari was funded by the ANR grant AVENUE (ANR-18-CE23-0011). Julien Mairal was funded by the ERC grant number 714381 (SOLARIS project) and by ANR 3IA MIAI@Grenoble Alpes (ANR-19-P3IA-0003). Xavier Alameda-Pineda was funded by the ARN grant ML3RI (ANR-19-CE33-0008-01). This project was granted access to the HPC resources of IDRIS under the allocation 2021-\[AD011013084\] made by GENCI.

[^1]: Enrico Fini and Victor G. Turrisi da Costa contributed equally.

[^2]: Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.
