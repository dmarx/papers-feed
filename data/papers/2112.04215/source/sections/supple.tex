




% change the section to A, B, C
\renewcommand\thesection{\Alph{section}}
\renewcommand\thefigure{\Alph{figure}}
\renewcommand\thetable{\Alph{table}}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}

\section*{Appendix}

\section{PyTorch-like pseudo-code}
We provide a PyTorch-like pseudo-code of our method. As you can see, \name{} is simple to implement and does not add much complexity to the base SSL method. In this snippet, the losses are made symmetric by summing the two contributions. In some cases, the two losses are averaged instead. In \name{}, we symmetrize in the same way as the base SSL method we are considering.
\begin{algorithm}[h]

   \caption{PyTorch-like pseudo-code for \name{}.}
   \label{algo:DINO}
    \definecolor{codeblue}{rgb}{0.25,0.5,0.5}
    \definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
    \lstset{
      basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily,
      commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
      keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codekw},
    }
\begin{lstlisting}[language=python]
# aug: stochastic image augmentation
# f: backbone and projector
# frozen_f: frozen backbone and projector
# g: CaSSLe's predictor
# loss_fn: any SSL loss in Tab. 1 (main paper)

# PyTorchLightning handles loading and optimization
def training_step(x):

    # correlated views
    x1, x2 = aug(x), aug(x)

    # forward backbone and projector
    z1, z2 = f(x1), f(x2)

    # optionally forward predictor...

    # compute SSL loss (symmetric)
    ssl_loss = loss_fn(z1, z2) \\
             + loss_fn(z2, z1)

    # forward frozen backbone and projector
    z1_bar, z2_bar = frozen_f(x1), frozen_f(x2)

    # compute distillation loss (symmetric)
    distill_loss = loss_fn(g(z1), z1_bar) \\
                 + loss_fn(g(z2), z2_bar)
    
    # no hyperparameter for loss weighting
    return ssl_loss + distill_loss
\end{lstlisting}
\end{algorithm}

\section{Derivation of distillation losses}
In this section, we derive distillation losses from the SSL losses in Tab. 1 of the main paper, starting from the definition of our distillation loss:
\begin{equation}
    \mathcal{L}_D(\zvect, \bar{\zvect}) = \mathcal{L}_{SSL} (g(\zvect), \bar{\zvect}),
\end{equation}
where $z$ and $\bar{z}$ are the representations of the current and frozen encoder, and $g$ is \name{}'s predictor network implemented as a two layer MLP with 2048 hidden neurons and ReLU activation.
\paragraph{Contrastive based.} Our distillation loss based on contrastive learning is implemented as follows:
\begin{equation}
    \mathcal{L}(z_i, \bar{z}_i) = -\log \frac{\exp \left(\operatorname{sim}\left(\zvect_i, \bar{\zvect}_i\right) / \tau\right)}{\sum_{\zvect_j \in \bar{\eta}(i)} \exp \left(\operatorname{sim}\left(\zvect_i, \zvect_{j}\right) / \tau\right)},
\end{equation}
where $\bar{\eta}(i)$ is the set of negatives for the sample with index $i$ in the batch. Note that the negatives are drawn both from the predicted and frozen features.
\paragraph{MSE based.} This distillation loss is simply the MSE between the predicted features and the frozen features:
\begin{equation}
    \mathcal{L}(z, \bar{z}) = -||g(\zvect) -  \bar{\zvect}||^2_2 .
\end{equation}
It can be implemented with the cosine similarity as stated in the main manuscript.
\paragraph{Cross-entropy based.} The cross-entropy loss, when used for distillation in an unsupervised setting, makes sure that the current encoder is able to assign samples to the frozen centroids (or prototypes) consistently with the frozen encoder:
\begin{equation}
    \mathcal{L}(z, \bar{z}) = -\sum_{d} \bar{\avect}_{d} \log \frac{\exp \left(\operatorname{sim}\left( g(\zvect), \cvect^{t-1}_{d}\right) / \tau\right)}{\sum_k \exp \left(\operatorname{sim}\left(g(\zvect), \cvect^{t-1}_k\right) / \tau\right)} 
\end{equation}
where:
\begin{equation}
    \bar{\avect}= \frac{\exp \left(\operatorname{sim}\left( \bar{\zvect}, \cvect^{t-1}_{d}\right) / \tau\right)}{\sum_k \exp \left(\operatorname{sim}\left(\bar{\zvect}, \cvect^{t-1}_k\right) / \tau\right)},
\end{equation}
and the set of frozen prototypes is denoted as follows: $\mathbf{C}^{t-1} = \left\{\mathbf{c}^{t-1}_{1}, \ldots, \mathbf{c}^{t-1}_{K}\right\}$.
 
\paragraph{Cross-correlation based.} We consider Barlow Twins'~ \cite{zbontar2021barlow} implementation of this objective. For VICReg~\cite{bardes2021vicreg} we only consider the invariance term. As a distillation loss, the cross-correlation matrix is computed with the predicted and frozen features:

\begin{equation}
    \mathcal{L}(z, \bar{z}) = \sum_{u}\left(1-\bar{\mathcal{C}}_{u v}\right)^{2}+\lambda \sum_{u} \sum_{v \neq u} \bar{\mathcal{C}}_{u v}^{2} ,
\end{equation}
where:
\begin{equation}
    \bar{\mathcal{C}}_{u v} = \frac{\sum_{i} g(\zvect_{i, u}) \bar{\zvect}_{i, v}}{\sqrt{\sum_{i}g\left(\zvect_{i, u}\right)^{2}}. \sqrt{\sum_{i}\left(\bar{\zvect}_{i, v}\right)^{2}}}.
\end{equation} 

\section{Further discussion and implementation details of the baselines}
\paragraph{Selection.} When evaluating our framework, we try to compare with as many existing related methods as possible. However, given that SSL models are computationally intensive, it was not possible to run all baselines and methods in all the CL settings we considered. As mentioned in the main manuscript, we choose eight baselines (seven related methods + fine-tuning) belonging to three CL macro-categories, and test them on CIFAR100 (class-incremental) in combination with three SSL methods. The selection was based on the ease of adaptation to CSSL and the similarity to our framework.

\begin{table*}[ht]
\centering
\scriptsize
\captionsetup{type=table}
\captionsetup{width=.99\linewidth}
\caption{Linear evaluation top-1 accuracy on DomainNet (6 tasks, domain-incremental setting) w/ and w/o \name{}. The sequence of tasks is Real$\rightarrow$Quickdraw$\rightarrow$Painting$\rightarrow$Sketch$\rightarrow$Infograph$\rightarrow$Clipart. ``Aw.'' stands for task-aware, ``Ag,'' for task-agnostic.}
\label{tab:domain-incremental-task-agnostic}
\vspace{-2px}
\begin{tabular}{lccccccccccccccc}
\toprule
\multirow{2}[1]{*}{\textbf{Method}} & \multirow{2}[1]{*}{\textbf{Strategy}} & \multicolumn{2}{c}{\textbf{Real}} & \multicolumn{2}{c}{\textbf{Quickdraw}} & \multicolumn{2}{c}{\textbf{Painting}} & \multicolumn{2}{c}{\textbf{Sketch}} & \multicolumn{2}{c}{\textbf{Infograph}} & \multicolumn{2}{c}{\textbf{Clipart}} & \multicolumn{2}{c}{\textbf{Avg.}} \\ 
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14} \cmidrule(lr){15-16}
&& Aw. & Ag. & Aw. & Ag. & Aw. & Ag. & Aw. & Ag. & Aw. & Ag. & Aw. & Ag. & Aw. & Ag.   \\
\midrule
\multirow{3}[2]{*}{{\parbox{1.5cm}{Barlow Twins}}} & \CC{ftcolor}Finetuning & \CC{ftcolor}56.3 & \CC{ftcolor}50.9 & \CC{ftcolor}54.1 & \CC{ftcolor}45.8 & \CC{ftcolor}42.7 & \CC{ftcolor}35.9 & \CC{ftcolor}49.0 & \CC{ftcolor}41.9 & \CC{ftcolor}22.0 & \CC{ftcolor}17.4 & \CC{ftcolor}59.0 & \CC{ftcolor}52.5 & \CC{ftcolor}50.3 & \CC{ftcolor}43.7 \\
                             & \CC{decorrcolor}\name{} 
                             & \CC{decorrcolor}\textbf{62.7} & \CC{decorrcolor}\textbf{57.1} & \CC{decorrcolor}\textbf{59.1} & \CC{decorrcolor}\textbf{50.6} & \CC{decorrcolor}\textbf{49.2} & \CC{decorrcolor}\textbf{42.1} & \CC{decorrcolor}\textbf{53.8} & \CC{decorrcolor}\textbf{47.7} & \CC{decorrcolor}\textbf{25.5} & \CC{decorrcolor}\textbf{20.6} & \CC{decorrcolor}\textbf{61.9} & \CC{decorrcolor}\textbf{55.6} & \CC{decorrcolor}\textbf{55.5} & \CC{decorrcolor}\textbf{48.9} \\ 
                             \cmidrule{2-16}
                             & \CC{offlinecolor} Offline & \CC{offlinecolor}67.1 & \CC{offlinecolor}63.0 & \CC{offlinecolor}60.3 & \CC{offlinecolor}53.9 & \CC{offlinecolor}52.4 & \CC{offlinecolor}46.3 & \CC{offlinecolor}51.9 & \CC{offlinecolor}46.9 & \CC{offlinecolor}25.9 & \CC{offlinecolor}21.0 & \CC{offlinecolor}58.8 & \CC{offlinecolor}52.6 & \CC{offlinecolor}57.2 & \CC{offlinecolor}51.8 \\
\midrule
\multirow{3}[2]{*}{SwAV}      & \CC{ftcolor}Finetuning & \CC{ftcolor}57.7 & \CC{ftcolor}52.3 & \CC{ftcolor}53.2 & \CC{ftcolor}43.5 & \CC{ftcolor}43.0 & \CC{ftcolor}35.9 & \CC{ftcolor}46.1 & \CC{ftcolor}39.0 & \CC{ftcolor}21.6 & \CC{ftcolor}16.5 & \CC{ftcolor}53.4 & \CC{ftcolor}46.6 & \CC{ftcolor}49.6 & \CC{ftcolor}42.5 \\
                             & \CC{knowcolor}\name{} 
                             & \CC{knowcolor}\textbf{62.8} & \CC{knowcolor}\textbf{57.8} & \CC{knowcolor}\textbf{59.5} & \CC{knowcolor}\textbf{50.2} & \CC{knowcolor}\textbf{47.5} & \CC{knowcolor}\textbf{41.2} & \CC{knowcolor}\textbf{49.5} & \CC{knowcolor}\textbf{42.5} & \CC{knowcolor}\textbf{22.5} & \CC{knowcolor}\textbf{17.9} & \CC{knowcolor}\textbf{56.5} & \CC{knowcolor}\textbf{49.6} & \CC{knowcolor}\textbf{54.3} & \CC{knowcolor}\textbf{47.5} \\
                             \cmidrule{2-16}
                             & \CC{offlinecolor} Offline  & \CC{offlinecolor}64.1  & \CC{offlinecolor}59.5 & \CC{offlinecolor}60.6 & \CC{offlinecolor}53.6 & \CC{offlinecolor}47.6 & \CC{offlinecolor}42.9 & \CC{offlinecolor}47.7 & \CC{offlinecolor}42.1 & \CC{offlinecolor}23.3 & \CC{offlinecolor}18.9 & \CC{offlinecolor}53.6 & \CC{offlinecolor}47.3 & \CC{offlinecolor}54.6 & \CC{offlinecolor}49.1 \\
\midrule
\multirow{3}[2]{*}{BYOL}      & \CC{ftcolor}Finetuning & \CC{ftcolor}58.7 & \CC{ftcolor}53.2& \CC{ftcolor}51.7 & \CC{ftcolor}41.6 & \CC{ftcolor}44.0 & \CC{ftcolor}37.4 & \CC{ftcolor}49.6 & \CC{ftcolor}43.9 & \CC{ftcolor}23.5 & \CC{ftcolor}19.0 & \CC{ftcolor}58.6 & \CC{ftcolor}53.5 & \CC{ftcolor}50.6 & \CC{ftcolor}43.8 \\
                             & \CC{predcolor}\name{} 
                             & \CC{predcolor}\textbf{63.7} & \CC{predcolor}\textbf{60.5} & \CC{predcolor}\textbf{59.3} & \CC{predcolor}\textbf{50.9} & \CC{predcolor}\textbf{48.6} & \CC{predcolor}\textbf{44.1} & \CC{predcolor}\textbf{50.4} & \CC{predcolor}\textbf{45.2} & \CC{predcolor}\textbf{24.1} & \CC{predcolor}\textbf{19.4} & \CC{predcolor}\textbf{59.0} & \CC{predcolor}\textbf{54.4} & \CC{predcolor}\textbf{55.1} & \CC{predcolor}\textbf{49.7} \\
                             \cmidrule{2-16}
                             & \CC{offlinecolor} Offline & \CC{offlinecolor}67.2 & \CC{offlinecolor}64.0 & \CC{offlinecolor}60.2 & \CC{offlinecolor}53.3 & \CC{offlinecolor}51.5 & \CC{offlinecolor}47.3 & \CC{offlinecolor}50.4 &  \CC{offlinecolor}46.2 & \CC{offlinecolor}24.5 & \CC{offlinecolor}20.8 & \CC{offlinecolor}57.0 & \CC{offlinecolor}51.5 & \CC{offlinecolor}56.6  & \CC{offlinecolor}51.9  \\
\midrule
\multirow{3}[2]{*}{VICReg}      & \CC{ftcolor}Finetuning & \CC{ftcolor}54.7 & \CC{ftcolor}49.6 & \CC{ftcolor}53.0 & \CC{ftcolor}44.9 & \CC{ftcolor}42.1 & \CC{ftcolor}34.7 & \CC{ftcolor}49.0 & \CC{ftcolor}41.9 & \CC{ftcolor}21.1 & \CC{ftcolor}16.4 & \CC{ftcolor}58.5 & \CC{ftcolor}52.6 & \CC{ftcolor}49.3 & \CC{ftcolor}42.8  \\
                             & \CC{predcolor}\name{} 
                             & \CC{predcolor}\textbf{59.0} & \CC{predcolor}\textbf{53.2} & \CC{predcolor}\textbf{56.4} & \CC{predcolor}\textbf{47.8} & \CC{predcolor}\textbf{46.0} & \CC{predcolor}\textbf{38.9} & \CC{predcolor}\textbf{52.3} & \CC{predcolor}\textbf{45.6} & \CC{predcolor}\textbf{23.9} & \CC{predcolor}\textbf{18.5} & \CC{predcolor}\textbf{60.9} & \CC{predcolor}\textbf{55.3} & \CC{predcolor}\textbf{52.9} & \CC{predcolor}\textbf{46.1} \\ 
                             \cmidrule{2-16}
                             & \CC{offlinecolor} Offline & \CC{offlinecolor}66.4 &  \CC{offlinecolor}62.7 & \CC{offlinecolor}59.2 & \CC{offlinecolor}53.5 & \CC{offlinecolor}52.4 & \CC{offlinecolor}47.2 & \CC{offlinecolor}53.2 & \CC{offlinecolor}48.1 & \CC{offlinecolor}25.3 & \CC{offlinecolor}20.7 & \CC{offlinecolor}58.3 & \CC{offlinecolor}53.2 & \CC{offlinecolor}56.7 & \CC{offlinecolor}51.9 \\
\midrule
\multirow{3}[2]{*}{SimCLR}      & \CC{ftcolor}Finetuning & \CC{ftcolor}52.5 & \CC{ftcolor}47.6 & \CC{ftcolor}48.2 & \CC{ftcolor}38.1 & \CC{ftcolor}37.5 & \CC{ftcolor}31.7 & \CC{ftcolor}42.8 & \CC{ftcolor}35.7 & \CC{ftcolor}18.8 & \CC{ftcolor}14.4 & \CC{ftcolor}50.9 & \CC{ftcolor}46.8 & \CC{ftcolor}45.1 & \CC{ftcolor}38.4 \\
                             & \CC{contrcolor}\name{} 
                             & \CC{contrcolor}\textbf{58.4} & \CC{contrcolor}\textbf{43.4} & \CC{contrcolor}\textbf{54.2} & \CC{contrcolor}\textbf{44.7} & \CC{contrcolor}\textbf{43.9} & \CC{contrcolor}\textbf{37.7} & \CC{contrcolor}\textbf{47.6} & \CC{contrcolor}\textbf{41.9} & \CC{contrcolor}\textbf{22.0} & \CC{contrcolor}\textbf{17.8} & \CC{contrcolor}\textbf{54.9} & \CC{contrcolor}\textbf{50.5} & \CC{contrcolor}\textbf{50.0} & \CC{contrcolor}\textbf{44.2} \\ 
                             \cmidrule{2-16}
                             & \CC{offlinecolor} Offline & \CC{offlinecolor}62.1 & \CC{offlinecolor}59.5& \CC{offlinecolor}58.3 & \CC{offlinecolor}52.9 & \CC{offlinecolor}46.1 & \CC{offlinecolor}42.5 & \CC{offlinecolor}45.6 & \CC{offlinecolor}41.3 & \CC{offlinecolor}22.1 & \CC{offlinecolor}18.8 & \CC{offlinecolor}51.0 & \CC{offlinecolor}45.9 & \CC{offlinecolor}52.6 & \CC{offlinecolor}48.6 \\
\midrule
\multirow{3}[2]{*}{MoCoV2+}      & \CC{ftcolor}Finetuning & \CC{ftcolor}50.9 & \CC{ftcolor}45.5 & \CC{ftcolor}45.8 & \CC{ftcolor}37.5 & \CC{ftcolor}36.0 & \CC{ftcolor}29.3 & \CC{ftcolor}39.5 & \CC{ftcolor}32.1 & \CC{ftcolor}17.9 & \CC{ftcolor}13.5 & \CC{ftcolor}50.3 & \CC{ftcolor}\textbf{44.5} & \CC{ftcolor}43.2 & \CC{ftcolor}36.7 \\
                             & \CC{contrcolor}\name{} 
                             & \CC{contrcolor}\textbf{56.0} & \CC{contrcolor}\textbf{50.3}  & \CC{contrcolor}\textbf{48.7} & \CC{contrcolor}\textbf{40.0} & \CC{contrcolor}\textbf{40.4} & \CC{contrcolor}\textbf{33.6} & \CC{contrcolor}\textbf{42.0} & \CC{contrcolor}\textbf{35.0} & \CC{contrcolor}\textbf{19.9} & \CC{contrcolor}\textbf{15.2} & \CC{contrcolor}\textbf{51.7} & \CC{contrcolor}\textbf{44.5} & \CC{contrcolor}\textbf{46.7} & \CC{contrcolor}\textbf{38.8} \\ 
                             \cmidrule{2-16}
                             & \CC{offlinecolor} Offline & \CC{offlinecolor}65.2 & \CC{offlinecolor}61.3 & \CC{offlinecolor}57.9 & \CC{offlinecolor}51.3 & \CC{offlinecolor}48.7 & \CC{offlinecolor}43.1 & \CC{offlinecolor}44.7 & \CC{offlinecolor}39.1 & \CC{offlinecolor}23.4 & \CC{offlinecolor}19.0 & \CC{offlinecolor}51.3 & \CC{offlinecolor}44.8 & \CC{offlinecolor}53.7 & \CC{offlinecolor}48.4 \\
\midrule
\multirow{3}[2]{*}{{\parbox{1.5cm}{Supervised Contrastive}}}      & \CC{ftcolor}Finetuning & \CC{ftcolor}57.7 & \CC{ftcolor}52.6 & \CC{ftcolor}55.3 & \CC{ftcolor}45.5 & \CC{ftcolor}44.9 & \CC{ftcolor}38.0 & \CC{ftcolor}51.7 & \CC{ftcolor}45.0 & \CC{ftcolor}22.6 & \CC{ftcolor}18.3 & \CC{ftcolor}64.0 & \CC{ftcolor}60.0 & \CC{ftcolor}52.1 & \CC{ftcolor}45.4 \\
                             & \CC{contrcolor}\name{} 
                             & \CC{contrcolor}\textbf{63.4} & \CC{contrcolor}\textbf{58.8} & \CC{contrcolor}\textbf{59.7} & \CC{contrcolor}\textbf{51.3} & \CC{contrcolor}\textbf{50.1} & \CC{contrcolor}\textbf{44.7} & \CC{contrcolor}\textbf{55.9} & \CC{contrcolor}\textbf{50.3} & \CC{contrcolor}\textbf{26.9} & \CC{contrcolor}\textbf{22.4} & \CC{contrcolor}\textbf{65.0} & \CC{contrcolor}\textbf{61.3} & \CC{contrcolor}\textbf{56.7} & \CC{contrcolor}\textbf{50.9} \\
                             \cmidrule{2-16}
                             & \CC{offlinecolor} Offline & \CC{offlinecolor}67.4 & \CC{offlinecolor}65.3 & \CC{offlinecolor}65.8 & \CC{offlinecolor}63.0 & \CC{offlinecolor}53.6 & \CC{offlinecolor}50.9 & \CC{offlinecolor}56.0 & \CC{offlinecolor}53.1 & \CC{offlinecolor}28.0 & \CC{offlinecolor}25.7 & \CC{offlinecolor}62.8 & \CC{offlinecolor}59.6 & \CC{offlinecolor}60.0 & \CC{offlinecolor}57.4 \\
\midrule

\multirow{2}[1]{*}{Supervised}   & \CC{ftcolor}Finetuning & \CC{ftcolor}63.0 & \CC{ftcolor}58.2 & \CC{ftcolor}56.9 & \CC{ftcolor}47.6 & \CC{ftcolor}49.1 & \CC{ftcolor}44.0 & \CC{ftcolor}55.7 & \CC{ftcolor}50.3 & \CC{ftcolor}27.7 & \CC{ftcolor}23.3 & \CC{ftcolor}68.6 & \CC{ftcolor}63.5 & \CC{ftcolor}55.9 & \CC{ftcolor}49.8 \\
                             \cmidrule{2-16}
                             & \CC{offlinecolor} Offline & \CC{offlinecolor}74.7  & \CC{offlinecolor}73.2 & \CC{offlinecolor}68.5  & \CC{offlinecolor}67.8 & \CC{offlinecolor}62.0  & \CC{offlinecolor}59.3 & \CC{offlinecolor}65.7  & \CC{offlinecolor}63.7 & \CC{offlinecolor}33.7  & \CC{offlinecolor}34.5 & \CC{offlinecolor}72.3  & \CC{offlinecolor}69.3 & \CC{offlinecolor}66.4  & \CC{offlinecolor}65.0 \\
\bottomrule
\end{tabular}
\end{table*}

The most similar to \name{} are data-focused regularization methods. Among them, a large majority leverage knowledge distillation using the outputs of a classifier learned with supervision \eg~\cite{Li17learning, castro2018end, fini2020online}, while a few works employ feature distillation~\cite{hou2019learning, douillard2020podnet} which is viable even without supervision. \cite{iscen2020memory} is also related to \name{}, but it focuses on memory efficiency which is less interesting in our setting. Also, \cite{iscen2020memory} explicitly uses the classifier after feature adaptation, hence it is unclear how to adapt it for CSSL, especially since in SSL positives are generated using image augmentations, which are not applicable to a memory bank of features. On the contrary, augmentations can be used in replay methods, among which we select the most common (ER~\cite{Robins95}) and one of the most recent (DER~\cite{buzzega2020dark}). Regarding prior-focused regularization methods, we choose EWC~\cite{kirkpatrick2017overcoming} over others (SI~\cite{Zenke17}, MAS~\cite{Aljundi17}, \etc) as it is considered the most influential and it works best with task boundaries. We also consider two CSSL baselines: LUMP~\cite{madaan2021rethinking} and Lin \etal~\cite{lin2021continual}. Finally, we do not consider methods based on VAEs~\cite{rao2019continual, achille2018life}, since they have been shown to yield poor performance in the large and medium scale. For instance, as found by~\cite{falcon2021aavae}, a VAE trained offline on CIFAR10 reaches an accuracy of 57.2\%, which is lower than any method (except VICReg) trained continually on CIFAR100 with \name{}.

\paragraph{Implementation.} For EWC, we use the SSL loss instead of the supervised loss to estimate importance weights. For POD and Less-Forget, we only re-implement the feature distillation without considering the parts of their methods that explicitly use the classifier. For DER, we replace the logits  of the classifier with the projected features in the buffer. We re-implement all these baselines by adapting them from the official implementation (POD), or from the Mammoth framework provided with~\cite{buzzega2020dark} (DER, ER, EWC), or from the paper (Less-Forget). We also compare with two concurrent works that propose approaches for CSSL (LUMP~\cite{madaan2021rethinking}, Lin \etal~\cite{lin2021continual}). LUMP uses k-NN evaluation, therefore we adapt the code provided by the authors to run in our code base. For Lin \etal, we compare directly with their published results, since they use the same evaluation protocol. We perform hyperparameter tuning for all baselines, searching over 5 values for the distillation loss weights of POD and Less-Forget, 3 values for the weight of the regularization in EWC and 3 replay batch sizes for replay methods. The size of the replay buffer is 500 samples for all replay based methods. 


\section{Additional results}
\paragraph{Continual supervised contrastive with \name{}.} After the popularization of contrastive learning~\cite{chen2020simple, he2020momentum} for unsupervised learning of representations, \cite{khosla2020supervised} proposed a supervised version of the contrastive loss. Here, we show that \name{} is easily extendable to support supervised contrastive learning. The implementation is basically the same as for our vanilla contrastive-based distillation loss. In Tab.~\ref{tab:supervised-contrastive}, we show the improvement that \name{} brings with respect to fine-tuning, which is sizeable in the class-incremental setting. We also report the same comparison on DomainNet in Tab.~\ref{tab:domain-incremental-task-agnostic}, showing interesting results in both task-aware and task-incremental evaluation.

\begin{table}[t]
\caption{Linear evaluation top-1 accuracy on ImageNet100 (5 tasks, class- and data-incremental).}
\label{tab:supervised-contrastive}
\vspace{-2px}
\scriptsize
\centering
\captionsetup{type=table}
\begin{tabular}{lccc}
\toprule
\multirow{2}[1]{*}{\textbf{Method}} & \multirow{2}[1]{*}{\textbf{Strategy}} & \multicolumn{2}{c}{\textbf{ImageNet100}} \\ 
\cmidrule(lr){3-4}
&& Class-inc. & Data-inc. \\
\midrule
\multirow{2}{*}{{\parbox{1.2cm}{Supervised Contrastive}}}      & \CC{ftcolor}Fine-tuning & 61.6 & \CC{ftcolor}74.3 \\
                             & \CC{contrcolor}\name{} 
                             & \CC{contrcolor}\textbf{69.6}& \CC{contrcolor}\textbf{76.9}  \\ 
\bottomrule
\end{tabular}
\captionsetup{width=.99\linewidth}
\end{table}


\paragraph{Task-agnostic evaluation and domain-wise accuracy on DomainNet.} In the main manuscript, we showed that \name{} significantly improved performance in the domain-incremental setting using task-aware evaluation. Here, ``task-aware'' refers to the fact that linear evaluation is performed on each domain separately, \ie a different linear classifier is learned for each domain. However, it might also be interesting to check the performance of the model when the domain is unknown at test time. For this reason, we report the performance of our model when evaluated in a task-agnostic fashion. In addition, we also show the accuracy on each task (\ie domain). All this information is presented in Tab.~\ref{tab:domain-incremental-task-agnostic}. \name{} \textbf{always} outperforms fine-tuning with both evaluation protocols. The accuracy of \name{} on ``Clipart'' is also higher than offline. This is probably due to a combination of factors: (i) Clipart is the last task, therefore it probably benefits in forward transfer and (ii) a similar effect to the one found in~\cite{tian2021divide}, where dividing data in subgroups tends to enable the learning of better representations. Also, we notice that task-agnostic accuracy is lower than the task-aware counterpart. This is expected and means that the class conditional distributions are not perfectly aligned in different domains. As in the main paper, the colors are related to the type of SSL loss.

\begin{table}[t]
\caption{k-NN evaluation on ImageNet100 (5 tasks, class-incremental) performed on backbone and projected features.}
\label{tab:knn}
\vspace{-2px}
\scriptsize
\centering
\captionsetup{type=table}
\begin{tabular}{lccc}
\toprule
\multirow{2}[1]{*}{\textbf{Method}} & \multirow{2}[1]{*}{\textbf{Strategy}} & \multicolumn{2}{c}{\textbf{k-NN accuracy ($\uparrow$)}} \\
\cmidrule(lr){3-4}
&& \textbf{Backbone ($f_b$)} & \textbf{Projector ($f_p$)} \\
\midrule
\multirow{2}{*}{{\parbox{0.8cm}{Barlow Twins}}}      & \CC{ftcolor}Fine-tuning & 59.1 & \CC{ftcolor}34.4 \\
                             & \CC{decorrcolor}\name{} 
                             & \CC{decorrcolor}\textbf{63.4}& \CC{decorrcolor}\textbf{53.2}  \\ 
\midrule
\multirow{2}{*}{SwAV}      & \CC{ftcolor}Fine-tuning & \textbf{60.0} & \CC{ftcolor}53.9 \\
                             & \CC{knowcolor}\name{} 
                             & \CC{knowcolor}59.7 & \CC{knowcolor}\textbf{61.3}  \\ 
\midrule
\multirow{2}{*}{BYOL}     & \CC{ftcolor}Fine-tuning & 57.1 & \CC{ftcolor}33.0 \\
                             & \CC{predcolor}\name{} 
                             & \CC{predcolor}\textbf{61.2}& \CC{predcolor}\textbf{60.8}  \\ 
\midrule
\multirow{2}{*}{VICReg}       & \CC{ftcolor}Fine-tuning & 56.7 & \CC{ftcolor}35.3 \\
                             & \CC{predcolor}\name{} 
                             & \CC{predcolor}\textbf{59.5}& \CC{predcolor}\textbf{43.4}  \\ 
\midrule
\multirow{2}{*}{MoCoV2+}      & \CC{ftcolor}Fine-tuning & 54.5 & \CC{ftcolor}39.0 \\
                             & \CC{contrcolor}\name{} 
                             & \CC{contrcolor}\textbf{61.5}& \CC{contrcolor}\textbf{53.1}  \\ 
\midrule
\multirow{2}{*}{SimCLR}      & \CC{ftcolor}Fine-tuning & 54.8 & \CC{ftcolor}40.1 \\
                             & \CC{contrcolor}\name{} 
                             & \CC{contrcolor}\textbf{61.7}& \CC{contrcolor}\textbf{53.2}  \\ 
\bottomrule
\end{tabular}
\captionsetup{width=.99\linewidth}
\vspace{-8px}
\end{table}

\paragraph{Additional results with k-NN evaluation.}
For completeness, in this supplementary material, we also show that \name{} yields superior performance when evaluated with a k-NN classifier instead of linear evaluation. We use weighted k-NN with l2-normalization (cosine similarity) and temperature scaling as in~\cite{caron2021emerging}. Since since k-NN is much faster than linear evaluation we could also assess the quality of the projected representations, instead of just using the backbone. The results can be inspected in Tab.~\ref{tab:knn}. Three interesting phenomena arise: (i) \name{} always improves with respect to fine-tuning, (ii) the features of the backbone $f_b$ are usually better than the features of the projector $f_p$ and (iii) \name{} causes information retention in the projector, which significantly increases the performance of the projected features. An exception is represented by SwAV~\cite{caron2020unsupervised}, that seems to behave differently to other methods. First, the accuracy of the projected features in SwAV is much higher than other methods. 
This might be due to the fact that it uses prototypes, which bring the representations 1 layer away from the loss, making them less specialized in the SSL task. Second, it seems that \name{} only improves the projected features when coupled with SwAV. However, this is probably an artifact of the evaluation procedure, as the l2-normalization probably causes loss of information. Indeed, although the overall performance is lower, SwAV + \name{} outperforms SwAV + fine-tuning (58.7\% vs 56.9\%) if the euclidean distance is used in place of the cosine similarity for the backbone features. We leave a deeper investigation of this phenomenon for future work.

\begin{table}[t]
\caption{Linear evaluation top-1 accuracy on CIFAR100 (10 tasks, class-incremental).}
\label{tab:10-tasks}
\scriptsize
\centering
\captionsetup{type=table}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Strategy} & \textbf{A ($\uparrow$)} \\ 
\midrule
\multirow{2}{*}{SimCLR}      & Fine-tuning & 39.3 \\
                             & \CC{contrcolor}\name{} & \CC{contrcolor}\textbf{52.7} \\ 
\midrule
\multirow{2}{*}{Barlow Twins}  & \CC{ftcolor}Fine-tuning & 49.9\\                      
  & \CC{decorrcolor}\name{} & \CC{decorrcolor}\textbf{53.7} \\ 
\bottomrule
\end{tabular}
\captionsetup{width=.99\linewidth}
\end{table}

\begin{table}[t]
\caption{Linear evaluation top-1 accuracy on ImageNet100 (5 tasks, class- and data-incremental) with ResNet50~\cite{he2016deep}.}
\label{tab:r50}
\scriptsize
\centering
\captionsetup{type=table}
\begin{tabular}{lccc}
\toprule
\multirow{2}[1]{*}{\textbf{Method}} & \multirow{2}[1]{*}{\textbf{Strategy}} & \multicolumn{2}{c}{\textbf{A ($\uparrow$)}} \\ 
\cmidrule(lr){3-4}
&& \textbf{Class-inc.} & \textbf{Data-inc.} \\
\midrule
\multirow{2}{*}{SimCLR}      & Fine-tuning & 70.7 & 75.6 \\
                             & \CC{contrcolor}\name{} & \CC{contrcolor}\textbf{74.0} & \CC{contrcolor}\textbf{77.2}  \\ 
\midrule
\multirow{2}{*}{Barlow Twins}  & \CC{ftcolor}Fine-tuning & 71.2 & 75.8 \\                      
  & \CC{decorrcolor}\name{} & \CC{decorrcolor}\textbf{74.8} & \CC{decorrcolor}\textbf{78.1} \\ 
\bottomrule
\end{tabular}
\captionsetup{width=.99\linewidth}
\end{table}



\paragraph{Different number of tasks.} The analysis of CSSL settings that we show in the main manuscript is limited to the 5 task scenario. However, it is interesting to run the same benchmarks with a longer task sequence. Nonetheless, one should also remember that SSL methods are data hungry, hence the less data is available per task, the higher the instability of the SSL models. In Tab.~\ref{tab:10-tasks}, we present additional results with 10 tasks on CIFAR100 (class-incremental). Barlow Twins seems to hold up surprisingly well, finishing up at roughly 50\% accuracy, while SimCLR suffers in the low data regime. Nonetheless, \name{} outperforms fine-tuning with Barlow Twins, and to a very large extent with SimCLR. 
\paragraph{Deeper architectures.} The experiments we propose in the main manuscript feature a ResNet18 network. This is a common choice in CL. However, in SSL, it is more common to use ResNet50. For this reason, in Tab.~\ref{tab:r50} we show that the same behavior observed with smaller networks is also obtained with deeper architectures. More specifically, \name{} outperforms fine-tuning in both class- and data-incremental settings by large margins. 

\paragraph{The role of the predictor.} In the main manuscript, we provided an intuitive explanation of the role of the predictor network that maps the current feature space to the frozen feature space. This intuition is corroborated by extensive experimentation and ablation studies. However, one more thing that is worth mentioning is that the success of the predictor network might also be related to the findings in SimSiam~\cite{chen2021exploring}, BYOL~\cite{grill2020bootstrap} and DirectPred~\cite{tian2021understanding}. 
Moreover, we perform additional ablations on the design of CaSSLe's predictor for SimCLR on CIFAR100 (5 tasks): adding BatchNorm after the hidden layer does not make any difference in terms of performance, and removing the non-linearity only causes a 0.3\% drop in accuracy.

\begin{table}[t]
\caption{Combinations of SSL methods and distillation losses on CIFAR100 (class-incremental, 2 tasks).}
\label{tab:comb-methods-distill}
\scriptsize
\centering
\captionsetup{type=table}
\begin{tabular}{lccc}
\toprule
\textbf{Distillation Loss}         & \textbf{SimCLR} & \textbf{Barlow Twins} & \textbf{BYOL}\\ 
\midrule
 InfoNCE & \CC{contrcolor}\textbf{61.8} & 64.5 & 64.8 \\
 Cross-correlation  & 60.1 & \CC{decorrcolor}\textbf{67.2} & 65.8  \\ 
 MSE & 61.3  & 64.6 & \CC{predcolor}\textbf{66.7} \\ 
\bottomrule
\end{tabular}

\end{table}


\paragraph{Combinations of SSL methods and distillation losses.} For computational reasons, it was not feasible to perform experiments combing all SSL methods with all possible distillation losses. However, in Tab.~\ref{tab:comb-methods-distill} we provide a subset of the possible combinations to validate our strategy that uses the same SSL loss for distillation.









