\vspace{-5pt}
\section{Related Work}
\label{sec:related}
\vspace{-8pt}
\noindent\textbf{Self-Supervised Learning.} 
Recent SSL approaches have shown performance comparable to their supervised learning equivalents~\cite{caron2020unsupervised, caron2021emerging, chen2020simple, he2020momentum, grill2020bootstrap, zbontar2021barlow, bardes2021vicreg, chen2020improved}. In a nutshell, most of these methods use image augmentation techniques to generate correlated views (positives) from a sample, and then learn a model that is invariant to these augmentations by enforcing the network to output similar representations for the positives. Initially, contrastive learning, based on instance discrimination~\cite{wu2018unsupervised} using noise-contrastive estimation~\cite{gutmann2010noise, oord2018representation}, was a popular strategy~\cite{chen2020simple, he2020momentum}. However, this learning paradigm 
requires large batch sizes or memory banks. A few methods that use a negative-free cosine similarity loss~\cite{grill2020bootstrap, chen2021exploring} have addressed such issues.

Concurrently, clustering-based methods (SwAV~\cite{caron2020unsupervised}, DeepCluster v2~\cite{caron2020unsupervised, caron2018deep} and DINO~\cite{caron2021emerging}) have also been proposed. They do not operate on the features directly, and instead compare positives through a cross-entropy loss using cluster prototypes as a proxy. Redundancy reduction-based methods have also been popular\cite{ermolov2021whitening, zbontar2021barlow, bardes2021vicreg}. Among them, BarlowTwins \cite{zbontar2021barlow} considers an objective function measuring the cross-correlation matrix between the features, and VicReg\cite{bardes2021vicreg} uses a mix of variance, invariance and covariance
regularizations. Methods such as~\cite{dwibedi2021little} have explored the use of nearest-neighbour retrieval and divide and conquer~\cite{tian2021divide}. However, none of these works studied the ability of SSL methods to learn continually and adaptively.


\noindent\textbf{Continual Learning.} A plethora of methods have been developed to counteract catastrophic forgetting~\cite{kirkpatrick2017overcoming, Rusu16progressive, shin2017continual, Lopez-Paz17, Chaudhry19, serra2018overcoming, chaudhry2018riemannian, Aljundi17, Zenke17, buzzega2020dark, fini2020online, douillard2020podnet, wu2019large, castro2018end, rebuffi2017icarl, hou2019learning, prabhu2020gdumb, Li17learning, ostapenko2019learning, cha2021co2l, Robins95}. Following~\cite{de2019continual}, these works can be organized into three macro-categories: replay-based~\cite{ostapenko2019learning, Robins95, rebuffi2017icarl, buzzega2020dark, Chaudhry19, Lopez-Paz17}, regularization-based~\cite{fini2020online, Li17learning, shin2017continual, kirkpatrick2017overcoming, Zenke17, Aljundi17, castro2018end, douillard2020podnet, hou2019learning, chaudhry2018riemannian, wu2019large, cha2021co2l}, and parameter isolation
methods~\cite{Rusu16progressive, serra2018overcoming}. All these works evaluate the effectiveness of CL methods using a linear classifier learned
sequentially over time. However, this evaluation does not reflect an important aspect, \textit{i.e.}, the internal dynamics of the hidden representations. Moreover, most CL methods tend to rely on supervision in order to mitigate catastrophic forgetting. A few of them can be adapted for the unsupervised setting, although their effectiveness is greatly reduced (see discussion in Sec.~\ref{sec:cassowary}, Sec.~\ref{sec:experiments} and the supplementary material). 

Works such as~\cite{rao2019continual, achille2018life, smith2019unsupervised} laid the foundations of unsupervised CL, but their studies are severely limited to digit-like datasets, \emph{e.g.}, MNIST and Omniglot, and the proposed methods are unfit for large-scale scenarios. Recently, \cite{gallardo2021self, caccia2021special} explored self-supervised pretraining for supervised continual learning with online and few-shot tasks, and \cite{cha2021co2l} presented a supervised contrastive CL approach. Two concurrent works~\cite{lin2021continual, madaan2021rethinking} have also attempted to address CSSL recently. The former~\cite{lin2021continual} extends~\cite{cha2021co2l} to the unsupervised setting, but is specifically designed for contrastive SSL, such as~\cite{chen2020simple,he2020momentum}, and lacks generalizability to other popular SSL paradigms. The latter~\cite{madaan2021rethinking} is also limited as it only shows small-scale experiments in the class-incremental setting and considers just two SSL methods. In contrast, we present a general framework for CSSL with superior performance, conduct large-scale experiments on three challenging settings, thereby presenting a deeper analysis of CSSL.