\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{achille2018life}
Alessandro Achille, Tom Eccles, Loic Matthey, Christopher~P Burgess, Nick
  Watters, Alexander Lerchner, and Irina Higgins.
\newblock Life-long disentangled representation learning with cross-domain
  latent homologies.
\newblock {\em NeurIPS}, 2018.

\bibitem{Aljundi17}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em ECCV}, 2018.

\bibitem{bardes2021vicreg}
Adrien Bardes, Jean Ponce, and Yann LeCun.
\newblock Vicreg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock {\em arXiv preprint arXiv:2105.04906}, 2021.

\bibitem{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
  Calderara.
\newblock Dark experience for general continual learning: a strong, simple
  baseline.
\newblock In {\em NeurIPS}, 2020.

\bibitem{caccia2021special}
Lucas Caccia and Joelle Pineau.
\newblock Special: Self-supervised pretraining for continual learning.
\newblock {\em arXiv preprint arXiv:2106.09065}, 2021.

\bibitem{caron2018deep}
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In {\em ECCV}, pages 132--149, 2018.

\bibitem{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock {\em NeurIPS}, 2020.

\bibitem{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal,
  Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock {\em ICCV}, 2021.

\bibitem{castro2018end}
Francisco~M Castro, Manuel~J Marin-Jimenez, Nicolas Guil, Cordelia Schmid, and
  Karteek Alahari.
\newblock End-to-end incremental learning.
\newblock In {\em ECCV}, 2018.

\bibitem{cha2021co2l}
Hyuntak Cha, Jaeho Lee, and Jinwoo Shin.
\newblock Co2l: Contrastive continual learning.
\newblock In {\em CVPR}, pages 9516--9525, 2021.

\bibitem{chaudhry2018riemannian}
Arslan Chaudhry, Puneet~K Dokania, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In {\em ECCV}, 2018.

\bibitem{Chaudhry19}
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed
  Elhoseiny.
\newblock Efficient lifelong learning with a-gem.
\newblock In {\em ICLR}, 2019.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em ICML}, 2020.

\bibitem{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{chen2021exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In {\em ICCV}, 2021.

\bibitem{turrisi2021sololearn}
Victor Guilherme~Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa
  Ricci.
\newblock solo-learn: A library of self-supervised methods for visual
  representation learning.
\newblock {\em Journal of Machine Learning Research}, 23(56):1--6, 2022.

\bibitem{de2019continual}
Matthias De~Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales
  Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.
\newblock Continual learning: A comparative study on how to defy forgetting in
  classification tasks.
\newblock {\em IEEE TPAMI}, 2(6), 2019.

\bibitem{douillard2020podnet}
Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo
  Valle.
\newblock Podnet: Pooled outputs distillation for small-tasks incremental
  learning.
\newblock In {\em ECCV}, 2020.

\bibitem{dwibedi2021little}
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew
  Zisserman.
\newblock With a little help from my friends: Nearest-neighbor contrastive
  learning of visual representations.
\newblock {\em ICCV}, 2021.

\bibitem{ermolov2021whitening}
Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe.
\newblock Whitening for self-supervised representation learning.
\newblock In {\em ICML}, 2021.

\bibitem{falcon2021aavae}
William Falcon, Ananya~Harsh Jha, Teddy Koker, and Kyunghyun Cho.
\newblock Aavae: Augmentation-augmented variational autoencoders.
\newblock {\em arXiv preprint arXiv:2107.12329}, 2021.

\bibitem{fini2020online}
Enrico Fini, Stèphane Lathuilière, Enver Sangineto, Moin Nabi, and Elisa
  Ricci.
\newblock Online continual learning under extreme memory constraints.
\newblock In {\em ECCV}, 2020.

\bibitem{French99}
Robert~M. French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock {\em Trends in Cognitive Sciences}, 3(4):128--135, 1999.

\bibitem{gallardo2021self}
Jhair Gallardo, Tyler~L Hayes, and Christopher Kanan.
\newblock Self-supervised training enhances online continual learning.
\newblock {\em arXiv preprint arXiv:2103.14010}, 2021.

\bibitem{Goodfellow13}
I.~J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock {\em arXiv preprint arXiv:1312.6211}, 2013.

\bibitem{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec,
  Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock {\em NeurIPS}, 2020.

\bibitem{gutmann2010noise}
Michael Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In {\em AISTATS}, 2010.

\bibitem{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em CVPR}, 2020.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hou2019learning}
Saihui Hou, Xinyu Pan, Chen~Change Loy, Zilei Wang, and Dahua Lin.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In {\em CVPR}, pages 831--839, 2019.

\bibitem{iscen2020memory}
Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, and Cordelia Schmid.
\newblock Memory-efficient incremental learning through feature adaptation.
\newblock In {\em European Conference on Computer Vision}, pages 699--715.
  Springer, 2020.

\bibitem{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock {\em NeurIPS}, 2020.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proc. of the national academy of sciences}, 2017.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Univ.\ Toronto, 2009.

\bibitem{Li17learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock {\em IEEE TPAMI}, 2017.

\bibitem{lin2021continual}
Zhiwei Lin, Yongtao Wang, and Hongxiang Lin.
\newblock Continual contrastive self-supervised learning for image
  classification.
\newblock {\em arXiv preprint arXiv:2107.01776}, 2021.

\bibitem{Lopez-Paz17}
David Lopez-Paz and Marc-Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In {\em NeurIPS}, 2017.

\bibitem{madaan2021rethinking}
Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung~Ju Hwang.
\newblock Rethinking the representational continuity: Towards unsupervised
  continual learning.
\newblock {\em arXiv preprint arXiv:2110.06976}, 2021.

\bibitem{Mccloskey89}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In {\em Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{ostapenko2019learning}
Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin
  Nabi.
\newblock Learning to remember: A synaptic plasticity driven framework for
  continual learning.
\newblock In {\em CVPR}, 2019.

\bibitem{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In {\em ICCV}, 2019.

\bibitem{prabhu2020gdumb}
Ameya Prabhu, Philip~HS Torr, and Puneet~K Dokania.
\newblock Gdumb: A simple approach that questions our progress in continual
  learning.
\newblock In {\em ECCV}, pages 524--540. Springer, 2020.

\bibitem{rao2019continual}
Dushyant Rao, Francesco Visin, Andrei~A Rusu, Yee~Whye Teh, Razvan Pascanu, and
  Raia Hadsell.
\newblock Continual unsupervised representation learning.
\newblock {\em NeurIPS}, 2019.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em CVPR}, 2017.

\bibitem{Robins95}
Anthony Robins.
\newblock Catastrophic forgetting, rehearsal and pseudorehearsal.
\newblock {\em Connection Science}, 1995.

\bibitem{Rusu16progressive}
A.~A. Rusu, N.~C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K.
  Kavukcuoglu, R. Pascanu, and R. Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv 1606.04671}, 2016.

\bibitem{serra2018overcoming}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock {\em ICML}, 2018.

\bibitem{shin2017continual}
Hanul Shin, Jung~Kwon Lee, Jaehong Kim, and Jiwon Kim.
\newblock Continual learning with deep generative replay.
\newblock In {\em NeurIPS}, 2017.

\bibitem{smith2019unsupervised}
James Smith, Cameron Taylor, Seth Baer, and Constantine Dovrolis.
\newblock Unsupervised progressive learning and the stam architecture.
\newblock {\em arXiv preprint arXiv:1904.02021}, 2019.

\bibitem{tian2021understanding}
Yuandong Tian, Xinlei Chen, and Surya Ganguli.
\newblock Understanding self-supervised learning dynamics without contrastive
  pairs.
\newblock In {\em International Conference on Machine Learning}, pages
  10268--10278. PMLR, 2021.

\bibitem{tian2021divide}
Yonglong Tian, Olivier~J Henaff, and Aaron van~den Oord.
\newblock Divide and contrast: Self-supervised learning from uncurated data.
\newblock {\em arXiv preprint arXiv:2105.08054}, 2021.

\bibitem{tian2020contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive multiview coding.
\newblock In {\em ECCV}, pages 776--794. Springer, 2020.

\bibitem{wu2019large}
Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and
  Yun Fu.
\newblock Large scale incremental learning.
\newblock In {\em CVPR}, 2019.

\bibitem{wu2018unsupervised}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In {\em CVPR}, 2018.

\bibitem{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock {\em arXiv preprint arXiv:1708.03888}, 2017.

\bibitem{zbontar2021barlow}
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock {\em ICML}, 2021.

\bibitem{Zenke17}
Friedeman Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em ICML}, 2017.

\end{thebibliography}
