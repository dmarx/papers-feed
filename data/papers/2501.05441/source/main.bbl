\begin{thebibliography}{101}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ln}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Biswas et~al.(2021)Biswas, Kumar, Banerjee, and Pandey]{smu}
Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, and Ashish~Kumar Pandey.
\newblock Smu: smooth activation function for deep networks using smoothing maximum technique.
\newblock \emph{arXiv preprint arXiv:2111.04682}, 2021.

\bibitem[Brock et~al.(2018)Brock, Donahue, and Simonyan]{biggan}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock \emph{arXiv preprint arXiv:1809.11096}, 2018.

\bibitem[Brock et~al.(2021)Brock, De, Smith, and Simonyan]{nfnet}
Andy Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1059--1071. PMLR, 2021.

\bibitem[Chollet(2017)]{xception}
Fran{\c{c}}ois Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  1251--1258, 2017.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and Hutter]{chrabaszcz2017downsampled}
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
\newblock A downsampled variant of imagenet as an alternative to the cifar datasets.
\newblock \emph{arXiv preprint arXiv:1707.08819}, 2017.

\bibitem[Dhariwal \& Nichol(2021)Dhariwal and Nichol]{adm}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 8780--8794, 2021.

\bibitem[Dieng et~al.(2019)Dieng, Ruiz, Blei, and Titsias]{presgan}
Adji~B Dieng, Francisco~JR Ruiz, David~M Blei, and Michalis~K Titsias.
\newblock Prescribed generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1910.04302}, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Fang et~al.(2022)Fang, Sun, and Schwing]{diggan}
Tiantian Fang, Ruoyu Sun, and Alex Schwing.
\newblock Dig{GAN}: Discriminator gradient gap regularization for {GAN} training with limited data.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=azBVn74t_2}.

\bibitem[Gidel et~al.(2019)Gidel, Hemmat, Pezeshki, Le~Priol, Huang, Lacoste-Julien, and Mitliagkas]{ganmomentum}
Gauthier Gidel, Reyhane~Askari Hemmat, Mohammad Pezeshki, R{\'e}mi Le~Priol, Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock Negative momentum for improved game dynamics.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}, pp.\  1802--1811. PMLR, 2019.

\bibitem[Gokaslan et~al.(2024)Gokaslan, Cooper, Collins, Seguin, Jacobson, Patel, Frankle, Stephenson, and Kuleshov]{gokaslan2024commoncanvas}
Aaron Gokaslan, A~Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov.
\newblock Commoncanvas: Open diffusion models trained on creative-commons images.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  8250--8260, 2024.

\bibitem[Goodfellow et~al.(2020)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{gan}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock \emph{Communications of the ACM}, 63\penalty0 (11):\penalty0 139--144, 2020.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and Courville]{wgan-gp}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C Courville.
\newblock Improved training of wasserstein gans.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{prelu}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{resnet2}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14}, pp.\  630--645. Springer, 2016{\natexlab{b}}.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{gelu}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter]{fid}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ddpm}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{bn}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\  448--456. pmlr, 2015.

\bibitem[Jolicoeur-Martineau(2018)]{rgan}
Alexia Jolicoeur-Martineau.
\newblock The relativistic discriminator: a key element missing from standard gan.
\newblock \emph{arXiv preprint arXiv:1807.00734}, 2018.

\bibitem[Jolicoeur-Martineau \& Mitliagkas(2019)Jolicoeur-Martineau and Mitliagkas]{ganmmc}
Alexia Jolicoeur-Martineau and Ioannis Mitliagkas.
\newblock Gradient penalty from a maximum margin perspective.
\newblock \emph{arXiv preprint arXiv:1910.06922}, 2019.

\bibitem[Jolicoeur-Martineau et~al.(2020)Jolicoeur-Martineau, Pich{\'e}-Taillefer, Combes, and Mitliagkas]{advsm}
Alexia Jolicoeur-Martineau, R{\'e}mi Pich{\'e}-Taillefer, R{\'e}mi Tachet~des Combes, and Ioannis Mitliagkas.
\newblock Adversarial score matching and improved sampling for image generation.
\newblock \emph{arXiv preprint arXiv:2009.05475}, 2020.

\bibitem[Kang et~al.(2023{\natexlab{a}})Kang, Shin, and Park]{studio}
Minguk Kang, Joonghyuk Shin, and Jaesik Park.
\newblock Studiogan: a taxonomy and benchmark of gans for image synthesis.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2023{\natexlab{a}}.

\bibitem[Kang et~al.(2023{\natexlab{b}})Kang, Zhu, Zhang, Park, Shechtman, Paris, and Park]{gigagan}
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.
\newblock Scaling up gans for text-to-image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  10124--10134, 2023{\natexlab{b}}.

\bibitem[Karnewar \& Wang(2020)Karnewar and Wang]{karnewar2020msg}
Animesh Karnewar and Oliver Wang.
\newblock Msg-gan: Multi-scale gradients for generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  7799--7808, 2020.

\bibitem[Karras et~al.(2017)Karras, Aila, Laine, and Lehtinen]{pggan}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and variation.
\newblock \emph{arXiv preprint arXiv:1710.10196}, 2017.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{sg1}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  4401--4410, 2019.

\bibitem[Karras et~al.(2020{\natexlab{a}})Karras, Aittala, Hellsten, Laine, Lehtinen, and Aila]{sg2ada}
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila.
\newblock Training generative adversarial networks with limited data.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 12104--12114, 2020{\natexlab{a}}.

\bibitem[Karras et~al.(2020{\natexlab{b}})Karras, Laine, Aittala, Hellsten, Lehtinen, and Aila]{sg2}
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
\newblock Analyzing and improving the image quality of stylegan.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  8110--8119, 2020{\natexlab{b}}.

\bibitem[Karras et~al.(2021)Karras, Aittala, Laine, H{\"a}rk{\"o}nen, Hellsten, Lehtinen, and Aila]{sg3}
Tero Karras, Miika Aittala, Samuli Laine, Erik H{\"a}rk{\"o}nen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
\newblock Alias-free generative adversarial networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 852--863, 2021.

\bibitem[Karras et~al.(2022)Karras, Aittala, Aila, and Laine]{edm}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 26565--26577, 2022.

\bibitem[Karras et~al.(2023)Karras, Aittala, Lehtinen, Hellsten, Aila, and Laine]{edm2}
Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine.
\newblock Analyzing and improving the training dynamics of diffusion models.
\newblock \emph{arXiv preprint arXiv:2312.02696}, 2023.

\bibitem[Kim et~al.(2021)Kim, Shin, Song, Kang, and Moon]{kim2021soft}
Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon.
\newblock Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation.
\newblock \emph{arXiv preprint arXiv:2106.05527}, 2021.

\bibitem[Kingma et~al.(2021)Kingma, Salimans, Poole, and Ho]{kingma2021variational}
Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock Variational diffusion models.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 21696--21707, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Thesis}, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In F.~Pereira, C.J. Burges, L.~Bottou, and K.Q. Weinberger (eds.), \emph{Advances in Neural Information Processing Systems}, volume~25. Curran Associates, Inc., 2012.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}.

\bibitem[Kumar et~al.(2019)Kumar, Ozair, Goyal, Courville, and Bengio]{meg}
Rithesh Kumar, Sherjil Ozair, Anirudh Goyal, Aaron Courville, and Yoshua Bengio.
\newblock Maximum entropy generators for energy-based models.
\newblock \emph{arXiv preprint arXiv:1901.08508}, 2019.

\bibitem[Kynk{\"a}{\"a}nniemi et~al.(2019)Kynk{\"a}{\"a}nniemi, Karras, Laine, Lehtinen, and Aila]{precrecall}
Tuomas Kynk{\"a}{\"a}nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila.
\newblock Improved precision and recall metric for assessing generative models.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Kynk{\"a}{\"a}nniemi et~al.(2022)Kynk{\"a}{\"a}nniemi, Karras, Aittala, Aila, and Lehtinen]{kynkaanniemi2022role}
Tuomas Kynk{\"a}{\"a}nniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen.
\newblock The role of imagenet classes in fr{\'e}chet inception distance.
\newblock \emph{arXiv preprint arXiv:2203.06026}, 2022.

\bibitem[Lee et~al.(2021)Lee, Chang, Jiang, Zhang, Tu, and Liu]{vitgan}
Kwonjoon Lee, Huiwen Chang, Lu~Jiang, Han Zhang, Zhuowen Tu, and Ce~Liu.
\newblock Vitgan: Training gans with vision transformers.
\newblock \emph{arXiv preprint arXiv:2107.04589}, 2021.

\bibitem[Lim et~al.(2017)Lim, Son, Kim, Nah, and Mu~Lee]{edsr}
Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu~Lee.
\newblock Enhanced deep residual networks for single image super-resolution.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition workshops}, pp.\  136--144, 2017.

\bibitem[Lim \& Ye(2017)Lim and Ye]{hingegan}
Jae~Hyun Lim and Jong~Chul Ye.
\newblock Geometric gan.
\newblock \emph{arXiv preprint arXiv:1705.02894}, 2017.

\bibitem[Lin et~al.(2021)Lin, Zhang, Ganz, Han, and Zhu]{anycostgan}
Ji~Lin, Richard Zhang, Frieder Ganz, Song Han, and Jun-Yan Zhu.
\newblock Anycost gans for interactive image synthesis and editing.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  14986--14996, 2021.

\bibitem[Lin et~al.(2018)Lin, Khetan, Fanti, and Oh]{pacgan}
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh.
\newblock Pacgan: The power of two samples in generative adversarial networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  10012--10022, 2021.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and Xie]{convnext}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  11976--11986, 2022.

\bibitem[Lu et~al.(2023)Lu, Poppe, et~al.]{compdiff}
Hui Lu, Ronald Poppe, et~al.
\newblock Compensation sampling for improved convergence in diffusion models.
\newblock \emph{arXiv preprint arXiv:2312.06285}, 2023.

\bibitem[Mao et~al.(2017)Mao, Li, Xie, Lau, Wang, and Paul~Smolley]{lsgan}
Xudong Mao, Qing Li, Haoran Xie, Raymond~YK Lau, Zhen Wang, and Stephen Paul~Smolley.
\newblock Least squares generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  2794--2802, 2017.

\bibitem[Mescheder et~al.(2017)Mescheder, Nowozin, and Geiger]{gannum}
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger.
\newblock The numerics of gans.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and Nowozin]{r1}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for gans do actually converge?
\newblock In \emph{International conference on machine learning}, pp.\  3481--3490. PMLR, 2018.

\bibitem[Metz et~al.(2016)Metz, Poole, Pfau, and Sohl-Dickstein]{metz2016unrolled}
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein.
\newblock Unrolled generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Miyato \& Koyama(2018)Miyato and Koyama]{cgans}
Takeru Miyato and Masanori Koyama.
\newblock cgans with projection discriminator.
\newblock \emph{arXiv preprint arXiv:1802.05637}, 2018.

\bibitem[Nagarajan \& Kolter(2017)Nagarajan and Kolter]{nagarajan2017gradient}
Vaishnavh Nagarajan and J~Zico Kolter.
\newblock Gradient descent gan optimization is locally stable.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ning et~al.(2023)Ning, Sangineto, Porrello, Calderara, and Cucchiara]{ning2023input}
Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, and Rita Cucchiara.
\newblock Input perturbation reduces exposure bias in diffusion models.
\newblock \emph{arXiv preprint arXiv:2301.11706}, 2023.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{nowozin2016f}
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.
\newblock f-gan: Training generative neural samplers using variational divergence minimization.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Peebles \& Xie(2023)Peebles and Xie]{dit}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  4195--4205, 2023.

\bibitem[Preechakul et~al.(2022)Preechakul, Chatthee, Wizadwongsa, and Suwajanakorn]{diffae}
Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn.
\newblock Diffusion autoencoders: Toward a meaningful and decodable representation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  10619--10629, June 2022.

\bibitem[Radford et~al.(2015)Radford, Metz, and Chintala]{dcgan}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1511.06434}, 2015.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and Le]{swish}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le.
\newblock Searching for activation functions.
\newblock \emph{arXiv preprint arXiv:1710.05941}, 2017.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and Brox]{unet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18}, pp.\  234--241. Springer, 2015.

\bibitem[Roth et~al.(2017)Roth, Lucchi, Nowozin, and Hofmann]{r1r2}
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann.
\newblock Stabilizing training of generative adversarial networks through regularization.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Sadat et~al.(2024)Sadat, Buhmann, Bradley, Hilliges, and Weber]{litevae}
Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann~M Weber.
\newblock Litevae: Lightweight and efficient variational autoencoders for latent diffusion models.
\newblock \emph{arXiv preprint arXiv:2405.14477}, 2024.

\bibitem[Sahoo et~al.(2023)Sahoo, Gokaslan, De~Sa, and Kuleshov]{sahoo2023diffusion}
Subham~Sekhar Sahoo, Aaron Gokaslan, Chris De~Sa, and Volodymyr Kuleshov.
\newblock Diffusion models with learned adaptive noise.
\newblock \emph{arXiv preprint arXiv:2312.13236}, 2023.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and Chen]{mobnet}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  4510--4520, 2018.

\bibitem[Sauer et~al.(2021)Sauer, Chitta, M{\"u}ller, and Geiger]{sauer2021projected}
Axel Sauer, Kashyap Chitta, Jens M{\"u}ller, and Andreas Geiger.
\newblock Projected gans converge faster.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 17480--17492, 2021.

\bibitem[Sauer et~al.(2022)Sauer, Schwarz, and Geiger]{sgxl}
Axel Sauer, Katja Schwarz, and Andreas Geiger.
\newblock {StyleGAN-XL}: Scaling stylegan to large diverse datasets.
\newblock In \emph{ACM SIGGRAPH 2022 conference proceedings}, pp.\  1--10, 2022.

\bibitem[Sauer et~al.(2023)Sauer, Karras, Laine, Geiger, and Aila]{sg-t}
Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila.
\newblock Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis.
\newblock In \emph{International conference on machine learning}, pp.\  30105--30118. PMLR, 2023.

\bibitem[Shi et~al.(2016{\natexlab{a}})Shi, Caballero, Husz{\'a}r, Totz, Aitken, Bishop, Rueckert, and Wang]{pixshuffle}
Wenzhe Shi, Jose Caballero, Ferenc Husz{\'a}r, Johannes Totz, Andrew~P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang.
\newblock Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  1874--1883, 2016{\natexlab{a}}.

\bibitem[Shi et~al.(2016{\natexlab{b}})Shi, Caballero, Husz{\'a}r, Totz, Aitken, Bishop, Rueckert, and Wang]{subpixel}
Wenzhe Shi, Jose Caballero, Ferenc Husz{\'a}r, Johannes Totz, Andrew~P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang.
\newblock Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  1874--1883, 2016{\natexlab{b}}.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Singh et~al.(2023)Singh, Shukla, and Turaga]{singh2023polynomial}
Rajhans Singh, Ankita Shukla, and Pavan Turaga.
\newblock Polynomial implicit neural representations for large diverse datasets.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  2041--2051, 2023.

\bibitem[S{\o}nderby et~al.(2016)S{\o}nderby, Caballero, Theis, Shi, and Husz{\'a}r]{instancenoise}
Casper~Kaae S{\o}nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Husz{\'a}r.
\newblock Amortised map inference for image super-resolution.
\newblock \emph{arXiv preprint arXiv:1610.04490}, 2016.

\bibitem[Song et~al.(2021)Song, Meng, and Ermon]{ddim}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Song \& Dhariwal(2024)Song and Dhariwal]{icm}
Yang Song and Prafulla Dhariwal.
\newblock Improved techniques for training consistency models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Song et~al.(2020)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{sde}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock \emph{arXiv preprint arXiv:2011.13456}, 2020.

\bibitem[Song et~al.(2023)Song, Dhariwal, Chen, and Sutskever]{cm}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  32211--32252. PMLR, 2023.

\bibitem[Srivastava et~al.(2017)Srivastava, Valkov, Russell, Gutmann, and Sutton]{srivastava2017veegan}
Akash Srivastava, Lazar Valkov, Chris Russell, Michael~U Gutmann, and Charles Sutton.
\newblock Veegan: Reducing mode collapse in gans using implicit variational learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Sun et~al.(2020)Sun, Fang, and Schwing]{rpgan}
Ruoyu Sun, Tiantian Fang, and Alexander Schwing.
\newblock Towards a better global loss landscape of gans.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 10186--10198, 2020.

\bibitem[Takida et~al.(2024)Takida, Imaizumi, Shibuya, Lai, Uesaka, Murata, and Mitsufuji]{takida2024san}
Yuhta Takida, Masaaki Imaizumi, Takashi Shibuya, Chieh-Hsin Lai, Toshimitsu Uesaka, Naoki Murata, and Yuki Mitsufuji.
\newblock {SAN}: Inducing metrizability of {GAN} with discriminative normalized linear layer.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=eiF7TU1E8E}.

\bibitem[Tao \& Wang(2020)Tao and Wang]{r1gradexpcvpr}
Song Tao and Jia Wang.
\newblock Alleviation of gradient exploding in gans: Fake can be real.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  1191--1200, 2020.

\bibitem[Thanh-Tung et~al.(2019)Thanh-Tung, Tran, and Venkatesh]{r1gradexp}
Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh.
\newblock Improving generalization and stability of generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ByxPYjC5KQ}.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and Lempitsky]{in}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{arXiv preprint arXiv:1607.08022}, 2016.

\bibitem[Vahdat et~al.(2021)Vahdat, Kreis, and Kautz]{lsgm}
Arash Vahdat, Karsten Kreis, and Jan Kautz.
\newblock Score-based generative modeling in latent space.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 11287--11302, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{trans}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2018)Wang, Yu, Wu, Gu, Liu, Dong, Qiao, and Change~Loy]{esrgan}
Xintao Wang, Ke~Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu~Qiao, and Chen Change~Loy.
\newblock Esrgan: Enhanced super-resolution generative adversarial networks.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV) workshops}, pp.\  0--0, 2018.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Schiff, Gokaslan, Pan, Wang, De~Sa, and Kuleshov]{wang2023infodiffusion}
Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De~Sa, and Volodymyr Kuleshov.
\newblock Infodiffusion: Representation learning using information maximizing diffusion models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  36336--36354. PMLR, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zheng, He, Chen, and Zhou]{diffusiongan}
Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou.
\newblock Diffusion-gan: Training gans with diffusion.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Wu \& He(2018{\natexlab{a}})Wu and He]{gn}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pp.\  3--19, 2018{\natexlab{a}}.

\bibitem[Wu \& He(2018{\natexlab{b}})Wu and He]{groupnorm}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pp.\  3--19, 2018{\natexlab{b}}.

\bibitem[Xiao et~al.(2020)Xiao, Kreis, Kautz, and Vahdat]{vaebm}
Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat.
\newblock Vaebm: A symbiosis between variational autoencoders and energy-based models.
\newblock \emph{arXiv preprint arXiv:2010.00654}, 2020.

\bibitem[Xiao et~al.(2021)Xiao, Kreis, and Vahdat]{ddgan}
Zhisheng Xiao, Karsten Kreis, and Arash Vahdat.
\newblock Tackling the generative learning trilemma with denoising diffusion gans.
\newblock \emph{arXiv preprint arXiv:2112.07804}, 2021.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and He]{resnext}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  1492--1500, 2017.

\bibitem[Yin et~al.(2024)Yin, Gharbi, Zhang, Shechtman, Durand, Freeman, and Park]{dmd}
Tianwei Yin, Micha{\"e}l Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William~T Freeman, and Taesung Park.
\newblock One-step diffusion with distribution matching distillation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  6613--6623, 2024.

\bibitem[Yu et~al.(2022)Yu, Luo, Zhou, Si, Zhou, Wang, Feng, and Yan]{metaformer}
Weihao Yu, Mi~Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan.
\newblock Metaformer is actually what you need for vision.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10819--10829, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Gu, Zhang, Bao, Chen, Wen, Wang, and Guo]{zhang2022styleswin}
Bowen Zhang, Shuyang Gu, Bo~Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, and Baining Guo.
\newblock Styleswin: Transformer-based gan for high-resolution image generation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  11304--11314, 2022.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019.

\bibitem[Zhang(2019)]{blurpool}
Richard Zhang.
\newblock Making convolutional networks shift-invariant again.
\newblock In \emph{International conference on machine learning}, pp.\  7324--7334. PMLR, 2019.

\bibitem[Zhao et~al.(2021)Zhao, Singh, Lee, Zhang, Odena, and Zhang]{zhao2021improved}
Zhengli Zhao, Sameer Singh, Honglak Lee, Zizhao Zhang, Augustus Odena, and Han Zhang.
\newblock Improved consistency regularization for gans.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pp.\  11033--11041, 2021.

\end{thebibliography}
