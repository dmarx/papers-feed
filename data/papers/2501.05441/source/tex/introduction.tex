\section{Introduction}
\label{sec:intro}
Generative adversarial networks~(GANs) let us generate high-quality images in a single forward pass. 
% and thanks to the flexibility permitted by adversarial training. 
However, the original objective in Goodfellow~\etal~\cite{gan}, is notoriously difficult to optimize due to its minimax nature. This leads to a fear that training might diverge at any point due to instability, and a fear that generated images might lose diversity through mode collapse. While there has been progress in GAN objectives~\cite{wgan-gp,rgan,rpgan,r1,r1r2}, practically, the effects of brittle losses are still regularly felt. This notoriety has had a lasting negative impact on GAN research.

A complementary issue---partly motivated by this instability---is that existing popular GAN backbones like StyleGAN~\cite{sg1,sg2,sg2ada,sg3} use many poorly-understood empirical tricks with little theory. 
% These reinforce notoriety.
For instance, StyleGAN uses a gradient penalized non-saturating loss~\cite{r1} to increase stability (affecting sample diversity), but then employs a minibatch standard deviation trick~\cite{pggan} to increase sample diversity.
Without tricks, the StyleGAN backbone still resembles DCGAN~\cite{dcgan} from 2015, yet it is still the common backbone of SOTA GANs such as GigaGAN~\cite{gigagan} and StyleGAN-T~\cite{sg-t}.
Advances in GANs have been conservative compared to other generative models such as diffusion models~\cite{ddpm,sde,edm,edm2}, where modern computer vision techniques such as multi-headed self attention~\cite{trans} and backbones such as preactivated ResNet~\cite{resnet2}, U-Net~\cite{unet} and vision transformers (ViTs)~\cite{vit} are the norm. 
Given outdated backbones, it is not surprising that there is a widely-spread belief that GANs do not scale in terms of quantitative metrics like Frechet Inception Distance~\cite{fid}. 
% In terms of overall performance, GANs have fallen behind diffusion models on the state of art of FID calculations. 
% \vk{Do not scale: what does it mean? that GANs cannot achieve the same FID as diffusion? this can be made more precise} \vk{aren't there other recent work that shows GANs can be competitive with diffusion? it should be cited and acknowledged (at least in a previous work section, and would probably need to be under baselines}

We reconsider this situation: we show that by combining progress in objectives into a regularized training loss, GANs gain improved training stability, which allows us to upgrade GANs with modern backbones. 
% We reconsider this situation: we show that, with a newly proposed objective, GANs feature improved training stability, and that this lets GANs exploit modern backbones. 
First, we propose a novel objective that augments the relativistic pairing GAN loss (RpGAN; \cite{rgan}) with zero-centered gradient penalties~\cite{r1,r1r2}, improving stability~\cite{wgan-gp,r1r2,r1}. 
% Then, following Mescheder~\etal~\cite{r1} who proved gradient penalties to be crucial to GAN convergence~\cite{r1}, 
We show mathematically that gradient-penalized RpGAN enjoys the same guarantee of local convergence as regularized classic GANs, and that removing our regularization scheme induces non-convergence. 

Once we have a well-behaved loss, none of the GAN tricks are necessary~\cite{pggan,sg2}, and we are free to engineer a modern SOTA backbone architecture. We strip StyleGAN of all its features, identify those that are essential, then borrow new architecture designs from modern ConvNets and transformers~\cite{convnext,metaformer}. Briefly, we find that proper ResNet design~\cite{resnet2,mobnet}, initialization~\cite{fixup}, and resampling~\cite{sg1,sg2,sg3,blurpool} are important, along with grouped convolution~\cite{resnext,xception} and no normalization~\cite{sg2,edm2,wgan-gp,esrgan,nfnet}. This leads to a design that is simpler than StyleGAN and improves FID performance for the same network capacity (2.75 vs.~3.78 on FFHQ-256).


% Equipped by the confidence that the improved loss allows us to experiment with any network architecture without training failures, we build a road map to simplify then modernize StyleGAN2~\cite{sg2}, the common backbone of all SOTA GANs such as GigaGAN~\cite{gigagan} and StyleGAN-T~\cite{sg-t}. 
% We first simplify StyleGAN2 to its bare minimum, this involves not only the removal of all tricks it has accumulated all the way back from progressive GAN~\cite{pggan} to its current version~\cite{sg1,sg2}, but the removal of the style-based generator as well, since we wish to build a minimalist baseline. This bare minimum model also allows us to pinpoint the few remaining features that are essential to the success of StyleGAN, thus providing us a guideline as we borrow architectural designs from modern ConvNets such as ConvNeXt~\cite{convnext}. Subsequently, we apply our loss function to make room for modernization. Among the simplified model and numerous architectural changes introduced in ConvNeXt, we find that a proper ResNet design~\cite{resnet2,mobnet}, not using normalization~\cite{sg2,edm2,wgan-gp,esrgan,nfnet}, careful initialization~\cite{fixup}, proper resampling~\cite{sg1,sg2,sg3,blurpool} and grouped convolution~\cite{resnext,xception} are essential to the performance. With all our contributions combined, our minimalistic model achieves superior performance than the StyleGAN family and sets a strong future baseline.

%\textbf{We debunk this belief} \vk{this sounds a bit strong and non-academic? i had some concerns about using that in mulan as well} 

% and show that:
% \begin{itemize}
%     \item None of the GAN tricks~\cite{pggan,sg2} are necessary once we have a more well-behaved loss~\cite{rgan,rpgan,r1,r1r2}.
%     \item With training instability no longer a concern, we are free to eliminate all GAN tricks and modernize the network backbone using SOTA architectures~\cite{convnext,metaformer} and drastically improve the performance.
% \end{itemize}
In summary, our work first argues mathematically that GANs need not be tricky to train via an improved regularized loss.
%\vk{one thing to be mindful of: if you claim that it is easy to train GAN, reviewers may ask for evidence; you may consider having extra experiments that confirm this (or explaining how you already have it), or you can make the claim more precise: e.g., the architecture is simpler}. 
Then, it empirically develops a simple GAN baseline that, without any tricks, compares favorably by FID to StyleGAN~\cite{sg1,sg2,sg3}, other SOTA GANs~\cite{biggan,vitgan,ddgan}, and diffusion models~\cite{ddpm,sde,lsgm} across FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets. %Our findings are another example of Sutton's bitter lesson, that using a simple modern model beats an outdated model with many tricks in the long run.

%The StyleGAN family, being the backbone of SOTA models such as GigaGAN, has accumulated many non-essential tricks over the years. A non-exhaustive list includes latent vector normalization, minibatch standard derivation, equalized learning rate, feature map normalization, noise injection, residual rescaling, output skips, and many regularizations. Almost none have a strong theoretical justification; we show that none are necessary. Certain features such as the style-basis or alias-free generator we also leave out since we wish to build a minimalist baseline.

%\vk{We haven't yet mentioned the math as an interesting contribution of the paper; it would be good to acknowledge it explicitly in the intro as something non-trivial that we have done}

% \section{Detailed Overview}

% \vk{What is below is interesting/important but way too verbse for an intro. Snippets of it should be integrated with what is above and the rest moved to the methods section, or even the appendix}

% To facilitate the adoption of more powerful backbones, we first need a well-behaved training objective that is resilient to common GAN optimization problems like mode dropping and non-convergence, so the full power of the backbone can be unleashed. 

% The gradient penalized non-saturating loss~\cite{r1} in StyleGAN is reasonably stable but can still fail to converge and does not prevent mode droppping. 

% However, we show later that it could still fail to converge and moreover, it does not prevent mode dropping. 

% As a result, StyleGAN resorts to tricks like minibatch standard deviation~\cite{pggan} to increase sample diversity.

% . Since tricks are highly undesirable, we address these two issues more directly by adopting improved loss functions with strong theoretical motivations. 

% We adopt relativistic pairing GAN (RpGAN) by Jolicoeur-Martineau~\etal~\cite{rgan} as the main GAN loss. Sun~\etal showed that RpGAN is especially effective against mode dropping~\cite{rpgan} as the loss landscape of RpGAN contains no local minima that correspond to mode dropping solutions. Next, we apply zero-centered gradient penalties~\cite{r1,r1r2} to RpGAN. Gradient penalty is a well known technique that stabilizes GAN training~\cite{wgan-gp,r1r2,r1} and has been proven to be crucial to GAN convergence~\cite{r1}. As our contribution to the theory, we follow Mescheder~\etal~\cite{r1} and prove that gradient-penalized RpGAN enjoys the same guarantee of local convergence as regularized classic GANs. In addition to theoretical guarantees, our empirical results suggest that gradient penalized RpGAN is sufficiently well-behaved, to the extent that allows us to remove all GAN tricks without encountering non-convergence or mode dropping.


%First, we validate that our baseline does not suffer from common GAN optimization problems like mode dropping and non-convergence. We modify the loss proposed by Mescheder~\etal. We replace the non-saturating loss with relativistic loss (RpGAN). Sun~\etal showed that RpGAN is resilient to mode dropping as the loss landscape of RpGAN contains no local minima that corresponds to mode drops. Next, we prove that gradient-penalized RpGAN is locally convergent following Mescheder~\etal. Finally, we show that, as predicted by Jolicoeur-Martineau~\etal, our modified loss turns the discriminator into a maximum margin classifier, maximizing the relativistic paired margin.

%roadmap to modernize backbone, improve training stuff not all convnext useful stuff from cfg b

%The architecture of the StyleGAN family is disconnected from recent advancements in CNNs such as ConvNeXt and ConvFormer; it is also not scalable as noted by Sauer~\etal. We remedy this by redesigning a modern CNN backbone for the generator and the discriminator, taking inspiration from ConvNeXt. The ResNet design adopted for the StyleGAN discriminator is sub-optimal. We apply a ResNet design to both the generator and the discriminator, then note that this makes the output skips in the StyleGAN generator redundant. Finally, from ConvNeXt we take grouped convolution and the inverted bottleneck to enhance representation power. Together with our modified loss, our minimalistic model achieves superior performance than the StyleGAN family and sets a strong future baseline.

% Equipped by the confidence that the improved loss allows us to experiment with any network architecture without training failures, we build a road map to simplify then modernize StyleGAN2~\cite{sg2}, the common backbone of all SOTA GANs such as GigaGAN~\cite{gigagan} and StyleGAN-T~\cite{sg-t}. We first simplify StyleGAN2 to its bare minimum, this involves not only the removal of all tricks it has accumulated all the way back from progressive GAN~\cite{pggan} to its current version~\cite{sg1,sg2}, but the removal of the style-based generator as well, since we wish to build a minimalist baseline. This bare minimum model also allows us to pinpoint the few remaining features that are essential to the success of StyleGAN, thus providing us a guideline as we borrow architectural designs from modern ConvNets such as ConvNeXt~\cite{convnext}. Subsequently, we apply our loss function to make room for modernization. Among the simplified model and numerous architectural changes introduced in ConvNeXt, we find that a proper ResNet design~\cite{resnet2,mobnet}, not using normalization~\cite{sg2,edm2,wgan-gp,esrgan,nfnet}, careful initialization~\cite{fixup}, proper resampling~\cite{sg1,sg2,sg3,blurpool} and grouped convolution~\cite{resnext,xception} are essential to the performance. With all our contributions combined, our minimalistic model achieves superior performance than the StyleGAN family and sets a strong future baseline.