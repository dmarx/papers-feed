

%------------------------------------------------------------------------
\section{Serving Two Masters: Stability and Diversity with RpGAN \texorpdfstring{$+ R_1+R_2$}{R-1R-2}}
%---An Improved Training Objective \vk{title can be tweaked; i recommend naming your new loss}}
% James Tompkin: I wrote 'serving two masters'. Note that 'serving two masters' is inherently a religious label; in the bible, Matthew writes that one cannot serve two masters (only God). In the traditions of monarchy (to which our title alludes in the procession of kings), a king had divine right and so avoided this problem of serving two masters (to serve God and to serve one's king is to serve two masters, except once we establish that kings are divine then this problem goes away as the same master - God - is then being served). I am not religious.
\label{sec:loss}

In defining a GAN objective, we tackle two challenges: stability and diversity. Some previous work deals with stability~\cite{sg1,sg2,sg3} and other previous work deals with mode collapse \cite{rgan}. We combine a stable method with a simple regularizer grounded by theory to overcome both.

\vspace{-1ex}
\subsection{Traditional GAN}
\vspace{-1ex}
A traditional GAN~\cite{gan,nowozin2016f} is formulated as a minimax game between a discriminator $D_\psi$ and a generator $G_\theta$. Given real data $x\sim p_\mathcal{D}$ and fake data $x\sim p_\theta$ produced by $G_\theta$, the most general form of a GAN is given by:
\begin{align}
\begin{split}
\label{eq:gan}
\resizebox{0.438\textwidth}{!}{$
\mathcal{L}(\theta,\psi)=\mathbb{E}_{z\sim p_z}\left[f\left(  D_\psi(G_\theta(z))\right)\right]+\mathbb{E}_{x\sim p_\mathcal{D}}\left[f\left( -D_\psi(x) \right)\right]
$}
\end{split}
\end{align}
\noindent where $G$ tries to minimize $\mathcal{L}$ while $D$ tries to maximize it. The choice of $f$ is flexible~\cite{lsgan,hingegan}. In particular, $f(t) = -\log(1+e^{-t})$ recovers the classic GAN by Goodfellow~\etal~\cite{gan}. For the rest of this work, this will be our choice of $f$~\cite{nowozin2016f}.

% It is shown that Eq.\ref{eq:gan} is actually convex if we can directly optimize $p_\theta$~\cite{gan,rpgan}. Though in practice, the empirical GAN loss moves fake samples past the decision boundary induced by $D$ rather than updating the density function $p_\theta$ directly. This turns out to be a much harder problem that is susceptible to two common failure cases: mode collapse/dropping\footnote{Mode collapse and mode dropping are technically two related yet distinct problems. We use these two terms interchangeably hereafter to refer to the general problem where $\supp p_\theta$ fails to fully cover $\supp p_\mathcal{D}$.} and non-convergence. \vk{Minor thing: could provide a bit more information (1 sentence) on mode dropping and collapse}
It has been shown that Equation~\ref{eq:gan} has convex properties when $p_\theta$ can be optimized directly~\cite{gan,rpgan}. However, in practical implementations, the empirical GAN loss typically shifts fake samples beyond the decision boundary set by $D$, as opposed to directly updating the density function $p_\theta$. This deviation leads to a significantly more challenging problem, characterized by susceptibility to two prevalent failure scenarios: mode collapse/dropping\footnote{While mode collapse and mode dropping are technically distinct issues, they are used interchangeably in this context to describe the common problem where $\supp\ p_\theta$ does not fully cover $\supp\ p_\mathcal{D}$.} and non-convergence.

\vspace{-1ex}
\subsection{Relativistic \texorpdfstring{$f$-GAN}{f-GAN}.}
\vspace{-1ex}
% We adopt relativistic pairing GAN (RpGAN) by Jolicoeur-Martineau~\etal~\cite{rgan} as the main GAN loss. 
% Sun~\etal showed that RpGAN is especially effective against mode dropping~\cite{rpgan} as the loss landscape of RpGAN contains no local minima that correspond to mode dropping solutions. 
% Next, we apply zero-centered gradient penalties~\cite{r1,r1r2} to RpGAN. Gradient penalty is a well known technique that stabilizes GAN training~\cite{wgan-gp,r1r2,r1} and has been proven to be crucial to GAN convergence~\cite{r1}. As our contribution to the theory, we follow Mescheder~\etal~\cite{r1} and prove that gradient-penalized RpGAN enjoys the same guarantee of local convergence as regularized classic GANs. In addition to theoretical guarantees, our empirical results suggest that gradient penalized RpGAN is sufficiently well-behaved, to the extent that allows us to remove all GAN tricks without encountering non-convergence or mode dropping.

We employ a slightly different minimax game named relativistic pairing GAN (RpGAN) by Jolicoeur-Martineau~\etal~\cite{rgan} to address mode dropping. The general RpGAN is defined as:
\begin{equation}
\label{eq:rpgan}
\mathcal{L}(\theta,\psi)=\mathbb{E}_{\substack{z\sim p_z\\x\sim p_\mathcal{D}}}\left[f\left(  D_\psi(G_\theta(z))-D_\psi(x) \right)\right]
\end{equation}
Although Eq.\ref{eq:rpgan} differs only slightly from Eq.\ref{eq:gan}, evaluating the critic difference has a fundamental impact on the landscape of $\mathcal{L}$. Since Eq.\ref{eq:gan} merely requires $D$ to separate real and fake data, in the scenario where all real and fake data can be separated by a single decision boundary, the empirical GAN loss encourages $G$ to simply move all fake samples barely past this single boundary---this degenerate solution is what we observe as mode collapse/dropping. Sun~\etal~\cite{rpgan} characterize such degenerate solutions as bad local minima in the landscape of $\mathcal{L}$, and show that Eq.\ref{eq:gan} has \emph{exponentially many} bad local minima. The culprit is the existence of a single decision boundary that naturally arises when real and fake data are considered in isolation. RpGAN introduces a simple solution by coupling real and fake data,~\ie a fake sample is critiqued by its realness \emph{relative to} a real sample, which effectively maintains a decision boundary in the neighborhood of \emph{each} real sample and hence forbids mode dropping. Sun~\etal~\cite{rpgan} show that the landscape of Eq.\ref{eq:rpgan} contains no local minima that correspond to mode dropping solutions, and that every basin is a global minimum.

\vspace{-1ex}
\subsection{Training Dynamics of RpGAN}
\vspace{-1ex}
Although the landscape result~\cite{rpgan} of RpGAN allows us to address mode dropping, the training dynamics of RpGAN have yet to be studied. The ultimate goal of Eq.~\ref{eq:rpgan} is to find the equilibrium $(\theta^*,\psi^*)$ such that $p_{\theta^*}=p_\mathcal{D}$ and $D_{\psi^*}$ is constant everywhere on $p_\mathcal{D}$. Sun~\etal~\cite{rpgan} show that $\theta^*$ is globally reachable along a non-increasing trajectory in the landscape of Eq.\ref{eq:rpgan} under reasonable assumptions. However, the existence of such a trajectory does not necessarily mean that gradient descent will find it. Jolicoeur-Martineau~\etal show empirically that unregularized RpGAN does not perform well~\cite{rgan}. 

\vspace{0.5ex}
\noindent \textbf{Proposition~\upperRomannumeral{1}.} (Informal) \emph{Unregularized RpGAN does not always converge using gradient descent.}
%\vk{Propositions should ideally be made precise or in the worst case, say it's an informal statement (but then you still have to make it intuitively understandable)}
\vspace{0.5ex}

\noindent We confirm this proposition with a proof in Appendix H. 
% In summary, we follow Mescheder~\etal~\cite{r1} and inherit their DiracGAN counterexample to apply it to RpGAN. 
We show analytically that RpGAN does not converge for certain types of $p_\mathcal{D}$, such as ones that approach a delta distribution. Thus, further regularization is necessary to fill in the missing piece of a well-behaved loss.

\vspace{-1.5ex}
\paragraph{Zero-centered gradient penalties.}
To tackle RpGAN non-convergence, we explore gradient penalties as the solution since it is proven that zero-centered gradient penalties (0-GP) facilitate convergent training for classic GANs~\cite{r1}. The two most commonly-used 0-GPs are $R_1$ and $R_2$:
\begin{equation}
\begin{aligned}
R_1(\psi)&=\frac{\gamma}{2}\mathbb{E}_{x\sim p_\mathcal{D}}\left[\left\| \nabla_x D_\psi \right \|^2\right]\\ 
R_2(\theta,\psi)&=\frac{\gamma}{2}\mathbb{E}_{x\sim p_\theta}\left[\left\| \nabla_x D_\psi \right \|^2\right]
\end{aligned}
\end{equation}
$R_1$ penalizes the gradient norm of $D$ on real data, and $R_2$ on fake data. Analysis on the training dynamics of GANs has thus far focused on local convergence~\cite{nagarajan2017gradient,gannum,r1},~\ie whether the training at least converges when $(\theta,\psi)$ are in a neighborhood of $(\theta^*,\psi^*)$. In such a scenario, the convergence behavior can be analyzed~\cite{nagarajan2017gradient,gannum,r1} by examining the spectrum of the Jacobian of the gradient vector field $\left(-\nabla_\theta\mathcal{L},\nabla_\psi\mathcal{L} \right )$ at $(\theta^*,\psi^*)$. The key insight here is that when $G$ already produces the true distribution, we want $\nabla_x D=0$, so that $G$ is not pushed away from its optimal state, and thus the training does not oscillate. $R_1$ and $R_2$ impose such a constraint when $p_\theta=p_\mathcal{D}$. This also explains why earlier attempts at gradient penalties, such as the one-centered gradient penalty (1-GP) in WGAN-GP~\cite{wgan-gp}, fail to achieve convergent training~\cite{r1} as they still encourage $D$ to have a non-zero slope when $G$ has reached optimality.

Since the same insight also applies to RpGAN, 
we extend our previous analysis and show that:
% our goal is to extend the proof of Mescheder~\etal~\cite{r1} to RpGAN and show that:

\vspace{0.5ex}
\noindent \textbf{Proposition~\upperRomannumeral{2}.} (Informal) \emph{RpGAN with $R_1$ or $R_2$ regularization is locally convergent subject to similar assumptions as in} Mescheder~\etal~\cite{r1}.
\vspace{0.5ex}

In Appendix I, our proof similarly analyzes the eigenvalues of the Jacobian of the regularized RpGAN gradient vector field at $(\theta^*,\psi^*)$. We show that all eigenvalues have a negative real part; thus, regularized RpGAN is convergent in a neighborhood of $(\theta^*,\psi^*)$ for small enough learning rates~\cite{r1}.

\vspace{-1ex}
\paragraph{Discussion.}
Another line of work~\cite{r1r2} links $R_1$ and $R_2$ to instance noise~\cite{instancenoise} as its analytical approximation. Roth et al.~\cite{r1r2} showed that for the classic GAN~\cite{gan} by Goodfellow~\etal, $R_1$ approximates convolving $p_\mathcal{D}$ with the density function of $\mathcal{N}(0, \gamma I)$, up to additional weighting and a Laplacian error term. $R_2$ likewise approximates convolving $p_\theta$ with $\mathcal{N}(0, \gamma I)$ up to similar error terms. The Laplacian error terms from $R_1$, $R_2$ cancel when $D_\psi$ approaches $D_{\psi^*}$. We do not extend Roth~\etal's proof~\cite{r1r2} to RpGAN; however, this approach might provide complimentary insights to our work, which follows the strategy of Mescheder~\etal~\cite{r1}. 

We demonstrate our loss in Appendix A where we focus on practical considerations such as global convergence. Building on Roth et al.~\cite{r1r2}, we apply both $R_1$ and $R_2$ to improve global stability.