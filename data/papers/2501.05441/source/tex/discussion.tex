\vspace{-0.2cm}
\section{Discussion and Limitations}
\vspace{-0.2cm}

We have shown that a simplification of GANs is possible for image generation tasks, built upon a more stable RpGAN$+ R_1 + R_2$ objective with mathematically-demonstrated convergence properties that still provides diverse output. This stability is what lets us re-engineer a modern network architecture without the tricks of previous methods, producing the \modelName model with competitive FID on the common datasets of Stacked-MNIST, FFHQ, CIFAR-10, and ImageNet as an empirical demonstration of the mathematical benefits.

The focus of our work is to elucidate the essential components of a minimum GAN for image generation. 
As such, we prioritize simplicity over functionality---we do not claim to beat the performance of every existing model on every dataset or task; merely to provide a new simple baseline that converges easily.
While this makes our model a possible backbone for future GANs, it also means that it is not suitable to apply our model directly to downstream applications such as image editing or controllable generation, as our model lacks dedicated features for easy image inversion or disentangled image synthesis. 
For instance, we remove style injection functionality from StyleGAN even though this has a clear use.
We also omitted common techniques that have been shown in previous literature to improve FID considerably. 
Examples include some form of adaptive normalization modulated by the latent code~\cite{adm,edm,sg1,zhang2022styleswin,dit,wang2023infodiffusion,sahoo2023diffusion}, and using multiheaded self attention at lower resolution stages~\cite{adm,edm,edm2}. 
We aim to explore these techniques in a future study. 

Further, our work is limited in its evaluation of the scalability of \modelName models. While they show promising results on 64$\times$64 ImageNet, we are yet to verify the scalability on higher resolution ImageNet data or large-scale text to image generation tasks \cite{gokaslan2024commoncanvas}.

Finally, as a method that can improve the quality of generative models, it would be amiss not to mention that generative models---especially of people---can cause direct harm (e.g., through personalized deep fakes) and societal harm through the spread of disinformation (e.g., fake influencers). 

\vspace{-0.2cm}
\section{Conclusion}
\vspace{-0.2cm}

This work introduced \modelName, a new baseline GAN that features increased stability, leverages modern architectures, and does not require ad-hoc tricks that are commonplace in existing GAN models.
Central to our approach is a regularized relativistic loss that provably features local convergence and that improves the stability of GAN training. This stable loss enables us to ablate various tricks that were previously necessary in GANs, and incorporate in their place modern deep architectures. The resulting streamlined baseline achieves competitive performance to SOTA models within its parameter size class. We anticipate that our backbone will help to drive future GAN research.



%Our model is deeper and relies on GroupedConvolution that are not as well optimized in common GPU libraries such as PyTorch. As such, they often run slower than more naive convolutions and are not as well as optimized. Furthermore,  \jt{TODO - we must have some.}