\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2020)Aghajanyan, Shrivastava, Gupta, Goyal,
  Zettlemoyer, and Gupta]{RXF}
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke
  Zettlemoyer, and Sonal Gupta.
\newblock Better fine-tuning by reducing representational collapse.
\newblock \emph{arXiv preprint arXiv:2008.03156}, 2020.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and
  Zhang]{compression_generalization_gap}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock \emph{arXiv preprint arXiv:1802.05296}, 2018.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and
  Carbin]{bert_lottery_ticket}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang
  Wang, and Michael Carbin.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and
  Manning]{what_does_bert_look_at}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D Manning.
\newblock What does bert look at? an analysis of bert's attention.
\newblock \emph{arXiv preprint arXiv:1906.04341}, 2019.

\bibitem[Clark et~al.(2020)Clark, Luong, Le, and Manning]{ELECTRA}
Kevin Clark, Minh-Thang Luong, Quoc~V Le, and Christopher~D Manning.
\newblock Electra: Pre-training text encoders as discriminators rather than
  generators.
\newblock \emph{arXiv preprint arXiv:2003.10555}, 2020.

\bibitem[Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{XLMR}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock \emph{arXiv preprint arXiv:1911.02116}, 2019.

\bibitem[Desai et~al.(2019)Desai, Zhan, and Aly]{hongyuan_lotter_ticket}
Shrey Desai, Hongyuan Zhan, and Ahmed Aly.
\newblock Evaluating lottery tickets under distributional shifts.
\newblock \emph{arXiv preprint arXiv:1910.12708}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{BERT}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{mrpc}
William~B Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proceedings of the Third International Workshop on
  Paraphrasing (IWP2005)}, 2005.

\bibitem[Hinton \& Zemel(1993)Hinton and Zemel]{min_desc_length}
Geoffrey~E Hinton and Richard Zemel.
\newblock Autoencoders, minimum description length and helmholtz free energy.
\newblock \emph{Advances in neural information processing systems}, 6:\penalty0
  3--10, 1993.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{adapter_network}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock \emph{arXiv preprint arXiv:1902.00751}, 2019.

\bibitem[Iyer et~al.(2017)Iyer, Dandekar, and Csernai]{qqp}
Shankar Iyer, Nikhil Dandekar, and Kornel Csernai.
\newblock First quora dataset release: Question pairs, 2017.
\newblock URL
  \url{https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs}.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{ALBERT}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[Le et~al.(2013)Le, Sarl{\'o}s, and Smola]{fastfood}
Quoc Le, Tam{\'a}s Sarl{\'o}s, and Alex Smola.
\newblock Fastfood-approximating kernel expansions in loglinear time.
\newblock In \emph{Proceedings of the international conference on machine
  learning}, volume~85, 2013.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{BART}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Lewis et~al.(2020)Lewis, Ghazvininejad, Ghosh, Aghajanyan, Wang, and
  Zettlemoyer]{MARGE}
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and
  Luke Zettlemoyer.
\newblock Pre-training via paraphrasing, 2020.

\bibitem[Li et~al.(2018)Li, Farkhoor, Liu, and Yosinski]{intrinsic_dimension}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock \emph{arXiv preprint arXiv:1804.08838}, 2018.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{ROBERTA}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Nie et~al.(2019)Nie, Williams, Dinan, Bansal, Weston, and Kiela]{anli}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
  Kiela.
\newblock Adversarial nli: A new benchmark for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1910.14599}, 2019.

\bibitem[Prasanna et~al.(2020)Prasanna, Rogers, and
  Rumshisky]{bert_lottery_all_winners}
Sai Prasanna, Anna Rogers, and Anna Rumshisky.
\newblock When bert plays the lottery, all tickets are winning.
\newblock \emph{arXiv preprint arXiv:2005.00561}, 2020.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{GPT}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{T5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{sst2}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pp.\  1631--1642, 2013.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{GLUE}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
  Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
  Analyzing and Interpreting Neural Networks for {NLP}}, pp.\  353--355,
  Brussels, Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-5446}.
\newblock URL \url{https://www.aclweb.org/anthology/W18-5446}.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{mnli}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pp.\  1112--1122. Association for
  Computational Linguistics, 2018.
\newblock URL \url{http://aclweb.org/anthology/N18-1101}.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{ArXiv}, pp.\  arXiv--1910, 2019.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{XLNET}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5753--5763, 2019.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{yelp_polarity}
Xiang Zhang, Junbo Zhao, and Yann LeCun.
\newblock Character-level {{Convolutional Networks}} for {{Text
  Classification}}.
\newblock \emph{arXiv:1509.01626 [cs]}, September 2015.

\end{thebibliography}
