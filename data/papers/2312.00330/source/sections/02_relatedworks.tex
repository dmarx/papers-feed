%---------------------------------
% \vspace{-0.5em}
\section{Related Works}
\label{sec:realtedworks}
%---------------------------------

\subsection{Text to Video Synthesis}
Text-to-video synthesis~(T2V) is a highly challenging task with significant application value, aiming to generate corresponding videos from text descriptions. Various approaches have been proposed, including autoregressive transformer~\cite{vaswani2017attention} models and diffusion models~\cite{DDPM, DDIM, nichol2021improved, song2020score}. 
% N{\"u}wa~\cite{wu2022nuwa} introduces a 3D transformer encoder-decoder framework to address various text-guided visual tasks including T2V generation.  Phenaki~\cite{villegas2022phenaki} presents a bidirectional masked transformer for compressing videos into discrete tokens, thereby enabling video generation. 
Video Diffusion Model~\cite{ho2022video} employs a space-time factorized U-Net to execute the diffusion process in pixel space. Imagen Video~\cite{ho2022imagen} proposes a cascade diffusion model and v-parameterization to enhance VDM. 
Another branch of techniques makes good use of pre-trained T2I models and further introduces some temporal blocks for video generation extension. CogVideo~\cite{hong2022cogvideo} builds upon CogView2~\cite{ding2022cogview2} and employs multi-frame-rate hierarchical training strategy to transition from T2I to T2V. Similarly,  Make-a-video~\cite{make-a-video}, MagicVideo~\cite{zhou2022magicvideo} and LVDM~\cite{he2022latent} inherit pretrained T2I diffusion models and extend them to T2V generation by incorporating temporal attention modules.
%Furthermore, some wrok have explored introducing additional control conditions in T2V diffusion models. Gen-1~\cite{esser2023structure} proposes a structure and content-guided VDM that utilizes frame-wise depth map to maintain structure. VideoComposer~\cite{wang2023videocomposer} focuses on video generation conditioned on multi-modal inputs, allowing textual, spatial, and temporal conditions.
%Follow Your Pose~\cite{ma2023follow} aims to generate pose-controllable character videos by employing a two-stage training process that exclusively utilizes image-pose and pose-free video. Nevertheless, example-based stylized video generation is seldom explored in the general video synthesis field.

\vspace{-0.7em}
\subsection{Stylized Image Generation}

Stylized image generation aims to create images that exhibit a specific style. Decoupling style and content is a classic challenge~\cite{tenenbaum2000separating}.
Early research primarily concentrated on image style transfer, a technique that involves the transfer of one image's style onto the content of another, requiring a source image to provide content. 
Traditional style transfer methods~\cite{hertzmann2001image,wang2004efficient, zhang2013style} employ low-level, hand-crafted features to align patches between content images and style images. Since Gatys et al.~\cite{gatys2016image} discovered that the feature maps in CNNs capture style patterns effectively, a number of studies~\cite{huang2017arbitrary, li2017universal, texler2020arbitrary, liu2021adaattn, an2021artflow, deng2022stytr2, zhang2022domain} have been denoted to utilize neural networks to achieve arbitrary style transfer. A common practice involves utilizing a pretrained VGG network~\cite{simonyan2014very} to extract style information or compute Gram matrix loss~\cite{gatys2016image} to enable self-supervised learning of visual styles.

As the field of generation models progressed, researchers began exploring stylized image generation for T2I models. Although T2I models can generate various artistic images from corresponding text prompts, words are often limited to accurately convey the stylistic elements in artistic works. Consequently, recent works have shifted towards example-guided artistic image generation. Several studies~\cite{dreambooth, shi2023instancebooth, customdiffusion, hu2022lora} developed various optimization techniques on a small collection of input images that share a common style concept. 
Inspired by Textural Inversion~(TI)~\cite{TI}, some methods~\cite{zhang2023inversion, ahn2023dreamstyler, sohn2023styledrop} propose to optimize a specific textual embedding to represent a certain style. Similarly to our work, IP-Adapter~\cite{ye2023ipadapter} trains an image adapter based on pretrained Stable Diffusion to adapt T2I models to image conditions. 
Although IP-Adapter can produce similar image variants, it fails to decouple style concepts from input images or generate images with other content through text conditions.


\vspace{-0.7em}
\subsection{Stylized Video Generation}
Building upon the foundation of stylized image generation, researchers have extended the concept to video style transfer and stylized video generation. Due to the scarcity of large-scale stylized video data, a common approach for video stylization involves applying image stylization techniques on a frame-by-frame basis. Before the advent of ML, researchers have explored methods for rendering specific artistic styles such as video watercolorization~\cite{bousseau2007video}. Early deep learning methods of video style transfer~\cite{ruder2016artistic,chen2017coherent,texler2020interactive,gao2020fast,jamrivska2019stylizing,deng2021arbitrary} apply style transfer in video sequences, generating stable stylized video sequences through the use of optical flow constraints. 
Additionally, Some video editing methods~\cite{wu2023tune,qi2023fatezero,khachatryan2023text2video,huang2023style,yang2023rerender,geyer2023tokenflow,yang2024fresco} based on pretrained T2I models also support text-guided video style transfer. Although these methods effectively improve temporal consistency, they often fail to handle frames with a large action span. Reliance on a source video also undermines flexibility. 
Similarly, certain image-to-video(I2V) methods~\cite{blattmann2023stable, xing2023dynamicrafter, xing2024tooncrafter} demonstrate capabilities in stylized video generation, particularly in the anime domain. However, I2V models still face challenges when tased with interpreting and animating highly artistic images, producing frames that veer towards realism, since real-world videos dominated its training data.

VideoComposer~\cite{wang2024videocomposer} focuses on controllable video generation, allowing multiple conditional input to govern the video generation, including structure, motion, style, etc. Although VideoComposer enables multiple controls including style, they fail to decouple style concepts, leading to limited visual quality and motion naturalness. AnimateDiff~\cite{guo2023animatediff} employs a T2I model as a base generator and adds a motion module to learn motion dynamics, which enables extending the success of personalized T2I models(e.g., LoRA~\cite{hu2022lora}, Dreambooth~\cite{dreambooth}) to video animation. However, the dependence on a personalized model restricts its ability to generate videos with arbitrary styles. Another associated research is Text2Cinemagraph~\cite{mahapatra2023text}, which utilizes pretrained text-to-image models to pioneer text-guided artistic cinemagraph creation. This approach surpasses some existing text-to-video models like VideoCrafter~\cite{chen2023videocrafter} in generating plausible motion in artistic scenes. Nevertheless, its main limitation lies in its confined applicability, primarily to landscapes, and its tendency to generate scanty motion patterns solely for fluid elements.
