%-------------------------------
\section{Conclusion and Limitations}
\label{sec:conclusion}
%-------------------------------

We have presented StyleCrafter, a generic method enabling pre-trained T2V model for video generation in any style by providing a reference image. To achieve this, we made exploration in three aspects, including the architecture of style adapter, the content and style feature fusion mechanism, and some tailor-made strategies for data augmentation and training stylized video generation without stylistic video data.  
All of these components allow our method to generate high quality stylized videos that align with text prompts and conform to style references.
Extensive experiments have evidenced the effectiveness of our proposed designs and comparisons with existing competitors demonstrate the superiority of our method in visual quality and efficiency.
Anyway, our method also has certain limitations, e.g., unable to generate desirable results when the reference image can not represent the target style sufficiently or the presented style is extremely unseen. Further explorations are demanded to address those issues.
%Due to the absence of high-quality stylized video data for training, the generation results may exhibit degraded motion under certain styles, e.g. abstract artwork. And the image quality of the generation video is somewhat diminished in comparison to the stylized image generation. We have further explored the potential of incorporating a small set of stylized video data to address these limitations in Supp.

\section*{ACKNOWLEDGMENTS}
This work was partly supported by the National Natural Science Foundation of China  (Grant No. 61991451) and the Shenzhen Science and Technology Program (JSGG20220831093004008).