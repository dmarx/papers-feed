\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Mixer Architecture}{}% 2
\BOOKMARK [1][-]{section.3}{Experiments}{}% 3
\BOOKMARK [2][-]{subsection.3.1}{Main results}{section.3}% 4
\BOOKMARK [2][-]{subsection.3.2}{The role of the model scale}{section.3}% 5
\BOOKMARK [2][-]{subsection.3.3}{The role of the pre-training dataset size}{section.3}% 6
\BOOKMARK [2][-]{subsection.3.4}{Invariance to input permutations}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.5}{Visualization}{section.3}% 8
\BOOKMARK [1][-]{section.4}{Related work}{}% 9
\BOOKMARK [1][-]{section.5}{Conclusions}{}% 10
\BOOKMARK [1][-]{appendix.A}{Things that did not help}{}% 11
\BOOKMARK [2][-]{subsection.A.1}{Modifying the token-mixing MLPs}{appendix.A}% 12
\BOOKMARK [2][-]{subsection.A.2}{Fine-tuning}{appendix.A}% 13
\BOOKMARK [1][-]{appendix.B}{Pre-training: hyperparameters, data augmentation and regularization}{}% 14
\BOOKMARK [1][-]{appendix.C}{Fine-tuning: hyperparameters and higher image resolution}{}% 15
\BOOKMARK [1][-]{appendix.D}{Weight visualizations}{}% 16
\BOOKMARK [1][-]{appendix.E}{MLP-Mixer code}{}% 17
