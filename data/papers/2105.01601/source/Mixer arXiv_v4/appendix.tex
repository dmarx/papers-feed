\section{Things that did not help}

\subsection{Modifying the token-mixing MLPs}
\label{appendix:did-not-work}

We ablated a number of ideas trying to improve the token-mixing MLPs for \name{} models of various scales pre-trained on JFT-300M.

\paragraph{Untying (not sharing) the parameters}

Token-mixing MLPs in the \name{} layer are shared across the columns of the input table $\mathbf{X}\in\mathbb{R}^{S\times C}$.
In other words, the \emph{same} MLP is applied to each of the $C$ different features.
Instead, we could introduce $C$ \emph{separate} MLPs with independent weights, effectively multiplying the number of parameters by $C$.
We did not observe any noticeable improvements.

\paragraph{Grouping the channels together}

Token-mixing MLPs take $S$-dimensional vectors as inputs.
Every such vector contains values of a single feature across $S$ different spatial locations.
In other words, token-mixing MLPs operate by looking at only \emph{one channel} at once.
One could instead group channels together by concatenating $G$ neighbouring columns in $\mathbf{X}\in\mathbb{R}^{S\times C}$, reshaping it to a matrix of dimension $(S\cdot G) \times (C / G)$.
This increases the MLP's input dimensionality from $S$ to $G \cdot S$ and reduces the number of vectors to be processed from $C$ to $C/G$.
Now the MLPs look at \emph{several channels at once} when mixing the tokens.
This concatenation of the column-vectors improved linear 5-shot top-1 accuracy on ImageNet by less than 1--2\%.

We tried a different version, where we replace the simple reshaping described above with the following:
(1) Introduce $G$ linear functions (with trainable parameters) projecting $\mathbb{R}^C$ to $\mathbb{R}^{C/G}$.
(2) Using them, map each of the $S$ rows (tokens) in $\mathbf{X}\in\mathbb{R}^{S\times C}$ to $G$ different $(C/G)$-dimensional vectors.
This results in $G$ different ``views'' on every token, each one consisting of $C/G$ features.
(3)~Finally, concatenate vectors corresponding to $G$ different views for each of the $C/G$ features. This results in a matrix of dimension $(S\cdot G) \times (C / G)$.
The idea is that MLPs can look at $G$ different \emph{views} of the original channels, when mixing the tokens.
This version improved the top-5 ImageNet accuracy by 3--4\% for the \name{}-S/32 architecture, however did not show any improvements for the larger scales.

\paragraph{Pyramids}
All layers in \name{} retain the same, isotropic design.
Recent improvements on the ViT architecture hint that this might not be ideal~\citep{wang2021pyramid}.
We tried using the token-mixing MLP to reduce the number of tokens by mapping from $S$ input tokens to $S' < S$ output tokens.
While first experiments showed that on JFT-300M such models significantly reduced training time without losing much performance, we were unable to transfer these findings to ImageNet or ImageNet-21k.
However, since pyramids are a popular design, exploring this design for other vision tasks may still be promising.

\subsection{Fine-tuning}
Following ideas from BiT \cite{kolesnikov2020-bit} and ViT \cite{Dosovitskiy2021}, we also tried using mixup~\cite{zhang2018mixup} and Polyak averaging~\cite{polyak} during fine-tuning. 
However, these did not lead to consistent improvements, so we dropped them. 
We also experimented with using inception cropping \cite{szegedy15inception} during fine-tuning, which also did not lead to any improvements.
We did these experiments for JFT-300M pre-trained \name{} models of all scales.


\section{Pre-training: hyperparameters, data augmentation and regularization}~\label{appendix:sec:reg}

In Table~\ref{appendix:table:reg} we describe 
optimal hyperparameter settings that were used for pre-training \name{} models.

For pre-training on ImageNet and ImageNet-21k we used additional augmentation and regularization.
For RandAugment~\cite{cubuk2020rand} we always use two augmentations layers and sweep magnitude, $m$, parameter in a set $\{0, 10, 15, 20\}$. For mixup~\cite{zhang2018mixup} we sweep mixing strength, $p$, in a set $\{0.0, 0.2, 0.5, 0.8\}$. For dropout~\cite{srivastava14dropout} we try dropping rates, $d$ of $0.0$ and $0.1$. For stochastic depth, following the original paper~\cite{huang2016deep}, we linearly increase the probability of dropping a layer from $0.0$ (for the first MLP) to $s$ (for the last MLP), where we try $s \in \{0.0, 0.1\}$. Finally, we sweep learning rate, $lr$, and weight decay, $wd$, from $\{0.003, 0.001\}$ and $\{0.1, 0.01\}$ respectively. 

\begin{table}
  \caption{Hyperparameter settings used for pre-training \name{} models.}
  \medskip
  \label{appendix:table:reg}
  \centering
  \small
  \begin{tabular}{@{}llccccccc@{}}
    \toprule
    Model & Dataset & Epochs & $lr$ & $wd$ & \hspace{-1ex}RandAug. & Mixup & Dropout & \hspace{-1ex}Stoch.\,depth  \\
    \cmidrule{1-9}
    \name{}-B & ImNet & 300 & 0.001 & 0.1 &  15 & 0.5 & 0.0 & 0.1 \\
    \name{}-L & ImNet & 300 & 0.001 & 0.1 &  15 & 0.5 & 0.0 & 0.1 \\
    \name{}-B & ImNet-21k\hspace{-1ex} & 300 & 0.001 & 0.1 & 10 & 0.2 & 0.0 & 0.1 \\
    \name{}-L & ImNet-21k\hspace{-1ex} & 300 & 0.001 & 0.1 & 20 & 0.5 & 0.0 & 0.1 \\
    \name{}-S& JFT-300M & 5 & 0.003 & 0.03 &  -- & -- & -- & -- \\
    \name{}-B& JFT-300M & 7 & 0.003 & 0.03 &  -- & -- & -- & -- \\
    \name{}-L& JFT-300M & 7/14 & 0.001 & 0.03 &  -- & -- & -- & -- \\
    \name{}-H& JFT-300M & 14 & 0.001 & 0.03 &  -- & -- & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Fine-tuning: hyperparameters and higher image resolution}
\label{appendix:sec:fine-tuning}
Models are fine-tuned at resolution 224 unless mentioned otherwise.
We follow the setup of~\cite{Dosovitskiy2021}.
The only differences are:
(1) We exclude $lr=0.001$ from the grid search and instead include $lr=0.06$ for CIFAR-10, CIFAR-100, Flowers, and Pets.
(2) We perform a grid search over $lr\in\{0.003,0.01, 0.03\}$ for VTAB-1k.
(3) We try two different ways of pre-processing during evaluation:
(i) ``resize-crop'': first resize the image to $256\times256$ pixels and then take a $224\times224$ pixel sized central crop.
(ii) ``resmall-crop'': first resize the shorter side of the image to $256$ pixels and then take a $224\times224$ pixel sized central crop.
For the \name{} and ViT models reported in Table~\ref{table:main-results-appendix} of the main text we used (ii) on ImageNet, Pets, Flowers, CIFAR-10 and CIFAR-100.
We used the same setup for the BiT models reported in Table~\ref{table:main-results-appendix} of the main text, with the only exception of using (i) on ImageNet.
For the \name{} models reported in Table~\ref{table:main-results-sota} of the main text we used (i) for all 5 downstream datasets.


Fine-tuning at higher resolution than the one used at pre-training time has been shown to substantially improve the transfer performance of existing vision models~\citep{touvron2019,kolesnikov2020-bit,Dosovitskiy2021}.
We therefore apply this technique to \name{} as well. 
When feeding images of higher resolution to the model, we do not change the patch size, which results in a longer sequence of tokens.
The token-mixing MLPs have to be adjusted to handle these longer sequences.
We experimented with several options and describe the most successful one below.

For simplicity we assume that the image resolution is increased by an integer factor~$K$.
The length $S$ of the token sequence increases by a factor of $K^2$.
We increase the hidden width $D_S$ of the token-mixing MLP by a factor of $K^2$ as well.
Now we need to initialize the parameters of this new (larger) MLP with the parameters of the pre-trained MLP.
To this end we split the input sequence into $K^2$ equal parts, each one of the original length $S$, and initialize the new MLP so that it processes all these parts independently in parallel with the pre-trained MLP.

Formally, the pre-trained weight matrix $\mathbf{W}_1\in\mathbb{R}^{D_S\times S}$ of the original MLP in Eq.\,\ref{eq:channel-wise-mlp} of the main text
will be now replaced with a larger matrix $\mathbf{W}'_1\in\mathbb{R}^{(K^2\cdot D_S)\times (K^2 \cdot S)}$.
Assume the token sequence for the resized input image is a concatenation of $K^2$ token sequences of length $S$ each, computed by splitting the input into $K \times K$ equal parts spatially.
We then initialize $\mathbf{W}'_1$ with a block-diagonal matrix that has copies of $\mathbf{W}_1$ on its main diagonal.
Other parameters of the MLP are handled analogously.

\section{Weight visualizations}\label{appendix:sec:weightvisualizations}
For better visualization, we sort all hidden units according to a heuristic that tries to show low frequency filters first.
For each unit, we also try to identify the unit that is closest to its inverse. Figure~\ref{appendix:fig:weight-plots-full} shows each unit followed by its closest inverse.
Note that the models pre-trained on ImageNet and ImageNet-21k used heavy data augmentation.
We found that this strongly influences the structure of the learned units.

\begin{figure}[t]
    \includegraphics[width=1.0\linewidth]{pics/weightvisualization_b16_full.pdf}
    \caption{Weights of all hidden dense units in the first two token-mixing MLPs ({\bf rows}) of the \name{}-B/16 model trained on three different datasets ({\bf columns}). 
    Each unit has $14 \times 14=196$ weights, which is the number of incoming tokens, and is depicted as a $14 \times 14$ image.
    In each block there are 384 hidden units in total.
    }
  \label{appendix:fig:weight-plots-full}
\end{figure}

We also visualize the linear projection units in the embedding layer learned by different models in Figure~\ref{appendix:fig:embedding-plots}. 
Interestingly, it appears that their properties strongly depend on the patch resolution used by the models.
Across all \name{} model scales, using patches of higher resolution 32$\times$32 leads to Gabor-like low-frequency linear projection units, while for the 16$\times$16 resolution the units show no such structure.

\begin{figure}[t]
    \includegraphics[width=0.49\linewidth]{pics/embedding_visualization_b16_jft300m.pdf}\hfill
    \includegraphics[width=0.49\linewidth]{pics/embedding_visualization_b32_jft300m.pdf}
    \caption{Linear projection units of the embedding layer for \name{}-B/16 ({\bf left}) and \name{}-B/32~({\bf right}) models pre-trained on JFT-300M.
    \name{}-B/32 model that uses patches of higher resolution $32\times 32$ learns very structured low frequency projection units, while most of the units learned by the \name{}-B/16 have high frequencies and no clear structure.
    }
  \label{appendix:fig:embedding-plots}
\end{figure}

\FloatBarrier

\section{MLP-Mixer code}
\label{appendix:sec:code}

\begin{lstlisting}[language=Python, caption=MLP-Mixer code written in JAX/Flax.]
import einops
import flax.linen as nn
import jax.numpy as jnp

class MlpBlock(nn.Module):
  mlp_dim: int
  @nn.compact
  def __call__(self, x):
    y = nn.Dense(self.mlp_dim)(x)
    y = nn.gelu(y)
    return nn.Dense(x.shape[-1])(y)

class MixerBlock(nn.Module):
  tokens_mlp_dim: int
  channels_mlp_dim: int
  @nn.compact
  def __call__(self, x):
    y = nn.LayerNorm()(x)
    y = jnp.swapaxes(y, 1, 2)
    y = MlpBlock(self.tokens_mlp_dim, name='token_mixing')(y)
    y = jnp.swapaxes(y, 1, 2)
    x = x+y
    y = nn.LayerNorm()(x)
    return x+MlpBlock(self.channels_mlp_dim, name='channel_mixing')(y)

class MlpMixer(nn.Module):
  num_classes: int
  num_blocks: int
  patch_size: int
  hidden_dim: int
  tokens_mlp_dim: int
  channels_mlp_dim: int
  @nn.compact
  def __call__(self, x):
    s = self.patch_size
    x = nn.Conv(self.hidden_dim, (s,s), strides=(s,s), name='stem')(x)
    x = einops.rearrange(x, 'n h w c -> n (h w) c')
    for _ in range(self.num_blocks):
      x = MixerBlock(self.tokens_mlp_dim, self.channels_mlp_dim)(x)
    x = nn.LayerNorm(name='pre_head_layer_norm')(x)
    x = jnp.mean(x, axis=1)
    return nn.Dense(self.num_classes, name='head',
                    kernel_init=nn.initializers.zeros)(x)

\end{lstlisting}