\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Araujo et~al.(2019)Araujo, Norris, and Sim]{araujo2019computing}
A.~Araujo, W.~Norris, and J.~Sim.
\newblock Computing receptive fields of convolutional neural networks.
\newblock \emph{Distill}, 2019.
\newblock \doi{10.23915/distill.00021}.
\newblock URL \url{https://distill.pub/2019/computing-receptive-fields}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bello(2021)]{bello2021lambdanetworks}
I.~Bello.
\newblock {LambdaNetworks}: Modeling long-range interactions without attention.
\newblock \emph{arXiv preprint arXiv:2102.08602}, 2021.

\bibitem[Bello et~al.(2021)Bello, Fedus, Du, Cubuk, Srinivas, Lin, Shlens, and
  Zoph]{bello2021revisiting}
I.~Bello, W.~Fedus, X.~Du, E.~D. Cubuk, A.~Srinivas, T.-Y. Lin, J.~Shlens, and
  B.~Zoph.
\newblock Revisiting {ResNets}: Improved training and scaling strategies.
\newblock \emph{arXiv preprint arXiv:2103.07579}, 2021.

\bibitem[Beyer et~al.(2020)Beyer, Hénaff, Kolesnikov, Zhai, and van~den
  Oord]{beyer2020-imagenet}
L.~Beyer, O.~J. Hénaff, A.~Kolesnikov, X.~Zhai, and A.~van~den Oord.
\newblock Are we done with {ImageNet}?
\newblock \emph{arXiv preprint arXiv:2006.07159}, 2020.

\bibitem[Bhojanapalli et~al.(2021)Bhojanapalli, Chakrabarti, Glasner, Li,
  Unterthiner, and Veit]{bhojanapalli2021understanding}
S.~Bhojanapalli, A.~Chakrabarti, D.~Glasner, D.~Li, T.~Unterthiner, and
  A.~Veit.
\newblock Understanding robustness of transformers for image classification.
\newblock \emph{arXiv preprint arXiv:2103.14586}, 2021.

\bibitem[Brock et~al.(2021)Brock, De, Smith, and Simonyan]{brock2021high}
A.~Brock, S.~De, S.~L. Smith, and K.~Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock \emph{arXiv preprint arXiv:2102.06171}, 2021.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019-sparsetransformers}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Chollet(2017)]{chollet2017xception}
F.~Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In \emph{CVPR}, 2017.

\bibitem[Cordonnier et~al.(2020)Cordonnier, Loukas, and
  Jaggi]{cordonnier2020-sacnn}
J.-B. Cordonnier, A.~Loukas, and M.~Jaggi.
\newblock On the relationship between self-attention and convolutional layers.
\newblock In \emph{ICLR}, 2020.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and Le]{autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock {AutoAugment}: Learning augmentation policies from data.
\newblock In \emph{CVPR}, 2019.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020rand}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le.
\newblock {RandAugment}: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{CVPR Workshops}, 2020.

\bibitem[{Deng} et~al.(2009){Deng}, {Dong}, {Socher}, {Li}, {Kai Li}, and {Li
  Fei-Fei}]{deng2009-imagenet}
J.~{Deng}, W.~{Dong}, R.~{Socher}, L.~{Li}, {Kai Li}, and {Li Fei-Fei}.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{Dosovitskiy2021}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gelu}
D.~Hendrycks and K.~Gimpel.
\newblock Gaussian error linear units {(GELUs)}.
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Hu et~al.(2018)Hu, Shen, and Sun]{hu2018squeeze}
J.~Hu, L.~Shen, and G.~Sun.
\newblock Squeeze-and-excitation networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{huang2016deep}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In \emph{ECCV}, 2016.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{jia2021scaling}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~V. Le, Y.~Sung,
  Z.~Li, and T.~Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock \emph{arXiv preprint arXiv:2102.05918}, 2021.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2020-bit}
A.~Kolesnikov, L.~Beyer, X.~Zhai, J.~Puigcerver, J.~Yung, S.~Gelly, and
  N.~Houlsby.
\newblock Big transfer {(BiT)}: General visual representation learning.
\newblock In \emph{ECCV}, 2020.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009-cifar}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{KrizhevskyNIPS12}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock {ImageNet} classification with deep convolutional neural networks.
\newblock In \emph{NeurIPS}, 2012.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{LeCun1989BackpropagationAT}
Y.~LeCun, B.~Boser, J.~Denker, D.~Henderson, R.~Howard, W.~Hubbard, and
  L.~Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural Computation}, 1:\penalty0 541--551, 1989.

\bibitem[Li et~al.(2021)Li, Hu, Wang, Li, She, Zhu, Zhang, and
  Chen]{li2021involution}
D.~Li, J.~Hu, C.~Wang, X.~Li, Q.~She, L.~Zhu, T.~Zhang, and Q.~Chen.
\newblock Involution: Inverting the inherence of convolution for visual
  recognition.
\newblock \emph{CVPR}, 2021.

\bibitem[Lin et~al.(2016)Lin, Memisevic, and Konda]{lin2016mlp}
Z.~Lin, R.~Memisevic, and K.~Konda.
\newblock How far can we go without convolution: Improving fullyconnected
  networks.
\newblock In \emph{ICLR, Workshop Track}, 2016.

\bibitem[Luo et~al.(2016)Luo, Li, Urtasun, and Zemel]{luo2017understanding}
W.~Luo, Y.~Li, R.~Urtasun, and R.~Zemel.
\newblock Understanding the effective receptive field in deep convolutional
  neural networks.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and van~der Maaten]{mahajan2018}
D.~Mahajan, R.~Girshick, V.~Ramanathan, K.~He, M.~Paluri, Y.~Li, A.~Bharambe,
  and L.~van~der Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{ECCV}, 2018.

\bibitem[Neyshabur(2020)]{neyshabur2020towards}
B.~Neyshabur.
\newblock Towards learning convolutions from scratch.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[{Nilsback} and {Zisserman}(2008)]{Nilsback2008-flowers}
M.~{Nilsback} and A.~{Zisserman}.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{ICVGIP}, 2008.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and
  Jawahar]{parkhi2012-pets}
O.~M. Parkhi, A.~Vedaldi, A.~Zisserman, and C.~V. Jawahar.
\newblock Cats and dogs.
\newblock In \emph{CVPR}, 2012.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku,
  and Tran]{parmar18-imagetransformer}
N.~Parmar, A.~Vaswani, J.~Uszkoreit, L.~Kaiser, N.~Shazeer, A.~Ku, and D.~Tran.
\newblock Image transformer.
\newblock In \emph{ICML}, 2018.

\bibitem[Pham et~al.(2021)Pham, Dai, Xie, Luong, and Le]{pham2020meta}
H.~Pham, Z.~Dai, Q.~Xie, M.-T. Luong, and Q.~V. Le.
\newblock Meta pseudo labels.
\newblock In \emph{CVPR}, 2021.

\bibitem[Pinz(2006)]{pinz2006object}
A.~Pinz.
\newblock Object categorization.
\newblock \emph{Foundations and Trends in Computer Graphics and Vision},
  1\penalty0 (4), 2006.

\bibitem[Polyak and Juditsky(1992)]{polyak}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Ramachandran et~al.(2019)Ramachandran, Parmar, Vaswani, Bello,
  Levskaya, and Shlens]{ramachandran19-sasa}
P.~Ramachandran, N.~Parmar, A.~Vaswani, I.~Bello, A.~Levskaya, and J.~Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Sandler et~al.(2019)Sandler, Baccash, Zhmoginov, and
  Howard]{Sandler2019}
M.~Sandler, J.~Baccash, A.~Zhmoginov, and Howard.
\newblock Non-discriminative data or weak model? {On} the relative importance
  of data and model resolution.
\newblock In \emph{ICCV Workshop on Real-World Recognition from Low-Quality
  Images and Videos}, 2019.

\bibitem[Shang et~al.(2016)Shang, Sohn, Almeida, and Lee]{shang2016crelu}
W.~Shang, K.~Sohn, D.~Almeida, and H.~Lee.
\newblock Understanding and improving convolutional neural networks via
  concatenated rectified linear units.
\newblock In \emph{ICML}, 2016.

\bibitem[Sifre(2014)]{Sifre2014phd}
L.~Sifre.
\newblock \emph{Rigid-Motion Scattering For Image Classification}.
\newblock PhD thesis, Ecole Polytechnique, 2014.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Srinivas et~al.(2021)Srinivas, Lin, Parmar, Shlens, Abbeel, and
  Vaswani]{srinivas2021bottleneck}
A.~Srinivas, T.-Y. Lin, N.~Parmar, J.~Shlens, P.~Abbeel, and A.~Vaswani.
\newblock Bottleneck transformers for visual recognition.
\newblock \emph{arXiv preprint arXiv:2101.11605}, 2021.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava14dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 15\penalty0 (56), 2014.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun2017-jft}
C.~Sun, A.~Shrivastava, S.~Singh, and A.~Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{ICCV}, 2017.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy15inception}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{CVPR}, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{inception}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Tay et~al.(2020)Tay, Bahri, Metzler, Juan, Zhao, and
  Zheng]{tay20synthesizer}
Y.~Tay, D.~Bahri, D.~Metzler, D.-C. Juan, Z.~Zhao, and C.~Zheng.
\newblock Synthesizer: Rethinking self-attention in transformer models.
\newblock \emph{arXiv}, 2020.

\bibitem[Touvron et~al.(2019)Touvron, Vedaldi, Douze, and Jegou]{touvron2019}
H.~Touvron, A.~Vedaldi, M.~Douze, and H.~Jegou.
\newblock Fixing the train-test resolution discrepancy.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Touvron et~al.(2020)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{deit}
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock \emph{arXiv preprint arXiv:2012.12877}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Vaswani et~al.(2021)Vaswani, Ramachandran, Srinivas, Parmar, Hechtman,
  and Shlens]{vaswani2021scaling}
A.~Vaswani, P.~Ramachandran, A.~Srinivas, N.~Parmar, B.~Hechtman, and
  J.~Shlens.
\newblock Scaling local self-attention for parameter efficient visual
  backbones.
\newblock \emph{arXiv preprint arXiv:2103.12731}, 2021.

\bibitem[Wang et~al.(2021)Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and
  Shao]{wang2021pyramid}
W.~Wang, E.~Xie, X.~Li, D.-P. Fan, K.~Song, D.~Liang, T.~Lu, P.~Luo, and
  L.~Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock \emph{arXiv preprint arXiv:2102.12122}, 2021.

\bibitem[Wang et~al.(2018)Wang, Girshick, Gupta, and He]{wang2018-nonlocalnn}
X.~Wang, R.~Girshick, A.~Gupta, and K.~He.
\newblock Non-local neural networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Wightman(2019)]{rw2019timm}
R.~Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wu et~al.(2019)Wu, Fan, Baevski, Dauphin, and Auli]{wu2019lightcnn}
F.~Wu, A.~Fan, A.~Baevski, Y.~Dauphin, and M.~Auli.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In \emph{ICLR}, 2019.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020-noisystudent}
Q.~Xie, M.-T. Luong, E.~Hovy, and Q.~V. Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{CVPR}, 2020.

\bibitem[Xie et~al.(2016)Xie, Girshick, Dollár, Tu, and He]{Xie2016}
S.~Xie, R.~Girshick, P.~Dollár, Z.~Tu, and K.~He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1611.05431}, 2016.

\bibitem[Zhai et~al.(2019)Zhai, Puigcerver, Kolesnikov, Ruyssen, Riquelme,
  Lucic, Djolonga, Pinto, Neumann, Dosovitskiy, et~al.]{vtab}
X.~Zhai, J.~Puigcerver, A.~Kolesnikov, P.~Ruyssen, C.~Riquelme, M.~Lucic,
  J.~Djolonga, A.~S. Pinto, M.~Neumann, A.~Dosovitskiy, et~al.
\newblock A large-scale study of representation learning with the visual task
  adaptation benchmark.
\newblock \emph{arXiv preprint arXiv:1910.04867}, 2019.

\bibitem[Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and Beyer]{vitg}
X.~Zhai, A.~Kolesnikov, N.~Houlsby, and L.~Beyer.
\newblock Scaling vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.04560}, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\end{thebibliography}
