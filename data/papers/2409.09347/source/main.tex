\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

% ready for submission

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  linkcolor={red!50!black},
  citecolor={blue!50!black},
%   urlcolor={blue!80!black}
  urlcolor={blue!50!black}
}
\usepackage[oxfordcolors,fancytheorems]{fancytheorems}
\usepackage[final]{neurips_2024}

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{shapes,snakes,backgrounds,arrows}

% PERSONAL PACKAGES
\usepackage{upgreek}
\usepackage{pbox}
\usepackage[inline]{enumitem}
\usepackage{algorithm,algorithmic}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{aliascnt}
\usepackage{cleveref}
\usepackage{autonum}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{caption}
\usepackage[most]{tcolorbox}
\usepackage[export]{adjustbox}


\input{def}

\definecolor{customred}{HTML}{e4c3d0} % Define your custom red color
\definecolor{customblue}{HTML}{c6d5e7}

\title{Schr\"odinger Bridge Flow \\ for Unpaired Data Translation}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Valentin De Bortoli \thanks{Equal contribution.} \\
    Google DeepMind
\And
    Iryna Korshunova \footnotemark[1] \\
    Google DeepMind
\And
    Andriy Mnih
 \\
    Google DeepMind
\And
    Arnaud Doucet \\
    Google DeepMind
}

\pdfcompresslevel=9


\begin{document}

\maketitle

\begin{abstract}
Mass transport problems arise in many areas of machine learning whereby one wants to compute a map transporting one distribution to another. Generative modeling techniques like Generative Adversarial Networks (GANs) and Denoising Diffusion Models (DDMs) have been successfully adapted to solve such transport problems, resulting in CycleGAN and Bridge Matching respectively. However, these methods do not approximate Optimal Transport (OT) maps, which are known to have desirable properties. Existing techniques approximating OT maps for high-dimensional data-rich problems, such as DDM-based Rectified Flow and Schr\"odinger Bridge procedures, require fully training a DDM-type model at each iteration, or use mini-batch techniques which can introduce significant errors. We propose a novel algorithm to compute the Schr\"odinger Bridge, a dynamic entropy-regularised version of OT, that eliminates the need to train multiple DDM-like models. This algorithm corresponds to a discretisation of a flow of path measures, which we call the Schr\"odinger Bridge Flow, whose only stationary point is the Schr\"odinger Bridge. We demonstrate the performance of our algorithm on a variety of unpaired data translation tasks.
\end{abstract}

\section{Introduction}
The problem of finding a map to transport one probability distribution to another one has numerous applications in machine learning. In particular, it is at the core of generative modeling where the idea is to transform a noise distribution into the data distribution, and is also central to transfer learning tasks such as image-to-image translation. For discrete probability distributions, it is possible to compute the Optimal Transport (OT) map but this is computationally expensive \citep{peyre2019computational}. By showing that an entropy-regularised version of OT, the Entropic OT (EOT), could be computed much more efficiently using the Sinkhorn algorithm, \cite{cuturi2013sinkhorn} has enabled transport ideas to be used in numerous applications \citep{ge2021ota,zhou2022rethinking}. % ranging from object detection \citep{ge2021ota} to semantic segmentation \citep{zhou2022rethinking}.
However, the computational complexity of Sinkhorn algorithm is quadratic in the sample size, which makes its application to very large datasets impractical. Mini-batch versions have been proposed, see e.g.~\citep{genevay18a}, but tend to introduce significant errors in high dimensions \citep{sommerfeld2019optimal}.

In the context of generative modeling,  Denoising Diffusion Models (DDMs) \citep{song_denoising_2021,ho_denoising_2020} have shown impressive performance in a variety of domains. DDMs define a forward process progressively noising the data, and sample generation is achieved by approximating the time-reversal of this diffusion. In order to leverage the iterative refinement properties of DDMs in the OT setting, methods exploiting the equivalence between the static versions of (E)OT and their dynamic counterparts \citep{benamou2000computational,leonard2014survey} have been developed. A procedure to approximate the dynamic OT is considered by \cite{liu_flow_2023}, while techniques to approximate the dynamic equivalent to EOT, the \schro Bridge (SB), have been proposed in \citep{debortoli2021diffusion,vargas2021solving,chen2021likelihood,peluchetti_diffusion_2023,shi2023DSBM}. These techniques are expensive however, as they require training multiple DDM-type models. Mini-batch versions of OT and Sinkhorn \citep{pooladian_2023_multisample,tong2024simulationfree} combined with bridge or flow matching have also been proposed to approximate the OT path and SB, but they optimise a minibatch OT objective that can introduce significant errors in high dimensions: the error in Wasserstein-$1$ distance is of order $O(B^{-1/(2d)})$, where $d$ is the dimension of the problem and $B$ the minibatch size, see \citep[Corollary 1]{sommerfeld2019optimal}.

In this paper, we propose a novel approach to computing the SB. Similarly to Iterative Markovian Fitting (IMF) and its practical implementation, Diffusion Schr\"odinger Bridge Matching (DSBM) \citep{shi2023DSBM,peluchetti_diffusion_2023}, it leverages the fact that the SB is the only Markov process with prescribed marginals at the endpoints which is in the reciprocal class of the Brownian motion, i.e.~it has the same bridge as the Brownian motion \citep{leonard2014survey}; see \Cref{sec:background} for more details on Markov processes and the reciprocal class. Compared to DSBM, our approach is easier to implement as it does not require caching samples, alternating between optimising two different losses, and, optionally, uses one neural network instead of two. In \Cref{sec:theoretical_results}, we start by introducing a flow of path measures whose time-discretisation yields a family of algorithms called $\alpha$-IMF and presented in \Cref{sec:online_dsbm}. Notably, we show that $\alpha$-IMF converges to the Schr\"odinger Bridge for any $\alpha \in (0,1]$.  Additionally, for a special value of the discretisation stepsize $\alpha = 1$, we recover the IMF procedure \citep{peluchetti_diffusion_2023,shi2023DSBM}, while $\alpha < 1$ corresponds to online versions of IMF. We implement a parametric version of the $\alpha$-IMF as an online DSBM procedure, called $\alpha$-DSBM. 
We illustrate the efficiency of our approach in \emph{unpaired} image-to-image translation settings in \Cref{sec:experiments}. 

\paragraph{Notation.} We denote the space of \emph{path
measures} by $\pathmeas$, i.e.~$\pathmeas=\mathcal{P}(\rmC(\ccint{0,1}, \rset^d))$, where $\rmC(\ccint{0,1}, \rset^d)$ is the space of continuous functions from $\ccint{0,1}$ to $\rset^d$.
The subset of \emph{Markov} path measures associated
with a diffusion of the form
$\rmd \bfX_t = v_t(\bfX_t) \rmd t + \sigma_t \rmd \bfB_t$, with $\sigma,v$
locally Lipschitz, is denoted $\calM$.  For $\Qbb$ induced by $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$, with $\vareps >0$ and $(\bfB_t)_{t \geq 0}$ a $d$-dimensional Brownian motion, the \emph{reciprocal class}  of $\Qbb$
is denoted $\calR(\Qbb)$, see
\Cref{def:reciprocal_projection}. For any $\Pbb \in \pathmeas$, we denote by $\Pbb_t$ its marginal
distribution at time $t$, $\Pbb_{s,t}$ the joint distribution at times $s,t$, $\Pbb_{s|t}$ the conditional distribution at time $s$ given the state at
time $t$, and $\Pbb_{|0,1} \in \pathmeas$ the distribution of the path on time interval $(0,1)$ given its endpoints; e.g.~$\Qbb_{|0,1}$ is a scaled Brownian bridge. Unless specified otherwise, all gradient operators $\nabla$ are
w.r.t.~the variable $x_t$ with time index $t$. Given probability spaces $(\msx, \mathcal{X})$ and $(\mathsf{Y}, \mathcal{Y})$, a Markov kernel $\mathrm{K}: \ \msx \times \mathcal{Y} \to [0,1]$, and a probability measure $\mu$ defined on $\mathcal{X}$, we write $\mu \mathrm{K}$ for the probability measure on $\mathcal{Y}$ such that for any $\mathsf{A} \in \mathcal{Y}$ we have $\mu \mathrm{K}(\mathsf{A}) = \int_{\msx} \mathrm{K}(x, \mathsf{A}) \rmd \mu(x)$. 
In particular, for any joint distribution $\Pi_{0,1}$ over $\rset^d \times \rset^d$, we denote the \emph{mixture of bridges} measure as $\Pi = \Pi_{0,1} \Pbb_{|0,1} \in \pathmeas$, which is short for $\Pi(\cdot) = \int_{\rset^d \times \rset^d} \Pbb_{|0,1}(\cdot|x_0, x_1) \rmd \Pi_{0,1}(x_0, x_1)$. Finally, we define the Kullback--Leibler (KL) divergence between two probability measures $\pi_0, \pi_1 \in \mathcal{P}(\msx)$ as $\KL(\pi_0 | \pi_1) = \int_{\msx} \log ((\rmd \pi_0 / \rmd \pi_1)(x)) \rmd \pi_0(x)$ if $\pi_0$ is absolutely continuous w.r.t.~$\pi_1$ and $\KL(\pi_0 | \pi_1) = +\infty$ otherwise. 

\section{Optimal Transport and Schr\"odinger Bridge}
\label{sec:background}

\paragraph{Unpaired Transfer and Optimal Transport.} Given unpaired data samples from $\pi_0$ and $\pi_1$, where $\pi_0,\pi_1$ are two distributions on $\rset^d$, we are interested in designing a transport map from $\pi_0$ to $\pi_1$. This corresponds to an \emph{unpaired data transfer task}.
We can formulate this problem as finding a distribution $\Pi$ on $\rset^d\times \rset^d$ with marginals $\Pi_0 = \pi_0$ and $\Pi_1 = \pi_1$ so that if $\bfX_0 \sim \pi_0$ then $\bfX_1|\bfX_0 \sim \Pi_{1|0}(\cdot|\bfX_0)$ satisfies $\bfX_1 \sim \pi_1$. Among an infinite number of such so-called coupling distributions $\Pi$, we are here interested in finding the Entropic Optimal Transport (EOT) coupling $\Pi^\star$ defined as
\begin{equation}
\label{eq:static_ot}
   \Pi^\star = \argmin_{\Pi \in \calP(\rset^d\times \rset^d)} \left\{ \int_{\rset^d \times \rset^d} \frac{1}{2} \| x - y \|^2 \rmd \Pi(x,y) - \varepsilon \mathrm{H}(\Pi) \ ; \ \Pi_0 = \pi_0, \ \Pi_1 = \pi_1 \right\} ,
\end{equation}
where 
$\mathrm{H}(\Pi)$ is the differential entropy of $\Pi$ and $\vareps > 0$ is a regularisation hyperparameter \citep{peyre2019computational}. For $\vareps= 0$, we recover the standard OT. 

In order to leverage the recent advances in generative modeling, and in particular the concept of \emph{iterative refinement} central to DDMs, we turn to a \emph{dynamic} formulation of EOT known as the \emph{Schr\"odinger Bridge} problem \citep{leonard2014survey}. It is defined as follows: find  $\Pbb^\star \in \mathcal{P}(\mathcal{C})$ such that 
\begin{equation}
    \label{eq:dynamic_ot}
    \Pbb^\star = \argmin_{\Pbb \in \calP(\calC)} \{ \KL(\Pbb | \Qbb)  \ ; \ \Pbb_0 = \pi_0, \ \Pbb_1 = \pi_1 \} ,
\end{equation}
with $\Qbb \in \pathmeas$ induced by a scaled $d$-dimensional Brownian motion $(\sqrt{\varepsilon} \bfB_t)_{t \in [0,1]}$. The term \emph{dynamic} here refers to the fact that \eqref{eq:dynamic_ot} is defined on path measures, i.e.~on (stochastic) processes, in contrast to the \emph{static} problem \eqref{eq:static_ot} which is defined on measures on the space $\rset^d \times \rset^d$. 
In \Cref{sec:theoretical_results}, we show that solving \eqref{eq:dynamic_ot} is equivalent to optimising the vector field of a stochastic process using objectives similar to the ones of bridge matching \citep{peluchettinon,albergo_building_2023,lipman_flow_2022,liu2023I2SB}. Under mild assumptions, it can be shown that $\Pbb^\star_{0,1} = \Pi^\star$, see e.g.~\citep{leonard2014survey,pavon2018data}. Hence solving \eqref{eq:static_ot} reduces to solving \eqref{eq:dynamic_ot}. Once we have found $\Pbb^\star$ associated with $(\bfX_t^\star)_{t \in [0,1]}$, we can sample from $\Pbb^\star$ by first sampling $\bfX_0^\star \sim \pi_0$ and then sampling the trajectory $(\bfX_t^\star)_{t \in (0,1]}$ which yields $(\bfX_0^\star, \bfX_1^\star)\sim \Pi^\star$.

\paragraph{Reciprocal and Markov projections.} To introduce our methodology, it is necessary to recall the notions of reciprocal and Markov projections. We refer to \cite{shi2023DSBM} for more details. For practitioners, a more intuitive explanation of these projections is given in \Cref{sec:background_dsbm}.

\begin{definitionblue}{Reciprocal projection}{reciprocal_projection}
  $\Pbb \in \pathmeas$ is in the reciprocal class $\calR(\Qbb)$ of $\Qbb$ if
  $\Pbb = \Pbb_{0,1} \Qbb_{|0,1}$. 
  % We denote by $\calR(\Qbb)$ the reciprocal class of $\Qbb$. 
  We define the
  \emph{reciprocal projection} of $\Pbb \in \pathmeas$ as
  $\Pbb^\star = \projR{\Qbb}(\Pbb) =  \Pbb_{0,1} \Qbb_{|0,1} $. We will write $\projsimpleR$ instead of $\projR{\Qbb}$ to simplify notation.
\end{definitionblue}
In other words, $\Pbb$ is in the reciprocal class of $\Qbb$ if the conditional distribution of a path given its endpoints is identical under $\Pbb$ and $\Qbb$, see \citep{roelly2013reciprocal}. Sampling from the reciprocal projection of $\Pbb$ can be achieved by sampling a path $(\bfX_t)_{t\in[0,1]}$ from $\Pbb$, keeping only the values of the endpoints, say $\bfX_0,\bfX_1$, and then sampling a new value for the bridge $(\bfX_t)_{t\in(0,1)}$ from $\Qbb_{|0,1}$.
\begin{definitionred}{Markov projection}{markovian_proj}
  Assume that $\Qbb$ is induced by $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ for $\vareps >0$. Then, when it is well-defined, for any $\Pbb \in \mathcal{R}(\Qbb)$, the \emph{Markovian projection} 
  $\Mbb = \projM(\Pbb) \in \calM$ is the path measure induced by the diffusion $(\bfX_t^\star)_{t \in [0,1]}$ with for any $t \in [0,1]$
  \begin{equation}
    \rmd \bfX^\star_t =  v_t^\star(\bfX^\star_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t, \qquad v_t^\star(x_t) =  \left(\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t\right)/(1-t) , \qquad \bfX^\star_0 \sim \Pbb_0.
  \end{equation}  
\end{definitionred}
In practice, implementing a Markovian projection requires solving a regression problem to approximate $\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t]$, similar to the one appearing in bridge matching and flow matching. One key property of the Markovian projection is that $\Mbb_t = \Pbb_t$ for all $t \in [0,1]$, i.e.~the Markovian projection preserves the marginals; see \citep{peluchettinon} for instance. 
% We provide more details on the reciprocal and Markovian projections for the practitioners in \Cref{sec:background_dsbm}.

\paragraph{Iterative Markovian Fitting.} Leveraging the reciprocal and Markovian projections, \citet{peluchetti_diffusion_2023} and \citet{shi2023DSBM} concurrently introduced IMF. Starting from $\hat{\Pbb}^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$, a measure where endpoints are sampled independently from $\pi_0$ and $\pi_1$ and then interpolated using a (scaled) Brownian bridge, they define a sequence of path measures $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ where $\Pbb^{n} = \projM(\hat{\Pbb}^{n})$ and $\hat{\Pbb}^{n+1} = \projsimpleR(\Pbb^{n})$. This ensures that $\Pbb^n_0 = \pi_0$, $\Pbb^n_1 = \pi_1$ for all $n$, and it can be shown that the sequence $(\Pbb^n)_{n \in \nset}$ converges to the SB, see \cite[Theorem 2]{peluchetti_diffusion_2023}. The practical implementation of this algorithm proposed by \cite{shi2023DSBM} is called DSBM. Implementing DSBM poses challenges, as each Markovian projection requires training a neural network to approximate the relevant conditional expectations by minimising a bridge matching loss. Furthermore, in practice, generated model samples are stored in a cache in order to train the next iterations of DSBM. This introduces additional hyperparameters that require tuning. In \Cref{sec:theoretical_results} we propose $\alpha$-IMF, an algorithm which can be interpreted as the discretisation of a \emph{flow of path measures}. This leads to $\alpha$-DSBM, an algorithm that is computationally much more efficient than DSBM as it does not rely on a Markovian projection at each step. 

\section{Schr\"odinger Bridge flow}
\label{sec:theoretical_results}

We will now introduce a flow of path measures $(\Pbb^s)_{s \geq 0}$, and show that the time-discretisation of this flow with an appropriate stepsize $\alpha \in (0,1]$ yields a family of procedures called $\alpha$-IMF, which all converge to the Schr\"odinger Bridge.  While $\alpha=1$ yields the classical IMF, $\alpha \in (0,1)$ yields an \emph{incremental} version of IMF. In Section \ref{sec:online_dsbm} we show that $\alpha$-IMF can be implemented as an \emph{online} version of DSBM.

\subsection{A flow of path measures} Let $(\Pbb^s, \hat{\Pbb}^s)_{s \geq 0}$ be a \emph{flow of path measures} defined for any $s \geq 0$ by
\begin{align}
    &\hat{\Pbb}^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}, \quad \partial_s \hat{\Pbb}^s = \projsimpleR(\projM(\hat{\Pbb}^s)) -\hat{\Pbb}^s , \quad \Pbb^s = \projM(\hat{\Pbb}^s), \label{eq:flow_hat_p}
\end{align}

\begin{wrapfigure}[18]{l}{0.4\textwidth}\vspace{-0.6cm}
\centering
    \includegraphics[width=0.34\textwidth]{img/schrodinger_flow.png}
    \caption{Illustration of the SB Flow and comparison with IMF. $\mathbb{Pbb}^\star$ is the SB, $(\hat{\Pbb}^n)_{n \in \nset}$ the IMF sequence and $(\hat{\Pbb}^s)_{s \geq 0}$ the flow we consider. See \Cref{sec:euclidean_flow} for the analysis of this example. }
    \label{fig:schrodinger_flow}
\end{wrapfigure}

which we assume is well-defined. Note that for any $s \geq 0$, $\Pbb^s$ is Markov while $\hat{\Pbb}^s$ is in the reciprocal class of $\Qbb$. 
Crucially, the only fixed point of \eqref{eq:flow_hat_p} is the SB. Indeed, let $\bar{\Pbb}$ be a fixed point of $(\Pbb^s)_{s \geq 0}$ in \eqref{eq:flow_hat_p}. Then, we have that $\bar{\Pbb} = \projsimpleR(\projM(\bar{\Pbb}))$. Hence, we get $\bar{\Pbb} = \projsimpleR(\projM(\dots(\projsimpleR(\projM(\bar{\Pbb})))\dots))$. Hence, under mild assumptions, $\bar{\Pbb}$ is a limit point of IMF and therefore $\bar{\Pbb}$ is the SB $\Pbb^\star$ given by \eqref{eq:dynamic_ot}, see \cite[Theorem 2]{peluchetti_diffusion_2023}. 



Next, for any $\alpha \in (0,1]$, we define the following discretisation of \eqref{eq:flow_hat_p} called $\alpha$-IMF:
\begin{equation}
\label{eq:discretisation_flow_path_measure}
        \hat{\Pbb}^{n+1} = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\projM(\hat{\Pbb}^n)) ,
\end{equation}
and $\Pbb^n = \projM(\hat{\Pbb}^n)$. Note that for any $n \in \nset$, $\hat{\Pbb}^n \in \calR(\Qbb)$.
This recovers the IMF procedure \citep{shi2023DSBM,peluchetti_diffusion_2023} when $\alpha = 1$.  
Using the definition of the sequence $(\hat{\Pbb}^n)_{n \in \nset}$, it is possible to analyse the sequence $(\Pbb^n)_{n \in \nset}$ using the properties of the KL divergence as well as the Pythagorean identities derived in \citep{shi2023DSBM,peluchetti_diffusion_2023}. Using the characterisation of the SB as the only path measure that preserves $\pi_0$, $\pi_1$, and is both Markov and in the reciprocal class of $\Qbb$ (see e.g.~\citep[Theorem 2.12]{leonard2014survey}), we get the following result.

\begin{theorem}{Convergence of $\alpha$-IMF}{convergence}
Let $\alpha \in (0, 1]$ and $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ defined by \eqref{eq:discretisation_flow_path_measure}. 
    Under mild assumptions, we have that 
    $\lim_{n \to +\infty} \Pbb^n = \Pbb^\star$, where $\Pbb^\star$ is the solution of the Schr\"odinger Bridge problem \eqref{eq:dynamic_ot}.
\end{theorem}

\subsection{Discretisation and non-parametric loss}
We show here that $\alpha$-IMF is associated with an \emph{incremental} version of DSBM for $\alpha \in (0,1)$. 

\paragraph{Iterative Markovian Fitting.}  For any $v: \ [0,1] \times \rset^d \to \rset^d$, we introduce the loss function
\begin{equation}
\label{eq:loss_function}
   \Lnonparam(v, \Pbb)  = \int_0^1\Lnonparam_t(v_t, \Pbb) \rmd t = \int_0^1 \int_{(\rset^d)^3} \Big\| v_t(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \Pbb_{0,1}(x_0,x_1) \rmd \Qbb_{t|0,1}(x_t | x_0, x_1) \rmd t ,
\end{equation}
where we recall that $\Qbb$ is induced by $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ for some $\vareps >0$. 
This loss was already considered in \citep{peluchettinon,lipman_flow_2022,liu2023I2SB,liu2022rectified,shi2023DSBM}.  We also define the path measure $\Pbb_v \in \pathmeas$ associated with  
\begin{equation}
\label{eq:mixture_bridge_sde}
    \rmd \bfX_t = v_t(\bfX_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 . 
\end{equation}
Consider first the sequence $(v^n)_{n \in \nset}$ defined by 
\begin{equation}
\label{eq:view_dsbm_non_param}
    v^{n+1} = \argmin_v \Lnonparam(v, \Pbb_{v^n}) .
\end{equation}
Using \Cref{def:markovian_proj}, we have that $\Pbb_{v^{n+1}} = \projM(\projsimpleR(\Pbb_{v^n}))$, which corresponds to $\Pbb^{n+1}$ in the IMF sequence. Therefore we have that $ \lim_{n \to +\infty} \Pbb_{v^n} = \Pbb^\star$ under mild assumptions \citep[Theorem 2]{peluchetti_diffusion_2023}. 

\paragraph{Functional gradient descent.} We now introduce a relaxation of \eqref{eq:view_dsbm_non_param}, where, instead of considering the $\argmin$, we update the vector field with one gradient step. To define this relaxation, we recall that for a functional $F: \ \mathcal{F} \to \rset$, where $\mathcal{F}$ is an appropriate function space, its functional derivative \citep{courant2008methods} with reference measure $\mu$ is denoted $\nabla_\mu F$ and is given for any $\phi \in \mathcal{F}$, when it exists, by 
\begin{equation}
\label{eq:functional_derivative}
 \textstyle \lim_{\gamma \to 0} (F(f + \gamma \phi) - F(f)) / \gamma = \int \langle \nabla_\mu F(f) (x), \phi(x) \rangle \rmd \mu(x).
\end{equation}
Initialised with $v_t^0(x) = (\mathbb{E}_{\hat{\Pbb}^0_{1|t}}[\bfX_1 \ | \ \bfX_t = x] - x)/(1-t)$,
where $\hat{\Pbb}^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$, we now introduce a sequence of vector fields $(v^n)_{n \in \nset}$.  This corresponds to training a bridge matching model (see e.g.~\cite{liu2023I2SB,albergo2023stochastic}), giving $\Pbb_{v^0} = \projM(\hat{\Pbb}^0)$. Then for $n \in \nset$, %$t \in [0,1]$ and $x \in \rset^d$,
let 
\begin{equation}
\label{eq:update_non_parametric}
    v^{n+1}_t(x) = v^n_t(x) - \delta_n \nabla_{\mu^n} \Lnonparam_t(v^n_t, \Pbb_{v^n})(x) ,
\end{equation}
with $\delta_n > 0$ and $\mu^n \in \calP(\calC)$. The parameters $(\delta_n, \mu^n)_{n \in \nset}$ will be made explicit in \Cref{prop:shadow_sequence}.
We emphasize that, in contrast to the IMF procedure, in the online update \eqref{eq:update_non_parametric} we do not need to solve a Markovian projection problem at every step; instead we simply take a gradient step on the loss \eqref{eq:loss_function}.

\paragraph{Connection with $\alpha$-IMF. }
The following proposition shows that $(\Pbb_{v^n})_{n \in \nset}$ defined by \eqref{eq:update_non_parametric} is associated with $\alpha$-IMF defined in  \eqref{eq:discretisation_flow_path_measure}. 
\begin{proposition}{Non-parametric updates are $\alpha$-IMF}{shadow_sequence}
Let $\alpha \in (0,1]$, $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ as in \eqref{eq:discretisation_flow_path_measure}, $\delta_n =  \alpha$ and $\mu^n = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\Pbb^n)$. Then, under mild assumptions, we have $\Pbb_{v^n} = \Pbb^n$ for all $n \in \nset$.
\end{proposition}
Combining \Cref{thm:convergence} to \Cref{prop:shadow_sequence}, we get that $\lim_{n \to +\infty} \Pbb_{v^n} = \Pbb^\star$, i.e.~the non-parametric procedure converges to the SB. 

\section{$\alpha$-Diffusion Schr\"odinger Bridge Matching}
\label{sec:online_dsbm}

\paragraph{From DSBM to $\alpha$-DSBM.} In \Cref{sec:theoretical_results}, we introduced $\alpha$-IMF, a scheme which defines a sequence of path measures converging to the SB for all $\alpha \in (0,1]$. For $\alpha=1$, this corresponds to the IMF, whose practical DSBM implementation \citep{shi2023DSBM} requires repeatedly solving an expensive minimisation problem \eqref{eq:view_dsbm_non_param}. In contrast, for $\alpha < 1$ we are only required to take one (non-parametric) gradient step to update the vector field, see \eqref{eq:update_non_parametric}. This suggests the following practical implementation of $\alpha$-IMF, called $\alpha$-DSBM: First, pretrain a bridge matching model so that for $t \in [0,1]$ and $x \in \rset^d$, $v_t^\theta(x) = (\mathbb{E}_{\hat{\Pbb}^0_{1|t}}[\bfX_1 \ | \ \bfX_t = x] - x)/(1-t)$, where $\hat{\Pbb}^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1} $. Then, perform the parametric version of the update \eqref{eq:update_non_parametric}:
\begin{equation}
\label{eq:update_alpha_imf}
    \theta \leftarrow \theta - \alpha \nabla_\theta \Lparam(\theta, \Pbb_{\bar{\theta}})  ; \  \Lparam(\theta, \Pbb) =  \int_0^1 \int_{(\rset^d)^3} \Big\| v_t^\theta(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \Pbb_{0,1}(x_0, x_1) \rmd \Qbb_{|0,1}(x_t|x_0,x_1) \rmd t ,
\end{equation}
where $\Pbb_{\bar{\theta}}$ is a stop-gradient version of $\Pbb_{v^\theta}$. In \Cref{sec:param_to_non_parametric}, we give a theoretical justification for this parametric equivalent of \eqref{eq:loss_function} and \eqref{eq:update_non_parametric} by showing that, as $\alpha \to 0$, the update on the velocity fields $v^\theta$ given by \eqref{eq:update_alpha_imf} corresponds to a direction of descent for the non-parametric loss \eqref{eq:update_non_parametric} on average. Once again, we emphasize that if we replace the gradient step in \eqref{eq:update_alpha_imf} with the minimisation $\theta \leftarrow \argmin_\theta \Lparam(\theta, \Pbb_{\bar{\theta}})$, we  recover DSBM. 

\paragraph{Bidirectional online procedure.} As with DSBM, directly implementing \eqref{eq:update_alpha_imf} leads to error quickly accumulating, see \Cref{sec:forward_forward_forward_backward} for details. One way to circumvent this error accumulation issue is to consider a \emph{bidirectional} procedure, in which we train both a forward and a backward model. This is possible because the Markovian projection coincides for forward and backward path measures, see \cite[Proposition 9]{shi2023DSBM}. This suggests considering the loss $\Lnonparam(v^\vsra, v^\vsla, \Pbb^\vsra, \Pbb^\vsla)=\int^1_0 \Lnonparam_t(v^\vsra_t, v^\vsla_t, \Pbb^\vsra, \Pbb^\vsla)\rmd t$, which is an extension of \eqref{eq:loss_function}, where 
\begin{align}
\label{eq:loss_function_bidirectional}
    \Lnonparam_t(v^\vsra_t, v^\vsla_t, \Pbb^\vsra, \Pbb^\vsla) &=   \int_{(\rset^d)^3} \Big\| v_t^\vsra(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \Pbb^\vsla_{0,1}(x_0, x_1) \rmd \Qbb_{t|0,1}(x_t|x_0,x_1)  \\
    &\qquad + \int_{(\rset^d)^3} \Big\| v_{1-t}^\vsla(x_t) - \frac{x_0 - x_t}{t} \Big\|^2 \rmd \Pbb^\vsra_{0,1}(x_0, x_1) \rmd \Qbb_{t|0,1}(x_t|x_0,x_1). 
\end{align}
Similarly to \eqref{eq:mixture_bridge_sde}, we define 
$\Pbb_{v^\vsra}, \Pbb_{v^\vsla}$, associated with $(\bfX_t)_{t \in [0,1]}$ and $(\bfY_{1-t})_{t \in [0,1]}$ respectively, which are defined by forward and backward SDEs
\begin{equation}
\label{eq:mixture_bridge_sde_forward_backward}
    \textrm{(fwd):} \  \rmd \bfX_t = v_t^\vsra(\bfX_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t , \ \bfX_0 \sim \pi_0 , \; \textrm{(bwd):} \  \rmd \bfY_t = v_t^\vsla(\bfY_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t , \ \bfY_0 \sim \pi_1 . 
\end{equation}
Similarly to \eqref{eq:update_non_parametric}, we define non-parametric updates for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$
\begin{equation}
\label{eq:update_non_parametric_fb}
    (v^{n+1,\vsra}_t(x),v^{n+1, \vsla}_t(x))  = \left(v^{n,\vsra}_t(x),v^{n, \vsla}_t(x)\right) - \delta_n \nabla_{\mu^n} \Lnonparam_t\left(v^{n, \vsra}_t(x),v^{n, \vsla}_t(x), \Pbb_{v^{n, \vsra}}, \Pbb_{v^{n, \vsla}}\right)(x).
\end{equation}
We have the following proposition which ensures our bidirectional procedure is still valid and that the results of \Cref{prop:shadow_sequence} still hold. 
\begin{proposition}{Bidirectional updates}{shadow_sequence_bidirectional}
Let $\alpha \in (0,1]$. For any $n \in \nset$, define $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ by \eqref{eq:discretisation_flow_path_measure}. Then,
under mild assumption and assuming that $\delta_n =  \alpha$ and $\mu^n = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\Pbb^n)$, we have that for any $n \in \nset$, $\Pbb_{v^{n,\vsra}} = \Pbb_{v^{n,\vsla}} =  \Pbb^n$. 
\end{proposition}
In \Cref{sec:forward_forward_forward_backward}, we show that in the Gaussian setting the bidirectional procedure \eqref{prop:shadow_sequence_bidirectional} does not accumulate error when the vector field is approximated, while the unidirectional one \eqref{eq:update_non_parametric} does. 

\paragraph{Vector field parameterisation.} Contrary to existing procedures \citep{shi2023DSBM,peluchetti_diffusion_2023,liu2022rectified}, we  do not parameterise $v^\vsra$ and $v^\vsla$ using two separate networks. Instead, we consider an additional input $s \in \{0,1\}$ such that $v_\theta(1, \cdot) \approx v^\vsra$ and $v_\theta(0, \cdot) \approx v^\vsla$. This allows us to substantially reduce the number of parameters in the model. The conditioning on $s$ in the network is detailed in \Cref{sec:experimental_details}. Before stating our full algorithm in \Cref{alg:online_DSBM_general}, we introduce a batched parametric version of \eqref{eq:loss_function_bidirectional}. For ease of notation, we write $\mathrm{Interp}_t$ for the operation corresponding to sampling from $\Qbb_{t|0,1}$, i.e. 
\begin{equation}
\label{eq:interpolation_brownian_motion}
    \mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ) = (1-t)\bfX_0 + t \bfX_1 + \sqrt{\vareps(1-t)t} \bfZ . 
\end{equation}
We are now ready to introduce the batched parametric version of \eqref{eq:loss_function_bidirectional}.
For a given batch of inputs $\bfX_0^{1:B}$ and $\bfX_1^{1:B}$, timesteps $t \sim \mathrm{Unif}([0,1])^{\otimes B}$, and $\bfX_t =\mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ)$ with $\bfZ \sim \gN(0, \Id)^{\otimes B} $, we compute the empirical forward and backward losses as
\begin{align}
\label{eq:empirical_loss}
   &\ell^{\vsra}(\theta; t, \bfX_1, \bfX_t)  = \frac{1}{B}\sum_{i=1}^B \| v_\theta\left(1, t^i, \bfX_t^i\right) - \left(\bfX_1^i - \bfX_t^i\right)/(1-t^i) \|^2,\\
    &\ell^{\vsla}(\theta; t, \bfX_0, \bfX_t)  = \frac{1}{B} \sum_{i=1}^B \Big\| v_\theta\left(0, 1 - t^i, \bfX_t^i\right) - \left(\bfX_0^i - \bfX_t^i\right)/t^i \Big\|^2 . 
\end{align}


We present the resulting $\alpha$-DSBM in \Cref{alg:online_DSBM_general}.
Note that in this algorithm, we maintain an Exponential Moving Average (EMA) of model parameters, as is common in diffusion models \citep{nichol2021improved}. During the finetuning stage, when we generate samples to use as model's inputs, we then have a choice of sampling using the EMA or non-EMA parameters. At test time, we always sample using the EMA parameters, as it is known to improve the visual quality \citep{song_improved_2020}. In \Cref{alg:online_DSBM_general}, we specify $\alpha \in (0,1]$ as a stepsize parameter. In practice, we use Adam~\citep{kingma:adam} for optimization, thus the choice of $\alpha$ is implicit and adaptive throughout the training. We refer to \Cref{sec:experimental_details} for more details on our experimental setup. 

\begin{algorithm}%[H]
\caption{$\alpha$-Diffusion Schr\"odinger Bridge Matching}
\label{alg:online_DSBM_general}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} datasets $\pi_0$ and $\pi_1$, entropic regularisation $\vareps$, number of pretraining and finetuning steps $N_{\mathrm{pretraining}}$ and $N_{\mathrm{finetuning}}$, batch size $B$ and half batch size $b = B/2$, EMA decay $\gamma$, initial parameters $\theta$ and initial EMA parameters $\theta^{\scriptscriptstyle\text{EMA}} = \theta$, $\alpha \in (0,1]$}
\FOR{$n \in \{1, \dots, N_{\mathrm{pretraining}}\}$} 
\STATE Sample $(\bfX_0, \bfX_1) \sim (\pi_0 \otimes \pi_1)^{\otimes B}$
\STATE Sample $t \sim \mathrm{Unif}([0,1])^{\otimes B}$ and $\bfZ \sim \gN(0, \Id)^{\otimes B}$ and compute $\bfX_t = \mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ)$
\STATE Update $\theta$ with a gradient step on $\frac{1}{2}\left[\ell^\vsra\left(t^{1:b},\bfX_1^{1:b}, \bfX_t^{1:b}\right) + \ell^\vsla\left(t^{b+1:B},\bfX_0^{b+1:B}, \bfX_t^{b+1:B}\right)\right]$ 
\STATE Update EMA parameters: $\theta^{\scriptscriptstyle\text{EMA}} = \gamma \theta^{\scriptscriptstyle\text{EMA}} + (1 - \gamma) \theta $
\ENDFOR
\FOR{$n \in \{1, \dots, N_{\mathrm{finetuning}}\}$}
\STATE Sample $(\bfX_0, \bfX_1) \sim (\pi_0 \otimes \pi_1)^{\otimes b}$
\STATE Sample  $\hat \bfX_1$ solving forward SDE \eqref{eq:mixture_bridge_sde_forward_backward}-\textrm{(fwd)} with $v_{\theta ^{\scriptscriptstyle\text{EMA}} }(1,\cdot)$ or $v_\theta(1,\cdot)$ starting from $\bfX_0$
\STATE Sample  $\hat \bfX_0$  solving backward SDE \eqref{eq:mixture_bridge_sde_forward_backward}-\textrm{(bwd)} with $v_{\theta ^{\scriptscriptstyle\text{EMA}} }(0,\cdot)$ or $v_\theta(0,\cdot)$ starting from $\bfX_1$
\STATE Sample $t^\vsra \sim \mathrm{Unif}([0,1])^{\otimes b}$  and $\bfZ^\vsra \sim \gN(0,\Id)^{\otimes b} $ and compute $\bfX_t^\vsra = \mathrm{Interp}_{t^\vsra}(\hat \bfX_0, \bfX_1, \bfZ^\vsra)$
\STATE Sample $t^\vsla \sim \mathrm{Unif}([0,1])^{\otimes b}$  and $\bfZ^\vsla \sim \gN(0,\Id)^{\otimes b} $ and compute $\bfX_t^\vsla = \mathrm{Interp}_{t^\vsla}(\bfX_0, \hat\bfX_1, \bfZ^\vsla)$
\STATE Update $\theta$ with a gradient step on $\frac{1}{2}\left[\ell^\vsra(t^\vsra,\bfX_1, \bfX_t^\vsra) + \ell^\vsla(t^\vsla,\bfX_0, \bfX_t^\vsla)\right]$ and stepsize $\alpha$
\STATE Update EMA parameters: $\theta^{\scriptscriptstyle\text{EMA}} = \gamma \theta^{\scriptscriptstyle\text{EMA}} + (1 - \gamma) \theta $ 
\ENDFOR
\STATE{\textbf{Output:} $(\theta, \theta^{\scriptscriptstyle\text{EMA}})$ parameters of the finetuned model }
\end{algorithmic}
\end{algorithm}

 
\section{Related work}

\paragraph{Solving Schr\"odinger Bridge problems.}
Schr\"odinger Bridges
\citep{schrodinger1932theorie} have been thoroughly studied through the lens of probability theory \citep{leonard2014survey} and stochastic control \citep{dai1991stochastic,chen2020optimal}. They recently found applications in generative modeling and related fields leveraging recent advances in diffusion models \citep{debortoli2021diffusion,vargas2021solving,chen2021likelihood}.
Extensions of these methods to other machine learning problems and modalities were studied in 
\citep{shi2022conditional,thornton2022riemannian,liu2022deep,chen2024deep,tamir2023transport}. 
\cite{shi2023DSBM,peluchetti_diffusion_2023} concurrently introduced the DSBM algorithm which relies on a new procedure called IMF, while the DSB algorithm introduced in  \citep{debortoli2021diffusion} is based on the standard Iterative Proportional Fitting (IPF) scheme.  \cite{neklyudov2023action,neklyudov2023computational,liu2022deep} generalise DSBM to arbitrary cost functions, albeit at the expense of having to learn the reciprocal projection which is no longer given by a Brownian bridge. These new methodologies translate to improved numerics when compared to their IPF counterparts, but they remain reliant on alternating between the optimisation of two losses. Finally, we note that the Schr\"odinger Bridge flow and the $\alpha$-IMF procedure can be linked to the Sinkhorn flow recently introduced by \cite{karimi2024sinkhorn}, see \Cref{sec:connection_sinkhorn_flows} for a detailed discussion.

\paragraph{Sampling-free methodologies.}
Sampling-free methodologies have been proposed to solve OT related objectives. In \citep{liu2023I2SB, somnath2023aligned, diefenbacher2024improving, cao2024neural}, the authors perform one step of DSBM, i.e.~only consider the pretraining stage of our algorithm. While the obtained bridge might enjoy transport properties, it does not solve an OT problem. 
% In fact, in the context of \emph{paired} transfer tasks, solving the OT problem might be undesirable as argued in \citep{debortoli2023augmented,chen2024probabilistic}. Indeed, in that case, one wants to \emph{preserve} the coupling from the training dataset. 
In another line of work, \cite{pooladian_2023_multisample,tong_conditional_2023,tong2024simulationfree,eyring2023unbalancedness} have proposed simulation-free methods to minimise OT objectives. However, they target not the OT problem, but a minibatch version of it which coincides with OT only in the limit of infinite batch size, see \cite[Theorem 4.2]{pooladian_2023_multisample}. Other sampling-free methods to solve the Schr\"odinger Bridge problem include \cite{kim2023unpaired,gushchin2024entropic} both of which rely on adversarial losses to solve the OT problem. In \citep{debortoli2021diffusion,vargas2021solving,liu2022deep,shi2023DSBM,peluchetti_diffusion_2023} the adversarial objective is dropped and instead the procedure requires alternating objectives during training and is not sampling-free. We also highlight the line of work of \cite{korotin2023light,gushchin2024light} in which the Schr\"odinger Bridge potentials are parameterised with mixtures of Gaussians, allowing for fast training in small dimensions. Finally, recently \citet{deng2024variational} introduced a variation on Schr\"odinger Bridge for generative modeling, which while still not sampling-free, does not require learning a forward process.

\vspace{-.25cm}
\section{Experiments}
\label{sec:experiments}

In this section, we illustrate the efficiency of $\alpha$-DSBM on different tasks. In \Cref{sec:gaussian_case}, we compare $\alpha$-DSBM to DSBM in a Gaussian setting where the EOT coupling is tractable and show that $\alpha$-DSBM recovers the solution faster than DSBM. In \Cref{sec:image_xps}, we illustrate the scalability of our method through a range of unpaired image translation experiments. 

\subsection{Gaussian case}
\label{sec:gaussian_case}

% This is new compared to NeurIPS May submission!!!

We compare $\alpha$-DSBM to DSBM in the Gaussian setting where $\pi_0 = \gN(0, \sigma_0^2 \Id)$, $\pi_1 = \gN(0, \sigma_1^2 \Id)$ and $\Qbb$ is associated with $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ with $\sqrt \vareps = 0.5$. In this case, the EOT coupling is $\gN(0, \Sigma_\star)$, with $\Sigma_\star$ given by 
\begin{equation}
    \Sigma_\star = \left( \begin{matrix} \sigma_0^2 \Id & \sigma_\star^2 \Id \\ \sigma_\star^2 \Id & \sigma_1^2 \Id \end{matrix} \right) ,~\textup{where~~}\sigma_\star^2 = (1/2)((\sigma_0^2 \sigma_1^2 + \vareps^2)^{1/2} - \vareps),
\end{equation}
with $\Id$ being a $d \times d$ identity matrix. We consider $d=50$, $\sigma_0 = \sigma_1 =1$, resulting in $\sigma_\star^2 \approx 0.88$. To showcase the robustness of $\alpha$-DSBM, we consider the initial coupling $\Pbb_{0,1}$, where $(\bfX_0, \bfX_1) \sim \Pbb_{0,1}$, $\bfX_0 \sim \gN(0, \Id)$, $\bfX_1 = -\bfX_0$, and let $\hat{\Pbb}^0 = \Pbb_{0,1} \Qbb_{|0,1}$. In this setting, the base model, i.e. bridge matching, significantly underestimates the true covariance $\sigma_\star^2$, as shown in \Cref{fig:gaussian_case_antithetic}. Additionally, the figure illustrates that online finetuning approaches the true solution faster than the original iterative DSBM finetuning. For the latter, we can set how often we alternate between updating the forward and backward networks, and as this frequency increases, the behaviour approaches that of the online finetuning. 


\begin{wrapfigure}{l}{0.4\textwidth}
\centering
    \includegraphics[width=0.4\textwidth]{img/gaussian/gaussian_ira_100nfe.png}
    \caption{Evolution of the covariance during online and iterative DSBM finetuning for forward and backward networks. The finetuning starts after 10K steps of training a bridge matching model. For the iterative case, we alternate between forward and backward updates with varying frequencies, i.e. changing after 1K, 2.5K and 5K steps. }
    \label{fig:gaussian_case_antithetic}
\end{wrapfigure}

\begin{figure}[htbp]
    \centering 
    \subfloat{\includegraphics[width=0.45\textwidth, valign=t]{img/mnist/mnist_barplot_sigma_sweep_bwd.png }}\hfill
    \subfloat{\includegraphics[width=0.5\textwidth, valign=t]{img/afhq_main/afhq64_sigma_sweep_small.png}}\hfill
    \caption{\textbf{Left}: FID and Mean Squared Distance (MSD) on EMNIST to MNIST translation before and after finetuning with different values of $\vareps$. \textbf{Right}: AFHQ-64 samples after the finetuning.  For both, we use a bidirectional model with online finetuning. More results are in \Cref{sec:experimental_details_mnist} and \ref{sec:experimental_details_afhq}.} 
    \label{fig:mnist_sigma_sweep_bars}
\end{figure}


\subsection{Image datasets}
\label{sec:image_xps}

Similarly to \cite{shi2023DSBM}, we apply our method to image translation problems, such as MNIST digits to EMNIST letters~\citep{mnist, cohen2017emnist}, and Wild to Cat domains from the Animal Faces-HQ (AFHQ) dataset~\citep{choi2020starganv2}, downsampled to 64 $\times$ 64 and 256 $\times$ 256 resolutions. 

The whole training procedure can be framed as a two-stage process: first, we train a base model on the true data samples, performing bridge matching~\citep{peluchettinon,albergo_building_2023,lipman_flow_2022,liu2023I2SB}, and then we finetune this model. 
We compare models that combine different vector field parameterisations (two networks vs.~one bidirectional net), finetuning methods (iterative vs.~online), and sample generation strategies during the finetuning stage. 
% For the latter, we have the choice of generating training samples either using EMA or non-EMA parameters. 

Following the established practice~\citep{choi2020starganv2}, we evaluate our models using FID \citep{heusel2017gans} for visual quality, and mean squared distance (MSD) or LPIPS ~\citep{zhang2018unreasonable} for alignment. It is important to note that for image translation tasks at hand, FID scores are not ideal, as FID was designed for natural RGB images, which is not the case for MNIST.  It is also not well suited for small sample sizes as it is the case with AFHQ, where the test set in each domain has fewer than 500 examples. Thus quantitative results in Table \ref{table:mnist_afhq} should be interpreted cautiously, and we recommend a visual inspection of samples to complement these quantitative measures, especially for the AFHQ models. Samples from the models along with the training and evaluation protocols are given in Appendix \ref{sec:experimental_details}.

Compared to the iterative DSBM, our online finetuning $\alpha$-DSBM reduces the number of tunable hyperparameters, i.e.~inner and outer iterations, refresh rate and the size of the cache for storing generated samples. This simplifies implementation and makes the algorithm more practical. The primary remaining hyperparameter, the variance of a Brownian motion $\vareps$,  requires careful tuning as it influences the trade-off between the visual quality and alignment, as was also observed in ~\cite{shi2023DSBM}. An appropriate $\vareps$ needs to balance the two: setting $\vareps$ too low results in poor visual quality, while high values of $\vareps$ cause poorly aligned and oversmoothed samples. 
\Cref{fig:mnist_sigma_sweep_bars} illustrates how FID and MSD metrics vary with $\vareps$ for the case of MNIST. Additionally, it demonstrates the impact of $\vareps$ on the generated samples for the AFHQ-64 model.
% To aid interpretation of these quantitative results, Figures~\ref{fig:mnist_sigma_sweep_samples} and \ref{fig:afhq_sigma_sweep} in Appendix \ref{sec:experimental_details} provide samples for models with different $\vareps$ values for both MNIST and AFHQ tasks.




\begin{table}[]
 \centering
    \begin{tabular}{lllll}
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{EMNIST $\rightarrow$ MNIST} & \multicolumn{2}{c}{AFHQ-64 Wild $\rightarrow$ Cat} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
                            & FID         & MSD         & FID        & LPIPS       \\
       \midrule
      DSBM* & 10.59 & 0.375 & --  &  -- \\
      \midrule
      Pretrained two-networks model & 6.02 & 0.564 & 25.97 & 0.589 \\
      (a) iterative finetuning  &  5.25\std{0.15}  &  0.345\std{0.001} & 25.41\std{0.84} & 0.485\std{0.003}    \\
      (b) online finetuning &  4.28\std{0.07}  &  0.368\std{0.001} & 28.752\std{1.191} & 0.487\std{0.003}    \\
      (c) online finetuning without EMA & 4.23\std{0.171}  & 0.361\std{0.002}  & 32.665\std{0.647} &  0.445\std{0.002} \\
      \midrule
      Pretrained bidirectional model & 6.33 & 0.572 & 29.44 & 0.584 \\
      (d) online finetuning &  4.39\std{0.09}  &  0.387\std{0.003} & 26.579\std{0.434} & 0.482\std{0.001}  \\  
      (e) online finetuning without EMA   &  4.57\std{0.17}  &  0.369\std{0.003} & 30.638\std{1.023} & 0.451\std{0.002} \\  
      \bottomrule
    \end{tabular}
    \vspace{5pt}
    \caption{Results of image translation between EMNIST and MNIST, and AFHQ 64$\times$64 between Wild and Cat domains. DSBM* results are from \cite{shi2023DSBM}. Our reimplementation of DSBM corresponds to row (a). For MNIST and AFHQ models, we used $\vareps=1$ and $\vareps=0.75^2$, respectively. Each finetuning run was done with 5 random seeds, and we report mean scores Â± standard deviation.}
    \label{table:mnist_afhq}
\end{table}


\begin{figure*}
    \centering
    % FWD: cat -> wild
    \begin{minipage}[t]{0.48\textwidth}  % Adjust width to fit all three images and caption
        \centering
        \begin{minipage}[t]{0.19\textwidth} 
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_fwd_6.png}
        \end{minipage}
        \begin{minipage}[t]{0.19\textwidth}
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_fwd_7.png}
        \end{minipage}
        \begin{minipage}[t]{0.19\textwidth}
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_fwd_3.png}
        \end{minipage}
        \begin{minipage}[t]{0.19\textwidth}
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_fwd_4.png}
        \end{minipage}
        \begin{minipage}[t]{0.19\textwidth}
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_fwd_5.png}
        \end{minipage}
        \caption*{(a) Cat $\rightarrow$ Wild} 
    \end{minipage}
    \hfill
    % BWD: wild -> cat
    \begin{minipage}[t]{0.48\textwidth}  % Adjust width to fit all three images and caption
        \centering
        \begin{minipage}[t]{0.19\textwidth} 
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_bwd_1.png}
        \end{minipage}
        \begin{minipage}[t]{0.19\textwidth}
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_bwd_2.png}
        \end{minipage}
        \begin{minipage}[t]{0.19\textwidth}
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_bwd_3.png}
        \end{minipage}
        \begin{minipage}[t]{0.19\textwidth}
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_bwd_4.png}
        \end{minipage}
        \begin{minipage}[t]{0.19\textwidth}
            \includegraphics[width=\linewidth]{img/afhq_main/afhq256_bwd_5.png}
        \end{minipage}
        \caption*{(b) Wild $\rightarrow$ Cat} 
    \end{minipage}
\caption{Online DSBM transfer results on AFHQ 256$\times$ 256 dataset between Cat and Wild domains. Top row---initial samples, bottom row---transferred samples.  } 
\end{figure*}


\section{Discussion}

In this paper we have introduced $\alpha$-Diffusion Schr\"odinger  Bridge Matching ($\alpha$-DSBM), a new methodology to solve Entropic Optimal Transport problems. $\alpha$-DSBM is an improved version of DSBM, which does not require training multiple DDM-type models. 
%Instead, the forward and backward objectives appearing in DSBM are minimised jointly using samples from the previous training iteration.
We have shown that a non-parametric version of this method recovers the Schr\"odinger Bridge (SB). In addition, $\alpha$-DSBM is easier to implement than existing SB methodologies while exhibiting similar performance. We illustrated the efficiency of our algorithm on a variety of unpaired transfer tasks. 

While $\alpha$-DSBM solves one of the most critical limitations of DBSM, namely the alternative optimisation, several issues remain to be addressed in order for the method to scale comparably to generative DDMs. In particular, the method is not sampling-free, as during training it requires sampling from the model from the previous iteration to obtain the training data for the current iteration. While it seems difficult to derive a completely sampling-free method to solve SB problems without resorting to the Minibatch OT approximation, there is still room for improvement. 
% As our methodology can be interpreted as an online finetuning procedure, a promising area of research is the study of the links between our procedure and Reinforcement Learning finetuning of diffusion models as in \cite{lee2023aligning,black2023training,fan2024reinforcement}. 
% Finally, while our theoretical results ensure the convergence of a non-parametric version of the algorithm, the convergence of the parametric methodology remains an open problem.
% , even though the experimental evidence is promising. 
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apalike}
\bibliography{main.bbl}

\newpage

\appendix

\section{Appendix organisation}

The supplementary material is organised as follows. First in \Cref{sec:euclidean_flow}, we analyze an Euclidean counterpart to the $\alpha$-IMF sequence identified in \Cref{sec:theoretical_results} and the associated flow. In \Cref{sec:optimality_interpolation}, we show that the Markovian projection can be recovered as the parameterisation of the vector field that minimises the accumulation of errors, extending the results of \cite{chen2024probabilistic}. 
    Theoretical results are gathered in \Cref{sec:theoretical_results_appendix}. In particular in \Cref{sec:non_parametric_convergence} we show that the proposed non-parametric method coincides with the $\alpha$-IMF and prove the convergence of the $\alpha$-IMF. In \Cref{sec:param_to_non_parametric}, we show the connection between the non-parametric and the parametric updates. In \Cref{sec:background_dsbm}, we provide more background on DSBM and propose an extension of the DSBM methodology. Consistency losses similar to \citep{daras2024ambient,debortoli2024target} are proposed in \Cref{sec:consistency_schrodinger_bridge}. Model stitching procedures are described in \Cref{sec:model_stitching}. We comment on extended related work in \Cref{sec:extended_related_work}. In particular we draw connections with Sinkhorn flows \citep{karimi2024sinkhorn}, Reinforcement Learning policies, Expectation-Maximisation schemes following \citep{brekelmans2023schrodinger} and comment on finetuning of diffusion models. In \Cref{sec:forward_forward_forward_backward}, we investigate the accumulation of bias in a Gaussian setting and compare forward-forward and forward-backward methods. In \Cref{sec:preconditioning_loss}, we derive the preconditioning of loss following the principles of \citep{karras2022elucidating} in the case of bridge matching. Additional results and experimental details are presented in \Cref{sec:experimental_details}.

\section{Euclidean flow and iterative procedure}
\label{sec:euclidean_flow}

In this section, we study a simplified counterpart of the Schr\"odinger flow and of DSBM in a Euclidean setting. The goal of this section is to draw some conclusions in the Euclidean case which also remain true empirically when analyzing the Schr\"odinger Bridge problem.


We consider the set $\mathsf{A}_1 = \ensembleLigne{(x,y) \in \rset^2}{y \ge x}$ and the set $\mathsf{A}_2 = \ensembleLigne{(x,y) \in \rset^2}{y \leq 0}$. Loosely  speaking, one can identify $\mathsf{A}_1$ with the reciprocal class $\mathcal{R}(\mathbb{Q})$ and $\mathsf{A}_2$ with the set of Markov path measures $\projM$. In that case, we have that for any $(x,y) \in \rset^2$, $\mathrm{proj}_{\mathsf{A}_1}((x,y)) = ((x+y)/2,(x+y)/2)$ if $(x,y) \notin \mathsf{A}_1$ and otherwise, $\mathrm{proj}_{\mathsf{A}_1}((x,y)) = (x,y)$. In addition, we have that for any $(x,y) \in \rset^2$, $\mathrm{proj}_{\mathsf{A}_2}((x,y)) = (x,0)$ if $(x,y) \notin \mathsf{A}_2$ and $\mathrm{proj}_{\mathsf{A}_2}((x,y)) = (x,y)$ otherwise. We consider the following flow $(x_t,y_t)_{t \geq 0}$ given by 
\begin{equation}
    \partial_t (x_t,y_t) = \mathrm{proj}_{\mathsf{A}_1}(\mathrm{proj}_{\mathsf{A}_2}((x_t,y_t))) - (x_t,y_t) . 
\end{equation}
Let $(x_0,y_0) \notin \mathsf{A}_1$ and $(x_0,y_0) \notin \mathsf{A}_2$. Denote $T$ the explosion time of $(x_t,y_t)$, i.e.~for any $t \geq T$ we have that $(x_t,y_t) = \infty$, where $\rset^2 \cup \{ \infty \}$ is the one-point compactification of $\rset^2$. Finally, denote $\tau \leq T$ such that for any $t \in [0, \tau]$, $(x_t, y_t) \not \in \mathsf{A}_1$ and $(x_t, y_t) \not \in \mathsf{A}_2$. Then, we have 
\begin{equation}
    \partial_t (x_t,y_t) = (-x_t/2, x_t/2 - y_t) . 
\end{equation}
Hence, we have that $x_t = x_0 \exp[-t/2]$ for any $t \in [0, \tau]$ and $y_t = x_0 \exp[-t/2] + (x_0 \exp[-t/2])^2 (y_0 - x_0)/x_0^2 $. Therefore, we get that $\tau = T = +\infty$ and we have that for any $t \geq 0$
\begin{equation}
    x_t = x_0 \exp[-t/2] , \qquad y_t = x_t +x_t^2 (y_0 - x_0)/x_0^2  . 
\end{equation}
Hence, $((x_t, y_t))_{t \geq 0}$ converges exponentially fast to $(0,0)$ with rate $1/2$. 

We now investigate the rate of convergence of the alternate projection scheme, i.e.~the Euclidean equivalent of DSBM. 
We define $((x_n, y_n))_{n \in \nset}$ such that for any $n \in \nset$, 
\begin{equation}
    (x_{n+1}, y_{n+1}) = \mathrm{proj}_{\mathsf{A}_1}(\mathrm{proj}_{\mathsf{A}_2}((x_n,y_n))) = (x_n/2, 0) . 
\end{equation}
Hence, we get that $x_n = x_0 2^{-n}$ and therefore $((x_n, y_n))_{n \in \nset}$ converges exponentially fast to $(0,0)$. Note that this procedure corresponds to a discretisation of the flow $((x_t, y_t))_{n \in \nset}$ with stepsize $\alpha =1 $. 

More generally, we define for any $\alpha \in (0, 1]$,  $((x_n^\alpha, y_n^\alpha))_{n \in \nset}$ such that for any $n \in \nset$, 
\begin{equation}
    (x_{n+1}^\alpha, y_{n+1}^\alpha) = \alpha \mathrm{proj}_{\mathsf{A}_1}(\mathrm{proj}_{\mathsf{A}_2}((x_n^\alpha,y_n^\alpha))) + (1-\alpha)  (x_{n}^\alpha, y_{n}^\alpha) .
\end{equation}
Hence, we get that $x_n = x_0 2^{-n}$ and therefore $((x_n, y_n))_{n \in \nset}$ converges exponentially fast to $(0,0)$. It can be shown that for any $n \in \nset$, $x_n^\alpha =x_0^\alpha (1 - \alpha /2)^n$ and in addition, we have that 
\begin{equation}
     y_n^\alpha = (1-\alpha)^n y_0^\alpha + \alpha x_0^\alpha  \sum_{k=0}^{n-1} (1-\alpha)^k (1-\alpha/2)^{n-k} .
\end{equation}
Therefore, we get that 
\begin{equation}
    y_n = (1-\alpha)^{n}y_0^\alpha + 2 (1 - (1 - \alpha/(2-\alpha))^n)(1-\alpha/2)^n x_0^\alpha . 
\end{equation}

We now analyse the complexity of the different discretisations assuming that the cost of discretising the flow with stepsize $\alpha \in (0,1]$ is $C^\alpha$. In that case in order to reach the threshold value $\vareps$, i.e.~$\abs{x_n^\alpha} \leq \vareps$, we get a total cost $C^\alpha_n = O(\log(1/\vareps)C^\alpha/\log(1/(1-\alpha/2)))$, where we have neglected the terms that do not depend on $\log(1/\vareps)$. Hence, if $C^\alpha$ is constant then the choice $\alpha = 1$ is the best possible one in the range $\alpha \in (0,1]$. Otherwise, one has to consider the ratio $C^\alpha / \log(1/(1-\alpha/2))$, where the lower is the better. The flow procedure and the iterative based one are presented in \Cref{fig:schrodinger_flow}.

Based on this simplified Euclidean experiment, we draw some conclusions which also remain true in our setting, see \Cref{sec:experimental_details} for more experimental details. First, we have that different discretisations of the flow yield different convergence rates. Large stepsizes incur faster convergence. This suggests to choose $\alpha =1$. However, if the cost of choosing $\alpha =1$ is too high then one might turn to alternative schemes with $\alpha \in (0,1)$ assuming that $C^\alpha < C^1$ in that case. To draw a parallel with our setting, in the case of DSBM (case $\alpha =1$), we need to solve the projection subproblem  at each step which incurs a great cost. On the other hand, one step of the online algorithm only requires sampling once from the model and performing one gradient step.
 
\section{Minimisation of errors and Markovian projection}
\label{sec:optimality_interpolation}
For a given non-Markovian (stochastic) interpolant process (see definition below), there exist an infinite number of Markov processes admitting the same marginals  \citep{albergo_building_2023}. In this section, when it is well-defined, we show that the Markovian projection corresponds to the process which minimises an error measure (defined further) in case one has access to the oracle of $x_t \mapsto \mathbb{E}[\bfX_1 \ | \ \bfX_t = x_t]$.

\paragraph{Stochastic Interpolant.} We first start by recalling the framework of \cite{albergo_building_2023}. Consider a coupling $\Pi$ between $\pi_0$ and $\pi_1$, one builds a (stochastic) flow between $\pi_0$ and $\pi_1$ using the following interpolation procedure
\begin{equation}
\label{eq:interpolant}
    \bfX_t = \mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ) = \alpha_t \bfX_0 + \beta_t \bfX_1 + \gamma_t \bfZ , \quad (\bfX_0, \bfX_1) \sim \Pi , \quad \bfZ \sim \gN(0, \Id) ,
\end{equation}
where $\alpha_1  = \beta_0 = \gamma_0 = \gamma_1 = 0$ and $\alpha_0 = \beta_1 = 1$. This defines a non-Markovian process. We denote by $\pi_t$ the induced unconditional distribution of $\bfX_t$.
Let us now consider the Markov process $(\bfX_t^\vareps)$ given by 
\begin{equation}
\label{eq:stochastic_flow}
    \rmd \bfX_t^\vareps = \mathbb{E}\left[\dot{\alpha}_t \bfX_0 + \dot{\beta}_t \bfX_1 + (\dot{\gamma}_t - \vareps_t^2 /(2\gamma_t)) \bfZ \ | \ \bfX_t = \bfX_t^\vareps\right] + \vareps_t \rmd \bfB_t , \qquad \bfX_0^\vareps \sim \pi_0,
\end{equation}
where $(\bfB_t)_{t \in [0,1]}$ is a $d$-dimensional Brownian motion and $\vareps_t$ is an additional hyperparameter. It can then be shown that $(\bfX_t^\vareps)_{t \in [0,1]}$ satisfies that $\bfX_t^\vareps \sim \pi_t$ for all $t\in[0,1]$; see e.g.~\citep[Theorem 2.8, Corollary 2.10]{albergo2023stochastic}. Hence $(\bfX_t^\vareps)_{t \in [0,1]}$ is a (stochastic) flow mapping $\pi_0$ onto $\pi_1$. Note that $(\bfX_t^\vareps)_{t \in [0,1]}$ in \eqref{eq:stochastic_flow} can be rewritten as 
\begin{equation}
\label{eq:stochastic_flow_rewrite}
    \rmd \bfX_t^\vareps = (\dot{\alpha}_t/\alpha_t) \bfX_t^\vareps + \mathbb{E}\left[(\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \bfX_1 + (\dot{\gamma}_t - \gamma_t \dot{\alpha}_t/\alpha_t -  \vareps_t^2 /(2\gamma_t)) \bfZ \ | \ \bfX_t = \bfX_t^\vareps\right] + \vareps_t \rmd \bfB_t.
\end{equation}

In the specific case where $\alpha_t = 1-t$, $\beta_t = t$ and $\gamma_t = \sigma_0 \sqrt{t(1-t)}$ then \eqref{eq:stochastic_flow_rewrite} becomes
\begin{equation}
\label{eq:interpolation_brownian_motion_appendix}
    \bfX_t = \mathrm{Interp}_t(\bfX_0, \bfX_1, \bfZ) = (1-t) \bfX_0 + t \bfX_1 + \sigma_0 \sqrt{t(1-t)} \bfZ.
\end{equation} 
This corresponds to the marginal distribution of the bridge associated with $(\sigma_0 \bfB_t)_{t \in [0,1]}$. In this case, \eqref{eq:stochastic_flow_rewrite} becomes
\begin{equation}
\label{eq:markov_projection}
    \rmd \bfX_t^\vareps = \mathbb{E}\left[(\bfX_1 - \bfX_t)/(1-t) | \bfX_t = \bfX_t^\vareps\right] \rmd t + \sqrt{2} \sigma_0 \rmd \bfB_t  
\end{equation}
for $\vareps_t^2 = (2\gamma_t)(\dot{\gamma}_t - \gamma_t \dot{\alpha}_t / \alpha_t)=2\sigma^2_0$. In \Cref{prop:choice_eps}, we will show that this choice of $(\vareps_t)_{t \in [0,1]}$ is optimal in some sense. 

Consider $(\hat{\bfX}_t^\vareps)_{t \in [0,1]}$ given by 
\begin{equation}
\label{eq:stochastic_flow_rewrite_approximate}
        \rmd \hat{\bfX}_t^\vareps = (\dot{\alpha}_t/\alpha_t) \hat{\bfX}_t^\vareps + (\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \mathbb{E}[\bfX_1 \ | \ \bfX_t = \hat{\bfX}_t^\vareps] + (\dot{\gamma}_t - \gamma_t \dot{\alpha}_t/\alpha_t -  \vareps_t^2 /(2\gamma_t)) \hat{\bfZ}(t, \hat{\bfX}_t^\vareps)  + \vareps_t \rmd \bfB_t , 
\end{equation}
with $\hat{\bfX}_0^\vareps \sim \pi_0$ and 
where $\hat{\bfZ}(t, x)$ is an approximation of $\mathbb{E}[ \bfZ | \bfX_t = x]$. We have the following result. 

\begin{proposition}{Optimality and stochastic interpolant}{choice_eps}
Denote $\Pbb^\vareps$, respectively $\hat{\Pbb}^\vareps$ the path measures associated with $(\bfX_t^\vareps)_{t \in [0,1]}$ and $(\hat{\bfX}_t)_{t \in [0,1]}$ respectively. 
Consider $\ell(\vareps) = \mathrm{KL}( \Pbb^\vareps | \hat{\Pbb}^\vareps)$. Let $\vareps^\star = \argmin_\vareps \ell(\vareps)$. Then we have 
\begin{equation}
    (\vareps^\star_t)^2 = 2 \gamma_t \dot{\gamma}_t - 2 \gamma_t^2 \dot{\alpha}_t/\alpha_t .
\end{equation}
In particular, if $\alpha_t = t$, $\beta_t = 1-t$ and $\gamma_t = \sigma_0 \sqrt{t (1-t)}$, then $\vareps_t = \sqrt{2} \sigma_0$. The value $(\vareps_t^\star)_{t \in [0,1]}$ corresponds to Markovian projection when it is well defined. 
\end{proposition}

\begin{proof}
We have that for any 
\begin{equation}
 \KL(\Pbb^\vareps | \hat{\Pbb}^\vareps) = \int_0^1 \frac{1}{\vareps_t^2}(\dot{\gamma}_t - \gamma_t \dot{\alpha}_t/\alpha_t -  \vareps_t^2 /(2\gamma_t))^2 \mathbb{E}[\Delta_t] \rmd t,
\end{equation}
where $\Delta_t = \| \hat{\bfZ}(t, \bfX_t^\vareps) - \mathbb{E}[\bfZ \ | \ \bfX_t = \bfX_t^\vareps]\|^2$ and the expectation is w.r.t.~$\Pbb^\vareps$. We have that  $\KL(\Pbb^\vareps | \hat{\Pbb}^\vareps) =0 $ if $\vareps = \vareps^\star$, which concludes the proof.
\end{proof}

\Cref{prop:choice_eps} is related to \cite[Section 3.4]{chen2024probabilistic}. Therein it is noticed that, in the case of Augmented Bridge matching \citep{debortoli2023augmented}, the choice of $\vareps_t$ does not affect the joint distribution of $(\bfX_0^\vareps, \bfX_1^\vareps)$. The authors then select $(\vareps_t)$ so as to minimise an approximation error. They show that, in that case, they recover the F\"ollmer process.


We now show that \Cref{prop:choice_eps} can be further strengthened to establish that $\vareps^\star$ is also the optimal value if we interpolate between $\pi_s$ and $\pi_1$, or $\pi_0$ and $\pi_s$, for any $s \in [0,1]$ and $\pi_s$ the distribution of $\bfX_s$. Consider in this context for any $s, t \in [0,1]$ with $t \geq s$, $\gamma_t / \gamma_s \geq \alpha_t \geq \alpha_s$ the following interpolation model.
\begin{equation}
\label{eq:interpolation_with_s}
    \bfX_t = (\alpha_t/\alpha_s) \bfX_s + (\beta_t - \alpha_t \beta_s /\alpha_s) \bfX_1 + \sqrt{\gamma^2_t - \alpha_t^2 \gamma^2_s / \alpha_s^2} \bfZ ,
\end{equation}
where $\bfX_s \sim \pi_s$, $\bfX_1 \sim \pi_1$ and $\bfZ \sim \gN(0, \Id)$. Assume that $\alpha_t = 1-t$, $\beta_t = t$ and $\gamma_t = \sigma_0 \sqrt{t (1-t)}$ for any $t \in [0,1]$ then \eqref{eq:interpolation_with_s} corresponds to the Brownian bridge associated with $(\sigma_0 \bfB_t)_{t \in [0,1]}$ with endpoints $\bfX_s$ at time $s$ and $\bfX_1$ at time $1$.
We have the following proposition. 
\begin{proposition}{Stochastic interpolant with intermediate time points}{interpolation_property_intermediary_appendix}
Define $(\bfX_{t,s}^\vareps)_{t\in [s,1]}$ given by 
\begin{align}
\label{eq:stochastic_flow_rewrite_appendix}
    \rmd \bfX_{t,s}^\vareps &= (\dot{\alpha}_t/\alpha_t) \bfX_{t,s}^\vareps + \mathbb{E}\left[(\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \bfX_1 \ | \ \bfX_t = \bfX_{t,s}^\vareps\right] \\
    & \qquad + \mathbb{E}\left[(\dot{\gamma}_{t,s} - \gamma_{t,s} \dot{\alpha}_t/\alpha_t -  \vareps_{t,s}^2 /(2\gamma_{t,s})) \bfZ \ | \ \bfX_t = \bfX_{t,s}^\vareps\right] + \vareps_{t,s} \rmd \bfB_t , \quad \bfX_{s,s}^\vareps \sim \pi_s ,
\end{align}
with $\gamma_{t,s} = \sqrt{\gamma_t^2 - \alpha_t^2 \gamma_s^2 / \alpha_s^2}$. Then for any $t  \in [s,1]$, $\bfX_{t,s}^\vareps$ and $\bfX_t$ defined by \eqref{eq:interpolation_with_s} have the same distribution.
\end{proposition}

\begin{proof}
We let $s \in [0,1]$ and $\bfX_1, \bfX_s \in \rset^d$. From \eqref{eq:interpolation_with_s}, we have directly that for any $t \in [s,1]$
\begin{equation}
    \rmd \bfX_t = [(\dot{\alpha}_t / \alpha_s) \bfX_s + (\dot{\beta}_t - \dot{\alpha}_t \beta_s / \alpha_s) \bfX_1 + \dot{\gamma}_{t,s} \bfZ] \rmd t ,
\end{equation}
where $\bfZ \sim \gN(0, \Id)$. In addition, rearranging \eqref{eq:interpolation_with_s}, we also have that 
\begin{equation}
    \bfX_s = (\alpha_s / \alpha_t) \bfX_t - (\alpha_s \beta_t / \alpha_t - \beta_s) \bfX_1 - \gamma_{t,s} (\alpha_s / \alpha_t) \bfZ .  
\end{equation}
Hence, by combining these two expressions, we get that 
\begin{equation}
    \rmd \bfX_t = [(\dot{\alpha}_t / \alpha_t) \bfX_t + (\dot{\beta}_t - \dot{\alpha}_t \beta_t / \alpha_s) \bfX_1 + (\dot{\gamma}_{t,s} - \gamma_{t,s}(\dot{\alpha}_t / \alpha_t)) \bfZ] \rmd t .
\end{equation}
It follows that $(\bfX_{t,s})_{t \in [s,1]}$ given by 
\begin{align}
\label{eq:stochastic_flow_determistic_rewrite_appendix}
    \rmd \bfX_{t,s} &= (\dot{\alpha}_t/\alpha_t) \bfX_{t,s} + (\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \mathbb{E}[ \bfX_1 \ | \ \bfX_t = \bfX_{t,s}] \\
    & \qquad + (\dot{\gamma}_{t,s} - \gamma_{t,s} \dot{\alpha}_t/\alpha_t) \mathbb{E}[ \bfZ \ | \ \bfX_t = \bfX_{t,s}]  , \quad \bfX_{s,s} \sim \pi_s ,
\end{align}
is such that for any $t\in [s,1]$ the same distribution as $\bfX_t$ defined by \eqref{eq:interpolation_with_s}. Then, we conclude similarly to \citep[Theorem 2.8, Corollary 2.10]{albergo2023stochastic}.
\end{proof}

We now consider the following approximate version of \eqref{eq:stochastic_flow_rewrite_appendix}
\begin{align}
    \rmd \hat{\bfX}_{t,s}^\vareps &= (\dot{\alpha}_t/\alpha_t) \hat{\bfX}_{t,s}^\vareps + (\dot{\beta}_t - \beta_t \dot{\alpha}_t / \alpha_t) \mathbb{E}\left[\bfX_1 \ | \ \bfX_t = \hat{\bfX}_{t,s}^\vareps\right] \\
    &\qquad + (\dot{\gamma}_{t,s} - \gamma_{t,s} \dot{\alpha}_t/\alpha_t -  \vareps_{t,s}^2 /(2\gamma_{t,s})) \hat{\bfZ}(t, \bfX_{t,s}^\vareps) + \vareps_{t,s} \rmd \bfB_t , \quad \bfX_{s,s}^\vareps \sim \pi_s ,
\label{eq:stochastic_flow_rewrite_appendix_approximate}
\end{align}
Similarly to \Cref{prop:choice_eps} we consider the best choice of $\vareps$ to minimise the interpolation cost. 
\begin{proposition}{Optimality and stochastic interpolant}{choice_eps_interpolation}
Let $s \in [0,1]$. Denote $\Pbb^\vareps$, respectively $\hat{\Pbb}^\vareps$, the path measure associated with $(\bfX_{t,s}^\vareps)_{t \in [0,1]}$, respectively $(\hat{\bfX}_{t,s})_{t \in [0,1]}$. 
Consider $\ell(\vareps) = \mathrm{KL}( \Pbb^\vareps | \hat{\Pbb}^\vareps)$. Let $\vareps^\star = \argmin_\vareps \ell(\vareps)$. Then we have 
\begin{equation}
    (\vareps^\star_t)^2 = 2 \gamma_t \dot{\gamma}_t - 2 \gamma_t^2 \dot{\alpha}_t/\alpha_t .
\end{equation}
In particular, $\vareps^\star$ does not depend on $s \in [0,1]$ and for every $s_1, s_2 \in [0,1]$ with $s_1 \leq s_2$, we have that $(\bfX_{t,s_1}^\vareps)_{t \in [s_2, 1]}$ and $(\bfX_{t,s_2}^\vareps)_{t \in [s_2, 1]}$ coincide.
\end{proposition}

\begin{proof}
Similarly to \Cref{prop:choice_eps}, we get first that 
for any $s, t \in [0,1]$ with $s \leq t$
\begin{equation}
\label{eq:uno_interp_appendix}
    \vareps_{t,s}^\star = 2 \gamma_{t,s} \dot{\gamma}_{t,s} - 2 \gamma_{t,s}^2 \dot{\alpha}_t/\alpha_t .
\end{equation}
Second, we have that for any $s, t \in [0,1]$ with $s \leq t$
\begin{equation}
\label{eq:duo_interp_appendix}
    2 \dot{\gamma}_{t,s} \gamma_{t,s} = \dot{\gamma}^2_{t,s}  = 2 \dot{\gamma}_t \gamma_t - 2 \dot{\alpha}_t \alpha_t \gamma_s^2 / \alpha_s^2 . 
\end{equation}
Third, we have that 
\begin{equation}
\label{eq:tertio_interp_appendix}
    \gamma_{t,s}^2 \dot{\alpha}_t / \alpha_t = \gamma_t^2 \dot{\alpha}_t / \alpha_t - \dot{\alpha}_t \alpha_t \gamma_s^2 / \alpha_s^2 . 
\end{equation}
Combining \eqref{eq:uno_interp_appendix}, \eqref{eq:duo_interp_appendix} and \eqref{eq:tertio_interp_appendix}, we can conclude. 
\end{proof}

\section{Theoretical results}
\label{sec:theoretical_results_appendix}

In this section, we prove the main theoretical results of the paper. In \Cref{sec:non_parametric_convergence},
we first prove the convergence of the $\alpha$-IMF sequence, i.e.~we prove \Cref{thm:convergence}. 
Second, we show that the non-parametric updates \eqref{eq:update_non_parametric} correspond to the $\alpha$-IMF sequence, i.e.~we prove \Cref{prop:shadow_sequence}. In \Cref{sec:param_to_non_parametric}, we link the non-parametric updates to the parametric updates. 

\subsection{Non-parametric sequence and convergence}
\label{sec:non_parametric_convergence}

Let $\Qbb \in \mathcal{P}(\mathcal{C})$ be associated with $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$, where $(\bfB_t)_{t \in [0,1]}$ is a $d$-dimensional Brownian motion and $\vareps > 0$.
In this section, we abuse notation and denote $\mathcal{P}(\mathcal{C})$ the set of \emph{path measures} which are not necessarily \emph{probability} path measures. In particular, we will consider $\Qbb \in \mathcal{P}(\mathcal{C})$  associated with $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ with $\Qbb_0 = \Leb$. In that case, the Kullback--Leibler divergence is still well-defined and we refer to \citep{leonard2014survey} for more details. 
We recall that we have defined $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ for any $n \in \nset$ and $\alpha \in (0,1]$ by
\begin{equation}
\label{eq:discretisation_flow_path_measure_repeat}
        \Pbb^n = \projM(\hat{\Pbb}^n) , \qquad \hat{\Pbb}^{n+1} = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\projM(\hat{\Pbb}^n)) . 
\end{equation}
In addition, for any $n \in \nset$, $t \in [0,1)$ and $x \in \rset^d$ we have defined
\begin{equation}
\label{eq:update_non_parametric_appendix_2}
    v^{n+1}_t(x) = v^n_t(x) - \delta_n \nabla_{\mu^n} \Lnonparam_t(v^n_t, \Pbb_{v^n})(x) ,
\end{equation}
 where
\begin{align}\label{eq:locallosst}
    \Lnonparam_t(v_t, \Pbb)&=\frac{1}{2} \int_{(\rset^d)^3} \Big\| v_t(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \Pbb_{0,1}(x_0,x_1) \rmd \Qbb_{t|0,1}(x_t | x_0, x_1) \\
    &=\frac{1}{2}  \int_{(\rset^d)^3} \Big\| v_t(x_t) - \frac{x_1 - x_t}{1 - t} \Big\|^2 \rmd \projsimpleR(\Pbb)_{1,t}.
\end{align}
We define $(\Pbb_{v^n})_{n \in \nset}$ associated with \eqref{eq:update_non_parametric_appendix}, where for any suitable vector field $v$, $\Pbb_v$ is associated with 
\begin{equation}
    \rmd \bfX_t = v_t(\bfX_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t ,
\end{equation}
where $(\bfB_t)_{t \in [0,1]}$ is a $d$-dimensional Brownian motion.

In order to rigorously prove \Cref{prop:shadow_sequence_appendix} detailed further, we introduce $\calP_2(\calC)$, such that $\Pbb \in \calP_2(\calC)$ if $\Pbb \in \calP(\calC)$ and for 
\begin{equation}
    \int_{(\rset^d)^2} \{ \| x_0 \|^2 + \|x_1 \|^2 \} \rmd \Pbb_{0,1}(x_0, x_1) < +\infty .
\end{equation}
Note that if $\Pbb \in \calP_2(\calC)$ then we have that for any $t \in [0,1]$
\begin{equation}
    \int_{\rset^d} \| x_t \|^2 \rmd \projsimpleR(\Pbb)_t < +\infty . 
\end{equation}
In addition, we recall that $\phi \in \mathrm{L}^2(\mu)$ for $\mu \in \calP(\rset^d)$ if $\phi: \ \rset^d \to \rset^d$ and 
\begin{equation}
   \int_{\rset^d} \| \phi(x) \|^2 \rmd \mu(x) < +\infty .
\end{equation}
Finally, we define 
\begin{equation}
    \mathsf{A}_2 = \ensembleLigne{(\phi, \Pbb)}{\Pbb \in \calP_2(\calC), \ \phi \in \mathrm{L}^2(\Pbb)} .  
\end{equation}
Then for any $t \in [0,1)$, we define $\Lnonparam_t : \ \mathsf{A}_2 \to \rset$ given for any $(v, \Pbb) \in \mathsf{A}_2$ by \eqref{eq:locallosst}.

\begin{proposition}{Non-parametric updates are $\alpha$-IMF}{shadow_sequence_appendix}
Let $\alpha \in (0,1]$, $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ as in \eqref{eq:discretisation_flow_path_measure}, $\delta_n =  \alpha$ and $\mu^n = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\Pbb^n)$. Assume that for any $n \in \nset$, $\Pbb_{v^n}$ is well-defined. Then, for any $n \in \nset$,  $\Pbb_{v^n} = \Pbb^n$.
\end{proposition}
\begin{proof}
First, we have that for any $t \in [0,1)$, $v, \Pbb \in \mathsf{A}_2$ and $\phi \in \mathrm{L}^2(\Pbb_t)$ we have 
 \begin{align}
     \Lnonparam_t(v_t + \vareps \phi,\Pbb) &= \Lnonparam_t(v_t,\Pbb) + \vareps \int_{(\rset^d)^3} \langle \phi(x_t), v_t(x_t) - \tfrac{x_1 - x_1}{1-t}\rangle \rmd \Pbb_{0,1}(x_0,x_1) \rmd \Qbb_{t|0,1}(x_t|x_0, x_1) \\
     &  \qquad + (\vareps^2/2) \int_{(\rset^d)^3} \| \phi(x_t) \|^2 \rmd \Pbb_{0,1}(x_0,x_1) \rmd \Qbb_{t|0,1}(x_t|x_0, x_1) \\ 
     &= \Lnonparam_t(v_t,\Pbb) + \vareps \int_{(\rset^d)^2} \langle \phi(x_t), v_t(x_t) \\
     & \quad - \Bigl(\int_{\rset^d} x_1 \rmd \projsimpleR(\Pbb)_{1|t}(x_1 | x_t) - x_t\Bigr)/(1-t) \rangle \rmd \projsimpleR(\Pbb)_t(x_t) \\
     &  \qquad + (\vareps^2/2) \int_{\rset^d} \| \phi(x_t) \|^2 \projsimpleR(\Pbb)_t(x_t) . 
 \end{align}
Hence, we have that 
\begin{equation}
\label{eq:functional_derivative_loss}
    \nabla_{\mu} \Lnonparam_t(v_t,\Pbb_v)(x_t) = (v_t(x_t) - \left(\mathbb{E}_{\projsimpleR(\Pbb)}[ \bfX_1 \ | \ \bfX_t = x_t] - x_t\right)/(1-t)) (\rmd \projsimpleR(\Pbb_v)_t / \rmd \mu_t)(x_t) . 
\end{equation}
Assume that for some $n \in \nset$ we have that for any $t \in [0,1)$ and $x_t \in \rset^d$, we have  $v^k_t(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^k}[\bfX_1 \ | \ \bfX_t = x_t] - x_t\right) / (1-t)$. We are going to show that for any $t \in [0,1)$ and $x_t \in \rset^d$, we have  $v^{n+1}_t(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^{n+1}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t\right) / (1-t)$. For any $t \in [0,1)$ and $x_t \in \rset^d$, we denote 
\begin{equation}
    \bar{\delta}^n_t(x_t) = \delta_n (\rmd \projsimpleR(\Pbb^n)_t / \rmd \mu_t^n)(x_t) . 
\end{equation}
Since we have that $\delta_n = \alpha$ and $\mu^n = (1-\alpha) \hat{\Pbb}^n + \alpha \projsimpleR(\Pbb^n)$, we obtain for any $t \in [0,1]$ and $x_t \in \rset^d$ 
\begin{equation}
\label{eq:delta_bar_simplif}
    \bar{\delta}^n_t(x_t) = \alpha (\rmd \projsimpleR(\Pbb^n)_t / \rmd ( (1-\alpha) \hat{\Pbb}^n_t + \alpha \projsimpleR(\Pbb^n)_t)(x_t),
\end{equation}
so that
\begin{equation}
\label{eq:one_minus_delta_bar_simplif}
    1 - \bar{\delta}^n_t(x_t) = (1-\alpha) (\rmd \hat{\Pbb}^n_t /  \rmd ( (1-\alpha) \hat{\Pbb}^n_t + \alpha \projsimpleR(\Pbb^n)_t)(x_t) .
\end{equation}
Therefore, combining \eqref{eq:update_non_parametric} with \eqref{eq:delta_bar_simplif}, \eqref{eq:one_minus_delta_bar_simplif}, \eqref{eq:functional_derivative_loss}, we get that for any $t \in [0,1)$ and $x_t \in \rset^d$
\begin{align}
    v_t^{n+1}(x_t) & =(1 - \bar{\delta}_t^n(x_t))v_t^{n}(x_t) \\
    & \qquad + \bar{\delta}^n_t(x_t) \left(\mathbb{E}_{\projsimpleR(\Pbb^n)}[\bfX_1 \ | \ \bfX_t=x_t] - x_t\right)/(1-t) \\
    &=(1 - \bar{\delta}_t^n(x_t)) \left(\mathbb{E}_{\hat{\Pbb}^n}[\bfX_1 \ | \ \bfX_t=x_t] - x_t\right)/(1-t) \\
    & \qquad + \bar{\delta}^n_t(x_t) \left(\mathbb{E}_{\projsimpleR(\Pbb^n)}[\bfX_1 \ | \ \bfX_t=x_t] - x_t\right)/(1-t) \\
    &=  (1 - \bar{\delta}_t^n(x_t)) \left(\int_{\rset^d} x_1 \rmd \hat{\Pbb}^n_{1|t}(x_1 | x_t) - x_t\right) / (1-t) \\
    & \qquad + \bar{\delta}^n_t(x_t) \left(\int_{\rset^d} x_1 \rmd \projsimpleR(\Pbb^n)_{1|t}(x_1 | x_t) - x_t\right) / (1-t) \\
    &=  \int_{\rset^d} (x_1 - x_t) / (1-t) \rmd [ (1 - \bar{\delta}_t^n(x_t)) \hat{\Pbb}^n_{1|t}+ \bar{\delta}_t^n(x_t) \projsimpleR(\Pbb^n)_{1|t}] (x_1 | x_t) \\
    &=  \int_{\rset^d} x_1 \rmd [ (1-\alpha) \hat{\Pbb}^n+ \alpha \projsimpleR(\Pbb^n)]_{1|t} (x_1 | x_t) . 
\end{align}
Hence, we have that for any $t \in [0,1)$ and $x_t \in \rset^d$, $v_t^{n+1}(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^{n+1}}[\bfX_1  \ | \ \bfX_t = x_t] - x_t\right)/(1-t)$. Since, for any $t \in [0,1)$ and $x_t \in \rset^d$, $v_t^0(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^{0}}[\bfX_1  \ | \ \bfX_t = x_t] - x_t\right / (1-t)$ by definition, we get that for any $n \in \nset$, $t \in [0,1)$ and $x_t \in \rset^d$,  $v_t^{n}(x_t) = \left(\mathbb{E}_{\hat{\Pbb}^{n}}[\bfX_1  \ | \ \bfX_t = x_t] - x_t\right) / (1-t)$. Using, \Cref{def:markovian_proj}, we get that $\Pbb_{v^n} = \projM(\hat{\Pbb}^n)$, which concludes the proof. 
\end{proof}

Before stating our convergence theorem, we show the following result which is a direct consequence of ~\citep[Theorem 2.12]{leonard2014survey} and \citep[Theorem 2.14]{leonard2014reciprocal}. We recall that the differential entropy of a probability measure $\pi$ is given by 
\begin{equation}
     \mathrm{H}(\pi) =- \int_{\rset^d} \log((\rmd \pi / \rmd \mathrm{Leb})(x)) \rmd \pi(x) ,
\end{equation}
if $\pi$ admits a density with respect to the Lebesgue measure and $+\infty$ otherwise. 

\begin{lemma}{Characterisation of Schr\"odinger Bridge}{}
\label{lemma:sb_characterisation}
Recall that $\Qbb$ is associated with $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ and assume that $\Qbb_0 = \mathrm{Leb}$. Let $\pi_0, \pi_1 \in \calP(\rset^d)$ such that 
\begin{equation}
     \int_{\rset^d} \| x \|^2 \rmd \pi_i(x) < +\infty  , \qquad \mathrm{H}(\pi_i) < +\infty ,
\end{equation}
for $i \in \{0, 1\}$. Let $\Pbb^\star$ such that $\Pbb^\star$ is Markov, $\Pbb^\star \in \mathcal{R}(\Qbb)$, $\Pbb^\star_0 = \pi_0$ and $\Pbb^\star_1 = \pi_1$. Then $\Pbb^\star$ is the Schr\"odinger Bridge, i.e.~the unique solution to  \eqref{eq:dynamic_ot}.
\end{lemma}

\begin{proof}
First, we have that $\Qbb_{0,1}$ is equivalent to $\Leb \otimes \Leb$. Indeed, we have that for any $x_0, x_1 \in \rset^d$
\begin{equation}
    (\rmd \Qbb_{0,1} / \rmd (\Leb \otimes \Leb))(x_0,x_1) =(2 \uppi \vareps)^{-d/2} \exp[-\| x_0 - x_1 \|^2 / (2 \vareps) ] . 
\end{equation}
Similarly, we have that for any $t \in (0,1)$ and $x_t \in \rset^d$, $\Qbb_{0,1|t}(\cdot|x_t)$ is equivalent to $\Leb \otimes \Leb$. Indeed, we have that for any $t \in (0,1)$ and $x_0, x_t, x_1 \in \rset^d$
\begin{align}
    (\rmd \Qbb_{0,1|t}(\cdot|x_t) / \rmd (\Leb \otimes \Leb))(x_0,x_1) &= (2 \uppi \vareps t)^{-d/2}  \exp[-\| x_0 - x_t \|^2 / (2 \vareps t)] \\
    & \qquad \times (2 \uppi \vareps(1-t))^{-d/2} \exp[- \| x_t -  x_1 \|^2 / (2 \vareps(1-t)) ] . 
\end{align}
Hence, for any $t \in (0,1)$ and $x_t \in \rset^d$, $\Qbb_{0,1|t}(\cdot|x_t)$ is equivalent to $\Qbb_{0,1}$. Since $\Pbb^\star$ is Markov and $\Pbb^\star \in \mathcal{R}(\Qbb)$ we get that there exist $\varphi^\circ_0$ and $\varphi^\star_1$ which are Lebesgue measurable such that for any $\omega \in \calC$ we have that 
\begin{equation}
\label{eq:existence_potentials}
    (\rmd \Pbb^\star / \rmd \Qbb)(\omega) = \varphi_0^\circ(\omega_0) \varphi_1^\star(\omega_1) .
\end{equation}    
Second we verify that the conditions (i)-(vii) of \cite[Theorem 2.12]{leonard2014survey} are satisfied. First, $\Qbb$ is Markov and hence (i) is satisfied.
Then, (ii) is satisfied since for any $t \in (0,1)$ and $x_t \in \rset^d$, $\Qbb_{0,1|t}(\cdot|x_t)$ is equivalent to $\Qbb_{0,1}$. 
We have that $\Qbb_0 = \Qbb_1 = \Leb$ and (iii) is satisfied. We have that for any $x_0, x_1 \in \rset^d$
\begin{align}
    (\rmd \Qbb_{0,1} / \rmd (\Leb \otimes \Leb))(x_0,x_1) &= (2 \uppi \vareps)^{-d/2} \exp[-\| x_0 - x_1 \|^2 / (2 \vareps) ]\\
    &\geq (2 \uppi \vareps)^{-d/2} \exp[-\| x_0 \|^2/\vareps -\| x_1 \|^2/\vareps]. 
\end{align}
Hence, (iv) is satisfied and we let $A: \ \rset^d \to \rset_+$ be given for any $x \in \rset^d$ by $A(x) = \| x \|^2 / \vareps$. 
In addition, we have that for any $x_0,x_1 \in \rset^d$
\begin{equation}
     \int_{(\rset^d)^2} \exp[-\| x_0 \|^2 / \vareps - \| x_1 \|^2 / \vareps] \rmd \Qbb_{0,1}(x_0,x_1) < +\infty . 
\end{equation}
Hence, (v) is satisfied and we let $B: \ \rset^d \to \rset_+$ given for any $x \in \rset^d$ by $B(x) = \| x \|^2 / \vareps$. By assumption (vi) and (vii) are satisfied. We conclude the proof upon using \cite[Theorem 2.12-(b)]{leonard2014survey} and \eqref{eq:existence_potentials}. 
\end{proof}

We are now ready to state our main convergence result. 

\begin{proposition}{Convergence of $\alpha$-IMF}{convergence_appendix}
Let $\alpha \in (0, 1]$ and $(\Pbb^n, \hat{\Pbb}^n)_{n \in \nset}$ defined by \eqref{eq:discretisation_flow_path_measure}. 
    Under mild assumptions, we have that 
    $\lim_{n \to +\infty} \Pbb^n = \Pbb^\star$, where $\Pbb^\star$ is the solution of the Schr\"odinger Bridge problem \eqref{eq:dynamic_ot}.
\end{proposition}

\begin{proof}
Using the convexity of the Kullback--Leibler divergence with respect to its first argument (see e.g.~\citep{dupuis2011weak}), the data processing inequality (see e.g.~\cite[Lemma 9.4.5]{ambrosio200gradient}), the fact that the Schr\"odinger Bridge is Markov and in the reciprocal class of $\Qbb$ (see e.g.~\citep[Theorem 2.12]{leonard2014survey} and \citep[Theorem 3.2]{leonard2014reciprocal}), and the Pythagorean theorem for the Markovian projection \citep[Lemma 6]{shi2023DSBM}, we have that for any $n \in \nset$
\begin{align}
    \KL(\hat{\Pbb}^{n+1} | \Pbb^\star) &= \KL((1-\alpha) \hat{\Pbb}^{n} + \alpha \projsimpleR(\projM(\hat{\Pbb}^n)) | \Pbb^\star) \\
    &\leq (1-\alpha) \KL( \hat{\Pbb}^{n} | \Pbb^\star) + \alpha \KL( \projsimpleR(\projM(\hat{\Pbb}^n)) | \Pbb^\star) \\
    &\leq (1-\alpha) \KL( \hat{\Pbb}^{n} | \Pbb^\star) + \alpha \KL( \projM(\hat{\Pbb}^n)_{0,1} | \Pbb^\star_{0,1}) \\
    &\leq (1-\alpha) \KL( \hat{\Pbb}^{n} | \Pbb^\star) + \alpha \KL( \projM(\hat{\Pbb}^n) | \Pbb^\star) \\
    &\leq (1-\alpha) \KL( \hat{\Pbb}^{n} | \Pbb^\star) + \alpha \KL( \hat{\Pbb}^n | \Pbb^\star) - \alpha \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) .
    \label{eq:kl_inequality_phat}
\end{align}
Therefore, we get that 
\begin{equation}
    \alpha \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) \leq \KL( \hat{\Pbb}^{n} | \Pbb^\star) - \KL(\hat{\Pbb}^{n+1} | \Pbb^\star) .
\end{equation}
Hence, it follows that
\begin{equation}
 \sum_{n \in \nset} \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) \leq 2 \KL(\hat{\Pbb}^0 | \Pbb^\star) < +\infty . 
\end{equation}
So we obtain $\lim_{n \to +\infty} \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) = 0$. In addition, using \eqref{eq:kl_inequality_phat} we have that $\KL(\hat{\Pbb}^n | \Pbb^\star) \leq \KL(\hat{\Pbb}^0 | \Pbb^\star) < +\infty$ for all $n \in \nset$. Using \cite[Lemma 6]{shi2023DSBM}, we also get that $\KL(\projM(\hat{\Pbb}^n) | \Pbb^\star) \leq \KL(\hat{\Pbb}^0 | \Pbb^\star) < +\infty$ for any $n \in \nset$. Hence both the sequences $(\hat{\Pbb}^n)_{n \in \nset}$ and $(\Pbb^n)_{n \in \nset} = (\projM(\hat{\Pbb}^n))_{n \in \nset}$ are relatively compact in $\mathcal{P}(\mathcal{C})$. 
Let $\bar{\Pbb} \in \calP(\calC)$ be an adherent point to the sequence $(\hat{\Pbb}^n)_{n \in \nset}$ and $\varphi: \ \nset \to \nset$ increasing such that $\lim_{n \to +\infty} \Pbb^{\varphi(n)} = \bar{\Pbb}$. Similarly, let $\phi: \ \nset \to \nset$ increasing such that $(\phi(n))_{n \in \nset}$ is a subsequence of $(\varphi(n))_{n \in \nset}$ such that $\lim_{n \to +\infty} \projM(\hat{\Pbb}^n) = \bar{\Pbb}'$, with $\bar{\Pbb}'$ and adherent point to the sequence $(\projM(\hat{\Pbb}^n))_{n \in \nset}$. Using the lower semi-continuity of the Kullback-Leibler divergence in both arguments \citep{dupuis2011weak}, we get that 
\begin{equation}
 \KL(\bar{\Pbb} | \bar{\Pbb}') \leq \liminf_{n \to + \infty} \KL(\hat{\Pbb}^{\phi(n)} | \projM(\hat{\Pbb}^{\phi(n)})) = 0 . 
\end{equation}
Since the set of Markov measures and the set of reciprocal measures w.r.t.~$\Qbb$ are both closed, we have that $\bar{\Pbb}$ is Markov and in the reciprocal class of $\Qbb$. Since we also have that $\bar{\Pbb}_0 = \pi_0$ and $\bar{\Pbb}_1 = \pi_1$, we get that $\bar{\Pbb} = \Pbb^\star$ using \Cref{lemma:sb_characterisation}. Since every adherent point of $(\hat{\Pbb}^n)_{n \in \nset}$ is $\Pbb^\star$, we have that $\lim_{n \to +\infty} \hat{\Pbb}^n = \Pbb^\star$. Similarly, using that $\lim_{n \to +\infty} \KL( \hat{\Pbb}^n | \projM(\hat{\Pbb}^n)) = 0$ and again the lower semi-continuity of the Kullback--Leibler divergence in both arguments, we get that every adherent point of $(\projM(\hat{\Pbb}^n))_{n \in \nset}$ is $\Pbb^\star$. Hence, we have that $\lim_{n \to +\infty} \Pbb^n = \Pbb^\star$, which concludes the proof. 
\end{proof}

\subsection{From parametric to non-parametric.} 
\label{sec:param_to_non_parametric}

In this section, we show that the parametric updates considered in \eqref{eq:update_alpha_imf} are a preconditioned version of the non-parametric updates considered in \eqref{eq:update_non_parametric}. We first recall the non-parametric loss 
\begin{equation}
\label{eq:loss_function_appendix}
    \Lnonparam(v, \Pbb) = \int_0^1 \Lnonparam_t(v_t, \Pbb) \rmd t = \frac{1}{2} \int_0^1 \int_{(\rset^d)^2}\Big\|  v_t(x_t) - \frac{x_1 - x_t}{1 - t} \Big\| ^2 \rmd \projsimpleR(\Pbb)_{t,1}(x_t,x_1) \rmd t 
\end{equation}
and the parametric loss
\begin{equation}
\label{eq:update_alpha_imf_appendix}
     \Lparam(\theta, \Pbb) = \frac{1}{2} \int_0^1 \int_{\rset^d \times \rset^d} \Big\|  v_t^\theta(x_t) - \frac{x_1 - x_t}{1 - t} \Big\| ^2 \rmd \projR{\Qbb}(\Pbb)_{t,1}(x_t, x_1) \rmd t.
\end{equation}
The non-parametric sequence $(v^n)_{n \in \nset}$ is given by \eqref{eq:update_non_parametric_appendix}, i.e.~we have for any $n \in \nset$, $t  \in [0,1]$ and $x \in \rset^d$
\begin{equation}
\label{eq:update_non_parametric_appendix}
    v^{n+1}_t(x) = v^n_t(x) - \delta_n \nabla_{\mu^n} \Lnonparam_t(v^n_t, \Pbb_{v^n})(x).
\end{equation}
Similarly the sequence of parametric updates is given for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$ by 
\begin{equation}
    \theta_{n+1} = \theta_n - \alpha \nabla_\theta \Lparam(\theta_n, \Pbb^{\bar{\theta}_n}) . 
\end{equation}
We recall that $\Pbb^{\bar{\theta}_n}$ is a stop gradient version of $\Pbb_{v^{\bar{\theta}_n}}$. 
We are going to show that on average the parametric algorithm yields a direction of descent for the non-parametric loss. We assume that the set of parameters $\Theta$ is an open subset of $\rset^p$ for some $p \in \nset$. For any $t \in [0,1]$ and $x \in \rset^d$ we assume that $\theta \mapsto v^\theta_t(x)$ is twice continuously differentiable and denote $\mathrm{D}_\theta v^\theta_t(x) \in \rset^{d \times p}$ its Jacobian and $\mathrm{D}_\theta^2 v^\theta_t(x)$ its Hessian.  
For any $\theta \in \Theta$, we denote 
\begin{equation}
    h_\theta = \nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}}) .
\end{equation}
We show the following result. 

\begin{proposition}{Velocity field parametric update}{}
Assume that there exists $C > 0$ such that for any $\theta \in \Theta$ and $x \in \rset^d$
\begin{equation}
\label{eq:integrability_condition}
    \int_0^1   (1-s) \mathrm{D}_\theta^2 v_s^{\theta - \alpha s h_\theta}(x) (h_\theta, h_\theta) \rmd s < C ,
\end{equation}
where $h_\theta = \nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}})$. 
We have that for any $n \in \nset$, $t \in [0,1)$ and $x \in \rset^d$
\begin{align}
\label{eq:taylor_expansion}
   & v^{\theta_{n+1}}_t(x) = v^{\theta_n}_t(x) \\
   & \quad - \alpha \mathrm{D}_\theta v_t^{\theta_n}(x) \int_0^1 \int_{\rset^d} \mathrm{D}_\theta v_{s}^\theta(\tilde{x})^\top \nabla_{\mu^n} \Lnonparam_s(v^{\theta_n}, \Pbb_{v^{\theta_n}})(\tilde{x})  \rmd \mu^n_s(\tilde{x}) \rmd s+ o(\alpha) ,
\end{align}
where $\mu^{n} = (1- \alpha) \Pbb^n + \alpha \Pbb_{v^{\theta_n}}$.
\end{proposition}

\begin{proof}
%Let $t \in [0,1)$ and $x \in \rset^d$. We denote $v^\theta_{t}(x) : \ \Theta \to \rset^d$.
First, we have that for any $\mu \in \calP(\calC)$ 
\begin{equation}
    \nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}}) = \int_0^1 \int_{\rset^d} \rmD_\theta v_s^\theta(x_s)^\top (v_s^\theta(x_s) - (x_1 - x_s)/(1-s)) \rmd \projsimpleR(\Pbb^{\bar{\theta}})_{s, 1}(x_s, x_1) \rmd s.
\end{equation}
Therefore, using \eqref{eq:functional_derivative_loss}, we get that 
\begin{equation}
    \nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}}) = \int_0^1 \int_{\rset^d} \rmD_\theta v_s^\theta(x_s)^\top  \nabla_\mu \Lnonparam_s(v_s^\theta, \Pbb^{\bar{\theta}})(x_s) \rmd \mu_s(x_s) \rmd s .
\end{equation}
Let $\theta \in \Theta$ and denote $\theta' = \theta - \alpha \nabla_\theta \Lparam(\theta, \Pbb^{\bar{\theta}})$. 
Using a Taylor expansion, we get that for any $\theta \in \Theta$, we have that 
\begin{align}
    v_t^{\theta'}(x) &= v_t^\theta(x) - \alpha \rmD_\theta v_t^\theta(x) \int_0^1 \int_{\rset^d} \rmD_\theta v_s^\theta(x_s)^\top  \nabla_\mu \Lnonparam_s(v_s^\theta, \Pbb^{\bar{\theta}})(x_s) \rmd \mu_s(x_s) \rmd s \\
    & \qquad \qquad + \alpha^2  \int_0^1 (1-s)  \mathrm{D}_\theta^2 v_s^{\theta - \alpha s h_\theta}(x) (h_\theta, h_\theta) \rmd s.  
\end{align}
Since the functional gradient is not applied on the second coordinate, we can drop the stop gradient operator and therefore we have for any $\theta \in \Theta$ 
\begin{align}
    v_t^{\theta'}(x) &= v_t^\theta(x) - \alpha \rmD_\theta v_t^\theta(x) \int_0^1 \int_{\rset^d} \rmD_\theta v_s^\theta(x_s)^\top  \nabla_\mu \Lnonparam_s(v_s^\theta, \Pbb_{v^\theta})(x_s) \rmd \mu_s(x_s) \rmd s \\
    & \qquad \qquad + \alpha^2  \int_0^1 (1-s)  \mathrm{D}_\theta^2 v_s^{\theta - \alpha s h_\theta}(x) (h_\theta, h_\theta) \rmd s . 
\end{align}
Combining this result with \eqref{eq:integrability_condition}, we conclude the proof. 

\end{proof}

The corresponding update on the velocity field is given for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$ by 
\begin{equation}
   d^n_t(x) = -\alpha \mathrm{D}_\theta v_t^{\theta_n}(x) \int_0^1 \int_{\rset^d} \mathrm{D}_\theta v_{s}^\theta(\tilde{x})^\top \nabla_{\mu^n} \Lnonparam_s(v^{\theta_n}, \Pbb_{v^{\theta_n}})(\tilde{x})  \rmd \mu^n_s(\tilde{x}) + o(\alpha) . 
\end{equation}
We immediately have the following corollary.

\begin{proposition}{Parametric direction of descent}{}
For any $n \in \nset$, if
\begin{equation}
\int_0^1 \int_{\rset^d} \mathrm{D}_\theta v_{s}^\theta(\tilde{x})^\top \nabla_{\mu^n} \Lnonparam_s(v^{\theta_n}, \Pbb_{v^{\theta_n}})(\tilde{x})  \rmd \mu^n_s(\tilde{x}) \neq 0 ,
\end{equation}
then we have
\begin{equation}
 \lim_{\alpha \to 0} \int_0^1 \int_{\rset^d} \langle \nabla_{\mu^n} \Lnonparam_t(v^{\theta_n}, \Pbb_{v^{\theta_n}})(x), d_t^n(x) \rangle \rmd \mu^n_t(x) \leq 0 . 
\end{equation}
\end{proposition}

\section{Background material on DSBM and extensions}
\label{sec:background_dsbm}

In this section we recall some basics on Markovian and reciprocal projections in \Cref{sec:reciprocal_markov}. We explain the link between the concept of \emph{iterative refinement} and Schr\"odinger Bridges in \Cref{sec:lit_dsbm_appendix}. Then, we briefly present Diffusion Schr\"odinger Bridge Matching (DSBM) \citep{shi2023DSBM} in \Cref{sec:sec:dsbm_appendix} and propose some new extensions in \Cref{sec:reflection_projection_appendix}. 

\subsection{Markov and reciprocal projections in practice}
\label{sec:reciprocal_markov}

In this section, we recall the definition of the reciprocal and Markov projection. We provide more details on how these different projections can be performed and illustrate them on simple examples.

\paragraph{Markov projection.} First, we recall the definition of the Markovian projection.

\begin{definitionred}{Markov projection}{markovian_proj_appendix}
  Assume that $\Qbb$ is induced by $(\sqrt{\vareps} \bfB_t)_{t \in [0,1]}$ for $\vareps >0$. Then, when it is well-defined, for any $\Pbb \in \mathcal{R}(\Qbb)$, the \emph{Markovian projection} 
  $\Mbb = \projM(\Pbb) \in \calM$ is the path measure induced by the diffusion
  \begin{equation}
    \rmd \bfX^\star_t =  v_t^\star(\bfX^\star_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t, \qquad v_t^\star(x_t) =  \left(\mathbb{E}_{\Pbb_{1|t}}\left[\bfX_1 \ | \ \bfX_t = x_t\right] - x_t\right)/(1-t) , \qquad \bfX^\star_0 \sim \Pbb_0.
  \end{equation}  
\end{definitionred}

In \Cref{fig:original_coupling} and \Cref{fig:markov_projection}, we illustrate the effect of the Markovian projection, following the example of \citep{liu2022rectified}. We consider two distributions $\pi_0$ and $\pi_1$ such that 
\begin{equation}
    \pi_0 = \frac{1}{2}\mathcal{N}([-2,-2], \Id) + \frac{1}{2} \mathcal{N}([-2,2], \Id) , \quad \pi_1 = \frac{1}{2} \mathcal{N}([2,-2], \Id) +\frac{1}{2}\mathcal{N}([2,2], \Id) .
\end{equation}
In \Cref{fig:original_coupling}, we display samples from the distributions $\pi_0$ and $\pi_1$ as well as trajectories from the path measure $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. Practically, this means that we sample $\bfX_0 \sim \pi_0$ and $\bfX_1 \sim \pi_1$ independently and then consider a Brownian bridge between $\bfX_0$ and $\bfX_1$. The SDE associated with the Brownian bridge with scale $\vareps >0$ is given for any $t \in [0,1]$ by
\begin{equation}
\label{eq:brownian_bridge_appendix}
    \rmd \bfX_t = (\bfX_1 - \bfX_t)/ (1-t) \rmd t + \sqrt{\vareps} \rmd \bfB_t . 
\end{equation}
Note that the measure $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$ is in the reciprocal class, i.e.~$\Pbb \in \mathcal{R}(\Qbb)$. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.4\linewidth]{img/original_coupling.png}
    \caption{Samples from the original distributions $\pi_0$ (left) and $\pi_1$ (right) are shown in red, while sample paths from $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$ are shown in blue.}
    \label{fig:original_coupling}
\end{figure}

Next in \Cref{fig:markov_projection}, we display samples from the distributions $\pi_0$ and $\pi_1$ as well as trajectories from the path measure $\Pbb^\star = \projM(\Pbb)$. Note that in \Cref{fig:markov_projection}, contrary to \Cref{fig:original_coupling}, we observe less crossings between the trajectories. Indeed in the limit case where $\vareps \to 0$ the Markov measures $\Pbb^\star$ is an ODE with regular coefficients and therefore admits a unique solution for every starting point in the space so no crossing is possible. In particular, note that most of the trajectories starting from the upper-left Gaussian end at the upper-right Gaussian. Similarly, most of the trajectories starting from the lower-left Gaussian end at the lower-right Gaussian.

\begin{figure}[H]
    \centering
o    \includegraphics[width=.4\linewidth]{img/markov_projection.png}
    \caption{Samples from the original distributions are shown in red, while sample paths from $\Mbb = \projM(\Pbb)$ are shown in blue.}
    \label{fig:markov_projection}
\end{figure}

In practice, computing the Markov projection involves finding the optimal drift $v_t^\star$. This optimal drift is the minimizer of a regression problem, see \citep{shi2023DSBM} for more details. Hence, computing the Markovian projection requires training a neural network to define a vector field. 


\paragraph{Reciprocal projection.} First, we recall the definition the reciprocal projection. 

\begin{definitionblue}{Reciprocal projection}{reciprocal_projection_appendix}
  $\Pbb \in \pathmeas$ is in the reciprocal class $\calR(\Qbb)$ of $\Qbb$ if
  $\Pbb = \Pbb_{0,1} \Qbb_{|0,1}$. 
  % We denote by $\calR(\Qbb)$ the reciprocal class of $\Qbb$. 
  We define the
  \emph{reciprocal projection} of $\Pbb \in \pathmeas$ as
  $\Pbb^\star = \projR{\Qbb}(\Pbb) =  \Pbb_{0,1} \Qbb_{|0,1} $. We will write $\projsimpleR$ instead of $\projR{\Qbb}$ to simplify notation.
\end{definitionblue}

To sample from $\Pbb^\star = \projsimpleR(\Pbb)$, we only need to sample $(\bfX_0, \bfX_1) \sim \Pbb_{0,1}$ and then to sample from the Brownian bridge conditioned on $(\bfX_0, \bfX_1)$. This means that in order to sample $\bfX_t^\star \sim \Pbb^\star_t$, we only need to sample $(\bfX_0, \bfX_1) \sim \Pbb_{0,1}$ and then compute 
\begin{equation}
\label{eq:interpolation_appendix_brownian}
    \bfX_t = (1-t) \bfX_0 + t \bfX_1 + \sqrt{\vareps t (1-t)} \bfZ ,
\end{equation}
with $\bfZ \sim \mathcal{N}(0, \Id)$. In particular, sampling from $\Pbb^\star = \projsimpleR(\Pbb)$ does \emph{not} require training any neural network. However, in practice, in order to obtain samples $(\bfX_0, \bfX_1) \sim \Pbb$, we have that $\Pbb$ is associated with an SDE and therefore obtaining $(\bfX_0, \bfX_1)$ requires unrolling the SDE associated with $\Pbb$. In \Cref{alg:online_DSBM_general}, the measure $\Pbb$ is associated with an SDE with parametric drift $v^\theta$. 

In \Cref{fig:reciprocal_projection}, we continue our study of the example of \citep{liu2022rectified} that we used to explain the concept of Markovian projection. We consider the path measure $\Mbb$ obtained as the Markov projection of $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. In \Cref{fig:reciprocal_projection}, we display samples from the distributions $\pi_0$ and $\pi_1$ as well as trajectories from the path measure $\Pbb^\star = \Mbb_{0,1} \Qbb_{|0,1}$. In order to sample from $\Pbb^\star$ we first sample $(\bfX_0, \bfX_1) \sim \Mbb_{0,1}$. This involves unrolling the SDE associated with $\Mbb$. Once we have access to samples $(\bfX_0, \bfX_1)$, we draw trajectories from the Brownian bridge following the SDE \eqref{eq:brownian_bridge_appendix}. We can also sample from any time $t$ without having to unroll the SDE \eqref{eq:brownian_bridge_appendix} by simply sampling from \eqref{eq:interpolation_appendix_brownian}. This is what is done in \cref{alg:online_DSBM_general}.

\begin{figure}[H]
    \centering
    \includegraphics[width=.4\linewidth]{img/reciprocal_projection.png}
    \caption{Samples from the original distributions are shown in red, while sample paths from $\Pbb^\star = \projsimpleR(\Mbb)$ are shown in blue.}
    \label{fig:reciprocal_projection}
\end{figure}


\subsection{Iterative refinement and Schr\"odinger Bridge}
\label{sec:lit_dsbm_appendix}

 The Schr\"odinger Bridge problem \eqref{eq:dynamic_ot} can be solved leveraging techniques from diffusion models and bridge matching. \cite{debortoli2021diffusion,vargas2021solving} consider an alternating projection algorithm, corresponding to a dynamic version of the celebrated Sinkhorn algorithm. \cite{peluchetti_diffusion_2023,shi2023DSBM} introduce the Iterative Markovian Fitting procedure which corresponds to perform an alternating projection algorithm on the class of Markov processes and the reciprocal class of the Brownian motion. It can be shown that the solution of this iterative algorithm converges to the Schr\"odinger Bridge under mild assumptions, see \citep{peluchetti_diffusion_2023,shi2023DSBM}. We highlight that in the case where $\vareps \to 0$ then DSBM is equivalent to the Rectified Flow algorithm \citep{liu_flow_2023}. One of the main limitation of those previously introduced procedures which provably converge to the solution of the Schr\"odinger Bridge problem is that they rely on these expensive iterative solvers and requires to consider two networks, one parameterising the forward process $\pi_0 \to \pi_1$ and one parameterising the backward $\pi_1 \to \pi_0$. 


\subsection{Diffusion Schr\"odinger Bridge Matching}
\label{sec:sec:dsbm_appendix}


Diffusion Schr\"odinger Bridge Matching corresponds to the practical implementation of the Iterative Markovian Fitting procedure proposed in \cite{shi2023DSBM,peluchetti_diffusion_2023}. The IMF procedure alternates between projecting on the Markov class $\calM$ and
the reciprocal class $\calR_\Qbb$. In what follows, we denote $\Mbb^{n+1} = \Pbb^{2n+1} \in \calM$ and $\Pi^n = \Pbb^{2n} \in \calR(\Qbb)$. We also recall that $\Qbb$ is a (rescaled) Brownian motion associated with $(\sigma_0 \bfB_t)_{t \in [0,1]}$ and that therefore sampling from $\Qbb_{|0,1}(\cdot | x_0, x_1)$ corresponds to sampling from
\begin{equation}
    \rmd \bfX_t = (x_1 - \bfX_t) / (1 - t) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfX_0 = x_0 . 
\end{equation}
We recall that the main computational bottleneck of the DSBM lies in the approximation of the Markovian projection. 
Indeed, using \cite[Definition 1, Proposition 2]{shi2023DSBM}, we have that $\Mbb^\star=\projM(\Pi)$ is associated with the process
\begin{equation}
  \rmd \bfX_t = (\mathbb{E}_{\Pi}[\bfX_1 \ | \ \bfX_t] - \bfX_t) / (1 - t) \rmd t + \sigma_0 \rmd \bfB_t \qquad \bfX_0 \sim \pi_0 . 
\end{equation}
We also have using \cite[Proposition 2]{shi2023DSBM} that % We can thus learn 
$\Mbb^\star$ can be approximated using $\Mbb^{\theta^\star}$ given by 
\begin{align}
&  \label{eq:approximate_markovian_proj_forward}
  \rmd \bfX_t =  v_{\theta^\star}(t, \bfX_t) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 , \\
  \label{eq:loss_function_theta}
 & \theta^\star = \argmin_{\theta \in \Theta} \int_0^1 \mathbb{E}_{\Pi_{t,1}}[\|(\bfX_1 - \bfX_t)/(1-t)  - v_\theta(t,\bfX_t)\|^2]\rmd t,
\end{align}
where $\ensembleLigne{v_\theta}{\theta \in \Theta}$ is a parametric family of
functions, usually given by a neural network. 

Hence, since we can approximate $\projR{\Qbb}(\Mbb)$ and $\projM(\Pi)$ we can approximate the IMF procedure. This is the DSBM algorithm introduced in \cite{shi2023DSBM,peluchetti_diffusion_2023}. We describe the first few iterations. 
Let $\Pi^0=   \Pi_{0,1}^0 \Qbb_{|0,1}$ where
$\Pi^0_0 = \pi_0$, $\Pi^0_1 = \pi_1$. 
% and $\Qbb^0_{|0,T}$ is a tractable bridge. 
% Let $\Pbb^0 = \Qbb^0$. 
Learn $\Mbb^1 \approx \projM(\Pi^0)$ given by
\eqref{eq:approximate_markovian_proj_forward} with $v_{\theta^\star}$ given by 
\eqref{eq:loss_function_theta}.  Next, sample from $\Pi^1 = \projR{\Qbb}(\Mbb^1)= \Mbb^1_{0,1} \Qbb_{|0,1}$ by
sampling from $\Mbb^1_{0,1}$ and reconstructing the bridge $\Qbb_{|0,1}$. 
Upon iterating the previous procedure, we obtain a sequence $(\Pi^n,\Mbb^{n+1})_{n \in \nset}$. To mitigate the bias accumulation problem caused by approximating \emph{only} the forward process, we alternate between a \emph{forward} Markovian
projection and a \emph{backward} Markovian projection. We give more details on the advantage of using a forward-backward parameterisation instead of a forward-forward in \Cref{sec:forward_forward_forward_backward}.  This procedure is
valid using \cite[Proposition 9]{shi2023DSBM}. The optimal backward process is approximated with
\begin{align}
&  \label{eq:approximate_markovian_proj_backward}
  \rmd \bfY_t = v_{\phi^\star}(1-t, \bfY_t) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfY_0 \sim \pi_1 , \\ 
  \label{eq:loss_function_backward}
 &  \phi^\star = \argmin_{\phi \in \Phi} \int_0^1 \expeLigne{\Pi_{0,t}}{\normLigne{(\bfX_0 - \bfX_t)/t - v_\phi(t,\bfX_t)}^2}  \rmd t.
\end{align}
We recall the full DSBM algorithm in \Cref{alg:DSBM_general}.

\begin{algorithm}[H]
\caption{Diffusion Schr\"odinger Bridge Matching}
\label{alg:DSBM_general}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} Joint distribution $\Pi_{0,1}^0$, tractable bridge $\Qbb_{|0,1}$, number of outer iterations $N \in \nset$.}
\STATE{Let $\Pi^0 = \Pi_{0,1}^0 \Qbb_{|0,1}$.}
\FOR{$n \in \{0, \dots, N-1\}$}
  \STATE Learn $v_{\phi^\star}$ using \eqref{eq:loss_function_backward} with $\Pi = \Pi^{2n}$. 
  \STATE Let $\Mbb^{2n+1}$ be given by \eqref{eq:approximate_markovian_proj_backward}. 
  \STATE Let $\Pi^{2n+1} = \Mbb^{2n+1}_{0,1} \Qbb_{|0,1}$.
  \STATE Learn $v_{\theta^\star}$ using \eqref{eq:loss_function} with $\Pi = \Pi^{2n+1}$. 
  \STATE Let $\Mbb^{2n+2}$ be given by \eqref{eq:approximate_markovian_proj_forward}. 
  \STATE Let $\Pi^{2n+2} = \Mbb^{2n+2}_{0,1} \Qbb_{|0,1}$.
  \ENDFOR
  \STATE \textbf{Output:} $v_{\theta^\star}$, $v_{\phi^\star}$
\end{algorithmic}
\end{algorithm}

\subsection{A Reflection-projection extension}
\label{sec:reflection_projection_appendix}

First, we consider a reflection-projection method similar to the one investigated in \cite{bauschke2004reflection}. We recall that the DSBM algorithm is associated with a sequence $(\Pbb^n)_{n \in \nset}$ such that for any $n \in \nset$
\begin{equation}
    \Pbb^{n+1/2} = \projM(\Pbb^n) , \qquad \Pbb^{n+1} = \projR{\Qbb}(\Pbb^{n+1/2}) .  
\end{equation}
In a reflection-projection scheme, one of the projection is replaced by a reflection. As noted in \cite{bauschke2004reflection}, this can yield faster convergence rates in practice. We consider the sequence $(\Pbb^n)_{n \in \nset}$ such that for any $n \in \nset$
\begin{equation}
\label{eq:iteration_reflection_projection}
    \Pbb^{n+1/2} = \projM(\Pbb^n) , \qquad \Pbb^{n+1} = \projR{\Qbb}(2\Pbb^{n+1/2} - \Pbb^n) .  
\end{equation}
In what follows, we make the assumption that $2\Pbb^{n+1/2} - \Pbb^n$ is a probability measure, even though it is not clear if this path measure is non-negative. However, even by making this strong assumption, we show that we can recover DSBM in \Cref{alg:reflection_projection_extended}. By considering a relaxation of the reflection-projection scheme. 

First, note that for any $n \in \nset$, $\Pbb^n_{|0}$ is Markov, see \cite{debortoli2023augmented} for instance. Hence, we assume that $\Pbb^n$ is associated with 
\begin{equation}
\label{eq:markov_reciprocal}
    \rmd \bfX_t^n = v_{t,0}^n(\bfX_t, \bfX_0) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 . 
\end{equation}
\paragraph{Estimating $\Pbb^{n+1}$.} First, we compute $v_{t,0}^{n+1}$ assuming that we can sample from $\Pbb^n$ and $\Pbb^{n+1/2}$. Since $\Pbb^{n+1}$ is in the reciprocal class, we have that $\Pbb^{n+1}$ is associated with 
\begin{equation}
    \rmd \bfX_t = (\mathbb{E}_{\Pbb^{n+1}_{1|0,t}}[\bfX_1 \ | \ \bfX_t, \bfX_0] - \bfX_t) / (1 - t) \rmd t + \sigma_0 \rmd \bfB_t . 
\end{equation}
We refer to \cite{debortoli2023augmented} for a proof of this fact. Hence, using \eqref{eq:iteration_reflection_projection}, we have that 
\begin{align}
     v_{t,0}^{n+1} &=   \argmin_{v} \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t,0}(t, x_t, x_0) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n+1}_{0,t,1}(x_0, x_t, x_1) \\
    &=   \argmin_{v} 2 \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t,0}(t, x_t, x_0) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n+1/2}_{0,1} \rmd \Qbb_{t|0,1}(x_t|x_0,x_1) \\
    &\qquad \qquad \qquad    - \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t,0}(t, x_t, x_0) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n}_{0,1} \rmd \Qbb_{t|0,1}(x_t|x_0,x_1) . \label{eq:non_markov_dr}
\end{align}
Next, we turn to the estimation of $\Pbb^{n+3/2}$.

\paragraph{Estimating $\Pbb^{n+3/2}$.} Next, we assume that for any $n \in \nset$, $\Pbb^{n+1/2}$ is associated with 
\begin{equation}
    \rmd \bfX_t^n = v_t^n(\bfX_t) \rmd t + \sigma_0 \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 . 
\end{equation}
Using \eqref{eq:markov_reciprocal}, we have that $v_t^{n+1}$ is given by 
\begin{equation}
     v_t^{n+1} = \argmin_{v} \int_0^1 \int_{\rset^d \times \rset^d} \| v_t(t, x_t) - v_{t,0}^{n+1}(t, x_t, x_0) \|^2 \rmd \Pbb^{n+1}_{0,t}(x_0, x_t) . 
\end{equation}
We note also that using \cite[Proposition 2]{shi2023DSBM} and the fact $\Pbb^n$ is in the reciprocal class of $\Qbb$, we also have that  
\begin{align}
v_t^{n+1} &=   \argmin_{v} 2 \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t}(t, x_t) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n+1/2}_{0,1} \rmd \Qbb_{t|0,1}(x_t|x_0,x_1) \\
    &\qquad \qquad \qquad     - \int_0^1 \int_{\rset^d \times \rset^d \times \rset^d} \| v_{t}(t, x_t) - (x_1 - x_t)/(1-t) \|^2 \rmd \Pbb^{n}_{0,1} \rmd \Qbb_{t|0,1}(x_t|x_0,x_1) . \label{eq:markov_dr}
\end{align}
Hence, assuming that we can sample from $\Pbb^n$ and $\Pbb^{n+1/2}$ then we can estimate $v_{t,0}^{n+1}$ and $v_t^{n+1}$, i.e.~sample from $\Pbb^{n+1}$ and $\Pbb^{n+3/2}$. Note that the losses \eqref{eq:non_markov_dr} and \eqref{eq:markov_dr} only differ by the conditioning with respect to the initial condition $x_0$ and therefore the optimisation can be conducted in parallel. We are now able to propose the following projection-reflection algorithm, see \Cref{alg:reflection_projection}.
\begin{algorithm}[H]
\caption{Reflection Diffusion Schr\"odinger Bridge Matching}
\label{alg:reflection_projection}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} Vector field and conditional vector field $v_t^0$ and $v_{t,0}^0$, noise level $\sigma_0$ and associated bridge $\Qbb_{|0,1}$, number of outer iterations $N \in \nset$, batch size $B$}
\FOR{$n \in \{0, \dots, N-1\}$}
\WHILE{not converged}
\STATE Sample $\bfX_0^{1:B} \sim \pi_0^{\otimes B}$ 
\STATE Sample $\bfX_1^{1:B}$ using $\rmd \bfX_t^{1:B} = v_t^n(\bfX_t^{1:B}) \rmd t + \sigma_0 \rmd \bfB_t $
\STATE Sample $\hat{\bfX}_1^{1:B}$ using $\rmd \hat{\bfX}_t^{1:B} = v_{t,0}^n(\hat{\bfX}_t^{1:B}, \bfX_0^{1:B}) \rmd t + \sigma_0 \rmd \bfB_t $
\STATE $\Lnonparam = \int_0^1 [\sum_{k=1}^{B} \| v_t(\bfX_t^k) - \tfrac{\bfX_1^k - \bfX_t^k}{1-t} \|^2 - (1/2)\sum_{k=1}^B \| v_t(\bfX_t^k) - \tfrac{\bfX_1^k - \bfX_t^k}{1-t} \|^2] \rmd t $
\STATE $\Lnonparam_0 = \int_0^1 [\sum_{k=1}^{B} \| v_{t,0}(\bfX_t^k, \bfX_0^k) - \tfrac{\bfX_1^k - \bfX_t^k}{1-t} \|^2 - (1/2) \sum_{k=1}^B \| v_{t,0}(\bfX_t^k, \bfX_0^k) - \tfrac{\bfX_1^k - \bfX_t^k}{1-t} \|^2] \rmd t $
\STATE $v^{n+1}_t = \mathrm{Gradient step}(\Lnonparam)$
\STATE $v^{n+1}_{t,0} = \mathrm{Gradient step}(\Lnonparam_0)$
\ENDWHILE
\ENDFOR
\STATE{\textbf{Output:} $v_t^{N+1}, v_{t,0}^{N+1}$}
\end{algorithmic}
\end{algorithm}
Note that in \Cref{alg:reflection_projection} we only consider the optimisation of a forward process but similarly to \Cref{alg:DSBM_general}, one can construct a forward backward extension to alleviate some of the bias accumulation problems. Finally, we can interpolate between DSBM and this new reflection algorithm and DSBM by introducing an hyperparameter $\alpha \geq 0$ and consider the following extension given in \Cref{alg:reflection_projection_extended}

\begin{algorithm}[H]
\caption{Reflection Diffusion Schr\"odinger Bridge Matching}
\label{alg:reflection_projection_extended}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} Vector field and conditional vector field $v_t^0$ and $v_{t,0}^0$, noise level $\sigma_0$ and associated bridge $\Qbb_{|0,1}$, number of outer iterations $N \in \nset$, batch size $B$}
\FOR{$n \in \{0, \dots, N-1\}$}
\WHILE{not converged}
\STATE Sample $\bfX_0^{1:B} \sim \pi_0^{\otimes B}$ 
\STATE Sample $\bfX_1^{1:B}$ using $\rmd \bfX_t^{1:B} = v_t^n(\bfX_t^{1:B}) \rmd t + \sigma_0 \rmd \bfB_t $
\STATE Sample $\hat{\bfX}_1^{1:B}$ using $\rmd \hat{\bfX}_t^{1:B} = v_{t,0}^n(\hat{\bfX}_t^{1:B}, \bfX_0^{1:B}) \rmd t + \sigma_0 \rmd \bfB_t $
\STATE $\Lnonparam = \int_0^1 [\sum_{k=1}^{B} \| v_t(\bfX_t^k) - \tfrac{\bfX_1^k - \bfX_t^k}{1-t} \|^2 - \alpha\sum_{k=1}^B \| v_t(\bfX_t^k) - \tfrac{\bfX_1^k - \bfX_t^k}{1-t} \|^2] \rmd t $
\STATE $\Lnonparam_0 = \int_0^1 [\sum_{k=1}^{B} \| v_{t,0}(\bfX_t^k, \bfX_0^k) - \tfrac{\bfX_1^k - \bfX_t^k}{1-t} \|^2 - \alpha \sum_{k=1}^B \| v_{t,0}(\bfX_t^k, \bfX_0^k) - \tfrac{\bfX_1^k - \bfX_t^k}{1-t} \|^2] \rmd t $
\STATE $v^{n+1}_t = \mathrm{Gradient step}(\Lnonparam)$
\STATE $v^{n+1}_{t,0} = \mathrm{Gradient step}(\Lnonparam_0)$
\ENDWHILE
\ENDFOR
\STATE{\textbf{Output:} $v_t^{N+1}, v_{t,0}^{N+1}$}
\end{algorithmic}
\end{algorithm}

Using different values of $\alpha \geq 0$ in \Cref{alg:reflection_projection_extended}, we recover different existing algorithms.  If $\alpha =1$, we recover DSBM \cite{shi2023DSBM}. Finally, if $\alpha = 1/2$, we recover the reflection algorithm \Cref{alg:reflection_projection}.

\section{Consistency in Schr\"odinger Bridge}
\label{sec:consistency_schrodinger_bridge}

The idea of training both the forward and the backward jointly was mentioned in \cite[Section G]{shi2023DSBM}. However, it was still assumed that, while being trained jointly, the forward and backward vector fields were obtained using an $\argmin$ operation, see \cite[Equation (43), (44)]{shi2023DSBM}. In addition, in \cite[Section G]{shi2023DSBM} a consistency loss was proposed in order to enforce that the forward and backward processes match, see \cite[Equation (49)]{shi2023DSBM}. In this section, we leverage new results from \cite{daras2024consistent,debortoli2024target} in order to enforce the internal consistency of the model. 

First, note that for any $(\bfX_t)_{t \in [0,1]}$ associated with $\Pbb \in \mathcal{R}(\Qbb)$ we have for any $0 \leq t_0 \leq t \leq t_1 \leq 1$ that 
\begin{equation}
    \bfX_t = \frac{t - t_0}{t_1 - t_0} \bfX_{t_1} + \frac{t_1 - t}{t_1 - t_0} \bfX_{t_0} +  \sigma_{t_0,t,t_1} \bfZ , \qquad \bfZ \sim \gN(0, \Id), 
\end{equation}
where 
\begin{equation}
    \sigma_{t_0,t,t_1} = \sqrt{\frac{(t - t_0)(t_1 - t)}{t_1 - t_0}}.
\end{equation}
Let $p_t$ be the density of $\bfX_t$ with respect to the Lebesgue measure, we have that for any $0 \leq t_0 \leq t \leq t_1 \leq 1$ and $x_t \in \rset^d$
\begin{equation}
    p_t(x_t) = \int_{\rset^d \times \rset^d} (2 \uppi \sigma_{t_0,t,t_1}^2)^{-d/2} \exp \biggl(-\frac{\| x_t - \frac{t - t_0}{t_1 - t_0} x_{t_1} - \frac{t_1 - t}{t_1 - t_0} x_{t_0} \|^2}{2 \sigma_{t_0, t, t_1}^2}\biggr) p_{t_0}(x_{t_0}) p_{t_1}(x_{t_1}) \rmd x_{t_0} \rmd x_{t_1} . 
\end{equation}
Using the change of variable $x_{t_0} \to x_{t_0} + x_t$ and $x_{t_1} \to x_{t_1} + x_t$ we get that for any $0 \leq t_0 \leq t \leq t_1 \leq 1$ and $x_t \in \rset^d$
\begin{align}
\label{eq:tsi}
  \nabla \log p_t(x_t) = \int_{\rset^d \times \rset^d} \{ \nabla \log p_{t_0}(x_{t_0}) + \nabla \log p_{t_1}(x_{t_1}) \}  p_{t_0, t_1 | t}(x_{t_0}, x_{t_1} | x_t) \rmd x_t . 
\end{align}
This identity for the score has already been presented in a bridge matching context in \cite[Section 3.3]{debortoli2024target}. 
Let $\Pbb \in \mathcal{R}(\Qbb)$ then we have that $\projM(\Pbb)$ is such that for any $t \in [0,1]$, $\projM(\Pbb)_t = \Pbb_t$, see \cite[Proposition 2]{shi2023DSBM}. We have that for any $t \in [0,1]$, $v_t^\vsra(x) + v_{1-t}^\vsla(x) =  \sigma_0^2 \nabla \log p_t(x)$. Combining this result and \eqref{eq:tsi}, this suggests considering the following consistency loss
\begin{align}
\label{eq:consistency_loss}
    \ell_{\mathrm{cons}, (t_0, t, t_1)}(\theta) &= \mathbb{E}[\| v_\theta(t, 1, \bfX_t) + v_\theta(1-t, 0, \bfX_t) \\
    & \qquad - v_\theta(t_0, 1, \bfX_t) - v_\theta(1-t_0, 0, \bfX_{t_0}) - v_\theta(t_1, 1, \bfX_{t_1}) - v_\theta(1-t_1, 0, \bfX_{t_1}) \|^2] . 
\end{align}
Similarly to \eqref{eq:empirical_loss}, we can consider an empirical version of \eqref{eq:consistency_loss}.

\section{Model stitching}
\label{sec:model_stitching}

In \Cref{alg:online_DSBM_general}, the finetuning stage requires a pretrained bridge matching model interpolating between $\pi_0$ and $\pi_1$ (lines 2-7). However, for large datasets with complex distributions $\pi_0$ and $\pi_1$, e.g. ImageNet, training this bridge model from scratch can be computationally expensive. To improve efficiency, we can leverage existing diffusion models targeting $\pi_0$ and $\pi_1$. Specifically, we assume access to generative models transferring between $\gN(0, \Id)$ and $\pi_0$, and between $\gN(0, \Id)$ and $\pi_1$. In the rest of this section, we show how one can adapt \Cref{alg:online_DSBM_general} to this setting. We then comment on the link between the proposed algorithm and Dual Diffusion Implicit Bridges \citep{su2022dual}. 

\paragraph{Setting.} For simplicity, assume that we have two pretrained diffusion models for $\pi_0$ and $\pi_1$. We describe our procedure for $\pi_0$. Consider a forward process of the form $\bfX_t = \bfX_0 + \sigma_t \bfZ$, with $\bfZ \sim \gN(0, \Id)$, where $\sigma_t$ is a hyperparameter. Note that we could have considered an interpolant of the form $\bfX_t = \alpha_t \bfX_0 + \sigma_t \bfZ$ instead, see \cite{song_denoising_2021} for instance. 

We assume that the model $\bfX_t = \bfX_0 + \sigma_t \bfZ$ is associated with the forward diffusion model 
\begin{equation}
\label{eq:forward_process_appendix}
    \rmd \bfX_t =  g_t \rmd \bfB_t,
\end{equation}
where we assume that $g_t \geq 0$ for all $t \in [0,1]$. Note that we have that for any $t \in [0,1]$, $\sigma_t^2 = \int_0^t g_s^2 \rmd s$. In particular, we have that for any $s, t \in [0,1]$ with $s \leq t$
\begin{equation}
    \bfX_t = \bfX_s + \sqrt{\sigma_t^2 - \sigma_s^2} \bfZ , \qquad \bfZ \sim \gN(0, \Id) . 
\end{equation}
Our goal is to solve the following Entropic Optimal Transport problem 
\begin{equation}
\label{eq:static_ot_appendix}
  \Pi^\star = \argmin_{\Pi \in \calP(\rset^{2d})} \Bigl\{ \int_{\rset^d \times \rset^d} c(x,y) \rmd \Pi(x,y) - \varepsilon \mathrm{H}(\Pi) \ ; \ \Pi_0 = \pi_0, \ \Pi_1 = \pi_1 \Bigr\} ,
\end{equation}
where $\vareps > 0$ is some entropic regularisation. We assume that $\vareps >0$ is fixed and assume that there exists $t' \in [0,1]$ such that $\sigma_{t'}^2 = \vareps/2$. We now consider a dynamic version of \eqref{eq:static_ot_appendix} with
\begin{equation}
    \label{eq:dynamic_ot_appendix}
    \Pbb^\star = \argmin_{\Pbb \in \calP(\calC)} \{ \KL(\Pbb | \Qbb)  \ ; \ \Pbb_0 = \pi_0, \ \Pbb_{t'} = \pi_1 \} ,
\end{equation}
where $\Qbb$ is associated with $(\bfX_t)_{t \in [0, t']}$ \eqref{eq:forward_process_appendix}. Note that contrary to the setting presented in the main paper, here we do not consider the integration between time $0$ and $1$ but between time $0$ and $t'$. It can be shown that for any $t \in [0, t']$, $(\bfX_t)_{t \in [0,t']}$ associated with $\Qbb_{t|0, t'}$ is given by 
\begin{equation}
\label{eq:interpolation_brownian_motion_appendix}
\bfX_t = \mathrm{Interp}_t(\bfX_0, \bfX_{t'}, \bfZ) = \left(1-\frac{\sigma_t^2}{\sigma_{t'}^2}\right) \bfX_0 + \frac{\sigma_t^2}{\sigma_{t'}^2} \bfX_{t'} + \sigma_t \sqrt{1 - \frac{\sigma_t^2}{\sigma_{t'}^2} } \bfZ , \quad \bfZ \sim \gN(0, \Id) .
\end{equation}
% This is simply because the generative model was not trained for interpolation tasks and therefore we need to rescale the time to use it as an interpolant proposal. 
Solving \eqref{eq:dynamic_ot_appendix} is equivalent to solving \eqref{eq:static_ot_appendix}. We now propose an algorithm to solve \eqref{eq:dynamic_ot_appendix}. It corresponds to the finetuning stage of \Cref{alg:online_DSBM_general} with a specific initialisation, similar to DSBM-IPF in \cite{shi2023DSBM}. 

By $v_\phi$, we denote a DDM model associated with $\pi_1$:
\begin{equation}
\label{eq:pi_one_gen_model}
    \rmd \bfX_t = v_\phi(t, \bfX_t) \rmd  t + g_t \rmd \bfB_t , \quad \bfX_0 \sim \gN(0, \Id), \quad \bfX_1 \sim \pi_1.
\end{equation}

Similarly, $v_\theta$ denotes a diffusion model associated with $\pi_0$: 
\begin{equation}
\label{eq:pi_zero_gen_model}
    \rmd \bfY_t = v_\theta(t, \bfY_t) \rmd  t + g_t \rmd \bfB_t , \quad \bfY_0 \sim \gN(0, \Id), \quad\bfY_1 \sim \pi_0
\end{equation}

In analogy to \Cref{eq:mixture_bridge_sde_forward_backward}, the two equations above correspond to the forward and backward SDEs. 


For a given batch of inputs $\bfX_0^{1:B}$ and $\bfX_1^{1:B}$, timesteps $t \sim \mathrm{Unif}([0,t'])^{\otimes B}$, and interpolations $\bfX_t^\vsla$  and $\bfX_t^\vsra$, we compute the empirical forward and backward losses as the following modification of \Cref{eq:empirical_loss}:
% \begin{align}
% \label{eq:empirical_loss_appendix}
%     &\ell_t^B\left(\theta, \phi, (\bfX_0^{\vsra, 1:B}, \bfX_1^{\vsra, 1:B}, \bfX_t^{\vsra, 1:B}), (\bfX_0^{\vsla, 1:B}, \bfX_1^{\vsla, 1:B}, \bfX_{t_0 -t}^{\vsla, 1:B})\right) \\
%     & \qquad \qquad =  \frac{1}{2B} \sum_{i=1}^B \| v_\theta(t, \bfX_t^{\vsla,i}) - (\bfX_{t_0}^{\vsla,i} - \bfX_t^{\vsla,i})/(\sigma_{t_0}^2 - \sigma_t^2)^{1/2} \|^2 \\
%         & \qquad \qquad \qquad +  \frac{1}{2B} \sum_{i=1}^B \| v_\phi(t_0 - t, \bfX_{t}^{\vsra,i}) - (\bfX_{0}^{\vsra,i} - \bfX_{t}^{\vsra,i})/\sigma_t \|^2 .
% \end{align}

\begin{align}
\label{eq:empirical_loss_stitched}
 &\ell^{\vsra}(\phi; t, \bfX_1, \bfX_t^\vsra)  = \frac{1}{B} \sum_{i=1}^B \Big\| v_\phi\left(t^i, \bfX_t^{\vsra i}\right) - \left(\bfX_1^i - \bfX_t^{\vsra i}\right)/\sigma_t^i \Big\|^2, \\
   &\ell^{\vsla}(\theta; t, \bfX_0, \bfX_t^\vsla)  = \frac{1}{B}\sum_{i=1}^B \| v_\theta\left(t^i, \bfX_t^{\vsla i}\right) - \left(\bfX_0^i - \bfX_t^{\vsla i}\right)/\sqrt{\sigma^2_{t'}-\sigma^2_{t^i})} \|^2.
\end{align}

\Cref{alg:online_DSBM_general_model_stitching} corresponds to an online version of DSBM-IPF \citep{shi2023DSBM} with the initialisation given by two generative models. In \Cref{alg:online_DSBM_general_model_stitching}, we finetune the trained vector fields to solve the interpolation task. At inference time, the SDE associated with vector field $v_\theta$ interpolates between $\pi_1 \to \pi_0$, while the SDE associated with the vector field $v_\phi$ interpolates between $\pi_0 \to \pi_1$. 

\begin{algorithm}[H]
\caption{$\alpha$-Diffusion Schr\"odinger Bridge Matching for DDM finetuning}
\label{alg:online_DSBM_general_model_stitching}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} datasets $\pi_0$ and $\pi_1$,  number  finetuning steps $N_{\mathrm{finetuning}}$, batch size $B$, DDM parameters $\phi$ and $\theta$.}
\FOR{$n \in \{1, \dots, N_{\mathrm{finetuning}}\}$} 
\STATE Sample $(\bfX_0, \bfX_1) \sim (\pi_0 \otimes \pi_1)^{\otimes B}$, $t \sim \mathrm{Unif}([0,1])$, $\bfZ^{1:B} \sim \gN(0, \Id)^{\otimes B}$
\STATE Sample $\hat \bfX_{t'}^{\vsla}$ by solving \eqref{eq:pi_one_gen_model} starting from $\bfX_0$
\STATE Sample $\hat \bfX_{t'}^{\vsra}$ by solving \eqref{eq:pi_zero_gen_model} starting from $\bfX_1$
\STATE Sample $t^\vsla \sim \mathrm{Unif}([0,t'])^{\otimes B}$, $\bfZ^\vsla \sim \gN(0,\Id)^{\otimes B} $, and compute $\bfX_t^{\vsla} = \mathrm{Interp}_{t^\vsla}(\bfX_0, \hat \bfX_{t'}^{\vsla}, \bfZ^\vsla)$
\STATE Sample $t^\vsra \sim \mathrm{Unif}([0,t'])^{\otimes B}$, $\bfZ^\vsra \sim \gN(0,\Id)^{\otimes B} $, and compute $\bfX_t^{\vsra} = \mathrm{Interp}_{t^\vsra}(\bfX_1, \hat \bfX_{t'}^{\vsra}, \bfZ^\vsra)$
\STATE Update $\theta$ with gradient step on $\ell^\vsla(\theta; t^\vsla, \bfX_0, \bfX_t^{\vsla})$
\STATE Update $\phi$ with gradient step on $\ell^\vsra(\phi; t^\vsra, \bfX_1, \bfX_t^{\vsra})$
\ENDFOR
\STATE{\textbf{Output:} $\theta, \phi$ parameters of the finetuned models}
\end{algorithmic}
\end{algorithm}

Our model stitching approach is related to  Dual Diffusion Implicit Bridges (DDIB) \citep{su2022dual}, which uses pretrained diffusion models, but without further finetuning. As highlighted in \cite{shi2023DSBM}, DDIB is inferior to DSBM in terms of quality and alignement of the samples.



\section{Extended related work}
\label{sec:extended_related_work}

We highlight links between our proposed flow and Sinkhorn flows in \Cref{sec:connection_sinkhorn_flows}. We draw connection between our practical approach and Reinforcement Learning in \Cref{sec:links_rl}. We discuss how $\alpha$-IMF is related to (incremental) Expectation-Maximisation in \Cref{sec:link_with_em}. Finally, we discuss how our algorithm can be seen as an instance of continual learning in \Cref{sec:continual_learning_appendix}.

\subsection{Links with Sinkhorn flow}
\label{sec:connection_sinkhorn_flows}

In this section, we discuss the links between our approach and the Sinkhorn flow introduced by \cite{karimi2024sinkhorn}. We start by recalling how Sinkhorn flows are defined and then discuss how they are related to our approach.

\paragraph{$\gamma$-Sinkhorn and Sinkhorn flows.} We first consider the static EOT problem and recall the Sinkhorn procedure, also called Iterative Proportional Fitting. We define a sequence of coupling $(\bar{\Pi}^n, \Pi^n)_{n \in \nset}$, i.e.~for any $n \in \nset$, $\Pi^n \in \mathcal{P}(\rset^d \times \rset^d)$. We let $\Pi^0 = \Qbb_{0,1}$ and we consider for any $n \in \nset$, \begin{align}
\label{eq:sinkhorn}
    &\Pi^n = \argmin \ensembleLigne{ \KL(\Pi \ | \ \bar{\Pi}^n) }{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_0 = \pi_0}, \\
     &\bar{\Pi}^{n+1} = \argmin \ensembleLigne{ \KL(\Pi \ | \ \Pi^n) }{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_1 = \pi_1},
\end{align}
In \cite{karimi2024sinkhorn}, the authors generalise \eqref{eq:sinkhorn} by introducing an extra hyperparameter $\gamma \in (0,1]$ and defining 
\begin{align}
\label{eq:gamma_sinkhorn}
    &\Pi^n = \argmin \ensembleLigne{ \KL(\Pi \ | \ \bar{\Pi}^n) }{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_1 = \pi_1}, \\
     &\bar{\Pi}^{n+1} = \argmin \ensembleLigne{ \gamma \KL(\Pi \ | \ \Pi^n) + (1-\gamma) \KL(\Pi \ | \ \bar{\Pi}^{n}) }{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_0 = \pi_0},
\end{align}
Using \cite[Lemma 2]{karimi2024sinkhorn}, we have that for any $\gamma \in (0,1]$, any $n \in \nset$ and any $x_0, x_1 \in \rset^d$
\begin{equation}
\label{eq:pi_n_sinkhorn}
    (\rmd \bar{\Pi}^n / \rmd \Qbb_{0,1})(x_0, x_1) = \exp[f_\gamma^n(x_0) + g_\gamma^n(x_1)] ,
\end{equation}
with $f_\gamma^0 = g_\gamma^0 = 0$ and for any $n \in \nset$, $\gamma \in (0,1]$ and $x_1 \in \rset^d$
\begin{equation}
\label{eq:g_gamma_update}
    g_\gamma^{n+1}(x_1) = g_\gamma^n(x_1) - \gamma \log (\rmd \bar{\Pi}^n_1 / \rmd \pi_1)(x_1) . 
\end{equation}
In addition, using \cite[Equation (9)]{karimi2024sinkhorn} we have that for any $n \in \nset$, $\gamma \in (0,1]$ and $x_0 \in \rset^d$
\begin{equation}
     f_\gamma^n(x_0) = - \log \left(\int_{\rset^d} \exp[g_\gamma^n(x_1) - (1/(2\vareps)) \| x_0 - x_1 \|^2] \rmd \pi_1(x_1)\right) . 
\end{equation}
When letting $\gamma \to 0$, \eqref{eq:pi_n_sinkhorn} and \eqref{eq:g_gamma_update} suggest to consider for any $s \geq 0$, $x_0, x_1 \in \rset^d$
\begin{equation}
    (\rmd \bar{\Pi}^s / \rmd \Qbb_{0,1})(x_0, x_1) = \exp[f^s(x_0) + g^s(x_1)] , \qquad \Pi^s = \argmin \ensembleLigne{ \KL(\Pi \ | \ \bar{\Pi}^s) }{\Pi, \ \Pi_1 = \pi_1} , 
\end{equation}
where for any $s \geq 0$, $x_1 \in \rset^d$
\begin{equation}
\label{eq:flow_f_g}
     \partial_s g^s(x_1) = - \log (\rmd \bar{\Pi}^s_1 / \rmd \pi_1)(x_1) , \qquad \partial_s f^s(x_0) = \int_{\rset^d} \log (\rmd \bar{\Pi}^s_1 / \rmd \pi_1)(x_1) \rmd \bar{\Pi}^s(x_1|x_0) . 
\end{equation}

\paragraph{Comparison with Schr\"odinger Bridge flows.} In order to compare our approach with the one of \cite{karimi2024sinkhorn}, we start by rewriting the $\gamma$-Sinkhorn algorithm defined by \eqref{eq:gamma_sinkhorn}. To do so, we introduce the projection on the measures with fixed marginal. 

\begin{definition}{Projection on marginals}{}
Let $\Pi \in \mathcal{P}(\rset^d \times \rset^d)$ and $\pi_0 \in \mathcal{P}(\rset^d)$, we define $\projzero{\pi_0}(\Pi)$ as follows
\begin{equation}
    \projzero{\pi_0}(\Pi) = \argmin \ensembleLigne{\KL(\tilde{\Pi} \ | \ \Pi)}{\tilde{\Pi} \in \mathcal{P}(\rset^d \times \rset^d), \tilde{\Pi}_0 = \pi_0} . 
 \end{equation}
 Similarly, for any 
 $\Pi \in \mathcal{P}(\rset^d \times \rset^d)$ and $\pi_1 \in \mathcal{P}(\rset^d)$, we define $\projone{\pi_1}(\Pi)$ as follows
\begin{equation}
    \projone{\pi_1}(\Pi) = \argmin \ensembleLigne{\KL(\tilde{\Pi} \ | \ \Pi)}{\tilde{\Pi} \in \mathcal{P}(\rset^d \times \rset^d), \tilde{\Pi}_1 = \pi_1} . 
 \end{equation}
\end{definition}

With these definitions, we have that for any $n \in \nset$, $\bar{\Pi}^{n+1} = \projzero{\pi_0}(\projone{\pi_1}(\bar{\Pi}^n))$, with $(\bar{\Pi}^n)_{n \in \nset}$ the original Sinkhorn sequence defined by \eqref{eq:sinkhorn}. Similarly, we have that the original Iterative Markovian Fitting (IMF) sequence $(\hat{\Pbb}^n)_{n \in \nset}$ as defined in \eqref{eq:discretisation_flow_path_measure} with $\alpha = 1$ satisfies for any $n \in \nset$, $\hat{\Pbb}^{n+1} = \projR{\Qbb}(\projM(\Pbb^n))$. The analogy between the Sinkhorn iterates and the IMF sequence was already highlighted in \cite{shi2023DSBM,peluchetti_diffusion_2023} and further studied in \cite{brekelmans2023schrodinger}. We know show that similarly, we can draw an analogy between the sequences defined in \eqref{eq:discretisation_flow_path_measure} with $\alpha \in (0,1)$ and the sequences obtained in $\gamma$-Sinkhorn. To do so, we start by introducing 
for any $\Pi, \tilde{\Pi} \in \mathcal{P}(\rset^d \times \rset^d)$
\begin{equation}
    \Lnonparam^{\mathrm{IPF}}(\Pi, \tilde{\Pi}) = \KL(\Pi \ | \projone{\pi_1}(\tilde{\Pi})) , \qquad R^{\mathrm{IPF}}(\Pi, \tilde{\Pi}) =  \KL(\Pi \ | \tilde{\Pi}) . 
\end{equation}

\begin{table}
\centering
\begin{tabular}{|l|c|c|}
\hline
 & $\gamma$-Sinkhorn & $\gamma$-IMF \\
 \hline \hline
Loss function & $\KL(\Pi \ | \projone{\pi_1}(\tilde{\Pi}))$ & $ \int_0^1 \mathbb{E}_{\projR{\Qbb}(\Pbb)} [\| v_t(\bfX_t)-\tfrac{\bfX_1 - \bfX_t}{1-t} \|^2]  \rmd t$\\
regularisation & $\KL(\Pi \ | \tilde{\Pi}) $ & $ \int_0^1 \int_{\rset^d} \| f_t(x_t) - \tilde{f}_t(x_t)\|^2 \rmd \mu_t(x_t) \rmd t$ \\
Update & Implicit & Explicit  \\
\hline
\end{tabular}
\caption{Comparison between $\gamma$-Sinkhorn and $\gamma$-IMF.}
\label{table:comparison_flow}
\end{table}

 With this notation, we can now rewrite \eqref{eq:gamma_sinkhorn} for any $n \in \nset$ as 
 \begin{align}
 \label{eq:rewriting_gamma_sinkhorn_2}
     &\bar{\Pi}^{n+1} = \argmin \ensembleLigne{\Lnonparam^{\mathrm{IPF}}(\Pi, \bar{\Pi}^n) + ((1-\gamma)/\gamma) R^{\mathrm{IPF}}(\Pi, \bar{\Pi}^n)}{\Pi \in \mathcal{P}(\rset^d \times \rset^d), \ \Pi_0 = \pi_0} . 
 \end{align}
  Now, we are going to see that \eqref{eq:rewriting_gamma_sinkhorn} is linked with the discretisation of the path measure flow described in \eqref{eq:discretisation_flow_path_measure}. Recall that for any suitable $v$, we define the path measure $\Pbb_v$ associated with  
\begin{equation}
%\label{eq:mixture_bridge_sde}
    \rmd \bfX_t = v_t(\bfX_t) \rmd t + \sqrt{\vareps} \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 . 
\end{equation}
 We define 
 \begin{equation}
\label{eq:loss_function_appendix}
    \Lnonparam(v, \Pbb) =\int_0^1  \Lnonparam(v_t, \Pbb)\rmd t= \int_0^1 \int_{\rset^d \times \rset^d} \Big\| v_t(x_t) - \frac{x_1 - x_t}{1-t}\Big\|^2 \rmd \projR{\Qbb}(\Pbb)_{t,1}(x_t, x_1) \rmd s . 
    % =  \int_0^1 \mathbb{E}_{\projR{\Qbb}(\Pbb)}[\| \bfX_1 - f_t(\bfX_t) \|^2] \rmd t .
\end{equation}
Similarly, for any $\mu \in \mathcal{P}(\mathcal{C})$, we define 
\begin{equation}
    R_\mu(\Pbb_v, \Pbb_{\tilde{v}}) = \int_0^1 \int_{\rset^d} \| v_t(x_t) - \tilde{v}_t(x_t)\|^2 \rmd \mu_t(x_t) \rmd t .  
\end{equation}
 Next, we define the sequence of path measures $(\bar{\Pbb}^n)_{n \in \nset}$ such that for any $n \in \nset$
 \begin{align}
 \label{eq:rewriting_gamma_sinkhorn}
     &\bar{\Pbb}^{n+1} = \argmin \ensembleLigne{\Lnonparam(\Pbb, \bar{\Pbb}^n) + (1/\alpha) R_{\mu^n}(\Pi, \bar{\Pbb}^n)}{\Pbb = \Pbb_v, \ \text{for some $v$}} .
 \end{align}
 Now, if we denote $(v^n)_{n \in \nset}$ the sequence such that for any $n \in \nset$, $\bar{\Pbb}^n = \Pbb_{v^n}$ then we have that for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$
 \begin{equation}
 \label{eq:update_implicit}
     v^{n+1}_t(x) = v^n_t(x) - \delta \nabla_{\mu^n} \Lnonparam_t(v^{n+1}, \bar{\Pbb}^n)(x) .
 \end{equation}
 Recall that $(\Pbb^n)_{n \in \nset}$ given by \eqref{eq:discretisation_flow_path_measure} is associated with $(v^n)_{n \in \nset}$ such that for any $n \in \nset$, $t \in [0,1]$ and $x \in \rset^d$
 \begin{equation}
  \label{eq:update_explicit}
     v^{n+1}_t(x) = v^n_t(x) - \delta \nabla_\mu^n \Lnonparam_t(v^{n}, \Pbb^n)_t(x) ,
 \end{equation}
 see \Cref{prop:shadow_sequence}. Therefore, the only difference between \eqref{eq:update_explicit} and \eqref{eq:update_implicit} is that \eqref{eq:update_explicit} is an explicit update whereas \eqref{eq:update_implicit} is an implicit update. We summarise the differences between $\gamma$-Sinkhorn and the discretisation we introduce in \Cref{table:comparison_flow}.


\subsection{Links with Reinforcement Learning}
\label{sec:links_rl}

In this section, we draw some connection between  \Cref{alg:online_DSBM_general} and self-play in Reinforcement Learning. In particular, we introduce a generalisation of \Cref{alg:online_DSBM_general} which uses the concept of replay buffer commonly used in Reinforcement learning, see \cite{mnih2015human} for instance. 


We first present a generalisation of \Cref{alg:online_DSBM_general} called Replay Buffer Diffusion Schr\"odinger Bridge Matching \Cref{alg:online_DSBM_general_replay_buffer}. We define a buffer $\mathcal{B}$ as a collection of samples $\{ (\bfX_0^k, \bfX_1^k) \}_{k=1}^N$, where $N \in \nset$ is the size of the buffer equipped with two functions $\mathrm{Add}$ and $\mathrm{Sample}$. We have that $\mathrm{Add}: \ \Omega \times \bigsqcup_{k \in \nset} (\rset^{2d})^k \times (\rset^{2d})^N \to (\rset^{2d})^N$, where $\Omega$ is a probability space. In practice $\mathrm{Add}$ takes a random number (the function can be stochastic), any number of proposed samples as well as the current buffer. As an output $\mathrm{Add}$ returns the updated buffer. We also define $\mathrm{Sample}: \ \Omega \times \nset \times (\rset^{2d})^N \to \bigsqcup_{k \in \nset} (\rset^{2d})^k$. This function takes a random number (the function can be stochastic), a natural number $k$ representing the number of samples to return as well as the current buffer. As an output $\mathrm{Sample}$ returns a batch of $k$ samples from the buffer.

\begin{algorithm}[H]
\caption{Replay Buffer Diffusion Schr\"odinger Bridge Matching}
\label{alg:online_DSBM_general_replay_buffer}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} $\pi_0$, $\pi_1$, $\vareps$ (entropic regularisation), $N_{\mathrm{pretraining}}$ (number of pretraining steps), $N_{\mathrm{finetuning}}$ (number of finetuning steps), $B$ (batch size), $\gamma$ (EMA parameter), $\theta$ (initial parameters), $\mathcal{B}^{\mathrm{fwd}}$ (forward buffer), $\mathcal{B}^{\mathrm{bwd}}$ (backward buffer)}
\STATE $\bar{\theta} = \theta$
\FOR{$n \in \{0, \dots, N_{\mathrm{pretraining}}\}$} 
\STATE Sample $(\bfX_0^{1:B}, \bfX_1^{1:B}) \sim (\pi_0 \otimes \pi_1)^{\otimes B}$, $t \sim \mathrm{Unif}([0,1])$, $\bfZ^{1:B} \sim \gN(0, \Id)^{\otimes B}$
\STATE Compute $\bfX_t^{1:B} = \mathrm{Interp}_t(\bfX_0^{1:B}, \bfX_1^{1:B}, \bfZ^{1:B})$ using \eqref{eq:interpolation_brownian_motion}
\STATE Update $\theta$ with gradient step on $\ell_t^B$, $\bar{\theta} = (1-\gamma) \bar{\theta} + \gamma \theta$
\ENDFOR
\FOR{$n \in \{0, \dots, N_{\mathrm{finetuning}}\}$} 
\IF{$n \equiv 0 [n_{\mathrm{refresh}}]$}
\STATE Sample $(\hat{\bfX}_0^{1:B}, \hat{\bfY}_0^{1:B}) \sim (\pi_0 \otimes \pi_1)^{\otimes B}$, $t \sim \mathrm{Unif}([0,1])$, $\bfZ^{1:B} \sim \gN(0, \Id)^{\otimes B}$
\STATE Sample  $(\bfX_1^{1:B},\bfY_1^{1:B})$ using \eqref{eq:mixture_bridge_sde_forward_backward} with initialisation $(\hat{\bfX}_0^{1:B},\hat{\bfY}_0^{1:B})$
\STATE $\mathcal{B}^{\mathrm{fwd}} = \mathrm{Add}((\hat{\bfX}_0^{1:B}, \bfX_1^{1:B}), \mathcal{B}^{\mathrm{fwd}})$
\STATE $\mathcal{B}^{\mathrm{bwd}} = \mathrm{Add}((\bfY_1^{B}, \hat{\bfY}_0^{1:B}), \mathcal{B}^{\mathrm{bwd}})$
\ENDIF
\STATE $(\hat{\bfX}_0^{1:B}, \bfX_1^{1:B}) = \mathrm{Sample}(B, \mathcal{B}^{\mathrm{fwd}})$
\STATE $(\bfY_1^{1:B}, \hat{\bfY}_0^{1:B}) = \mathrm{Sample}(B, \mathcal{B}^{\mathrm{bwd}})$
\STATE Compute $\bfX_t^{1:B} = \mathrm{Interp}_t(\hat{\bfX}_0^{1:B}, \bfX_1^{1:B}, \bfZ^{1:B})$ using \eqref{eq:interpolation_brownian_motion}
\STATE Compute $\bfY_{1-t}^{1:B} = \mathrm{Interp}_t(\bfY_1^{1:B}, \hat{\bfY}_0^{1:B}, \bfZ^{1:B})$ using \eqref{eq:interpolation_brownian_motion}
\STATE Update $\theta$ with gradient step on $\ell_t^B$, $\bar{\theta} = (1-\gamma) \bar{\theta} + \gamma \theta$
\ENDFOR
\STATE{\textbf{Output:} $(\theta, \bar{\theta})$ parameters of the finetuned model }
\end{algorithmic}
\end{algorithm}

In \Cref{alg:online_DSBM_general_replay_buffer}, we allow for more flexibility than the online procedure by leveraging the concept of replay buffer originally introduced in Reinforcement Learning \cite{mnih2015human}.
The concept of replay buffer has been used previously in Schr\"odinger Bridge works, with the notion of cache where every $n_{\mathrm{refresh}}$ steps a cache is emptied and filled with new samples.
If $n_{\mathrm{refresh}} = 1$, $N=B$ for both $\mathcal{B}^{\mathrm{fwd}}$ and $\mathcal{B}^{\mathrm{fwd}}$ we have that for any $\omega \in \Omega$ and $(\bfX_0^{1:B}, \bfX_1^{1:B}) \in (\rset^{2d})^{B}$ 
\begin{align}
    &\mathrm{Add}(\omega, (\bfX_0^{1:B}, \bfX_1^{1:B}), \mathcal{B}) = (\bfX_0^{1:B}, \bfX_1^{1:B}) , \\ &\mathrm{Sample}(\omega, B, (\bfX_0^{1:B}, \bfX_1^{1:B})) = (\bfX_0^{1:B}, \bfX_1^{1:B}) .
\end{align}
This means that the $\mathrm{Add}$ simply fills the buffer with the new samples while $\mathrm{Sample}$ just return the whole current buffer. In that case we recover \Cref{alg:online_DSBM_general}. For more general update rules, the replay buffers $\mathcal{B}^{\mathrm{fwd}}$ and $\mathcal{B}^{\mathrm{bwd}}$ allow us to collect previous samples and therefore to keep a memory of the past experiences. In future work, we plan to investigate popular choice in experience replay and their impact on the performance of \Cref{alg:online_DSBM_general_replay_buffer}.

% \subsection{Replay Buffer in Reinforcement Learning}
% \label{sec:replay_buffer_rl}


\subsection{Links with Expectation Maximisation}
\label{sec:link_with_em}
In this section, we make a connection between DSBM and the Expectation Maximisation (EM) algorithm, and show that the discretisation of the Schr\"odinger Flow proposed in \Cref{alg:online_DSBM_general} corresponds to some incremental version of an idealised algorithm, as discussed in \cite{neal1998view}. We would like to emphasize that the link between the EM algorithm and Diffusion Schr\"odinger Bridge based methodologies was already highlighted by \cite{vargas2023transport,brekelmans2023schrodinger}. Below, we follow the framework of \cite{brekelmans2023schrodinger} and recall the following definitions.

\begin{definition}{Projections and maximisations}{}
 Let $\msa$ be a subset of $\calP(\calC)$. Then, for any $\Pbb \in \calP(\calC)$, when it is well-defined, we define its \Eproj on $\mathsf{A}$ as $\Pbb^\star = \argmin_{\Qbb \in \mathsf{A}} \KL(\Qbb \ | \ \Pbb)$. Similarly, for any $\Pbb \in \calP(\calC)$, when it is well-defined, we define its \Mproj on $\mathsf{A}$ as $\Pbb^\star = \argmin_{\Qbb \in \mathsf{A}} \KL(\Pbb \ | \ \Qbb)$.   
\end{definition}

In \cite{brekelmans2023schrodinger}, the authors choose \Mproj because this corresponds to the \emph{Maximisation} step in an EM algorithm while the \Eproj corresponds to the \emph{expectation} step in the EM algorithm. In \cite{brekelmans2023schrodinger}, the authors highlight that the Iterative Proportional Fitting procedure is a Expectation-Expectation procedure, i.e.~the alternating projections are both $\mathrm{E}$-projections. In contrast, the Iterative Markovian Fitting procedure is a Maximisation-Maximisation procedure, i.e.~the alternating projections are both $\mathrm{M}$-projections. In particular, we can define the following sequence of path measures $(\Pbb^n)_{n \in \nset}$, where for any $n \in \nset$ we have
\begin{equation}
    \Pbb^{n+1/2} = \argmin_{\Pbb \in \projM}\KL(\Pbb^n \ | \Pbb) , \qquad \Pbb^{n+1} = \argmin_{\Pbb \in \mathcal{R}(\Qbb)}\KL(\Pbb^{n+1/2} \ | \Pbb) .  
\end{equation}
In addition, we have that 
\begin{equation}
    \Pbb^{n+1/2} = \Pbb_{v_\star^{n+1}}, \qquad v_\star^{n+1} = \argmin_v \Lnonparam(v, \Pbb^{n}) , \qquad \Pbb^{n+1} = \argmin_{\Pbb \in \mathcal{R}(\Qbb)}\KL(\Pbb_{v_\star^{n+1}} \ | \ \Pbb)  ,
\end{equation}
since we have that $\Pbb^n = \Pbb^{v_\star^n}$. Hence, our online procedure \Cref{alg:online_DSBM_general}, which corresponds to the discretisation of the flow of path measures \eqref{eq:flow_hat_p} can be rewritten as 
\begin{equation}
    \Pbb^{n+1/2} = \Pbb_{v_\star^{n+1}}, \quad v_\star^{n+1} = \mathrm{Gradientstep}(\Lnonparam(v, \Pbb^n) , \quad \Pbb^{n+1} = \argmin_{\Pbb \in \mathcal{R}(\Qbb)}\KL(\Pbb_{v_\star^{n+1}} \ | \ \Pbb) .  
\end{equation}
Therefore, our proposed algorithm can be seen as an incremental version of the Maximisation-Maximisation algorithm associated with DSBM instead of an incremental version of the Expectation-Maximisation algorithm discussed in \citep{neal1998view}.

\subsection{Links with finetuning of diffusion models}
 \Cref{alg:online_DSBM_general} can be seen as a method to finetune bridge matching. Finetuning of diffusion models and flow matching procedures is an active research area. Most of the existing methodologies optimise for an external cost after a pretraining phase. These procedures rely on Reinforcement Learning strategies \citep{lee2023aligning,black2023training,fan2024reinforcement}. Recently Direct Preference optimisation (DPO) \citep{rafailov2024direct} has been applied to the finetuning of diffusion models in \citep{yang2023using,rafailov2024direct}. Our approach departs from these works as the objective we minimise is given by the EOT cost. However all of these approaches involve some level of self-play, i.e.~are not simulation free.
 
 \subsection{Links with continual learning}
 \label{sec:continual_learning_appendix}
 
 Continual learning develops techniques to train models when the dataset changes during the training, usually to solve different tasks \cite{de2021continual,parisi2019continual,zajkac2023exploring}. In the context of diffusion models, continual learning has been investigated in \cite{masip2023continual,zajkac2023exploring,smith2023continual}. In \citep{masip2023continual}, the authors consider a weighted loss between a diffusion model loss and a distillation loss which ensures some consistency between the model being trained and the previous task model. Similarly to our approach this distillation loss is not simulation-free but, contrary to our loss, the clean samples are not obtained by unrolling the diffusion model but by applying a one-step prediction operator. In \citep{zajkac2023exploring}, consider different replay buffer techniques to train continual diffusion models and observe that experience replay with a small coefficient can bring improvements. Finally, in \citep{smith2023continual}, the authors consider the continual training of a text-to-image diffusion model with LoRA \citep{hu2021lora}. 

\section{Forward-Forward, Forward-Backward and accumulation of error}
\label{sec:forward_forward_forward_backward}

In this section, we investigate how error accumulates in the context of DSBM. In practice, we observe similar conclusions in the case of the online version of DSBM. We compare two methods: one which only trains a forward model and one which trains a forward and a backward model.

In what follows, we assume that $\pi_0 = \pi_1 = \gN(0, \Id)$, we also assume that $\Qbb$ is associated with $(\sqrt{2} \bfB_t)_{t \in [0,1]}$. We recall that for any $t \in [0,1]$, we have that 
\begin{equation}
    \bfX_t = (1-t) \bfX_0 + t \bfX_1 + \sqrt{2t(1-t)} \bfZ , \qquad \bfZ \sim \gN(0, \Id) . 
\end{equation}
We are going to consider to approximate schemes to implement IMF.

\paragraph{Forward-forward.} First, we consider the following sequence of path measures $(\Pbb^n)_{n \in \nset}$. We set $\Pbb^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. For any $n \in \nset$, we define $\Pbb^{2n+2} = \Pbb^{2n+1}_{0,1} \Qbb_{|0,1}$, i.e.~$\Pbb^{2n+2} = \projsimpleR(\Pbb^{2n+1})$. In addition, we define $\Pbb^{2n+1} = \projM^{\vareps, \vsra}(\Pbb^{2n})$ such that  $\Pbb^{2n+1}$ is associated with $(\bfX_t)_{t \in [0,1]}$ where for any $t \in [0,1]$
\begin{equation}
\label{eq:markov_proj_approximate}
    \rmd \bfX_t = \{ (\mathbb{E}_{\Pbb^{2n}_{1|t}}[\bfX_1 \ | \ \bfX_t] - \bfX_t) / (1 - t) + \vareps \bfX_t \} \rmd t + \sqrt{2} \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 , 
\end{equation}
with $\vareps \in \rset$. Recall that if we define $\bar{\Pbb}^{2n+1} = \projM(\Pbb^{2n})$ we have that for any $t \in [0,1]$, $\bar{\Pbb}^{2n+1}$ is associated with $(\bfX_t)_{t \in [0,1]}$ where for any $t \in [0,1]$
\begin{equation}
    \rmd \bfX_t = (\mathbb{E}_{\Pbb^{2n}_{1|t}}[\bfX_1 \ | \ \bfX_t] - \bfX_t) / (1 - t) \rmd t + \sqrt{2} \rmd \bfB_t . 
\end{equation}
Hence, $\projM^{\vareps, \vsra}$ corresponds to making an error of order $x \mapsto \vareps x$ on the estimated velocity field. Doing so, we now longer have that for any $n \in \nset$, $\Pbb^n_1 = \pi_1$. In what follows, we are going to show how the error accumulates for the sequence $\Pbb^n_{0,1}$. 

Before stating \Cref{prop:forward_forward_appendix}, we introduce $f: \ \rset^4 \to \rset$ such that for any $c_{0,0}, c_{1,1}, c_{0,1} > 0$ and $t \in [0,1]$
\begin{align}
    f(c_{0,0}, c_{1,1}, c_{0,1}, t) &= [-(1-t)c_{0,0} + t c_{1,1} + (1-2t) c_{0,1}  - 2t] \\
    & \qquad / [(1-t)^2 c_{0,0} + t^2 c_{1,1} + 2t(1-t) c_{0,1} + 2t(1-t)] .
\end{align}
We define $F(c_{0,0}, c_{1,1}, c_{0,1}, \vareps, t) = 2 \int_0^t f(c_{0,0}, c_{1,1}, c_{0,1}, s) \rmd s + 2 \vareps t$.
Finally, we define 
\begin{equation}
    f_{\mathrm{cov}}(c_{0,0}, c_{1,1}, c_{0,1}, \vareps) = \exp[\tfrac{1}{2} F(c_{0,0}, c_{1,1}, c_{0,1}, \vareps, 1)] ,
\end{equation}
as well as 
\begin{equation}
    f_{\mathrm{var}}(c_{0,0}, c_{1,1}, c_{0,1}, \vareps) = \exp[\tfrac{1}{2} F(c_{0,0}, c_{1,1}, c_{0,1}, \vareps, 1)] (1 + 2 \int_0^1 \exp[-F(c_{0,0}, c_{1,1}, c_{0,1}, \vareps, s)] \rmd s) .
\end{equation}

\begin{proposition}{Forward-Forward updates}{forward_forward_appendix}
For any $n \in \nset$, we have that $\Pbb^{2n+1}_{0,1} = \gN(0, \Sigma^{n+1} \Id)$ where 
\begin{equation}
    \Sigma^{n+1} = \left( \begin{matrix} \Id & c_{0,1}^{n+1} \Id \\
    c_{0,1}^{n+1} \Id & c_{1,1}^{n+1} \Id \end{matrix} \right ) , 
\end{equation}
and for any $n \in \nset$
\begin{align}
    &c_{1,1}^{n+1} = f_{\mathrm{var}}(1, c_{1,1}^n, c_{0,1}^n, \vareps) , \\
    &c_{0,1}^{n+1} = f_{\mathrm{cov}}(1, c_{1,1}^n, c_{0,1}^n, \vareps) . 
\end{align}
\end{proposition}

\begin{proof}
Let $\Pbb = (\Pbb_{0,1}) \Qbb_{|0,1}$ where $\Pbb_{0,1}$ is a Gaussian random variable with zero mean and covariance matrix $\Sigma \in \rset^{2d \times 2d}$ such that
\begin{equation}
    \Sigma = \left( \begin{matrix} \Id & c_{0,1} \Id \\
    c_{0,1} \Id & c_{1,1} \Id \end{matrix} \right ) ,
\end{equation}
where $\Id$ is the $d$-dimensional identity matrix and we assume that $c_{0,1}, c_{1,1} > 0$. We denote $\Pbb^\star = \projM^{\vareps, \vsra}(\Pbb)$. We have that $\Pbb_{1|t}$ is a Gaussian random variable with zero mean. We now compute its covariance matrix. First, we have that 
\begin{equation}
    \mathbb{E}[\bfX_t \bfX_1^\top] = (1-t) \mathbb{E}[\bfX_0 \bfX_1^\top] + t \mathbb{E}[\bfX_1 \bfX_1^\top] = [(1-t) c_{0,1}  + t c_{1,1}] \Id . 
\end{equation}
We also have that 
\begin{align}
    \mathbb{E}[\bfX_t \bfX_t^\top] &= (1-t)^2 \mathbb{E}[\bfX_0 \bfX_0^\top] + t(1-t) (\mathbb{E}[\bfX_1 \bfX_0^\top] + \mathbb{E}[\bfX_1 \bfX_0^\top]) + t^2 \mathbb{E}[\bfX_1 \bfX_1^\top] + 2t(1-t) \Id \\
    &= [(1-t)^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1} + 2t(1-t)] \Id \\
    &= [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}] \Id . 
\end{align}
Therefore, we get that for any $t \in [0,1]$ and $x_t  \in \rset^d$  
\begin{equation}
    \mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] = ([(1-t) c_{0,1}  + t c_{1,1}] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}] ) x_t .
\end{equation}
Hence, we have that for any $t \in [0,1]$ and $x_t  \in \rset^d$ 
\begin{align}
    & \mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t = ([(1-t) c_{0,1}  + t c_{1,1}] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}] - 1) x_t \\
    & \quad = ([(1-t) c_{0,1}  + t c_{1,1} - 1 + t^2 - t^2 c_{1,1} - 2t(1-t) c_{0,1}] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) x_t \\
    & \quad = ([(1-t)(1-2t) c_{0,1}  + t(1-t) c_{1,1} - 1 + t^2] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) x_t \\
    & \quad = (1-t) ([(1-2t) c_{0,1}  + tc_{1,1} - 1 - t] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) x_t .
\end{align}
So it follows that
\begin{align}
    & (\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t)/(1-t) \\
    & \qquad \qquad = ([(1-2t) c_{0,1}  + tc_{1,1} - 1 - t] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) x_t . \label{eq:conditional_expectation_appendix}
\end{align}
Note that if we set $c_{0,1} = c^2$ and $c_{1,1} = 1$, we recover \cite[Lemma 13]{shi2023DSBM} with $\sigma = 2$. Denote $\Pbb^\star = \projM^{\vareps, \vsra}(\Pbb)$. Combining \eqref{eq:conditional_expectation_appendix} and \eqref{eq:markov_proj_approximate} we get that $\Pbb^\star$ is associated with $(\bfX_t)_{t \in [0,1]}$ such that for any $t \in [0,1]$ we have
\begin{equation}
    \rmd \bfX_t = \{([(1-2t) c_{0,1}  + tc_{1,1} - 1 - t] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}]) + \vareps \} \bfX_t \rmd t + \sqrt{2} \rmd \bfB_t . 
\end{equation}
Hence, we get that 
\begin{equation}
     \bfX_t = \exp[\tfrac{1}{2} G(t, c_{0,1}, c_{1,1}, \vareps)] \bfX_0 + \Bigl(2 \int_0^t \exp[- G(s, c_{0,1}, c_{1,1}, \vareps)] \rmd s \exp[G(t, c_{0,1}, c_{1,1}, \vareps)]\Bigr)^{1/2} \bfZ ,
\end{equation}
where $\bfZ \sim \gN(0, \Id)$ is independent from $\bfX_0$ and for any $t \in [0,1]$, $c_{0,1}, c_{1,1}, \vareps > 0$ we have 
\begin{equation}
    G(t, c_{0,1}, c_{1,1}, \vareps) = 2 \int_0^t [(1-2t) c_{0,1}  + tc_{1,1} - 1 - t] / [1 - t^2 + t^2 c_{1,1} + 2t(1-t) c_{0,1}] \rmd t + 2\vareps t . 
\end{equation}
In addition, we define 
\begin{align}
    &g_{\mathrm{cov}}(c_{0,1}, c_{1,1}, \vareps) = \exp[G(1, c_{0,1}, c_{1,1}, \vareps)] , \\
    & g_{\mathrm{var}}(c_{0,1}, c_{1,1}, \vareps) = \exp[G(1, c_{0,1}, c_{1,1}, \vareps)] \Bigl(1 + 2 \int_0^1 \exp[-G(t, c_{0,1}, c_{1,1}, \vareps)] \rmd t \Bigr) .
\end{align}
Hence, we have that 
\begin{equation}
    \mathbb{E}[\bfX_0 \bfX_1^\top] = g_{\mathrm{cov}}(c_{0,1}, c_{1,1}, \vareps) \Id , \qquad \mathbb{E}[\bfX_1 \bfX_1^\top] = g_{\mathrm{var}}(c_{0,1}, c_{1,1}, \vareps) \Id .
\end{equation}
Therefore, since for any $n \in \nset$, we have that $\Pbb^{2n+1} = \projM^{\vareps, \vsra}(\Pbb^{2n})$ and $\Pbb^{2n+2} = \Pbb^{2n+1}_{0,1} \Qbb_{|0,1}$, we define $(c_{0,1}^n, c_{1,1}^n)_{n \in \nset}$ such that for any $n \in \nset$
\begin{equation}
    \mathbb{E}_{\Pbb^{2n}}[\bfX_0 \bfX_1^\top] = c_{0,1}^n \Id , \qquad \mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_1^\top] = c_{1,1}^n \Id . 
\end{equation}
Note that for any $n \in \nset$, we have that 
\begin{equation}
    \mathbb{E}_{\Pbb^{2n+1}}[\bfX_0 \bfX_1^\top] = c_{0,1}^{n+1} \Id , \qquad \mathbb{E}_{\Pbb^{2n+1}}[\bfX_1 \bfX_1^\top] = c_{1,1}^{n+1} \Id . 
\end{equation}
Since $\Pbb^0 = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$ we get that $c_{0,1}^0 = 0$ and $c_{1,1} = 1$. We have that for any $n \in \nset$
\begin{equation}
    c_{0,1}^{n+1} = g_{\mathrm{cov}}(c_{0,1}^n, c_{1,1}^n, \vareps) , \qquad c_{1,1}^{n+1} = g_{\mathrm{var}}(c_{1,1}^n, c_{1,1}^n, \vareps) ,
\end{equation}
which concludes the proof. 
\end{proof}

\paragraph{Forward-backward.} Next, we consider the following sequences of path measures $(\Pbb^{n, \vsra})_{n \in \nset}$ and $(\Pbb^{n, \vsla})_{n \in \nset}$. We set $\Pbb^{0, \vsra} = \Pbb^{0, \vsla} = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. For any $n \in \nset$, we define $\Pbb^{2n+2, \vsra} = \Pbb^{2n+1, \vsla}_{0,1} \Qbb_{|0,1}$ and $\Pbb^{2n+2, \vsla} = \Pbb^{2n+1, \vsra}_{0,1} \Qbb_{|0,1}$, i.e.~$\Pbb^{2n+2, \vsra} = \projsimpleR(\Pbb^{2n+1, \vsla})$ and $\Pbb^{2n+2, \vsla} = \projsimpleR(\Pbb^{2n+1, \vsra})$. In addition, we define $\Pbb^{2n+1, \vsra} = \projM^{\vareps, \vsra}(\Pbb^{2n, \vsla})$ such that for any $t \in [0,1]$, $\Pbb^{2n+1, \vsra}$ is associated with $(\bfX_t)_{t \in [0,1]}$ where 
\begin{equation}
    \rmd \bfX_t = \{ (\mathbb{E}_{\Pbb^{2n, \vsra}_{1|t}}[\bfX_1 \ | \ \bfX_t] - \bfX_t) / (1 - t) + \vareps \bfX_t \} \rmd t + \sqrt{2} \rmd \bfB_t , \qquad \bfX_0 \sim \pi_0 , 
\end{equation}
with $\vareps \in \rset$. Similarly, we define $\Pbb^{2n+1, \vsla} = \projM^{\vareps, \vsla}(\Pbb^{2n, \vsra})$ such that for any $t \in [0,1]$, $\Pbb^{2n+1, \vsla}$ is associated with $(\bfY_{1-t})_{t \in [0,1]}$ where 
\begin{equation}
    \rmd \bfY_t = \{ (\mathbb{E}_{\Pbb^{2n, \vsla}_{0|t}}[\bfX_0 \ | \ \bfY_t] - \bfY_t) / (1 - t) + \vareps \bfY_t \} \rmd t + \sqrt{2} \rmd \bfB_t , \qquad \bfY_0 \sim \pi_1 . 
\end{equation}
\begin{proposition}{Forward-Backward updates}{forward_backward_appendix}
For any $n \in \nset$, we have that $\Pbb^{2n+1, \vsra}_{0,1} = \gN(0, \Sigma^{n+1, \vsra} \Id)$ and $\Pbb^{2n+1, \vsla}_{0,1} = \gN(0, \Sigma^{n+1, \vsla} \Id)$ where 
\begin{equation}
    \Sigma^{n+1, \vsra} = \left( \begin{matrix} \Id & c_{0,1}^{n+1, \vsra} \Id \\
    c_{0,1}^{n+1, \vsra} \Id & c_{1,1}^{n+1, \vsra} \Id \end{matrix} \right ) , \qquad 
    \Sigma^{n+1, \vsla} = \left( \begin{matrix} c_{0,0}^{n+1, \vsla} \Id & c_{0,1}^{n+1, \vsla} \Id \\
    c_{0,1}^{n+1, \vsla} \Id &  \Id \end{matrix} \right ) , 
\end{equation}
and for any $n \in \nset$
\begin{align}
    &c_{1,1}^{n+1, \vsra} = f_{\mathrm{var}}(c_{0,0}^{n, \vsla}, 1, c_{0,1}^{n, \vsla}, \vareps) , \\
    &c_{0,1}^{n+1, \vsra} = f_{\mathrm{cov}}(c_{0,0}^{n, \vsla}, 1, c_{0,1}^{n, \vsla}, \vareps) , \\
    &c_{0,0}^{n+1, \vsla} = f_{\mathrm{var}}(1, c_{1,1}^{n, \vsra}, c_{0,1}^{n, \vsra}, \vareps) , \\
    &c_{0,1}^{n+1, \vsla} = f_{\mathrm{cov}}(1, c_{1,1}^{n, \vsra}, c_{0,1}^{n, \vsra}, \vareps) . 
\end{align}
\end{proposition}

The proof is similar to the one of \Cref{prop:forward_forward_appendix}.

\begin{proof}
Let $\Pbb = (\Pbb_{0,1}) \Qbb_{|0,1}$ where $\Pbb_{0,1}$ is a Gaussian random variable with zero mean and covariance matrix $\Sigma \in \rset^{2d \times 2d}$ such that
\begin{equation}
    \Sigma = \left( \begin{matrix} c_{0,0} \Id & c_{0,1} \Id \\
    c_{0,1} \Id &  \Id \end{matrix} \right ) ,
\end{equation}
where $\Id$ is the $d$-dimensional identity matrix and $c_{0,1}, c_{0,0} > 0$. We denote $\Pbb^\star = \projM^{\vareps, \vsra}(\Pbb)$. We have that $\Pbb_{1|t}$ is a Gaussian random variable with zero mean. We now compute its covariance matrix. First, we have that 
\begin{equation}
    \mathbb{E}[\bfX_t \bfX_1^\top] = (1-t) \mathbb{E}[\bfX_0 \bfX_1^\top] + t \mathbb{E}[\bfX_1 \bfX_1^\top] = [(1-t) c_{0,1}  + t ] \Id . 
\end{equation}
We also have that 
\begin{align}
    \mathbb{E}[\bfX_t \bfX_t^\top] &= (1-t)^2 \mathbb{E}[\bfX_0 \bfX_0^\top] + t(1-t) (\mathbb{E}[\bfX_1 \bfX_0^\top] + \mathbb{E}[\bfX_1 \bfX_0^\top]) + t^2 \mathbb{E}[\bfX_1 \bfX_1^\top] + 2t(1-t) \Id \\
    &= [(1-t)^2 c_{0,0} + t^2  + 2t(1-t) c_{0,1} + 2t(1-t)] \Id \\
    &= [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}] \Id . 
\end{align}
Therefore, we get that for any $t \in [0,1]$ and $x_t  \in \rset^d$  
\begin{equation}
    \mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] = ([(1-t) c_{0,1}  + t ] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}] ) x_t .
\end{equation}
Hence, we have that for any $t \in [0,1]$ and $x_t  \in \rset^d$ 
\begin{align}
    & \mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t = ([(1-t) c_{0,1}  + t ] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}] - 1) x_t \\
    & \qquad = ([(1-t) c_{0,1}  + t - 2t + t^2 - (1-t)^2 c_{0,0} - 2t(1-t) c_{0,1}] \\
    & \qquad \qquad / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}]) x_t \\
    & \qquad = ([(1-t)(1-2t) c_{0,1} - (1-t)^2 c_{0,0} - t(1-t)] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}]) x_t \\
    & \qquad = (1-t)([(1-2t) c_{0,1} - (1-t) c_{0,0} - t] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}]) x_t .
\end{align}
Finally, we have that for any $t \in [0,1]$ and $x_t  \in \rset^d$ 
\begin{align}
    & (\mathbb{E}_{\Pbb_{1|t}}[\bfX_1 \ | \ \bfX_t = x_t] - x_t)/(1-t) \\
    & \qquad \qquad = ([(1-2t) c_{0,1} - (1-t) c_{0,0} - t] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}]) x_t . \label{eq:conditional_expectation_duo_appendix}
\end{align}
Note that if we set $c_{0,1} = c^2$ and $c_{0,0} = 1$, we recover \cite[Lemma 13]{shi2023DSBM} with $\sigma = 2$. Denote $\Pbb^\star = \projM^{\vareps, \vsra}(\Pbb)$. Combining \eqref{eq:conditional_expectation_duo_appendix} and \eqref{eq:markov_proj_approximate} we get that $\Pbb^\star$ is associated with $(\bfX_t)_{t \in [0,1]}$ such that for any $t \in [0,1]$ we have
\begin{equation}
    \rmd \bfX_t = \{([(1-2t) c_{0,1} - (1-t) c_{0,0} - t] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}])  + \vareps \} \bfX_t \rmd t + \sqrt{2} \rmd \bfB_t . 
\end{equation}
Hence, we get that 
\begin{equation}
     \bfX_t = \exp[\tfrac{1}{2} H(t, c_{0,1}, c_{0,0}, \vareps)] \bfX_0 + (2 \int_0^t \exp[- H(s, c_{0,1}, c_{0,0}, \vareps)] \rmd s \exp[H(t, c_{0,1}, c_{0,0}, \vareps)])^{1/2} \bfZ ,
\end{equation}
where $\bfZ \sim \gN(0, \Id)$ is independent from $\bfX_0$ and for any $t \in [0,1]$, $c_{0,1}, c_{1,1}, \vareps > 0$ we have 
\begin{equation}
    H(t, c_{0,1}, c_{0,0}, \vareps) = 2 \int_0^t [(1-2t) c_{0,1} - (1-t) c_{0,0} - t] / [2t - t^2 + (1-t)^2 c_{0,0} + 2t(1-t) c_{0,1}] \rmd t + 2 \vareps t . 
\end{equation}
In addition, we define 
\begin{align}
    &g_{\mathrm{cov}}(c_{0,1}, c_{0,0}, \vareps) = \exp[\tfrac{1}{2}H(1, c_{0,1}, c_{0,0}, \vareps)] , \\
    & g_{\mathrm{var}}(c_{0,1}, c_{0,0}, \vareps) = \exp[H(1, c_{0,1}, c_{0,0}, \vareps)] \Bigl(1 + 2 \int_0^1 \exp[-H(t, c_{0,1}, c_{0,0}, \vareps)] \rmd t \Bigr) .
\end{align}
Hence, we have that 
\begin{equation}
    \mathbb{E}[\bfX_0 \bfX_1^\top] = g_{\mathrm{cov}}(c_{0,1}, c_{0,0}, \vareps) \Id , \qquad \mathbb{E}[\bfX_1 \bfX_1^\top] = g_{\mathrm{var}}(c_{0,1}, c_{0,0}, \vareps) \Id . \label{eq:update_equation_appendix}
\end{equation}
Remember that  $\Pbb^{0, \vsra} = \Pbb^{0, \vsla} = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$. In addition, for any $n \in \nset$, we have $\Pbb^{2n+2, \vsra} = \Pbb^{2n+1, \vsla}_{0,1} \Qbb_{|0,1}$ and $\Pbb^{2n+2, \vsla} = \Pbb^{2n+1, \vsra}_{0,1} \Qbb_{|0,1}$, i.e.~$\Pbb^{2n+2, \vsra} = \projsimpleR(\Pbb^{2n+1, \vsla})$ and $\Pbb^{2n+2, \vsla} = \projsimpleR(\Pbb^{2n+1, \vsra})$. In addition, we also have $\Pbb^{2n+1, \vsra} = \projM^{\vareps, \vsra}(\Pbb^{2n, \vsla})$ and $\Pbb^{2n+1, \vsla} = \projM^{\vareps, \vsla}(\Pbb^{2n, \vsra})$. We also define $(c_{0,1}^{n, \vsra}, c_{1,1}^{n, \vsra})_{n \in \nset}$ such that for any $n \in \nset$
\begin{equation}
    \mathbb{E}_{\Pbb^{2n, \vsra}}[\bfX_0 \bfX_1^\top] = c_{0,1}^{n, \vsra} \Id , \qquad \mathbb{E}_{\Pbb^{2n, \vsra}}[\bfX_1 \bfX_1^\top] = c_{1,1}^{n, \vsra} \Id . 
\end{equation}
Finally, we define $(c_{0,1}^{n, \vsla}, c_{1,1}^{n, \vsla})_{n \in \nset}$ such that for any $n \in \nset$
\begin{equation}
    \mathbb{E}_{\Pbb^{2n, \vsla}}[\bfX_0 \bfX_1^\top] = c_{0,1}^{n, \vsla} \Id , \qquad \mathbb{E}_{\Pbb^{2n, \vsla}}[\bfX_0 \bfX_0^\top] = c_{0,0}^{n, \vsla} \Id . 
\end{equation}
Using this definition and \eqref{eq:update_equation_appendix} we get that for any $n \in \nset$
\begin{align}
    &c_{0,1}^{n+1, \vsra} = g_{\mathrm{cov}}(c_{0,1}^{n, \vsla}, c_{0,0}^{n, \vsla}, \vareps) , \qquad c_{1,1}^{n+1, \vsra} = g_{\mathrm{var}}(c_{0,1}^{n, \vsla}, c_{0,0}^{n, \vsla}, \vareps) , \\
    &c_{0,1}^{n+1, \vsla} = g_{\mathrm{cov}}(c_{0,1}^{n, \vsra}, c_{1,1}^{n, \vsra}, \vareps) , \qquad c_{0,0}^{n+1, \vsla} = g_{\mathrm{var}}(c_{0,1}^{n, \vsra}, c_{1,1}^{n ,\vsra}, \vareps) .
\end{align}
In addition, we have that $c_{0,1}^{n, \vsla} = c_{0,1}^{n, \vsra} = 0$ and $c_{1,1}^{0, \vsra} = c_{0,0}^{0, \vsla} = 1$. This concludes the proof. 
\end{proof}

\paragraph{Error accumulation.} In \Cref{prop:forward_forward_appendix} and \Cref{prop:forward_backward_appendix}, we derive the sequences corresponding to the evolution of the variance and the covariance throughout the DSBM iterations in forward-forward mode or forward-backward mode. In what follows, we showcase the behavior of these sequences for different values of $\vareps > 0$. We recall that $\vareps$ corresponds to the error made in the Markov projection, i.e.~$\projM$ is replaced by $\projM^{\vareps, \vsra}$ in the forward-forward mode and $\projM$ is replaced by $\projM^{\vareps, \vsra}$ and $\projM^{\vareps, \vsla}$ in the forward-backward mode. First, if we consider the perfect scenario, i.e.~$\vareps = 0$, then we observe that both the forward-forward mode and the forward-backward mode satisfy that $\mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_1^\top] = \Id$, see \Cref{fig:error_accumulation_variance} and \Cref{fig:error_accumulation_covariance}. Additionally, we can show that in the perfect scenario, i.e.~$\vareps = 0$, then both the forward-forward mode and the forward-backward mode satisfy that $\lim_{n \to +\infty} \mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_0^\top] = (\sqrt{2} - 1) \Id$, see \Cref{fig:error_accumulation_variance} and \Cref{fig:error_accumulation_covariance}. However, as $\vareps$ increases the behavior between the forward-forward sequence and the forward-backward sequence significantly differs. More precisely, the error explodes as $\vareps$ increases along the DSBM iteration for the forward-forward mode. On the contrary, in the forward-backward mode, the error remains bounded along the DSBM iterations, see \Cref{fig:error_accumulation_variance} and \Cref{fig:error_accumulation_covariance}. 

\begin{figure}
    \centering
    \includegraphics[width=.45\linewidth]{img/error_accumulation_11_ff.png} \hfill
    \includegraphics[width=.45\linewidth]{img/error_accumulation_11_fb.png}
    \caption{Evolution of $(\|\mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_1^\top] - \Id\|)_{n \in \nset}$ in log-space along DSBM iterations (x-axis). Different curves correspond to different values of $\vareps$, i.e.~the larger $\vareps$ the larger the error in the Markovian projection. Left: evolution in the forward-forward mode. Right: evolution in the forward-backward mode.}
    \label{fig:error_accumulation_variance}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.45\linewidth]{img/error_accumulation_01_ff.png} \hfill
    \includegraphics[width=.45\linewidth]{img/error_accumulation_01_fb.png}
    \caption{Evolution of $(\|\mathbb{E}_{\Pbb^{2n}}[\bfX_1 \bfX_0^\top] - \Id\|)_{n \in \nset}$ in log-space along DSBM iterations (x-axis). Different curves correspond to different values of $\vareps$, i.e.~the larger $\vareps$ the larger the error in the Markovian projection. Left: evolution in the forward-forward mode. Right: evolution in the forward-backward mode.}
    \label{fig:error_accumulation_covariance}
\end{figure}

\section{Preconditioning of the loss function}
\label{sec:preconditioning_loss}

In this section, we provide details on the scaling of the loss function we implement when training our online version of DSBM. We adapt the method of \cite[Appendix B.2]{karras2022elucidating} to the case of bridge matching. We only present our derivations in the case of the forward training of the online version of DSBM, i.e.~\eqref{eq:update_alpha_imf}. The preconditioning of the loss described in this setting can be readily extended to the forward-backward loss we consider in practice, i.e.~the parametric version of \eqref{eq:loss_function_bidirectional}.

We consider the following objective function for any $t  \in [0,1]$
\begin{equation}
\label{eq:rescaled_loss_function}
    \ell_t = \lambda_t \mathbb{E}_{\Pbb}[\| c_t^o \mathrm{nn}_t^\theta(c_t^i \bfX_t) + c_t^s \bfX_t - \tfrac{\bfX_1 - \bfX_t}{1-t} \|^2] . 
\end{equation}
We also define for any $t \in [0,1]$ and $x_t \in \rset^d$, $v_t^\theta(x_t) = c_t^o \mathrm{nn}_t^\theta(c_t^i x_t) + c_t^s x_t$. Hence, $c_t^i$ is an input scaling function, $c_t^o$ is an output scaling function and $c_t^s$ is a skip-connection function. 
During the training of the online version of DSBM, $\Pbb$ will be given by $\Pbb^n$, where $\Pbb^n = \Pbb_{v^{\theta_n}}$, where the sequence $(\theta_n)_{n \in \nset}$ is given by \eqref{eq:update_alpha_imf}. Here, we apply the principles of \cite{karras2022elucidating} to the case where $\Pbb = (\pi_0 \otimes \pi_1) \Qbb_{|0,1}$, i.e.~at initialisation of the sequence. In what follows, we assume that $\mathbb{E}_{\pi_0}[\| \bfX_0 \|^2] = \mathbb{E}_{\pi_1}[\| \bfX_1 \|^2] = d$. Note that our considerations can be generalised to $\mathbb{E}_{\pi_0}[\| \bfX_0 \|^2] = \sigma_0^2 d$ and $\mathbb{E}_{\pi_1}[\| \bfX_1 \|^2] = \sigma_1^2 d$. We also have that 
\begin{equation}
\label{eq:interpolation_preconditioning_appendix}
    \bfX_t = (1-t) \bfX_0 + t \bfX_1 + \sqrt{\vareps t (1-t)} \bfZ , \qquad \bfZ \sim \gN(0, \Id) . 
\end{equation}
Using \eqref{eq:interpolation_preconditioning_appendix}, we have that for any $t \in [0,1]$
\begin{align}
    \mathbb{E}_{\Pbb_t}[\| \bfX_t \|^2] &= (1-t)^2 \mathbb{E}_{\pi_0}[\| \bfX_0 \|^2] + t^2 \mathbb{E}_{\pi_1}[\| \bfX_1 \|^2] + \vareps t (1-t) d \\
    &= [(1-t)^2 + t^2 + \vareps t (1-t)] d . 
\end{align}
We set $c_t^i$ so that $\mathbb{E}[\| c_t^i \bfX_t \|^2] = d$ for every $t \in [0,1]$. Hence, we have that for any $t \in [0,1]$
\begin{equation}
    c_t^i = 1 / \sqrt{(1-t)^2 + t^2 + \vareps t (1-t)} . 
\end{equation}
Next, we rewrite \eqref{eq:rescaled_loss_function}. For any $t \in [0,1]$ we have that 
\begin{align}
    \ell_t &= \lambda_t \mathbb{E}_{\Pbb}[\| c_t^o \mathrm{nn}_t^\theta(c_t^i \bfX_t) + c_t^s \bfX_t - \tfrac{\bfX_1 - \bfX_t}{1-t} \|^2] \\
    &= (c_t^o)^2 \lambda_t \mathbb{E}_{\Pbb}[\| \mathrm{nn}_t^\theta(c_t^i \bfX_t) - [-c_t^s \bfX_t + \tfrac{\bfX_1 - \bfX_t}{1-t}]/c_t^o \|^2] \\
    &= (c_t^o)^2 \lambda_t \mathbb{E}_{\Pbb}[\| \mathrm{nn}_t^\theta(c_t^i \bfX_t) - [\tfrac{\bfX_1 - (1 + c_t^s(1-t))\bfX_t}{1-t}]/c_t^o \|^2]
\end{align}
Hence, we get that for any $t \in [0,1]$, $\bfT_t = [\tfrac{\bfX_1 - (1 + c_t^s(1-t))\bfX_t}{1-t}]/c_t^o$ is the target of the network in the regression loss. We are going to fix $c_t^o$ and $c_t^s$ such that i) $\mathbb{E}[\| \bfT_t \|^2] = d$, ii) $c_t^o$ is as small as possible in order not to minimise the error propagation made by the neural network. 
Using \eqref{eq:interpolation_preconditioning_appendix}, we have that for any $t \in [0,1]$
\begin{align}
    \mathbb{E}_{\Pbb_{1,t}}[\| \bfX_1 - (1 + c_t^s(1-t)) \bfX_t \|^2] &= (1 + c_t^s(1-t))^2 \mathbb{E}_{\Pbb_t}[\| \bfX_t \|^2]   + \mathbb{E}_{\pi_1}[\| \bfX_1 \|^2] \\
    & \qquad - 2(1+c_t^s(1-t))\mathbb{E}_{\Pbb_{1,t}}[\langle \bfX_t, \bfX_1 \rangle] \\
    &= (1 + c_t^s(1-t))^2 \mathbb{E}_{\Pbb_t}[\| \bfX_t \|^2] + d - 2(1+c_t^s(1-t)) t d 
\end{align}
Hence, we get that for any $t \in [0,1]$
\begin{equation}
    (c_t^o)^2 = ((1 + c_t^s(1-t))^2 \mathbb{E}_{\Pbb_t}[ \| \bfX_t \|^2]/d + 1 - 2(1+c_t^s(1-t)) t) / (1-t)^2 . 
\end{equation}
We now minimise $(c_t^o)^2$ with respect to $(1+c_t^s(1-t))$. We get that 
\begin{equation}
    1 + c_t^s(1-t) = t / (\mathbb{E}_{\Pbb_t}[ \| \bfX_t \|^2] /d) .
\end{equation}
Hence, we get that $c_t^s = t/[(1-t)((1-t)^2 + t^2 + \vareps t (1-t))]-1/(1-t)$. With that choice, we get that for any $t \in [0,1]$
\begin{align}
    (c_t^o)^2 &= (1 - t^2/((1-t)^2 + t^2 + \vareps t (1-t)))/(1-t)^2 . 
\end{align}
    In \cite{karras2022elucidating}, the weighting function $\lambda_t$ is set so that the weight in front of the regression loss is equal to one for all times $t \in [0,1]$. Hence, \cite{karras2022elucidating} suggests to set $\lambda_t = 1 / (c_t^o)^2$. However, in practice we observe better results by letting $\lambda_t = 1$. This means that the effective weight is given by $1/(c_t^o)^2$. Therefore, for any $t \in [0,1]$ we have 
    \begin{align}
        &(c_t^i)^2 = (1 + (\vareps -2)t(1-t))^{-1} , \\
        &c_t^s = ((\vareps - 2)t - 1) / (1 + (\vareps - 2)t(1-t)) , \\
        &(c_t^o)^2 = (1 + t + (\vareps - 2)t (1-t)) / (1 - t) . 
    \end{align}
    
\begin{figure}
    \centering
    \includegraphics[width=.31\linewidth]{img/ci_bridge.png} \hfill
    \includegraphics[width=.31\linewidth]{img/cs_bridge.png} \hfill
    \includegraphics[width=.31\linewidth]{img/co_sq_bridge.png}
    \caption{From left to right $((c_t^i)^2)_{t \in [0,1]}$, $(c_t^s)_{t \in [0,1]}$ and $((c_t^o)^2)_{t \in [0,1]}$ for different values of $\vareps \in [0, 10]$.}
    \label{fig:my_label}
\end{figure}


\section{Experimental details}
\label{sec:experimental_details}

In this section, we delve deeper into the specifics of each experiment, implementation details,  and share additional results.

We consider two ways of parameterising the vector fields: as in DSBM, we can use two separate neural networks to approximate the forward and backward vector fields, or we can use a single neural network that is conditioned on the direction. In the latter case, we do the conditioning in a similar fashion to how DDM's neural networks, U-Nets or MLPs, are conditioned on time embeddings. After all, if we work with continuous time variables $t \in [0, 1]$, then the direction signal $s \in \{0, 1\}$ can be thought of as a target time. Thus, we perform the same initial transformations on $t$ and $s$, i.e.~computing sinusoidal embeddings followed by a 2-layer MLP, and use the concatenated outputs in adaptive group normalisation layers~\citep{dhariwal2021diffusion, hudson2023soda, perez2018film}.          

To optimise our networks, we use Adam~\citep{kingma:adam} with $\beta=(0.9, 0.999)$, and we modify the gradients to keep their global norm below 1.0. We re-initialise the optimiser's state when the finetuning phase starts. 

All image samples in the paper are generated using EMA parameters as it has been known to increase the visual quality of resulting images~\citep{song_improved_2020}. Sampling is also the integral part of DSBM's finetuning stage, both iterative and online. Here, we have two options: sample with EMA or non-EMA parameters. The non-EMA sampling might be easier to implement, while EMA sampling results in a more stable training and slightly better quality, e.g.~see AFHQ samples in \Cref{fig:afhq64_bidirectional_online_non_ema} and \Cref{fig:afhq64_bidirectional_online_ema} for comparison. 

For every model used in the paper, we provide hyperparameters in \Cref{table:hypers}.

\begin{table}[htb]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{l c c c c c}
\toprule
 & 2D & Gaussian & MNIST & AFHQ-64 & AFHQ-256\\
\midrule
Channels/hidden units & 256 & 256 & 64 & 128 & 128  \\
Depth & 3 & 3 & 2 & 4 & 4 \\
Channels multiple & n/a  & n/a & 1, 2, 2 & 1,2,3,4 & 1, 1, 2, 2, 3, 4 \\
Heads & n/a  & n/a & n/a & 4 & 4 \\
Heads channels & n/a & n/a & n/a & 64 & 64 \\ 
Attention resolution & n/a & n/a & n/a & 32, 16, 8 &  32, 16, 8\\
Dropout & 0.0 & 0.0 & 0.1 & 0.0 & 0.0\\
Batch size & 128 & 256 & 128 & 128 & 128 \\
Pretraining iterations & 50K & 10K  & 100K & 100K & 100K  \\
Finetuning iterations & 150K & 40K & 150K & 20K & 20K \\
Pretraining learning rate & 1e-4 & 1e-4 & 1e-4 & 2e-4 & 2e-4\\
Finetuning learning rate & 1e-5 & 1e-4 & 1e-4 & 2e-4 & 2e-4\\
Pretraining warmup steps & n/a & n/a & n/a & 5K & 5K\\
EMA decay & n/a & n/a & 0.999 & 0.999 & 0.999 \\
Parameters count & 133.4K & 371K  & 8.8M & 194.4M & 226.7M \\
\bottomrule
\end{tabular}
}
\vspace{1mm}
\caption{Hyper-parameters for each model. Note that for 2-networks models, the architectural hyper-parameters describe only one of the two identical networks. Approximate parameters counts are given for bidirectional networks, except for the Gaussian case, where we only experimented with a 2-networks model.}
\label{table:hypers}
\end{table}


\subsection{2D Experiments}

In addition to the experiments presented in the main text, we test our models in the simplest 2D data settings used in \citet{tong_conditional_2023} and \citet{shi2023DSBM}. Note, that low-dimensional datasets might not be the ideal showcase for $\alpha$-DSBM given that one can successfully employ less computationally demanding techniques based on minibatch-OT methods~\citep{tong2024simulationfree}. 

The results of our bidirectional model finetuned with online updates are given in \Cref{table:2d}. During finetuning, we generate samples using 100 EulerâMaruyama steps to solve the forward and backward SDEs.
At test time, we solve the forward probability flow ODE (PF-ODE) given by:
\begin{equation}
\label{eq:pf-ode}
    \rmd\bfX_t = \frac{1}{2} \big[ v_\theta(1, t, \bfX_t) - v_\theta(0, 1 - t, \bfX_t) \big] \rmd t, \qquad \bfX_0 \sim \pi_0. 
\end{equation}
To evaluate model fit, we compute 2-Wasserstein distance between the true and generated samples (generated with 20 Euler steps). Additionally, we estimate path energy as a measure of trajectory simplicity: $\mathbb{E}_{\bfX_0\sim \pi_0}[\int_0^1 \| v_\theta(t, \bfX_t)\|^2 \rmd t]$ where $v_\theta(t, \bfX_t)$ is the drift of PF-ODE in \eqref{eq:pf-ode}, and the integral is approximated using 100 steps.
We have made a deliberate effort to closely replicate the experimental setup of \citet{shi2023DSBM} to ensure the comparability of our results. However, as illustrated in \Cref{fig:w2}, 2-Wasserstein distance can be very noisy even with 10K samples in the test set. To mitigate this variance, we averaged the 2-Wasserstein distance across five random sets of 10K samples per run, and then averaged these results across multiple runs.  Despite these measures, we recommend a future redesign of these 2D experiments to facilitate more robust comparisons between methods.       
\begin{table}[htb]
 \resizebox{\linewidth}{!}{
 \centering
    \begin{tabular}{lllllllll}
    \multirow{2}{*}{Method} & \multicolumn{4}{c}{2-Wasserstein} & \multicolumn{4}{c}{Path energy} \\
    \cmidrule(lr){2-5}\cmidrule(lr){6-9}
                            & $\gN \rightarrow$  moons         & $\gN \rightarrow$ scurve & $\gN \rightarrow$ 8gaussians & moons $\rightarrow$ 8gaussians & $\gN \rightarrow$moons & $\gN \rightarrow$scurve & $\gN \rightarrow$8gaussians & moons $\rightarrow$ 8gaussians  \\
      \midrule
    % DSB* & 0.190\textpm 0.049 & 0.272\textpm 0.065 & 0.411\textpm 0.084 & 0.987\textpm 0.324 & - & - & - & -\\
      DSBM-IMF* & 0.144\textpm 0.024 & 0.145\textpm 0.037 & 0.338\textpm 0.091 & 0.838\textpm 0.098 & 1.580\textpm 0.036 & 2.092\textpm 0.053 & 14.81\textpm 0.255 & 41.00\textpm 1.495 \\
      OT-CFM~\citep{tong_conditional_2023}* & 0.111\textpm0.005 & 0.102\textpm0.013 & 0.253\textpm0.040 & 0.716\textpm0.187 & 1.178\textpm0.020 & 1.577\textpm0.036 & 15.10\textpm0.215 & 30.50\textpm 0.626\\
    \midrule
      $\alpha$-DSBM & 0.168 \textpm 0.011 & 0.213\textpm0.031 & 0.292\textpm0.047 & 1.374\textpm0.286 & 1.439 \textpm 0.024 & 2.052 \textpm0.025 & 15.038\textpm0.150 & 37.626\textpm0.590 \\
      \bottomrule
    \end{tabular}
    }
    \vspace{1mm}
    \caption{2-Wasserstein distance and path energy for the 2D experiments. We report means $\pm1$ standard deviations across 5 random seeds. DSBM-IMF* and OT-CFM* results are copied from \cite{shi2023DSBM}.}
    \label{table:2d}
\end{table}


\begin{figure}[htbp]
    \centering 
   {\includegraphics[width=0.4\textwidth]{img/w2_hist.png}}
    \caption{A histogram of 2-Wasserstein distances for the `moons$\rightarrow$ 8gaussians' task. These distances are calculated between 10K samples from a finetuned $\alpha$-DSBM model and 8gaussians distribution, with both sets generated using 100 different random seeds. The wide spread of scores indicates that 2-Wasserstein distance, even computed on 10K samples, may not be an ideal metric for evaluating model fit in this context.} 
    \label{fig:w2} 
\end{figure}



\subsection{Gaussian data}

%% Valentin:
% We parameterise the vector-field with a neural network composed of two blocks of Residual MultiLayer Perceptron (ResMLP). Each ResMLP first applies a layer normalisation operation on the neural activations. Then, the ELU activation function is applied followed by a linear layer with 64 hidden units. We apply a linear layer to the time embeddings and add them to the current activations. Finally, we apply a normalisation layer, followed by ELU activation and a linear layer whose output dimension is the same as the input of the ResMLP. Finally, we add the inputs to the obtained activations. Before the ResMLP blocks we process the inputs with a linear layer whose output dimension is 64. After the ResMLP blocks, we apply some layer normalisation and a final linear layer to output a vector-field with appropriate dimension.
% The time is processed using sinusoidal time embedding with 256 features and a scale of 1000.

%% Ira
To parameterise the forward and backward drifts, we use a 2-layer MLP network with 256 hidden units. To process time variables, we compute sinusoidal time embeddings, followed by a 2-layer MLP with 256 hidden units and 50 output units. The resulting time embeddings are then concatenated with $\bfX_t$, so the drift networks receive 100-dimensional input vectors.

For iterative DSBM finetuning, we perform 40K steps with varying number of outer iterations, i.e. when we switch between training the forward and the backward networks. Alternating every 5K steps, corresponds to 8 outer DSBM iteration. Similarly, changing the direction every 1K steps, leads to 40 outer iterations.    

We do not have a cache dataloader like in the original DSBM implementation\footnote{\url{https://github.com/yuyang-shi/dsbm-pytorch}}, thus we generate training samples on the fly by sampling either from the forward or the backward network. For this simple task, we also do not use EMA.

During training and evaluation, we use EulerâMaruyama method with 100 equidistant time steps between 0 and 1. The covariance is evaluated using 10K samples. 

%% Extra images
% \begin{figure}[htbp] 
%     \centering 
%     \subfloat[Finetuning learning rate 1e-4]{\includegraphics[width=0.5\textwidth]{img/gaussian/gaussian_ira_100K_lr1e-4_small.png}}\hfill
%     \subfloat[Finetuning learning rate 1e-5]{\includegraphics[width=0.5\textwidth]{img/gaussian/gaussian_ira_100K_lr1e-5_small.png}}\hfill
%     \caption{Iterative vs. online DSBM convergence depending on the learning rate during finetuning.} 
%     \label{fig:mnist_2net} 
% \end{figure}


\subsection{MNIST $\leftrightarrow$ EMNIST transfer}
\label{sec:experimental_details_mnist}

We closely follow the setup of \cite{shi2023DSBM} and \cite{debortoli2021diffusion}, and train the models to transfer between 10 EMNIST letters, A-E and a-e, and 10 MNIST digits (CC BY-ND 4.0 license). We use the same U-Net architecture with hyperparameters given in~\Cref{table:hypers}. 

For DSBM finetuning, we perform 30 outer iterations, i.e.~alternating between training the forward and the backward networks, while at each outer iteration a network is trained for 5000 steps. We do not have a cache dataloader and generate training samples on the fly by sampling either from the forward or the backward network with EMA parameters. 

During training and evaluation, we use EulerâMaruyama method with 30 equidistant time steps between 0 and 1. For evaluation, we compute FID based on the whole MNIST training set of 60000 examples and a set of 4000 samples that were initialised from each test image in the EMNIST dataset. MSD is computed between 4000 initial EMNIST test examples and their corresponding MNIST samples.  

In Figures \ref{fig:mnist_2net}--\ref{fig:emnist_bidirectional}, we provide forward and backward samples, i.e.~EMNIST $\rightarrow$ MNIST and MNIST $\rightarrow$ EMNIST, from models that differ in parameterisation, finetuning methods, and sampling strategy. For all the models above, we used $\vareps=1$. \Cref{fig:mnist_sigma_sweep_samples} illustrated the behaviour of the samples when we sweep over the $\vareps$ hyperparameter.   

Pretraining a bidirectional model on 4 v3 TPUs takes 1 hour, while the online finetuning stage requires 4 hours on 16 v3 TPUs. The number of pretraining and finetuning steps is chosen to match the experimental setup of \cite{shi2023DSBM}. 

\begin{figure}[htbp] 
    \centering 
    \subfloat[Initial EMNIST letters]{\includegraphics[width=0.25\textwidth]{img/mnist/emnist.png}}\hfill
    \subfloat[Bridge matching: \\ FID=6.02, MSD=0.564]{\includegraphics[width=0.25\textwidth]{img/mnist/mnist_108142214.png}}\hfill
    \subfloat[Bridge matching \\ + DSBM finetuning: \\ FID=5.25, MSD=0.345]{\includegraphics[width=0.25\textwidth]{img/mnist/mnist_108411896.png}}
    \subfloat[Bridge matching \\ + online finetuning: \\ FID=4.28, MSD=0.368]{\includegraphics[width=0.25\textwidth]{img/mnist/mnist_108539545.png}}
    \caption{EMNIST to MNIST transfer with a 2-networks model. } 
    \label{fig:mnist_2net} 
\end{figure}

\begin{figure}[htbp]
    \centering 
    \subfloat[Initial EMNIST letters]{\includegraphics[width=0.25\textwidth]{img/mnist/emnist.png}}\hfill
    \subfloat[Bridge matching: \\ FID=6.33, MSD=0.572 ]{\includegraphics[width=0.25\textwidth]{img/mnist/mnist_108137951.png}}\hfill
    \subfloat[Bridge matching \\ + online non-EMA  \\ finetuning: \\ FID=4.57, MSD=0.369]{\includegraphics[width=0.25\textwidth]{img/mnist/mnist_108413796.png}}
    \subfloat[Bridge matching \\ + online finetuning: \\ FID=4.39, MSD=0.387]{\includegraphics[width=0.25\textwidth]{img/mnist/mnist_108412875.png}}
    \caption{EMNIST to MNIST transfer with a bidirectional model.} 
    \label{fig:mnist_bidirectional} 
\end{figure}

\begin{figure}[htbp] 
    \centering 
    \subfloat[Initial MNIST digits]{\includegraphics[width=0.25\textwidth]{img/mnist/mnist.png}}\hfill
    \subfloat[Bridge matching: \\ FID=7.50, MSD=0.553]{\includegraphics[width=0.25\textwidth]{img/mnist/emnist_108142214.png}}\hfill
    \subfloat[Bridge matching \\ + DSBM finetuning: \\ FID=3.56, MSD=0.330]{\includegraphics[width=0.25\textwidth]{img/mnist/emnist_108411896.png}}
    \subfloat[Bridge matching \\ + online finetuning: \\ FID=3.67, MSD=0.357]{\includegraphics[width=0.25\textwidth]{img/mnist/emnist_108539545.png}}
    \caption{MNIST to EMNIST transfer with a 2-networks model. } 
    \label{fig:emnist_2net} 
\end{figure}

\begin{figure}[htbp]
    \centering 
    \subfloat[Initial MNIST digits]{\includegraphics[width=0.25\textwidth]{img/mnist/mnist.png}}\hfill
    \subfloat[Bridge matching: \\ FID=7.97, MSD=0.572 ]{\includegraphics[width=0.25\textwidth]{img/mnist/emnist_108137951.png}}\hfill
    \subfloat[Bridge matching \\ + online non-EMA \\ finetuning: \\ FID=4.16, MSD=0.370]{\includegraphics[width=0.25\textwidth]{img/mnist/emnist_108413796.png}}
    \subfloat[Bridge matching \\ + online finetuning: \\ FID=3.97, MSD=0.392]{\includegraphics[width=0.25\textwidth]{img/mnist/emnist_108412875.png}}
    \caption{MNIST to EMNIST transfer with a bidirectional model.} 
    \label{fig:emnist_bidirectional} 
\end{figure}

\begin{figure*}[bt]
\centering
\subfloat[Base model]{\includegraphics[width=0.5\textwidth]{img/mnist/mnist_base.png}}\hfill
\subfloat[Finetuned model]{\includegraphics[width=0.5\textwidth]{img/mnist/mnist_finetuned.png}}
\caption{MNIST samples transferred from EMNIST letter inputs (top row) using base (pretrained) and fine-tuned models for different values of $\vareps$. Low noise values result in poor sample quality, particularly in the base model, which finetuning cannot fully rectify.  Conversely, excessively high $\vareps$ restricts information passing from the inputs to the outputs, leading to poor alignment. Additionally, high $\vareps$ increases blurriness due to increased noise levels, thus requiring more denoising steps.
}
\label{fig:mnist_sigma_sweep_samples}
\end{figure*}

\subsection{AFHQ: Cat $\leftrightarrow$ Wild}
\label{sec:experimental_details_afhq}

We consider the problem of image translation between Cat and Wild domains of AFHQ~\citedcc{choi2020starganv2}{CC BY-NC 4.0 DEED licence} as introduced by \cite{shi2023DSBM}. Each domain has approximately 5000 samples in the training set, and around 500 samples in the test set. We resize the original 512 $\times$ 512 images to 64$\times$64 or 256$\times$256 resolutions. 

Our U-Net~\citep{unet} implementation is based on \cite{ho_denoising_2020} with a few improvements suggested in \cite{dhariwal2021diffusion, song2020score} such as rescaling of skip connections by $\nicefrac{1}{\sqrt{2}}$, using residual blocks from BigGAN~\citep{brock2018large}, and convolution-based up- and downsampling. Hyperparameters are given in \Cref{table:hypers}.
Compared to the straightforward parameterisation of the vector fields, we obtained slightly better results using EDM preconditioning~\cite{karras2022elucidating}, which we derive in \Cref{sec:preconditioning_loss} for the case of bridge matching. 
During training, we use horizontal flips as a way to augment the data.  

During training and evaluation, we use EulerâMaruyama method with 100 equidistant time steps between 0 and 1. When evaluating the quality of Cat $\rightarrow$ Wild transfer, we compute FID based on the whole training set of 4576 examples in the Wild domain and a set of 480 samples that were initialised from test images in the Cat domain. LPIPS and MSD are computed between 480 initial Cat images and Wild samples from the model. The same procedure is followed when evaluating in the reverse direction from Wild to Cat. Given that train, and especially the test sets are small, the quantitative results for AFHQ are likely unreliable~\citep{unbiased_fid}. In \Cref{fig:afhq64_dsbm_vs_online} we provide samples from the models finetuned either with an iterative or an online method. While their FID scores are different, the samples look similar between the two models.  

As we discussed in the main text, hyperparameter $\vareps$ trades off the visual quality and alignment of the samples in the resulting transfer models. In \Cref{fig:afhq_sigma_sweep}, we provide AFHQ 64 $\times$ 64 samples for pretrained and finetuned models with different values of $\vareps$. In addition to its relation to EOT, from a DDM perspective, $\vareps$ can be seen as the controlling factor of the noise schedule. As observed by \cite{hoogeboom2023simple}, noise schedules should be adjusted for different image sizes by shifting the noise schedule of some reference resolution where it is proven to be successful.  
In our case, if we find a good value of $\vareps$ for 64 $\times$ 64 images, then a shifted $\vareps$ for the 256 $\times$ 256 resolution can be computed as  $\vareps_{256} = \vareps_{64} \left(\frac{256}{64} \right)^2$. Thus, if we choose $\sqrt\vareps=0.75$ for AFHQ-64, then for AFHQ-256, we can expect $\sqrt{\vareps} = 3.0$ to also work well. Samples from an AFHQ-256 model trained with $\sqrt{\vareps} = 3.0$ are given in \Cref{fig:afhq256_bidirectional_online_ema}.

On 16 v3 TPUs, the bidirectional base and finetuned AFHQ-64 models take 4 and 14 hours to train, respectively. For AFHQ-256, the base model trains for 15 hours, and finetuning takes an additional 37 hours. While we did not experiment with varying pretraining and fine-tuning iterations, these training times suggest that a longer pretraining stage followed by fewer fine-tuning steps may be desirable.

\begin{figure}[htbp] 
    \centering 
    \subfloat[Base model]{\includegraphics[width=0.5\textwidth]{img/afhq/afhq64_base_xid109840770_sigma_sweep_bwd.png}}\hfill
    \subfloat[Finetuned model]{\includegraphics[width=0.5\textwidth]{img/afhq/afhq64_finetune_xid109840770_sigma_sweep_bwd.png}}\hfill
    
    \caption{AFHQ 64 $\times$ 64 Wild $\rightarrow$ Cat transfer results for different values of $\sqrt{\vareps}$ in a bidirectional model before and after online finetuning. Low values of $\vareps$ lead to poor sample quality in both base and finetuned models. Excessively high $\vareps$ values impede information passing from the inputs to the outputs, resulting in poor alignment. High values of $\vareps$ also increase blurriness due to noisier SDE trajectories, thus requiring more denoising steps during sampling.} 
    \label{fig:afhq_sigma_sweep} 
\end{figure}


\begin{figure}[htbp] 
    \centering 
    \subfloat[Base model]{\includegraphics[width=0.5\textwidth]{img/afhq/afhq64_nfe_sweep_base.png}}\hfill
    \subfloat[Finetuned model]{\includegraphics[width=0.5\textwidth]{img/afhq/afhq64_nfe_sweep_finetuned.png}}\hfill
    \caption{AFHQ 64 $\times$ 64 Wild $\rightarrow$ Cat transfer results for varying number of function evaluations (equivalent to time discretisation steps in the Euler-Maruyama method) in a bidirectional model with $\sqrt\vareps=0.75$, both  before and after online finetuning. Post-finetuning, clearer images are achievable with fewer steps. This observation aligns with findings from Rectified Flows~\citep{liu_flow_2023}.} 
    \label{fig:afhq_nfe_sweep} 
\end{figure}




\begin{figure}[htbp] 
    \centering 
    \subfloat[Iterative finetuning: Cat $\rightarrow$ Wild. \\ FID=27.76, LPIPS=0.503, MSD=0.093]{\includegraphics[width=0.49\textwidth ]{img/afhq/afhq64_dsbm_xid110331534_fwd_1_crop.png}}\hfill
    \subfloat[Iterative finetuning: Wild $\rightarrow$ Cat. \\ FID=25.24, LPIPS=0.483, MSD=0.094]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_dsbm_xid110334501_bwd_1.png}}\hfill
    
    \subfloat[Online finetuning: Cat $\rightarrow$ Wild. \\ FID=32.12, LPIPS=0.503, MSD=0.097]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_dsbm_online_xid110440242_fwd_1_crop.png}}\hfill
    \subfloat[Online finetuning: Wild $\rightarrow$ Cat.\\ FID=27.32, LPIPS=0.485, MSD=0.116]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_dsbm_online_xid110440184_bwd_1_crop.png}}\hfill
    
    
    \caption{Samples and metrics from a 2-networks model architecture finetuned with DSBM's iterative procedure vs online finetuning. Within each two rows, initial and transferred samples are on the top and bottom respectively. } 
    \label{fig:afhq64_dsbm_vs_online} 
\end{figure}


\begin{figure}[htbp] 
    \centering 
    \subfloat[Forward: Cat $\rightarrow$ Wild]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_online_xid110967624_fwd_0.png}}\hfill
    \subfloat[Backward: Wild $\rightarrow$ Cat]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_online_xid110967789_bwd_0.png}}\hfill
    \caption{Uncurated samples for AFHQ 64 $\times$ 64 transfer in a bidirectional model with online finetuning with non-EMA sampling and $\sqrt{\vareps}=0.75$. Within each two rows, initial and transferred samples are on the top and bottom respectively.} 
    \label{fig:afhq64_bidirectional_online_non_ema} 
\end{figure}



\begin{figure}[htbp] 
    \centering 
    \subfloat[Forward: Cat $\rightarrow$ Wild]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_online_ema_xid110331223_fwd_0.png}}\hfill
    \subfloat[Backward: Wild $\rightarrow$ Cat]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_online_ema_xid110334871_bwd_0.png}}\hfill
    \caption{Uncurated samples for AFHQ 64 $\times$ 64 transfer in a bidirectional model with online finetuning and $\sqrt{\vareps}=0.75$. Within each two rows, initial and transferred samples are on the top and bottom respectively.} 
    \label{fig:afhq64_bidirectional_online_ema} 
\end{figure}


\begin{figure}[htbp] 
    \centering 
    
    \subfloat[Forward: Cat $\rightarrow$ Wild with inputs from Wild.]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_sigma_sweep_wild2wild_finetuned_from_xid109840770_compressed.png}}\hfill
    \subfloat[Backward: Wild $\rightarrow$ Cat with inputs from Cat.]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_sigma_sweep_cat2cat_finetuned_from_xid109840770_compressed.png}}\hfill
    \caption{Samples for AFHQ 64 $\times$ 64 transfer in bidirectional models with online finetuning and different values of $\vareps$. The models are only trained on Cat and Wild domains, $\pi_0$ and $\pi_1$, respectively. Thus, in the forward direction the models expect Cat samples as inputs at $t=0$, and transfer them to the Wild domain at $t=1$. The reverse transfer holds in the backward direction. Here, we test the models' behaviour when inputs do not come from the same distribution as during training: we feed Wild samples in the forward direction, and Cat samples in the backward, which is the opposite from what the models expect. 
    Ideally, the model should leave these inputs unchanged, which it does to varying degrees depending on $\vareps$, variance of the Gaussian noise. As we increase $\vareps$, less information can pass from the input to the output, thus making them less alike.   
    } 
    \label{fig:afhq64_reverse} 
\end{figure}


\begin{figure}[htbp] 
    \centering 
    \subfloat[Forward: Cat $\rightarrow$ Wild with inputs from Dog.]{\includegraphics[width=0.49\textwidth ]{img/afhq/afhq64_xid111132078_dog2wild_4.png}}\hfill
    \subfloat[Backward: Wild $\rightarrow$ Cat with inputs from Dog.]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq64_xid111131875_dog2cat_4.png}}\hfill
    \caption{Samples for AFHQ 64 $\times$ 64 transfer in a bidirectional model with online finetuning and $\sqrt \vareps=2.0$. The model is only trained on Cat and Wild domains, $\pi_0$ and $\pi_1$, respectively. Thus, in the forward direction the model expects Cat samples as inputs at $t=0$, and transfers them to the Wild domain at $t=1$. The reverse holds in the backward direction.
    Notably, the model generalises well to the unseen AFHQ Dog domain, often producing high-quality translations. These results come from a model with $\sqrt{\vareps} = 2.0$, which is higher than our chosen default value of $\sqrt{\vareps} = 0.75$. Higher noise allows the model to better deal with out-of-distribution inputs.      
    } 
    \label{fig:afhq64_dogs} 
\end{figure}






\begin{figure}[htbp] 
    \centering 
    \subfloat[Forward: Cat $\rightarrow$ Wild]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq256_xid110332915_fwd_3.png}}\hfill
    \subfloat[Backward: Wild $\rightarrow$ Cat]{\includegraphics[width=0.49\textwidth]{img/afhq/afhq256_xid110438824_bwd_3.png}}\hfill
    \caption{Uncurated samples for AFHQ 256 $\times$ 256 transfer in a bidirectional model with online finetuning and $\sqrt{\vareps}=3$. Within each two rows, initial and transferred samples are on the top and bottom respectively.} 
    \label{fig:afhq256_bidirectional_online_ema} 
\end{figure}



\FloatBarrier
\newpage
\section*{NeurIPS Paper Checklist}

\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{}  % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: our main theoretical and experimental contributions are claimed in the abstract and demonstrated in the paper. We summarize our main contributions hereafter. Theoretically, we identify a new family of sequence of path measures related to the IMF algorithm, called $\alpha$-IMF. We show that these sequences correspond to non-parametric updates. We then introduce a parametric update that corresponds to an online version of the DSBM algorithm. We show that our procedure retains the favorable properties of DSBM while not requiring the expensive repeated inner minimisation procedure of DSBM.

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The limitations are addressed in the discussion section. The main limitation of our algorithm is that it is not a sampling free methodology. In future work, we would like to see how to mitigate the fact that our algorithm depends on some self-play.

\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: All theoretical results are proven in the supplementary material, see \Cref{sec:theoretical_results_appendix}.

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Full experimental details are provided in \Cref{sec:experimental_details}.


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Due to IP restrictions, we cannot share the codebase used for this paper. However, we plan to release some notebooks in order to reproduce experiments in a small scale setting. 



\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimiser, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Full experimental details are provided in \Cref{sec:experimental_details}.

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: All metrics are computed using multiple random seeds and error bars are provided. 


\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: Full details on the compute requirements are given in \Cref{sec:experimental_details}.

    
\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: After careful review of the NeurIPS Code of Ethics, we can ensure that the research presented in this paper conforms with the Code of Ethics in every respect. Indeed, we see no immediate safety, security, discrimination, surveillance, deception, harassment, environment, human rights or bias and fairness concerns to our work. In addition, we release details and documentation regarding the datasets and models used. We disclose essential details for reproducibility and have ensured that our work is legally compliant. 



\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: This paper addresses the problem of unpaired dataset translation and proposes an improvement to the DSBM methodology. As the current paper is mostly theoretical and methodological we do not see immediate societal impact of this work and therefore do not discuss these issues. However, we acknowledge that large scale implementation of our algorithm might suffer from the same societal biases as generative models. We hope to address the limitations of such models when turning to more experimental work.

    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper poses no such risks.


\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: We have referenced the license of the datasets we use and cite the original papers that produced the code packages and datasets that we use in that paper. 


\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not release new assets.

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.


\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification:  The paper does not involve crowdsourcing nor research with human subjects.


\end{enumerate}



\end{document}
