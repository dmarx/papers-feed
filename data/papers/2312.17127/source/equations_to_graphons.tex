
\section{From program equations to graphons}\label{sec:prog-eq-to-graphons}

The graph interface for the probabilistic programming language (Ex.~\ref{ex:interfaces}(3))
does not have one fixed equational theory. Rather, we want to consider different equational theories for the language, corresponding to different implementations of the interface for the graph (see also \S\ref{sec:intro:eqthys}).
We now show how the different equational theories for the graph language each
give rise to a graphon, by building adjacency matrices for finite graphs~(shown in \eqref{eqn:programgraph}). To do this, we set up the well-behaved equational theories (\S\ref{sec:bernoulli-base}), recall the connection between graphons and finite random graphs (\S\ref{sec:graphons}), and then show the main result (\S\ref{sec:program-equivalence-graphons}, Theorem~\ref{thm:eq-theory-to-graphon}). 


%  \todo[inline]{Comment that the geometric example satisfies these conditions.}
\subsection{Graphons as Consistent and Local Random Graph Models}
\label{sec:graphons}
For all $n ≥ 1$, let $[n]$ be the set $\{1,…,n\}$. (We sometimes omit
the square brackets, when it is clear.)
A simple undirected graph $g$ with $n$ nodes can be represented by its adjacency matrix $A_g\in 2^{[n]^2}$ such that $A_g(i,i)=0$ and $A_g(i,j)= A_g(j,i)$. Henceforth, we will assume that finite graphs are simple and undirected, unless otherwise stated. 
A random finite graph, then, has a probability distribution in
$\DistM\big(2^{[n]^2}\big)$ that only assigns non-zero probability to adjacency matrices.

\begin{definition}[e.g.~{\cite[\S11.2.1]{MR3012035}}]\label{def:rgm}
        %[Random graph model]
  A \emph{random graph model} is a sequence of distributions of random finite graphs of the form:
\begin{equation*}
  p_1\in \DistM\big(2^{[1]^2}\big),\,
  p_2\in \DistM\big(2^{[2]^2}\big),\,
  \dots,\, 
  p_n\in \DistM\big(2^{[n]^2}\big),
  \dots 
\end{equation*}

We say such a sequence is
\begin{itemize}
\item \emph{exchangeable} if each of its elements is invariant under permuting nodes:
  for every $n$ and bijection $\sigma \colon [n]\to [n]$, we have 
  $\DistM\big(2^{(\sigma^2)}\big)(p_n)=p_n$ (where
  $2^{(\sigma^2)}: 2^{[n]^2}\to 2^{[n]^2}$ is the function that
  permutes the rows and columns according to $\sigma$;
  we are regarding $\DistM$ as a covariant functor, Def.~\ref{def:distribution-monad}, and $2^{(-)}$ as a contravariant functor);
\item \emph{consistent} if the sequence is related by marginals:
  for every $n$ and for the inclusion function $\iota：[n] ↪ [n+1]$,
  $\DistM\big(2^{(\iota^2)}\big)(p_{n+1})=p_n$ (where
  $2^{(\iota^2)}:2^{([n+1]^2)}\to 2^{[n]^2}$ is the evident projection);
\item \emph{local} if the subgraphs are independent:
  if $A \subseteq [n]$ and $B \subseteq [n]$ are disjoint,
  then we have an injective function ${\jmath\colon A^2 + B^2\hookrightarrow [n]^2}$, and
  $\DistM\big(2^{\jmath}\big)(p_{n})\in \DistM\big(2^{(A^2)}\times 2^{(B^2)}\big)$
  is a product measure $p_A\otimes p_B$
(where \raisebox{0pt}[0pt]{$2^{\jmath}:2^{[n]^2}\to 2^{(A^2)}\times 2^{(B^2)}$} is
the evident pairing of projections).

\end{itemize}
\end{definition}

 \begin{definition}[e.g.~\cite{MR3012035}]
        A \emph{graphon} $W$ is a symmetric measurable function $W : [0,1]^2 \to [0,1]$.
\end{definition}
\newcommand{\graphonrgm}[3]{p_{#1,#2}(A_{#3})}
Given a graphon $W$, we can generate a finite simple undirected graph
$g$ with vertex set $[n]$ by sampling $n$ points $x_1, \ldots, x_n$
uniformly from $[0,1]$ and, then, including the edge $(i, j)$ with
probability $W(x_i, x_j)$ for all $1 ≤ i, j ≤ n$. This sampling
procedure defines a distribution over finite graphs: the probability
$\graphonrgm W n g$ of the graph $g = ([n], E)$ is:
%
\begin{equation}\int_{[0,1]^n} 
\!\prod_{(i,j) ∈ E} \!\!\! W(x_i, x_j) 
\prod_{(i,j) ∉ E} \!\!\!\! \left(1 - W(x_i, x_j) \right)
\dd(x_1\ldots x_n)\label{eqn:pwg}\end{equation}

%Graphons give rise to exchangeable, consistent, and local random graph models, and conversely, they all arise from graphons too:

\begin{proposition}[\hspace{1sp}\cite{lovasz-2006}, {\cite[\S11.2]{MR3012035}}]%[Exchangeable consistent local
                                %random graph models are equivalent to
                                %graphons~\cite{lovasz-2006}]
  \label{prop:lovasz}
  Every graphon generates an exchangeable, consistent, and local random
  graph model, by the sampling procedure of \eqref{eqn:pwg}.
  Conversely, every exchangeable, consistent, and local random graph
  model is of the form~$p_{W,n}$ for some graphon~$W$.
\end{proposition}
\begin{proof}[Note]
 There are various methods for constructing $W$ from an exchangeable, consistent and local random graph model, however all are highly non-trivial. A general idea is that $W$ is a kind of limit object. For examples see e.g.~\cite[\S11.3]{lovasz-2006} or \cite{tao-graphons}. Fortunately though, we will not need explicit constructions in this paper. 
\end{proof}

\subsection{Theories of Program Equivalence Induce Graphons}
\label{sec:program-equivalence-graphons}
In this section we consider the instance of the generic language with the graph interface
(Ex.~\ref{ex:interfaces}(3)):
\[
  \tvertex\qquad
  \tnew : \tunit \to \tvertex\qquad
  \tedge:\tvertex\ast\tvertex\to \tbool
\]
We consider a theory of program equivalence, i.e.~a distributive Markov
category with a distinguished object $\sem\tvertex$
and morphisms $\sem \tnew :1\to \sem \tvertex$ and $\sem \tedge:\sem \tvertex\otimes
\sem \tvertex\to 1+1$.
We make two assumptions about the theory:
\begin{itemize}
\item  The graphs are simple and undirected:
\begin{equation}
  \begin{aligned}
  x\colon \tvertex \vdash \tedge(x,x)\equiv \tfalse
\qquad x,y\colon \tvertex \vdash \tedge(x,y)\equiv \tedge(y,x)
\end{aligned}\label{eqn:simplegraph}\end{equation}
and $\tedge$ is deterministic.
\item The theory is Bernoulli based (\S\ref{sec:bernoulli-base}).\end{itemize}
For each $n\in\NN$, we can build a
random graph with $n$ vertices as follows.
We consider the following  program $t_n$:
\begin{equation}\begin{aligned}
\vdash\ & \letin {x_1}{\tnew()}{
        \dots {} }\letin {x_n}{\tnew()}{}
    \begin{pmatrix}\tedge (x_1,x_1)&{\dots}&\tedge (x_1,x_n)
\\
\vdots &&{\vdots}\\
\tedge(x_n,x_1)&\dots &\tedge(x_n,x_n)\end{pmatrix}:\tbool^{(n^2)}
\end{aligned}\label{eqn:programgraph}\end{equation}
(Here we use syntactic sugar, writing a matrix instead of iteratively using pairs.)

Because the equational theory is Bernoulli-based, the
interpretation $\sem {t_n}$ induces a probability distribution $\bbase\sem{t_n}$
on~$2^{(n^2)}$. For clarity, we elide $\bbase$ in what follows, since
it is faithful. 
%\todo[inline]{JK: $\translation{t_n}$ is not defined until \Cref{sec:graphons-to-equational-theories}}
%\todo[inline]{YK: can't we say the same of $\sem{t_n}$?}
\begin{proposition}
  Each random matrix in \eqref{eqn:programgraph} is a random adjacency matrix, \ie a random graph. 
\end{proposition}
\begin{proof}[Proof note]This follows from (\ref{eqn:simplegraph}).\end{proof}
\begin{theorem}\label{thm:eq-theory-to-graphon}
  For any Bernoulli-based equational theory, the random graph model $(\sem {t_n})_n$ in \eqref{eqn:programgraph}
  is exchangeable, consistent, and local. Thus, the equational theory
  induces a graphon. 
\end{theorem}
\begin{proof}[Proof] We denote the matrix in~\eqref{eqn:programgraph} by $(\tedge (x_i,x_j))_{i, j ∈ [n]}$.
\paragraph{Exchangeability} We show that the distribution $\sem {t_n}$ is invariant under relabeling the nodes. By commutativity of the $\mathsf{let}$ construct~\eqref{eq:let-comm}, the program
 \begin{align*}
 & t_n^σ ≝ 
 \letin {x_{σ^{-1}(1)}}{\tnew()}{\ldots\letin {x_{σ^{-1}(n)}}{\tnew()}{}}
(\tedge (x_i,x_j))_{i, j \in [n]}
 \end{align*}
 satisfies $\sem{t_n^σ} = \sem{t_n}$. Hence, $\DistM(2^{\sigma^2})(\sem{t_n}) = \sem{t_n^σ} = \sem{t_n}$, for every $n$ and bijection $σ：[n] → [n]$.

\paragraph{Consistency}
    % $\DistM(2^{\iota^2})(p_{n+1})=p_n$ for every $n$ and the inclusion function $\iota：n\to (n+1)$
    We define a macro $\subm_I$ in the graph programming language to extract a submatrix at the index set $I ⊆ [n]$: we have the (definitional) equality
    $$\subm_I((a_{i,j})_{i, j ∈ [n]}) ≝ (a_{i,j})_{i, j ∈ I} \quad \text{for } I ⊆ [n].$$
    We need to show that, if we delete the last node from a graph sampled from $\sem{t_{n+1}}$, the resulting graph has distribution $\sem{t_n}$. 
    This amounts to the affineness property (\ref{eq:let-affine}), as follows. 
    Let $g \sim \sem{t_{n+1}}$ be a random graph, and let $g' ≝ g\restrict{[n]}$ be the graph obtained by deleting the last node from $g$. Then clearly, the adjacency matrix of $g'$ is the adjacency matrix of $g$ where the last row and column have been removed, \ie $g'$ is sampled from the interpretation of the program:
    \begin{align*}
    t' &≝ \letin {x_1}{\tnew()}{\dots {} }\letin {x_n}{\tnew()}{}%\\
       %& \hspace*{1.6em}
         \letin {x_{n+1}}{\tnew()}{} \subm_{[n]}\big((\tedge(x_i,x_j))_{i,j ∈ [n+1]}\big)\\
    &\equiv \letin {x_1}{\tnew()}{\dots {}}\letin {x_n}{\tnew()}{}%\\
       %& \hspace*{1.3em}
         \letin {x_{n+1}}{\tnew()}{} (\tedge(x_i,x_j))_{i,j ∈ [n]}\\
            &\equiv \letin {x_1}{\tnew()}{\dots {}}\letin {x_n}{\tnew()}{} 
    (\tedge(x_i,x_j))_{i,j∈[n]} \hspace{3em} \text{(by \eqref{eq:let-affine})}\\ 
    &\equiv t_n.
    \end{align*}

\paragraph{Locality}
  % $\DistM(2^{\jmath})(p_{n})\in \DistM(2^{m_1^2}\times 2^{m_2^2})$ is the product measure $p_{m_1}\otimes p_{m_2}$ for every $m_1\subseteq n$ and $m_2\subseteq n$ are disjoint, and $\jmath\colon m_1^2+m_2^2\to n^2$ is an injection.
  Without loss of generality (by exchangeability and consistency), we need to show that for every random graph $g \sim \sem{t_n}$ and $1 < k < n$, the subgraphs $g_{A_k}, \, g_{B_k}$ respectively induced by the sets $A_k ≝ [k]$ and $B_k ≝ \{k+1,… , n\}$ are independent as random variables. 
  Let $\jmath$ be the injection $\jmath：A_k^2+B_k^2 ↪ n^2$, and $g' \sim \DistM(2^{\jmath})(\sem{t_n}) ∈ \DistM(2^{(A_k^2)}×2^{(B_k^2)})$. We want to show that $g'$ and $(g_{A_k}, g_{B_k})\sim \sem{t_k} ⊗ \sem{t_{n-k}}$ (by consistency) are equal in distribution.
%  Note that we have the following axiom of Moggi's computational $λ$-calculus (\hspace{1sp}\cite{dario-thesis,moggi-computational-lambda}), which holds by Prop.~\ref{prop:derivable-axioms}:\todo{This has moved}
  Modulo $α$-renaming,
  $(g_{A_k}, g_{B_k})$ is sampled from the interpretation of the
  program:%\todo{Could use few line breaks}
  \begingroup
  \allowdisplaybreaks
  \begin{align*}
  t' &≝ \big(\letin {x_1}{\tnew()}{\dots{} }\letin {x_k}{\tnew()}{}(\tedge(x_i,x_j))_{i,j ∈ [k]}, \\*
  & \hspace*{2em} \letin {x_{k+1}}{\tnew()}{\dots{} }\letin {x_n}{\tnew()}{} (\tedge(x_i,x_j))_{k+1 ≤ i,j ≤ n}\big)\\
  & ≡ \letin {u_1}{(\letin {x_1}{\tnew()}{\dots{} }\letin {x_k}{\tnew()}{}
    (\tedge(x_i,x_j))_{i,j ∈ A_k})}{}\\*
  & \phantom{{} \equiv {}} \letin {u_2}{(\letin {x_{k+1}}{\tnew()}{\dots{}}\letin {x_n}{\tnew()}{}(\tedge(x_i,x_j))_{i,j ∈ B_k})}{}(u_1, u_2) &&\text{(by \eqref{eq:moggi-let-pair})}\\
  &≡ \letin {x_1}{\tnew()}{\dots{} }\letin {x_k}{\tnew()}{}\letin {x_{k+1}}{\tnew()}{\dots{}}\letin {x_n}{\tnew()}{}&&\text{(\eqref{eq:let-assoc},\eqref{eq:let-comm})}\\*
  & \hspace*{1.3em} \letin {u_1}{\subm_{A_k}\big((\tedge(x_i,x_j))_{i,j ∈ [n]}\big)}{} 
\letin{u_2}{\subm_{B_k}\big((\tedge(x_i,x_j))_{i,j ∈ [n]}\big)}{} 
    (u_1, u_2)\hspace{-4cm}
      \\
  &≡ \letin {x_1}{\tnew()}{\dots{}}\letin {x_n}{\tnew()}{}\\*
  & \hspace*{1.3em} \letin {t}{(\tedge(x_i,x_j))_{i,j ∈ [n]}}{} \letin {u_1}{\subm_{A_k}(t)}{}\letin {u_2}{\subm_{B_k}(t)}{}(u_1, u_2) &&\text{(by \eqref{cd:let_val})}\\
  &≡ \letin {x_1}{\tnew()}{\dots{}}\letin {x_n}{\tnew()}{}\\*
  & \hspace*{1.3em} \letin {t}{(\tedge(x_i,x_j))_{i,j ∈ [n]}}{} 
  % & \hspace*{1.6em} \big(\begin{aligned}[t]
  %         & \subm_{A_k}((\tedge(x_i,x_j))_{i,j ∈ [n]}),\,
  %         \\
  %         & \subm_{B_k}((\tedge(x_i,x_j))_{i,j ∈ [n]})\big)
  % \end{aligned}
  % \\
  % &≡ \letin {x_1}{\tnew()}{\dots{}}\letin {x_n}{\tnew()}{}\\*
  % & \hspace*{1.3em} \letin {t}{(\tedge(x_i,x_j))_{i,j ∈ [n]}}{} \hspace{1.5em} \text{(by \eqref{cd:beta-eta}, $A_k \uplus B_k = [n]$)} \\*
\big(\subm_{A_k}(t),\, \subm_{B_k}(t)\big) &&\text{(by \eqref{eq:moggi-let-pair})}
  \\
  &≡ \letin {t}{\big(\letin {x_1}{\tnew()}{\dots{}} 
    \letin {x_n}{\tnew()}{} (\tedge(x_i,x_j))_{i,j ∈ [n]}\big)}{}\\*
& \hspace*{1.6em}     \big(\subm_{A_k}(t), \, \subm_{B_k}(t)) &&\text{(by \eqref{eq:let-assoc})}
  \end{align*}
  \endgroup
  and $g' \sim \DistM(2^{\jmath})(\sem{t_n})$ is indeed sampled from the
  interpretation of the latter program, %(by compositionality of
%  $\sem{-}$, Def.~\ref{def:bernoulli-based})
  which yields the result.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "popl24"
%%% End:
