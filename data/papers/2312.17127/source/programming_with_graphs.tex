\newcommand{\CatADet}{\CatA_{\mathsf{det}}}
\newcommand{\Kl}{\mathrm{Kl}}
\newcommand{\FP}{\mathrm{FP}}
\newcommand{\op}{^\mathrm{op}}

\section{Programming interfaces for random graphs: equational theories
and Markov categories}
In Section~\ref{sec:intro:graphs}, 
we considered probabilistic
programming over a graph interface. To make this formal, we now recall
syntax, types, and equational reasoning for simple probabilistic
programming languages.
We begin with a general syntax (\S\ref{sec:generic-ppl}), which can
accommodate various interfaces in the form of type and term
constants, including the interface for graphs (Ex.~\ref{ex:interfaces}(3)). 

We study different instantiations of the probabilistic programming
language in terms of the equational theories that they satisfy. We
consider two equivalent ways of understanding equational theories: as
distributive Markov categories (\S\ref{sec:markov-cats}) and in terms
of affine monads (\S\ref{sec:affine-monad}). Markov categories are a
  categorical formulation of probability theory (e.g.~\cite{fritz}), and affine
  monads arise in the categorical analysis of probability (e.g.~\cite{fgpt-weakly-affine,jacobs-commutative-effectus,kock:commutative-monads-as-a-theory-of-distributions})
  as well as in the semantics for probabilistic programming
  (e.g.~\cite{lazyppl,dsp-layer,amorim-markov}). 
We make a connection with traditional probability via the notion of
Bernoulli base (\S\ref{sec:bernoulli-base}).

Much of this section will be unsurprising to experts: the main purpose is to collect
definitions and results. The definition of \emph{distributive} Markov
category appears to be novel, and so we go over that definition and
correspondence with monads (Propositions~\ref{prop:distr-affine-monad} and~\ref{prop:markov-embed-kleisli}).
In Section~\ref{sec:quotients}, we give a construction for quotienting a distributive Markov category, which we will need in Section~\ref{sec:graphons-to-equational-theories}.
We include the result in the section because it may be of independent interest.
  

\subsection{Syntax for a Generic Probabilistic Programming Language}
\label{sec:generic-ppl}
Our generic probabilistic programming language is, very roughly, an idealized,
typed fragment of a typical language like Church~\cite{goodman2008church}. 
%
We start with a simple programming language (following
\cite{s-finite,prob-cbpv,dario-thesis} but also
\cite{moggi-computational-lambda}) with at least the following product
and sum type constructors:
\[ A,A_1,A_2,B ::= \tunit \s 0\s A_1 \ast A_2 \s A_1+A_2\s \dots 
\]
and terms, including the typical constructors and destructors but also
explicit sequencing ($\mathsf{let\,in}$)
\[\begin{array}{l@{}l}t,t_1,t_2,u ::= x &\s () \s (t_1,t_2) \s \pi_1\,t \s \pi_2\,t\s
  \inj_1\,t \s \inj_2\,t \\&\s \letin
  x {t_1} {t_2} \s\casezero t \s \case t {x_1}{u_1}{x_2}{u_2}\s \dots\end{array}
\]
We consider the standard typing rules (where $i\in\{1,2\}$):
  \begin{align*}
  &
   \infer{\Gamma, x : A, \Gamma' \vdash x : A}{} 
\ \quad   \infer{\Gamma \vdash () : \tunit}{}
  \ \quad \infer{\Gamma \vdash (t_1,t_2) : A_1 \ast A_2}{\Gamma \vdash t_1 : A_1 \quad \Gamma \vdash t_2 : A_2} 
   \ \quad \infer{\Gamma \vdash \pi_i\,t : A_i}{\Gamma \vdash t : A_1
    \ast A_2}
     \ \quad \infer{\Gamma \vdash \inj_i\,t : A_1+A_2}{\Gamma \vdash t : A_i}
   \\[6pt]
   &
     \infer{\Gamma \vdash \letin x t u : B}
     {\Gamma \vdash t : A \quad \Gamma, x : A \vdash u : B }
     \ \quad 
   \infer{\Gamma \vdash \casezero t : B}
           {\Gamma \vdash t : 0 }
     \ \quad 
   \infer{\Gamma \vdash \case t {x_1}{u_1}{x_2}{u_2}: B}
%           {\Gamma \vdash t : A_1 + A_2\quad \Gamma,x_1:A_1\vdash u_1:B\quad \Gamma,x_2:A_2\vdash u_2:B }
           {\Gamma \vdash t : A_1 + A_2\quad \big(\Gamma,x_i:A_i\vdash u_i:B\big)_{i\in\{1,2\}} }
  \end{align*}
  (Here, a context $\Gamma$ is a sequence of assignments of types~$A$ to variables~$x$.)

  In what follows, we use shorthands such as $\tbool=\tunit+\tunit$, and
  if-then-else instead of case. 

  This language is intended to be a generic probabilistic programming
  language, but so far there is nothing specifically probabilistic about this syntax.
  Different probabilistic programming languages support distributions
  over different kinds of structures. 
  Thus, our language is extended according to an `interface' by specifying
  type constants and typed term constants $f:A\to B$. For each term constant $f:A\to B$,
  we include a new typing rule,
 \[    \infer{\Gamma \vdash f(t) : B}
   {\Gamma \vdash t : A }\]
         \begin{example}\label{ex:interfaces}
           We consider the following examples of interfaces. 
  \begin{enumerate}
  \item For probabilistic programming over finite domains, we may have term constants such as
    $\tbernoulli_{0.5}:\tunit \to \tbool$, intuitively a fair coin toss.
  \item For probabilistic programming over real numbers, we may have a type constant $\treal$ and
    term constants such as $\tnormal:\treal\ast\treal \to\treal$,
    intuitively a parameterized normal distribution,
    and arithmetic operations such as
    $(+):\treal\ast\treal\to\treal$.
  \item The main interface of this paper is for random graphs: this
    has a type constant $\tvertex$ and term constants $
    \tnew:\tunit\to \tvertex$ and $\tedge:\tvertex\ast \tvertex \to \tbool$. 
\end{enumerate}\end{example}
%  Interesting languages extend the types and terms with further constructors; see Ex.~\ref{ex:reals-ppl} and 
 % Sections~\ref{sec:bernoulli-ppl} and \ref{sec:graph-ppl}.
 (We have kept this language as simple as possible, to focus on the interesting aspects.
 A practical probabilistic programming language will include other features, which are largely orthogonal,
 and indeed within our implementation in Haskell (\S\ref{sec:intro:practice}), programming features like higher order functions and recursion are
 present and useful. See also the discussion in~\S\ref{sec:dmc-to-am}.)

\subsection{Equational Theories and Markov Categories}
\label{sec:markov-cats}
Section~\ref{sec:generic-ppl} introduced a syntax for various probabilistic programming interfaces. 
The idea is that this is a generic language which applies to different
interfaces with different distributions that are implemented in
different ways. Rather than considering various ad hoc operational
semantics, we study the instances of interfaces by the program equations
that they support.

Regardless of the specifics of a particular implementation, we
expect basic equational reasoning principles for probabilistic
programming to hold, such as the
following laws:
\begin{align}
  &(\letinsqueeze y {(\letinsqueeze x t u)} {t'}) \equiv (\letinsqueeze x t {\letinsqueeze y u {t'}})\label{eq:let-assoc}
  && 
  (\text{where $x ∉ \fv(t')$})
\\
&  (t, u) ≡ (\letin {x}{t}{}\letin {y}{u}{(x,y)})
    \label{eq:moggi-let-pair}
  \\
  &(\letinsqueeze x t {\letinsqueeze {x'} {t'} u}) \equiv (\letinsqueeze {x'} {t'} {\letinsqueeze x t u})\label{eq:let-comm}&&(\text{where $x ∉ \fv(t')$ and $x' ∉ \fv(t)$})
\\\label{eq:let-affine}
   & (\letin x {t'} t) \equiv t && (\text{where $x ∉ \fv(t)$})
\intertext{The following law does not always hold, but does hold when
       $v$ is `deterministic'.}
         &(\letin x v t) \equiv t[v/x] \label{cd:let_val}
\end{align}
Equations~\eqref{eq:let-comm} and \eqref{eq:let-affine} say that parts of programs can be
re-ordered and discarded, as long as the dataflow is respected. This is a feature of
probabilistic programming. For example, coins do not remember
the order nor how many times they have been tossed. But these
equations would typically not hold in a language with state.

The cleanest way to study equational theories of programs is via a categorical semantics, and for Markov categories have arisen as a canonical setting for categorical probability. Informally, a category is a structure for composition, and this matches the composition structure of $\mathsf{let}$ in our language. We also have monoidal structure which allows for the type constructor $A\times B$ and for the compound contexts $\Gamma$, comonoid structure which allows duplication of variables, and distributive coproduct structure which allows for the sum types.

\begin{definition}\label{def:distr-markov}
  A \emph{symmetric monoidal category} $(\CatA,\otimes,I)$ is
  a category $\CatA$ equipped with a functor ${\otimes :\CatA\times \CatA \to\CatA}$ and an object
  $I$ together with associativity, unit and symmetry structure (\cite[XI.1.]{maclane}).
  A \emph{Markov category (\cite{fritz})} is a symmetric monoidal category in which
  \begin{itemize}
  \item the monoidal unit $I$ is a terminal object ($I=1$), and
  \item 
    every object $X$ is equipped with a comonoid $\Delta_X:X\to X\otimes X$,
    compatible with the tensor product ($\Delta_{X\otimes Y}=(X\otimes\mathsf{swp}\otimes Y)\cdot (\Delta_X\otimes \Delta_Y)$, where $\mathsf{swp}$ is the swap map of $\CatA$). 
  \end{itemize}
  A morphism $f\colon X\to Y$ in a Markov category is
  \emph{deterministic} if it commutes with the comonoids:
  $(f\otimes f)\cdot \Delta_X=\Delta_Y\cdot f$. 

  A \emph{distributive symmetric monoidal category (e.g.~\cite{walters,jay-distr-monoidal})} is a symmetric monoidal category equipped with chosen finite coproducts such that
  the canonical maps $X\otimes Z + Y\otimes Z\to (X+Y)\otimes Z$
    and $0\to 0\otimes Z$ are isomorphisms.
  A \emph{distributive Markov category} is a Markov category whose
  underlying monoidal category is also distributive and
  whose chosen coproduct injections $X\to X+Y\leftarrow Y$ are
  deterministic.
  A \emph{distributive category~\cite{clw,cockett-distr}} is a distributive Markov category
  where all morphisms are deterministic.
  
  A \emph{(strict) distributive Markov functor} is a functor $F : \CatA \to \CatB$ between distributive Markov categories which strictly preserves the chosen symmetric monoidal, coproduct, and comonoid structures.
\end{definition}
In this paper we mainly focus on functors between distributive Markov
categories that strictly preserve the relevant structure, so we elide
`strict'. (Nonetheless, non-strict functors are important,
e.g.~\cite[\S10.2]{fritz} and Prop.~\ref{prop:markov-embed-kleisli}.)

We interpret the language of Section~\ref{sec:generic-ppl} in a
distributive Markov category~$\CatA$ by interpreting types $A$ and
type contexts $\Gamma$ as objects $\sem A$ and $\sem \Gamma$, and
typed terms $\Gamma\vdash t :A $ as morphisms $\sem \Gamma\to \sem A$.
(See e.g.~\cite{pitts-cat-logic} for a general discussion of terms
as morphisms.) 

In more detail, to give such an interpretation, type constants must
first be given chosen interpretations as objects of $\CatA$.
We can then interpret types and contexts using the monoidal and coproduct structure of $\CatA$.
Following this, term constants $f:A\to B$ must be given chosen
interpretations as morphisms $\sem f:\sem A\to \sem B$ in $\CatA$.
The interpretation of other terms is made by induction on the
structure of typing derivations in a standard manner, using the
structure of the distributive Markov category (e.g.~\cite{bpdh},
\cite[\S7.2]{dario-thesis}).
For example,
\begin{align*}
  &\sem{\Gamma,x:A,\Gamma'\vdash x:A}
  =
  \sem{\Gamma,x:A,\Gamma'}
  \cong
  \sem{\Gamma}\otimes \sem A \otimes\sem{\Gamma'}
  \xrightarrow{!\otimes \sem A\otimes!}
  1\otimes
  \sem A\otimes 1
  \cong \sem A
\displaybreak[0]\\
&  \sem{\Gamma\vdash\letin x t u:B}
  =
  \sem\Gamma\xrightarrow{\Delta_{\sem\Gamma}} \sem\Gamma\otimes \sem\Gamma
  \xrightarrow{\sem\Gamma\otimes \sem t}
  \sem\Gamma\otimes \sem A
  =
  \sem{\Gamma,x:A}
  \xrightarrow{\sem u}
     \sem B
\displaybreak[0]  \\
  &
    \sem{\Gamma\vdash\case t {x_1}{u_1}{x_2}{u_2}:B}
    =\\&\qquad
    \sem \Gamma
    \xrightarrow{\Delta_{\sem\Gamma}}
    \sem\Gamma\otimes \sem\Gamma
    \xrightarrow{\sem \Gamma\otimes \sem t}
    \sem \Gamma\otimes {\sem {A_1+A_2}}
    \cong
    \sem{\Gamma ,x:A_1}+\sem{\Gamma,x:A_2}
    \xrightarrow{\langle\sem {u_1},\sem{u_2}\rangle}
  \sem B
  \\&\sem{\Gamma \vdash f(t):B}=
  \sem\Gamma\xrightarrow{\sem t}\sem A\xrightarrow{\sem f} \sem B
\end{align*}


An interpretation in a Markov category induces an equational theory
between programs: let $\Gamma\vdash t=u :A$ if $\sem t=\sem u$.
\begin{proposition}[e.g.~\cite{dario-thesis}, \S7.1]
The equational theory induced by the interpretation in a distributive Markov
category, with given interpretations of type and term constants,
always includes the
equations~\eqref{eq:let-assoc}--~\eqref{eq:let-affine}, 
and also~\eqref{cd:let_val} whenever $\sem v$ is a deterministic morphism. 
\end{proposition}
\begin{example}\label{ex:finset}
  The category $(\FinSet, ×, 1)$ of finite sets is a distributive Markov
    category. As in any category with products, each object has a
    unique comonoid structure, and all morphisms are deterministic. 
    This is a good Markov category for interpreting the plain language
    with no type or term constants. For example, $\sem{\tbool}$ is a
    set with two elements. 
  \end{example}\begin{example}\label{ex:finstoch} The category $\FinStoch$ has natural numbers as objects and
    the morphisms are stochastic matrices. In more detail, a morphism $m\to
    n$ is a matrix in $(\RR_{\geq 0})^{m\times n}$ such that each row sums
    to~$1$.
    Composition is by matrix multiplication. The monoidal structure is
    given on objects by multiplication of numbers, and on morphisms by
    Kronecker product of matrices.
    By choosing an enumeration of each finite set, we get a functor $\FinSet\to\FinStoch$ that converts a function to the corresponding
    $(0/1)$-valued matrix. So every object of $\FinStoch$ can be
    regarded with the comonoid structure from $\FinSet$. 
    The deterministic morphisms in
    $\FinStoch$ are exactly the morphisms from $\FinSet$~\cite[10.3]{fritz}. 

    This is a good Markov category for interpreting the language with
    Bernoulli distributions (Ex.~\ref{ex:interfaces}(1)). We
    interpret the fair coin as the $1\times 2$ matrix $(0.5,0.5)$.

    We can also give some interpretations for the graph interface (Ex.~\ref{ex:interfaces}(3)) in $\FinStoch$.
    For instance, consider random graphs made of two disjoint complete subgraphs,
    as is typical in a clustering model.
    We can interpret this by putting $\sem\tvertex =2$,
    $\sem\tedge=(\begin{smallmatrix}1&0&0&1
      \\0&1&1&0\end{smallmatrix})^\top$, and $\sem\tnew=(0.5,0.5)$.
\end{example}
We look at other examples of distributive Markov categories and
interpretations of these interfaces in
Sections~\ref{sec:distr-monad} and~\ref{sec:probthy}, and then in Sections~\ref{sec:graphons-to-equational-theories}--\ref{sec:ER-Rado}. 

\subsection{Equational Theories and Affine Monads}
\label{sec:affine-monad}
\subsubsection{Distributive Markov Categories from Affine Monads}
One way to generate equational theories via Markov categories is by
considering certain kinds of monads, following
Moggi~\cite{moggi-computational-lambda}.

\begin{definition}\label{def:monad}A \emph{strong monad} on a category $\CatC$
  with finite products is given by
  \begin{itemize}
  \item for each object $X$, an object $T(X)$;
%  \item for each morphism $f : X \to Y$, a morphism $T(f) : T(X) \to T(Y)$;
  \item for each object $X$, a morphism $\eta_X:X\to T(X)$;
  \item for objects $Z,X,Y$, a family of functions natural in $Z$
    \[
      (\bind):\CatC(Z,T(X))\times \CatC(Z\times X,T(Y))\to \CatC(Z,T(Y))
    \]
  \end{itemize}
  such that $\bind$ is associative with unit $\eta$. 
\end{definition}
(There are various different formulations of this structure. 
When $\CatC$ is cartesian
closed, as in Defs.~\ref{def:distribution-monad} and~\ref{def:monad-radonom}, then the bind ($\bind$) is represented by a morphism 
$(\bind ) : {T(X)\times (X\Rightarrow T(Y))\to T(Y)}$, by the Yoneda lemma.)
%\item When $\CatC$ is well-pointed, the bind is entirely
 % determined by specifying the case when $Z=1$, or alternatively by giving
 % a function of type
 % $\CatC(X,T(Y))\to \CatC(T(X),T(Y))$. 
%\end{itemize}
\begin{definition}[\hspace{1sp}\cite{kock-comm,jacobs-weakening,lindner-affine}]\label{def:affine-monad}
  Given a strong monad~$T$, we say that two morphisms $f:X_1\to T(X_2)$,
  $g:X_1\to T(X_3)$
  \emph{commute} if
\[  f \bind ((g \circ \pi_1)\bind (\eta\circ \langle \pi_2 \circ \pi_1,\pi_2\rangle))
  {} =
    g \bind ((f \circ \pi_1)\bind (\eta\circ \langle \pi_2, \pi_2 \circ \pi_1\rangle))：X_1 → T(X_2 × X_3) \text .
\]
  A strong monad is \emph{commutative} if all morphisms commute. It is \emph{affine} if $T(1)\to 1$ is an isomorphism.
\end{definition}

The Kleisli category $\Kl(T)$ of a strong monad $T$ has the same objects as $\CatC$,
but the morphisms are different: $\Kl(T)(A,B)=\CatC(A,T(B))$.
There is a functor $J:\CatC\to\Kl(T)$, given on morphisms by composing
with $\eta$ (e.g.~\cite[\S VI.5]{maclane}, \cite{moggi-computational-lambda}).
\begin{proposition}\label{prop:distr-affine-monad}
        Let\, $T$ be a strong monad on a category $\CatC$. If\, $T$ is commutative
        and affine and $\CatC$ has finite products, then
the Kleisli category $\Kl(T)$ has a canonical structure of a Markov
category. Furthermore, if $\CatC$ is distributive, then $\Kl(T)$ can be regarded as a distributive Markov category. 
\end{proposition}
\begin{proof}[Proof notes]
  The Markov structure follows~\cite[\S3]{fritz}. Since $T$ is commutative, the product structure of $\CatC$ extends
  to a symmetric monoidal structure on $\Kl(T)$.
  Since $T(1)=1$, the monoidal unit ($1$) is terminal in $\Kl(T)$.
  Every object in $\CatC$ has a comonoid structure, and this is
  extended to $\Kl(T)$ via $J$. 
  The morphisms in the image of $J$ are deterministic, although this
  need not be a full characterization of determinism.
  
  For the distributive structure, recall that $J$ preserves
  coproducts and indeed it has a right adjoint. Hence, the coproduct injections will be deterministic.
\end{proof}
We can thus interpret the language of Section~\ref{sec:generic-ppl} using any
strong monad, interpreting the types~$A$ as objects~$\sem A$ of $\CatC$,
and a term $\Gamma\vdash t:A$ as a morphism
$\sem t:\sem{\Gamma} \to T(\sem A)$.
This interpretation matches Moggi's interpretation of the language of Section~\ref{sec:generic-ppl} in a strong monad. 

\subsubsection{Example Affine Monad: Distribution Monad}
\label{sec:distr-monad}
\begin{definition}[e.g.~\cite{jacobs-coalgebra}, \S4.1]\label{def:distribution-monad} The distribution monad $\DistM$ on $\Set$ is defined as follows:
\begin{itemize}
 \item On objects: each set $X$ is mapped to the set of all finitely-supported discrete probability measures on $X$, that is, all functions $p : X \to \RR$ that are non-zero for only finitely many elements and satisfy $\sum_{x \in X} p(x) = 1$.
% \item On morphisms: $\DistM f：\DistM X \to \DistM Y$ is the pushforward: 
% \[
% \DistM f(p)(y) = \textstyle\sum_{x \in f^{-1}(y)} p(x)
% \] 
% for all $p \in \DistM X$, $f : X \to Y$, and $y \in Y$.
 \item The unit $\eta_X :  X\to \DistM (X)$ maps $x \in X$ to the indicator function 
 $\lambda y.\, [y=x]$, i.e.~the Dirac distribution~$\delta_x$.
 \item The bind function $(\bind)$ is defined as follows: 
 \[
         (f \bind g)(z)(y) = \textstyle\sum_{x \in X} f(z)(x) \cdot g(z,x)(y)
 \] 
%for $f\colon  Z \to \DistM(X)$, $g \colon Z {\times} X \to \DistM(Y)$, $z \in Z$, 
%${y \in Y}$.
% for all $f : Z \to \DistM(X)$, $g : Z \times X \to \DistM(Y)$, $z \in Z$
 %and $y \in Y$.
% \item The multiplication $\mu_X : \DistM^2X \to \DistM X$ is given by: 
% \[
% \mu_A(P)(x) = \sum_{p \in \DistM X} P(p) \cdot p(x)
% \] 
% for all $P \in \DistM^2(A)$ and $x \in X$.
 \end{itemize}
\end{definition}
By the standard construction for strong monads, each morphism $f : X \to Y$ 
gets mapped to $\DistM f：\DistM X \to \DistM Y$, that is, the pushforward in this case: 
$\DistM f(p)(y) = \textstyle\sum_{x \in f^{-1}(y)} p(x)$.
%for all $p \in \DistM X$ and $y \in Y$.

% \paragraph*{Kleisli composition} A map $f：X \to \DistM Y$ is a probability kernel if for every $x \in X$, $f(x)(-) = f(x, -)$ is a probability distribution over $Y$. For two maps $f：X \to \DistM Y$ and $g： Y \to \DistM Z$, the composite is given by marginalizing over $\DistM Y$ (Chapman-Kolmogorov):
% \[
%   (g \comp f)(x)(z) = \sum_{y \in Y} f(x, y)\cdot g(y, z)
% \]


% \begin{proposition} $\Set$ is symmetric monoidal category with monoidal product given by cartesian product $X \tensor Y := X \times Y$. Then, the distribution monad $\DistM$ is left- and right-strong. Left-strength $l: X \times \DistM Y \to \DistM(X \times Y)$ sends a pair $(x, p)$ to a distribution $l(x, y) = l(x)(y)$. This gives a map $\productmeas: \DistM X \times \DistM Y \to \DistM(X \times Y)$, which, for a pair of marginal distributions over $X$ and $Y$, gives their product distribution.
% \end{proposition}

% \begin{proposition} There is also a map in the other direction $\marginal: \DistM(X \times Y) \to \DistM X \times \DistM Y$, which, for a distribution $p(-, -)$ over $X \times Y$ gives the pair of marginal distributions over $X$ and $Y$:
% \[
% \marginal(p) = \langle x \mapsto \sum_{y \in Y} p(-, y) , y \mapsto \sum_{x \in Y} p(x, -) \rangle
% \]
% \end{proposition}
% \todo[inline]{What are the properties of the marginal map?}

% \subsection{Semantics of the Bernoulli language}
% Semantics of the Bernoulli probabilistic programming language $\semD{-}$ is given in the Kleisli category for $\DistM$.
% \begin{itemize}

% \item $\semD{-}$ on types gives objects in $\Set$
%     \begin{align*}
%       \semD{\tbool} &= 1 + 1 \\
%       \semD{()} &= 1 \\
%       \semD{A * B} &= \semD{A}\times\semD{B}
%     \end{align*}

% \item $\semD{-}$ on contexts gives objects in $\Set$
%     \begin{align*}
%       \semD{-} &= 1 \\
%       \semD{\Gamma, (x: A)} &= \semD{\Gamma} \times \semD{A}
%     \end{align*}

% \item $\semD{\Gamma \vdash t: A}$ gives a Kleisli map $\semD{t} \in Hom(\semD{\Gamma}, \DistM\semD{A})$
%     \begin{align*}
%       \semD{\Gamma, (x_n: A_n), \Gamma' \vdash x_n: A_n} =& \eta_{A_n} \comp \pi_n \\
%       \semD{\Gamma \vdash \ttrue: \tbool} =& \eta_{\tbool} \comp \iota_1 \comp 1 \\
%       \semD{\Gamma \vdash \tfalse: \tbool} =& \eta_{\tbool} \comp \iota_2 \comp 1 \\
%       \semD{\Gamma \vdash (t_1, t_2): (A_1 * A_2)} =& s \comp \langle \semD{\Gamma \vdash t_1: A_1}, \semD{\Gamma \vdash t_2: A_2}\rangle \\
%       \semD{\Gamma \vdash \pi_1 t: A_1} =& \pi_1 \comp \marginal \comp \semD{\Gamma \vdash t: (A_1 \times A_2)} \\
%       \semD{\Gamma \vdash \pi_2 t: A_2} =& \pi_2 \comp \marginal \comp \semD{\Gamma \vdash t: (A_1 \times A_2)} \\
%       \semD{\Gamma \vdash \letin x t u: B} =& \semD{\Gamma, (x: A) \vdash u: B} \\
%       & \comp (id_\Gamma, \semD{\Gamma \vdash t : A}) \\
%       \semD{\Gamma \vdash \tbernoulli_r(): \tbool} =& (r\delta_{\iota_1} + (1-r)\delta_{\iota_2}) \comp 1 \\
%       \semD{\Gamma \vdash \ite u {t_1} {t_2} A} =& (\semD{\Gamma \vdash t_1: A} + \semD{\Gamma \vdash t_2: A}) \\
%       & \comp (\semD{\Gamma \vdash u: \tbool} \times id) \comp \Delta_{\semD{\Gamma}}
%       \end{align*}
% \end{itemize}
% In case of deterministic terms, we simply lift the semantics in finite sets by $\eta$. If the term does not depend on the context, we pre-compose with $1: \Gamma \to 1$. In the let case, given a map $\semD{ u }: \semD{\Gamma} \to \DistM(1 + 1)$ and two maps $\semD{t_1}, \semD{t_2} : \semD{\Gamma} \to \DistM\semD{A}$ we have to give a map $\semD{\Gamma} \to \DistM\semD{A}$; it is constructed as the composition:
% \begin{align*}
%   \semD{\Gamma}
%   \xrightarrow{\Delta_{\semD{\Gamma}}}
%   \semD{\Gamma} \times& \semD{\Gamma}
%   \xrightarrow{(\semD{u} \times id)}
%   2 \times \semD{\Gamma} \simeq \\
%   &\simeq  \semD{\Gamma} + \semD{\Gamma} \xrightarrow{\semD{t_1} + \semD{t_2}}
%   G\semD{A}
% \end{align*}
Consider the language with no type constants, and just the term constant $\tbernoulli_{0.5}$ (Ex.~\ref{ex:interfaces}(1)).
This can be interpreted in the distribution monad.
Since every type $A$ is interpreted as a finite set~$\sem A$, and every context $\Gamma$ as a finite set~$\sem\Gamma$,
a term $\Gamma \vdash t:A$ is interpreted as a function $\sem \Gamma\to \DistM\sem A$.
To give a Kleisli morphism between finite sets is to give a stochastic matrix, and so the induced equational theory is the same as
the interpretation in $\FinStoch$ (Ex.~\ref{ex:finstoch}). 
\subsubsection{Example Affine Monad: Giry Monad}
% \subsection{Rudiments of Measure-Theoretic Probability}
\label{sec:probthy}
We recall some rudiments of measure-theoretic probability.
\begin{definition}
  A \emph{$\sigma$-algebra} on a set is a non-empty collection of subsets that contains the empty set and is closed under countable unions and complements. A \emph{measurable space} is a pair $(X,\Sigma)$ of a set and a $\sigma$-algebra on it.
  A measurable function $(X,\Sigma_X)\to (Y,\Sigma_Y)$ is a function $f\colon X\to Y$
  such that $f\inv(U)\in \Sigma_X$ for all $U\in \Sigma_Y$.
  
  A \emph{probability measure}
  on a measurable space $(X,\Sigma)$ is a function $\mu:\Sigma\to[0,1]$ that has total mass $1$ ($\mu(X)=1$) and that is $\sigma$-additive:
  $\mu(\biguplus_{i=1}^\infty U_i)=\sum_{i=1}^\infty \mu(U_i)$ for any sequence of disjoint $U_i$.
\end{definition}
Examples of measurable spaces include: the finite sets $X$ equipped with their powerset $\sigma$-algebras; the unit interval $[0,1]$ equipped with its Borel $\sigma$-algebra, which is the least $\sigma$-algebra containing the open sets.
Examples of probability measures include: discrete probability measures (Def.~\ref{def:distribution-monad}); the uniform measure on $[0,1]$; the Dirac distribution $\delta_x(U)={[x\in U]}$.

The product of two measurable spaces $(X,\Sigma_X)\times (Y,\Sigma_Y)=(X\times Y,\Sigma_X\otimes \Sigma_Y)$ comprises the product of sets with the least $\sigma$-algebra making the projections $X\leftarrow X\times Y\to Y$ measurable.
The category of measurable spaces and measurable functions is a distributive category. 

A \emph{probability kernel} between measurable spaces $(X,\Sigma_X)$ and $(Y,\Sigma_Y)$ is a function $k\colon X\times \Sigma_Y\to [0,1]$ that is measurable in the first argument and that is $\sigma$-additive and has mass $1$ in the second argument. 

To compose probability kernels, we briefly recall Lebesgue integration. Consider a measurable space $(X,\Sigma_X)$, 
a measure $\mu:\Sigma_X\to[0,1]$, and a measurable function $f\colon X\to [0,1]$. If $f$ is a simple function, i.e. $f(x) = \sum_{i = 1}^m r_i \cdot [x \in U_i]$ for some $m$, $r_i \in [0,1]$, and $U_i \in \Sigma_X$, the Lebesgue integral $\int f\,\dd \mu = \int f(x)\,\mu(\dd x)\in[0,1]$ is defined to be $\sum_{i = 1}^m r_i \times \mu(U_i)$. If $f$ is not a simple function, there exists a sequence of increasing simple functions $f_1,f_2,\ldots : X \to [0,1]$ such that $\sup_k f_k(x) = f(x)$ (for example, by taking $f_k(x) ≝ \lfloor 10^k f(x) \rfloor / 10^k$). In that case, the integral is defined to be the limit of the integrals of the $f_k$'s (which exists by monotone convergence). 


%\todo[inline]{Recall a little bit more so we can refer back to it? HY: I wrote something.}

Probability kernels can be equivalently formulated as morphisms $X\to \Giry (Y)$, where $\Giry$ is the Giry monad:
\begin{definition}[\hspace{1sp}\cite{giry}]\label{def:giry}
  The Giry monad $\Giry$ is a strong monad on the category $\Meas$ of measurable spaces given by
  \begin{itemize}
\item  $\Giry(X)$ is the set of probability measures on $X$, with the least $\sigma$-algebra making $\int f\, \dd(-):\Giry(X)\to [0,1]$ measurable for all measurable $f:X\to [0,1]$;
% \item $\Giry(f)(\mu)$ for a measurable $f : X \to Y$ and a probability measure $\mu$ on $X$ is
%         the so called pushforward measure\todo{derivable from bind, could omit}, which maps $U \in \Sigma_Y$ to $\mu(f\inv(U))$;
\item the unit $\eta$ maps $x$ to the Dirac distribution $\delta_x$;
\item the bind is given by composing kernels:
  \begin{equation}
  (k\bind l) (z,U) = \int l((z,x),U) \, k(z,\dd x)\text.
  \end{equation}
  \end{itemize}
\end{definition}
\begin{proposition}\label{prop:giry-comm}
  The monad $\Giry$ is commutative and affine.
\end{proposition}
\begin{proof}[Proof notes] Commutativity boils down to Fubini's theorem for reordering integrals and affineness is marginalization (since probability measures have mass $1$). See also~\cite{jacobs-commutative-effectus}.\end{proof}

\label{giry-real-lang}
  %For a first example, 
Consider the real-numbers language (Ex.~\ref{ex:interfaces}(2)).
Let $\sem \treal=\RR$, with the Borel sets, and interpret $\tnormal$ as the normal probability measure on $\RR$. The basic arithmetic operations are all measurable.

Among the following three programs
  \begin{align}
    &\letin x {\tnormal(0,1)} {x+x} \label{eqn:norm-eg-1}\\
    &\letin x {\tnormal(0,1)} {\letin y {\tnormal(0,1)} {x+y} }\label{eqn:norm-eg-2}\\
    &\tnormal(0,1)+\tnormal(0,1) \label{eqn:norm-eg-3}\end{align}
  the programs $\eqref{eqn:norm-eg-2}$ and $\eqref{eqn:norm-eg-3}$
  denote the same normal distribution with variance~$2$, whereas
  \eqref{eqn:norm-eg-1} denotes a distribution with variance~$4$.
  Notice that we cannot use~\eqref{cd:let_val} to equate all the
  programs, because $\sem{\tnormal}$ is not deterministic.

We can also interpret the Bernoulli language (Ex.~\ref{ex:interfaces}(1)) in the Giry monad; this interpretation gives the same equational theory as the interpretation in $\FinStoch$ and in the distribution monad in Section~\ref{sec:distr-monad}.

    We can also give some interpretations for the graph interface
    (Ex.~\ref{ex:interfaces}(3)) in the Giry monad.
    For an informal example, consider the geometric example from Section~\ref{sec:intro:graphs}, let $\sem{\tvertex}=S_2$ (the sphere), and define $\sem{\tnew}$ to be the uniform distribution on the sphere.
    (See also Section~\ref{sec:all-bw}.) 

    \subsubsection{Affine Monads from Distributive Markov Categories}\label{sec:dmc-to-am}
    The following result, a converse to Proposition~\ref{prop:distr-affine-monad}, demonstrates that the new notion of distributive Markov category (Def.~\ref{def:distr-markov}) is a canonical one, and emphasizes the close relationship between semantics with distributive Markov categories and semantics with commutative affine monads. 
\begin{proposition}\label{prop:markov-embed-kleisli}
  Let $\CatA$ be a small distributive Markov category.
  Then, there is a distributive category~$\CatC$ with a commutative affine monad $T$ on it
  and a full and faithful functor $\CatA\to \Kl(T)$ that preserves
  symmetric monoidal structure, comonoids, and sums.
\end{proposition}
\begin{proof}[Proof notes]
  Our proof is essentially a recasting of \cite[\S 7]{power-universal} to this different
  situation, as follows. 
  
  Let $\CatADet$ be the wide subcategory of $\CatA$ comprising the
  deterministic morphisms, and write $J:\CatADet\to\CatA$ for the
  identity-on-objects inclusion functor. Note that $\CatADet$ is a distributive category. We would
  like to exhibit $\CatA$ as the Kleisli category for a monad on
  $\CatADet$, but this might not be possible: intuitively, $\CatADet$
  might be too small for the monad to exist. Instead, we first embed
  $\CatADet$ in a larger category~$\CatC$ and construct a monad on $\CatC$.

  The main construction in our proof is the idea that if $\CatX$ is a
  small distributive monoidal category, then the category
  $\FP(\CatX\op,\Set)$
  of finite-product-preserving functors is such that
  \begin{itemize}
  \item   $\FP(\CatX\op,\Set)$ is cocomplete and moreover total (\cite{STREET1978350}) as a
    category;
  \item $\FP(\CatX\op,\Set)$ admits a distributive monoidal structure;
  \item   the Yoneda embedding $\CatX\to[\CatX\op,\Set]$, which is
    full and faithful, factors through
    $\FP(\CatX\op,\Set)$,
    and this embedding $\CatX\to \FP(\CatX\op,\Set)$ preserves finite sums and is
    strongly monoidal;
  \item  the Yoneda embedding exhibits $\FP(\CatX\op,\Set)$ as a free
    colimit completion of $\CatX$ as a monoidal category that already has
    finite coproducts.
  \end{itemize}

  So we let $\CatC=\FP(\CatADet\op,\Set)$ comprise the
  finite-product-preserving functors $\CatADet\op\to \Set$.
  This is a distributive category. 
  To get a monad on $\CatC$, we note that since
  $\FP(\CatA\op,\Set)$ has finite coproducts and
  $\CatADet\to \CatA\to \FP(\CatA\op,\Set)$  preserves finite
  coproducts and is monoidal, the
  monoidal structure
  induces a canonical colimit-preserving monoidal functor
  $J_!:\FP(\CatADet\op,\Set)\to \FP(\CatA\op,\Set)$.
  Any colimit-preserving functor~$J_!$ out of a total category 
  has a right adjoint~$J^*$, and hence a monoidal monad~$(J^*J_!)$ is induced on
  $\CatC$. 
  
  It remains for us to check that the embedding $\CatA\to
  \FP(\CatA\op,\Set)$ factors through the comparison functor
  $\Kl(J^*J_!)\to \FP(\CatA\op,\Set)$, which follows from the fact
  that $J:\CatADet\to\CatA$ is identity on objects.
\end{proof}
As an aside, we note that, although our simple language in Section~\ref{sec:generic-ppl} did not include higher-order functions, the category $\CatC$ constructed in the proof of Proposition~\ref{prop:markov-embed-kleisli} is cartesian closed, and since the embedding is full and faithful, this shows that higher-order functions would be a conservative extension of our language. Indeed, this kind of conservativity result was part of the motivation of~\cite{power-universal}.
For the same reason, inductive types (lists, and so on) would also be a conservative extension. 
We leave conservativity with other language features for future work. Recursion in probabilistic programming is still under investigation~\cite{DBLP:conf/lics/JiaLMZ21,DBLP:conf/lics/MatacheMS22,DBLP:journals/pacmpl/VakarKS19,DBLP:journals/corr/abs-2106-16190,DBLP:journals/pacmpl/EhrhardPT18}; there is also the question of conservativity with respect to combining Markov categories, e.g. combining real number distributions (\eqref{eqn:tnormal}--\eqref{eqn:add}) with graph programming (\eqref{eqn:intro-new}--\eqref{eqn:intro-edge}).


\subsection{Bernoulli Bases, Numerals and Observation}\label{sec:bernoulli-base}
Although an interface may have different type constants, it will
always have the `numeral' types, sometimes called `finite' types:
\[
0\quad  \tunit \quad \tbool=\tunit+\tunit \quad \tunit+\tunit+\tunit\quad\dots
\]
For probabilistic programming languages, there is a clear expectation
of what will happen when we run a program of type $\tbool$: it will
randomly produce either $\ttrue$ or $\tfalse$, each with some
probability. Similarly for other
numeral types. For type constants, we might not have evident notions of
observation or expected outcomes. But for numeral types, it should be
routine. We now make this precise via the notion of Bernoulli base. 

On the semantic side, distributive Markov categories will always have `numeral' objects
\[
  0\quad 1\quad 2\defeq1+1\quad 3\defeq 1+1+1\quad\dots
\]
For any type~$A$ formed without type constants, and any Markov
category, we have
that $\sem A\cong n$ for some numeral object.
Any equational theory for the programming language induces in
particular an equational theory for the sub-language without any type
constants.

\begin{proposition}
  For any distributive Markov category $\CatA$, let $\CatA_\NN$ be the category whose objects are natural numbers, and where the morphisms are the morphisms in $\CatA$ between the corresponding numeral objects.
  This
  is again a distributive Markov category.
\end{proposition}

\begin{example}\label{ex:bernoulli-base}\begin{enumerate}
%\item For $\FinSet$ and $\FinStoch$ (Ex.~\ref{ex:finset}--\ref{ex:finstoch}),
 % $\FinSet_\NN\simeq\FinSet$ and $\FinStoch_\NN\simeq \FinStoch$. 
  \item $\FinSet_\NN = \Set_\NN$ is equivalent to $\FinSet$ as a category.
  \item For the finite distributions and the Giry monad (\S\ref{sec:distr-monad}--\ref{sec:probthy}), $\Kl(\DistM)_\NN\simeq \Kl(\Giry)_\NN\simeq \FinStoch$.\end{enumerate}
\end{example}

Recall that a functor is \emph{faithful} if it is injective on hom-sets.
\newcommand{\faithfulfr}{\rightarrowtail}
\begin{definition}
  A \emph{Bernoulli base} for a distributive Markov category $\CatA$
  is a faithful distributive Markov functor $\bbase:\CatA_\NN\faithfulfr \FinStoch$.
\end{definition}
Thus, for any distributive Markov category with a Bernoulli base, 
for any closed term $\vdash t : A$ of numeral type ($\sem A=n$), we
can regard its interpretation $\sem t:1\to n$ as nothing but 
a probability distribution $\bbase(\sem t)$ on $n$ outcomes. This is the case even if
$t$ uses term constants and has intermediate subterms using type
constants. 

\begin{example}
  All the examples seen so far can be given Bernoulli bases. In fact, for $\FinStoch$, $\Kl(\DistM)$ and $\Kl(\Giry)$,
  the functor $\bbase:\CatA_\NN\faithfulfr \FinStoch$ is an isomorphism of distributive Markov categories.
\end{example}
When $\bbase$ is an isomorphism of categories, that means that \emph{all} the finite probabilities are present in~$\CatA$. This is slightly stronger than we need in general. 
For instance, when $\CatA=\FinSet$, there is a unique Bernoulli base $\bbase:\FinSet_\NN\faithfulfr\FinStoch$, taking a function to a $0/1$-valued matrix,
but it is not full.
We could also consider variations on $\FinStoch$. For example,
consider the subcategory $\mathbf{Fin\mathbb{Q}Stoch}$ of $\FinStoch$ where the matrices are rational-valued;
this has a Bernoulli base that is not an isomorphism.

\subsection{Quotients of Distributive Markov Categories}\label{sec:quotients}
\newcommand{\Ctx}{\mathcal{C}}

We provide a new, general method for constructing a Bernoulli-based Markov
category out of a distributive Markov category.
Our construction is a categorical formulation of the notion of
contextual equivalence.

Recall that, in general, contextual equivalence for a programming language starts
with a notion of basic observation for closed programs at ground
types. We then say that programs $\Gamma\vdash t,u:A$ at other types
are \emph{contextually equivalent} if for every context $\Ctx$ with $\vdash\Ctx[t],\Ctx[u]:n$, for some ground type $n$,
we have that $\Ctx[t]$ and $\Ctx[u]$ satisfy the same observations. 
In the categorical setting, the notion of observation is given by a
distributive Markov functor $\CatA_\NN\to\FinStoch$, and the notion of
context~$\Ctx$ is replaced by suitable morphisms ($h$, $k$ below). 
We now introduce a quotient construction that will be key in showing that every graphon arises from a distributive Markov category (Corollary~\ref{corollary:graphon-markov}), via Theorem~\ref{thm:eq-theory-to-graphon}.
We note that this is a general new method for building Markov categories.

\begin{proposition}\label{prop:quotient}
Let $\CatA$ be a distributive Markov category, and let $\bbase\colon
\CatA_\NN\to \FinStoch$ be a distributive Markov functor.
Suppose that for every object $X\in\CatA$, either $X=0$ or there
exists a morphism $1\to X$. 
Then, there is a distributive Markov category $\CatAp$ with a Bernoulli base, equipped with 
a distributive Markov functor $\CatA\to \CatAp$ and a factorization of distributive Markov functors
$\bbase=\CatA_\NN\to (\CatAp)_\NN\faithfulfr \FinStoch$.
\end{proposition}
\begin{proof}
  Define an equivalence relation~$\sim$ on each hom-set $\CatA(X,Y)$,
  by $f\sim g:X\to Y$ if 
  \[
    \forall Z,n.\,\forall h:1\to X\otimes Z.\,\forall k:Y\otimes Z\to
    n.\,
    \quad
    \bbase(k\cdot (f\otimes Z)\cdot h)=
    \bbase(k\cdot (g\otimes Z)\cdot h)
    \text{ in }\FinStoch(1,n)\text.
  \]
  Informally, our equivalence relation considers all ways of generating $X$'s
  via precomposition ($h$),  all ways for testing $Y$'s via postcomposition ($k$), and all ways of combining with some ancillary data ($Z$).
  It is essential that we consider all these kinds of composition in order for the quotient category to have the categorical structure. 
  
  It is immediate that composition of morphisms respects $\sim$,
  and hence we have a category: the objects are the same as $\CatA$,
  and the morphisms are $\sim$-equivalence classes. This is our category $\CatAp$.

  It is also immediate that if $f\sim g$ and $f'\sim
  g'$ then $(f\otimes f')\sim(g\otimes g')$. Thus, $\CatAp$ is a
  monoidal category. 

  For the coproduct structure, we must show that if $f\sim g:X\to Y$ and
  $f'\sim g':X'\to Y'$ then $(f+f')\sim (g+g'):X+X'\to Y+Y'$.
  We proceed by noting that since we have morphisms
  $x:1\to X$ and $x':1\to X'$, as well as terminal morphisms $X\to 1$ and
  $X'\to 1$, we have that $X+X'$ is a retract of $X\otimes X'\otimes
  2$, with the section and retraction given by:
  \[
    X + X'\xrightarrow{X\otimes x'+x\otimes X} X\otimes X' + X\otimes
    X'\cong X\otimes X'\otimes 2
    \quad
     X\otimes X'\otimes 2\cong
    X\otimes X' + X\otimes X'
    \xrightarrow{X\otimes !+!\otimes X}
    X + X'
  \]
  Thus, by composing with this retract, it suffices to check that
  $(f\otimes f'\otimes 2)\sim (g\otimes g'\otimes 2)$,
  which we have already shown.

  The functor $\bbase:\CatA_\NN\to \FinStoch$ clearly factors through $(\CatAp)_\NN$,
  but it remains to check that the functor $(\CatAp)_\NN\to \FinStoch$ is now
  faithful (Bernoulli base). 
  So suppose that $\bbase(f)=\bbase(g)$. To show that $f\sim g :1\to
  m$, 
  we consider $h:1\to 1\otimes Z$, and $k:m\otimes Z\to n$.
  We must show that $\bbase(k\cdot (f\otimes Z)\cdot h)=\bbase(k\cdot (g\otimes Z)\cdot
  h)$.
  Since $h=1\otimes h'$, for some $h':1\to Z$,
  we have
  \begin{align*}
    \bbase(k\cdot (f\otimes Z)\cdot h)&=\bbase(k\cdot(m\otimes h')\cdot f)
    =
    \bbase(k\cdot(m\otimes h'))\cdot \bbase(f)\\
    &=
    \bbase(k\cdot(m\otimes h'))\cdot \bbase(g)
    =
    \bbase(k\cdot(m\otimes h')\cdot g)
    =
    \bbase(k\cdot (g\otimes Z)\cdot h)\text.\end{align*}
\end{proof}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "popl24"
%%% End:
