\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2024}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{url}
\usepackage{graphicx}
\usepackage{animate}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{silence}
\usepackage{makecell}
\usepackage{graphicx}  % For including images
\usepackage{wrapfig}   % For wrapping figures
% \usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\hbadness=10000 \vbadness=10000 \vfuzz=30pt \hfuzz=30pt
\emergencystretch 1em

% \usepackage{fontspec}
% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\newcommand{\nameofmethod}{HunyuanVideo}


\newcommand{\dq}[1]{\textcolor{cyan}{\bf\small [Daquan: #1]}}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\tabref}[1]{Tab.~\ref{#1}}
\newcommand{\eqnref}[1]{Eqn.~(\ref{#1})}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\myPara}[1]{\noindent\textbf{#1}}
\newcommand{\sArt}{state-of-the-art~}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\para}[1]{\noindent\textbf{#1}}

\usepackage[pagebackref,breaklinks,colorlinks,citecolor=gray]{hyperref}


\title{\nameofmethod{}: A Systematic Framework For Large Video Generative Models}

\newcommand{\revist}[1]{\textcolor{red}{#1}}
\newcommand{\revisit}[2]{\textcolor{red}{#1(revisit: #2)}}

\begin{document}


\maketitle

\vspace{-20mm}
\begin{quote}
\vspace{-1mm}
\author{Hunyuan Foundation Model Team}
    ``\textit{Bridging the gap between closed-source and open-source video foundation models to accelerate community exploration.}'' \hfill --- \textbf{Hunyuan Foundation Model Team }
    % }
\vspace{-1mm}
\end{quote}


\begin{abstract}
Recent advancements in video generation have profoundly transformed daily life for individuals and industries alike. However, the leading video generation models remain closed-source, creating a substantial performance disparity in video generation capabilities between the industry and the public community. 
%
In this report, we present \nameofmethod{}, a novel open-source video foundation model that exhibits performance in video generation that is comparable to, if not superior to, leading closed-source models. \nameofmethod{} features a comprehensive framework that integrates several key contributions, including data curation, advanced architecture design, progressive model scaling and training, and an efficient infrastructure designed to facilitate large-scale model training and inference.
%
With those, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. 
%
We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to professional human evaluation results, \nameofmethod{} outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and 3 top performing Chinese video generative models. By releasing the code of the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower everyone in the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at \url{https://github.com/Tencent/HunyuanVideo}.
\end{abstract}

\begin{figure}[!h]
\vspace{-0.5cm}
    \centering
    \begin{minipage}[b]{0.3\textwidth}
        \centering
        \animategraphics[width=\textwidth]{24}{./videos/teaser/walking_woman/}{0}{24}
        % \caption{(a)}
    \end{minipage}%
    \hspace{0.01\textwidth} 
    \begin{minipage}[b]{0.5\textwidth}
        \centering
        \begin{minipage}[b]{0.48\textwidth}
            \centering
            \animategraphics[width=\textwidth]{30}{./videos/teaser/girl/}{52}{82}
            % \caption{(b)}
        \end{minipage}%
        \hspace{0.01\textwidth} % Space between the square videos
        \begin{minipage}[b]{0.48\textwidth}
            \centering

            \animategraphics[width=\textwidth]{20}{./videos/teaser/machine/}{42}{62}
            % \caption{(c))}
        \end{minipage}

        \vspace{0.01\textwidth} 

        \animategraphics[width=\textwidth]{24}{./videos/teaser/water_butterfly/}{52}{76}
        % \caption{(d)}
    \end{minipage}
    \caption{Non-curated multi-ratio generation samples with \nameofmethod{}, showing realistic, concept generalization and automatic scene-cut features.}
\end{figure}

\section{Introduction}
\label{sec:intro}

With extensive pre-training and advanced architectures, diffusion models~\cite{li2024hunyuandit,peebles2023scalable,esser2024scaling,rombach2022high,blattmann2023stable,girdhar2023emu,polyak2024movie,FLUX} have demonstrated superior performance in generating high-quality images and videos compared to previous generative adversarial network (GAN) methods \citep{brock2018large}.
However, unlike the image generation field, which has seen a proliferation of novel algorithms and applications across various open platforms, diffusion-based video generative models remain relatively inactive. We contend that one of the primary reasons for this stagnation is the lack of robust open-source foundation models as in T2I filed \cite{FLUX}. In contrast to the image generative model community, a significant gap has emerged between open-source and closed-source video generation models. Closed-source models tend to overshadow publicly available open-source alternatives, severely limiting the potential for algorithmic innovation from the public community. While the recent state-of-the-art model MovieGen \cite{polyak2024movie} has demonstrated promising performance, its milestone for open-source release has yet to be established.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/compute_resource_v1.pdf} 
        % \caption{}
        % \label{fig:figure1}
    \end{subfigure}
    % \hspace{0.05\textwidth} 
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ranking.pdf} 
        % \caption{}
        % \label{fig:figure2}
    \end{subfigure}
    \label{fig:side_by_side}
    \caption{Left: Computation resources used for closed-source and open-source video generation models. Right: Performance comparison between \nameofmethod{} and other selected strong baselines.}
\end{figure}

To address the existing gap and enhance the capabilities of the public community, this report presents our open-sourced foundational video generative model, \nameofmethod{}. This systematic framework encompasses training infrastructure, data curation, model architecture optimization, and model training.
%
Through our experiments, we discovered that randomly scaling the training data, computational resources, and model parameters of a simple Transformer-based generative model \cite{peebles2023scalable} trained with Flow Matching \cite{lipman2022flow} was not sufficiently efficient. Consequently, we explored an effective scaling strategy that can reduce computational resource requirements by up to 5Ã— while achieving the desired model performance. With this optimal scaling approach and dedicated infrastructure, we successfully trained a large video model comprising 13 billion parameters, pre-training it on internet-scale images and videos.
%
After a dedicated progressive fine-tuning strategy, \nameofmethod{} excels in four critical aspects of video generation: visual quality, motion dynamics, video-text alignment, and semantic scene cut. We conducted a comprehensive comparison of \nameofmethod{} with leading global video generation models, including Gen-3 and Luma 1.6 and 3 top performing commercial models in China, using over 1,500 representative text prompts accessed by a group of 60 people. The results indicate that \nameofmethod{} achieves the highest overall satisfaction rates, particularly excelling in motion dynamics.


\section{Overview}
\label{sec:framework}
% \dq{Daquan, Weijie, Tianqi}
\nameofmethod{} is a comprehensive video training system encompassing all aspects from data processing to model deployment. This technical report is structured as follows:

\begin{itemize}
    \item In \textbf{Section \ref{sec:data}}, we introduce our data preprocessing techniques, including filtering and re-captioning models.
    \item \textbf{Section \ref{sec:model_arch}} presents detailed information about the architecture of all components of \nameofmethod{}, along with our training and inference strategies.
    \item In \textbf{Section \ref{sec:accelerate}}, we discuss methods for accelerating model training and inference, enabling the development of a large model with 13 billion parameters.
    \item \textbf{Section \ref{sec:exp}} evaluates the performance of our text-to-video foundation models and compares them with state-of-the-art video generation models, both open-source and proprietary.
    \item Finally, in \textbf{Section \ref{sec:application}}, we showcase various applications built on the pre-trained foundation model, accompanied by relevant visualizations as well as some video related functional models such as video to audio generative model.
\end{itemize}
\begin{figure}[h]
    \hfill
    \includegraphics[width=\linewidth]{figures/overall.png}

    \caption{The overall training system for \nameofmethod{}.}
    \label{fig:pipeline_overview}
\end{figure}

\input{tax/data}

\section{Model Architecture Design}
The overview of our \nameofmethod{} model is shown in Fig.~\ref{fig:hunyuanvideo_overview}. This section describes the Causal 3D VAE, diffusion backbone, and scaling laws experiments.

\label{sec:model_arch}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/hunyuanvideo_overview.png}

    \caption{The overall architecture of \nameofmethod{}. The model is trained on a spatial-temporally compressed latent space, which is compressed through Causal 3D VAE. Text prompts are encoded using a large language model, and used as the condition. Gaussian noise and condition are taken as input, our model generates a output latent, which is decoded into images or videos through the 3D VAE decoder.}
    \label{fig:hunyuanvideo_overview}
\end{figure}

% \dq{add in architecture design of 3D VAE and DiT. Put diagram here also.}
\input{tax/vae}
% \dq{@Common Utility Team}

\input{tax/scaling_law}

\input{tax/pre-training}

% \input{tax/t2v}

\input{tax/accelerate}



\section{Fundation Model Performance}
\label{sec:exp}

\input{exp/text_encoder}

\input{exp/sft}

\input{exp/human_eval}

\section{Applications}
\label{sec:application}

\input{tax/tv2a}


\input{applications/i2v}

\input{applications/human_i2v}

\input{applications/audio}

\input{applications/expr_pose}

\input{applications/demo}


\section{Related Works}
\label{sec:related_works}

Due to the success of diffusion models in the field of image generation~\citep{rombach2022high, ho2020denoising}, the exploration in the domain of video generation~\citep{guo2023animatediff,jiang2023text2performer,singer2022make,wang2023lavie,yang2023probabilistic,zhang2023show,ma2024follow,chen2024follow,xue2024follow,ma2024follow2} is also becoming popular. VDM~\citep{ho2022video} is among the first that extends the 2D U-Net from image diffusion models to a 3D U-Net to achieve text-based generation.
%
Later works, such as MagicVideo~\citep{zhou2023magicvideo} and Mindscope~\citep{wang2023modelscope}, introduce 1D temporal attention mechanisms, reducing computations by building upon latent diffusion models. In this report, we do not use the 2D + 1D temporal block manner for motion learning. Instead, we use similar dual flow attention blocks as in FLUX \cite{FLUX}, which are used for processing all video frames.
%
Following Imagen, Imagen Video~\citep{ho2022imagen} employs a cascaded sampling pipeline that generates videos through multiple stages.
%
In addition to traditional end-to-end text-to-video (T2V) generation, 
video generation using other conditions is also an important direction.
%
This type of methods generates videos with other auxiliary controls, such as depth maps~\citep{guo2023sparsectrl,he2023animate}, pose maps~\citep{xu2023magicanimate,hu2023animate,wang2023disco,ma2023follow}, RGB images~\citep{blattmann2023stable,chen2023seine,ni2023conditional}, or other guided motion videos~\citep{zhao2023motiondirector,wu2023lamp}.  
%
Despite the excellent generation performance of the recent open-source models such as Stable video diffusion~\citep{blattmann2023stable}, Open-sora \cite{opensora}, Open-sora-plan \cite{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109}, Mochi-1 \cite{genmo2024mochi} and Allegro \cite{zhou2024allegro}, their performance still falls far behind the closed-source
%
state-of-the-art video generation models such as Sora \cite{videoworldsimulators2024} and MovieGen \cite{polyak2024movie}. 


\input{tax/contributors}

\clearpage
{%\small
\bibliographystyle{plain}
\bibliography{egbib}
}

\end{document}