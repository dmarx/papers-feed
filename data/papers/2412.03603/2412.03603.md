---
abstract: |
  Recent advancements in video generation have profoundly transformed daily life for individuals and industries alike. However, the leading video generation models remain closed-source, creating a substantial performance disparity in video generation capabilities between the industry and the public community. In this report, we present HunyuanVideo, a novel open-source video foundation model that exhibits performance in video generation that is comparable to, if not superior to, leading closed-source models. HunyuanVideo features a comprehensive framework that integrates several key contributions, including data curation, advanced architecture design, progressive model scaling and training, and an efficient infrastructure designed to facilitate large-scale model training and inference. With those, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to professional human evaluation results, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and 3 top performing Chinese video generative models. By releasing the code of the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower everyone in the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at <https://github.com/Tencent/HunyuanVideo>.
author:
- Hunyuan Foundation Model Team
bibliography:
- egbib.bib
citation-style: ieee
header-includes:
- 
- 
link-citations: true
reference-section-title: References
title: "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
---





> “*Bridging the gap between closed-source and open-source video foundation models to accelerate community exploration.*” — **Hunyuan Foundation Model Team**

<figure>

<figcaption>Non-curated multi-ratio generation samples with HunyuanVideo, showing realistic, concept generalization and automatic scene-cut features.</figcaption>
</figure>

# Introduction

With extensive pre-training and advanced architectures, diffusion models  have demonstrated superior performance in generating high-quality images and videos compared to previous generative adversarial network (GAN) methods . However, unlike the image generation field, which has seen a proliferation of novel algorithms and applications across various open platforms, diffusion-based video generative models remain relatively inactive. We contend that one of the primary reasons for this stagnation is the lack of robust open-source foundation models as in T2I filed . In contrast to the image generative model community, a significant gap has emerged between open-source and closed-source video generation models. Closed-source models tend to overshadow publicly available open-source alternatives, severely limiting the potential for algorithmic innovation from the public community. While the recent state-of-the-art model MovieGen has demonstrated promising performance, its milestone for open-source release has yet to be established.

<figure id="fig:side_by_side">
<figure>
<span class="image placeholder" data-original-image-src="figures/compute_resource_v1.pdf" data-original-image-title="" width="\textwidth"></span>
</figure>
<figure>
<span class="image placeholder" data-original-image-src="figures/ranking.png" data-original-image-title="" width="\textwidth"></span>
</figure>
<figcaption>Left: Computation resources used for closed-source and open-source video generation models. Right: Performance comparison between HunyuanVideo and other selected strong baselines.</figcaption>
</figure>

To address the existing gap and enhance the capabilities of the public community, this report presents our open-sourced foundational video generative model, HunyuanVideo. This systematic framework encompasses training infrastructure, data curation, model architecture optimization, and model training. Through our experiments, we discovered that randomly scaling the training data, computational resources, and model parameters of a simple Transformer-based generative model trained with Flow Matching was not sufficiently efficient. Consequently, we explored an effective scaling strategy that can reduce computational resource requirements by up to 5× while achieving the desired model performance. With this optimal scaling approach and dedicated infrastructure, we successfully trained a large video model comprising 13 billion parameters, pre-training it on internet-scale images and videos. After a dedicated progressive fine-tuning strategy, HunyuanVideo excels in four critical aspects of video generation: visual quality, motion dynamics, video-text alignment, and semantic scene cut. We conducted a comprehensive comparison of HunyuanVideo with leading global video generation models, including Gen-3 and Luma 1.6 and 3 top performing commercial models in China, using over 1,500 representative text prompts accessed by a group of 60 people. The results indicate that HunyuanVideo achieves the highest overall satisfaction rates, particularly excelling in motion dynamics.

# Overview

HunyuanVideo is a comprehensive video training system encompassing all aspects from data processing to model deployment. This technical report is structured as follows:

- In **Section **, we introduce our data preprocessing techniques, including filtering and re-captioning models.

- **Section ** presents detailed information about the architecture of all components of HunyuanVideo, along with our training and inference strategies.

- In **Section **, we discuss methods for accelerating model training and inference, enabling the development of a large model with 13 billion parameters.

- **Section ** evaluates the performance of our text-to-video foundation models and compares them with state-of-the-art video generation models, both open-source and proprietary.

- Finally, in **Section **, we showcase various applications built on the pre-trained foundation model, accompanied by relevant visualizations as well as some video related functional models such as video to audio generative model.

<figure id="fig:pipeline_overview">
<span class="image placeholder" data-original-image-src="figures/overall.png" data-original-image-title="" width="\linewidth"></span>
<figcaption>The overall training system for HunyuanVideo.</figcaption>
</figure>

# Model Architecture Design

The overview of our HunyuanVideo model is shown in Fig. . This section describes the Causal 3D VAE, diffusion backbone, and scaling laws experiments.

<figure id="fig:hunyuanvideo_overview">
<span class="image placeholder" data-original-image-src="figures/hunyuanvideo_overview.png" data-original-image-title="" width="0.95\linewidth"></span>
<figcaption>The overall architecture of HunyuanVideo. The model is trained on a spatial-temporally compressed latent space, which is compressed through Causal 3D VAE. Text prompts are encoded using a large language model, and used as the condition. Gaussian noise and condition are taken as input, our model generates a output latent, which is decoded into images or videos through the 3D VAE decoder.</figcaption>
</figure>

# Fundation Model Performance

# Applications

# Related Works

Due to the success of diffusion models in the field of image generation , the exploration in the domain of video generation  is also becoming popular. VDM  is among the first that extends the 2D U-Net from image diffusion models to a 3D U-Net to achieve text-based generation. Later works, such as MagicVideo  and Mindscope , introduce 1D temporal attention mechanisms, reducing computations by building upon latent diffusion models. In this report, we do not use the 2D + 1D temporal block manner for motion learning. Instead, we use similar dual flow attention blocks as in FLUX , which are used for processing all video frames. Following Imagen, Imagen Video  employs a cascaded sampling pipeline that generates videos through multiple stages. In addition to traditional end-to-end text-to-video (T2V) generation, video generation using other conditions is also an important direction. This type of methods generates videos with other auxiliary controls, such as depth maps , pose maps , RGB images , or other guided motion videos . Despite the excellent generation performance of the recent open-source models such as Stable video diffusion , Open-sora , Open-sora-plan , Mochi-1 and Allegro , their performance still falls far behind the closed-source state-of-the-art video generation models such as Sora and MovieGen .
