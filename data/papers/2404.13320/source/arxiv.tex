\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage{subfig}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}    
\usepackage{color, colortbl}% colors
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{algpseudocode}
\usepackage[pdftex]{graphicx}
\usepackage{appendix}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{comment}
\usepackage[colorlinks]{hyperref} 
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage[normalem]{ulem}

\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}

\definecolor{LightCyan}{rgb}{0.88,1,1}

% hyperlinks
\hypersetup{colorlinks,allcolors=[rgb]{0.6,0.15,0.00098}}

\newcommand{\chen}[1]{{\color{cyan}{#1}}}
\newcommand{\haotian}[1]{{\color{red}{#1}}}
\newcommand{\alex}[1]{{\color{blue}{\textbf{alex}: #1}}}

\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors



\title{Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than We Think}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Haotian Xue \\
  Georgia Institute of Technology\\
  htxue.ai@gatech.edu\\
  \And
   Yongxin Chen\\
  Georgia Institute of Technology\\
  yongchen@gatech.edu\\
}


\begin{document}


\maketitle


\begin{abstract}

% Adversarial examples for diffusion models are widely used as solutions for safety concerns. By adding adversarial perturbations to personal images, attackers can not edit or imitate them easily. However, it is essential to note that all these protections target the latent diffusion model (LDMs), the adversarial examples for diffusion models in the pixel space (PDMs) are largely overlooked. This may mislead us to think that the diffusion models are vulnerable to adversarial attacks like most deep models. In this paper, we show novel findings that: even though gradient-based white-box attacks can be used to attack the LDMs, they fail to attack PDMs. This finding is supported by extensive experiments of almost a wide range of attacking methods on various PDMs and LDMs with different model structures, meaning diffusion models are much more robust against adversarial attacks. Moreover, we find that PDMs can be used as an off-the-shelf purifier to effectively remove the adversarial patterns that were generated on LDMs to protect the images, which means that most protection methods nowadays, to some extent, cannot protect our images from malicious attacks. We hope that our insights will inspire the community to rethink the adversarial samples for diffusion models as protection methods and move forward to more effective protection. Codes are available in \url{https://github.com/xavihart/PDM-Pure}.

Diffusion models have demonstrated an impressive capability to edit or imitate images, which has raised concerns regarding the safeguarding of intellectual property. To address these concerns, the adoption of adversarial attacks, which introduce adversarial perturbations into protected images, has proven successful. Consequently, diffusion models, like many other deep network models, are believed to be susceptible to adversarial attacks. However, in this work, we draw attention to an important oversight in existing research, as all previous studies have focused solely on attacking latent diffusion models (LDMs), neglecting adversarial examples for diffusion models in the pixel space (PDMs). Through extensive experiments, we demonstrate that nearly all existing adversarial attack methods designed for LDMs fail when applied to PDMs. We attribute the vulnerability of LDMs to their encoders, indicating that diffusion models exhibit strong robustness against adversarial attacks. Building upon this insight, we propose utilizing PDMs as an off-the-shelf purifier to effectively eliminate adversarial patterns generated by LDMs, thereby maintaining the integrity of images. Notably, we highlight that most existing protection methods can be easily bypassed using PDM-based purification. We hope our findings prompt a reevaluation of adversarial samples for diffusion models as potential protection methods. Codes are available in \url{https://github.com/xavihart/PDM-Pure}.


% Generative Diffusion Models excel at generating high-quality images however, they can cause safety issues by maliciously editing or mimicking 







\end{abstract}


\section{Introduction}

Generative diffusion models (DMs)~\citep{ddpm,song2020score,ldm} have achieved great success in generating images with high fidelity. However, this remarkable generative capability of diffusion models is accompanied by safety concerns~\cite{zhang2023text}, especially on the unauthorized editing or imitation of personal images such as portraits or individual artworks~\cite{andersen2023,setty2023}.
%
Recent works~\cite{advdm, glaze,salman2023raising, sdsattack, mist-v2, chen2024smoothattack,ahn2024imperceptible, metacloak} show that adversarial samples (adv-samples) for diffusion models can be applied as a protection against malicious editing. Small perturbations generated by conventional methods in adversarial machine learning~\citep{pgd,goodfellow2014fgsm} can effectively fool popular diffusion models such as Stable Diffusion~\cite{ldm} to produce chaotic results when an imitation attempt is made. However, a significantly overlooked aspect is that all the existing works focus on latent diffusion models (LDMs) and the pixel-space diffusion models (PDMs) are not studied. For LDMs, perturbations are not directly introduced to the input of the diffusion models. Instead, they are applied externally and propagated through an encoder. It has been shown that the encoder-decoder of LDMs is vulnerable to adversarial perturbations ~\cite{zhang2023robustness,sdsattack}, which means that the adv-samples for LDMs have a very different mechanism compared with the adv-samples for PDMs. 
Moreover, some existing works~\cite{liang2023mist, salman2023raising} show that combining encoder-specific loss can enhance the adversary, ~\cite{sdsattack} further demonstrating that 
% the gradient of the denoising process is weak and unstable, and 
the encoder is the bottleneck for attacking LDMs. Building upon this observation, in this paper, we draw attention to
rethink existing adversarial attack methods for diffusion models:

\begin{center}
    \textit{Can we generate adversarial examples for PDMs as we did for LDMs?}
\end{center}

% \chen{combine this with the first paragraph before raising our research question} 

% Instead of attacking the diffusion process itself, current adversarial examples for Latent Diffusion Models heavily rely on attacking the encoder: Glaze~\cite{glaze} is built on minimizing the distance between the attacked image and the target image in the latent space defined by the encoder. Additionally, Mist~\cite{liang2023mist} demonstrates the significance of combining the textural loss derived from the encoder to generate better adversarial samples. Moreover, SDS-attack \cite{sdsattack} further investigates that, the gradient of denoising process is weak and unstable, and the real bottleneck of attacking an LDM is attacking the encoder.
%

We address this question by systematically investigating adv-samples for PDMs. We conduct experiments on various LDMs or PDMs with different network architectures (e.g. U-Net~\cite{ddpm} or Transformer~\cite{dit}), different training datasets, and different input resolutions (e.g. 64, 256, 512). Through extensive experiments, we demonstrate that all the existing methods we tested ~\citep{liang2023mist,mist-v2, glaze, sdsattack, chen2024smoothattack, salman2023raising, advdm}, targeting to attack LDMs, fail to generate effective adv-samples for PDMs. This implies that PDMs are more adversarial robust than we think. 

% \chen{this is confusing More importantly, it means that the previous adversarial examples for diffusion models (AdvDM) are, in fact, one special case of adv-samples for LDMs (AdvLDM) only.}

Building on this insight that PDMs are strongly robust against adversarial perturbations, we further propose PDM-Pure, a universal purifier that can effectively remove the protective perturbations of different scales (e.g. Mist-v2~\cite{mist-v2} and Glaze~\cite{glaze}) based on PDMs trained on large datasets. Through extensive experiments, we demonstrate that PDM-Pure achieves way better performance than all baseline methods.

% \chen{no need to mention this in introduction While GrIDPure~\cite{zhao2023can} shows that we can apply diffusion models in different resolutions to purify our image patch-by-patch, they miss that the key is in fact the pixel space, not the resolution.}



% Pixel is a barrier, the original reverse process of PDMs introduces large randomness directly in the pixel space, making the whole system quite robust to be fooled to generate bad samples. Pixel is also a barrier preventing us from achieving real protection using adversarial perturbations since strong PDMs can be utilized to remove the out-of-distribution perturbations~\cite{diff-pgd}. Our work calls for attention to rethink the problem of adv-samples for DM, and also rethink whether adv-samples can protect our images. Our contributions are summarized as follows.

% \begin{enumerate}[parsep=0pt,topsep=0pt,leftmargin=12pt]
%     \item We revist adversarial examples for diffusion models by investigating PDMs, an area that has largely been overlooked in this field. Existing attacks on LDMs \textbf{can not} be applied to attack PDMs, which means that PDMs show superior robustness against adversarial perturbations.
%     \item Based on the new insights, we propose a simple but effective framework to apply strong PDMs as \textbf{a universal purifier} to remove attack-agnostic adversarial perturbations, easily bypassing current protective methods. 
%     \item \chen{is this a contribution?} We emphasize that \textbf{pixel is a barrier}, the community should rethink the adv-samples for DMs and whether current protections based on adv-samples for LDMs, really provide effective protection.
% \end{enumerate}


To summarize, the pixel is a barrier to adversarial attack; the diffusion process in the pixel space makes PDMs much more robust than LDMs. This property of PDMs also makes real protection against the misusage of diffusion models difficult since all the existing protections can be easily purified using a strong PDM. Our contributions are listed below.

\begin{enumerate}[parsep=0pt,topsep=0pt,leftmargin=12pt]
    \item We observe that most existing works on adversarial examples for protection focus on LDMs. Adversarial attacks against PDMs are \textbf{largely overlooked} in this field. 
    \item We fill in the gap in the literature by conducting extensive experiments on various LDMs and PDMs. We discover that all the existing methods \textbf{fail} to attack the PDMs, indicating that PDMs are much more adversarially robust than LDMs.
    \item Based on this novel insight, we propose a simple yet effective framework termed PDM-Pure that applies strong PDMs as \textbf{a universal purifier} to remove attack-agnostic adversarial perturbations, easily bypassing almost all existing protective methods. 

\end{enumerate}



% we made an observation that most of the works focus on LDM, PDMs are not studied

% we test it and find that PDM is much more robust than LDMs 

% PDM-Pure 




\input{figText/teaser}

% background

% issues


% what we find


%

\section{Related Works}

\paragraph{Safety Issues in Diffusion Models}
The impressive generative capability of the diffusion models has raised numerous safety issues~\cite{zhang2023text,setty2023,andersen2023}. As a result, there has been a growing interest in preventing DMs from being abused. Some of the existing works focus on the protection of intellectual property of diffusion models by applying watermarks~\citep{zhao2023recipe, peng2023protecting, cui2023diffusionshield} and some of them are on concept removal to prevent the DMs from generating NSFW images~\citep{heng2023continual,zhang2023forget,gandikota2023unified}. In the era of generative models, caution should be taken to guarantee safe and responsible applications of these models.

\paragraph{Adversarial Examples for DMs} Adversarial samples~\cite{goodfellow2014fgsm, carlini2017towards, glaze} are clean samples perturbed by an imperceptible small noise that can fool the deep neural networks into making wrong decisions. Under the white-box settings, gradient-based methods are widely used to generate adv-samples. Among them, the projected gradient descent (PGD) algorithm~\cite{pgd} is one of the most effective methods. Recent works~\citep{advdm, salman2023raising} show that it is also easy to find adv-samples for diffusion models (AdvDM): with a proper loss to attack the denoising process, the perturbed image can fool the diffusion models to generate chaotic images when operating diffusion-based mimicry. Furthermore, many improved algorithms~\cite{mist-v2,chen2024smoothattack,sdsattack} have been proposed to generate better AdvDM samples. However, to our best knowledge, all the AdvDM methods listed above are used on LDMs, and those for the PDMs are rarely explored.

\paragraph{Adversarial Perturbation as Protection} Adversarial perturbation against DMs turns out to be an effective method to safeguard images against unauthorized editing~\cite{advdm, glaze,salman2023raising, sdsattack, mist-v2, chen2024smoothattack,ahn2024imperceptible, metacloak}. It has found applications (e.g., Glaze~\cite{glaze} and Mist~\cite{mist-v2, liang2023mist}) for individual artists to protect their creations. SDS-attack~\cite{sdsattack} further investigates the mechanism behind the attack and proposes some tools to make the protection more effective. However, they are limited to protecting LDMs only.
% without further investigating whether they can work for more general PDMs. 
In addition, some works~\cite{zhao2023can, sandoval2023jpeg} find that these protective perturbations can be purified. For instance, GrIDPure~\cite{zhao2023can} find that DiffPure~\cite{nie2022diffusion} can be used to purify the adversarial patterns, but they did not realize that the reason behind this is the robustness of PDMs.


\input{figText/attack}


\section{Preliminaries}

\paragraph{Generative Diffusion Models}

The generative diffusion model~
\cite{ddpm, song2020score} is one type of generative model, and it has demonstrated remarkable generative capability in numerous fields such as image~\cite{ldm, balaji2022ediffi}, 3D~\cite{poole2022dreamfusion, lin2022magic3d}, video~\cite{vdm,makeavideo}, story~\cite{pan2022story, rahman2023make} and music~\cite{musicdiff, huang2023noise2music} generation. Diffusion models, like other generative models, are parametrized models $p_{\theta}(\hat{x}_0)$ that can estimate an unknown distribution $q(x_0)$. For image generation tasks, $q(x_0)$ is the distribution of real images.

There are two processes involved in a diffusion model, a forward diffusion process and a reverse denoising process. The forward diffusion process progressively injects noise into the clean image, and the $t$-th step diffusion is formulated as $q(x_t \mid x_{t-1} ) = \mathcal{N} (x_t; \sqrt{1 - \beta_t}x_{t-1}, \, \beta_t \mathbf{I})$. Accumulating the noise, we have $    q_t(x_t \mid x_0 ) = \mathcal{N} (x_t; \sqrt{\bar{\alpha}_t} \, x_{t-1}, \, (1-\bar{\alpha}_t) \mathbf{I})$. Here $\beta_t$ growing from $0$ to $1$ are pre-defined values,  $\alpha_t = 1-\beta_t$, and $\bar{\alpha}_t = \Pi_{s=1}^{t} \alpha_s$. Finally, $x_T$ will become approximately an isotropic Gaussian random variable when $\bar{\alpha}_t \rightarrow 0$. 

Reversely, $p_{\theta}(\hat{x}_{t-1}|\hat{x}_{t})$ can generate samples from Gaussian $\hat{x}_T \sim\mathcal{N} (0, \textbf{I})$, where $p_{\theta}$ be re-parameterized by learning a noise estimator $\epsilon_{\theta}$, the training loss is $\mathbb{E}_{t, x_0, \epsilon}[\lambda(t)\|\epsilon_{\theta}(x_t,t) - \epsilon \|^2]$ weighted by $\lambda(t)$, where $\epsilon$ is the noise used to diffuse $x_0$ following $q_t(x_t|x_0)$. Finally, by iteratively applying $p_{\theta}(\hat{x}_{t-1}|\hat{x}_{t})$, we can sample realistic images following $p_{\theta}(\hat{x}_0)$.


Since the above diffusion process operates directly in the pixel space, we call such diffusion models Pixel-Space Diffusion Models (PDMs). Another popular choice is to move the diffusion process into the latent space to make it more scalable, resulting in the Latent Diffusion Models (LDMs)~\cite{ldm}. More specifically, LDMs first use an encoder $\mathcal{E}_{\phi}$ parameterized by $\phi$ to encode $x_0$ into a latent variable $z_0 = \mathcal{E}_{\phi}(x_0)$. The denoising diffusion process is the same as PDMs. At the end of the denoising process, $\hat{z}_0$ can be projected back to the pixel space using decoder $\mathcal{D}_{\psi}$ parameterized by $\psi$ as $\hat{x}_0 = \mathcal{D}_{\psi}(\hat{z}_0)$.

% \chen{breifly introduce SDEdit?}




\input{figText/table_sdedit}





\paragraph{Adversarial Examples for Diffusion Models} 
Recent works~\cite{salman2023raising, advdm} find that adding small perturbations to clean images will make the diffusion models perform badly in noise prediction, and further generate chaotic results in tasks like image editing and customized generation. The adversarial perturbations for LDMs can be generated by optimizing the Monte-Carlo-based adversarial loss:

\vspace{-0.4cm}
\begin{equation}\label{semantic_loss}
        \mathcal{L}_{adv}(x) = \mathbb{E}_{t, \epsilon} \mathbb{E}_{z_t \sim q_t(\mathcal{E}_{\phi}(x))}\|\epsilon_{\theta}(z_t, t) -\epsilon \|_2^2.
\end{equation}
\vspace{-0.4cm}

Other encoder-based losses~\cite{glaze, liang2023mist, mist-v2, sdsattack} further enhance the attack to make it more effective. With the carefully designed adversarial loss, we can run Projected Gradient Descent (PGD)~\cite{pgd} with $\ell_{\infty}$ budget $\delta$ to generate adversarial perturbations: 
% \chen{make it clear that $x$ is the clean image}
% \chen{use $x^k$? also $B_\infty(x^k, \delta)$?} \haotian{should be $B_\infty(x, \delta)$, since the budget is computed on the clean sample}

\vspace{-0.4cm}
\begin{equation}\label{pgd_update}    x^{k+1} = \mathcal{P}_{B_\infty(x^0, \delta)} \left[ x^{k} + \eta\, \text{sign}\nabla_{x^k}\mathcal{L}_{adv}(x^k) \right]
\end{equation}

 In the above equation, $\mathcal{P}_{B_\infty(x^0, \delta)}(\cdot)$ is the projection operator on the $\ell_\infty$ ball, where $x^0$ is the clean image to be perturbed. We use superscript $x^k$ to represent the iterations of the PGD and subscript $x_t$ for the diffusion steps. 

    
\input{figText/purifier}


\section{Rethink Adversarial Examples for Diffusion Models}

Adversarial examples of LDMs are widely adopted as a protection mechanism to prevent unauthorized images from being edited or imitated~\cite{glaze, liang2023mist}. However, a significant issue overlooked is that all the adversarial examples in existing work are generated using LDMs, primarily due to the wide impact of the Stable Diffusion; no attempts have been made to attack PDMs. 

This lack of investigation may mislead us to conclude that diffusion models, like most deep neural networks, are vulnerable to adversarial perturbations, and that the algorithms used in LDMs can be transferred to PDMs by simply applying the same adversarial loss in the pixel space formulated as:

\vspace{-0.4cm}
\begin{equation}\label{pixel_diffusion_adversarial_loss}
    \mathcal{L}_{adv}(x) = \mathbb{E}_{t, \epsilon} \mathbb{E}_{x_t \sim q_t(x)}\|\epsilon_{\theta}(x_t, t) -\epsilon \|_2^2
\end{equation}

However, we show through experiments that PDMs are robust against this form of attack (Figure~\ref{fig:attack_various_models}), which means all the existing attacks against diffusion models are, in fact, special cases of attacks against the LDMs only.
Prior to this study, there may have been a prevailing belief that diffusion models could be easily deceived. However, our research reveals an important distinction: it is the LDMs that exhibit vulnerability, while the PDMs demonstrate significantly higher adversarial robustness.
% Before this work, people may think that diffusion models can be easily fooled, but the truth is that only LDMs are, the original PDMs are much more adversarially robust. 
We conduct extensive experiments on popular LDMs and PDMs structures including DiT, Guided Diffusion, Stable Diffusion, and DeepFloyd, and demonstrate in Table~\ref{quant_protect} that only the LDMs can be attacked and PDMs are not that susceptible to adversarial perturbations. More details and analysis can be found in the experiment section.



The vulnerability of the LDMs is caused by the vulnerability of the latent space~\cite{sdsattack}, meaning that although we may set budgets for perturbations in the pixel space, the perturbations in the latent space can be large. In~\cite{sdsattack}, the authors show statistics of perturbations in the latent space over the perturbations in the pixel space and this value $\frac{|\delta_z|}{|\delta_x|}$ can be as large as $10$. In contrast, the PDMs directly work in the pixel space, and thus the injected noise combined with the random Gaussian noise will not easily fool the denoiser as it is trained to be robust to Gaussian noise of different levels. 








Almost all the copyright protection perturbations~\cite{glaze, liang2023mist, mist-v2} are based on the insight that it is easy to craft adversarial examples to fool the diffusion models.  We need to rethink the adversarial samples of diffusion models since there are a lot of PDMs that cannot be attacked easily. Next, we show that PDMs can be utilized to purify all adversarial patterns generated by existing methods in Section~\ref{sec:pdm_pure}.  This new landscape poses new challenges to ensure the security and robustness of diffusion-based copyright protection techniques.




\section{PDM-Pure: PDM as a Strong  Universal Purifier}~\label{sec:pdm_pure}
\vspace{-0.4cm}

Given the robustness of PDMs, a natural idea emerges: we can utilize PDMs as a universal purification network. This approach could potentially eliminate any adversarial patterns without knowing the nature of the attacks. We term this framework \textbf{PDM-Pure}, which is a general framework to deal with all the perturbations nowadays. To fully harness the capabilities of PDM-Pure, we need to fulfill two basic requirements: (1) The perturbation shows out-of-distribution pattern as reflected in existing works on adversarial purification/attacks using diffusion models~\cite{nie2022diffusion,diff-pgd} (2) The PDM being used is strong enough to represent $p(x_0)$, which can be largely determined by the dataset they are trained on. 

It is \textbf{effortless} to design a PDM-Pure. The key idea behind this method is to run SDEdit in the pixel space. Given any strong pixel-space diffusion model, we add a small noise to the protected images and run the denoising process (Figure~\ref{fig:purification_pipeline}), and then the adversarial pattern should be removed. The key idea of PDM-Pure is simple. In practice, we need to adjust the pipeline to fit the resolution of the PDMs being used. 

Here, we explain in detail how to adapt DeepFloyd-IF~\cite{deepfloyd}, the strongest open-source PDM as far as we know, for PDM-Pure. DeepFloyd-IF is a cascaded text-to-image diffusion model trained on 1.2B text-image pairs from LAION dataset~\cite{schuhmann2022laion}. It contains three stages named IF-Stage I, II, and III. Here we only use Stage II and III since Stage I works in a resolution of $64$ which is too low. Given a perturbed image $x_{W\times H}$ sized $W\times H$, we first resize it into $x_{64\times 64}$ and $x_{256\times 256}$. Then we use a general prompt  $\mathcal{P}$ to do SDEdit~\cite{meng2021sdedit} using the Stage II model: 
% $x_t = \textbf{IF-II}(x_{t+1}, x_{64\times 64}, p)$

\begin{equation}
    x_t = \textbf{IF-II}(x_{t+1}, x_{64\times 64}, \mathcal{P})
\end{equation}

where $t=T_{\text{edit}}-1, ...,1, 0$, $x_{T_{\text{edit}}}=x_{256\times 256}$. A larger $T_{\text{edit}}$ may be used for larger noise. $x_0$ is the purified image we get in the $256\times 256$ resolution space, where the adversarial patterns should be already purified. We can then use IF Stage III to further up-sample it into $1024\times 1024$ with $x_{1024\times 1024} = \textbf{IF-III}(x_0, p)$. Finally, we can sample into $H\times W$ as we want through downsampling. This whole process is demonstrated in Figure~\ref{fig:purification_pipeline}. After purification, the image is no longer adversarial to the targeted diffusion models and can be effectively used in downstream tasks.

In the main paper, we conduct experiments on purifying protected images sized $512\times 512$. For images with a larger resolution, purifying in the resolution of $256\times 256$ may lose information. In Appendix~\ref{supp:section:pdm_pure_for_higher_resolution} we show PDM-Pure can also applied to purify patches of high-resolution inputs.






\input{figText/table_purify}

\section{Experiments}

In this section, we conduct experiments on various attacking methods and various models to support the following two conclusions:

\begin{itemize}[parsep=0pt,topsep=0pt,leftmargin=12pt]
    \item \textbf{(C1)}: PDMs are much more adversarial robust than LDMs, and PDMs can not be effectively attacked using all the existing attacks for LDMs.
    \item \textbf{(C2)}: PDMs can be applied to effectively purify all of the existing protective perturbations. Our PDM-Pure based on DeepFloyd-IF shows state-of-the-art purification power.
    % \item \textbf{(C3)}: Pixel is a barrier for us to achieve real protection against diffusion-based mimicry. PDM-Pure can make the protective perturbation no more protective, and there is currently no effective way to attack PDMs.
\end{itemize}

\subsection{Models, Datasets, and Metrics} 
The models we used can be categorized into LDMs and PDMs. For LDMs, we use Stable Diffusion V-1.4, V-1.5 (SD-V-1.4, SD-V-1.5)~\cite{ldm}, and Diffusion Transformer (DiT-XL/2)~\cite{dit}, and for PDMs we use Guided Diffusion (GD)~\cite{guideddiffusion} trained on ImageNet~\cite{deng2009imagenet}, and DeepFloyd Stage I and Stage II~\cite{deepfloyd}. 

For models trained on the ImageNet (DiT, GD), we run adversarial attacks and purification on a 1k subset of the ImageNet validation dataset. For models trained on LAION, we run tests on the dataset proposed in~\cite{sdsattack}, which includes $400$ cartoon, artwork, landscape, and portrait images. The metrics for testing the quality of generated images are included in the Appendix.

For protection methods, we consider almost all the representative approaches, including AdvDM~\cite{advdm}, SDS~\cite{sdsattack}, Mist~\cite{liang2023mist}, Mist-v2~\cite{mist-v2}, Photoguard~\cite{salman2023raising} and Glaze~\cite{glaze}. We also test the methods in the design space proposed in ~\cite{sdsattack}, including  SDS(-), AdvDM(-), and SDST. In contrast to other existing methods, they are based on gradient descent and have shown great performance in deceiving the LDMs.






\subsection{(C1) PDMs are Much More Robust Than We Think} 

In Table~\ref{quant_protect}, we attack different LDMs and PDMs with one of the most popular adversarial loss~\cite{mist-v2} in Equation~\ref{semantic_loss} and Equation~\ref{pixel_diffusion_adversarial_loss}, which can be interpreted as fooling the denoiser using a Monte-Carlo-based loss. Given the attacked samples, we test the SDEdit results on the attacked samples, which can be generally used to test whether the samples are adversarial for the diffusion model or not. We use FID-score~\cite{fid}, SSIM~\cite{ssim}, LPIPS~\cite{lpips}, and IA-Score~\cite{la-score} to measure the quality of the attack. If the quality of generated images decreases a lot compared with editing the clean images, then the attack is successful. We can see that LDMs can be easily attacked, while PDMs are quite robust; the quality of the edited images is still good. We also show some visualizations in Figure~\ref{fig:attack_various_models}, which illustrates that the perturbation will affect the LDMs but not the PDMs.

To further investigate how robust PDM is, we test other advanced attacking methods, including the End-to-End Diffusion Attacks (E2E-Photoguard) proposed in~\cite{salman2023raising} and the Improved Targeted Attack (ITA) proposed in ~\cite{mist-v2}. Though the End-to-End attack is usually impractical to run, it shows the strongest performance to attack LDMs.  We find that both attacks are not successful in PDM settings. We show attacked samples and edited samples in Figure~\ref{fig:attack_various_models} as well as the Appendix. In conclusion, existing adversarial attack methods for diffusion models can only work for the LDMs, and PDMs are more robust than we think.


\subsection{(C2) PDM-Pure: A Universal Purifier that is Simple yet Effective}

PDM-Pure is simple: basically, we just run SDEdit to purify the protected image in the pixel space. Given our assumption that PDMs are quite robust, we can use PDMs trained on large-scale datasets as a universal black-box purifier. We follow the model pipeline introduced in Section~\ref{sec:pdm_pure} and purify images protected by various methods in Table~\ref{quant_purify}.

PDM-Pure is effective: from Table~\ref{quant_purify} we can see that the purification will remove adversarial patterns for all the protection methods we tested, largely decreasing the FID score for the SDEdit task. Also, we test the protected images and purified images in more tasks including Image Inpainting~\cite{song2020score}, Textual-Inversion~\cite{textualinversion}, and LoRA customization~\cite{lora} in Figure~\ref{fig:purification_results}. Both qualitative and quantitative results show that the purified images are no more adversarial and can be effectively edited or imitated in different tasks without any obstruction. 

Also, PDM-Pure shows SOTA results compared with previous purification methods, including some simple purifiers based on compression and filtering like Adv-Clean, crop-and-resize, JPEG Compression, and  SDEdit-based methods like GrIDPure~\cite{zhao2023can}, which uses patchified SDEdit with a GD~\cite{guideddiffusion}. We also add LDM-Pure as a baseline to show that LDMs can not be used to purify the protected images. For GrIDPure, we use Guided-Diffusion trained on ImageNet to run patchified purification. All the experiments are conducted on the datasets collected in ~\cite{sdsattack} under the resolution of $512\times 512$. Results for higher resolutions are presented in Appendix~\ref{supp:section:pdm_pure_for_higher_resolution}.




% \subsection{(C3) Pixel is A Barrier: What Should We Do in the Future?}

% \chen{why is this in the experiment section? Are any experiments involved?} Pixel is a barrier for us to do real protection against adversarial attacks. Since PDMs are quite robust, they cannot be easily attacked and can even be used to purify the protective perturbations, challenging the current assumption for safety protection of generative diffusion models. The community should rethink the problem of adversarial samples for generative diffusion models and rethink can we rely on them to protect unauthorized images. Hence, diffusion models turn out to be quite robust, more research should be conducted to study them and the reason behind them. If the robustness can be verified and guaranteed, we may rely on it as a new structure for many other tasks.\haotian{move it the the conclusion}


\section{Conclusions and Future Directions}


In this paper, we present novel insights that while many studies demonstrate the ease of finding adversarial samples for Latent Diffusion Models (LDMs), Pixel Diffusion Models (PDMs) exhibit far greater adversarial robustness than previously assumed. We are the first to investigate the adversarial samples for PDMs, revealing a surprising discovery that existing attacks fail to fool PDMs. Leveraging this insight, we propose utilizing strong PDMs as universal purifiers, resulting in PDM-Pure, a simple yet effective framework that can generate protective perturbations in a black-box manner. 


Pixel is a barrier for us to do
real protection against adversarial attacks. Since PDMs are quite robust, they cannot be easily attacked.
PDMs can even be used to purify the protective perturbations, challenging the current assumption for
the safe protection of generative diffusion models. We advocate rethinking the problem of
adversarial samples for generative diffusion models and 
unauthorized image protection based on it. 
% Diffusion models turn out to be quite robust, 
More rigorous study can be
conducted to better understand the mechanism behind the robustness of PDMs. Furthermore, we can utilize it as a new structure for many other tasks

\input{figText/purification_results}

%% test citation
% \citep{diff-pgd, sdsattack}

{
\small
\bibliographystyle{abbrvnat}
\bibliography{bibliography}
}

\newpage  
\tableofcontents

\input{tex/supp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}