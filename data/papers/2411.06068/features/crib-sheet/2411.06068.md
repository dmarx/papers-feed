- **Zyda-2 Overview**: A five trillion token dataset for language model pretraining, used to train the Zamba2 series of models.
- **Dataset Quality Determinants**: Quality and scale are critical for language model performance; Zyda-2 aims to enhance both compared to previous datasets like Zyda-1.
- **Dataset Sources**: Constructed from DCLM, FineWeb-Edu2, Zyda-1, and Dolma-CC, emphasizing high-quality educational content.
- **Cross-Deduplication Process**:
  - Utilizes approximate minhash LSH deduplication with a signature size of 128 and 85% Jaccard similarity threshold.
  - Removes approximately 11% of total tokens (13% of documents) due to deduplication.
  - Maintains a ranking for document retention: FineWeb-Edu2 > DCLM > Zyda-1 > Dolma-CC.
- **Model-Based Filtering**:
  - Applied to Zyda-1 and Dolma-CC to enhance quality, using a quality-classifier model from NeMo Curator.
  - Keeping the highest quality 10-20% of documents significantly improved model performance.
- **Final Dataset Composition**: Approximately five trillion tokens post-processing, with significant contributions from DCLM and FineWeb-Edu.
- **Performance Evaluation**:
  - Utilizes annealing training approach for evaluating dataset quality, showing better sensitivity to dataset composition changes.
  - Zyda-2 outperforms leading datasets in aggregate evaluation scores due to enhanced filtering and deduplication.
- **Optimal Weighting of Datasets**: Uniform weighting is suboptimal; upweighting FineWeb-Edu improves performance, highlighting the importance of diversity in sources.
- **Key Findings**:
  - Model-based filtering significantly boosts performance on benchmarks like MMLU and ARC.
  - Questions remain regarding the impact of deduplication and the robustness of quality classifiers.
- **Open Source Release**: Zyda-2 is released under a permissive open license (ODC-BY) and available at [HuggingFace](https://huggingface.co/datasets/Zyphra/Zyda-2).