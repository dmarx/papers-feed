- **Zyda-2 Overview**: A 5 trillion token dataset for language model pretraining, used to train the Zamba2 series of models.
- **Dataset Sources**: Constructed from DCLM, FineWeb-Edu2, Zyda-1, and Dolma-CC.
- **Quality Improvement**: Utilizes cross-deduplication and model-based filtering to enhance dataset quality.
- **Cross-Deduplication Method**: 
  - Approximate minhash LSH deduplication with:
    - Signature size: 128
    - Character-based 25grams
    - 8 bands for 85% Jaccard similarity threshold.
  - Retains top documents based on dataset ranking: FineWeb-Edu2 > DCLM > Zyda-1 > Dolma-CC.
- **Model-Based Filtering**: 
  - Applied to Zyda-1 and Dolma-CC using a quality-classifier model (DeBERTa).
  - Focused on retaining the highest quality 10-20% of documents for improved performance.
- **Performance Evaluation**: 
  - Utilizes annealing approach for training, showing better sensitivity in evaluation scores.
  - Zyda-2 outperforms leading datasets and its components due to enhanced filtering and diversity.
- **Optimal Weighting**: 
  - Uniform weighting of datasets was suboptimal; upweighting FineWeb-Edu improved performance.
  - Zyda-1 and Dolma-CC contribute diversity despite their smaller token counts.
- **Release Information**: Zyda-2 is available under an open-source license (ODC-BY) at [HuggingFace](https://huggingface.co/datasets/Zyphra/Zyda-2).
- **Key Findings**: 
  - Model-based filtering is effective for unfiltered datasets but not for already filtered ones.
  - Internal duplicates in datasets raise questions about their impact on model training.