\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2024)Bai, He, Wang, Guo, Hu, Liu, and Bian]{bai2024uniedit}
Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian.
\newblock Uniedit: A unified tuning-free framework for video motion and appearance editing.
\newblock \emph{arXiv preprint arXiv:2402.13185}, 2024.

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, et~al.]{blattmann2023stable}
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et~al.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Chameleon(2024)]{team2024chameleon}
Chameleon.
\newblock Chameleon: Mixed-modal early-fusion foundation models.
\newblock \emph{arXiv preprint arXiv:2405.09818}, 2024.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Li, Lin, Zhu, Wang, Yuan, Zhou, Cheng, and Yuan]{chen2024od}
Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinghua Cheng, and Li~Yuan.
\newblock Od-vae: An omni-dimensional video compressor for improving latent video diffusion model.
\newblock \emph{arXiv preprint arXiv:2409.01199}, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Guo, He, Zhang, Zhang, Yang, Zhao, and Bian]{chen2024igor}
Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek~Cathera Yang, Li~Zhao, and Jiang Bian.
\newblock Igor: Image-goal representations are the atomic control units for foundation models in embodied ai.
\newblock \emph{arXiv preprint arXiv:2411.00785}, 2024{\natexlab{b}}.

\bibitem[CompVis()]{repolatent}
CompVis.
\newblock latent-diffusion.
\newblock \url{https://github.com/CompVis/latent-diffusion}.

\bibitem[Guo et~al.(2024)Guo, Yang, Rao, Liang, Wang, Qiao, Agrawala, Lin, and Dai]{guo2023animatediff}
Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu~Qiao, Maneesh Agrawala, Dahua Lin, and Bo~Dai.
\newblock Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Ho et~al.(2022{\natexlab{a}})Ho, Chan, Saharia, Whang, Gao, Gritsenko, Kingma, Poole, Norouzi, Fleet, et~al.]{ho2022imagen}
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik~P Kingma, Ben Poole, Mohammad Norouzi, David~J Fleet, et~al.
\newblock Imagen video: High definition video generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.02303}, 2022{\natexlab{a}}.

\bibitem[Ho et~al.(2022{\natexlab{b}})Ho, Salimans, Gritsenko, Chan, Norouzi, and Fleet]{ho2022video}
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David~J Fleet.
\newblock Video diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 8633--8646, 2022{\natexlab{b}}.

\bibitem[Hore \& Ziou(2010)Hore and Ziou]{hore2010image}
Alain Hore and Djemel Ziou.
\newblock Image quality metrics: Psnr vs. ssim.
\newblock In \emph{2010 20th international conference on pattern recognition}, pp.\  2366--2369. IEEE, 2010.

\bibitem[hpcaitech()]{repoopensora}
hpcaitech.
\newblock Open-sora.
\newblock \url{https://github.com/hpcaitech/Open-Sora}.

\bibitem[HuggingFace()]{repomuse}
HuggingFace.
\newblock open-muse.
\newblock \url{https://github.com/huggingface/open-muse}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{Kingma2014}
Diederik~P. Kingma and Max Welling.
\newblock {Auto-Encoding Variational Bayes}.
\newblock In \emph{2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings}, 2014.

\bibitem[Kondratyuk et~al.(2024)Kondratyuk, Yu, Gu, Lezama, Huang, Schindler, Hornung, Birodkar, Yan, Chiu, et~al.]{kondratyuk2024videopoet}
Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et~al.
\newblock Videopoet: A large language model for zero-shot video generation.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Lei~Ba et~al.(2016)Lei~Ba, Kiros, and Hinton]{lei2016layer}
Jimmy Lei~Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Li et~al.(2023)Li, He, Wang, Li, Wang, Luo, Wang, Wang, and Qiao]{li2023videochat}
KunChang Li, Yinan He, Yi~Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu~Qiao.
\newblock Videochat: Chat-centric video understanding.
\newblock \emph{arXiv preprint arXiv:2305.06355}, 2023.

\bibitem[Liu et~al.(2024)Liu, Li, Wu, and Lee]{liu2024visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\bibitem[Mentzer et~al.(2024)Mentzer, Minnen, Agustsson, and Tschannen]{mentzer2024finite}
Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen.
\newblock Finite scalar quantization: Vq-vae made simple.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[NVIDIA()]{repocosmos}
NVIDIA.
\newblock Cosmos-tokenizer.
\newblock \url{https://github.com/NVIDIA/Cosmos-Tokenizer}.

\bibitem[OpenAI(2024)]{sora}
OpenAI.
\newblock Sora.
\newblock \url{https://openai.com/index/sora/}, 2024.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Patil et~al.(2024)Patil, Berman, Rombach, and von Platen]{patil2024amused}
Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen.
\newblock amused: An open muse reproduction.
\newblock \emph{arXiv preprint arXiv:2401.01808}, 2024.

\bibitem[PKU-YuanGroup()]{repoopensoraplan}
PKU-YuanGroup.
\newblock Open-sora-plan.
\newblock \url{https://github.com/PKU-YuanGroup/Open-Sora-Plan}.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[Singer et~al.(2023)Singer, Polyak, Hayes, Yin, An, Zhang, Hu, Yang, Ashual, Gafni, et~al.]{singer2023make}
Uriel Singer, Adam Polyak, Thomas Hayes, Xi~Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Sullivan et~al.(2012)Sullivan, Ohm, Han, and Wiegand]{sullivan2012overview}
Gary~J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand.
\newblock Overview of the high efficiency video coding (hevc) standard.
\newblock \emph{IEEE Transactions on circuits and systems for video technology}, 22\penalty0 (12):\penalty0 1649--1668, 2012.

\bibitem[TencentARC()]{repomagvit2}
TencentARC.
\newblock Open-magvit2.
\newblock \url{https://github.com/TencentARC/Open-MAGVIT2}.

\bibitem[Unterthiner et~al.(2018)Unterthiner, Van~Steenkiste, Kurach, Marinier, Michalski, and Gelly]{unterthiner2018towards}
Thomas Unterthiner, Sjoerd Van~Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly.
\newblock Towards accurate generative models of video: A new metric \& challenges.
\newblock \emph{arXiv preprint arXiv:1812.01717}, 2018.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neuralvqvae}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock In \emph{Advances in neural information processing systems}, volume~30, 2017.

\bibitem[Wang et~al.(2016)Wang, Gan, Hu, Lin, Jin, Song, Wang, Katsavounidis, Aaron, and Kuo]{wang2016mcl}
Haiqiang Wang, Weihao Gan, Sudeng Hu, Joe~Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang, Ioannis Katsavounidis, Anne Aaron, and C-C~Jay Kuo.
\newblock Mcl-jcv: a jnd-based h. 264/avc video quality assessment dataset.
\newblock In \emph{2016 IEEE international conference on image processing (ICIP)}, pp.\  1509--1513. IEEE, 2016.

\bibitem[Wang et~al.(2024)Wang, Jiang, Yuan, Peng, Wu, and Jiang]{wang2024omnitokenizer}
Junke Wang, Yi~Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang.
\newblock Omnitokenizer: A joint image-video tokenizer for visual generation.
\newblock \emph{arXiv preprint arXiv:2406.09399}, 2024.

\bibitem[Wang et~al.(2004)Wang, Bovik, Sheikh, and Simoncelli]{wang2004image}
Zhou Wang, Alan~C Bovik, Hamid~R Sheikh, and Eero~P Simoncelli.
\newblock Image quality assessment: from error visibility to structural similarity.
\newblock \emph{IEEE transactions on image processing}, 13\penalty0 (4):\penalty0 600--612, 2004.

\bibitem[Wu et~al.(2024)Wu, Chen, Wu, Ma, Liu, Pan, Liu, Xie, Yu, Ruan, et~al.]{wu2024janus}
Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et~al.
\newblock Janus: Decoupling visual encoding for unified multimodal understanding and generation.
\newblock \emph{arXiv preprint arXiv:2410.13848}, 2024.

\bibitem[Yan et~al.(2021)Yan, Zhang, Abbeel, and Srinivas]{yan2021videogpt}
Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas.
\newblock Videogpt: Video generation using vq-vae and transformers.
\newblock \emph{arXiv preprint arXiv:2104.10157}, 2021.

\bibitem[Yang et~al.(2024{\natexlab{a}})Yang, Du, Ghasemipour, Tompson, Kaelbling, Schuurmans, and Abbeel]{yang2024learning}
Sherry Yang, Yilun Du, Seyed Kamyar~Seyed Ghasemipour, Jonathan Tompson, Leslie~Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel.
\newblock Learning interactive real-world simulators.
\newblock In \emph{International Conference on Learning Representations}, 2024{\natexlab{a}}.

\bibitem[Yang et~al.(2024{\natexlab{b}})Yang, Walker, Parker-Holder, Du, Bruce, Barreto, Abbeel, and Schuurmans]{pmlr-v235-yang24z}
Sherry Yang, Jacob~C Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans.
\newblock Position: Video as the new language for real-world decision making.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}, volume 235 of \emph{Proceedings of Machine Learning Research}, pp.\  56465--56484. PMLR, 21--27 Jul 2024{\natexlab{b}}.

\bibitem[Yang et~al.(2024{\natexlab{c}})Yang, Teng, Zheng, Ding, Huang, Xu, Yang, Hong, Zhang, Feng, et~al.]{yang2024cogvideox}
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et~al.
\newblock Cogvideox: Text-to-video diffusion models with an expert transformer.
\newblock \emph{arXiv preprint arXiv:2408.06072}, 2024{\natexlab{c}}.

\bibitem[Yu et~al.(2022)Yu, Li, Koh, Zhang, Pang, Qin, Ku, Xu, Baldridge, and Wu]{yu2022vectorquantized}
Jiahui Yu, Xin Li, Jing~Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu.
\newblock Vector-quantized image modeling with improved {VQGAN}.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Yu et~al.(2023)Yu, Cheng, Sohn, Lezama, Zhang, Chang, Hauptmann, Yang, Hao, Essa, et~al.]{yu2023magvit}
Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos{\'e} Lezama, Han Zhang, Huiwen Chang, Alexander~G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et~al.
\newblock Magvit: Masked generative video transformer.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  10459--10469, 2023.

\bibitem[Yu et~al.(2024)Yu, Lezama, Gundavarapu, Versari, Sohn, Minnen, Cheng, Gupta, Gu, Hauptmann, et~al.]{yu2024language}
Lijun Yu, Jose Lezama, Nitesh~Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander~G Hauptmann, et~al.
\newblock Language model beats diffusion-tokenizer is key to visual generation.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and Wang]{zhang2018unreasonable}
Richard Zhang, Phillip Isola, Alexei~A Efros, Eli Shechtman, and Oliver Wang.
\newblock The unreasonable effectiveness of deep features as a perceptual metric.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  586--595, 2018.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Guo, He, Zhao, Xu, and Bian]{zhang2024videoicl}
Wentao Zhang, Junliang Guo, Tianyu He, Li~Zhao, Linli Xu, and Jiang Bian.
\newblock Video in-context learning.
\newblock \emph{arXiv preprint arXiv:2407.07356}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Wu, Li, Li, Ma, Liu, and Li]{zhang2024video}
Yuanhan Zhang, Jinming Wu, Wei Li, Bo~Li, Zejun Ma, Ziwei Liu, and Chunyuan Li.
\newblock Video instruction tuning with synthetic data.
\newblock \emph{arXiv preprint arXiv:2410.02713}, 2024{\natexlab{b}}.

\bibitem[Zhao et~al.(2024)Zhao, Zhang, Cun, Yang, Niu, Li, Hu, and Shan]{zhao2024cv}
Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan.
\newblock Cv-vae: A compatible video vae for latent generative video models.
\newblock \emph{arXiv preprint arXiv:2405.20279}, 2024.

\bibitem[Zheng et~al.(2022)Zheng, Vuong, Cai, and Phung]{zheng2022movq}
Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung.
\newblock Movq: Modulating quantized vectors for high-fidelity image generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 23412--23425, 2022.

\bibitem[Zhu et~al.(2024)Zhu, He, Tang, Guo, Chen, and Bian]{zhu2024compositional}
Hanxin Zhu, Tianyu He, Anni Tang, Junliang Guo, Zhibo Chen, and Jiang Bian.
\newblock Compositional 3d-aware video generation with llm director.
\newblock In \emph{Advances in neural information processing systems}, 2024.

\end{thebibliography}
