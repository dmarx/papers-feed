\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdin et~al.(2024)Abdin, Aneja, Awadalla, Awadallah, Awan, Bach, Bahree, Bakhtiari, Bao, Behl, et~al.]{abdin2024phi}
Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar~Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et~al.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{ArXiv preprint}, abs/2404.14219, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.14219}.

\bibitem[An et~al.(2024)An, Ma, Lin, Zheng, and Lou]{an2024make}
Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou.
\newblock Make your llm fully utilize the context.
\newblock \emph{ArXiv preprint}, abs/2404.16811, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.16811}.

\bibitem[BAAI()]{BAAI_Infinity-Instruct}
BAAI.
\newblock Infinity-instruct.
\newblock URL \url{https://huggingface.co/datasets/BAAI/Infinity-Instruct}.

\bibitem[Bai et~al.(2023)Bai, Lv, Zhang, Lyu, Tang, Huang, Du, Liu, Zeng, Hou, et~al.]{bai2023longbench}
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et~al.
\newblock Longbench: A bilingual, multitask benchmark for long context understanding.
\newblock \emph{ArXiv preprint}, abs/2308.14508, 2023.
\newblock URL \url{https://arxiv.org/abs/2308.14508}.

\bibitem[Chen et~al.(2023)Chen, Wong, Chen, and Tian]{chen2023extending}
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.
\newblock Extending context window of large language models via positional interpolation.
\newblock \emph{ArXiv preprint}, abs/2306.15595, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.15595}.

\bibitem[Computer(2023)]{together2023redpajama}
Together Computer.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Dao(2024)]{daoflashattention}
Tri Dao.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=mZn2Xyh9Ec}.

\bibitem[Ding et~al.(2023)Ding, Ma, Dong, Zhang, Huang, Wang, Zheng, and Wei]{ding2023longnet}
Jiayu Ding, Shuming Ma, Li~Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei.
\newblock Longnet: Scaling transformers to 1,000,000,000 tokens.
\newblock \emph{ArXiv preprint}, abs/2307.02486, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.02486}.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{ArXiv preprint}, abs/2407.21783, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.21783}.

\bibitem[Fu et~al.(2024)Fu, Panda, Niu, Yue, Hajishirzi, Kim, and Peng]{fu2024data}
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.
\newblock Data engineering for scaling language models to 128k context.
\newblock In \emph{Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=TaAqeo7lUh}.

\bibitem[Gao et~al.(2024{\natexlab{a}})Gao, Wu, Fu, and Hu]{gao2024quest}
Chaochen Gao, Xing Wu, Qi~Fu, and Songlin Hu.
\newblock Quest: Query-centric data synthesis approach for long-context scaling of large language model.
\newblock \emph{ArXiv preprint}, abs/2405.19846, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2405.19846}.

\bibitem[Gao et~al.(2024{\natexlab{b}})Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 2024{\natexlab{b}}.
\newblock URL \url{https://zenodo.org/records/12608602}.

\bibitem[Gao et~al.(2024{\natexlab{c}})Gao, Wettig, Yen, and Chen]{gao2024train}
Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen.
\newblock How to train long-context language models (effectively).
\newblock \emph{ArXiv preprint}, abs/2410.02660, 2024{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2410.02660}.

\bibitem[Ge et~al.(2024)Ge, Chan, Wang, Yu, Mi, and Yu]{ge2024scaling}
Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu.
\newblock Scaling synthetic data creation with 1,000,000,000 personas.
\newblock \emph{ArXiv preprint}, abs/2406.20094, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.20094}.

\bibitem[Hsieh et~al.(2024)Hsieh, Sun, Kriman, Acharya, Rekesh, Jia, Zhang, and Ginsburg]{hsieh2024ruler}
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg.
\newblock Ruler: What's the real context size of your long-context language models?
\newblock \emph{ArXiv preprint}, abs/2404.06654, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.06654}.

\bibitem[Hurst et~al.(2024)Hurst, Lerer, Goucher, Perelman, Ramesh, Clark, Ostrow, Welihinda, Hayes, Radford, et~al.]{hurst2024gpt}
Aaron Hurst, Adam Lerer, Adam~P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ~Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et~al.
\newblock Gpt-4o system card.
\newblock \emph{ArXiv preprint}, abs/2410.21276, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.21276}.

\bibitem[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, et~al.]{ivison2023camels}
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al.
\newblock Camels in a changing climate: Enhancing lm adaptation with tulu 2.
\newblock \emph{ArXiv preprint}, abs/2311.10702, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.10702}.

\bibitem[Jiang et~al.(2024{\natexlab{a}})Jiang, Li, Zhang, Wu, Luo, Ahn, Han, Abdi, Li, Lin, et~al.]{jiang2024minference}
Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir~H Abdi, Dongsheng Li, Chin-Yew Lin, et~al.
\newblock Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention.
\newblock \emph{ArXiv preprint}, abs/2407.02490, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2407.02490}.

\bibitem[Jiang et~al.(2024{\natexlab{b}})Jiang, Ma, and Chen]{jiang2024longrag}
Ziyan Jiang, Xueguang Ma, and Wenhu Chen.
\newblock Longrag: Enhancing retrieval-augmented generation with long-context llms.
\newblock \emph{ArXiv preprint}, abs/2406.15319, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2406.15319}.

\bibitem[Jimenez et~al.(2024)Jimenez, Yang, Wettig, Yao, Pei, Press, and Narasimhan]{jimenezswe}
Carlos~E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik~R. Narasimhan.
\newblock Swe-bench: Can language models resolve real-world github issues?
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=VTF8yNQM66}.

\bibitem[Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen, and Yih]{Karpukhin2020DensePR}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 6769--6781, Online, 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.550}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.550}.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems Principles}, pages 611--626, 2023.

\bibitem[Lambert et~al.(2024)Lambert, Morrison, Pyatkin, Huang, Ivison, Brahman, Miranda, Liu, Dziri, Lyu, et~al.]{lambert2024t}
Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James~V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et~al.
\newblock T$\backslash$" ulu 3: Pushing frontiers in open language model post-training.
\newblock \emph{ArXiv preprint}, abs/2411.15124, 2024.
\newblock URL \url{https://arxiv.org/abs/2411.15124}.

\bibitem[Lee et~al.(2024)Lee, Chen, Dai, Dua, Sachan, Boratko, Luan, Arnold, Perot, Dalmia, et~al.]{lee2024can}
Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra~Singh Sachan, Michael Boratko, Yi~Luan, S{\'e}bastien~MR Arnold, Vincent Perot, Siddharth Dalmia, et~al.
\newblock Can long-context language models subsume retrieval, rag, sql, and more?
\newblock \emph{ArXiv preprint}, abs/2406.13121, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.13121}.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"{u}}ttler, Lewis, Yih, Rockt{\"{a}}schel, Riedel, and Kiela]{lewis2020retrieval}
Patrick S.~H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"{u}}ttler, Mike Lewis, Wen{-}tau Yih, Tim Rockt{\"{a}}schel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html}.

\bibitem[Li et~al.(2024)Li, Yu, Zhou, Schick, Levy, Zettlemoyer, Weston, and Lewis]{li2023self}
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis.
\newblock Self-alignment with instruction backtranslation.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=1oijHJBRsT}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Chen, Lu, Jiang, Han, Zhang, Chen, Zhang, Ding, Zhang, et~al.]{liu2024retrievalattention}
Di~Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi~Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, et~al.
\newblock Retrievalattention: Accelerating long-context llm inference via vector retrieval.
\newblock \emph{ArXiv preprint}, abs/2409.10516, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2409.10516}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Yan, Zaharia, and Abbeel]{liu2024world}
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.
\newblock World model on million-length video and language with ringattention.
\newblock \emph{ArXiv preprint}, abs/2402.08268, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2402.08268}.

\bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Zaharia, and Abbeel]{liu2023ring}
Hao Liu, Matei Zaharia, and Pieter Abbeel.
\newblock Ringattention with blockwise transformers for near-infinite context.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=WsRHpHH4s0}.

\bibitem[Liu et~al.(2024{\natexlab{d}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang]{liu2024lost}
Nelson~F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:\penalty0 157--173, 2024{\natexlab{d}}.
\newblock \doi{10.1162/tacl_a_00638}.
\newblock URL \url{https://aclanthology.org/2024.tacl-1.9}.

\bibitem[Park et~al.(2023)Park, O'Brien, Cai, Morris, Liang, and Bernstein]{park2023generative}
Joon~Sung Park, Joseph O'Brien, Carrie~Jun Cai, Meredith~Ringel Morris, Percy Liang, and Michael~S Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock In \emph{Proceedings of the 36th annual acm symposium on user interface software and technology}, pages 1--22, 2023.

\bibitem[Pekelis et~al.(2024)Pekelis, Feil, Moret, Huang, and Peng]{gradientlongcontextllama3}
Leonid Pekelis, Michael Feil, Forrest Moret, Mark Huang, and Tiffany Peng.
\newblock Llama 3 gradient: A series of long context models, 2024.
\newblock URL \url{https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models}.

\bibitem[Penedo et~al.(2024)Penedo, Kydl{\'\i}{\v{c}}ek, Lozhkov, Mitchell, Raffel, Von~Werra, Wolf, et~al.]{penedo2024fineweb}
Guilherme Penedo, Hynek Kydl{\'\i}{\v{c}}ek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von~Werra, Thomas Wolf, et~al.
\newblock The fineweb datasets: Decanting the web for the finest text data at scale.
\newblock \emph{ArXiv preprint}, abs/2406.17557, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.17557}.

\bibitem[Peng et~al.(2024)Peng, Quesnelle, Fan, and Shippole]{pengyarn}
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
\newblock Yarn: Efficient context window extension of large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=wHBfxhZu1u}.

\bibitem[Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman]{rein2023gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R. Bowman.
\newblock Gpqa: A graduate-level google-proof q\&a benchmark, 2023.

\bibitem[Shaham et~al.(2023)Shaham, Ivgi, Efrat, Berant, and Levy]{shaham2023zeroscrolls}
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.
\newblock {Z}ero{SCROLLS}: A zero-shot benchmark for long text understanding.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 7977--7989, Singapore, 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-emnlp.536}.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.536}.

\bibitem[Sprague et~al.(2024)Sprague, Ye, Bostrom, Chaudhuri, and Durrett]{sprague2024musrtestinglimitschainofthought}
Zayne Sprague, Xi~Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett.
\newblock Musr: Testing the limits of chain-of-thought with multistep soft reasoning.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=jenyYQzue1}.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Sun et~al.(2024)Sun, Dong, Zhu, Huang, Wang, Ma, Zhang, Wang, and Wei]{sun2024you}
Yutao Sun, Li~Dong, Yi~Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei.
\newblock You only cache once: Decoder-decoder architectures for language models.
\newblock \emph{ArXiv preprint}, abs/2405.05254, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.05254}.

\bibitem[Suzgun et~al.(2023)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, and Wei]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc Le, Ed~Chi, Denny Zhou, and Jason Wei.
\newblock Challenging {BIG}-bench tasks and whether chain-of-thought can solve them.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 13003--13051, Toronto, Canada, 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.824}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.824}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ArXiv preprint}, abs/2302.13971, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Yang, Huang, Yang, Majumder, and Wei]{wang2023improving}
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei.
\newblock Improving text embeddings with large language models.
\newblock \emph{ArXiv preprint}, abs/2401.00368, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2401.00368}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Ma, Zhang, Ni, Chandra, Guo, Ren, Arulraj, He, Jiang, Li, Ku, Wang, Zhuang, Fan, Yue, and Chen]{wang2024mmluprorobustchallengingmultitask}
Yubo Wang, Xueguang Ma, Ge~Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen.
\newblock Mmlu-pro: A more robust and challenging multi-task language understanding benchmark, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2406.01574}.

\bibitem[Xiong et~al.(2024)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, Khabsa, Fang, Mehdad, Narang, Malik, Fan, Bhosale, Edunov, Lewis, Wang, and Ma]{xiong2024effective}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.
\newblock Effective long-context scaling of foundation models.
\newblock In Kevin Duh, Helena Gomez, and Steven Bethard, editors, \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 4643--4663, Mexico City, Mexico, 2024. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2024.naacl-long.260}.

\bibitem[Zhang et~al.(2024)Zhang, Chen, Hu, Xu, Chen, Hao, Han, Thai, Wang, Liu, et~al.]{zhang2024bench}
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu~Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, et~al.
\newblock \(\infty\) bench: Extending long context evaluation beyond 100k tokens.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 15262--15277, 2024.

\bibitem[Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou]{zhou2023instructionfollowing}
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou.
\newblock Instruction-following evaluation for large language models.
\newblock \emph{ArXiv preprint}, abs/2311.07911, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.07911}.

\bibitem[Zhu et~al.(2024)Zhu, Yang, Wang, Song, Wu, Wei, and Li]{zhu2023pose}
Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.
\newblock Pose: Efficient context window extension of llms via positional skip-wise training.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=3Z1gxuAQrA}.

\end{thebibliography}
