@inproceedings{Karpukhin2020DensePR,
 address = {Online},
 author = {Karpukhin, Vladimir  and
Oguz, Barlas  and
Min, Sewon  and
Lewis, Patrick  and
Wu, Ledell  and
Edunov, Sergey  and
Chen, Danqi  and
Yih, Wen-tau},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.550},
 editor = {Webber, Bonnie  and
Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 pages = {6769--6781},
 publisher = {Association for Computational Linguistics},
 title = {Dense Passage Retrieval for Open-Domain Question Answering},
 url = {https://aclanthology.org/2020.emnlp-main.550},
 year = {2020}
}

@inproceedings{zhu2023pose,
 author = {Dawei Zhu and
Nan Yang and
Liang Wang and
Yifan Song and
Wenhao Wu and
Furu Wei and
Sujian Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Zhu00SWWL24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise
Training},
 url = {https://openreview.net/forum?id=3Z1gxuAQrA},
 year = {2024}
}

@article{gao2024train,
 author = {Gao, Tianyu and Wettig, Alexander and Yen, Howard and Chen, Danqi},
 journal = {ArXiv preprint},
 title = {How to train long-context language models (effectively)},
 url = {https://arxiv.org/abs/2410.02660},
 volume = {abs/2410.02660},
 year = {2024}
}

@article{chen2023extending,
 author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
 journal = {ArXiv preprint},
 title = {Extending context window of large language models via positional interpolation},
 url = {https://arxiv.org/abs/2306.15595},
 volume = {abs/2306.15595},
 year = {2023}
}

@inproceedings{liu2023ring,
 author = {Hao Liu and
Matei Zaharia and
Pieter Abbeel},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/0055ZA24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {RingAttention with Blockwise Transformers for Near-Infinite Context},
 url = {https://openreview.net/forum?id=WsRHpHH4s0},
 year = {2024}
}

@article{hsieh2024ruler,
 author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
 journal = {ArXiv preprint},
 title = {RULER: What's the Real Context Size of Your Long-Context Language Models?},
 url = {https://arxiv.org/abs/2404.06654},
 volume = {abs/2404.06654},
 year = {2024}
}

@article{wang2023improving,
 author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
 journal = {ArXiv preprint},
 title = {Improving text embeddings with large language models},
 url = {https://arxiv.org/abs/2401.00368},
 volume = {abs/2401.00368},
 year = {2024}
}

@article{jiang2024minference,
 author = {Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others},
 journal = {ArXiv preprint},
 title = {Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention},
 url = {https://arxiv.org/abs/2407.02490},
 volume = {abs/2407.02490},
 year = {2024}
}

@article{liu2024retrievalattention,
 author = {Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and others},
 journal = {ArXiv preprint},
 title = {RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval},
 url = {https://arxiv.org/abs/2409.10516},
 volume = {abs/2409.10516},
 year = {2024}
}

@article{hurst2024gpt,
 author = {Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
 journal = {ArXiv preprint},
 title = {GPT-4o System Card},
 url = {https://arxiv.org/abs/2410.21276},
 volume = {abs/2410.21276},
 year = {2024}
}

@inproceedings{lewis2020retrieval,
 author = {Patrick S. H. Lewis and
Ethan Perez and
Aleksandra Piktus and
Fabio Petroni and
Vladimir Karpukhin and
Naman Goyal and
Heinrich K{\"{u}}ttler and
Mike Lewis and
Wen{-}tau Yih and
Tim Rockt{\"{a}}schel and
Sebastian Riedel and
Douwe Kiela},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
 year = {2020}
}

@article{jiang2024longrag,
 author = {Jiang, Ziyan and Ma, Xueguang and Chen, Wenhu},
 journal = {ArXiv preprint},
 title = {Longrag: Enhancing retrieval-augmented generation with long-context llms},
 url = {https://arxiv.org/abs/2406.15319},
 volume = {abs/2406.15319},
 year = {2024}
}

@article{lee2024can,
 author = {Lee, Jinhyuk and Chen, Anthony and Dai, Zhuyun and Dua, Dheeru and Sachan, Devendra Singh and Boratko, Michael and Luan, Yi and Arnold, S{\'e}bastien MR and Perot, Vincent and Dalmia, Siddharth and others},
 journal = {ArXiv preprint},
 title = {Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?},
 url = {https://arxiv.org/abs/2406.13121},
 volume = {abs/2406.13121},
 year = {2024}
}

@inproceedings{fu2024data,
 author = {Yao Fu and
Rameswar Panda and
Xinyao Niu and
Xiang Yue and
Hannaneh Hajishirzi and
Yoon Kim and
Hao Peng},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/FuPNYHK024.bib},
 booktitle = {Forty-first International Conference on Machine Learning, {ICML} 2024,
Vienna, Austria, July 21-27, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 02 Sep 2024 01:00:00 +0200},
 title = {Data Engineering for Scaling Language Models to 128K Context},
 url = {https://openreview.net/forum?id=TaAqeo7lUh},
 year = {2024}
}

@inproceedings{xiong2024effective,
 address = {Mexico City, Mexico},
 author = {Xiong, Wenhan  and
Liu, Jingyu  and
Molybog, Igor  and
Zhang, Hejia  and
Bhargava, Prajjwal  and
Hou, Rui  and
Martin, Louis  and
Rungta, Rashi  and
Sankararaman, Karthik Abinav  and
Oguz, Barlas  and
Khabsa, Madian  and
Fang, Han  and
Mehdad, Yashar  and
Narang, Sharan  and
Malik, Kshitiz  and
Fan, Angela  and
Bhosale, Shruti  and
Edunov, Sergey  and
Lewis, Mike  and
Wang, Sinong  and
Ma, Hao},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 editor = {Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven},
 pages = {4643--4663},
 publisher = {Association for Computational Linguistics},
 title = {Effective Long-Context Scaling of Foundation Models},
 url = {https://aclanthology.org/2024.naacl-long.260},
 year = {2024}
}

@inproceedings{zhang2024bench,
 author = {Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and others},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {15262--15277},
 title = {\(\infty\) Bench: Extending long context evaluation beyond 100k tokens},
 year = {2024}
}

@article{bai2023longbench,
 author = {Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
 journal = {ArXiv preprint},
 title = {Longbench: A bilingual, multitask benchmark for long context understanding},
 url = {https://arxiv.org/abs/2308.14508},
 volume = {abs/2308.14508},
 year = {2023}
}

@inproceedings{pengyarn,
 author = {Bowen Peng and
Jeffrey Quesnelle and
Honglu Fan and
Enrico Shippole},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/PengQFS24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {YaRN: Efficient Context Window Extension of Large Language Models},
 url = {https://openreview.net/forum?id=wHBfxhZu1u},
 year = {2024}
}

@article{an2024make,
 author = {An, Shengnan and Ma, Zexiong and Lin, Zeqi and Zheng, Nanning and Lou, Jian-Guang},
 journal = {ArXiv preprint},
 title = {Make Your LLM Fully Utilize the Context},
 url = {https://arxiv.org/abs/2404.16811},
 volume = {abs/2404.16811},
 year = {2024}
}

@article{gao2024quest,
 author = {Gao, Chaochen and Wu, Xing and Fu, Qi and Hu, Songlin},
 journal = {ArXiv preprint},
 title = {Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model},
 url = {https://arxiv.org/abs/2405.19846},
 volume = {abs/2405.19846},
 year = {2024}
}

@article{sun2024you,
 author = {Sun, Yutao and Dong, Li and Zhu, Yi and Huang, Shaohan and Wang, Wenhui and Ma, Shuming and Zhang, Quanlu and Wang, Jianyong and Wei, Furu},
 journal = {ArXiv preprint},
 title = {You only cache once: Decoder-decoder architectures for language models},
 url = {https://arxiv.org/abs/2405.05254},
 volume = {abs/2405.05254},
 year = {2024}
}

@inproceedings{kwon2023efficient,
 author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
 booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
 pages = {611--626},
 title = {Efficient memory management for large language model serving with pagedattention},
 year = {2023}
}

@inproceedings{daoflashattention,
 author = {Tri Dao},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Dao24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {FlashAttention-2: Faster Attention with Better Parallelism and Work
Partitioning},
 url = {https://openreview.net/forum?id=mZn2Xyh9Ec},
 year = {2024}
}

@article{dubey2024llama,
 author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
 journal = {ArXiv preprint},
 title = {The llama 3 herd of models},
 url = {https://arxiv.org/abs/2407.21783},
 volume = {abs/2407.21783},
 year = {2024}
}

@inproceedings{xiao2023efficient,
 author = {Guangxuan Xiao and
Yuandong Tian and
Beidi Chen and
Song Han and
Mike Lewis},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/XiaoTCHL24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {Efficient Streaming Language Models with Attention Sinks},
 url = {https://openreview.net/forum?id=NG7sS51zVF},
 year = {2024}
}

@misc{gradientlongcontextllama3,
 author = {Leonid Pekelis and Michael Feil and Forrest Moret and Mark Huang and Tiffany Peng},
 title = {Llama 3 Gradient: A series of long context models},
 url = {https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models},
 year = {2024}
}

@article{liu2024lost,
 address = {Cambridge, MA},
 author = {Liu, Nelson F.  and
Lin, Kevin  and
Hewitt, John  and
Paranjape, Ashwin  and
Bevilacqua, Michele  and
Petroni, Fabio  and
Liang, Percy},
 doi = {10.1162/tacl_a_00638},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {157--173},
 publisher = {MIT Press},
 title = {Lost in the Middle: How Language Models Use Long Contexts},
 url = {https://aclanthology.org/2024.tacl-1.9},
 volume = {12},
 year = {2024}
}

@article{su2024roformer,
 author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
 journal = {Neurocomputing},
 pages = {127063},
 publisher = {Elsevier},
 title = {Roformer: Enhanced transformer with rotary position embedding},
 volume = {568},
 year = {2024}
}

@inproceedings{ding2024longrope,
 author = {Yiran Ding and
Li Lyna Zhang and
Chengruidong Zhang and
Yuanyuan Xu and
Ning Shang and
Jiahang Xu and
Fan Yang and
Mao Yang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/DingZZXSX0Y24.bib},
 booktitle = {Forty-first International Conference on Machine Learning, {ICML} 2024,
Vienna, Austria, July 21-27, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 21 Oct 2024 01:00:00 +0200},
 title = {LongRoPE: Extending {LLM} Context Window Beyond 2 Million Tokens},
 url = {https://openreview.net/forum?id=ONOtpXLqqw},
 year = {2024}
}

@article{ding2023longnet,
 author = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu},
 journal = {ArXiv preprint},
 title = {Longnet: Scaling transformers to 1,000,000,000 tokens},
 url = {https://arxiv.org/abs/2307.02486},
 volume = {abs/2307.02486},
 year = {2023}
}

@article{penedo2024fineweb,
 author = {Penedo, Guilherme and Kydl{\'\i}{\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others},
 journal = {ArXiv preprint},
 title = {The fineweb datasets: Decanting the web for the finest text data at scale},
 url = {https://arxiv.org/abs/2406.17557},
 volume = {abs/2406.17557},
 year = {2024}
}

@software{together2023redpajama,
 author = {Together Computer},
 title = {RedPajama: an Open Dataset for Training Large Language Models},
 url = {https://github.com/togethercomputer/RedPajama-Data},
 year = {2023}
}

@inproceedings{shaham2023zeroscrolls,
 address = {Singapore},
 author = {Shaham, Uri  and
Ivgi, Maor  and
Efrat, Avia  and
Berant, Jonathan  and
Levy, Omer},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.536},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {7977--7989},
 publisher = {Association for Computational Linguistics},
 title = {{Z}ero{SCROLLS}: A Zero-Shot Benchmark for Long Text Understanding},
 url = {https://aclanthology.org/2023.findings-emnlp.536},
 year = {2023}
}

@article{glm2024chatglm,
 author = {GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and Lai, Hanyu and others},
 journal = {ArXiv preprint},
 title = {ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},
 url = {https://arxiv.org/abs/2406.12793},
 volume = {abs/2406.12793},
 year = {2024}
}

@article{achiam2023gpt,
 author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
 journal = {ArXiv preprint},
 title = {Gpt-4 technical report},
 url = {https://arxiv.org/abs/2303.08774},
 volume = {abs/2303.08774},
 year = {2023}
}

@article{abdin2024phi,
 author = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
 journal = {ArXiv preprint},
 title = {Phi-3 technical report: A highly capable language model locally on your phone},
 url = {https://arxiv.org/abs/2404.14219},
 volume = {abs/2404.14219},
 year = {2024}
}

@article{liu2024world,
 author = {Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
 journal = {ArXiv preprint},
 title = {World model on million-length video and language with ringattention},
 url = {https://arxiv.org/abs/2402.08268},
 volume = {abs/2402.08268},
 year = {2024}
}

@inproceedings{jimenezswe,
 author = {Carlos E. Jimenez and
John Yang and
Alexander Wettig and
Shunyu Yao and
Kexin Pei and
Ofir Press and
Karthik R. Narasimhan},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/JimenezYWYPPN24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 29 Jul 2024 01:00:00 +0200},
 title = {SWE-bench: Can Language Models Resolve Real-world Github Issues?},
 url = {https://openreview.net/forum?id=VTF8yNQM66},
 year = {2024}
}

@inproceedings{park2023generative,
 author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
 booktitle = {Proceedings of the 36th annual acm symposium on user interface software and technology},
 pages = {1--22},
 title = {Generative agents: Interactive simulacra of human behavior},
 year = {2023}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {ArXiv preprint},
 title = {Llama: Open and efficient foundation language models},
 url = {https://arxiv.org/abs/2302.13971},
 volume = {abs/2302.13971},
 year = {2023}
}

@inproceedings{li2023self,
 author = {Xian Li and
Ping Yu and
Chunting Zhou and
Timo Schick and
Omer Levy and
Luke Zettlemoyer and
Jason Weston and
Mike Lewis},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/LiYZSLZWL24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 29 Jul 2024 01:00:00 +0200},
 title = {Self-Alignment with Instruction Backtranslation},
 url = {https://openreview.net/forum?id=1oijHJBRsT},
 year = {2024}
}

@article{team2024gemini,
 author = {Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
 journal = {ArXiv preprint},
 title = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
 url = {https://arxiv.org/abs/2403.05530},
 volume = {abs/2403.05530},
 year = {2024}
}

@misc{eval-harness,
 author = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
 doi = {10.5281/zenodo.12608602},
 publisher = {Zenodo},
 title = {A framework for few-shot language model evaluation},
 url = {https://zenodo.org/records/12608602},
 version = {v0.4.3},
 year = {2024}
}

@article{ge2024scaling,
 author = {Ge, Tao and Chan, Xin and Wang, Xiaoyang and Yu, Dian and Mi, Haitao and Yu, Dong},
 journal = {ArXiv preprint},
 title = {Scaling synthetic data creation with 1,000,000,000 personas},
 url = {https://arxiv.org/abs/2406.20094},
 volume = {abs/2406.20094},
 year = {2024}
}

@dataset{BAAI_Infinity-Instruct,
 author = {BAAI},
 date = {2024},
 title = {Infinity-Instruct},
 url = {https://huggingface.co/datasets/BAAI/Infinity-Instruct}
}

@article{ivison2023camels,
 author = {Ivison, Hamish and Wang, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A and Beltagy, Iz and others},
 journal = {ArXiv preprint},
 title = {Camels in a changing climate: Enhancing lm adaptation with tulu 2},
 url = {https://arxiv.org/abs/2311.10702},
 volume = {abs/2311.10702},
 year = {2023}
}

@inproceedings{suzgun2022challenging,
 address = {Toronto, Canada},
 author = {Suzgun, Mirac  and
Scales, Nathan  and
Sch{\"a}rli, Nathanael  and
Gehrmann, Sebastian  and
Tay, Yi  and
Chung, Hyung Won  and
Chowdhery, Aakanksha  and
Le, Quoc  and
Chi, Ed  and
Zhou, Denny  and
Wei, Jason},
 booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
 doi = {10.18653/v1/2023.findings-acl.824},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 pages = {13003--13051},
 publisher = {Association for Computational Linguistics},
 title = {Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
 url = {https://aclanthology.org/2023.findings-acl.824},
 year = {2023}
}

@misc{rein2023gpqa,
 archiveprefix = {arXiv},
 author = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
 eprint = {2311.12022},
 primaryclass = {cs.AI},
 title = {GPQA: A Graduate-Level Google-Proof Q\&A Benchmark},
 year = {2023}
}

@article{zhou2023instructionfollowing,
 author = {Jeffrey Zhou and Tianjian Lu and Swaroop Mishra and Siddhartha Brahma and Sujoy Basu and Yi Luan and Denny Zhou and Le Hou},
 journal = {ArXiv preprint},
 title = {Instruction-Following Evaluation for Large Language Models},
 url = {https://arxiv.org/abs/2311.07911},
 volume = {abs/2311.07911},
 year = {2023}
}

@misc{wang2024mmluprorobustchallengingmultitask,
 author = {Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen},
 journal = {ArXiv preprint},
 title = {MMLU-Pro: A More Robust and Challenging Multi-Task Language
Understanding Benchmark},
 url = {https://arxiv.org/abs/2406.01574},
 volume = {abs/2406.01574},
 year = {2024}
}

@inproceedings{sprague2024musrtestinglimitschainofthought,
 author = {Zayne Sprague and
Xi Ye and
Kaj Bostrom and
Swarat Chaudhuri and
Greg Durrett},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/SpragueYBCD24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 29 Jul 2024 01:00:00 +0200},
 title = {MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning},
 url = {https://openreview.net/forum?id=jenyYQzue1},
 year = {2024}
}

@article{lambert2024t,
 author = {Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
 journal = {ArXiv preprint},
 title = {T$\backslash$" ULU 3: Pushing Frontiers in Open Language Model Post-Training},
 url = {https://arxiv.org/abs/2411.15124},
 volume = {abs/2411.15124},
 year = {2024}
}
