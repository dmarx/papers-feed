\section{Proof of Theorem \ref{thm:vp}}
%
\subsection{Preliminary Lemmas}
%
\begin{lemma}[Propagation of Uncertainty under DFT/DCT] \label{lem:A1}
    %
    Let $X = Wx$ with $x\in\mathbb R^N$ and $W\in\mathbb C^{N\times N}$. Then
    %
    \[
        \Sigma_X = W \Sigma_x W^*
    \]
    %
\end{lemma}
%
\proof
    %
    \[
        \begin{aligned}
            \Sigma_X &= \mathbb E\left[ (Wx - \mathbb{E}[Wx])\wedge (Wx - \mathbb{E}[Wx])\right]\\
                     &= \mathbb E\left[ W(x - \mathbb{E}[x])\wedge W(x - \mathbb{E}[x])\right]\\
                     &= \mathbb E\left[ W(x - \mathbb{E}[x])(x - \mathbb{E}[x])^\top W^*\right]\\
                     &= W\mathbb E\left[ W(x - \mathbb{E}[x])(x - \mathbb{E}[x])^\top \right]W^*\\
                     &= W \Sigma_x W^*
        \end{aligned}
    \]
%
\endproof
%
\begin{lemma}[Propagation of Total Variance under DFT/DCT] \label{lem:A2} Let $X = Wx$ with $x\in\mathbb R^N$ and $W\in\mathbb C^{N\times N}$. Then
$$
    \mathbb V[X] = \mathbb{V}[x]
$$
\end{lemma}
%
\proof
    Recalling that the total variance of a random variable is equal to the trace of its covariance matrix, i.e. 
    %
    \[
        \mathbb{V}[x] = \text{tr}(\Sigma_x),\quad\mathbb{V}[X] = \text{tr}(\Sigma_X)
    \]
    %
    then
    %
    \[
        \begin{aligned}
                            \text{tr}(\Sigma_x) =  \text{tr}(\Sigma_X) \Leftrightarrow  \mathbb V[X] = \mathbb{V}[x]
\        \end{aligned}
    \]
    
    Recalling Lemma \ref{lem:A1} yields
    %
    \[
        \begin{aligned}
                                  & \mathbb V[X] = \mathbb{V}[x]\\
            \Leftrightarrow \quad & \text{tr}(\Sigma_x) =  \text{tr}(W\Sigma_x W^*)\\
            \Leftrightarrow \quad & \text{tr}(\Sigma_x) -  \text{tr}(W\Sigma_x W^*) = 0\\
            \Leftrightarrow \quad & \text{tr}(\Sigma_x) -  \text{tr}(\Sigma_x W^*W) = 0\\
            %\Leftrightarrow \quad & \sum_{n}[\Sigma_x]_{nn} -  \sum_{n,k,j}[W]_{nk}[\Sigma_x]_{kj}[W^*]_{jn} = 0\\
            %\Leftrightarrow \quad & \sum_{n}[\Sigma_x]_{nn} -  \sum_{n}[\Sigma_x]_{nn}[W]_{nk}[\Sigma_x]_{kj}[W^*]_{jn} = 0
        \end{aligned}
    \]
    %
    Since the DCT/DFT matrix is orthonormal, i.e. $W^* = W^{-1}$ we have that 
    %
    \[
        \text{tr}(\Sigma_x W^*W) = \text{tr}(\Sigma_x),
    \]
    %
    proving the result.
\endproof
%
\begin{lemma}[Gaussian initialization in rank--deficient linear layers] \label{lem:A3} Let $\hat X = S^\top_m A S_m X$ with $X\in\R^N$, $A\in\bC^{m\x m}$ and $S_m\in\bC^{m\times N}$,

\input{drawings/selection_matrix}


If $\bE[X_k] = 0$, $\bV[X_k] = \sigma^2$ for all $k$ the following hold:
%and $p_{\Re(A_{ij})} = p_{\Im(A_{ij})} = \cN(0, \sigma_A)$ for all entries of $A$, then 
%
\begin{itemize}
    \item[$i.$] for $k\geq m$
    \[
        \bE[\hat X_k] = 0,\quad
        \bV[\hat X_k] = 0
    \]
    \item[$ii.$] for $k<m$ and $\Re(A_{ij}),\Im(A_{ij})\sim \cN(0, \sigma_A^2)$
    \[
        \bE[\hat X_k] = 0,\quad
        \bV[\hat X_k] = 2 m \sigma^2 \sigma_A^2
    \]
    \item[$iii.$] for $k<m$ and $\Re(A_{ij})\sim \cN(0, \sigma_A^2)$, $\Im(A_{ij})=0$
    \[
        \bE[\hat X_k] = 0,\quad
        \bV[\hat X_k] = m \sigma^2 \sigma_A^2
    \]
\end{itemize}
%
\end{lemma}
%
\proof
    Let $M = S_m^\top A S_m$. It holds,
    %
    \[
        M = 
            \begin{bmatrix}
                A & \x\\
                \x & \x
            \end{bmatrix}\in\bC^{N\x N}
    \]
    %
    where``$\x$'' are blocks of complex zeros. By expanding component--wise the layer computation, i.e.
    %
    \[
        \hat X_k = \sum_{j=0}^{N-1} M_{kj}X_j,
    \]
    %
    it holds that for $k<m$
    %
    \[
        \hat X_k = \sum_{j=0}^{m-1} A_{kj}X_j,
    \]
    %
    while $\hat X_k = 0$ for $k\geq m$. Hence $i.$ follows naturally from the latter and we focus on proving $ii.$ and $iii.$
    %
    \begin{itemize}
        \item[Case $ii.$] The probability distribution of $\hat X_k$ is a sum of product distributions involving independent random variables $A_{kj}$ and $X_j$. The first central moment is readily obtained
        %
        \[
            \mathbb{E}[\hat{X}_k] = \sum_{t=0}^{m-1} \mathbb{E}[A_{kj}] \mathbb{E}[X_j] = 0
        \]
        %
    since both $\mathbb{E}[X_k] = 0$ and $\forall~ k,j<m: ~ \mathbb{E}[A_{kj}] = 0$. $\bV[\hat X_k]$ can be then obtained by computing the variance of the product of two random variables, i.e.
    %
    \begin{equation*}
        %
        \begin{aligned}
            \mathbb{V}[\hat X_k] &= \sum_{j=0}^{m-1} \Big(\mathbb{V}[A_{kj}] + \cancel{\mathbb{E}[A_{kj}]}^2) (\mathbb{V}[{X_{j}}] + \cancel{\mathbb{E}[{X_{j}}]}^2) - \cancel{\mathbb{E}[A_{kj}]^2\mathbb{E}[{X_{j}}]^2}\Big) \\
            &= \sum_{j=0}^{m-1} \mathbb{V}[A_{kj}] \mathbb{V}[{X_{j}}] \\
            &= \sum_{j=0}^{m-1} \sigma^2 \mathbb{V}[A_{kj}] \\
            &= \sigma^2\sum_{j=0}^{m-1}\left(\mathbb{V}[\Re(A_{kj})] + \mathbb{V}[\Im(A_{kj})]\right)\\
            &= \sigma^2\sum_{j=0}^{m-1}2\sigma^2_A =  2 m \sigma^2 \sigma_{A}^2
        \end{aligned}
        %
    \end{equation*}
    %
    \item[Case $iii.$] Similarly to the previous case we get 
    %
    \[
        \begin{aligned}
            \mathbb{V}[\hat X_k] &= \sigma^2\sum_{j=0}^{m-1}\left(\mathbb{V}[\Re(A_{kj})] + \cancel{\mathbb{V}[\Im(A_{kj})}]\right)\\
            &= \sigma^2\sum_{j=0}^{m-1}\sigma^2_A=  m \sigma^2 \sigma_{A}^2
        \end{aligned}
    \]
    %
    \end{itemize}
    %
\endproof
%
\subsection{Proof of Main Result}

%
\proof
    According to Lemma \ref{lem:A2}, the total variance is preserved under the normalized DCT. Therefore, with $X = W\hat x$ and $\hat X = Wx$ we have
    %
    \[
        \bV[X] = \bV[x], \quad \bV[\hat X] = \bV[\hat x].
    \]
    %
    Using $\hat X = S_m^\top A S_m X$, we can find the condition under which the variance is preserved by the map $x\mapsto \hat x$:
    %
    \[
        \begin{aligned}
                             & \mathbb{V}[\hat x] =  \mathbb{V}[x]\\
        \Leftrightarrow\quad & \sum_{n=0}^{N-1} \mathbb{V}[\hat x_n] = \sum_{n=0}^{N-1} \mathbb{V}[x_n]\\
        \Leftrightarrow\quad & \sum_{k=0}^{N-1} \mathbb{V}[\hat X_k] = \sum_{k=0}^{N-1} \mathbb{V}[X_k]\\
        \Leftrightarrow\quad & \sum_{k=0}^{m-1} m\sigma^2\sigma^2_A = \sum_{k=0}^{N-1} \sigma^2&&\quad \text{Lemma \ref{lem:A3}}\\
        \Leftrightarrow\quad & m^2\sigma^2\sigma^2_A =  N \sigma^2 \\
        \Leftrightarrow\quad & \sigma^2_A =  \frac{N}{m^2}
        \end{aligned}
    \]
    %
    Hence, initializing $A$ by sampling its entries from a normal distribution with zero mean and variance $N/m^2$ is sufficient for preserving the variance under the reduced-order FDM layer, i.e.
    %
    \[
        A_{ij} \sim \cN\left(0, \frac{N}{m^2}\right)~~\Rightarrow~~\bV[\hat x] = \bV[x],
    \]
    %
    proving the result.
\endproof

\vpdft*
%
\proof
    The proof follows directly from the one of Theorem \ref{thm:vp} using the fact that since the DFT's $k$-space is complex ($\cD_k\equiv\bC^N$) as $W\in\bC^{N\times N}$, the weights are typically chosen complex, i.e. $A\in\bC^{m\times m}$. Therefore, in this case $\bV[\hat X] = 2m\sigma^2\sigma_A^2$ according to Lemma \ref{lem:A3}.
\endproof


\begin{restatable}[({\tt vp}) initialization with diagonal layers]{corollary}{vpinit_diag}\label{cor:vp_diag}
%
Under the assumptions of Theorem \ref{thm:vp}, if $A$ is diagonal s.t $\forall i \neq j: A_{ij} = 0$, we have
%
$
    A_{ii} \sim \cN\left(0, \frac{N}{m}\right)~~\Rightarrow~~\bV[\hat x] = \bV[x].
$
    
\end{restatable}
%
\proof
The proof follows directly from Lemma \ref{lem:A3}
%
\begin{equation*}
    %
    \begin{aligned}
        \mathbb{V}[\hat X_k] &= 
        \sum_{j=0}^{m-1} \mathbb{V}[A_{kj}] \mathbb{V}[{X_{j}}] \\
        &= \mathbb{V}[A_{kk}] \mathbb{V}[{X_{k}}] \\
        &= \sigma^2 \left(\mathbb{V}[\Re(A_{kk})] + \cancel{\mathbb{V}[\Im(A_{kk})]}\right)\\
        &= \sigma^2\sigma^2_A 
    \end{aligned}
    %
\end{equation*}
   % 
leading to the condition 
    \[
        \begin{aligned}
         & \mathbb{V}[\hat x] =  \mathbb{V}[x]\\
        \Leftrightarrow\quad & \sum_{k=0}^{m-1} \sigma^2\sigma^2_A = \sum_{k=0}^{N-1} \sigma^2\\
        \Leftrightarrow\quad & m\sigma^2\sigma^2_A =  N \sigma^2 \\
        \Leftrightarrow\quad & \sigma^2_A =  \frac{N}{m}
        \end{aligned}
    \]
\endproof

The layer structure treated by \ref{cor:vp_diag} is common among many FDMs, e.g. FNOs in \citep{li2020fourier}.