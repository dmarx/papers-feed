\vspace{-5mm}
\begin{abstract}
    % Spectral analysis provides one of the most effective paradigms for information--preserving dimensionality reduction in data: often, a simple description of naturally occurring \textit{signals} can be obtained via few terms of periodic basis functions. Neural operators designed for frequency domain learning -- \textit{frequency domain models} (FDMs) -- are based on complex--valued transforms i.e. \textit{Fourier Transforms} (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: \textit{transform once} (\ourmethod{}). To enable efficient, direct learning in the frequency domain we develop a variance preserving weight initialization scheme and investigate various choices of transforms. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of $3$ x to $10$ x that increase with data resolution and model size. We perform extensive experiments on learning to solve partial differential equations, including incompressible Navier--Stokes, turbulent flows around airfoils and high-resolution video of smoke dynamics. \ourmethod{} models improve on the test performance of SOTA FDMs while requiring significantly less computation, with over $20\%$ reduction in predictive error across tasks.


    Spectral analysis provides one of the most effective paradigms for information-preserving dimensionality reduction, as simple descriptions of naturally occurring \textit{signals} are often obtained via few terms of periodic basis functions. In this work, we study deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time: frequency-domain models (FDMs). Existing FDMs are based on complex-valued transforms i.e. \textit{Fourier Transforms} (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: \textit{transform once} (\ourmethod{}). To enable efficient, direct learning in the frequency domain we derive a variance preserving weight initialization scheme and investigate methods for frequency selection in reduced-order FDMs. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of $3$ x to $10$ x that increase with data resolution and model size. We perform extensive experiments on learning the solution operator of spatio-temporal dynamics, including incompressible Navier-Stokes, turbulent flows around airfoils and high-resolution video of smoke. \ourmethod{} models improve on the test performance of FDMs while requiring significantly less computation ($5$ hours instead of $32$ for our large-scale experiment), with over $20\%$ reduction in predictive error across tasks.
    
    % pruning schemes in frequency domain. 
    
    % investigate pruning schemes in frequency domain.
    
    % % various choices of transforms. 
    

    
    % Neural operators designed for frequency domain learning
    
    % : often, a simple description of naturally occurring \textit{signals} can be obtained via few terms of periodic basis functions. 
    
    
    % Neural operators designed for frequency domain learning -- \textit{frequency domain models} (FDMs) -- are based on complex--valued transforms i.e. \textit{Fourier Transforms} (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: \textit{transform once} (\ourmethod{}). To enable efficient, direct learning in the frequency domain we develop a variance preserving weight initialization scheme and investigate various choices of transforms. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of $3$ x to $10$ x that increase with data resolution and model size. We perform extensive experiments on learning to solve partial differential equations, including incompressible Navier--Stokes, turbulent flows around airfoils and high-resolution video of smoke dynamics. \ourmethod{} models improve on the test performance of SOTA FDMs while requiring significantly less computation, with over $20\%$ reduction in predictive error across tasks.
    
    % we address open challenges such as choice of basis and parameter initialization schemes.
    
    % Reducing the number of redundant transforms allows T1 models to achieve speedups of $3$ x to $30$ x which scale with data resolution and model size, while simultaneously streamlining the design process of neural operator architectures. 
    
    % learning in this smaller space unlocks the possibility to design far more compact and efficient networks which can be manipulated with far smaller neural networks than would otherwise be necessary.
    % 
    % % State--of--the--art deep learning models designed for frequency domain learning -- \textit{neural operators} -- are based on complex--valued transforms i.e. Fourier Transforms (FT), and layers which perform computation on the spectrum as well as input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. 

    % %
    % This work introduces an alternative blueprint for frequency domain learning through a \textit{single} transform: \textit{transform once} (\ourmethod). By removing redundant transforms, \ourmethod{} achieves speedups of $3$x to $20$x which increase with data resolution and model size, while simultaneously streamlining the design process of neural operator architectures. To enable efficient, direct learning in the frequency domain we address open challenges such as choosing the right basis and parameter initialization schemes. We perform extensive experiments on learning to solve partial differential equations, including incompressible Navier--Stokes and turbulent flows around airfoils. \ourmethod{} models match or improve on the task performance of neural operators with significantly less computation. 
    
    
    % In prediction of flow around airfoils, \ourmethod{} reduce prediction errors by over $20 \%$ over comparable SOTA. {\color{red} pending scalarflow Finally, we show how the efficient \ourmethod recipe allows ... learning in previously unattainable... high-resolution ...}.
    
    % The efficiencyhigh-reso
    
    % Moreover, the complex--valued nature of the chosen transforms requires layers of a neural operator to also be complex--valued, leading to an unnecessarily large number of parameters.

    
    % first, we base  cosine transforms into
    
    % real--valued transform... cosine .. more stable. We further
    
    % a principled variance--preserving initialization scheme, and 
    
    % The \ourmethod recipe
    
    % Our efforts lead to... lmao 
    

    
    % we .. . To enable efficient learning,....
    
    % frequency domain i.e. with a \textit{single} forward transform (\textit{transform once}, \ourmethod{}). We address 
    
    % optimization instabilities and an explosion in the number of parameters... dio
    
    % % da mettere sotto
    % We empirically observe \ourmethod{} to address instabilities ...
    

    
    
    % one stream performs (1) forward transform from data to spectral (2) selection of frequencies (2) learned layer (3) inverse transform, whereas the second residual stream performs a learned transformation directly. This introduces a considerable overhead. 
    
%      In this work, we propose an alternative recipe for learning neural operators by constructing hierarchical representations directly in the target basis of choice  
  
%   We show how the optimization instabilities observed by following the \ourmethod{} recipe after \textit{Discrete Fourier Transforms} (DFTs) can be avoided by appropriately introducing \textit{Discrete Cosine Transforms} (DCTs). 
    
%     Further, we derive a principled variance--preserving initialization scheme for \ourmethod{} models and other neural operators that avoids zero--collapse of the outputs. 
    
    % We perform extensive experiments on learning to solve partial differential equations, including incompressible Navier--Stokes and turbulent flows around airfoils. Our \ourmethod{} models match or improve on the task performance of other neural operators with significantly less computation (between $300\%$ to $600\%$ faster on 2D domains). In flow around airfoil tasks, \ourmethod{} reduce steady--state prediction errors by over $20 \%$ over comparable state--of--the--art.
    
    
    % repeating computation of the form (1) forward Fourier transform and frequency pruning (2) learned layer (3) inverse transform, optionally followed by a nonlinear activation function. 
    

    % Spectrum analysis is an established paradigm to information--preserving dimensionality reduction in a wide array of data.... A class of deep learning models devoted to exploiting... is called neural operators. 
    

    % One of the most effective paradigms to reduce dimensionality in data while preserving information is .. to ... spectrum. The class of neural operators focuses on ...
    
    % Packing information about data in less dimensions   
    
    % Manifesting \textbf{sparsity} patterns in data by choosing appropriate bases 


    


    % A {\tt T1} layer operates on spectral transform coefficients. Rather than stacking forward and inverse Discrete Fourier Transforms (DFTs), we show how a \textit{single} spectral transform, followed by a deep neural network, is sufficient to match task performance of other neural operators with significantly less computation  ($10$x and $40$x speedups...). A principled initialization scheme is derived
    
    
    % In particular, when  Neural operators such as Fourier Neural Operators
    
    % Spectral neural operators exploit sparsity of input signals in their frequency spectra by constructing hierarchical representations in the following way: (1) application of a transform, followed by a learned map and the inverse transform (2) some kind of nonlinearity.
    
    % been proposed.
    
    % To this end, Neural operators learn perform stack forward -- linear -- inverse 
    
    % In this work, we propose a novel class of neural operators, {\tt T1}, designed to exploit sparsity of input signals in their frequency spectra. 
    
    % We show how a \textbf{single} spectral transform, followed by allows ..
    
    % Similarly to other contemporary neural operators which stack learned representations transformations
    
    % such as Fourier Neural Operators (FNOs), 
    

    
    % Modeling data as signals has been a key step in the derivation and implementation of efficient neural network models for a range of domains, including vision and differential equations. In this work,  operators . We propose a novel weight initialization scheme and choice of transform. We show how through a ..., it is possible to train, with savings of ~$10 - 50x$. 
    
    % We present {\tt T1}
    
    % Finally,
    
    % (1) We develop a variance preserving initialization scheme for truncated neural operators and propose. Moreover, ...
    
    
    % (2)
    

    % Most applications of deep learning methods in "learning to solve"
    
    % {\color{blue!30!white} KEY POINTS:
    % (1) The rate of adoption of deep learning in various engineering and science disciplines is increasing. 
    % (2) Virtually all instances optimize models from random initialization (\textit{from scratch}) and target specific systems. However, several amenable phenomena in deep learning models only manifest at a large--enough scale. 
    % (3) As the computational costs of training large--scale deep learning models from scratch increase to levels unattainable to most, finding new techniques to utilize large models pretrained or developed for common modalities becomes a necessity.
    % (4) Large pretrained models are only available for some data modalities.}
    
    % We empirically verify that large vision models can be used as backbone feature extractors in several downstream tasks involving differential equation solving and computational fluid dynamics.
    
    % In this work, we advocate for
    % In this work, we develop and benchmark techniques to transfer 
    % * As the costs of training large--scale deep learning models from scratch reaches levels  
    % is an important 
    % Deep learning models 
    % Deep learning 
    % Scientific deep learning, an important application area ..., firmly belongs to (2) as most deep models are trained from scratch
    % Deep learning is undergoing a paradigm shift
    % As the costs of training large--scale deep learning models from scratch reaches levels 
\end{abstract}