\section{Introduction}
\begin{center}
    \small \textit{Nature uses only the longest threads to weave her patterns, so that each small piece of her fabric reveals the organization of the entire tapestry. \citep{feynman1965character}}
\end{center}

Naturally occurring \textit{signals} are often sparse when projected on periodic basis functions \citep{strang1999discrete}.
Central to recently-introduced instances of frequency-domain neural operators \citep{li2020fourier,tran2021factorized}, which we refer to as \textit{frequency-domain models} (FDMs), is the idea of learning to modify specific frequency components of inputs to obtain a desired output in data space. With a hierarchical structure that blends learned transformations on frequency domain coefficients with regular convolutions, FDMs are able to effectively approximate global, long-range dependencies in higher resolution signals without requiring prohibitively deep architectures. 

\newpage

Yet, existing FDMs suffer from several drawbacks:

\begin{itemize}[leftmargin=1cm]
    \item[1.] \textbf{Slow inference:} every layer of an FDM performs a forward and inverse frequency domain transform, introducing a considerable computational overhead. 
    \item[2.] \textbf{Expensive parameter scaling:} each layer of typical FDMs \citep{li2020fourier} performs a long convolution over the inputs by parametrizing it in frequency domain, which scales poorly in the signal resolution.
    \item[3.] \textbf{Incompatibility:} parameter initialization schemes and layers devised to learn directly in data space can be highly suboptimal when introduced without modifications to FDMs.        
\end{itemize}

Despite attempts to improve performance \citep{gupta2021multiwavelet,tran2021factorized}, scaling FDMs to larger data resolutions and model sizes remains fundamentally challenging\footnote{Existing methods to overcome this limitation avoid the frequency domain of inputs, instead introducing an intermediate patch embedding step \citep{guibas2021adaptive,pathak2022fourcastnet}.}.

In this work, we start by posing the question: 

{\centering
\textit{To reap the benefits of learning on frequency domain representations, is it necessary to construct hierarchical deep models that perform forward and inverse frequency transforms at each layer?}
}

We provide the answer in \textit{Transform Once} (\ourmethod{}), a model that builds representations directly in frequency domain, after a \textit{single} forward transform. Each aspect of \ourmethod{} addresses a specific limitation of existing FDMs:
\begin{itemize}[leftmargin=1cm]
    \item[1.] \textbf{Fast:} by performing a single forward transform and optimizing directly on frequency domain coefficients of target data, \ourmethod{} iterations are \textit{at least} $3$x to $10$x faster. When scaling to larger models and higher resolutions, the relative speedups increase as the overhead of each transform grows.
    \item[2.] \textbf{Favourable scaling:} \ourmethod{} employs a single real-valued transform, which we observe to stabilize training and finetuning of deep networks in frequency domain.
    \item[3.] \textbf{Enhanced compatibility:} removing redundant transforms streamlines the design space for \ourmethod{} architectures compared to existing FDMs, allowing direct introduction of optimized layers developed for other applications e.g. UNets \citep{ronneberger2015u}.
    % the unique structure of a neural operator layer requires specialized tricks, with far less opportunities for transfer of current best practices e.g. weight initialization.
\end{itemize}


In \cref{subsec:history} we provide a (short) history on frequency domain approaches in deep learning, followed by background on FDMs \cref{subsec:background}. In \cref{sec:t1rec}, we describe how to train \ourmethod{} directly in frequency domain and motivate the choice of DCT, in \cref{subsec:inverse} we discuss how to choose modes of reduced-order FDMs and in \cref{subsec:init} we introduce a simple variance-preserving weight initialization scheme for all FDMs. Finally, in \cref{sec:experiments} we evaluate \ourmethod{} on a suite of benchmarks related to learning solution operators for a variety of dynamics: incompressible Navier-Stokes, flow around different airfoil geometries, and high-resolution videos of turbulent smoke \citep{eckert2019scalarflow}.

Across tasks, \ourmethod{} is $3\x$ to $10\x$ faster and reduces predictive errors by $20\%$ on average. Training \ourmethod{} models on high resolution videos ($600$ x $1062$) of turbulent dynamics is significantly faster, requiring $5$ hours instead of $32$ hours (FNOs) for the same number of iterations.

