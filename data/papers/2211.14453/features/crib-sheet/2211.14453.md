- **Title**: Efficient Operator Learning in Frequency Domain
- **Key Concept**: Frequency-domain models (FDMs) leverage the structure in the frequency domain for efficient learning of long-range correlations in signals.
- **Transform Once (T1)**: Introduces a single transform approach to reduce computational overhead associated with traditional FDMs that require both forward and inverse transforms.
- **Weight Initialization**: Variance preserving weight initialization scheme is derived to enhance learning efficiency in frequency domain.
- **Speedup**: T1 achieves speedups of 3x to 10x compared to traditional FDMs, increasing with data resolution and model size.
- **Predictive Accuracy**: T1 models show over 20% reduction in predictive error across various tasks while requiring significantly less computation time (5 hours vs. 32 hours for large-scale experiments).
- **Learning Objective**: The learning objective for T1 is defined as minimizing the approximation error \( J_\theta \) of the output signal spectrum \( Y \) in the k-space:
  \[
  J_\theta = \min_\theta E_{x,y} \| T(y) - \hat{Y} \|^2
  \]
- **Parseval-Plancherel Identity**: Establishes the equivalence of minimizing errors in n-space and k-space under certain conditions, allowing for direct learning in the frequency domain.
- **Transform Selection**: The normalized Discrete Cosine Transform (DCT-II) is chosen for T1 due to its effective representation of smooth signals and superior energy compaction properties compared to DFT.
- **Reduced-Order T1 Model**: Operates on a reduced k-space \( D_m \) by selecting \( m < N \) elements, allowing for smaller neural networks and efficient learning.
- **Mode Selection**: Optimal mode selection for reduced-order T1 is based on retaining the top \( m \) modes with the highest magnitude to minimize irreducible loss.
- **Irreducible Loss Bound**: The overall loss \( L_\theta \) includes both the training objective \( J_\theta \) and the irreducible residual loss \( R_o \) due to truncation:
  \[
  L_\theta = J_\theta + R_o
  \]
- **General Prediction Tasks**: For tasks where \( Y \neq X \), a priori analysis on \( R_o \) can inform the choice of modes to retain for optimal performance.

```mermaid
flowchart TD
    A[Input Signal x] -->|Transform T| B[Frequency Domain X]
    B -->|Learned Map fθ| C[Transformed Output Y]
    C -->|Inverse Transform T⁻¹| D[Output Signal ŷ]
    D -->|Residual g| E[Final Output]
```