\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech representations.
\newblock {\em Advances in neural information processing systems}, 33:12449--12460, 2020.

\bibitem{bao2024vidu}
Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu.
\newblock Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models.
\newblock {\em arXiv preprint arXiv:2405.04233}, 2024.

\bibitem{blanz2003face}
Volker Blanz and Thomas Vetter.
\newblock Face recognition based on fitting a 3d morphable model.
\newblock {\em IEEE Transactions on pattern analysis and machine intelligence}, 25(9):1063--1074, 2003.

\bibitem{Bredin23}
Hervé Bredin.
\newblock {pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}.
\newblock In {\em Proc. INTERSPEECH 2023}, 2023.

\bibitem{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\'e J\'egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In {\em Proceedings of the International Conference on Computer Vision (ICCV)}, 2021.

\bibitem{Chung16a}
J.~S. Chung and A. Zisserman.
\newblock Out of time: automated lip sync in the wild.
\newblock In {\em Workshop on Multi-view Lip-reading, ACCV}, 2016.

\bibitem{corona2024vlogger}
Enric Corona, Andrei Zanfir, Eduard~Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, and Cristian Sminchisescu.
\newblock Vlogger: Multimodal diffusion for embodied avatar synthesis.
\newblock {\em arXiv preprint arXiv:2403.08764}, 2024.

\bibitem{cui2024hallo2}
Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, and Jingdong Wang.
\newblock Hallo2: Long-duration and high-resolution audio-driven portrait image animation.
\newblock {\em arXiv preprint arXiv:2410.07718}, 2024.

\bibitem{insightface2024}
DeepInsight.
\newblock Insightface: An open-source 2d and 3d deep face analysis toolkit.
\newblock \url{https://github.com/deepinsight/insightface}, 2024.

\bibitem{gao2023high}
Yue Gao, Yuan Zhou, Jinglu Wang, Xiao Li, Xiang Ming, and Yan Lu.
\newblock High-fidelity and freely controllable talking head video generation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 5609--5619, 2023.

\bibitem{guo2024liveportrait}
Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang.
\newblock Liveportrait: Efficient portrait animation with stitching and retargeting control.
\newblock {\em arXiv preprint arXiv:2407.03168}, 2024.

\bibitem{guo2023animatediff}
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai.
\newblock Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.
\newblock {\em arXiv preprint arXiv:2307.04725}, 2023.

\bibitem{huang2023vbench}
Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu.
\newblock {VBench}: Comprehensive benchmark suite for video generative models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024.

\bibitem{jiang2024loopy}
Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng.
\newblock Loopy: Taming audio-driven portrait avatar with long-term motion dependency.
\newblock {\em arXiv preprint arXiv:2409.02634}, 2024.

\bibitem{karaev24cotracker3}
Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht.
\newblock Cotracker3: Simpler and better point tracking by pseudo-labelling real videos.
\newblock In {\em Proc. {arXiv:2410.11831}}, 2024.

\bibitem{karaev23cotracker}
Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht.
\newblock Cotracker: It is better to track together.
\newblock In {\em Proc. {ECCV}}, 2024.

\bibitem{liu2024anitalker}
Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, and Kai Yu.
\newblock Anitalker: Animate vivid and diverse talking faces through identity-decoupled facial motion encoding.
\newblock {\em arXiv preprint arXiv:2405.03121}, 2024.

\bibitem{liu2024sora}
Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et~al.
\newblock Sora: A review on background, technology, limitations, and opportunities of large vision models.
\newblock {\em arXiv preprint arXiv:2402.17177}, 2024.

\bibitem{ma2023dreamtalk}
Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, and Zhidong Deng.
\newblock Dreamtalk: When expressive talking head generation meets diffusion probabilistic models.
\newblock {\em arXiv preprint arXiv:2312.09767}, 2023.

\bibitem{Peebles2022DiT}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 4195--4205, 2023.

\bibitem{Plaquet23}
Alexis Plaquet and Hervé Bredin.
\newblock {Powerset multi-class cross entropy loss for neural speaker diarization}.
\newblock In {\em Proc. INTERSPEECH 2023}, 2023.

\bibitem{polyak2024movie}
Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et~al.
\newblock Movie gen: A cast of media foundation models.
\newblock {\em arXiv preprint arXiv:2410.13720}, 2024.

\bibitem{prajwal2020lip}
KR Prajwal, Rudrabha Mukhopadhyay, Vinay~P Namboodiri, and CV Jawahar.
\newblock A lip sync expert is all you need for speech to lip generation in the wild.
\newblock In {\em Proceedings of the 28th ACM international conference on multimedia (ACM MM)}, pages 484--492, 2020.

\bibitem{raffel2023t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.

\bibitem{ren2023pbidr}
Xingyu Ren, Alexandros Lattas, Baris Gecer, Jiankang Deng, Chao Ma, and Xiaokang Yang.
\newblock Facial geometric detail recovery via implicit representation.
\newblock In {\em 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)}, 2023.

\bibitem{ren2021pirenderer}
Yurui Ren, Ge Li, Yuanqi Chen, Thomas~H Li, and Shan Liu.
\newblock Pirenderer: Controllable portrait image generation via semantic neural rendering.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 13759--13768, 2021.

\bibitem{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em arXiv preprint arXiv:2202.00512}, 2022.

\bibitem{schneider2019wav2vec}
Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli.
\newblock wav2vec: Unsupervised pre-training for speech recognition.
\newblock {\em arXiv preprint arXiv:1904.05862}, 2019.

\bibitem{Seitzer2020FID}
Maximilian Seitzer.
\newblock {pytorch-fid: FID Score for PyTorch}.
\newblock \url{https://github.com/mseitzer/pytorch-fid}, August 2020.
\newblock Version 0.3.0.

\bibitem{shen2023difftalk}
Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, and Jiwen Lu.
\newblock Difftalk: Crafting diffusion models for generalized audio-driven portraits animation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1982--1991, 2023.

\bibitem{siarohin2019first}
Aliaksandr Siarohin, St{\'e}phane Lathuili{\`e}re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe.
\newblock First order motion model for image animation.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)}, 32, 2019.

\bibitem{singer2022make}
Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock {\em arXiv preprint arXiv:2209.14792}, 2022.

\bibitem{sun2023vividtalk}
Xusen Sun, Longhao Zhang, Hao Zhu, Peng Zhang, Bang Zhang, Xinya Ji, Kangneng Zhou, Daiheng Gao, Liefeng Bo, and Xun Cao.
\newblock Vividtalk: One-shot audio-driven talking head generation based on 3d hybrid prior.
\newblock {\em arXiv preprint arXiv:2312.01841}, 2023.

\bibitem{teed2020raft}
Zachary Teed and Jia Deng.
\newblock Raft: Recurrent all-pairs field transforms for optical flow.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16}, pages 402--419. Springer, 2020.

\bibitem{tian2024emo}
Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo.
\newblock Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions.
\newblock {\em arXiv preprint arXiv:2402.17485}, 2024.

\bibitem{unterthiner2018towards}
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly.
\newblock Towards accurate generative models of video: A new metric \& challenges.
\newblock {\em arXiv preprint arXiv:1812.01717}, 2018.

\bibitem{wang2023videocomposer}
Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou.
\newblock Videocomposer: Compositional video synthesis with motion controllability.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)}, 36, 2024.

\bibitem{wei2024aniportrait}
Huawei Wei, Zejun Yang, and Zhisheng Wang.
\newblock Aniportrait: Audio-driven synthesis of photorealistic portrait animation.
\newblock {\em arXiv preprint arXiv:2403.17694}, 2024.

\bibitem{xu2024hallo}
Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu zhu.
\newblock Hallo: Hierarchical audio-driven visual synthesis for portrait image animation, 2024.

\bibitem{xu2024vasa}
Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo.
\newblock Vasa-1: Lifelike audio-driven talking faces generated in real time.
\newblock {\em arXiv preprint arXiv:2404.10667}, 2024.

\bibitem{yang2024cogvideox}
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et~al.
\newblock Cogvideox: Text-to-video diffusion models with an expert transformer.
\newblock {\em arXiv preprint arXiv:2408.06072}, 2024.

\bibitem{yao2024minicpm}
Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et~al.
\newblock Minicpm-v: A gpt-4v level mllm on your phone.
\newblock {\em arXiv preprint arXiv:2408.01800}, 2024.

\bibitem{zakharov2020fast}
Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, and Victor Lempitsky.
\newblock Fast bi-layer neural synthesis of one-shot realistic head avatars.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XII 16}, pages 524--540. Springer, 2020.

\bibitem{zhang2023metaportrait}
Bowen Zhang, Chenyang Qi, Pan Zhang, Bo Zhang, HsiangTao Wu, Dong Chen, Qifeng Chen, Yong Wang, and Fang Wen.
\newblock Metaportrait: Identity-preserving talking head generation with fast personalized adaptation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 22096--22105, 2023.

\bibitem{zhang2022sadtalker}
Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang.
\newblock Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 8652--8661, 2023.

\bibitem{zhang2023sadtalker}
Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang.
\newblock Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 8652--8661, 2023.

\bibitem{zhang2021flow}
Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan.
\newblock Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3661--3670, 2021.

\bibitem{zhang2024tora}
Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, and Weizhi Wang.
\newblock Tora: Trajectory-oriented diffusion transformer for video generation.
\newblock {\em arXiv preprint arXiv:2407.21705}, 2024.

\bibitem{zhou2022magicvideo}
Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng.
\newblock Magicvideo: Efficient video generation with latent diffusion models.
\newblock {\em arXiv preprint arXiv:2211.11018}, 2022.

\bibitem{zhu2022celebvhq}
Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen~Change Loy.
\newblock {CelebV-HQ}: A large-scale video facial attributes dataset.
\newblock In {\em ECCV}, 2022.

\bibitem{champ2024}
Shenhao Zhu, Junming~Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu.
\newblock Champ: Controllable and consistent human image animation with 3d parametric guidance, 2024.

\end{thebibliography}
