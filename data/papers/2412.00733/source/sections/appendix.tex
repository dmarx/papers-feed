\section{Appendix}

% \section{Experiment}
\subsection{Experimental Setups}
% \noindent\textbf{Comparison on dynamic scene scenarios.}

% \noindent\textbf{Implementation.}
% {The model was trained using 64 NVIDIA A100 GPUs. The first and second stages of the model were trained 20,000 steps respectively. During training, the batch size of each GPU was 1, and the learning rate was set to 1e-5. The resolution of the training video is 480 x 720, and it can generate a video with 49 frames at a time. In the training process, the audio embedding is dropped with a probability of 0.05, and the motion frames are randomly masked with a probability of 0.25.}

\noindent\textbf{Evaluation Metrics.}
We employed a range of evaluation metrics for generated videos across benchmark datasets, including HDTF and Celeb-V. 
These metrics comprise Fréchet Inception Distance (FID)~\cite{Seitzer2020FID}, Fréchet Video Distance (FVD)~\cite{unterthiner2018towards}, Synchronization-C (Sync-C)~\cite{Chung16a}, Synchronization-D (Sync-D)~\cite{Chung16a}, and E-FID~\cite{tian2024emo}. 
FID and FVD quantify the similarity between generated images and real data, while Sync-C and Sync-D assess lip synchronization accuracy. E-FID evaluates image quality based on features extracted from the Inception network.

For the wild dataset, we introduced V-bench~\cite{huang2023vbench} metrics to enhance evaluation, focusing on dynamic degree. 
% Subject consistency is measured through DINO feature similarity, ensuring uniformity of a subject's appearance across frames. 
% Background consistency is evaluated via CLIP feature similarity, assessing the temporal stability of backgrounds. 
%Temporal flickering is quantified by mean absolute differences in static frames, while motion smoothness is analyzed using motion priors from a video frame interpolation model. 
Dynamic degree is measured using RAFT~\cite{teed2020raft} to quantify the extent of motion in generated videos, providing a comprehensive assessment of temporal quality.

\noindent\textbf{Datasets} Our dataset comprises HDTF~\cite{zhang2021flow} as well as additional data sourced from the Internet. In order to build a dataset of both high quality and diversity, we have designed a comprehensive data curation pipeline capable of processing a wide variety of Internet videos, including YouTube and movie clips. Specifically, we collected 1200 hours of YouTube videos and 2346 hours of movie footage, from which we curated 128 hours of high-quality video clips. The details of the pipeline and related statistics are presented in Figure~\ref{fig:data_statistics}. For further discussions, please refer to the Appendix.

\noindent\textbf{Baseline Approaches.}
We considered several representative audio-driven talking face generation methods for comparison, all of which have publicly available source code or implementations. These methods include SadTalker~\cite{zhang2022sadtalker}, DreamTalk~\cite{ma2023dreamtalk}, AniPortrait~\cite{wei2024aniportrait}, and Hallo~\cite{xu2024hallo,cui2024hallo2}. 
The selected approaches encompass both GANs and diffusion models, as well as techniques utilizing intermediate facial representations alongside end-to-end frameworks. 
This diversity in methodologies allows for a comprehensive evaluation of the effectiveness of our proposed approach in comparison to existing solutions.

