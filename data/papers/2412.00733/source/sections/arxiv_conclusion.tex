
\section{Conclusion}

This paper introduces advancements in portrait image animation utilizing the enhanced capabilities of a transformer-based diffusion model. By integrating audio conditioning through cross-attention mechanisms, our approach effectively captures the intricate relationship between audio signals and facial expressions, achieving substantial lip synchronization. To preserve facial identity across video sequences, we incorporate an identity reference network. Additionally, we utilize motion frames to enable the model to generate long-duration video extrapolations. Our model produces animated portraits from diverse perspectives, seamlessly blending dynamic foreground and background elements while maintaining temporal consistency and high fidelity.

