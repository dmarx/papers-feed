\section{Related Work}
\noindent\textbf{Portrait Image Animation.}
Recent advancements in the domain of portrait image animation have been significantly propelled by innovations in audio-driven techniques. 
Notable frameworks, such as LipSyncExpert~\cite{prajwal2020lip} and SadTalker~\cite{zhang2023sadtalker}, have tackled challenges related to facial synchronization and expression modulation, achieving dynamic lip movements and coherent head motions. 
Concurrently, DiffTalk~\cite{shen2023difftalk} and VividTalk~\cite{sun2023vividtalk} have integrated latent diffusion models, enhancing output quality while generalizing across diverse identities without the necessity for extensive fine-tuning. 
Furthermore, studies such as DreamTalk~\cite{ma2023dreamtalk} and EMO~\cite{tian2024emo} underscore the importance of emotional expressiveness by showcasing the integration of audio cues with facial dynamics. 
AniPortrait~\cite{wei2024aniportrait} and VASA-1~\cite{xu2024vasa} propose methodologies that facilitate the generation of high-fidelity animations, emphasizing temporal consistency along with effective exploitation of static images and audio clips. 
In addition, recent innovations like LivePortrait~\cite{guo2024liveportrait} and Loopy~\cite{jiang2024loopy} focus on enhancing computational efficiency while ensuring realism and fluid motion. 
Furthermore, the works of Hallo~\cite{xu2024hallo} and Hallo2~\cite{cui2024hallo2} have made significant progress in extending capabilities to facilitate long-duration video synthesis and integrating adjustable semantic inputs, thereby marking a step towards richer and more controllable content generation. 
Nevertheless, existing facial animation techniques still encounter limitations in addressing extreme facial poses, accommodating background motion in dynamic environments, and incorporating camera movements dictated by textual prompts.

\noindent\textbf{Diffusion-Based Video Generation.}
Unet-based diffusion model has made notable strides, exemplified by frameworks such as Make-A-Video and MagicVideo~\cite{zhou2022magicvideo}. 
Specifically, Make-A-Video~\cite{singer2022make} capitalizes on pre-existing Text-to-Image~(T2I) models to enhance training efficiency without necessitating paired text-video data, thereby achieving state-of-the-art results across a variety of qualitative and quantitative metrics. 
Simultaneously, MagicVideo~\cite{zhou2022magicvideo} employs an innovative 3D U-Net architecture to operate within a low-dimensional latent space, achieving efficient video synthesis while significantly reducing computational requirements. 
Building upon these foundational principles, AnimateDiff~\cite{guo2023animatediff} introduces a motion module that integrates seamlessly with personalized T2I models, allowing for the generation of temporally coherent animations without the need for model-specific adjustments. 
Additionally, VideoComposer~\cite{wang2023videocomposer} enhances the controllability of video synthesis by incorporating spatial, temporal, and textual conditions, which facilitates improved inter-frame consistency. 
The development of diffusion models continues with the advent of DiT-based approaches such as CogVideoX~\cite{yang2024cogvideox} and Movie Gen~\cite{polyak2024movie}. 
CogVideoX employs a 3D Variational Autoencoder to improve video fidelity and narrative coherence, whereas Movie Gen establishes a robust foundation for high-quality video generation complemented by advanced editing capabilities. 
In the present study, we adopt the DiT diffusion formulation to optimize the generalization capabilities of the generated video.
