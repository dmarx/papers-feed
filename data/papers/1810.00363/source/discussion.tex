%!TEX root = main.tex

Making generic machine learning techniques more data efficient is crucial to
reduce the costs related to annotation. While other approaches may also be
important to solve this challenge, such as incorporating more prior
knowledge in the architecture~\citep[\eg,][]{oyallon2017scaling},
semi-supervised learning~\citep{chapelle06semi} or
meta-learning~\citep[when multiple tasks or datasets are available, see, \eg,][]{thrun1998lifelong},
basic regularization principles are needed.
Such principles are also essential for obtaining robust models in applications where
security is a concern, such as self-driving cars.
Our paper presents various algorithmic strategies for regularization on generic deep
convolutional networks, by leveraging the structure of an appropriate RKHS,
leading to many existing principles to regularization, as well as new effective variants,
in addition to providing theoretical guarantees and insights on different methods.

