%!TEX root = main.tex

In this section, we recall the kernel perspective on deep networks introduced
by~\citet{bietti2018group}, and present upper and lower bounds on the RKHS norm
of a given model, leading to various regularization strategies.  For
simplicity, we first consider real-valued networks and binary classification,
before discussing multi-class extensions.

\subsection{Relation between deep networks and RKHSs}
\label{sub:rkhs_construction}
Kernel methods consist of mapping data living in a set~$\Xcal$ to a
RKHS~$\Hcal$ associated to a positive definite kernel~$K$ through a mapping
function $\Phi: \Xcal \to \Hcal$, and then learning simple machine learning
models in~$\Hcal$. Specifically, when considering a real-valued regression or
binary classification problem, classical kernel methods find a prediction
function $f : \mathcal{X} \to \R$ living in the RKHS which can be written in linear
form, i.e., such that $f(x) = \langle f, \Phi(x) \rangle_\Hc$ for all~$x$
in~$\Xcal$.  While explicit mapping to a possibly infinite-dimensional space is
of course only an abstract mathematical operation, learning~$f$ can be done
implicitly by computing kernel evaluations and typically by using convex
programming~\citep{scholkopf2001learning}.

Moreover, the RKHS norm~$\|f\|_\Hc$ acts as a natural regularization function,
which controls the variations of model predictions
according to the geometry induced by~$\Phi$:
\begin{equation}
\label{eq:cs}
|f(x) - f(x')| \leq \|f\|_\Hc \cdot \|\Phi(x) - \Phi(x') \|_\Hc.
\end{equation}
Unfortunately, our setup does not allow us to use the RKHS norm in a traditional way since evaluating the kernel is intractable. Instead, we
propose a different approach that considers explicit parameterized
representations of functions contained in the RKHS, given by generic CNNs,
and leverage properties of the RKHS and the
kernel mapping in order to regularize when learning the network parameters.


Consider indeed a real-valued deep convolutional network $f : \mathcal{X} \to
\R$, where $\mathcal X$ is simply $\R^d$, with rectified linear unit (ReLU) activations and no bias units.
By constructing an appropriate multi-layer hierarchical kernel, \citet{bietti2018group} show
that the corresponding RKHS~$\Hc$ contains a CNN with the same architecture and parameters as~$f$,
but with activations that are smooth approximations of ReLU.
Although the model predictions might not be strictly equal, we will abuse notation and denote this
approximation with smooth ReLU by~$f$ as well,
with the hope that the regularization procedures derived from the RKHS model
will be effective in practice on the original CNN~$f$.

Besides, the mapping~$\Phi(\cdot)$ is shown to be non-expansive:
\begin{equation}
\label{eq:non_expansive}
\|\Phi(x) - \Phi(x') \|_\Hc \leq \|x - x'\|_2,
\end{equation}
so that controlling~$\|f\|_\Hc$ provides some robustness to additive $\ell_2$-perturbations, by~\eqref{eq:cs}.
Additionally, with appropriate pooling operations, \citet{bietti2018group} show that the kernel mapping is also
stable to deformations, meaning that the RKHS norm also controls robustness to translations
and other transformations including scaling and rotations,
which can be seen as deformations when they are small.

In contrast to standard kernel methods, where the RKHS norm is typically available in closed form, this norm is difficult to compute in our setup, and requires approximations.
The following sections present upper and lower bounds on~$\|f\|_{\Hcal}$,
with linear convolutional operations denoted by~$W_k$ for $k=1, \ldots, L$, where~$L$ is the number of layers.
Defining~$\theta := \{W_k : k = 1, \ldots, L\}$, we then leverage these bounds to approximately solve the following
penalized or constrained optimization problems on a training set~$(x_i, y_i), i = 1, \ldots, n$:
\begin{align}
\label{eq:penalty_or_constraint}
& \min_\theta \frac{1}{n} \sum_{i=1}^n \ell(y_i, f_\theta(x_i)) + \lambda \|f_\theta\|_\Hc^2 \quad \text{or } \\
& \min_{\theta: \|f_\theta\|_\Hc \leq C} \frac{1}{n} \sum_{i=1}^n \ell(y_i, f_\theta(x_i)).
\end{align}
We also note that while the construction of~\citet{bietti2018group} considers VGG-like networks~\citep{simonyan2014very},
the regularization algorithms we obtain in practice can be easily adapted to different architectures
such as residual networks~\citep{he2016deep}.

\subsection{Exploiting lower bounds of the RKHS norm}
\label{sub:lower_bounds}

In this section, we devise regularization algorithms by leveraging lower bounds on~$\|f\|_\Hc$,
obtained by relying on the following variational characterization of Hilbert norms:
\begin{equation*}
\|f\|_\Hc = \sup_{\|u\|_\Hc \leq 1} \langle f, u \rangle_\Hc.
\end{equation*}
At first sight, this definition is not useful since the set $U = \{u \in \Hc : \|u\|_\Hc \leq 1\}$ may be 
infinite-dimensional
 and the inner products $\langle f, u \rangle_\Hc$ cannot be
computed in general. Thus, we devise tractable lower bound approximations by considering smaller sets~$\bar{U} \subset U$.

\paragraph{Adversarial perturbation penalty.}
Thanks to the non-expansiveness of $\Phi$, we can consider the subset $\bar U \subset U$ defined as $\bar{U} = \{\Phi(x + \delta) - \Phi(x) : x \in \mathcal X, \|\delta\|_2 \leq 1 \}$,
leading to the bound
\begin{equation}
\label{eq:lower_bound}
\|f\|_\Hc  \geq  \|f\|_\delta^2 := \sup_{x \in \mathcal X, \|\delta\|_2 \leq 1} f(x + \delta) - f(x),
\end{equation}
which is reminiscent of adversarial perturbations. Adding a regularization parameter $\epsilon > 0$ in front of the norm
then corresponds to different sizes of perturbations:
\begin{equation}
\label{eq:kernel_adv}
\epsilon \|f\|_\Hc = \sup_{\|u\|_\Hc \leq \epsilon} \langle f, u \rangle_\Hc \geq \sup_{x \in \mathcal X, \|\delta\|_2 \leq \epsilon} f(x + \delta) - f(x).
\end{equation}
Using this lower bound or its square as a penalty in the objective~\eqref{eq:penalty_or_constraint}
when training a CNN provides a way to regularize.
Optimizing over adversarial perturbations has been useful to obtain robust models~\citep[\eg, the PGD method of~][]{madry2018towards};
yet our approach differs in two important ways: 

(i) it involves a penalty that is decoupled from the loss term such that 
in principle, our penalty could be used beyond the supervised empirical risk paradigm.
In contrast, PGD optimizes the robust formulation~\eqref{eq:robust} below, which 
fits training data while considering
perturbations on the loss.

(ii) our penalty involves a global maximization problem
on the input space~$\Xcal$, as opposed to only maximizing on perturbations near
training data. In practice, optimizing over~$\Xcal$ is however
difficult and instead, we replace~$\Xcal$ by random mini-batches of examples,
yielding further lower bounds on the RKHS norm. These examples may be labeled or not,
in contrast to PGD that perturb labeled examples only.
When using such a mini-batch,
a gradient of the penalty can be obtained by first finding maximizers~$\hat x, \hat \delta$
(where~$\hat x$ is an element of the mini-batch and $\hat{\delta}$ is a perturbation), and then computing gradients
of $f_\theta(\hat x + \hat \delta) - f_\theta(\hat x)$ with respect to~$\theta$ by using back-propagation.
In practice, we compute the perturbations~$\delta$ for each example~$x$ by using a few steps of
projected gradient ascent with constant step-lengths.

\paragraph{Robust optimization yields another lower bound.}
In some contexts, our penalized approach is related to solving the robust optimization problem
\begin{equation}
\label{eq:robust}
\min_\theta \frac{1}{n} \sum_{i=1}^n \sup_{\|\delta\|_2 \leq \epsilon} \ell(y_i, f_\theta(x_i + \delta)),
\end{equation}
which is commonly considered for training adversarially robust classifiers~\citep{wong2018provable,madry2018towards,raghunathan2018certified}.
In particular, \citet{xu2009robustness} show that the penalized and
robust objectives are equivalent in the case of the hinge loss with linear predictors,
when the data is non-separable.
They also show the equivalence for kernel methods when considering the (intractable) full perturbation set~$U$
around each point in the RKHS~$\Phi(x_i)$, that is, predictions $\langle f, \Phi(x_i) + u \rangle_\Hc$ with~$u$ in $U$.
Intuitively, when a training example $(x_i, y_i)$ is misclassified, we are in the ``linear'' part of the hinge loss, such~that
\begin{equation*}
\sup_{\|u\|_\Hc \leq \epsilon} \ell(y_i, \langle f, \Phi(x_i) + u \rangle_\Hc) = \ell(y_i, f(x_i)) + \epsilon \|f\|_{\Hcal}.
\end{equation*}
For other losses such as the logistic loss, a regularization effect is still present even
for correctly classified examples,
though it may be smaller since the loss has a reduced slope for such points.
This leads to an \emph{adaptive} regularization mechanism that may automatically reduce
the amount of regularization when the data is easily separable.
However, the robust optimization approach might only encourage local stability around training examples, while the global quantity~$\|f\|_\Hc$
may become large in order to better fit the data.
We note that a perfect fit of the data with large complexity does not prevent generalization~\citep[see, \eg,][]{belkin2018overfitting,belkin2018understand};
yet, such mechanisms are still poorly understood.
Nevertheless, it is easy to show that the robust objective~\eqref{eq:robust}
lower bounds the penalized objective with penalty~$\epsilon \|f\|_{\Hcal}$.

\paragraph{Gradient penalties.}
Taking $\bar{U} \!= \! \{\frac{\Phi(x) - \Phi(y)}{\|x - y\|_2} : x, y \!\in\! \mathcal X\}$, which is a subset of~$U$
by Eq.~\eqref{eq:non_expansive}---it turns out that this is the same set as for adversarial perturbation penalties,
since~$\Phi$ is homogeneous~\citep{bietti2018group} and $\mathcal X =
\Real^d$---we obtain a lower bound based on the Lipschitz constant of~$f$:
\begin{equation}
\|f\|_\Hc \geq \sup_{x, y \in \mathcal{X}} \frac{f(x) - f(y)}{\|x - y\|_2} \geq \|\nabla f\| := \sup_{x \in \mathcal X} \|\nabla f(x)\|_2, \label{eq:gradientpenalty}
\end{equation}
where the second inequality becomes an equality when~$\Xcal$ is convex,
and the supremum is taken over points where~$f$ is differentiable.
Although we are unaware of previous work using this exact lower bound for a generic regularization penalty,
we note that variants replacing the supremum over~$x$ by an expectation over data have been recently used
to stabilize the training of generative adversarial networks~\citep{gulrajani2017improved,roth2017stabilizing},
and we provide insights in Section~\ref{sub:gan_reg} on the benefits of RKHS regularization in such a setting.
Related penalties have been considered in the context of robust optimization,
for regularization or robustness,
noting that a penalty based on the gradient of the loss function $x \mapsto \ell(y, f(x))$ can give a good approximation of~$\eqref{eq:robust}$
when~$\epsilon$ is small~\citep{drucker1991double,lyu2015unified,roth2018adversarially,simon2018adversarial}.

\paragraph{Penalties based on deformation stability.}
We may also obtain new penalties by considering more exotic sets
$\bar{U} = \{\Phi(\tilde x) - \Phi(x) : x~\in~{\mathcal X},~ \tilde x \text{ is a small} \text{ deformation of }x\}$,
where the amount of deformation is dictated by the stability bounds of~\citet{bietti2018group} in order to ensure that $\bar{U} \subset U$.
More precisely, such bounds depend on the maximum displacement and Jacobian norm of
the diffeomorphisms considered. These can be easily computed for various parameterized families
of transformations, such as translations, scaling or rotations, leading to simple ways to control
the regularization strength through the parameters of these transformations.
One can also consider infinitesimal deformations from such parameterized transformations,
which approximately yields the \emph{tangent propagation} regularization
strategy of~\citet{simard1998transformation}.
These approaches are detailed in Appendix~\ref{sec:deformation_penalties}.
If instead we consider the robust optimization formulation~\eqref{eq:robust}, we obtain a form
of \emph{data augmentation} where transformations are optimized instead of sampled, as done by~\citep{engstrom2017rotation}.


\paragraph{Extensions to multiple classes and beyond}
\label{sub:multiclass}

We now extend the regularization strategies based on lower bounds to multi-valued networks,
in order to deal with multiple classes.
For that purpose, we consider a multi-class penalty $\|f_1\|_\Hc^2 + \ldots + \|f_K\|_\Hc^2$
for an~$\R^K$-valued function $f = (f_1, f_2, \ldots, f_K)$, and we define 
\begin{align*}
\|f\|_\delta^2 := \sum_{k=1}^K \|f_k\|_\delta^2 \text{~~~and~~~} \|\nabla f\|^2 := \sum_{k=1}^K \|\nabla f_k\|^2,
\end{align*}
where $\|f_k\|_\delta$ is the adversarial penalty~(\ref{eq:lower_bound}), and $\|\nabla f_k\|$ is defined in~(\ref{eq:gradientpenalty}).
For deformation stability penalties, we proceed in a similar manner,
and for robust optimization formulations~\eqref{eq:robust}, the extension is straightforward,
given that multi-class losses such as cross-entropy can be
directly optimized in an adversarial training or gradient penalty setup.

Finally, we note that while the kernel approach we introduce considers
the Euclidian geometry in the input space, it is possible to consider heuristic alternatives for other
geometries, such as~$\ell_\infty$ perturbations, as discussed in Appendix~\ref{sec:non_euclidian_appx}.

\subsection{Exploiting upper bounds with spectral norms}
\label{sub:upper_bounds}

Instead of lower bounds, one may use instead
 the following upper bound from~\citet[Proposition 14]{bietti2018group}:
\begin{equation}
\label{eq:upper_bound}
\|f\|_\Hc \leq \omega(\|W_1\|, \ldots, \|W_L\|),
\end{equation}
where~$\omega$ is increasing in all of its arguments, and~$\|W_k\|$ is the spectral norm of the linear
operator~$W_k$.
Here, we simply consider the spectral norm on the filters, given by~$\|W\| := \sup_{\|x\|_2 \leq 1} \|W x\|_2$.
Other generalization bounds relying on similar quantities have been proposed for
controlling complexity~\citep{bartlett2017spectrally,neyshabur2017pac}, suggesting that using them for
regularization is relevant even beyond our kernel perspective,
as observed by~\citet{cisse2017parseval,sedghi2018singular,yoshida2017spectral}.
Extensions to multiple classes are simple to obtain by simply considering spectral norms up to
the last layer.


\paragraph{Penalizing the spectral norms.}
One way to control the upper bound~\eqref{eq:upper_bound} when learning a neural network~$f_\theta$
is to consider a regularization penalty based on spectral norms
\begin{equation} 
	\label{eq:optimization_problem_penalized} 
	\min_{\theta}\frac{1}{n} \sum_{i=1}^{n} \ell(y_i, f_{\theta}(x_i)) + \lambda \sum_{l=1}^{L} \|W_l\|^2,
\end{equation}
where~$\lambda$ is a regularization parameter.
To optimize this cost, one can obtain (sub)gradients of the penalty by computing singular vectors
associated to the largest singular value of each~$W_l$.
We consider the method of~\citet{yoshida2017spectral}, which computes such singular vectors approximately
using one or two iterations of the power method, as well as a more costly approach using the full SVD.

\paragraph{Constraining the spectral norms with a continuation approach.}
In the constrained setting, we want to optimize:
\begin{equation*}
\begin{aligned}
\min_{\theta}\frac{1}{n} \sum_{i=1}^{n} \ell(y_i, f_{\theta}(x_i)) \text{~~s.t.~~} \|W_l\| \leq \tau \text{ ; } l\in 1, \dots, L~,
\end{aligned}
\end{equation*}
where $\tau$ is a user-defined constraint.
This objective may be optimized by projecting each~$W_l$ in the
spectral norm ball of radius $\tau$ after each gradient step.
Such a projection is achieved by truncating the singular values to be smaller than~$\tau$ (see Appendix~\ref{sec:spectral_norms_appx}).
We found that the loss was hardly optimized with this approach,
and therefore introduce a continuation approach with an exponentially
decaying schedule for~$\tau$ reaching a constant $\tau_0$
after a few epochs, which we found to be important for good empirical performance.

\subsection{Combining upper and lower bounds.}
One advantage of lower bound penalties is that they are independent of
the model parameterization, making them flexible enough to use with more complex architectures.
In addition, the connection with robust optimization can provide a useful mechanism for adaptive regularization.
However, they do not provide a guaranteed control on the RKHS norm, unlike the upper bound strategies.
This is particularly true for robust optimization approaches, which may favor small training loss
and local stability over global stability through~$\|f\|_\Hc$.
Nevertheless, we observed that our new approaches based on separate penalties
sometimes do help in controlling upper bounds as well (see Section~\ref{sec:experiments}).

While these upper bound strategies are useful for limiting model complexity,
we found them empirically less effective for robustness (see Section~\ref{sub:exp_robust}).
However, we observed that combining with lower bound approaches can overcome this weakness,
perhaps due to a better control of local stability.
In particular, such combined approaches often provide the best generalization performance
in small data scenarios, as well as better guarantees on adversarially robust generalization
thanks to a tighter control of the RKHS norm.

