%!TEX root = main.tex

In this section, we study how the kernel perspective allows us to extend standard margin-based generalization bounds 
to an adversarial setting in order to
provide theoretical guarantees on adversarially robust generalization.
We then discuss how our kernel approach provides novel interpretations for training
generative adversarial networks.

\subsection{Guarantees on adversarial generalization}
\label{sub:guarantees}

While various methods have been introduced to empirically gain robustness to adversarial perturbations,
the ability to generalize with such perturbations, also known as \emph{adversarial generalization}~\citep{schmidt2018adversarially},
still lacks theoretical understanding.
Margin-based bounds have been useful
to explain the generalization behavior of learning algorithms that can fit the training data
well, such as kernel methods, boosting and neural networks~\citep{koltchinskii2002empirical,boucheron2005theory,bartlett2017spectrally}.
Here, we show how such arguments can be adapted to obtain guarantees on adversarial generalization,
\ie, on the expected classification error in the presence of an~$\ell_2$-bounded adversary,
based on the RKHS norm of a learned model.
For a binary classification task with labels in $\mathcal Y = \{-1,1\}$ and data distribution~$\mathcal D$,
we would like to bound the expected adversarial error of a classifier~$f$, given for some $\epsilon > 0$ by
\begin{equation}
\label{eq:adv_error}
\text{err}_\mathcal D(f, \epsilon) := P_{(x,y) \sim \mathcal D} (\exists \|\delta\|_2 \leq \epsilon: ~y f(x + \delta) < 0).
\end{equation}
Leveraging the fact that~$f$ is $\|f\|_\Hc$-Lipschitz,
we now show how to further bound this quantity using empirical margins,
following the usual approach to obtaining margin bounds for kernel methods~\citep[\eg,][]{boucheron2005theory}.
Consider a training dataset $(x_1, y_1), \ldots, (x_n, y_n) \in \mathcal X \times \mathcal Y$.
Defining $L_n^\gamma(f) := \frac{1}{n} \sum_{i=1}^n \textbf{1}\{y_i f(x_i) < \gamma\}$,
we have the following bound, proved in Appendix~\ref{sec:generalization_appx}:
\begin{proposition}[Adversarially robust margin bound]
\label{prop:robust_margin_bound}
With probability~$1 - \delta$ over a dataset $\{(x_i, y_i)\}_{i=1, \ldots, n}$, we have,
for all choices of $\gamma > 0$ and~$f \in \Hc$,
\begin{align}
\label{eq:robust_margin_bound}
& \text{err}_\mathcal D(f, \epsilon) \leq L_n^{\gamma + 2 \epsilon \|f\|_\Hc}(f) + \tilde{O}\left( \frac{\|f\|_\Hc \bar{B}}{\gamma \sqrt{n}}  \right),
\end{align}
where $\bar{B} = \sqrt{\frac{1}{n}\sum_{i=1}^n K(x_i, x_i)}$ and $\tilde{O}$ hides a term depending logarithmically on~$\|f\|_\Hc, \gamma$, and $\delta$.
\end{proposition}

When $\epsilon = 0$, we obtain the usual margin bound, while $\epsilon > 0$ yields
a bound on adversarial error~$\text{err}_\mathcal D(f, \epsilon)$,
for some neural network~$f$ learned from data.
Note that other complexity measures based on products of spectral norms may be used instead of~$\|f\|_\Hcal$,
as well as multi-class extensions, following~\citet{bartlett2017spectrally,neyshabur2017pac}.
In concurrent work, \citet{khim2018adversarial,yin2019rademacher} derive similar bounds
in the context of fully-connected networks.
In contrast to these works, which bound complexity of a modified function class,
our bound uses the complexity of the original class and leverages smoothness properties
of functions to derive the margin bound.

One can then study the effectiveness of a regularization algorithm by inspecting
cumulative distribution (CDF) plots of the
\emph{normalized margins} $\bar{\gamma}_i = y_i f(x_i) / \|f\|_\Hc$,
for different strengths of regularization (an example is given in Figure~\ref{fig:norms_and_margins}, Section~\ref{sub:exp_robust}).
According to the bound~\eqref{eq:robust_margin_bound}, one can assess expected adversarial error with $\epsilon$-bounded perturbations
by looking at the part of the plot to the right of~$\bar{\gamma} = 2\epsilon$.
In particular, the value of the CDF at such a value of~$\bar{\gamma}$ is representative of the
bound for large~$n$ (since the second term is negligible),
while for smaller~$n$, the best bound is obtained for a larger value of~$\bar{\gamma}$, which also suggests that
the right side of the plots is indicative of performance on small datasets.

When the RKHS norm can be well approximated, our bound provides a certificate on
test error in the presence of adversaries. 
While such an approximation is difficult to
obtain in general, the guarantee is most useful when lower and upper bounds of the RKHS norm are controlled together.


\subsection{New insights on generative adversarial networks}
\label{sub:gan_reg}

Generative adversarial networks (GANs) attempt to learn a \emph{generator} neural network~$G_\phi : \mathcal Z \to \mathcal X$,
so that the distribution of~$G_\phi(z)$ with~$z \sim D_z$ a noise vector resembles a data distribution~$D_x$.
In this section, we discuss connections between recent regularization techniques for
training GANs, and approaches to learning generative models
based on a MMD criterion~\citep{gretton2012kernel}, in view of our RKHS framework.
Our goal is to provide a new insight on these methods, but not necessarily to provide a new one.

Various recent approaches have relied on regularization strategies on a \emph{discriminator} network
in order to improve the stability of GAN training and the quality of the produced samples.
Some of these resemble the approaches presented in Section~\ref{sec:kernel_reg}
such as gradient penalties~\citep{gulrajani2017improved,roth2017stabilizing}
and spectral norm regularization~\citep[]{miyato2018spectral}.
We provide an RKHS interpretation of these methods as
optimizing an MMD distance with the convolutional kernel introduced in Section~\ref{sec:kernel_reg}:
\begin{equation}
\label{eq:ckn_mmd}
\min_\phi \sup_{\|f\|_\Hc \leq 1} \E_{x \sim D_x}[ f(x)] - \E_{z \sim D_z}[f(G_\phi(z))].
\end{equation}
When learning from an empirical distribution over~$n$ samples,
the MMD criterion is known to have much better sample complexity than the Wasserstein-1
distance considered by~\citet{arjovsky2017wasserstein} for high-dimensional data
such as images~\citep{sriperumbudur2012empirical}.
While the MMD approach has been used for training generative models, it generally relies on a generic kernel function,
such as a Gaussian kernel, that appears explicitly in the objective~\citep{dziugaite2015training,li2017mmd,binkowski2018demystifying}.
Although using a learned feature extractor can improve this, the Gaussian kernel might be a poor choice when
dealing with natural signals such as images, while the hierarchical kernel we consider in our paper is better suited
for this type of data, by providing useful invariance and stability properties.
Leveraging the variational form of the MMD~\eqref{eq:ckn_mmd} with this kernel suggests for instance using convolutional networks
as the discriminator~$f$, with constraints on the spectral norms in order to ensure~$\|f\|_\Hc \leq C$ for some~$C$,
as done by~\citet{miyato2018spectral} through normalization.
