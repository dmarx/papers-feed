%!TEX root = main.tex

This section provides more details on the deformation stability penalties mentioned in
Section~\ref{sub:lower_bounds}, and the practical versions we use in our experiments on the
Infinite MNIST dataset~\citep{loosli-canu-bottou-2006}.

\paragraph{Stability to deformations.}
We begin by providing some background on deformation stability,
recalling that these can provide new lower bound penalties as explained in Section~\ref{sub:lower_bounds}.
Viewing an element $x \in \mathcal X$ as a signal $x(u)$, where $u$ denotes the location
(\eg~a two-dimensional vector for images),
we denote by $x_\tau$ a deformed version of~$x$ given by $x_\tau(u) = x(u - \tau(u))$,
where~$\tau$ is a diffeomorphism.
The deformation stability bounds of~\citet{bietti2018group} take the form:
\begin{equation}
\label{eq:stability_bound}
\|\Phi(x_\tau) - \Phi(x)\|_\Hcal \leq (C_1 \|\tau\|_\infty + C_2 \|\nabla \tau\|_\infty) \|x\|,
\end{equation}
where $\nabla \tau (u)$ is the Jacobian of $\tau$ at location~$u$.
Here, $C_1$ controls translation invariance and typically decreases with the total amount of pooling
(\ie, translation invariance more or less corresponds to the resolution at the final layer),
while~$C_2$ controls stability to deformations (note that $\nabla \tau = 0$ for translations)
and is typically smaller when using small patches.
We note that the bounds assume linear pooling layers with a certain spatial decay,
adapted to the resolution of the current layer;
our experiments on Infinite MNIST with deformation stability penalties
thus use average pooling layers on 2x2 neighborhoods.


\paragraph{Adversarial deformation penalty.}
We can obtain lower bound penalties by exploiting the above stability bounds in
a similar manner to the adversarial perturbation penalty introduced in Section~\ref{sub:lower_bounds}.
In particular, assuming a scalar-valued convolutional network~$f$:
\begin{equation}
\label{eq:adv_deformation}
\|f\|_\tau^2 := \sup_{x \in \mathcal X, \tau \in \mathcal T} (f(x_\tau) - f(x))^2 \\
\end{equation}
where~$\mathcal T$ is a collection of diffeomorphisms.
When the diffeomorphisms in~$\mathcal T$ have bounded norm~$\|\tau\|_\infty$ and
Jacobian norm~$\|\nabla \tau\|_\infty$,
and assuming $\mathcal X$ (or, in practice, the training data) is bounded,
the stability bound~\ref{eq:stability_bound} ensures that
the set $U_{\mathcal T} = \{\Phi(x_\tau) - \Phi(x) : x \in \mathcal X, \tau \in \mathcal T\}$ is included in an
RKHS ball with some radius $r$, so that~$\|f\|_\tau$ is a lower bound on~$r \|f\|_\Hcal$.

\paragraph{Tangent gradient penalty.}
We also consider the following gradient penalty along tangent vectors,
which provides an approximation of the above adversarial penalty when
considering small, parameterized deformations,
and recovers the tangent propagation strategy of~\citet{simard1998transformation}:
\begin{equation}
\label{eq:tangent_gradient}
\|D_\tau f\|^2 := \sup_{x \in \mathcal X} \|\partial_\alpha f(x + \sum_i \alpha_i t_{x,i}) \|^2,
\end{equation}
where $\{t_{x,i}\}_{i=1,\ldots, q}$ are
tangent vectors at~$x$ obtained from a given set of deformations.
To see the link with the adversarial deformation penalty~\ref{eq:adv_deformation},
consider for simplicity a single deformation,~$\mathcal T = \{\tau_0\}$.
For small~$\alpha$, we have
\begin{align*}
x_{\alpha \tau_0} \approx x + \alpha t_x, \quad \text{where} \quad t_x(u) = \tau_0(u) \cdot \nabla x(u),
\end{align*}
where~$t_x$ denotes the tangent vector of the deformation
manifold $\{\alpha \tau_0 : \alpha\}$ at~$\alpha = 0$~\citep{simard1998transformation}.
Then,
\[
f(x_{\alpha\tau_0}) - f(x) \approx \alpha \partial_\alpha f(x + \alpha t_x) = \alpha \langle \nabla f(x), t_x \rangle.
\]
In this case, denoting $\alpha \mathcal T = \{\alpha \tau_0\}$, we have
\[
\sup_{x \in \mathcal X, \tau \in \alpha \mathcal T} (f(x_\tau) - f(x))^2 \approx \alpha^2 \sup_{x \in \mathcal X} |\partial_\alpha f(x + \alpha t_x)|^2,
\]
so that when $\alpha$ is small, the adversarial penalty can be approximated by $\alpha \|D_\tau f\|$
(note that using $\alpha \mathcal T$ instead of~$\mathcal T$ in the adversarial penalty
would also yield a scaling by~$\alpha$, since the stability bounds imply $\alpha$ times smaller
perturbations in the RKHS).

\paragraph{Practical implementations on Infinite MNIST.}
In our experiments on Infinite MNIST, we compute $\|f\|_\tau^2$ by considering 32 random transformations
of each digit in a mini-batch of training examples,
and taking the maximum over both the example and the transformation.
We do this separately for each class, as for the other lower bound penalties $\|f\|_\delta^2$ and $\|\nabla f\|^2$.
For~$\|D_\tau f\|^2$, we take $\{t_{x,i}\}_{i=1,\ldots,q}$ with~$q=30$ to be tangent vectors given
by random diffeomorphisms from Infinite MNIST around each example~$x$.
