\section{Discussion and Future Directions}
\label{sec:discussion}


\sysname is 2$\times$ faster than \sysnameone, which means that we can train models
with 16k longer context for the same price as previously training a 8k context
model.
We are excited about how this can be used to understand long books and reports,
high resolution images, audio and video.
\sysname will also speed up training, finetuning, and inference of
existing models.

In the near future, we plan to collaborate with researchers and engineers to
make FlashAttention widely applicable in different kinds of devices (e.g., H100
GPUs, AMD GPUs), as well as new data types such as FP8.
As an immediate next step, we plan to optimize FlashAttention-2 for H100 GPUs to
use new hardware features (TMA, 4th-gen Tensor Cores, fp8).
Combining the low-level optimizations in FlashAttention-2 with high-level
algorithmic changes (e.g., local, dilated, block-sparse attention) could allow
us to train AI models with much longer context.
We are also excited to work with compiler researchers to make these optimization
techniques easily programmable.


% Next step: (1) Optimizing for other hardware, such as H100, AMD GPUs.
% (2) LLM inference, with kv cache
% (3) Quantization and sparsity (cite new paper).

% Hardware \& software lottery. Talk about Triton. Other emerging models (RNNs).
