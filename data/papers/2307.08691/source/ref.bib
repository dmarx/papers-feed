@inproceedings{wolf1991data,
  title={A data locality optimizing algorithm},
  author={Wolf, Michael E and Lam, Monica S},
  booktitle={Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
  pages={30--44},
  year={1991}
}

@inproceedings{mcsherry2015scalability,
  title={Scalability! but at what $\{$COST$\}$?},
  author={McSherry, Frank and Isard, Michael and Murray, Derek G},
  booktitle={15th Workshop on Hot Topics in Operating Systems (HotOS XV)},
  year={2015}
}

@article{denning1968working,
  title={The working set model for program behavior},
  author={Denning, Peter J},
  journal={Communications of the ACM},
  volume={11},
  number={5},
  pages={323--333},
  year={1968},
  publisher={ACM New York, NY, USA}
}

@article{jia2018dissecting,
  title={Dissecting the NVIDIA {V}olta {GPU} architecture via microbenchmarking},
  author={Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P},
  journal={arXiv preprint arXiv:1804.06826},
  year={2018}
}

@inproceedings{gu2022efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R\'e, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2022}
}

@article{aggarwal1988input,
  title={The input/output complexity of sorting and related problems},
  author={Aggarwal, Alok and Vitter, Jeffrey, S},
  journal={Communications of the ACM},
  volume={31},
  number={9},
  pages={1116--1127},
  year={1988},
  publisher={ACM New York, NY, USA}
}

@article{hennessy2003memory,
    title={Memory hierarchy design}, 
    author={Hennessy, John and Patterson, David}, 
    journal={Computer Architecture: A Quantitative Approach}, 
    pages={390--525}, 
    year={2003}, 
    publisher={Morgan Kaufmann Publishers San Francisco, CA} 
}

@article{williams2009roofline,
  title={Roofline: an insightful visual performance model for multicore architectures},
  author={Williams, Samuel and Waterman, Andrew and Patterson, David},
  journal={Communications of the ACM},
  volume={52},
  number={4},
  pages={65--76},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@InProceedings{chalkidis-etal-2019-neural,
    title = "Neural Legal Judgment Prediction in {E}nglish",
    author = "Chalkidis, Ilias  and Androutsopoulos, Ion  and Aletras, Nikolaos",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1424",
    doi = "10.18653/v1/P19-1424",
    pages = "4317--4323"
}

@InProceedings{chalkidis-et-al-2021-ecthr,
    title = "Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases",
    author = "Chalkidis, Ilias and Fergadiotis, Manos and Tsarapatsanis, Dimitrios and Aletras, Nikolaos and Androutsopoulos, Ion and Malakasiotis, Prodromos",
    booktitle = "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    year = "2021",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics"
}

@article{recht2013parallel,
  title={Parallel stochastic gradient algorithms for large-scale matrix completion},
  author={Recht, Benjamin and R{\'e}, Christopher},
  journal={Mathematical Programming Computation},
  volume={5},
  number={2},
  pages={201--226},
  year={2013},
  publisher={Springer}
}

@article{dai2022revisiting,
  title={Revisiting Transformer-based Models for Long Document Classification},
  author={Dai, Xiang and Chalkidis, Ilias and Darkner, Sune and Elliott, Desmond},
  journal={arXiv preprint arXiv:2204.06683},
  year={2022}
}

@article{blackford2002updated,
  title={An updated set of basic linear algebra subprograms (BLAS)},
  author={Blackford, L Susan and Petitet, Antoine and Pozo, Roldan and Remington, Karin and Whaley, R Clint and Demmel, James and Dongarra, Jack and Duff, Iain and Hammarling, Sven and Henry, Greg and others},
  journal={ACM Transactions on Mathematical Software},
  volume={28},
  number={2},
  pages={135--151},
  year={2002}
}

@article{ragan2013halide,
  title={Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
  author={Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'e}do and Amarasinghe, Saman},
  journal={Acm Sigplan Notices},
  volume={48},
  number={6},
  pages={519--530},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@book{ramakrishnan2003database,
  title={Database management systems},
  author={Ramakrishnan, Raghu and Gehrke, Johannes and Gehrke, Johannes},
  volume={3},
  year={2003},
  publisher={McGraw-Hill New York}
}

@inproceedings{mullenbach2018explainable,
  title={Explainable Prediction of Medical Codes from Clinical Text},
  author={Mullenbach, James and Wiegreffe, Sarah and Duke, Jon and Sun, Jimeng and Eisenstein, Jacob},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1},
  pages={1101--1111},
  year={2018}
}

@article{johnson2016mimic,
  title={MIMIC-III, a freely accessible critical care database},
  author={Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Lehman, Li-wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G},
  journal={Scientific data},
  volume={3},
  number={1},
  pages={1--9},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{rae-razavi-2020-transformers,
    title   = "Do Transformers Need Deep Long-Range Memory?",
    author  = "Rae, Jack  and Razavi, Ali",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month   = jul,
    year    = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url     = "https://www.aclweb.org/anthology/2020.acl-main.672"
}

@article{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Tony Z and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal={arXiv preprint arXiv:2102.09690},
  year={2021}
}

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@misc{Gokaslan2019OpenWeb,
    title={OpenWebText Corpus},
    author={Gokaslan, Aaron and Cohen, Vanya and Ellie, Pavlick and Tellex, Stefanie},
    publisher={\url{http://Skylion007.github.io/OpenWebTextCorpus}},
    year={2019}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hooker2020hardware,
  title={The hardware lottery},
  author={Hooker, Sara},
  journal={arXiv preprint arXiv:2009.06489},
  year={2020}
}

@book{cook2012cuda,
author = {Cook, Shane},
title = {CUDA Programming: A Developer’s Guide to Parallel Computing with GPUs},
year = {2012},
isbn = {9780124159334},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st}
}

@inproceedings{dao2020kaleidoscope,
    title={Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps},
    author={Tri Dao and Nimit Sohoni and Albert Gu and Matthew Eichhorn and Amit Blonder and Megan Leszczynski and Atri Rudra and Christopher R{\'e}},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2020}
}

@inproceedings{dao2021pixelated,
  title={Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},
  author={Dao, Tri and Chen, Beidi and Liang, Kaizhao and Yang, Jiaming and Song, Zhao and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{tolstikhin2021mlp,
  title={Mlp-{M}ixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and others},
  journal={arXiv preprint arXiv:2105.01601},
  year={2021}
}

@inproceedings{dao2019learning,
    title={Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations},
    author={Tri Dao and Albert Gu and Matthew Eichhorn and Atri Rudra and Christopher R{\'e}},
    booktitle={International Conference on Machine Learning (ICML)},
    year={2019}
}

@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{parker1995random,
  title={Random butterfly transformations with applications in computational linear algebra},
  author={Parker, D Stott},
  year={1995},
  publisher={Citeseer}
}

@inproceedings{sindhwani2015structured,
  title={Structured transforms for small-footprint deep learning},
  author={Sindhwani, Vikas and Sainath, Tara and Kumar, Sanjiv},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3088--3096},
  year={2015}
}

@article{kailath1979displacement,
  title={Displacement ranks of matrices and linear equations},
  author={Kailath, Thomas and Kung, Sun-Yuan and Morf, Martin},
  journal={Journal of Mathematical Analysis and Applications},
  volume={68},
  number={2},
  pages={395--407},
  year={1979},
  publisher={Elsevier}
}

@inproceedings{desa2018two,
  title={A Two-pronged Progress in Structured Dense Matrix Vector Multiplication},
  author={De Sa, Christopher and Gu, Albert and Puttagunta, Rohan and R{\'e}, Christopher and Rudra, Atri},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1060--1079},
  year={2018},
  organization={SIAM}
}

@inproceedings{thomas2018learning,
  title={Learning compressed transforms with low displacement rank},
  author={Thomas, Anna and Gu, Albert and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in neural information processing systems},
  pages={9052--9060},
  year={2018}
}

@book{pan2012structured,
  title={Structured matrices and polynomials: unified superfast algorithms},
  author={Pan, Victor Y},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{cooley1965algorithm,
  title={An algorithm for the machine calculation of complex Fourier series},
  author={Cooley, James W and Tukey, John W},
  journal={Mathematics of computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  publisher={JSTOR}
}

@article{bailey1990ffts,
  title={{FFT}s in external or hierarchical memory},
  author={Bailey, David H},
  journal={The journal of Supercomputing},
  volume={4},
  number={1},
  pages={23--35},
  year={1990},
  publisher={Springer}
}


%%% diffeq, numerics, PDEs, operators %%%
@inproceedings{li2020fourier,
  title={Fourier Neural Operator for Parametric Partial Differential Equations},
  author={Li, Zongyi and Kovachki, Nikola Borislavov and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{rackauckas2020universal,
  title={Universal differential equations for scientific machine learning},
  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
  journal={arXiv preprint arXiv:2001.04385},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{poli2020hypersolvers,
  title={Hypersolvers: Toward Fast Continuous-Depth Models},
  author={Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{wang2020towards,
  title={Towards physics-informed deep learning for turbulent flow prediction},
  author={Wang, Rui and Kashinath, Karthik and Mustafa, Mustafa and Albert, Adrian and Yu, Rose},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1457--1466},
  year={2020}
}

@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational Physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@article{massaroli2021differentiable,
  title={Differentiable Multiple Shooting Layers},
  author={Massaroli, Stefano and Poli, Michael and Sonoda, Sho and Suzuki, Taji and Park, Jinkyoo and Yamashita, Atsushi and Asama, Hajime},
  journal={arXiv preprint arXiv:2106.03885},
  year={2021}
}

@article{kidger2020neural,
  title={Neural controlled differential equations for irregular time series},
  author={Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},
  journal={arXiv preprint arXiv:2005.08926},
  year={2020}
}

@article{kochkov2021machine,
  title={Machine learning--accelerated computational fluid dynamics},
  author={Kochkov, Dmitrii and Smith, Jamie A and Alieva, Ayya and Wang, Qing and Brenner, Michael P and Hoyer, Stephan},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={21},
  year={2021},
  publisher={National Acad Sciences}
}

@article{fan2020solving,
  title={Solving Inverse Problems in Steady-State Navier-Stokes Equations using Deep Neural Networks},
  author={Fan, Tiffany and Xu, Kailai and Pathak, Jay and Darve, Eric},
  journal={arXiv preprint arXiv:2008.13074},
  year={2020}
}

@article{jolicoeur2021gotta,
  title={Gotta Go Fast When Generating Data with Score-Based Models},
  author={Jolicoeur-Martineau, Alexia and Li, Ke and Pich{\'e}-Taillefer, R{\'e}mi and Kachman, Tal and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:2105.14080},
  year={2021}
}


@InProceedings{Goli_2020_CVPR,
author = {Goli, Negar and Aamodt, Tor M.},
title = {ReSprop: Reuse Sparsified Backpropagation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{raihan2020sparse,
  title={Sparse weight activation training},
  author={Raihan, Md Aamir and Aamodt, Tor M},
  journal={arXiv preprint arXiv:2001.01969},
  year={2020}
}

@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nyst{\"o}m-based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14138},
  year={2021},
}

@inproceedings{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{jalali2011clustering,
  title={Clustering partially observed graphs via convex optimization},
  author={Jalali, Ali and Chen, Yudong and Sanghavi, Sujay and Xu, Huan},
  booktitle={ICML},
  year={2011}
}

@article{luo2011high,
  title={High dimensional low rank and sparse covariance matrix estimation via convex minimization},
  author={Luo, Xi},
  journal={arXiv preprint arXiv:1111.1133},
  volume={199},
  year={2011}
}

@article{candes2011robust,
  title={Robust principal component analysis?},
  author={Cand{\`e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and Wright, John},
  journal={Journal of the ACM (JACM)},
  volume={58},
  number={3},
  pages={1--37},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@inproceedings{scatterbrain,
  title={Scatterbrain: Unifying Sparse and Low-rank Attention},
  author={Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and Rudra, Atri and Ré, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@article{foldiak2003sparse,
  title={Sparse coding in the primate cortex},
  author={Foldiak, Peter},
  journal={The handbook of brain theory and neural networks},
  year={2003},
  publisher={MIT Press}
}

@article{candes2006stable,
  title={Stable signal recovery from incomplete and inaccurate measurements},
  author={Candes, Emmanuel J and Romberg, Justin K and Tao, Terence},
  journal={Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
  volume={59},
  number={8},
  pages={1207--1223},
  year={2006},
  publisher={Wiley Online Library}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}

@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{lee2018snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}

@article{chen2019slide,
  title={Slide: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems},
  author={Chen, Beidi and Medini, Tharun and Farwell, James and Gobriel, Sameh and Tai, Charlie and Shrivastava, Anshumali},
  journal={arXiv preprint arXiv:1903.03129},
  year={2019}
}


@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  booktitle = {The International Conference on Machine Learning ({ICML})},
  year={2020}
}

@inproceedings{yen2018loss,
  title={Loss decomposition for fast learning in large output spaces},
  author={Yen, Ian En-Hsu and Kale, Satyen and Yu, Felix and Holtmann-Rice, Daniel and Kumar, Sanjiv and Ravikumar, Pradeep},
  booktitle={International Conference on Machine Learning},
  pages={5640--5649},
  year={2018},
  organization={PMLR}
}

@inproceedings{dzps19,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={ICLR},
  publisher={\url{https://arxiv.org/pdf/1810.02054}},
  year={2019}
}

@article{DLRM19,
  author    = {Maxim Naumov and Dheevatsa Mudigere and Hao{-}Jun Michael Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole{-}Jean Wu and Alisson G. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and Ilia Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and Misha Smelyanskiy},
  title     = {Deep Learning Recommendation Model for Personalization and Recommendation Systems},
  journal   = {CoRR},
  volume    = {abs/1906.00091},
  year      = {2019},
  url       = {https://arxiv.org/abs/1906.00091},
}

@inproceedings{dai2019transformer,
  title={Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@article{zaheer2020bigbird,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@inproceedings{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NACCL)},
  year={2019}
}

@inproceedings{yuan2021tokens,
  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={558--567},
  year={2021}
}

@article{touvron2021resmlp,
  title={Resmlp: Feedforward networks for image classification with data-efficient training},
  author={Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and El-Nouby, Alaaeldin and Grave, Edouard and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2105.03404},
  year={2021}
}

@article{liu2021pay,
  title={Pay Attention to MLPs},
  author={Liu, Hanxiao and Dai, Zihang and So, David R and Le, Quoc V},
  journal={arXiv preprint arXiv:2105.08050},
  year={2021}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{sy19,
	title={Quadratic suffices for over-parametrization via matrix chernoff bound},
	author={Song, Zhao and Yang, Xin},
	journal={arXiv preprint arXiv:1906.03593},
	year={2019}
}

@inproceedings{bpsw21,
  title={Training (Overparametrized) Neural Networks in Near-Linear Time},
  author={Brand, Jan van den and Peng, Binghui and Song, Zhao and Weinstein, Omri},
  journal={arXiv preprint arXiv:2006.11648},
  booktitle={ITCS},
  year={2021}
}


@inproceedings{lsswy20,
  title={Generalized Leverage Score Sampling for Neural Networks},
  author={Lee, Jason D and Shen, Ruoqi and Song, Zhao and Wang, Mengdi and Yu, Zheng},
  booktitle={NeurIPS},
  journal={arXiv preprint arXiv:2009.09829},
  year={2020}
}

@article{yz05,
  title={Fast sparse matrix multiplication},
  author={Yuster, Raphael and Zwick, Uri},
  journal={ACM Transactions On Algorithms (TALG)},
  volume={1},
  number={1},
  pages={2--13},
  year={2005},
  publisher={ACM New York, NY, USA}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{lou2021sparse,
  title={Sparse-MLP: A Fully-MLP Architecture with Conditional Computation},
  author={Lou, Yuxuan and Xue, Fuzhao and Zheng, Zangwei and You, Yang},
  journal={arXiv preprint arXiv:2109.02008},
  year={2021}
}

@article{han2016eie,
  title={EIE: Efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={243--254},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural networks},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William J},
  journal={arXiv preprint arXiv:1506.02626},
  year={2015}
}

@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}

@inproceedings{NIPS2017_a51fb975,
 author = {Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Runtime Neural Pruning},
 volume = {30},
 year = {2017}
}

@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno Jialin},
  journal={arXiv preprint arXiv:1705.07565},
  year={2017}
}

@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander M},
  journal={arXiv preprint arXiv:2005.07683},
  year={2020}
}

@article{lagunas2021block,
  title={Block Pruning For Faster Transformers},
  author={Lagunas, Fran{\c{c}}ois and Charlaix, Ella and Sanh, Victor and Rush, Alexander M},
  journal={arXiv preprint arXiv:2109.04838},
  year={2021}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

@article{morcos2019one,
  title={One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
  author={Morcos, Ari S and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  journal={arXiv preprint arXiv:1906.02773},
  year={2019}
}

@article{orseau2020logarithmic,
  title={Logarithmic pruning is all you need},
  author={Orseau, Laurent and Hutter, Marcus and Rivasplata, Omar},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{blalock2020state,
  title={What is the state of neural network pruning?},
  author={Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
  journal={Proceedings of machine learning and systems},
  volume={2},
  pages={129--146},
  year={2020}
}

@article{frankle2019stabilizing,
  title={Stabilizing the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}

@inproceedings{malach2020proving,
  title={Proving the lottery ticket hypothesis: Pruning is all you need},
  author={Malach, Eran and Yehudai, Gilad and Shalev-Schwartz, Shai and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={6682--6691},
  year={2020},
  organization={PMLR}
}

@article{pensia2020optimal,
  title={Optimal lottery tickets via subsetsum: Logarithmic over-parameterization is sufficient},
  author={Pensia, Ankit and Rajput, Shashank and Nagle, Alliot and Vishwakarma, Harit and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2006.07990},
  year={2020}
}

@inproceedings{liu2020finding,
  title={Finding trainable sparse networks through neural tangent transfer},
  author={Liu, Tianlin and Zenke, Friedemann},
  booktitle={International Conference on Machine Learning},
  pages={6336--6347},
  year={2020},
  organization={PMLR}
}

@article{wang2020picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  journal={arXiv preprint arXiv:2002.07376},
  year={2020}
}

@article{paganini2020iterative,
  title={On iterative neural network pruning, reinitialization, and the similarity of masks},
  author={Paganini, Michela and Forde, Jessica},
  journal={arXiv preprint arXiv:2001.05050},
  year={2020}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
  journal={arXiv preprint arXiv:2006.05467},
  year={2020}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@inproceedings{fkt15,
  title={Variable selection is hard},
  author={Foster, Dean and Karloff, Howard and Thaler, Justin},
  booktitle={Conference on Learning Theory},
  pages={696--709},
  year={2015},
  organization={PMLR}
}

@inproceedings{rsw16,
  title={Weighted low rank approximations with provable guarantees},
  author={Razenshteyn, Ilya and Song, Zhao and Woodruff, David P},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={250--263},
  year={2016}
}



@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8572--8583},
  year={2019}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1904.11955},
  year={2019}
}


@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks. arxiv e-prints, art},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}

@article{hayou2019training,
  title={Training dynamics of deep networks using stochastic gradient descent via neural tangent kernel},
  author={Hayou, Soufiane and Doucet, Arnaud and Rousseau, Judith},
  journal={arXiv preprint arXiv:1905.13654},
  year={2019}
}



@inproceedings{als19_rnn,
  title={On the convergence rate of training recurrent neural networks},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={NeurIPS},
  year={2019}
}
@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{os20,
  title={Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={84--105},
  year={2020},
  publisher={IEEE}
}


%%%allen2019convergence
@inproceedings{als19_dnn,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@inproceedings{cao2020generalization,
  title={Generalization error bounds of gradient descent for learning over-parameterized deep relu networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3349--3356},
  year={2020}
}


@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@inproceedings{all19,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in neural information processing systems},
  pages={6155--6166},
  year={2019}
}

@article{spigler2020asymptotic,
  title={Asymptotic learning curves of kernel methods: empirical data versus teacher--student paradigm},
  author={Spigler, Stefano and Geiger, Mario and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={12},
  pages={124001},
  year={2020},
  publisher={IOP Publishing}
}

@inproceedings{ll18,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={NeurIPS},
  year={2018}
}

@article{li2019enhanced,
  title={Enhanced convolutional neural tangent kernels},
  author={Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S and Hu, Wei and Salakhutdinov, Ruslan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:1911.00809},
  year={2019}
}

@article{nakkiran2019deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1912.02292},
  year={2019}
}

@article{d2020triple,
  title={Triple descent and the two kinds of overfitting: Where \& why do they appear?},
  author={d'Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio},
  journal={arXiv preprint arXiv:2006.03509},
  year={2020}
}

@inproceedings{sagawa2020investigation,
  title={An investigation of why overparameterization exacerbates spurious correlations},
  author={Sagawa, Shiori and Raghunathan, Aditi and Koh, Pang Wei and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={8346--8356},
  year={2020},
  organization={PMLR}
}

@inproceedings{yang2020rethinking,
  title={Rethinking bias-variance trade-off for generalization of neural networks},
  author={Yang, Zitong and Yu, Yaodong and You, Chong and Steinhardt, Jacob and Ma, Yi},
  booktitle={International Conference on Machine Learning},
  pages={10767--10777},
  year={2020},
  organization={PMLR}
}

@inproceedings{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018},
  organization={PMLR}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zhang2021cpm,
  title={CPM: A large-scale generative Chinese pre-trained language model},
  author={Zhang, Zhengyan and Han, Xu and Zhou, Hao and Ke, Pei and Gu, Yuxian and Ye, Deming and Qin, Yujia and Su, Yusheng and Ji, Haozhe and Guan, Jian and others},
  journal={AI Open},
  volume={2},
  pages={93--99},
  year={2021},
  publisher={Elsevier}
}

@article{naumov2019deep,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{bommasani2021opportunities,
  title={On the Opportunities and Risks of Foundation Models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{zhai2021scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.04560},
  year={2021}
}

@article{cgh+19,
  title={Gram-gauss-newton method: Learning overparameterized neural networks for regression problems},
  author={Cai, Tianle and Gao, Ruiqi and Hou, Jikai and Chen, Siyu and Wang, Dong and He, Di and Zhang, Zhihua and Wang, Liwei},
  journal={arXiv preprint arXiv:1905.11675},
  year={2019}
}

@article{mozer1989using,
  title={Using relevance to reduce network size automatically},
  author={Mozer, Michael C and Smolensky, Paul},
  journal={Connection Science},
  volume={1},
  number={1},
  pages={3--16},
  year={1989},
  publisher={Taylor \& Francis}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@inproceedings{adh+19,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@article{lee2020generalized,
  title={Generalized leverage score sampling for neural networks},
  author={Lee, Jason D and Shen, Ruoqi and Song, Zhao and Wang, Mengdi and Yu, Zheng},
  journal={arXiv preprint arXiv:2009.09829},
  year={2020}
}

@article{udell2019big,
  title={Why are big data matrices approximately low rank?},
  author={Udell, Madeleine and Townsend, Alex},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={1},
  number={1},
  pages={144--160},
  year={2019},
  publisher={SIAM}
}

@article{wu2020lite,
  title={Lite transformer with long-short range attention},
  author={Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  journal={arXiv preprint arXiv:2004.11886},
  year={2020}
}

@book{cook2012cuda,
author = {Cook, Shane},
title = {CUDA Programming: A Developer’s Guide to Parallel Computing with GPUs},
year = {2012},
isbn = {9780124159334},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st}
}

@article{lin2020dynamic,
  title={Dynamic model pruning with feedback},
  author={Lin, Tao and Stich, Sebastian U and Barba, Luis and Dmitriev, Daniil and Jaggi, Martin},
  journal={arXiv preprint arXiv:2006.07253},
  year={2020}
}

@article{gray2017gpu,
  title={{GPU} kernels for block-sparse weights},
  author={Gray, Scott and Radford, Alec and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1711.09224},
  volume={3},
  year={2017}
}

@inproceedings{jing2017tunable,
  title={Tunable efficient unitary neural networks ({EUNN}) and their application to {RNN}s},
  author={Jing, Li and Shen, Yichen and Dubcek, Tena and Peurifoy, John and Skirlo, Scott and LeCun, Yann and Tegmark, Max and Soljaci{\'c}, Marin},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1733--1741},
  year={2017},
  organization={JMLR. org}
}

@article{li2015butterfly,
  title={Butterfly factorization},
  author={Li, Yingzhou and Yang, Haizhao and Martin, Eileen R. and Ho, Kenneth L. and Ying, Lexing},
  journal={Multiscale Modeling \& Simulation},
  volume={13},
  number={2},
  pages={714--732},
  year={2015},
  publisher={SIAM}
}

@article{mathieu2014fast,
  title={Fast approximation of rotations and {H}essians matrices},
  author={Mathieu, Michael and LeCun, Yann},
  journal={arXiv preprint arXiv:1404.7195},
  year={2014}
}

@inproceedings{choromanski2019unifying,
  title={Unifying Orthogonal {M}onte {C}arlo Methods},
  author={Choromanski, Krzysztof and Rowland, Mark and Chen, Wenyu and Weller, Adrian},
  booktitle={International Conference on Machine Learning},
  pages={1203--1212},
  year={2019}
}

@incollection{munkhoeva2018quadrature,
title = {Quadrature-based features for kernel approximation},
author = {Munkhoeva, Marina and Kapushev, Yermek and Burnaev, Evgeny and Oseledets, Ivan},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {9165--9174},
year = {2018},
publisher = {Curran Associates, Inc.}
}

@inproceedings{vahid2020butterfly,
  title={Butterfly transform: An efficient fft based neural architecture design},
  author={Vahid, Keivan Alizadeh and Prabhu, Anish and Farhadi, Ali and Rastegari, Mohammad},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={12021--12030},
  year={2020},
  organization={IEEE}
}

@article{alon2009perturbed,
  title={Perturbed identity matrices have high rank: Proof and applications},
  author={Alon, Noga},
  journal={Combinatorics, Probability and Computing},
  volume={18},
  number={1-2},
  pages={3--15},
  year={2009},
  publisher={Cambridge University Press}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{collins1971diagonal,
  title={The diagonal decomposition technique applied to the dynamic programming solution of elliptic partial differential equations},
  author={Collins, DC and Angel, ES},
  journal={Journal of Mathematical Analysis and Applications},
  volume={33},
  number={3},
  pages={467--481},
  year={1971},
  publisher={Academic Press}
}

@article{taylor2003penn,
  title={The Penn treebank: an overview},
  author={Taylor, Ann and Marcus, Mitchell and Santorini, Beatrice},
  journal={Treebanks},
  pages={5--22},
  year={2003},
  publisher={Springer}
}

% ================== MRI (start) ==================
@article{lustig2007sparse,
  title={Sparse {MRI}: The application of compressed sensing for rapid MR imaging},
  author={Lustig, Michael and Donoho, David and Pauly, John M},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={58},
  number={6},
  pages={1182--1195},
  year={2007},
  publisher={Wiley Online Library}
}

@article{pruessmann1999sense,
  title={SENSE: sensitivity encoding for fast {MRI}},
  author={Pruessmann, Klaas P and Weiger, Markus and Scheidegger, Markus B and Boesiger, Peter},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={42},
  number={5},
  pages={952--962},
  year={1999},
  publisher={Wiley Online Library}
}

@article{ong2016beyond,
  title={Beyond low rank+ sparse: Multiscale low rank matrix decomposition},
  author={Ong, Frank and Lustig, Michael},
  journal={IEEE journal of selected topics in signal processing},
  volume={10},
  number={4},
  pages={672--687},
  year={2016},
  publisher={IEEE}
}

@article{ravishankar2017low,
  title={Low-rank and adaptive sparse signal (LASSI) models for highly accelerated dynamic imaging},
  author={Ravishankar, Saiprasad and Moore, Brian E and Nadakuditi, Raj Rao and Fessler, Jeffrey A},
  journal={IEEE transactions on medical imaging},
  volume={36},
  number={5},
  pages={1116--1128},
  year={2017},
  publisher={IEEE}
}

@article{haldar2013low,
  title={Low-rank modeling of local $ k $-space neighborhoods (LORAKS) for constrained {MRI}},
  author={Haldar, Justin P},
  journal={IEEE transactions on medical imaging},
  volume={33},
  number={3},
  pages={668--681},
  year={2013},
  publisher={IEEE}
}

@article{griswold2002generalized,
  title={Generalized autocalibrating partially parallel acquisitions (GRAPPA)},
  author={Griswold, Mark A and Jakob, Peter M and Heidemann, Robin M and Nittka, Mathias and Jellus, Vladimir and Wang, Jianmin and Kiefer, Berthold and Haase, Axel},
  journal={Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={47},
  number={6},
  pages={1202--1210},
  year={2002},
  publisher={Wiley Online Library}
}

@article{hammernik2018learning,
  title={Learning a variational network for reconstruction of accelerated {MRI} data},
  author={Hammernik, Kerstin and Klatzer, Teresa and Kobler, Erich and Recht, Michael P and Sodickson, Daniel K and Pock, Thomas and Knoll, Florian},
  journal={Magnetic resonance in medicine},
  volume={79},
  number={6},
  pages={3055--3071},
  year={2018},
  publisher={Wiley Online Library}
}

@article{sandino2020compressed,
  title={Compressed sensing: From research to clinical practice with deep neural networks: Shortening scan times for magnetic resonance imaging},
  author={Sandino, Christopher M and Cheng, Joseph Y and Chen, Feiyu and Mardani, Morteza and Pauly, John M and Vasanawala, Shreyas S},
  journal={IEEE signal processing magazine},
  volume={37},
  number={1},
  pages={117--127},
  year={2020},
  publisher={IEEE}
}

@article{knoll2020deep,
  title={Deep-learning methods for parallel magnetic resonance imaging reconstruction: A survey of the current approaches, trends, and issues},
  author={Knoll, Florian and Hammernik, Kerstin and Zhang, Chi and Moeller, Steen and Pock, Thomas and Sodickson, Daniel K and Akcakaya, Mehmet},
  journal={IEEE signal processing magazine},
  volume={37},
  number={1},
  pages={128--140},
  year={2020},
  publisher={IEEE}
}

@article{chaudhari2020prospective,
  title={Prospective deployment of deep learning in {MRI}: A framework for important considerations, challenges, and recommendations for best practices},
  author={Chaudhari, Akshay S and Sandino, Christopher M and Cole, Elizabeth K and Larson, David B and Gold, Garry E and Vasanawala, Shreyas S and Lungren, Matthew P and Hargreaves, Brian A and Langlotz, Curtis P},
  journal={Journal of Magnetic Resonance Imaging},
  year={2020},
  publisher={Wiley Online Library}
}

@article{darestani2021accelerated,
  title={Accelerated {MRI} with un-trained neural networks},
  author={Darestani, Mohammad Zalbagi and Heckel, Reinhard},
  journal={IEEE Transactions on Computational Imaging},
  volume={7},
  pages={724--733},
  year={2021},
  publisher={IEEE}
}

@article{darestani2021measuring,
  title={Measuring Robustness in Deep Learning Based Compressive Sensing},
  author={Darestani, Mohammad Zalbagi and Chaudhari, Akshay and Heckel, Reinhard},
  journal={arXiv preprint arXiv:2102.06103},
  year={2021}
}

@article{cole2020unsupervised,
  title={Unsupervised {MRI} reconstruction with generative adversarial networks},
  author={Cole, Elizabeth K and Pauly, John M and Vasanawala, Shreyas S and Ong, Frank},
  journal={arXiv preprint arXiv:2008.13065},
  year={2020}
}

@article{mardani2018deep,
  title={Deep generative adversarial neural networks for compressive sensing {MRI}},
  author={Mardani, Morteza and Gong, Enhao and Cheng, Joseph Y and Vasanawala, Shreyas S and Zaharchuk, Greg and Xing, Lei and Pauly, John M},
  journal={IEEE transactions on medical imaging},
  volume={38},
  number={1},
  pages={167--179},
  year={2018},
  publisher={IEEE}
}

@inproceedings{yaman2020self,
  title={Self-supervised physics-based deep learning {MRI} reconstruction without fully-sampled data},
  author={Yaman, Burhaneddin and Hosseini, Seyed Amir Hossein and Moeller, Steen and Ellermann, Jutta and U{\u{g}}urbil, K{\^a}mil and Ak{\c{c}}akaya, Mehmet},
  booktitle={2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},
  pages={921--925},
  year={2020},
  organization={IEEE}
}

@article{lahiri2021blind,
  title={Blind Primed Supervised (BLIPS) Learning for MR Image Reconstruction},
  author={Lahiri, Anish and Wang, Guanhua and Ravishankar, Saiprasad and Fessler, Jeffrey A},
  journal={arXiv preprint arXiv:2104.05028},
  year={2021}
}

@inproceedings{desai2021skm,
  title={{SKM-TEA}: A dataset for accelerated {MRI} reconstruction with dense image labels for quantitative clinical evaluation},
  author={Desai, Arjun D and Schmidt, Andrew M and Rubin, Elka B and Sandino, Christopher Michael and Black, Marianne Susan and Mazzoli, Valentina and Stevens, Kathryn J and Boutin, Robert and Re, Christopher and Gold, Garry E and others},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}

@article{desai2021noise2recon,
  title={Noise2recon: A semi-supervised framework for joint {MRI} reconstruction and denoising},
  author={Desai, Arjun D and Ozturkler, Batu M and Sandino, Christopher M and Vasanawala, Shreyas and Hargreaves, Brian A and Re, Christopher M and Pauly, John M and Chaudhari, Akshay S},
  journal={arXiv preprint arXiv:2110.00075},
  year={2021}
}

@article{desai2021vortex,
  title={VORTEX: Physics-Driven Data Augmentations for Consistency Training for Robust Accelerated {MRI} Reconstruction},
  author={Desai, Arjun D and Gunel, Beliz and Ozturkler, Batu M and Beg, Harris and Vasanawala, Shreyas and Hargreaves, Brian A and R{\'e}, Christopher and Pauly, John M and Chaudhari, Akshay S},
  journal={arXiv preprint arXiv:2111.02549},
  year={2021}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

% ================== MRI (end) ==================

@article{gray2006toeplitz,
  title={Toeplitz and circulant matrices: A review},
  author={Gray, Robert M.},
  journal={Foundations and Trends{\textregistered} in Communications and Information Theory},
  volume={2},
  number={3},
  pages={155--239},
  year={2006},
  publisher={Now Publishers, Inc.}
}

@incollection{yu2016orthogonal,
title = {Orthogonal Random Features},
author = {Yu, Felix X. and Suresh, Ananda T. and Choromanski, Krzysztof M. and Holtmann-Rice, Daniel N. and Kumar, Sanjiv},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {1975--1983},
year = {2016},
publisher = {Curran Associates, Inc.}
}


@inproceedings{le2013fastfood,
  title={Fastfood-computing hilbert space expansions in loglinear time},
  author={Le, Quoc and Sarl{\'o}s, Tam{\'a}s and Smola, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={244--252},
  year={2013}
}

@inproceedings{moczulski2015acdc,
    author = "Moczulski, Marcin and Denil, Misha and Appleyard, Jeremy and de Freitas, Nando",
    title = "{ACDC: a structured efficient linear layer}",
    year = "2016",
    arxiv = "http://arxiv.org/abs/1511.05946",
    booktitle = "International Conference on Learning Representations",
    project = "parameter-prediction"
}

@article{driscoll1997fast,
  title={Fast discrete polynomial transforms with applications to data analysis for distance transitive graphs},
  author={Driscoll, James R and Healy Jr, Dennis M and Rockmore, Daniel N},
  journal={SIAM Journal on Computing},
  volume={26},
  number={4},
  pages={1066--1099},
  year={1997},
  publisher={SIAM}
}

@book{trefethen2000spectral,
  title={Spectral methods in MATLAB},
  author={Trefethen, Lloyd N},
  year={2000},
  publisher={SIAM}
}

@book{hsieh2003computed,
  title={Computed tomography: principles, design, artifacts, and recent advances},
  author={Hsieh, Jiang},
  volume={114},
  year={2003},
  publisher={SPIE press}
}

@article{geva2020transformer,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2012.14913},
  year={2020}
}

@article{eckart1936approximation,
  title={The approximation of one matrix by another of lower rank},
  author={Eckart, Carl and Young, Gale},
  journal={Psychometrika},
  volume={1},
  number={3},
  pages={211--218},
  year={1936},
  publisher={Springer}
}

@article{schonemann1966generalized,
  title={A generalized solution of the orthogonal procrustes problem},
  author={Sch{\"o}nemann, Peter H},
  journal={Psychometrika},
  volume={31},
  number={1},
  pages={1--10},
  year={1966},
  publisher={Springer}
}

@article{lin2021deformable,
  title={Deformable Butterfly: A Highly Structured and Sparse Linear Transform},
  author={Lin, Rui and Ran, Jie and Chiu, King Hung and Chesi, Graziano and Wong, Ngai},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{khalitov2021sparse,
  title={Sparse Factorization of Large Square Matrices},
  author={Khalitov, Ruslan and Yu, Tong and Cheng, Lei and Yang, Zhirong},
  journal={arXiv preprint arXiv:2109.08184},
  year={2021}
}

@article{le2016flexible,
  title={Flexible multilayer sparse approximations of matrices and applications},
  author={Le Magoarou, Luc and Gribonval, R{\'e}mi},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={10},
  number={4},
  pages={688--700},
  year={2016},
  publisher={IEEE}
}

@article{evci2019difficulty,
  title={The difficulty of training sparse neural networks},
  author={Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
  journal={arXiv preprint arXiv:1906.10732},
  year={2019}
}

@article{akema2020approximate,
title={Approximate Simultaneous Diagonalization of Matrices via Structured Low-Rank Approximation},
author={Riku Akema and Masao Yamagishi and Isao Yamada},
journal={arXiv preprint arXiv:2010.06305},
year={2020}
}

@article{gerstner1993numerical,
title={Numerical Methods for Simultaneous Diagonalization},
author={Angelika Bunse-Gerstner and Ralph Byers and Volker Mehrmann},
journal={SIAM Journal on Matrix Analysis and Applications},
year={1993},
}

@book{tewarson1973sparse,
  title={Sparse matrices},
  author={Tewarson, Reginald P},
  volume={69},
  year={1973},
  publisher={Academic Press New York}
}

@article{shoeybi2019megatron,
  title={Megatron-{LM}: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}

@article{eidelman1999new,
  title={On a new class of structured matrices},
  author={Eidelman, Y and Gohberg, I},
  journal={Integral Equations and Operator Theory},
  volume={34},
  number={3},
  pages={293--324},
  year={1999},
  publisher={Springer}
}

@MISC{Conrad_theminimal,
    author = {Keith Conrad},
    title = {THE MINIMAL POLYNOMIAL AND SOME APPLICATIONS},
    year = {}
}

@book{jurafsky2014speech,
  title={Speech and language processing},
  author={Jurafsky, Dan and Martin, James H.},
  volume={3},
  year={2014},
  publisher={Pearson London}
}

@inproceedings{gu2020hippo,
  title={HiPPO: Recurrent Memory with Optimal Polynomial Projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in neural information processing systems (NeurIPS)},
  year={2020}
}

@inproceedings{ailon2021sparse,
  title={Sparse linear networks with a fixed butterfly structure: theory and practice},
  author={Ailon, Nir and Leibovitch, Omer and Nair, Vineet},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1174--1184},
  year={2021},
  organization={PMLR}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{jayakumar2021top,
  title={Top-{KAST}: Top-{K} always sparse training},
  author={Jayakumar, Siddhant M and Pascanu, Razvan and Rae, Jack W and Osindero, Simon and Elsen, Erich},
  journal={arXiv preprint arXiv:2106.03517},
  year={2021}
}

@inproceedings{guo2020accelerating,
  title={Accelerating sparse dnn models without hardware-support via tile-wise sparsity},
  author={Guo, Cong and Hsueh, Bo Yang and Leng, Jingwen and Qiu, Yuxian and Guan, Yue and Wang, Zehuan and Jia, Xiaoying and Li, Xipeng and Guo, Minyi and Zhu, Yuhao},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2020},
  organization={IEEE}
}

@article{han2016dsd,
  title={Dsd: Dense-sparse-dense training for deep neural networks},
  author={Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Gong, Enhao and Tang, Shijian and Elsen, Erich and Vajda, Peter and Paluri, Manohar and Tran, John and others},
  journal={arXiv preprint arXiv:1607.04381},
  year={2016}
}

@article{peste2021ac,
  title={Ac/dc: Alternating compressed/decompressed training of deep neural networks},
  author={Peste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{mattson2020mlperf,
  title={Mlperf training benchmark},
  author={Mattson, Peter and Cheng, Christine and Diamos, Gregory and Coleman, Cody and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={336--349},
  year={2020}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@article{rabe2021self,
  title={Self-attention Does Not Need $ {O} (n^2) $ Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@unpublished{jia2021dissecting,
  title={Dissecting the {A}mpere {GPU} architecture via microbenchmarking},
  author={Jia, Zhe and Van Sandt, Peter},
  note={{GPU} Technology Conference},
  year={2021}
}

@inproceedings{woodruff2004optimal,
  title={Optimal space lower bounds for all frequency moments},
  author={Woodruff, David P},
  booktitle={SODA},
  volume={4},
  pages={167--175},
  year={2004},
  organization={Citeseer}
}

@inproceedings{wu2019pay,
  title={Pay less attention with lightweight and dynamic convolutions},
  author={Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N and Auli, Michael},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2019}
}

@inproceedings{choromanski2020rethinking,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Quincy and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{sukhbaatar2019adaptive,
  title={Adaptive attention span in transformers},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@inproceedings{rae2019compressive,
  title={Compressive Transformers for Long-Range Sequence Modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2020}
}

@article{likhosherstov2020sub,
  title={Sub-Linear Memory: How to Make Performers SLiM},
  author={Likhosherstov, Valerii and Choromanski, Krzysztof and Davis, Jared and Song, Xingyou and Weller, Adrian},
  journal={arXiv preprint arXiv:2012.11346},
  year={2020}
}

@inproceedings{lan2019albert,
  title={Albert: A lite {BEDRT} for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2020}
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@article{wang2022deepnet,
  title={DeepNet: Scaling Transformers to 1,000 Layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={arXiv preprint arXiv:2203.00555},
  year={2022}
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are {RNN}s: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@misc{nvidia2017nvidia,
  title={Nvidia {T}esla {V}100 {GPU} architecture},
  author={NVIDIA},
  year={2017},
  publisher={Aug}
}

@misc{nvidia2020nvidia,
  title={Nvidia {A}100 Tensor Core {GPU} Architecture},
  author={NVIDIA},
  year={2020}
}

@misc{nvidia2022nvidia,
  title={Nvidia {H}100 Tensor Core {GPU} Architecture},
  author={NVIDIA},
  year={2022}
}

@article{ivanov2021data,
  title={Data movement is all you need: A case study on optimizing transformers},
  author={Ivanov, Andrei and Dryden, Nikoli and Ben-Nun, Tal and Li, Shigang and Hoefler, Torsten},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={711--732},
  year={2021}
}

@article{gray1997data,
  title={Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals},
  author={Gray, Jim and Chaudhuri, Surajit and Bosworth, Adam and Layman, Andrew and Reichart, Don and Venkatrao, Murali and Pellow, Frank and Pirahesh, Hamid},
  journal={Data mining and knowledge discovery},
  volume={1},
  number={1},
  pages={29--53},
  year={1997},
  publisher={Springer}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@article{jia2019dissecting,
  title={Dissecting the graphcore {IPU} architecture via microbenchmarking},
  author={Jia, Zhe and Tillman, Blake and Maggioni, Marco and Scarpazza, Daniele Paolo},
  journal={arXiv preprint arXiv:1912.03413},
  year={2019}
}

@article{li2020deep,
  title={The deep learning compiler: A comprehensive survey},
  author={Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={32},
  number={3},
  pages={708--727},
  year={2020},
  publisher={IEEE}
}

@article{sabne2020xla,
  title={{XLA}: Compiling Machine Learning for Peak Performance},
  author={Sabne, Amit},
  year={2020}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@book{flum2006parameterized,
  title={Parameterized Complexity Theory},
  author={Flum, J{\"o}rg and Grohe, Martin},
  year={2006},
  publisher={Springer}
}

@article{hua2022transformer,
  title={Transformer quality in linear time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc V},
  journal={arXiv preprint arXiv:2202.10447},
  year={2022}
}

@article{ren2021combiner,
  title={Combiner: Full attention transformer with sparse computation cost},
  author={Ren, Hongyu and Dai, Hanjun and Dai, Zihang and Yang, Mengjiao and Leskovec, Jure and Schuurmans, Dale and Dai, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zhu2021long,
  title={Long-short transformer: Efficient transformers for language and vision},
  author={Zhu, Chen and Ping, Wei and Xiao, Chaowei and Shoeybi, Mohammad and Goldstein, Tom and Anandkumar, Anima and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{ma2021luna,
  title={Luna: Linear unified nested attention},
  author={Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zhai2021attention,
  title={An attention free transformer},
  author={Zhai, Shuangfei and Talbott, Walter and Srivastava, Nitish and Huang, Chen and Goh, Hanlin and Zhang, Ruixiang and Susskind, Josh},
  journal={arXiv preprint arXiv:2105.14103},
  year={2021}
}

@inproceedings{dao2022monarch,
  title={Monarch: Expressive Structured Matrices for Efficient and Accurate Training},
  author={Dao, Tri and Chen, Beidi and Sohoni, Nimit and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022}
}

@article{gu2021combining,
  title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers},
  author={Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{goel2022s,
  title={It's Raw! Audio Generation with State-Space Models},
  author={Goel, Karan and Gu, Albert and Donahue, Chris and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022}
}

@article{bello2021lambdanetworks,
  title={Lambda{N}etworks: Modeling long-range interactions without attention},
  author={Bello, Irwan},
  journal={arXiv preprint arXiv:2102.08602},
  year={2021}
}

@book{griewank2008evaluating,
  title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  year={2008},
  publisher={SIAM}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{daras2020smyrf,
  title={Smyrf-efficient attention using asymmetric clustering},
  author={Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6476--6489},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{milakov2018online,
  title={Online normalizer calculation for softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={arXiv preprint arXiv:1805.02867},
  year={2018}
}

@article{feydy2020fast,
    title={Fast geometric learning with symbolic matrices},
    author={Feydy, Jean and Glaun{\`e}s, Joan and Charlier, Benjamin and Bronstein, Michael},
    journal={Advances in Neural Information Processing Systems},
    volume={33},
    year={2020}
}

@article{charlier2021kernel,
  author  = {Benjamin Charlier and Jean Feydy and Joan Alexis Glaunès and François-David Collin and Ghislain Durif},
  title   = {Kernel Operations on the GPU, with Autodiff, without Memory Overflows},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {74},
  pages   = {1-6},
  url     = {http://jmlr.org/papers/v22/20-275.html}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@misc{mlperf11,
  title={MLPerf 1.1 Training Results},
  author={MLCommons},
  year={2021},
  url={https://mlcommons.org/en/training-normal-11/}
}

@inproceedings{chen2018tvm,
  title={$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={578--594},
  year={2018}
}

@inproceedings{tillet2019triton,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Tillet, Philippe and Kung, Hsiang-Tsung and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={10--19},
  year={2019}
}

@Misc{xFormers2022,
  author =       {Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza},
  title =        {xFormers: A modular and hackable Transformer modelling library},
  howpublished = {\url{https://github.com/facebookresearch/xformers}},
  year =         {2022}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@article{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{OpenAI2023GPT4TR,
  added-at = {2023-07-01T22:03:32.000+0200},
  author = {OpenAI},
  journal = {ArXiv},
  keywords = {machine-learning GPT-4 OpenAI AI deep-learning},
  timestamp = {2023-07-01T23:51:55.000+0200},
  title = {GPT-4 Technical Report},
  volume = {abs/2303.08774},
  year = 2023
}
