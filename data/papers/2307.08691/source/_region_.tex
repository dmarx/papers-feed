\message{ !name(../flash2.tex)}\documentclass{article}

\usepackage{etoolbox}
\newtoggle{arxiv}
% \togglefalse{arxiv}
\toggletrue{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath,amsthm,amssymb}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{subcaption}
\usepackage{multirow}

\usepackage{xspace}
\usepackage{wrapfig}

% the X mark
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}

\usepackage[capitalise]{cleveref}  % Otherwise I get an error saying cleveref must be loaded after hyperref
\usepackage{comment}
\usepackage[inline]{enumitem}

\usepackage{graphicx}
\usepackage{grffile}
\usepackage{color}

% https://tex.stackexchange.com/questions/26637/how-do-you-get-mathbb1-to-work-characteristic-function-of-a-set
\usepackage{newtxmath}

\iftoggle{arxiv}{
\usepackage[numbers,sort]{natbib}
}{}

\usepackage{import}
% https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{.}

\newtoggle{todo}
\toggletrue{todo}
% \togglefalse{todo} % comment this line to show comments
\newcommand{\todo}[1]{\iftoggle{todo}{\textcolor{cyan}{[TODO: #1]}}{}}

\newtoggle{comment}
% \toggletrue{comment}
\togglefalse{comment} % comment this line to show comments
\newcommand{\TD}[1]{\iftoggle{comment}{\textcolor{magenta}{[TD: #1]}}{}}

\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\|{#1}\right\|} % A norm with 1 argument
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\softmax}{\mathrm{softmax}}
\providecommand{\tr}{\mathop{\rm tr}}

\newcommand{\defeq}{:=}

\newcommand{\vQ}{\mathbf{Q}}
\newcommand{\vK}{\mathbf{K}}
\newcommand{\vV}{\mathbf{V}}
\newcommand{\vdQ}{\mathbf{dQ}}
\newcommand{\vdK}{\mathbf{dK}}
\newcommand{\vdV}{\mathbf{dV}}
\newcommand{\vS}{\mathbf{S}}
\newcommand{\vdS}{\mathbf{dS}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\vdP}{\mathbf{dP}}
\newcommand{\vU}{\mathbf{U}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vT}{\mathbf{T}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vO}{\mathbf{O}}
\newcommand{\vdO}{\mathbf{dO}}
\newcommand{\vM}{\mathbf{M}}
\newcommand{\vZ}{\mathbf{Z}}

\newcommand{\sysnameone}{\textsc{FlashAttention}\xspace}
\newcommand{\sysname}{\textsc{FlashAttention-2}\xspace}


\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{claim}
\newtheorem{example}{Example}
\newtheorem{proposition}[theorem]{Proposition}

\iftoggle{arxiv}{
  \setlength{\textwidth}{6.5in}
  \setlength{\textheight}{9in}
  \setlength{\oddsidemargin}{0in}
  \setlength{\evensidemargin}{0in}
  \setlength{\topmargin}{-0.5in}
  \newlength{\defbaselineskip}
  \setlength{\defbaselineskip}{\baselineskip}
  \setlength{\marginparwidth}{0.8in}
}{
% --------------------
% Space-saving options
% comment now for longer appendix
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*1}{*0}
\titlespacing{\subsection}{0pt}{*1.5}{*0}

\usepackage[subtle, mathdisplays=tight, charwidths=tight, leading=normal]{savetrees}
% \usepackage[subtle, mathdisplays=tight, charwidths=normal, leading=normal]{savetrees}
% \usepackage[subtle]{savetrees}

% Can always cheat a bit on the margins
% \addtolength\titlebox{-0.3in}
% \addtolength\columnsep{-0.15in}
% \addtolength\textwidth{0.15in}
% \addtolength\textheight{0.15in}
\addtolength\textfloatsep{-0.5em}
\addtolength\intextsep{-0.2em}

% https://tex.stackexchange.com/questions/146890/how-to-apply-looseness-1-to-all-the-paragraphs
% Does savetrees do this already?
% \linepenalty=1000

\def\setstretch#1{\renewcommand{\baselinestretch}{#1}}
\setstretch{0.985}
\addtolength{\parskip}{-1pt}

}

\title{\sysname: Faster Attention with Better Work Scheduling}

\iftoggle{arxiv}{
  \usepackage{authblk}
  \author[$\dagger$]{Tri Dao}
  \affil[$\dagger$]{Department of Computer Science, Princeton University}
  % \affil[ ]{{\texttt{\{trid}@cs.stanford.edu}}}
}{
}

\begin{document}

\message{ !name(src/background.tex) !offset(-152) }
\section{Background}
\label{sec:background}

We provide some background on the performance characteristics of common deep
learning operations on modern hardware (GPUs).
We also describe the standard implementation of attention, as well as \sysnameone.

\subsection{Hardware Performance}
\label{subsec:hardware}

We focus here on GPUs.
Performance on other hardware accelerators are similar~\citep{jouppi2017datacenter, jia2019dissecting}.

\textbf{GPU Memory Hierarchy.}
% The GPU consists of compute elements (e.g., floating
% point arithmetic units) and a memory hierarchy (\cref{fig:banner}).
% Most modern GPUs contain specialized units to accelerate matrix multiply in
% low-precision (e.g., Tensor Cores on Nvidia GPUs for FP16 matrix multiply).
The GPU memory hierarchy (\cref{fig:banner} left) comprises multiple forms of memory of different
sizes and speeds, with smaller memory being faster.
As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with
bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming
multiprocessors with
bandwidth estimated around 19TB/s~\citep{jia2018dissecting, jia2021dissecting}.
% The memory hierarchy comprise of high bandiwdth memory(HBM), cache, and on-chip
% SRAM (aka shared memory), ordered from larger to smaller size, and from slower
% to faster speed.
% As an example, the A100 GPU has 40-80GB of HBM with bandwidth 1.5-2.0TB/s, 40MB
% of L2 cache, and 164KB of on-chip SRAM per each of 108 streaming multiprocessor with
% bandwidth estimated around 19TB/s~\citep{jia2018dissecting, jia2021dissecting}.
% As the L2 cache is not controllable by the programmer, we focus on the HBM and SRAM.
The on-chip SRAM is an order of magnitude faster than HBM but many orders of
magnitude smaller in size.
As compute has gotten faster relative to memory speed~\citep{nvidia2017nvidia,nvidia2020nvidia,nvidia2022nvidia}, operations
are increasingly bottlenecked by memory (HBM) accesses.
Thus exploiting fast SRAM becomes more important.

\textbf{Execution Model.}
GPUs have a massive number of threads to execute an operation
(called a kernel).
Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.

\textbf{Performance characteristics.} Depending on the balance of computation and memory accesses, operations can be
classified as either compute-bound or memory-bound.
This is commonly measured by the \emph{arithmetic intensity}~\citep{williams2009roofline},
which is the number of arithmetic operations per byte of memory access.
\begin{enumerate}[itemsep=0.1pt,topsep=0pt,leftmargin=*]
  \item Compute-bound: the time taken by the operation is determined by how many
  arithmetic operations there are, while time accessing HBM
  is much smaller. Typical examples are matrix multiply with large inner
  dimension, and convolution with large number of channels.
  \item Memory-bound: the time taken by the operation is determined by the
  number of memory accesses, while time spent in computation is much smaller.
  Examples include most other operations:
  elementwise (e.g., activation, dropout), and reduction (e.g., sum,
  softmax, batch norm, layer norm).
%   Matrix multiply with small inner dimensions can also fall into this category.
\end{enumerate}

\textbf{Kernel fusion.}
The most common approach to accelerate memory-bound operations is
kernel fusion: if there are multiple operations applied to the same input,
the input can be loaded once from HBM, instead of multiple times for each operation.
% Compilers can automatically fuse many elementwise operations~\citep{li2020deep, paszke2019pytorch, sabne2020xla}, but
% fusing other types of operations (e.g., reductions like softmax) is more challenging.
Compilers can automatically fuse many elementwise operations~\citep{li2020deep, paszke2019pytorch, sabne2020xla}.
However, in the context of model training, the intermediate values still need
to be written to HBM to save for the backward pass, reducing the
effectiveness of naive kernel fusion.

\subsection{Standard Attention Implementation}
\label{subsec:standard_attn}

Given input sequences $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$ where $N$ is the sequence length and
$d$ is the head dimension, we want to compute the attention output $\vO \in \mathbb{R}^{N \times d}$:
\begin{equation*}
  \vS = \vQ \vK^\top \in \mathbb{R}^{N \times N}, \quad \vP = \softmax(\vS) \in \mathbb{R}^{N \times N}, \quad \vO = \vP\vV \in \mathbb{R}^{N \times d},
\end{equation*}
where $\softmax$ is applied row-wise.\footnote{For clarity of exposition, we
  omit the scaling of $\vQ \vK^\top$ (typically by $1/\mathrm{d}$), and optionally
  elementwise masking on $\vS$ and/or dropout applied to $\vP$} For multi-head
attention (MHA), this same computation is performed in parallel across many
heads, and parallel over the batch dimension (number of input sequences in a
batch).

The backward pass of attention proceeds as follows.
Let $\vdO \in \mathbb{R}^{N \times d}$ be the gradient of $\vO$ with respect to some loss
function. Then by the chain rule (aka backpropagation):
\begin{align*}
  \vdV &= \vP^\top \vdO \in \mathbb{R}^{N \times d} \\
  \vdP &= \vdO \vV^\top \in \mathbb{R}^{N \times N} \\
\end{align*}

Standard attention implementations materialize the matrices $\vS$ and $\vP$ to
HBM, which takes $O(N^2)$ memory.
Often $N \gg d$ (typically $N$ is on the order of 1k--8k and d is around 64--128).
The standard attention implementation (1) calls the matrix multiply (GEMM)
subroutine to multiply $\vS = \vQ \vK^\top$, writes the result to HBM, then (2) loads
$\S$ from HBM to compute softmax and write the result $\vP$ to HBM, and finally
(3) calls matrix multiply to get $\vO = \vP \vV$.
As most of the operations are bounded by memory bandwidth, the large number of
memory accesses translates to slow wall-clock time.
Moreover, the required memory is $O(N^2)$ due to having to store

\subsection{\sysnameone}
\label{subsec:flashv1}

To speed up attention

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../flash2"
%%% End:

\message{ !name(../flash2.tex) !offset(-59) }

\end{document}