
\section{Conclusion}
We proposed a scalable approach to finetune large language models to follow instructions. Our method leverages large amounts of unlabeled data by developing an iterative self-training algorithm that we dub instruction backtranslation. Our method uses the model itself to both augment  and curate
high quality training examples to improve its own performance. On the Alpaca leaderboard, our finetuned models outperform all other non-distilled instruction-following models, while using fewer human annotated examples.
Future work should scale this method further by considering larger unlabeled corpora, which our analysis suggests should  yield further gains.