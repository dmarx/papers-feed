\section{Introduction}


Aligning large language models (LLMs) to perform instruction following typically requires finetuning on large amounts of human-annotated instructions or preferences~\citep{ouyang2022training,touvron2023llama, bai2022training}  or distilling outputs from more powerful models~\citep{wang2022self,honovich2022unnatural,alpaca,vicuna2023,peng2023instruction,xu2023wizardlm}.
Recent work highlights the importance of human-annotation data quality~\citep{zhou2023lima,kopf2023openassistant}. However, annotating instruction following datasets with such quality is hard to scale. 


\if 0
Aligning large language models (LLMs) to perform generic instruction following typically requires finetuning on large amounts of human-annotated instructions or preferences~\citep{ouyang2022training,touvron2023llama, bai2022training} or using a stronger LLM in data creation (e.g. via knowledge distillation) or curation~\citep{wang2022self,honovich2022unnatural,alpaca,vicuna2023,peng2023instruction,xu2023wizardlm}.
 
Recent work on instruction finetuning highlights the importance of data quality~\cite{zhou2023lima,kopf2023openassistant}. However, handcrafting instruction following datasets is hard to scale. 
\fi

In this work, we instead leverage large amounts of \emph{unlabelled} data to create a high quality instruction tuning dataset by developing an iterative self-training algorithm. The method uses the model itself to both augment  and curate
high quality  training examples to improve its own performance. Our approach, named {\em instruction backtranslation}, is inspired by the classic {backtranslation} method from machine translation, in which human-written target sentences are automatically annotated with model-generated source sentences in another language \citep{sennrich2015improving}. 

Our method starts with a seed instruction following model and a web corpus. The model is first used to \textit{self-augment} its training set: for each web document, it creates an instruction following training example by predicting a  prompt (instruction) that would be correctly answered by (a portion of) that document. Directly training on such data (similarly to \cite{koksal2023longform}) gives poor results in our experiments, 
both because of the mixed quality of human written web text, and noise in the generated instructions. To remedy this, we show that the same seed model can be used to \textit{self-curate}
the set of newly created augmentation data by predicting their quality, and  can then be  self-trained on only the highest quality (instruction, output) pairs. 
The procedure is then iterated, using the improved model to better curate the instruction  data, and re-training to produce a better model.


Our resulting model, {\em Humpback}, outperforms
all other existing non-distilled models on the Alpaca leaderboard \citep{alpaca_eval}. 
Overall, instruction backtranslation is a scalable method for enabling language models to improve their own ability to follow instructions.



\if 0
\begin{itemize}
\item We propose a scalable approach to improve LLMs to follow instructions. At the core of our approach is to leverage an seed instruction following model to \textit{self-augment} and \textit{self-select} training data to perform self-training. Self-augmentation is performed by creating instruction following training examples from unlabeled data source such as a web corpus. The specific data augmentation steps include generating instructions given outputs, selecting high quality (instruction, output) pairs as self-training examples to improve the next iteration of intermediate instruction following models.


\item Our method demonstrate more efficient data scaling compared to other hand-crafted and distilled instruction following datasets.

\item Our method achieves high quality instruction following models evaluated on Alpaca leaderboard, outperforming all other models not relying on distillation data, and with the best data efficiency. 

\item We compare to existing LM alignment approach, and discuss the strengths and weakness of our approach.
\end{itemize}
\fi 