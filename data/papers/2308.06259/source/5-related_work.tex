\section{Related Work}

\paragraph{Instruction tuning for LLMs.} Our work shares the same goal as the broad category of efforts on finetuning large language models to follow instructions. Early work on instruction tuning mainly focused on NLP tasks, with the finding that finetuning with NLP datasets formatted as instruction-output pairs improves cross-task generalization \citep{wei2021finetuned,mishra2021cross,sanh2021multitask,wang2022super}. Recent work \citet{ouyang2022training} extends instruction tuning to a broader range of general tasks, especially incorporating instructions from users of language models.

\vspace{-2mm}
\paragraph{Instruction generation and curation.} A key challenge to enable LLMs to perform general instruction-following is gathering demonstration examples for finetuning. Existing high-quality instruction-following LLMs rely on human annotations in various steps, including writing instructions, writing model responses, providing preferences to indicate desired response, etc. Those instruction sets are often proprietary, one exception being the recent OpenAssistant datasets \citep{kopf2023openassistant}. Overall, the human annotation approach is difficult to scale since collecting annotations on a wide range of tasks is expensive, time consuming and requires expertise in different domains. 

Several works have explored using LLMs to generate instructions. Unnatural instructions prompts GPT-3 to generate more instructions given a few in-context seed instructions \citep{honovich2022unnatural}. Self-instruct \citep{wang2022self} uses the same approach to generate instructions, as well as outputs for those instructions. They further perform manually engineered filtering rules to remove low-quality instruction-output pairs. \citet{xu2023wizardlm} generates more complex instructions by creating variants of user instructions sent to ChatGPT.  

All these approaches use model-generated responses for training data. More similar to our method is the concurrent work  of \citet{koksal2023longform}, which takes human-written text as a natural response, and uses the LLM to generate the corresponding instruction conditioning on the response. A critical difference in our work is that we show that the self-curation step is vital to improve such a procedure.
A further difference is that they use distillation via an instruction tuned LLM (InstructGPT) to generate instructions, while our approach does not rely on distilling from a more powerful model in the loop, and is instead an instance of self-alignment. 
\vspace{-2mm}
\paragraph{Self-alignment.} Our work is  an instance of the growing body of work on \textit{self-alignment}, i.e. utilizing the model to improve itself and  align its response with desired behaviors such as model-written feedback, critique, explanations, etc. Differently to our work, many of these works either construct training data in an unsupervised way
\citep{sun2023principledriven,bai2022constitutional}, whereas we augment human-written web pages,
or they use the model to generate additional context to condition on at inference time to improve the output \citep{saunders2022self, zhang2023self,madaan2023self}.

\vspace{-2mm}

\paragraph{Data quality.}

Several approaches have shown that curating high-quality human-written data results in strong performance, for example PALMS \citep{solaiman2021process} and
LIMA \citep{zhou2023lima}. Instead of manually curating high-quality data, our work focus on selecting high-quality using the model itself. In concurrent work, \cite{chen2023alpagasus} also provides an algorithmic approach to select high quality data. They differ from our work in that they prompt a stronger model (ChatGPT) to score the quality of model generated responses from distillation, while this work scores the quality of human-written data as a response to a self-generated instruction. 



\paragraph{Distillation.} Most finetuned LLaMA models are based on knowledge distillation from ChatGPT or GPT-4, such as Alpaca \citep{alpaca}, Alpaca-GPT 4\citep{peng2023instruction}, Vicuna \citep{vicuna2023}, FalconInstruct \citep{falcon40b}, OpenChat \citep{openchat}, UltraChat \citep{ding2023enhancing}. 
Hence, these approaches require that you already have a strong model, but do not provide a recipe for building a strong model from scratch.
Drawbacks of these approaches are also discussed in \cite{gudibande2023false}.