\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Heslow, Launay, Malartic, Noune, Pannier, and Penedo]{falcon40b}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.
\newblock {Falcon-40B}: an open large language model with state-of-the-art performance.
\newblock 2023.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pp.\  7432--7439, 2020.

\bibitem[Caswell et~al.(2019)Caswell, Chelba, and Grangier]{caswell2019tagged}
Isaac Caswell, Ciprian Chelba, and David Grangier.
\newblock Tagged back-translation.
\newblock \emph{arXiv preprint arXiv:1906.06442}, 2019.

\bibitem[Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang, et~al.]{chen2023alpagasus}
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al.
\newblock Alpagasus: Training a better alpaca with fewer data.
\newblock \emph{arXiv preprint arXiv:2307.08701}, 2023.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2210, 2022.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}, 2023.

\bibitem[Ganguli et~al.(2023)Ganguli, Askell, Schiefer, Liao, Luko{\v{s}}i{\=u}t{\.e}, Chen, Goldie, Mirhoseini, Olsson, Hernandez, et~al.]{ganguli2023capacity}
Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil{\.e} Luko{\v{s}}i{\=u}t{\.e}, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et~al.
\newblock The capacity for moral self-correction in large language models.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2302, 2023.

\bibitem[Geng et~al.(2023)Geng, Gudibande, Liu, Wallace, Abbeel, Levine, and Song]{koala_blogpost_2023}
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song.
\newblock Koala: A dialogue model for academic research.
\newblock Blog post, April 2023.
\newblock URL \url{https://bair.berkeley.edu/blog/2023/04/03/koala/}.

\bibitem[Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel, Levine, and Song]{gudibande2023false}
Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song.
\newblock The false promise of imitating proprietary llms.
\newblock \emph{arXiv preprint arXiv:2305.15717}, 2023.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Honovich et~al.(2022)Honovich, Scialom, Levy, and Schick]{honovich2022unnatural}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
\newblock Unnatural instructions: Tuning language models with (almost) no human labor.
\newblock \emph{arXiv preprint arXiv:2212.09689}, 2022.

\bibitem[K{\"o}ksal et~al.(2023)K{\"o}ksal, Schick, Korhonen, and Sch{\"u}tze]{koksal2023longform}
Abdullatif K{\"o}ksal, Timo Schick, Anna Korhonen, and Hinrich Sch{\"u}tze.
\newblock Longform: Optimizing instruction tuning for long text generation with corpus extraction.
\newblock \emph{arXiv preprint arXiv:2304.08460}, 2023.

\bibitem[K{\"o}pf et~al.(2023)K{\"o}pf, Kilcher, von R{\"u}tte, Anagnostidis, Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi, et~al.]{kopf2023openassistant}
Andreas K{\"o}pf, Yannic Kilcher, Dimitri von R{\"u}tte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver Stanley, Rich{\'a}rd Nagyfi, et~al.
\newblock Openassistant conversations--democratizing large language model alignment.
\newblock \emph{arXiv preprint arXiv:2304.07327}, 2023.

\bibitem[Li et~al.(2023)Li, Zhang, Dubois, Taori, Gulrajani, Guestrin, Liang, and Hashimoto]{alpaca_eval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2023self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}, 2023.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[Mishra et~al.(2021)Mishra, Khashabi, Baral, and Hajishirzi]{mishra2021cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing instructions.
\newblock \emph{arXiv preprint arXiv:2104.08773}, 2021.

\bibitem[Nangia et~al.(2020)Nangia, Vania, Bhalerao, and Bowman]{nangia2020crows}
Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel~R Bowman.
\newblock Crows-pairs: A challenge dataset for measuring social biases in masked language models.
\newblock \emph{arXiv preprint arXiv:2010.00133}, 2020.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Overwijk et~al.(2022)Overwijk, Xiong, and Callan]{overwijk2022clueweb22}
Arnold Overwijk, Chenyan Xiong, and Jamie Callan.
\newblock Clueweb22: 10 billion web documents with rich information.
\newblock In \emph{Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pp.\  3360--3362, 2022.

\bibitem[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}, 2023.

\bibitem[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2021multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock \emph{arXiv preprint arXiv:2110.08207}, 2021.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock \emph{arXiv preprint arXiv:1904.09728}, 2019.

\bibitem[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Ouyang, Ward, and Leike]{saunders2022self}
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike.
\newblock Self-critiquing models for assisting human evaluators.
\newblock \emph{arXiv preprint arXiv:2206.05802}, 2022.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{sennrich2015improving}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Improving neural machine translation models with monolingual data.
\newblock \emph{arXiv preprint arXiv:1511.06709}, 2015.

\bibitem[Solaiman \& Dennison(2021)Solaiman and Dennison]{solaiman2021process}
Irene Solaiman and Christy Dennison.
\newblock Process for adapting language models to society (palms) with values-targeted datasets.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 5861--5873, 2021.

\bibitem[Sun et~al.(2023)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan]{sun2023principledriven}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.
\newblock Principle-driven self-alignment of language models from scratch with minimal human supervision, 2023.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023)Wang, Cheng, Yu, and Liu]{openchat}
Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu.
\newblock {OpenChat: Advancing Open-source Language Models with Imperfect Data}, 7 2023.
\newblock URL \url{https://github.com/imoneoi/openchat}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated instructions.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2212, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Mishra, Alipoormolabashi, Kordi, Mirzaei, Arunkumar, Ashok, Dhanasekaran, Naik, Stap, et~al.]{wang2022super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva Naik, David Stap, et~al.
\newblock Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.
\newblock \emph{arXiv preprint arXiv:2204.07705}, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}, 2021.

\bibitem[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex instructions.
\newblock \emph{arXiv preprint arXiv:2304.12244}, 2023.

\bibitem[Zhang \& Yang(2023)Zhang and Yang]{zhang2023self}
Xuanyu Zhang and Qing Yang.
\newblock Self-qa: Unsupervised knowledge guided language model alignment.
\newblock \emph{arXiv preprint arXiv:2305.11952}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, et~al.]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{arXiv preprint arXiv:2305.11206}, 2023.

\end{thebibliography}
