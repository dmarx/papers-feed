
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{textcomp} \usepackage{scalerel}
\usepackage{enumitem}
% \usepackage[outdir=./]{epstopdf}
\newenvironment{pcrfont}{\fontfamily{pcr}\selectfont}{\par}
\DeclareTextFontCommand{\textpcr}{\pcrfont}
\newenvironment{lmttfont}{\fontfamily{lmtt}\selectfont}{\par}


\title{Self-Alignment with Instruction Backtranslation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer\\
\textbf{Jason Weston} \& \textbf{Mike Lewis}  \\
Meta\\
\texttt{\{xianl,jase,mikelewis\}@meta.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named {\em instruction backtranslation}, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents ({\em self-augmentation}), and then  selecting high quality examples from among these candidates ({\em self-curation}).  This data is then used to finetune a stronger model.  Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.

\end{abstract}


\input{1-intro}
\input{2-method}
\input{3-results}

\input{5-related_work}
\input{6-conclusion}
\newpage


\bibliography{ref}
\bibliographystyle{iclr2024_conference}
\newpage
\input{4-limitations}
\input{7-appendix}

\end{document}

