\appendix


\section{Limitations}
\subsection{Bias}
Since the augmented data is sourced from a web corpus, one potential consequence is that the finetuned model could amplify biases from web data. We evaluate on the CrowS-Pairs dataset \cite{nangia2020crows} to measure the model's performance in recognizing potential bias. Specifically, we evaluate the accuracy in detecting biased statements in nine categories: gender, religion, race/color, sexual orientation, age, nationality, disability, physical appearance and socioeconomic status.  Compared to the base model, our model has improved accuracy in detecting biases as is summarized in Table \ref{tab:crows}. However, this does not mean our model is less likely to generate responses that contain biases.


\begin{table}[h]
      \caption{Accuracy of detecting various types of biases in the CrowS-Pair benchmark.
      \label{tab:crows}
      }
  \centering
  \begin{tabular}{lcc}
    \toprule
         &  Humpback & LLaMA \\
    \midrule
race-color & 60.27 & 48.64  \\
socioeconomic & 60.47 & 54.65 \\
gender & 45.42 & 50.0 \\
disability & 80.0 & 45.0 \\
nationality & 66.67 & 50.94 \\
sexual-orientation & 58.33 & 52.38 \\
physical-appearance & 58.73 & 44.44 \\
religion & 73.33 & 50.48 \\
age & 66.67 & 51.72 \\
    \midrule
    Average & 60.28 & 50.0   \\
    \bottomrule
  \end{tabular}
\vspace{1mm}

\end{table}


\subsection{Safety}
Since neither the seed data nor the augmented data intentionally include ``red teaming" demonstration examples nor does the finetuning stage optimize for detecting and reducing potential harm, we evaluate the model on 30 potentially sensitive prompts to understand our model's safety implications. We found that for these set of prompts the model tends to produce a cautious response, or even refuses to provide information to fulfill the instruction. Further, we compared responses using different system prompts and found that using the seed data's system prompt $S_a$ tends to yield safer responses. This indicates that leveraging  system prompts could be an effective solution to enhance safety. Table \ref{tab:safety_1} provides representative examples. Incorporating red teaming or other safety measures into our augmentation procedure could be a further avenue to explore, in particular existing work has shown that instruction following models are capable of ``morally self-correcting" to mitigate producing harmful responses when instructed to do so \cite{ganguli2023capacity}. 

