\begin{thebibliography}{}

\bibitem[Abramson et~al., 2020]{abramson2020imitating}
Abramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M.,
  Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., et~al. (2020).
\newblock Imitating interactive intelligence.
\newblock {\em arXiv preprint arXiv:2012.05672}.

\bibitem[Achiam et~al., 2017]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017).
\newblock Constrained policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages 22--31.
  PMLR.

\bibitem[Anthony et~al., 2017]{anthony2017thinking}
Anthony, T., Tian, Z., and Barber, D. (2017).
\newblock Thinking fast and slow with deep learning and tree search.
\newblock {\em arXiv preprint arXiv:1705.08439}.

\bibitem[Aribandi et~al., 2021]{aribandi2021ext5}
Aribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H.~S., Mehta, S.~V.,
  Zhuang, H., Tran, V.~Q., Bahri, D., Ni, J., et~al. (2021).
\newblock Ext5: Towards extreme multi-task scaling for transfer learning.
\newblock {\em arXiv preprint arXiv:2111.10952}.

\bibitem[Askell et~al., 2021]{askell2021general}
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A.,
  Joseph, N., Mann, B., DasSarma, N., et~al. (2021).
\newblock A general language assistant as a laboratory for alignment.
\newblock {\em arXiv preprint arXiv:2112.00861}.

\bibitem[Bahdanau et~al., 2016]{bahdanau2016actor}
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville,
  A., and Bengio, Y. (2016).
\newblock An actor-critic algorithm for sequence prediction.
\newblock {\em arXiv preprint arXiv:1607.07086}.

\bibitem[Bahdanau et~al., 2018]{bahdanau2018learning}
Bahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and
  Grefenstette, E. (2018).
\newblock Learning to understand goal specifications by modelling reward.
\newblock {\em arXiv preprint arXiv:1806.01946}.

\bibitem[Bender et~al., 2021]{bender2021dangers}
Bender, E.~M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021).
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In {\em Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 610--623.

\bibitem[Blodgett et~al., 2020]{blodgett2020language}
Blodgett, S.~L., Barocas, S., Daum{\'e}~III, H., and Wallach, H. (2020).
\newblock Language (technology) is power: A critical survey of" bias" in nlp.
\newblock {\em arXiv preprint arXiv:2005.14050}.

\bibitem[B{\"o}hm et~al., 2019]{bohm2019better}
B{\"o}hm, F., Gao, Y., Meyer, C.~M., Shapira, O., Dagan, I., and Gurevych, I.
  (2019).
\newblock Better rewards yield better summaries: Learning to summarise without
  references.
\newblock {\em arXiv preprint arXiv:1909.01214}.

\bibitem[Bojar et~al., 2015]{bojar-etal-2015-findings}
Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C.,
  Koehn, P., Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia,
  L., and Turchi, M. (2015).
\newblock Findings of the 2015 workshop on statistical machine translation.
\newblock In {\em Proceedings of the Tenth Workshop on Statistical Machine
  Translation}, pages 1--46, Lisbon, Portugal. Association for Computational
  Linguistics.

\bibitem[Bommasani et~al., 2021]{bommasani2021opportunities}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
  Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al. (2021).
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}.

\bibitem[Bostrom, 2014]{bostrom2014superintelligence}
Bostrom, N. (2014).
\newblock {\em Superintelligence}.
\newblock Dunod.

\bibitem[Brown et~al., 2020]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al. (2020).
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}.

\bibitem[Buchanan et~al., 2021]{buchanan2021truth}
Buchanan, B., Lohn, A., Musser, M., and Sedova, K. (2021).
\newblock Truth, lies, and automation.
\newblock Technical report, Center for the Study of Emerging Technology.

\bibitem[Caliskan et~al., 2017]{caliskan2017semantics}
Caliskan, A., Bryson, J.~J., and Narayanan, A. (2017).
\newblock Semantics derived automatically from language corpora contain
  human-like biases.
\newblock {\em Science}, 356(6334):183--186.

\bibitem[Carlini et~al., 2021]{carlini2021extracting}
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K.,
  Roberts, A., Brown, T., Song, D., Erlingsson, U., et~al. (2021).
\newblock Extracting training data from large language models.
\newblock In {\em 30th USENIX Security Symposium (USENIX Security 21)}, pages
  2633--2650.

\bibitem[Chen et~al., 2021]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J.,
  Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al. (2021).
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}.

\bibitem[Cho et~al., 2018]{cho2018towards}
Cho, W.~S., Zhang, P., Zhang, Y., Li, X., Galley, M., Brockett, C., Wang, M.,
  and Gao, J. (2018).
\newblock Towards coherent and cohesive long-form text generation.
\newblock {\em arXiv preprint arXiv:1811.00511}.

\bibitem[Choi et~al., 2018]{choi2018quac}
Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and
  Zettlemoyer, L. (2018).
\newblock Quac: Question answering in context.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2174--2184.

\bibitem[Christiano et~al., 2021]{christiano2021eliciting}
Christiano, P., Cotra, A., and Xu, M. (2021).
\newblock Eliciting latent knowledge: How to tell if your eyes deceive you.
\newblock {\em
  https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge}.

\bibitem[Christiano et~al., 2018]{christiano2018supervising}
Christiano, P., Shlegeris, B., and Amodei, D. (2018).
\newblock Supervising strong learners by amplifying weak experts.
\newblock {\em arXiv preprint arXiv:1810.08575}.

\bibitem[Christiano et~al., 2017]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
  (2017).
\newblock Deep reinforcement learning from human preferences.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4299--4307.

\bibitem[Dathathri et~al., 2019]{dathathri2019plug}
Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski,
  J., and Liu, R. (2019).
\newblock Plug and play language models: A simple approach to controlled text
  generation.
\newblock {\em arXiv preprint arXiv:1912.02164}.

\bibitem[Dhamala et~al., 2021]{dhamala2021bold}
Dhamala, J., Sun, T., Kumar, V., Krishna, S., Pruksachatkun, Y., Chang, K.-W.,
  and Gupta, R. (2021).
\newblock Bold: Dataset and metrics for measuring biases in open-ended language
  generation.
\newblock In {\em Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 862--872.

\bibitem[Dinan et~al., 2019a]{dinan2019queens}
Dinan, E., Fan, A., Williams, A., Urbanek, J., Kiela, D., and Weston, J.
  (2019a).
\newblock Queens are powerful too: Mitigating gender bias in dialogue
  generation.
\newblock {\em arXiv preprint arXiv:1911.03842}.

\bibitem[Dinan et~al., 2019b]{dinan2019build}
Dinan, E., Humeau, S., Chintagunta, B., and Weston, J. (2019b).
\newblock Build it break it fix it for dialogue safety: Robustness from
  adversarial human attack.
\newblock {\em arXiv preprint arXiv:1908.06083}.

\bibitem[Dua et~al., 2019]{dua2019drop}
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M.
  (2019).
\newblock Drop: A reading comprehension benchmark requiring discrete reasoning
  over paragraphs.
\newblock {\em arXiv preprint arXiv:1903.00161}.

\bibitem[Fedus et~al., 2021]{fedus2021switch}
Fedus, W., Zoph, B., and Shazeer, N. (2021).
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em arXiv preprint arXiv:2101.03961}.

\bibitem[Gabriel, 2020]{gabriel2020artificial}
Gabriel, I. (2020).
\newblock Artificial intelligence, values, and alignment.
\newblock {\em Minds and machines}, 30(3):411--437.

\bibitem[Gehman et~al., 2020]{gehman2020realtoxicityprompts}
Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N.~A. (2020).
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models.
\newblock {\em arXiv preprint arXiv:2009.11462}.

\bibitem[Hancock et~al., 2019]{hancock2019learning}
Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J. (2019).
\newblock Learning from dialogue after deployment: Feed yourself, chatbot!
\newblock {\em arXiv preprint arXiv:1901.05415}.

\bibitem[Henderson et~al., 2018]{henderson2018ethical}
Henderson, P., Sinha, K., Angelard-Gontier, N., Ke, N.~R., Fried, G., Lowe, R.,
  and Pineau, J. (2018).
\newblock Ethical challenges in data-driven dialogue systems.
\newblock In {\em Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 123--129.

\bibitem[Huang et~al., 2019]{huang2019reducing}
Huang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini,
  V., Yogatama, D., and Kohli, P. (2019).
\newblock Reducing sentiment bias in language models via counterfactual
  evaluation.
\newblock {\em arXiv preprint arXiv:1911.03064}.

\bibitem[Ibarz et~al., 2018]{ibarz2018reward}
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D. (2018).
\newblock Reward learning from human preferences and demonstrations in atari.
\newblock In {\em Advances in neural information processing systems}, pages
  8011--8023.

\bibitem[Irving et~al., 2018]{irving2018ai}
Irving, G., Christiano, P., and Amodei, D. (2018).
\newblock {AI} safety via debate.
\newblock {\em arXiv preprint arXiv:1805.00899}.

\bibitem[Jaques et~al., 2019]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R. (2019).
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock {\em arXiv preprint arXiv:1907.00456}.

\bibitem[Kenton et~al., 2021]{kenton2021alignment}
Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik, V., and Irving,
  G. (2021).
\newblock Alignment of language agents.
\newblock {\em arXiv preprint arXiv:2103.14659}.

\bibitem[Keskar et~al., 2019]{keskar2019ctrl}
Keskar, N.~S., McCann, B., Varshney, L.~R., Xiong, C., and Socher, R. (2019).
\newblock Ctrl: A conditional transformer language model for controllable
  generation.
\newblock {\em arXiv preprint arXiv:1909.05858}.

\bibitem[Khashabi et~al., 2020]{khashabi2020unifiedqa}
Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and
  Hajishirzi, H. (2020).
\newblock Unifiedqa: Crossing format boundaries with a single qa system.
\newblock {\em arXiv preprint arXiv:2005.00700}.

\bibitem[Kirk et~al., 2021]{kirk2021true}
Kirk, H., Jun, Y., Iqbal, H., Benussi, E., Volpin, F., Dreyer, F.~A.,
  Shtedritski, A., and Asano, Y.~M. (2021).
\newblock How true is gpt-2? an empirical analysis of intersectional
  occupational biases.
\newblock {\em arXiv preprint arXiv:2102.04130}.

\bibitem[Krause et~al., 2020]{krause2020gedi}
Krause, B., Gotmare, A.~D., McCann, B., Keskar, N.~S., Joty, S., Socher, R.,
  and Rajani, N.~F. (2020).
\newblock Gedi: Generative discriminator guided sequence generation.
\newblock {\em arXiv preprint arXiv:2009.06367}.

\bibitem[Kreutzer et~al., 2018]{kreutzer2018can}
Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. (2018).
\newblock Can neural machine translation be improved with user feedback?
\newblock {\em arXiv preprint arXiv:1804.05958}.

\bibitem[Lawrence and Riezler, 2018]{lawrence2018improving}
Lawrence, C. and Riezler, S. (2018).
\newblock Improving a neural semantic parser by counterfactual learning from
  human bandit feedback.
\newblock {\em arXiv preprint arXiv:1805.01252}.

\bibitem[Leike et~al., 2018]{leike2018scalable}
Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S.
  (2018).
\newblock Scalable agent alignment via reward modeling: a research direction.
\newblock {\em arXiv preprint arXiv:1811.07871}.

\bibitem[Leike et~al., 2017]{leike2017ai}
Leike, J., Martic, M., Krakovna, V., Ortega, P.~A., Everitt, T., Lefrancq, A.,
  Orseau, L., and Legg, S. (2017).
\newblock {AI} safety gridworlds.
\newblock {\em arXiv preprint arXiv:1711.09883}.

\bibitem[Liang et~al., 2021]{liang2021towards}
Liang, P.~P., Wu, C., Morency, L.-P., and Salakhutdinov, R. (2021).
\newblock Towards understanding and mitigating social biases in language
  models.
\newblock In {\em International Conference on Machine Learning}, pages
  6565--6576. PMLR.

\bibitem[Lin et~al., 2021]{lin2021truthfulqa}
Lin, S., Hilton, J., and Evans, O. (2021).
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock {\em arXiv preprint arXiv:2109.07958}.

\bibitem[Liu et~al., 2019]{liu2019does}
Liu, H., Dacon, J., Fan, W., Liu, H., Liu, Z., and Tang, J. (2019).
\newblock Does gender matter? towards fairness in dialogue systems.
\newblock {\em arXiv preprint arXiv:1910.10486}.

\bibitem[Madaan et~al., 2022]{madaan2022memory}
Madaan, A., Tandon, N., Clark, P., and Yang, Y. (2022).
\newblock Memory-assisted prompt editing to improve gpt-3 after deployment.
\newblock {\em arXiv preprint arXiv:2201.06009}.

\bibitem[Manela et~al., 2021]{manela2021stereotype}
Manela, D. d.~V., Errington, D., Fisher, T., van Breugel, B., and Minervini, P.
  (2021).
\newblock Stereotype and skew: Quantifying gender bias in pre-trained and
  fine-tuned language models.
\newblock {\em arXiv preprint arXiv:2101.09688}.

\bibitem[Mishra et~al., 2021]{mishra2021cross}
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. (2021).
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock {\em arXiv preprint arXiv:2104.08773}.

\bibitem[Nadeem et~al., 2020]{nadeem2020stereoset}
Nadeem, M., Bethke, A., and Reddy, S. (2020).
\newblock Stereoset: Measuring stereotypical bias in pretrained language
  models.
\newblock {\em arXiv preprint arXiv:2004.09456}.

\bibitem[Nahian et~al., 2021]{nahian2021training}
Nahian, M. S.~A., Frazier, S., Harrison, B., and Riedl, M. (2021).
\newblock Training value-aligned reinforcement learning agents using a
  normative prior.
\newblock {\em arXiv preprint arXiv:2104.09469}.

\bibitem[Nakano et~al., 2021]{nakano2021webgpt}
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C.,
  Jain, S., Kosaraju, V., Saunders, W., et~al. (2021).
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock {\em arXiv preprint arXiv:2112.09332}.

\bibitem[Nallapati et~al., 2016]{nallapati2016abstractive}
Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et~al. (2016).
\newblock Abstractive text summarization using sequence-to-sequence rnns and
  beyond.
\newblock {\em arXiv preprint arXiv:1602.06023}.

\bibitem[Nangia et~al., 2020]{nangia2020crows}
Nangia, N., Vania, C., Bhalerao, R., and Bowman, S.~R. (2020).
\newblock {CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in
  Masked Language Models}.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing}, Online. Association for Computational
  Linguistics.

\bibitem[Ngo et~al., 2021]{ngo2021mitigating}
Ngo, H., Raterink, C., Ara{\'u}jo, J.~G., Zhang, I., Chen, C., Morisot, A., and
  Frosst, N. (2021).
\newblock Mitigating harm in language models with conditional-likelihood
  filtration.
\newblock {\em arXiv preprint arXiv:2108.07790}.

\bibitem[Perez et~al., 2019]{perez2019finding}
Perez, E., Karamcheti, S., Fergus, R., Weston, J., Kiela, D., and Cho, K.
  (2019).
\newblock Finding generalizable evidence by learning to convince q\&a models.
\newblock {\em arXiv preprint arXiv:1909.05863}.

\bibitem[Qian et~al., 2019]{qian2019reducing}
Qian, Y., Muaz, U., Zhang, B., and Hyun, J.~W. (2019).
\newblock Reducing gender bias in word-level language models with a
  gender-equalizing loss function.
\newblock {\em arXiv preprint arXiv:1905.12801}.

\bibitem[Radford et~al., 2019]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019).
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI Blog}, 1(8):9.

\bibitem[Rae et~al., 2021]{rae2021scaling}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,
  Aslanides, J., Henderson, S., Ring, R., Young, S., et~al. (2021).
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock {\em arXiv preprint arXiv:2112.11446}.

\bibitem[Rajpurkar et~al., 2018]{rajpurkar2018know}
Rajpurkar, P., Jia, R., and Liang, P. (2018).
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock {\em arXiv preprint arXiv:1806.03822}.

\bibitem[Rudinger et~al., 2018]{rudinger2018gender}
Rudinger, R., Naradowsky, J., Leonard, B., and {Van Durme}, B. (2018).
\newblock Gender bias in coreference resolution.
\newblock In {\em Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, New Orleans, Louisiana. Association for Computational
  Linguistics.

\bibitem[Sanh et~al., 2021]{sanh2021multitask}
Sanh, V., Webson, A., Raffel, C., Bach, S.~H., Sutawika, L., Alyafeai, Z.,
  Chaffin, A., Stiegler, A., Scao, T.~L., Raja, A., et~al. (2021).
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock {\em arXiv preprint arXiv:2110.08207}.

\bibitem[Schick et~al., 2021]{schick2021self}
Schick, T., Udupa, S., and Sch{\"u}tze, H. (2021).
\newblock Self-diagnosis and self-debiasing: A proposal for reducing
  corpus-based bias in nlp.
\newblock {\em arXiv preprint arXiv:2103.00453}.

\bibitem[Schulman et~al., 2016]{schulman2016gae}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2016).
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Sheng et~al., 2019]{sheng2019woman}
Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. (2019).
\newblock The woman worked as a babysitter: On biases in language generation.
\newblock {\em arXiv preprint arXiv:1909.01326}.

\bibitem[Silver et~al., 2017]{silver2017mastering}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al. (2017).
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock {\em arXiv preprint arXiv:1712.01815}.

\bibitem[Soares et~al., 2015]{soares2015corrigibility}
Soares, N., Fallenstein, B., Armstrong, S., and Yudkowsky, E. (2015).
\newblock Corrigibility.
\newblock In {\em Workshops at the Twenty-Ninth AAAI Conference on Artificial
  Intelligence}.

\bibitem[Socher et~al., 2013]{socher2013recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A.~Y., and
  Potts, C. (2013).
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642.

\bibitem[Solaiman et~al., 2019]{solaiman2019release}
Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J.,
  Radford, A., Krueger, G., Kim, J.~W., Kreps, S., et~al. (2019).
\newblock Release strategies and the social impacts of language models.
\newblock {\em arXiv preprint arXiv:1908.09203}.

\bibitem[Solaiman and Dennison, 2021]{solaiman2021process}
Solaiman, I. and Dennison, C. (2021).
\newblock Process for adapting language models to society (palms) with
  values-targeted datasets.
\newblock {\em arXiv preprint arXiv:2106.10328}.

\bibitem[Stiennon et~al., 2020]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D.~M., Lowe, R., Voss, C., Radford,
  A., Amodei, D., and Christiano, P. (2020).
\newblock Learning to summarize from human feedback.
\newblock {\em arXiv preprint arXiv:2009.01325}.

\bibitem[Tamkin et~al., 2021]{tamkin2021understanding}
Tamkin, A., Brundage, M., Clark, J., and Ganguli, D. (2021).
\newblock Understanding the capabilities, limitations, and societal impact of
  large language models.
\newblock {\em arXiv preprint arXiv:2102.02503}.

\bibitem[Thoppilan et~al., 2022]{thoppilan2022lamda}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
  H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et~al. (2022).
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}.

\bibitem[Vig et~al., 2020]{vig2020investigating}
Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and
  Shieber, S.~M. (2020).
\newblock Investigating gender bias in language models using causal mediation
  analysis.
\newblock In {\em NeurIPS}.

\bibitem[V{\"o}lske et~al., 2017]{volske2017tl}
V{\"o}lske, M., Potthast, M., Syed, S., and Stein, B. (2017).
\newblock Tl; dr: Mining reddit to learn automatic summarization.
\newblock In {\em Proceedings of the Workshop on New Frontiers in
  Summarization}, pages 59--63.

\bibitem[Wang et~al., 2019]{wang2019superglue}
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F.,
  Levy, O., and Bowman, S.~R. (2019).
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock {\em arXiv preprint arXiv:1905.00537}.

\bibitem[Wei et~al., 2021]{wei2021finetuned}
Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,
  A.~M., and Le, Q.~V. (2021).
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}.

\bibitem[Weidinger et~al., 2021]{weidinger2021ethical}
Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S.,
  Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., et~al. (2021).
\newblock Ethical and social risks of harm from language models.
\newblock {\em arXiv preprint arXiv:2112.04359}.

\bibitem[Welbl et~al., 2021]{welbl2021challenges}
Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L.~A.,
  Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. (2021).
\newblock Challenges in detoxifying language models.
\newblock {\em arXiv preprint arXiv:2109.07445}.

\bibitem[Wu et~al., 2021]{wu2021recursively}
Wu, J., Ouyang, L., Ziegler, D.~M., Stiennon, N., Lowe, R., Leike, J., and
  Christiano, P. (2021).
\newblock Recursively summarizing books with human feedback.
\newblock {\em arXiv preprint arXiv:2109.10862}.

\bibitem[Xu et~al., 2021]{xu2021detoxifying}
Xu, A., Pathak, E., Wallace, E., Gururangan, S., Sap, M., and Klein, D. (2021).
\newblock Detoxifying language models risks marginalizing minority voices.
\newblock {\em arXiv preprint arXiv:2104.06390}.

\bibitem[Xu et~al., 2020]{xu2020recipes}
Xu, J., Ju, D., Li, M., Boureau, Y.-L., Weston, J., and Dinan, E. (2020).
\newblock Recipes for safety in open-domain chatbots.
\newblock {\em arXiv preprint arXiv:2010.07079}.

\bibitem[Yi et~al., 2019]{yi2019towards}
Yi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Hedayatnia, B.,
  Venkatesh, A., Gabriel, R., and Hakkani-Tur, D. (2019).
\newblock Towards coherent and engaging spoken dialog response generation using
  automatic conversation evaluators.
\newblock {\em arXiv preprint arXiv:1904.13015}.

\bibitem[Zellers et~al., 2019]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019).
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In {\em Association for Computational Linguistics}, pages 4791--4800.

\bibitem[Zhao et~al., 2021]{zhao2021evaluation}
Zhao, M., Anderson, P., Jain, V., Wang, S., Ku, A., Baldridge, J., and Ie, E.
  (2021).
\newblock On the evaluation of vision-and-language navigation instructions.
\newblock {\em arXiv preprint arXiv:2101.10504}.

\bibitem[Zhou and Xu, 2020]{zhou2020learning}
Zhou, W. and Xu, K. (2020).
\newblock Learning to compare for better training and evaluation of open domain
  natural language generation models.
\newblock {\em arXiv preprint arXiv:2002.05058}.

\bibitem[Ziegler et~al., 2019]{ziegler2019fine}
Ziegler, D.~M., Stiennon, N., Wu, J., Brown, T.~B., Radford, A., Amodei, D.,
  Christiano, P., and Irving, G. (2019).
\newblock Fine-tuning language models from human preferences.
\newblock {\em arXiv preprint arXiv:1909.08593}.

\end{thebibliography}
