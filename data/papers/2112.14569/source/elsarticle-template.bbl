\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, I.~Polosukhin, Attention is all you need, in: Advances in
  neural information processing systems, 2017, pp. 5998--6008.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever, Improving language
  understanding by generative pre-training, URL https://s3-us-west-2.
  amazonaws. com/openai-assets/researchcovers/languageunsupervised/language
  understanding paper. pdf.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, Language models
  are unsupervised multitask learners, OpenAI Blog 1~(8).

\bibitem{devlin2019bert}
J.~Devlin, M.-W. Chang, K.~Lee, K.~Toutanova, Bert: Pre-training of deep
  bidirectional transformers for language understanding, in: Proceedings of the
  2019 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies, Volume 1 (Long and
  Short Papers), 2019, pp. 4171--4186.

\bibitem{raffel2019exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, P.~J. Liu, Exploring the limits of transfer learning with a unified
  text-to-text transformer, arXiv preprint arXiv:1910.10683.

\bibitem{sennrich-etal-2016-neural}
R.~Sennrich, B.~Haddow, A.~Birch,
  \href{https://www.aclweb.org/anthology/P16-1162}{Neural machine translation
  of rare words with subword units}, in: Proceedings of the 54th Annual Meeting
  of the Association for Computational Linguistics (Volume 1: Long Papers),
  Association for Computational Linguistics, Berlin, Germany, 2016, pp.
  1715--1725.
\newblock \href {http://dx.doi.org/10.18653/v1/P16-1162}
  {\path{doi:10.18653/v1/P16-1162}}.
\newline\urlprefix\url{https://www.aclweb.org/anthology/P16-1162}

\bibitem{bostrom2020byte}
K.~Bostrom, G.~Durrett, Byte pair encoding is suboptimal for language model
  pretraining, in: Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: Findings, 2020, pp. 4617--4624.

\bibitem{provilkov2020bpe}
I.~Provilkov, D.~Emelianenko, E.~Voita, Bpe-dropout: Simple and effective
  subword regularization, in: Proceedings of the 58th Annual Meeting of the
  Association for Computational Linguistics, 2020, pp. 1882--1892.

\bibitem{gage1994new}
P.~Gage, A new algorithm for data compression, C Users Journal 12~(2) (1994)
  23--38.

\bibitem{lakew2019controlling}
S.~M. Lakew, M.~Di~Gangi, M.~Federico, Controlling the output length of neural
  machine translation, arXiv preprint arXiv:1910.10408.

\bibitem{aji2020neural}
A.~F. Aji, N.~Bogoychev, K.~Heafield, R.~Sennrich, In neural machine
  translation, what does transfer learning transfer?, in: Proceedings of the
  58th Annual Meeting of the Association for Computational Linguistics, 2020,
  pp. 7701--7710.

\bibitem{wang2021multi}
X.~Wang, S.~Ruder, G.~Neubig, Multi-view subword regularization, in:
  Proceedings of the 2021 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies, 2021,
  pp. 473--482.

\bibitem{sato2020vocabulary}
S.~Sato, J.~Sakuma, N.~Yoshinaga, M.~Toyoda, M.~Kitsuregawa, Vocabulary
  adaptation for domain adaptation in neural machine translation, in:
  Proceedings of the 2020 Conference on Empirical Methods in Natural Language
  Processing: Findings, 2020, pp. 4269--4279.

\bibitem{chronopoulou2020lmu}
A.~Chronopoulou, D.~Stojanovski, V.~Hangya, A.~Fraser, The lmu munich system
  for the wmt 2020 unsupervised machine translation shared task, arXiv preprint
  arXiv:2010.13192.

\bibitem{arase2019transfer}
Y.~Arase, J.~Tsujii, Transfer fine-tuning: A bert case study, in: Proceedings
  of the 2019 Conference on Empirical Methods in Natural Language Processing
  and the 9th International Joint Conference on Natural Language Processing
  (EMNLP-IJCNLP), 2019, pp. 5396--5407.

\bibitem{bojanowski2019updating}
P.~Bojanowski, O.~Celebi, T.~Mikolov, E.~Grave, A.~Joulin, Updating pre-trained
  word vectors and text classifiers using monolingual alignment, arXiv preprint
  arXiv:1910.06241.

\bibitem{kudo2018subword}
T.~Kudo, Subword regularization: Improving neural network translation models
  with multiple subword candidates, in: Proceedings of the 56th Annual Meeting
  of the Association for Computational Linguistics (Volume 1: Long Papers),
  2018, pp. 66--75.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, V.~Stoyanov, Roberta: A robustly optimized bert pretraining
  approach, arXiv preprint arXiv:1907.11692.

\bibitem{schuster2012japanese}
M.~Schuster, K.~Nakajima, Japanese and korean voice search, in: 2012 IEEE
  International Conference on Acoustics, Speech and Signal Processing (ICASSP),
  IEEE, 2012, pp. 5149--5152.

\bibitem{kudo2018sentencepiece}
T.~Kudo, J.~Richardson, Sentencepiece: A simple and language independent
  subword tokenizer and detokenizer for neural text processing, in: Proceedings
  of the 2018 Conference on Empirical Methods in Natural Language Processing:
  System Demonstrations, 2018, pp. 66--71.

\bibitem{Wen2020MeDALMA}
Z.~Wen, X.~H. Lu, S.~Reddy, Medal: Medical abbreviation disambiguation dataset
  for natural language understanding pretraining, in: CLINICALNLP, 2020.

\end{thebibliography}
