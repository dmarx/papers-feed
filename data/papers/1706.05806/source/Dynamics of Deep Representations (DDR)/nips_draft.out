\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Measuring Representations in Neural Networks}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Distributed Representations}{section.2}% 3
\BOOKMARK [1][-]{section.3}{Scaling SVCCA for Convolutional Layers}{}% 4
\BOOKMARK [2][-]{subsection.3.1}{Scaling SVCCA with Discrete Fourier Transforms}{section.3}% 5
\BOOKMARK [1][-]{section.4}{Applications of SVCCA}{}% 6
\BOOKMARK [2][-]{subsection.4.1}{Learning Dynamics with SVCCA}{section.4}% 7
\BOOKMARK [2][-]{subsection.4.2}{Freeze Training}{section.4}% 8
\BOOKMARK [2][-]{subsection.4.3}{Interpreting Representations: when are classes learned?}{section.4}% 9
\BOOKMARK [2][-]{subsection.4.4}{Other Applications: Cross Model Comparison and compression}{section.4}% 10
\BOOKMARK [1][-]{section.5}{Conclusion}{}% 11
\BOOKMARK [1][-]{appendix.A}{Mathematical details of CCA and SVCCA}{}% 12
\BOOKMARK [1][-]{appendix.B}{Additional Proofs and Figures from Section 2.1}{}% 13
\BOOKMARK [2][-]{subsubsection.B.0.1}{The importance of SVD: how many directions matter?}{appendix.B}% 14
\BOOKMARK [1][-]{appendix.C}{Proof of Theorem 1}{}% 15
\BOOKMARK [2][-]{subsection.C.1}{Proof of Lemma 1}{appendix.C}% 16
\BOOKMARK [2][-]{subsection.C.2}{Proof of Lemma 2}{appendix.C}% 17
\BOOKMARK [2][-]{subsection.C.3}{Proof of Theorem 2}{appendix.C}% 18
\BOOKMARK [2][-]{subsection.C.4}{Proof of Theorem 1}{appendix.C}% 19
\BOOKMARK [2][-]{subsection.C.5}{Computational Gains}{appendix.C}% 20
\BOOKMARK [1][-]{appendix.D}{Per Layer Learning Dynamics Plots from Section 4.1}{}% 21
\BOOKMARK [1][-]{appendix.E}{Additional Figure from Section 4.4}{}% 22
\BOOKMARK [1][-]{appendix.F}{Experiment from Section 4.4}{}% 23
\BOOKMARK [1][-]{appendix.G}{Learning Dynamics for an LSTM}{}% 24
