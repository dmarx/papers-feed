\begin{thebibliography}{15}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Alexandrov et~al.(2024)Alexandrov, Raychev, M{\"u}ller, Zhang,
  Vechev, and Toutanova}]{alexandrov2024mitigating}
Anton Alexandrov, Veselin Raychev, Mark~Niklas M{\"u}ller, Ce~Zhang, Martin
  Vechev, and Kristina Toutanova. 2024.
\newblock Mitigating catastrophic forgetting in language transfer via model
  merging.
\newblock \emph{arXiv preprint arXiv:2407.08699}.

\bibitem[{Gao et~al.(2021)Gao, Alawad, Young, Gounley, Schaefferkoetter, Yoon,
  Wu, Durbin, Doherty, Stroup et~al.}]{gao2021limitations}
Shang Gao, Mohammed Alawad, M~Todd Young, John Gounley, Noah Schaefferkoetter,
  Hong~Jun Yoon, Xiao-Cheng Wu, Eric~B Durbin, Jennifer Doherty, Antoinette
  Stroup, et~al. 2021.
\newblock Limitations of transformers on clinical text classification.
\newblock \emph{IEEE journal of biomedical and health informatics},
  25(9):3596--3607.

\bibitem[{Gee et~al.(2022)Gee, Zugarini, Rigutini, and Torroni}]{gee2022fast}
Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, and Paolo Torroni. 2022.
\newblock Fast vocabulary transfer for language model compression.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing: Industry Track}, pages 409--416.

\bibitem[{Gu et~al.(2021)Gu, Tinn, Cheng, Lucas, Usuyama, Liu, Naumann, Gao,
  and Poon}]{gu2021domain}
Yu~Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu,
  Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021.
\newblock Domain-specific language model pretraining for biomedical natural
  language processing.
\newblock \emph{ACM Transactions on Computing for Healthcare (HEALTH)},
  3(1):1--23.

\bibitem[{Hersh et~al.(1994)Hersh, Buckley, Leone, and
  Hickam}]{hersh1994ohsumed}
William Hersh, Chris Buckley, TJ~Leone, and David Hickam. 1994.
\newblock Ohsumed: An interactive retrieval evaluation and new large test
  collection for research.
\newblock In \emph{SIGIR’94}, pages 192--201. Springer.

\bibitem[{Huang et~al.(2019)Huang, Altosaar, and
  Ranganath}]{Huang2019ClinicalBERT}
Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019.
\newblock \href {http://arxiv.org/abs/1904.05342} {Clinicalbert: Modeling
  clinical notes and predicting hospital readmission}.

\bibitem[{Hughes et~al.(2017)Hughes, Li, Kotoulas, and
  Suzumura}]{hughes2017medical}
Mark Hughes, Irene Li, Spyros Kotoulas, and Toyotaro Suzumura. 2017.
\newblock Medical text classification using convolutional neural networks.
\newblock In \emph{Informatics for Health: Connected Citizen-Led Wellness and
  Population Health}, pages 246--250. IOS Press.

\bibitem[{Lee et~al.(2020)Lee, Yoon, Kim, Kim, Kim, So, and
  Kang}]{lee2020biobert}
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan~Ho So,
  and Jaewoo Kang. 2020.
\newblock Biobert: a pre-trained biomedical language representation model for
  biomedical text mining.
\newblock \emph{Bioinformatics}, 36(4):1234--1240.

\bibitem[{Mosin et~al.(2023)Mosin, Samenko, Kozlovskii, Tikhonov, and
  Yamshchikov}]{mosin2023fine}
Vladislav Mosin, Igor Samenko, Borislav Kozlovskii, Alexey Tikhonov, and Ivan~P
  Yamshchikov. 2023.
\newblock Fine-tuning transformers: Vocabulary transfer.
\newblock \emph{Artificial Intelligence}, 317:103860.

\bibitem[{Mujtaba et~al.(2019)Mujtaba, Shuib, Idris, Hoo, Raj, Khowaja, Shaikh,
  and Nweke}]{mujtaba2019clinical}
Ghulam Mujtaba, Liyana Shuib, Norisma Idris, Wai~Lam Hoo, Ram~Gopal Raj, Kamran
  Khowaja, Khairunisa Shaikh, and Henry~Friday Nweke. 2019.
\newblock Clinical text classification research trends: Systematic literature
  review and open issues.
\newblock \emph{Expert systems with applications}, 116:494--520.

\bibitem[{Noa P. Cruz~Díaz(2015)}]{Lopez2015}
Manuel Maña~López Noa P. Cruz~Díaz. 2015.
\newblock \href {https://aclanthology.org/W15-2605.pdf} {An analysis of
  biomedical tokenization: Problems and strategies}.
\newblock \emph{Proceedings of the Sixth International Workshop on Health Text
  Mining and Information Analysis}, pages 40--49.

\bibitem[{Remy et~al.(2024)Remy, Delobelle, Avetisyan, Khabibullina,
  de~Lhoneux, and Demeester}]{remy2024trans}
Fran{\c{c}}ois Remy, Pieter Delobelle, Hayastan Avetisyan, Alfiya Khabibullina,
  Miryam de~Lhoneux, and Thomas Demeester. 2024.
\newblock Trans-tokenization and cross-lingual vocabulary transfers: Language
  adaptation of llms for low-resource nlp.
\newblock \emph{arXiv preprint arXiv:2408.04303}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008.

\bibitem[{Yamshchikov et~al.(2022)Yamshchikov, Tikhonov, Pantis, Schubert, and
  Jost}]{yamshchikov2022bert}
Ivan Yamshchikov, Alexey Tikhonov, Yorgos Pantis, Charlotte Schubert, and
  J{\"u}rgen Jost. 2022.
\newblock Bert in plutarch’s shadows.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 6071--6080.

\bibitem[{Zuo et~al.(2019)Zuo, Yang, Dou, and Wen}]{Zuo2019MedGPT}
Xiaochen Zuo, Xue Yang, Zhicheng Dou, and Ji~Rong Wen. 2019.
\newblock \href {https://doi.org/10.1145/1122445.1122456} {Rucir at trec 2019:
  Conversational assistance track}.
\newblock In \emph{28th Text REtrieval Conference, TREC 2019 - Proceedings}.
  National Institute of Standards and Technology (NIST).

\end{thebibliography}
