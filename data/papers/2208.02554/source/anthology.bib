@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf},
  year={2018}
}


@inproceedings{voita2019bottom,
  title={The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives},
  author={Voita, Elena and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4387--4397},
  year={2019}
}

@inproceedings{arase2019transfer,
  title={Transfer Fine-Tuning: A BERT Case Study},
  author={Arase, Yuki and Tsujii, Jun’ichi},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5396--5407},
  year={2019}
}



@article{mozafari2019bert,
  title={A BERT-based transfer learning approach for hate speech detection in online social media},
  author={Mozafari, Marzieh and Farahbakhsh, Reza and Crespi, Noel},
  journal={arXiv preprint arXiv:1910.12574},
  year={2019}
}

@article{bojanowski2019updating,
  title={Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment},
  author={Bojanowski, Piotr and Celebi, Onur and Mikolov, Tomas and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1910.06241},
  year={2019}
}

@article{wiedemann2019does,
  title={Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings},
  author={Wiedemann, Gregor and Remus, Steffen and Chawla, Avi and Biemann, Chris},
  journal={arXiv preprint arXiv:1909.10430},
  year={2019}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@inproceedings{Wen2020MeDALMA,
  title={MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining},
  author={Zhi Wen and Xing Han Lu and Siva Reddy},
  booktitle={CLINICALNLP},
  year={2020}
}

@inproceedings{kudo2018subword,
  title={Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
  author={Kudo, Taku},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={66--75},
  year={2018}
}

@inproceedings{kudo2018sentencepiece,
  title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author={Kudo, Taku and Richardson, John},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={66--71},
  year={2018}
}

@inproceedings{provilkov2020bpe,
  title={BPE-Dropout: Simple and Effective Subword Regularization},
  author={Provilkov, Ivan and Emelianenko, Dmitrii and Voita, Elena},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={1882--1892},
  year={2020}
}

@inproceedings{wang2021multi,
  title={Multi-view Subword Regularization},
  author={Wang, Xinyi and Ruder, Sebastian and Neubig, Graham},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={473--482},
  year={2021}
}

@inproceedings{bostrom2020byte,
  title={Byte Pair Encoding is Suboptimal for Language Model Pretraining},
  author={Bostrom, Kaj and Durrett, Greg},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={4617--4624},
  year={2020}
}

@inproceedings{petroni2019language,
  title={Language Models as Knowledge Bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2463--2473},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@inproceedings{sun2019patient,
  title={Patient Knowledge Distillation for BERT Model Compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4314--4323},
  year={2019}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@inproceedings{golovanov2019large,
  title={Large-scale transfer learning for natural language generation},
  author={Golovanov, Sergey and Kurbanov, Rauf and Nikolenko, Sergey and Truskovskyi, Kyryl and Tselousov, Alexander and Wolf, Thomas},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6053--6058},
  year={2019}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle={Proceedings of NAACL-HLT},
  pages={2227--2237},
  year={2018}
}

@article{gage1994new,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={McPherson, KS: R \& D Publications, c1987-1994.}
}


@inproceedings{sennrich2017grammatical,
  title={How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs},
  author={Sennrich, Rico},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={376--382},
  year={2017}
}

@inproceedings{sennrich2016linguistic,
  title={Linguistic Input Features Improve Neural Machine Translation},
  author={Sennrich, Rico and Haddow, Barry},
  booktitle={Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers},
  pages={83--91},
  year={2016}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@inproceedings{schuster2012japanese,
  title={Japanese and korean voice search},
  author={Schuster, Mike and Nakajima, Kaisuke},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5149--5152},
  year={2012},
  organization={IEEE}
}


@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5754--5764},
  year={2019}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{domingo2018much,
  title={How Much Does Tokenization Affect Neural Machine Translation?},
  author={Domingo, Miguel and Garc{\i}a-Mart{\i}nez, Mercedes and Helle, Alexandre and Casacuberta, Francisco and Herranz, Manuel},
  journal={arXiv preprint arXiv:1812.08621},
  year={2018}
}

@article{dodge2020fine,
  title={Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping},
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  journal={arXiv preprint arXiv:2002.06305},
  year={2020}
}

@inproceedings{aji2020neural,
  title={In Neural Machine Translation, What Does Transfer Learning Transfer?},
  author={Aji, Alham Fikri and Bogoychev, Nikolay and Heafield, Kenneth and Sennrich, Rico},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7701--7710},
  year={2020}
}

@article{lakew2019controlling,
  title={Controlling the output length of neural machine translation},
  author={Lakew, Surafel Melaku and Di Gangi, Mattia and Federico, Marcello},
  journal={arXiv preprint arXiv:1910.10408},
  year={2019}
}

@inproceedings{sato2020vocabulary,
  title={Vocabulary adaptation for domain adaptation in neural machine translation},
  author={Sato, Shoetsu and Sakuma, Jin and Yoshinaga, Naoki and Toyoda, Masashi and Kitsuregawa, Masaru},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={4269--4279},
  year={2020}
}

@article{chronopoulou2020lmu,
  title={The LMU Munich system for the WMT 2020 unsupervised machine translation shared task},
  author={Chronopoulou, Alexandra and Stojanovski, Dario and Hangya, Viktor and Fraser, Alexander},
  journal={arXiv preprint arXiv:2010.13192},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}

@inproceedings{conneau2020unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}

@inproceedings{clark2019electra,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{mosin2023fine,
  title={Fine-tuning transformers: Vocabulary transfer},
  author={Mosin, Vladislav and Samenko, Igor and Kozlovskii, Borislav and Tikhonov, Alexey and Yamshchikov, Ivan P},
  journal={Artificial Intelligence},
  volume={317},
  pages={103860},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{wen2020medal,
  title={MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining},
  author={Wen, Zhi and Lu, Xing Han and Reddy, Siva},
  booktitle={Proceedings of the 3rd Clinical Natural Language Processing Workshop},
  pages={130--135},
  year={2020}
}

@inproceedings{moschitti2004complex,
  title={Complex linguistic features for text classification: A comprehensive study},
  author={Moschitti, Alessandro and Basili, Roberto},
  booktitle={European conference on information retrieval},
  pages={181--196},
  year={2004},
  organization={Springer}
}

@inproceedings{hersh1994ohsumed,
  title={OHSUMED: An interactive retrieval evaluation and new large test collection for research},
  author={Hersh, William and Buckley, Chris and Leone, TJ and Hickam, David},
  booktitle={SIGIR’94},
  pages={192--201},
  year={1994},
  organization={Springer}
}

@article{mujtaba2019clinical,
  title={Clinical text classification research trends: Systematic literature review and open issues},
  author={Mujtaba, Ghulam and Shuib, Liyana and Idris, Norisma and Hoo, Wai Lam and Raj, Ram Gopal and Khowaja, Kamran and Shaikh, Khairunisa and Nweke, Henry Friday},
  journal={Expert systems with applications},
  volume={116},
  pages={494--520},
  year={2019},
  publisher={Elsevier}
}

@article{gao2021limitations,
  title={Limitations of transformers on clinical text classification},
  author={Gao, Shang and Alawad, Mohammed and Young, M Todd and Gounley, John and Schaefferkoetter, Noah and Yoon, Hong Jun and Wu, Xiao-Cheng and Durbin, Eric B and Doherty, Jennifer and Stroup, Antoinette and others},
  journal={IEEE journal of biomedical and health informatics},
  volume={25},
  number={9},
  pages={3596--3607},
  year={2021},
  publisher={IEEE}
}

@incollection{hughes2017medical,
  title={Medical text classification using convolutional neural networks},
  author={Hughes, Mark and Li, Irene and Kotoulas, Spyros and Suzumura, Toyotaro},
  booktitle={Informatics for Health: Connected Citizen-Led Wellness and Population Health},
  pages={246--250},
  year={2017},
  publisher={IOS Press}
}

@inproceedings{Zuo2019MedGPT,
   author = {Xiaochen Zuo and Xue Yang and Zhicheng Dou and Ji Rong Wen},
   doi = {10.1145/1122445.1122456},
   booktitle = {28th Text REtrieval Conference, TREC 2019 - Proceedings},
   keywords = {conversational search,information retrieval,neural network,reference resolution},
   publisher = {National Institute of Standards and Technology (NIST)},
   title = {RUCIR at TREC 2019: Conversational Assistance Track},
   year = {2019},
}

@article{Huang2019ClinicalBERT,
   author = {Kexin Huang and Jaan Altosaar and Rajesh Ranganath},
   month = {4},
   title = {ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission},
   url = {http://arxiv.org/abs/1904.05342},
   year = {2019},
}

@article{Lopez2015,
    author = {Noa P. Cruz Díaz, Manuel Maña López},
    title = {An Analysis of Biomedical Tokenization: Problems and Strategies},
    journal = {Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis},
    year = {2015},
    pages = {40-49},
    url={https://aclanthology.org/W15-2605.pdf}
}

@article{Mosin2022,
   author = {Vladislav D. Mosin and Ivan P. Yamshchikov},
   month = {8},
   title = {Vocabulary Transfer for Medical Texts},
   url = {http://arxiv.org/abs/2208.02554},
   year = {2022},
}



@article{Mosin2023,
author = {Mosin, Vladislav and Samenko, Igor and Kozlovskii, Borislav and Tikhonov, Alexey and Yamshchikov, Ivan P.},
title = {Fine-tuning transformers: Vocabulary transfer},
year = {2023},
issue_date = {Apr 2023},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {317},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2023.103860},
doi = {10.1016/j.artint.2023.103860},
journal = {Artif. Intell.},
month = {apr},
numpages = {11},
keywords = {Vocabulary tokenization, Transformers, Vocabulary transfer}
}

@article{gu2021domain,
  title={Domain-specific language model pretraining for biomedical natural language processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{JinhyukLee2019,
   author = {Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho and Jaewoo Kang},
   title = {BioBert: a pre-trained biomedical language representation model for biomedical text mining},
   year = {2019},
}

@article{Gu2022,
   author = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
   doi = {10.1145/3458754},
   issn = {26378051},
   issue = {1},
   journal = {ACM Transactions on Computing for Healthcare},
   keywords = {Biomedical,NLP,domain-specific pretraining},
   month = {1},
   publisher = {Association for Computing Machinery},
   title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
   volume = {3},
   year = {2022},
}

@article{Harris2022,
    author = {Zellig S. Harris},
    title = {The structure of science information},
    journal = {Journal of biomedical informatics, 35:4},
    year = {2022},
    pages = {215-221},
}

@article{Buonocore2023,
author = {Buonocore, Tommaso Mario and Crema, Claudio and Redolfi, Alberto and Bellazzi, Riccardo and Parimbelli, Enea},
title = {Localizing in-domain adaptation of transformer-based biomedical language models},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {144},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2023.104431},
doi = {10.1016/j.jbi.2023.104431},
journal = {J. of Biomedical Informatics},
month = {aug},
numpages = {8},
keywords = {Natural language processing, Deep learning, Language model, Biomedical text mining, Transformer}
}

@inproceedings{gee2022fast,
  title={Fast Vocabulary Transfer for Language Model Compression},
  author={Gee, Leonidas and Zugarini, Andrea and Rigutini, Leonardo and Torroni, Paolo},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={409--416},
  year={2022}
}

@inproceedings{yamshchikov2022bert,
  title={BERT in Plutarch’s Shadows},
  author={Yamshchikov, Ivan and Tikhonov, Alexey and Pantis, Yorgos and Schubert, Charlotte and Jost, J{\"u}rgen},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={6071--6080},
  year={2022}
}

@article{remy2024trans,
  title={Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP},
  author={Remy, Fran{\c{c}}ois and Delobelle, Pieter and Avetisyan, Hayastan and Khabibullina, Alfiya and de Lhoneux, Miryam and Demeester, Thomas},
  journal={arXiv preprint arXiv:2408.04303},
  year={2024}
}

@article{alexandrov2024mitigating,
  title={Mitigating Catastrophic Forgetting in Language Transfer via Model Merging},
  author={Alexandrov, Anton and Raychev, Veselin and M{\"u}ller, Mark Niklas and Zhang, Ce and Vechev, Martin and Toutanova, Kristina},
  journal={arXiv preprint arXiv:2407.08699},
  year={2024}
}