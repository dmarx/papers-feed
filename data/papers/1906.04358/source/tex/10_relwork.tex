%!TeX root=../main.tex

Our work has connections to existing work not only in deep learning, but also to various other fields:

\textbf{Architecture Search}\: Search algorithms for neural network topologies originated from the field of evolutionary computing~\cite{turing1948intelligent,harp1990designing,dasgupta1992designing,fullmer1992using,gruau1996comparison,krishnan1994delta,braun1993evolving,mandischer1993representation,zhang1993evolving,maniezzo1994genetic,angeline1994evolutionary,lee1996evolutionary,opitz1997connectionist,pujol1998evolving,yao1998towards}. Our method is based on NEAT~\cite{neat}, an established topology search algorithm notable for its ability to optimize the weights and structure of networks simultaneously. In order to achieve state-of-the-art results, recent methods narrow the search space to architectures composed of basic building blocks with strong domain priors such as CNNs~\cite{zoph2016neural,real2017large,liu2017hierarchical,miikkulainen2019evolving}, recurrent cells~\cite{jozefowicz2015empirical,zoph2016neural,miikkulainen2019evolving} and self-attention~\cite{so2019evolved}. It has been shown that random search can already achieve SOTA results if such priors are used~\cite{li2019random,sciuto2019evaluating,real2018regularized}. The inner loop for training the weights of each candidate architecture before evaluation makes the search costly, although efforts have been made to improve efficiency~\cite{pham2018efficient,brock2017smash,liu2018darts}. In our approach, we evaluate architectures without weight training, bypassing the costly inner loop,
%
similar to the random trial approach in \cite{hinton1996learning,smith1987learning} that evolved architectures to be more weight tolerant.

\textbf{Bayesian Neural Networks}\: The weight parameters of a BNN~\cite{mackay1992bayesian,hinton1993keeping,barber1998ensemble,bishop2006pattern,neal2012bayesian,gal2016uncertainty} are not fixed values, but sampled from a distribution. While the parameters of this distribution can be learned~\cite{hanson1990meiosis,hanson1990stochastic,graves2011practical,krueger2017bayesian}, the number of parameters is often greater than the number of weights. Recently, Neklyudov et al.~\cite{neklyudov2018variance} proposed variance networks, which sample each weight from a distribution with a zero mean and a learned variance parameter, and show that ensemble evaluations can improve performance on image recognition tasks. We employ a similar approach, sampling weights from a fixed uniform distribution with zero mean, as well as evaluating performance on network ensembles.

\textbf{Algorithmic Information Theory}\: In AIT~\cite{solomonoff1964formal}, the Kolmogorov complexity~\cite{kolmogorov1965three} of a computable object is the minimum length of the program that can compute it. The Minimal Description Length (MDL)~\cite{rissanen1978modeling,grunwald2007minimum,rissanen2007information} is a formalization of Occam's razor, in which a good model is one that is best at compressing its data, including the cost of describing of the model itself. Ideas related to MDL for making neural networks “simple” was proposed in the 1990s, such as simplifying networks by soft-weight sharing~\cite{nowlan1992simplifying}, reducing the amount of information in weights by making them noisy~\cite{hinton1993keeping}, and simplifying the search space of its weights~\cite{schmidhuber1997discovering}. Recent works offer a modern treatment~\cite{blier2018description} and application~\cite{li2018measuring,trask2018neural} of these principles in the context of larger, deep neural network architectures.

While the aforementioned works focus on the information capacity required to represent the \textit{weights} of a predefined network architecture, in this work we focus on finding minimal \textit{architectures} that can represent solutions to various tasks. As our networks still require weights, we borrow ideas from AIT and BNN, and take them a bit further. Motivated by MDL, in our approach, we apply weight-sharing to the entire network and treat the weight as a random variable sampled from a fixed distribution.

\textbf{Network Pruning}\; By removing connections with small weight values from a trained neural network, pruning approaches~\cite{lecun1990optimal,hassibi1993second,han2015learning,guo2016dynamic,li2016pruning,molchanov2016pruning,luo2017thinet,liu2018rethinking,mallya2018piggyback} can produce sparse networks that keep only a small fraction of the connections, while maintaining similar performance on image classification tasks compared to the full network. By retaining the original weight initialization values, these sparse networks can even be trained from scratch to achieve a higher test accuracy~\cite{frankle2018lottery,lee2018snip} than the original network. Similar to our work, a concurrent work~\cite{zhou2019deconstructing} found pruned networks that can achieve image classification accuracies that are much better than chance even with randomly initialized weights.

Network pruning is a complementary approach to ours; it starts with a full, trained network, and takes away connections, while in our approach, we start with no connections, and add complexity as needed. Compared to our approach, pruning requires prior training of the full network to obtain useful information about each weight in advance. In addition, the architectures produced by pruning are limited by the full network, while in our method there is no upper bound on the network's complexity.

\textbf{Neuroscience}\: A \textit{connectome}~\cite{seung2012connectome} is the “wiring diagram” or mapping of all neural connections of the brain. While it is a challenge to map out the human connectome~\cite{sporns2005human}, with our 90 billion neurons and 150 trillion synapses, the connectome of simple organisms such as roundworms~\cite{white1986structure,varshney2011structural} has been constructed, and recent works~\cite{eichler2017complete,takemura2017connectome} mapped out the entire brain of a small fruit fly. A motivation for examining the connectome, even of an insect, is that it will help guide future research on how the brain learns and represents memories in its connections. For humans it is evident, especially during early childhood~\cite{huttenlocher1990morphometric,tierney2009brain}, that we learn skills and form memories by forming new synaptic connections, and our brain rewires itself based on our new experiences~\cite{black1990learning,bruer1999neural,kleim2002motor,dayan2011neuroplasticity}.

The connectome can be viewed as a graph~\cite{bullmore2009complex,he2010graph,van2011rich}, and analyzed using rich tools from graph theory, network science and computer simulation. Our work also aims to learn network graphs that can encode skills and knowledge for an artificial agent in a simulation environment. By deemphasizing learning of weight parameters, we encourage the agent instead to develop ever-growing networks that can encode acquired skills based on its interactions with the environment. Like the connectome of simple organisms, the networks discovered by our approach are small enough to be analyzed.
