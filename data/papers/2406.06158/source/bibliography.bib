@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}

@article{kovachki2021continuous,
  title={Continuous time analysis of momentum methods},
  author={Kovachki, Nikola B and Stuart, Andrew M},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={17},
  pages={1--40},
  year={2021},
  publisher={Journal of Machine Learning Research}
}

@article{cooper2018loss,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@article{wang2021large,
  title={Large learning rate tames homogeneity: Convergence and balancing effect},
  author={Wang, Yuqing and Chen, Minshuo and Zhao, Tuo and Tao, Molei},
  journal={arXiv preprint arXiv:2110.03677},
  year={2021}
}

@article{damian2021label,
  title={Label noise sgd provably prefers flat global minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{li2021happens,
  title={What Happens after SGD Reaches Zero Loss?--A Mathematical Framework},
  author={Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2110.06914},
  year={2021}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@article{nguyen2019first,
  title={First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise},
  author={Nguyen, Thanh Huy and Simsekli, Umut and Gurbuzbalaban, Mert and Richard, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{dai2020large,
  title={On large batch training and sharp minima: a Fokker--Planck perspective},
  author={Dai, Xiaowu and Zhu, Yuhua},
  journal={Journal of Statistical Theory and Practice},
  volume={14},
  number={3},
  pages={1--31},
  year={2020},
  publisher={Springer}
}

@article{mori2021logarithmic,
  title={Logarithmic landscape and power-law escape rate of sgd},
  author={Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito},
  journal={arXiv preprint arXiv:2105.09557},
  year={2021}
}

@article{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{ma2021linear,
  title={On Linear Stability of SGD and Input-Smoothness of Neural Networks},
  author={Ma, Chao and Ying, Lexing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}

@article{lyu2021gradient,
  title={Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018},
  organization={PMLR}
}

@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}

@article{neyshabur2017geometry,
  title={Geometry of optimization and implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
  journal={arXiv preprint arXiv:1705.03071},
  year={2017}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{geiping2021stochastic,
  title={Stochastic training is not necessary for generalization},
  author={Geiping, Jonas and Goldblum, Micah and Pope, Phillip E and Moeller, Michael and Goldstein, Tom},
  journal={arXiv preprint arXiv:2109.14119},
  year={2021}
}

@article{Tieleman2012,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{malladi2022sdes,
  title={On the SDEs and Scaling Rules for Adaptive Gradient Algorithms},
  author={Malladi, Sadhika and Lyu, Kaifeng and Panigrahi, Abhishek and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2205.10287},
  year={2022}
}

@incollection{lee2013smooth,
  title={Smooth manifolds},
  author={Lee, John M},
  booktitle={Introduction to smooth manifolds},
  pages={1--31},
  year={2013},
  publisher={Springer}
}

@article{herzog2013stochastic,
  title={Stochastic differential equations},
  author={Herzog, Florian},
  journal={lecture notes},
  year={2013}
}

@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{li2021validity,
  title={On the validity of modeling sgd with stochastic differential equations (sdes)},
  author={Li, Zhiyuan and Malladi, Sadhika and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12712--12725},
  year={2021}
}

@article{keskar2017improving,
  title={Improving generalization performance by switching from adam to sgd},
  author={Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07628},
  year={2017}
}

@article{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@article{bartlett2022dynamics,
  title={The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima},
  author={Bartlett, Peter L and Long, Philip M and Bousquet, Olivier},
  journal={arXiv preprint arXiv:2210.01513},
  year={2022}
}

@article{wen2022does,
  title={How Does Sharpness-Aware Minimization Minimize Sharpness?},
  author={Wen, Kaiyue and Ma, Tengyu and Li, Zhiyuan},
  journal={arXiv preprint arXiv:2211.05729},
  year={2022}
}

@inproceedings{andriushchenko2022towards,
  title={Towards understanding sharpness-aware minimization},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={639--668},
  year={2022},
  organization={PMLR}
}

@inproceedings{kwon2021asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}

@article{du2021efficient,
  title={Efficient sharpness-aware minimization for improved training of neural networks},
  author={Du, Jiawei and Yan, Hanshu and Feng, Jiashi and Zhou, Joey Tianyi and Zhen, Liangli and Goh, Rick Siow Mong and Tan, Vincent YF},
  journal={arXiv preprint arXiv:2110.03141},
  year={2021}
}

@article{ni2022k,
  title={K-SAM: Sharpness-Aware Minimization at the Speed of SGD},
  author={Ni, Renkun and Chiang, Ping-yeh and Geiping, Jonas and Goldblum, Micah and Wilson, Andrew Gordon and Goldstein, Tom},
  journal={arXiv preprint arXiv:2210.12864},
  year={2022}
}

@article{barrett2020implicit,
  title={Implicit gradient regularization},
  author={Barrett, David GT and Dherin, Benoit},
  journal={arXiv preprint arXiv:2009.11162},
  year={2020}
}

@article{damian2022self,
  title={Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability},
  author={Damian, Alex and Nichani, Eshaan and Lee, Jason D},
  journal={arXiv preprint arXiv:2209.15594},
  year={2022}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@misc{ma2022beyond,
  url = {https://arxiv.org/abs/2204.11326},
  author = {Ma, Chao and Kunin, Daniel and Wu, Lei and Ying, Lexing},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Beyond the Quadratic Approximation: the Multiscale Structure of Neural Network Loss Landscapes},
  year = {2022},
}

@article{li2021happens,
  title={What Happens after SGD Reaches Zero Loss?--A Mathematical Framework},
  author={Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2110.06914},
  year={2021}
}

@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{damian2021label,
  title={Label noise sgd provably prefers flat global minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27449--27461},
  year={2021}
}

@inproceedings{haochen2021shape,
  title={Shape matters: Understanding the implicit bias of the noise covariance},
  author={HaoChen, Jeff Z and Wei, Colin and Lee, Jason and Ma, Tengyu},
  booktitle={Conference on Learning Theory},
  pages={2315--2357},
  year={2021},
  organization={PMLR}
}

@article{lyu2022understanding,
  title={Understanding the generalization benefit of normalization layers: Sharpness reduction},
  author={Lyu, Kaifeng and Li, Zhiyuan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2206.07085},
  year={2022}
}

@article{arora2022understanding,
  title={Understanding Gradient Descent on Edge of Stability in Deep Learning},
  author={Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  journal={arXiv preprint arXiv:2205.09745},
  year={2022}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={arXiv preprint arXiv:1912.02178},
  year={2019}
}

@article{he2019asymmetric,
  title={Asymmetric valleys: Beyond sharp and flat local minima},
  author={He, Haowei and Huang, Gao and Yuan, Yang},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{simsek2021geometry,
  title={Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances},
  author={Simsek, Berfin and Ged, Fran{\c{c}}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gerstner, Wulfram and Brea, Johanni},
  booktitle={International Conference on Machine Learning},
  pages={9722--9732},
  year={2021},
  organization={PMLR}
}

@article{brea2019weight,
  title={Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape},
  author={Brea, Johanni and Simsek, Berfin and Illing, Bernd and Gerstner, Wulfram},
  journal={arXiv preprint arXiv:1907.02911},
  year={2019}
}

@article{kunin2020neural,
  title={Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics},
  author={Kunin, Daniel and Sagastuy-Brena, Javier and Ganguli, Surya and Yamins, Daniel LK and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2012.04728},
  year={2020}
}

@inproceedings{kunin2019loss,
  title={Loss landscapes of regularized linear autoencoders},
  author={Kunin, Daniel and Bloom, Jonathan and Goeva, Aleksandrina and Seed, Cotton},
  booktitle={International Conference on Machine Learning},
  pages={3560--3569},
  year={2019},
  organization={PMLR}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{kaur2022maximum,
  title={On the maximum hessian eigenvalue and generalization},
  author={Kaur, Simran and Cohen, Jeremy and Lipton, Zachary C},
  journal={arXiv preprint arXiv:2206.10654},
  year={2022}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@article{dauphin2014identifying,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{baldi1989neural,
  title={Neural networks and principal component analysis: Learning from examples without local minima},
  author={Baldi, Pierre and Hornik, Kurt},
  journal={Neural networks},
  volume={2},
  number={1},
  pages={53--58},
  year={1989},
  publisher={Elsevier}
}

@article{lampinen2018analytic,
  title={An analytic theory of generalization dynamics and transfer learning in deep linear networks},
  author={Lampinen, Andrew K and Ganguli, Surya},
  journal={arXiv preprint arXiv:1809.10374},
  year={2018}
}

@article{advani2020high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M and Sompolinsky, Haim},
  journal={Neural Networks},
  volume={132},
  pages={428--446},
  year={2020},
  publisher={Elsevier}
}

@article{fukumizu1998effect,
  title={Effect of batch learning in multilayer neural networks},
  author={Fukumizu, Kenji},
  journal={Gen},
  volume={1},
  number={04},
  pages={1E--03},
  year={1998}
}

@inproceedings{tarmoun2021understanding,
  title={Understanding the dynamics of gradient flow in overparameterized linear models},
  author={Tarmoun, Salma and Franca, Guilherme and Haeffele, Benjamin D and Vidal, Rene},
  booktitle={International Conference on Machine Learning},
  pages={10153--10161},
  year={2021},
  organization={PMLR}
}

@inproceedings{braun2022exact,
  title={Exact learning dynamics of deep linear networks with prior knowledge},
  author={Braun, Lukas and Domin{\'e}, Cl{\'e}mentine Carla Juliette and Fitzgerald, James E and Saxe, Andrew M},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{goldt2019dynamics,
  title={Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
  author={Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{refinetti2022dynamics,
  title={The dynamics of representation learning in shallow, non-linear autoencoders},
  author={Refinetti, Maria and Goldt, Sebastian},
  journal={arXiv preprint arXiv:2201.02115},
  year={2022}
}

@article{zhu2022understanding,
  title={Understanding Edge-of-Stability Training Dynamics with a Minimalist Example},
  author={Zhu, Xingyu and Wang, Zixuan and Wang, Xiang and Zhou, Mo and Ge, Rong},
  journal={arXiv preprint arXiv:2210.03294},
  year={2022}
}

@article{benaych2012singular,
  title={The singular values and vectors of low rank perturbations of large rectangular random matrices},
  author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal={Journal of Multivariate Analysis},
  volume={111},
  pages={120--135},
  year={2012},
  publisher={Elsevier}
}

@article{ahn2022learning,
  title={Learning threshold neurons via the" edge of stability"},
  author={Ahn, Kwangjun and Bubeck, S{\'e}bastien and Chewi, Sinho and Lee, Yin Tat and Suarez, Felipe and Zhang, Yi},
  journal={arXiv preprint arXiv:2212.07469},
  year={2022}
}

@inproceedings{tian2021understanding,
  title={Understanding self-supervised learning dynamics without contrastive pairs},
  author={Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={10268--10278},
  year={2021},
  organization={PMLR}
}

@article{tipping1999probabilistic,
  title={Probabilistic principal component analysis},
  author={Tipping, Michael E and Bishop, Christopher M},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={61},
  number={3},
  pages={611--622},
  year={1999},
  publisher={Wiley Online Library}
}

@article{wang2021large,
  title={Large learning rate tames homogeneity: Convergence and balancing effect},
  author={Wang, Yuqing and Chen, Minshuo and Zhao, Tuo and Tao, Molei},
  journal={arXiv preprint arXiv:2110.03677},
  year={2021}
}

@article{du2018algorithmic,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018},
  organization={PMLR}
}

@article{moroshko2020implicit,
  title={Implicit bias in deep linear classification: Initialization scale vs training accuracy},
  author={Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22182--22193},
  year={2020}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{pesme2021implicit,
  title={Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29218--29230},
  year={2021}
}

@inproceedings{nacson2022implicit,
  title={Implicit bias of the step size in linear diagonal neural networks},
  author={Nacson, Mor Shpigel and Ravichandran, Kavya and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={16270--16295},
  year={2022},
  organization={PMLR}
}

@inproceedings{mel2021theory,
  title={A theory of high dimensional regression with arbitrary correlations between input features and target functions: sample complexity, multiple descent curves and a hierarchy of phase transitions},
  author={Mel, Gabriel and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={7578--7587},
  year={2021},
  organization={PMLR}
}

@article{stephenson2021and,
  title={When and how epochwise double descent happens},
  author={Stephenson, Cory and Lee, Tyler},
  journal={arXiv preprint arXiv:2108.12006},
  year={2021}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}

@article{li2022implicit,
  title={Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent},
  author={Li, Zhiyuan and Wang, Tianhao and Lee, Jason D and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34626--34640},
  year={2022}
}

@inproceedings{gunasekar2021mirrorless,
  title={Mirrorless mirror descent: A natural derivation of mirror descent},
  author={Gunasekar, Suriya and Woodworth, Blake and Srebro, Nathan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2305--2313},
  year={2021},
  organization={PMLR}
}

@inproceedings{azulay2021implicit,
  title={On the implicit bias of initialization shape: Beyond infinitesimal mirror descent},
  author={Azulay, Shahar and Moroshko, Edward and Nacson, Mor Shpigel and Woodworth, Blake E and Srebro, Nathan and Globerson, Amir and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={468--477},
  year={2021},
  organization={PMLR}
}

@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@inproceedings{vardi2021implicit,
  title={Implicit regularization in relu networks with the square loss},
  author={Vardi, Gal and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={4224--4258},
  year={2021},
  organization={PMLR}
}

@article{bordelon2022self,
  title={Self-consistent dynamical field theory of kernel evolution in wide neural networks},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32240--32256},
  year={2022}
}

@article{xu2024does,
  title={When Does Feature Learning Happen? Perspective from an Analytically Solvable Model},
  author={Xu, Yizhou and Ziyin, Liu},
  journal={arXiv preprint arXiv:2401.07085},
  year={2024}
}

@inproceedings{atanasov2021neural,
  title={Neural Networks as Kernel Learners: The Silent Alignment Effect},
  author={Atanasov, Alexander and Bordelon, Blake and Pehlevan, Cengiz},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{hanin2019complexity,
  title={Complexity of linear regions in deep networks},
  author={Hanin, Boris and Rolnick, David},
  booktitle={International Conference on Machine Learning},
  pages={2596--2604},
  year={2019},
  organization={PMLR}
}

@article{hanin2019deep,
  title={Deep relu networks have surprisingly few activation patterns},
  author={Hanin, Boris and Rolnick, David},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  year={2015}
}

@inproceedings{serra2018bounding,
  title={Bounding and counting linear regions of deep neural networks},
  author={Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
  booktitle={International Conference on Machine Learning},
  pages={4558--4566},
  year={2018},
  organization={PMLR}
}

@inproceedings{raghu2017expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  booktitle={international conference on machine learning},
  pages={2847--2854},
  year={2017},
  organization={PMLR}
}

@article{boursier2022gradient,
  title={Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs},
  author={Boursier, Etienne and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20105--20118},
  year={2022}
}

@article{min2023early,
  title={Early neuron alignment in two-layer relu networks with small initialization},
  author={Min, Hancheng and Vidal, Ren{\'e} and Mallada, Enrique},
  journal={arXiv preprint arXiv:2307.12851},
  year={2023}
}

@inproceedings{phuong2020inductive,
  title={The inductive bias of ReLU networks on orthogonally separable data},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{maennel2018gradient,
  title={Gradient descent quantizes relu network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}

@article{jacot2021saddle,
  title={Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity},
  author={Jacot, Arthur and Ged, Fran{\c{c}}ois and {\c{S}}im{\c{s}}ek, Berfin and Hongler, Cl{\'e}ment and Gabriel, Franck},
  journal={arXiv preprint arXiv:2106.15933},
  year={2021}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{li2020towards,
  title={Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning},
  author={Li, Zhiyuan and Luo, Yuping and Lyu, Kaifeng},
  journal={arXiv preprint arXiv:2012.09839},
  year={2020}
}


@inproceedings{abbe2023sgd,
  title={Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  author={Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2552--2623},
  year={2023},
  organization={PMLR}
}

@inproceedings{saxe2022neural,
  title={The neural race reduction: Dynamics of abstraction in gated networks},
  author={Saxe, Andrew and Sodhani, Shagun and Lewallen, Sam Jay},
  booktitle={International Conference on Machine Learning},
  pages={19287--19309},
  year={2022},
  organization={PMLR}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{fort2020deep,
  title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel},
  author={Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5850--5861},
  year={2020}
}

@article{geiger2020disentangling,
  title={Disentangling feature and lazy training in deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={11},
  pages={113301},
  year={2020},
  publisher={IOP Publishing}
}

@article{domine2023exact,
  title={Exact learning dynamics of deep linear networks with prior knowledge},
  author={Domin{\'e}, Cl{\'e}mentine CJ and Braun, Lukas and Fitzgerald, James E and Saxe, Andrew M},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2023},
  number={11},
  pages={114004},
  year={2023},
  publisher={IOP Publishing}
}

@inproceedings{novak2022fast,
    title={Fast Finite Width Neural Tangent Kernel},
    author={Roman Novak and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Machine Learning},
    year={2022},
    pdf={https://arxiv.org/abs/2206.08720},
    url={https://github.com/google/neural-tangents}
}

@article{frei2023random,
  title={Random feature amplification: Feature learning and generalization in neural networks},
  author={Frei, Spencer and Chatterji, Niladri S and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={303},
  pages={1--49},
  year={2023}
}

@inproceedings{xu2023benign,
  title={Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data},
  author={Xu, Zhiwei and Wang, Yutong and Frei, Spencer and Vardi, Gal and Hu, Wei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{baratin2021implicit,
  title={Implicit regularization via neural feature alignment},
  author={Baratin, Aristide and George, Thomas and Laurent, C{\'e}sar and Hjelm, R Devon and Lajoie, Guillaume and Vincent, Pascal and Lacoste-Julien, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2269--2277},
  year={2021},
  organization={PMLR}
}

@article{cortes2012algorithms,
  title={Algorithms for learning kernels based on centered alignment},
  author={Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={795--828},
  year={2012},
  publisher={JMLR. org}
}

@article{gidel2019implicit,
  title={Implicit regularization of discrete gradient dynamics in linear neural networks},
  author={Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{gissin2019implicit,
  title={The implicit bias of depth: How incremental learning drives generalization},
  author={Gissin, Daniel and Shalev-Shwartz, Shai and Daniely, Amit},
  journal={arXiv preprint arXiv:1909.12051},
  year={2019}
}

@article{kunin2022asymmetric,
  title={The asymmetric maximum margin bias of quasi-homogeneous neural networks},
  author={Kunin, Daniel and Yamamura, Atsushi and Ma, Chao and Ganguli, Surya},
  journal={arXiv preprint arXiv:2210.03820},
  year={2022}
}

@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}

@article{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@article{rotskoff2022trainability,
  title={Trainability and accuracy of artificial neural networks: An interacting particle system approach},
  author={Rotskoff, Grant and Vanden-Eijnden, Eric},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={9},
  pages={1889--1935},
  year={2022},
  publisher={Wiley Online Library}
}

@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A law of large numbers},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={SIAM Journal on Applied Mathematics},
  volume={80},
  number={2},
  pages={725--752},
  year={2020},
  publisher={SIAM}
}

@article{yang2023spectral,
  title={A spectral condition for feature learning},
  author={Yang, Greg and Simon, James B and Bernstein, Jeremy},
  journal={arXiv preprint arXiv:2310.17813},
  year={2023}
}

@article{yang2020feature,
  title={Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on learning theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{luo2021phase,
  title={Phase diagram for two-layer relu neural networks at infinite-width limit},
  author={Luo, Tao and Xu, Zhi-Qin John and Ma, Zheng and Zhang, Yaoyu},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={71},
  pages={1--47},
  year={2021}
}

@article{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}

@inproceedings{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International conference on machine learning},
  pages={244--253},
  year={2018},
  organization={PMLR}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{nacson2019lexicographic,
  title={Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models},
  author={Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={4683--4692},
  year={2019},
  organization={PMLR}
}

@article{saad1995exact,
  title={Exact solution for on-line learning in multilayer neural networks},
  author={Saad, David and Solla, Sara A},
  journal={Physical Review Letters},
  volume={74},
  number={21},
  pages={4337},
  year={1995},
  publisher={APS}
}

@article{goldt2019generalisation,
  title={Generalisation dynamics of online learning in over-parameterised neural networks},
  author={Goldt, Sebastian and Advani, Madhu S and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1901.09085},
  year={2019}
}

@article{neyshabur2015path,
  title={Path-sgd: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{pascanu2013number,
  title={On the number of response regions of deep feed forward networks with piece-wise linear activations},
  author={Pascanu, Razvan and Montufar, Guido and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6098},
  year={2013}
}

@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}

@article{pezeshki2021gradient,
  title={Gradient starvation: A learning proclivity in neural networks},
  author={Pezeshki, Mohammad and Kaba, Oumar and Bengio, Yoshua and Courville, Aaron C and Precup, Doina and Lajoie, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1256--1272},
  year={2021}
}

@article{liu2023connectivity,
  title={How connectivity structure shapes rich and lazy learning in neural circuits},
  author={Liu, Yuhan Helena and Baratin, Aristide and Cornford, Jonathan and Mihalas, Stefan and Shea-Brown, Eric and Lajoie, Guillaume},
  journal={ArXiv},
  year={2023},
  publisher={arXiv}
}

@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}

@article{ziyin2022exact,
  title={Exact solutions of a deep linear network},
  author={Ziyin, Liu and Li, Botao and Meng, Xiangming},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24446--24458},
  year={2022}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International conference on machine learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine learning},
  volume={109},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@book{neal2012bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{lehky1988network,
  title={Network model of shape-from-shading: neural function arises from both receptive and projective fields},
  author={Lehky, Sidney R and Sejnowski, Terrence J},
  journal={Nature},
  volume={333},
  number={6172},
  pages={452--454},
  year={1988},
  publisher={Nature Publishing Group UK London}
}

@article{chen2024stochastic,
  title={Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks},
  author={Chen, Feng and Kunin, Daniel and Yamamura, Atsushi and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{petrini2023deep,
  title={How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model},
  author={Petrini, Leonardo and Cagnetta, Francesco and Tomasini, Umberto M and Favero, Alessandro and Wyart, Matthieu},
  journal={arXiv preprint arXiv:2307.02129},
  year={2023}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{kumar2023grokking,
  title={Grokking as the transition from lazy to rich training dynamics},
  author={Kumar, Tanishq and Bordelon, Blake and Gershman, Samuel J and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2310.06110},
  year={2023}
}

@inproceedings{lyu2023dichotomy,
  title={Dichotomy of early and late phase implicit biases can provably induce grokking},
  author={Lyu, Kaifeng and Jin, Jikai and Li, Zhiyuan and Du, Simon Shaolei and Lee, Jason D and Hu, Wei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@article{krizhevsky2017imagenet,
  title={ImageNet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@article{saxe2019mathematical,
  title={A mathematical theory of semantic development in deep neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={23},
  pages={11537--11546},
  year={2019},
  publisher={National Acad Sciences}
}

@article{tanaka2021noether,
  title={Noetherâ€™s learning dynamics: Role of symmetry breaking in neural networks},
  author={Tanaka, Hidenori and Kunin, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25646--25660},
  year={2021}
}

@article{ba2022high,
  title={High-dimensional asymptotics of feature learning: How one gradient step improves the representation},
  author={Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37932--37946},
  year={2022}
}

@article{cui2024asymptotics,
  title={Asymptotics of feature learning in two-layer networks after one gradient-step},
  author={Cui, Hugo and Pesce, Luca and Dandi, Yatin and Krzakala, Florent and Lu, Yue M and Zdeborov{\'a}, Lenka and Loureiro, Bruno},
  journal={arXiv preprint arXiv:2402.04980},
  year={2024}
}

@article{zhu2023catapults,
  title={Catapults in sgd: spikes in the training loss and their impact on generalization through feature learning},
  author={Zhu, Libin and Liu, Chaoyue and Radhakrishnan, Adityanarayanan and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2306.04815},
  year={2023}
}

@article{radhakrishnan2024mechanism,
  title={Mechanism for feature learning in neural networks and backpropagation-free machine learning models},
  author={Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
  journal={Science},
  volume={383},
  number={6690},
  pages={1461--1467},
  year={2024},
  publisher={American Association for the Advancement of Science}
}

@article{wang2024understanding,
  title={Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks},
  author={Wang, Mingze and Ma, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wang2022early,
  title={Early stage convergence and global convergence of training mildly parameterized neural networks},
  author={Wang, Mingze and Ma, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={743--756},
  year={2022}
}

@article{fiat2019decoupling,
  title={Decoupling gating from linearity},
  author={Fiat, Jonathan and Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1906.05032},
  year={2019}
}

@article{yang2020tensor,
  title={Tensor programs ii: Neural tangent kernel for any architecture},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:2006.14548},
  year={2020}
}

@inproceedings{
rubin2024grokking,
title={Grokking as a First Order Phase Transition in Two Layer Networks},
author={Noa Rubin and Inbar Seroussi and Zohar Ringel},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=3ROGsTX3IR}
}

@article{ziyin2024implicit,
  title={The Implicit Bias of Gradient Noise: A Symmetry Perspective},
  author={Ziyin, Liu and Wang, Mingze and Wu, Lei},
  journal={arXiv preprint arXiv:2402.07193},
  year={2024}
}

@article{tu2024mixed,
  title={Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes},
  author={Tu, Zhenfeng and Aranguri, Santiago and Jacot, Arthur},
  journal={arXiv preprint arXiv:2405.17580},
  year={2024}
}

@article{varre2024spectral,
  title={On the spectral bias of two-layer linear networks},
  author={Varre, Aditya Vardhan and Vladarean, Maria-Luiza and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}