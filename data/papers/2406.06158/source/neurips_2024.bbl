\begin{thebibliography}{89}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pages 1675--1685. PMLR, 2019.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and Liang]{allen2019learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks, going beyond two layers.
\newblock \emph{Advances in neural information processing systems}, 32, 2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International conference on machine learning}, pages 242--252. PMLR, 2019{\natexlab{b}}.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine learning}, 109:\penalty0 467--492, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Yang(2020)]{yang2020tensor}
Greg Yang.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock \emph{arXiv preprint arXiv:2006.14548}, 2020.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Azulay et~al.(2021)Azulay, Moroshko, Nacson, Woodworth, Srebro, Globerson, and Soudry]{azulay2021implicit}
Shahar Azulay, Edward Moroshko, Mor~Shpigel Nacson, Blake~E Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry.
\newblock On the implicit bias of initialization shape: Beyond infinitesimal mirror descent.
\newblock In \emph{International Conference on Machine Learning}, pages 468--477. PMLR, 2021.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Saxe et~al.(2019)Saxe, McClelland, and Ganguli]{saxe2019mathematical}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock A mathematical theory of semantic development in deep neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0 (23):\penalty0 11537--11546, 2019.

\bibitem[Jacot et~al.(2021)Jacot, Ged, {\c{S}}im{\c{s}}ek, Hongler, and Gabriel]{jacot2021saddle}
Arthur Jacot, Fran{\c{c}}ois Ged, Berfin {\c{S}}im{\c{s}}ek, Cl{\'e}ment Hongler, and Franck Gabriel.
\newblock Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity.
\newblock \emph{arXiv preprint arXiv:2106.15933}, 2021.

\bibitem[Li et~al.(2020)Li, Luo, and Lyu]{li2020towards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning.
\newblock \emph{arXiv preprint arXiv:2012.09839}, 2020.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese, Golan, Soudry, and Srebro]{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, pages 3635--3673. PMLR, 2020.

\bibitem[Liu et~al.(2023)Liu, Baratin, Cornford, Mihalas, Shea-Brown, and Lajoie]{liu2023connectivity}
Yuhan~Helena Liu, Aristide Baratin, Jonathan Cornford, Stefan Mihalas, Eric Shea-Brown, and Guillaume Lajoie.
\newblock How connectivity structure shapes rich and lazy learning in neural circuits.
\newblock \emph{ArXiv}, 2023.

\bibitem[Yang and Hu(2020)]{yang2020feature}
Greg Yang and Edward~J Hu.
\newblock Feature learning in infinite-width neural networks.
\newblock \emph{arXiv preprint arXiv:2011.14522}, 2020.

\bibitem[Luo et~al.(2021)Luo, Xu, Ma, and Zhang]{luo2021phase}
Tao Luo, Zhi-Qin~John Xu, Zheng Ma, and Yaoyu Zhang.
\newblock Phase diagram for two-layer relu neural networks at infinite-width limit.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (71):\penalty0 1--47, 2021.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
Greg Yang, Edward~J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and Gur-Ari]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult mechanism.
\newblock \emph{arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Ba et~al.(2022)Ba, Erdogdu, Suzuki, Wang, Wu, and Yang]{ba2022high}
Jimmy Ba, Murat~A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.
\newblock High-dimensional asymptotics of feature learning: How one gradient step improves the representation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 37932--37946, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Liu, Radhakrishnan, and Belkin]{zhu2023catapults}
Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin.
\newblock Catapults in sgd: spikes in the training loss and their impact on generalization through feature learning.
\newblock \emph{arXiv preprint arXiv:2306.04815}, 2023.

\bibitem[Cui et~al.(2024)Cui, Pesce, Dandi, Krzakala, Lu, Zdeborov{\'a}, and Loureiro]{cui2024asymptotics}
Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue~M Lu, Lenka Zdeborov{\'a}, and Bruno Loureiro.
\newblock Asymptotics of feature learning in two-layer networks after one gradient-step.
\newblock \emph{arXiv preprint arXiv:2402.04980}, 2024.

\bibitem[Xu and Ziyin(2024)]{xu2024does}
Yizhou Xu and Liu Ziyin.
\newblock When does feature learning happen? perspective from an analytically solvable model.
\newblock \emph{arXiv preprint arXiv:2401.07085}, 2024.

\bibitem[Cortes et~al.(2012)Cortes, Mohri, and Rostamizadeh]{cortes2012algorithms}
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh.
\newblock Algorithms for learning kernels based on centered alignment.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0 (1):\penalty0 795--828, 2012.

\bibitem[Geiger et~al.(2020)Geiger, Spigler, Jacot, and Wyart]{geiger2020disentangling}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2020\penalty0 (11):\penalty0 113301, 2020.

\bibitem[Baratin et~al.(2021)Baratin, George, Laurent, Hjelm, Lajoie, Vincent, and Lacoste-Julien]{baratin2021implicit}
Aristide Baratin, Thomas George, C{\'e}sar Laurent, R~Devon Hjelm, Guillaume Lajoie, Pascal Vincent, and Simon Lacoste-Julien.
\newblock Implicit regularization via neural feature alignment.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 2269--2277. PMLR, 2021.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and Ganguli]{fort2020deep}
Stanislav Fort, Gintare~Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel~M Roy, and Surya Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 5850--5861, 2020.

\bibitem[Lampinen and Ganguli(2018)]{lampinen2018analytic}
Andrew~K Lampinen and Surya Ganguli.
\newblock An analytic theory of generalization dynamics and transfer learning in deep linear networks.
\newblock \emph{arXiv preprint arXiv:1809.10374}, 2018.

\bibitem[Fukumizu(1998)]{fukumizu1998effect}
Kenji Fukumizu.
\newblock Effect of batch learning in multilayer neural networks.
\newblock \emph{Gen}, 1\penalty0 (04):\penalty0 1E--03, 1998.

\bibitem[Braun et~al.(2022)Braun, Domin{\'e}, Fitzgerald, and Saxe]{braun2022exact}
Lukas Braun, Cl{\'e}mentine Carla~Juliette Domin{\'e}, James~E Fitzgerald, and Andrew~M Saxe.
\newblock Exact learning dynamics of deep linear networks with prior knowledge.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by overparameterization.
\newblock In \emph{International conference on machine learning}, pages 244--253. PMLR, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Ziyin et~al.(2022)Ziyin, Li, and Meng]{ziyin2022exact}
Liu Ziyin, Botao Li, and Xiangming Meng.
\newblock Exact solutions of a deep linear network.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24446--24458, 2022.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Tarmoun et~al.(2021)Tarmoun, Franca, Haeffele, and Vidal]{tarmoun2021understanding}
Salma Tarmoun, Guilherme Franca, Benjamin~D Haeffele, and Rene Vidal.
\newblock Understanding the dynamics of gradient flow in overparameterized linear models.
\newblock In \emph{International Conference on Machine Learning}, pages 10153--10161. PMLR, 2021.

\bibitem[Gissin et~al.(2019)Gissin, Shalev-Shwartz, and Daniely]{gissin2019implicit}
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely.
\newblock The implicit bias of depth: How incremental learning drives generalization.
\newblock \emph{arXiv preprint arXiv:1909.12051}, 2019.

\bibitem[Atanasov et~al.(2021)Atanasov, Bordelon, and Pehlevan]{atanasov2021neural}
Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan.
\newblock Neural networks as kernel learners: The silent alignment effect.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0 (1):\penalty0 2822--2878, 2018.

\bibitem[Ji and Telgarsky(2018)]{ji2018gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{arXiv preprint arXiv:1810.02032}, 2018.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018{\natexlab{a}}.

\bibitem[Moroshko et~al.(2020)Moroshko, Woodworth, Gunasekar, Lee, Srebro, and Soudry]{moroshko2020implicit}
Edward Moroshko, Blake~E Woodworth, Suriya Gunasekar, Jason~D Lee, Nati Srebro, and Daniel Soudry.
\newblock Implicit bias in deep linear classification: Initialization scale vs training accuracy.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 22182--22193, 2020.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:1906.05890}, 2019.

\bibitem[Nacson et~al.(2019)Nacson, Gunasekar, Lee, Srebro, and Soudry]{nacson2019lexicographic}
Mor~Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry.
\newblock Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models.
\newblock In \emph{International Conference on Machine Learning}, pages 4683--4692. PMLR, 2019.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.
\newblock In \emph{Conference on learning theory}, pages 1305--1338. PMLR, 2020.

\bibitem[Kunin et~al.(2022)Kunin, Yamamura, Ma, and Ganguli]{kunin2022asymmetric}
Daniel Kunin, Atsushi Yamamura, Chao Ma, and Surya Ganguli.
\newblock The asymmetric maximum margin bias of quasi-homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:2210.03820}, 2022.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages 1832--1841. PMLR, 2018{\natexlab{b}}.

\bibitem[Gunasekar et~al.(2021)Gunasekar, Woodworth, and Srebro]{gunasekar2021mirrorless}
Suriya Gunasekar, Blake Woodworth, and Nathan Srebro.
\newblock Mirrorless mirror descent: A natural derivation of mirror descent.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 2305--2313. PMLR, 2021.

\bibitem[Li et~al.(2022)Li, Wang, Lee, and Arora]{li2022implicit}
Zhiyuan Li, Tianhao Wang, Jason~D Lee, and Sanjeev Arora.
\newblock Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 34626--34640, 2022.

\bibitem[Maennel et~al.(2018)Maennel, Bousquet, and Gelly]{maennel2018gradient}
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly.
\newblock Gradient descent quantizes relu network features.
\newblock \emph{arXiv preprint arXiv:1803.08367}, 2018.

\bibitem[Phuong and Lampert(2020)]{phuong2020inductive}
Mary Phuong and Christoph~H Lampert.
\newblock The inductive bias of relu networks on orthogonally separable data.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{lyu2021gradient}
Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora.
\newblock Gradient descent on two-layer nets: Margin maximization and simplicity bias.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Boursier et~al.(2022)Boursier, Pillaud-Vivien, and Flammarion]{boursier2022gradient}
Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 20105--20118, 2022.

\bibitem[Wang and Ma(2022)]{wang2022early}
Mingze Wang and Chao Ma.
\newblock Early stage convergence and global convergence of training mildly parameterized neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 743--756, 2022.

\bibitem[Min et~al.(2023)Min, Vidal, and Mallada]{min2023early}
Hancheng Min, Ren{\'e} Vidal, and Enrique Mallada.
\newblock Early neuron alignment in two-layer relu networks with small initialization.
\newblock \emph{arXiv preprint arXiv:2307.12851}, 2023.

\bibitem[Wang and Ma(2024)]{wang2024understanding}
Mingze Wang and Chao Ma.
\newblock Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0 (33):\penalty0 E7665--E7671, 2018.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized models using optimal transport.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Sirignano and Spiliopoulos(2020)]{sirignano2020mean}
Justin Sirignano and Konstantinos Spiliopoulos.
\newblock Mean field analysis of neural networks: A law of large numbers.
\newblock \emph{SIAM Journal on Applied Mathematics}, 80\penalty0 (2):\penalty0 725--752, 2020.

\bibitem[Rotskoff and Vanden-Eijnden(2022)]{rotskoff2022trainability}
Grant Rotskoff and Eric Vanden-Eijnden.
\newblock Trainability and accuracy of artificial neural networks: An interacting particle system approach.
\newblock \emph{Communications on Pure and Applied Mathematics}, 75\penalty0 (9):\penalty0 1889--1935, 2022.

\bibitem[Bordelon and Pehlevan(2022)]{bordelon2022self}
Blake Bordelon and Cengiz Pehlevan.
\newblock Self-consistent dynamical field theory of kernel evolution in wide neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 32240--32256, 2022.

\bibitem[Yang et~al.(2023)Yang, Simon, and Bernstein]{yang2023spectral}
Greg Yang, James~B Simon, and Jeremy Bernstein.
\newblock A spectral condition for feature learning.
\newblock \emph{arXiv preprint arXiv:2310.17813}, 2023.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Hu, and Lee]{du2018algorithmic}
Simon~S Du, Wei Hu, and Jason~D Lee.
\newblock Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018{\natexlab{b}}.

\bibitem[Tu et~al.(2024)Tu, Aranguri, and Jacot]{tu2024mixed}
Zhenfeng Tu, Santiago Aranguri, and Arthur Jacot.
\newblock Mixed dynamics in linear networks: Unifying the lazy and active regimes.
\newblock \emph{arXiv preprint arXiv:2405.17580}, 2024.

\bibitem[Varre et~al.(2024)Varre, Vladarean, Pillaud-Vivien, and Flammarion]{varre2024spectral}
Aditya~Vardhan Varre, Maria-Luiza Vladarean, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock On the spectral bias of two-layer linear networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and Sohl-Dickstein]{raghu2017expressive}
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock On the expressive power of deep neural networks.
\newblock In \emph{international conference on machine learning}, pages 2847--2854. PMLR, 2017.

\bibitem[Krizhevsky et~al.(2017)Krizhevsky, Sutskever, and Hinton]{krizhevsky2017imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Communications of the ACM}, 60\penalty0 (6):\penalty0 84--90, 2017.

\bibitem[Petrini et~al.(2023)Petrini, Cagnetta, Tomasini, Favero, and Wyart]{petrini2023deep}
Leonardo Petrini, Francesco Cagnetta, Umberto~M Tomasini, Alessandro Favero, and Matthieu Wyart.
\newblock How deep neural networks learn compositional data: The random hierarchy model.
\newblock \emph{arXiv preprint arXiv:2307.02129}, 2023.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Kumar et~al.(2023)Kumar, Bordelon, Gershman, and Pehlevan]{kumar2023grokking}
Tanishq Kumar, Blake Bordelon, Samuel~J Gershman, and Cengiz Pehlevan.
\newblock Grokking as the transition from lazy to rich training dynamics.
\newblock \emph{arXiv preprint arXiv:2310.06110}, 2023.

\bibitem[Lyu et~al.(2023)Lyu, Jin, Li, Du, Lee, and Hu]{lyu2023dichotomy}
Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon~Shaolei Du, Jason~D Lee, and Wei Hu.
\newblock Dichotomy of early and late phase implicit biases can provably induce grokking.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Rubin et~al.(2024)Rubin, Seroussi, and Ringel]{rubin2024grokking}
Noa Rubin, Inbar Seroussi, and Zohar Ringel.
\newblock Grokking as a first order phase transition in two layer networks.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=3ROGsTX3IR}.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and Steinhardt]{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock \emph{arXiv preprint arXiv:2301.05217}, 2023.

\bibitem[Saxe et~al.(2022)Saxe, Sodhani, and Lewallen]{saxe2022neural}
Andrew Saxe, Shagun Sodhani, and Sam~Jay Lewallen.
\newblock The neural race reduction: Dynamics of abstraction in gated networks.
\newblock In \emph{International Conference on Machine Learning}, pages 19287--19309. PMLR, 2022.

\bibitem[Kunin et~al.(2020)Kunin, Sagastuy-Brena, Ganguli, Yamins, and Tanaka]{kunin2020neural}
Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel~LK Yamins, and Hidenori Tanaka.
\newblock Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics.
\newblock \emph{arXiv preprint arXiv:2012.04728}, 2020.

\bibitem[Tanaka and Kunin(2021)]{tanaka2021noether}
Hidenori Tanaka and Daniel Kunin.
\newblock Noetherâ€™s learning dynamics: Role of symmetry breaking in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 25646--25660, 2021.

\bibitem[Chen et~al.(2024)Chen, Kunin, Yamamura, and Ganguli]{chen2024stochastic}
Feng Chen, Daniel Kunin, Atsushi Yamamura, and Surya Ganguli.
\newblock Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Ziyin et~al.(2024)Ziyin, Wang, and Wu]{ziyin2024implicit}
Liu Ziyin, Mingze Wang, and Lei Wu.
\newblock The implicit bias of gradient noise: A symmetry perspective.
\newblock \emph{arXiv preprint arXiv:2402.07193}, 2024.

\bibitem[Saad and Solla(1995)]{saad1995exact}
David Saad and Sara~A Solla.
\newblock Exact solution for on-line learning in multilayer neural networks.
\newblock \emph{Physical Review Letters}, 74\penalty0 (21):\penalty0 4337, 1995.

\bibitem[Goldt et~al.(2019)Goldt, Advani, Saxe, Krzakala, and Zdeborov{\'a}]{goldt2019generalisation}
Sebastian Goldt, Madhu~S Advani, Andrew~M Saxe, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Generalisation dynamics of online learning in over-parameterised neural networks.
\newblock \emph{arXiv preprint arXiv:1901.09085}, 2019.

\bibitem[Vardi and Shamir(2021)]{vardi2021implicit}
Gal Vardi and Ohad Shamir.
\newblock Implicit regularization in relu networks with the square loss.
\newblock In \emph{Conference on Learning Theory}, pages 4224--4258. PMLR, 2021.

\bibitem[Pascanu et~al.(2013)Pascanu, Montufar, and Bengio]{pascanu2013number}
Razvan Pascanu, Guido Montufar, and Yoshua Bengio.
\newblock On the number of response regions of deep feed forward networks with piece-wise linear activations.
\newblock \emph{arXiv preprint arXiv:1312.6098}, 2013.

\bibitem[Montufar et~al.(2014)Montufar, Pascanu, Cho, and Bengio]{montufar2014number}
Guido~F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio.
\newblock On the number of linear regions of deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Telgarsky(2015)]{telgarsky2015representation}
Matus Telgarsky.
\newblock Representation benefits of deep feedforward networks.
\newblock \emph{arXiv preprint arXiv:1509.08101}, 2015.

\bibitem[Arora et~al.(2016)Arora, Basu, Mianjy, and Mukherjee]{arora2016understanding}
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee.
\newblock Understanding deep neural networks with rectified linear units.
\newblock \emph{arXiv preprint arXiv:1611.01491}, 2016.

\bibitem[Serra et~al.(2018)Serra, Tjandraatmadja, and Ramalingam]{serra2018bounding}
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam.
\newblock Bounding and counting linear regions of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 4558--4566. PMLR, 2018.

\bibitem[Hanin and Rolnick(2019{\natexlab{a}})]{hanin2019complexity}
Boris Hanin and David Rolnick.
\newblock Complexity of linear regions in deep networks.
\newblock In \emph{International Conference on Machine Learning}, pages 2596--2604. PMLR, 2019{\natexlab{a}}.

\bibitem[Hanin and Rolnick(2019{\natexlab{b}})]{hanin2019deep}
Boris Hanin and David Rolnick.
\newblock Deep relu networks have surprisingly few activation patterns.
\newblock \emph{Advances in neural information processing systems}, 32, 2019{\natexlab{b}}.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 1026--1034, 2015.

\end{thebibliography}
