\begin{thebibliography}{98}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Albergo et~al.(2023)Albergo, Boffi, and Vanden-Eijnden]{albergo2023stochastic}
Michael~S Albergo, Nicholas~M Boffi, and Eric Vanden-Eijnden.
\newblock Stochastic interpolants: A unifying framework for flows and diffusions.
\newblock \emph{arXiv preprint arXiv:2303.08797}, 2023.

\bibitem[Albergo and Vanden-Eijnden(2023)]{albergo2023building}
Michael~Samuel Albergo and Eric Vanden-Eijnden.
\newblock Building normalizing flows with stochastic interpolants.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Amos et~al.(2023)]{amos2023tutorial}
Brandon Amos et~al.
\newblock Tutorial on amortized optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 16\penalty0 (5):\penalty0 592--732, 2023.

\bibitem[Astolfi et~al.(2024)Astolfi, Careil, Hall, Ma{\~n}as, Muckley, Verbeek, Soriano, and Drozdzal]{astolfi2024consistency}
Pietro Astolfi, Marlene Careil, Melissa Hall, Oscar Ma{\~n}as, Matthew Muckley, Jakob Verbeek, Adriana~Romero Soriano, and Michal Drozdzal.
\newblock Consistency-diversity-realism pareto fronts of conditional image generative models.
\newblock \emph{arXiv preprint arXiv:2406.10429}, 2024.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, El-Showk, Elhage, Hatfield-Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022.

\bibitem[Bartosh et~al.(2024{\natexlab{a}})Bartosh, Vetrov, and Naesseth]{bartosh2024neural}
Grigory Bartosh, Dmitry Vetrov, and Christian~A. Naesseth.
\newblock Neural diffusion models.
\newblock \emph{arXiv preprint arXiv:2310.08337}, 2024{\natexlab{a}}.

\bibitem[Bartosh et~al.(2024{\natexlab{b}})Bartosh, Vetrov, and Naesseth]{bartosh2024neural2}
Grigory Bartosh, Dmitry Vetrov, and Christian~A. Naesseth.
\newblock Neural flow diffusion models: Learnable forward process for improved diffusion modelling.
\newblock \emph{arXiv preprint arXiv:2404.12940}, 2024{\natexlab{b}}.

\bibitem[Bellman(1957)]{bellman1957}
Richard Bellman.
\newblock \emph{Dynamic programming}.
\newblock Princeton Landmarks in Mathematics. Princeton University Press, Princeton, NJ, 2010., 1957.

\bibitem[Ben-Hamu et~al.(2024)Ben-Hamu, Puny, Gat, Karrer, Singer, and Lipman]{benhamu2024dflow}
Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman.
\newblock D-flow: Differentiating through flows for controlled generation.
\newblock \emph{arXiv preprint arXiv:2402.14017}, 2024.

\bibitem[Berner et~al.(2023)Berner, Richter, and Ullrich]{berner2023optimal}
Julius Berner, Lorenz Richter, and Karen Ullrich.
\newblock An optimal control perspective on diffusion-based generative modeling.
\newblock \emph{arXiv preprint arXiv:2211.01364}, 2023.

\bibitem[Bierkens and Kappen(2014)]{bierkens2014explicit}
Joris Bierkens and Hilbert~J Kappen.
\newblock Explicit solution of relative entropy weighted control.
\newblock \emph{Systems \& Control Letters}, 72:\penalty0 36--43, 2014.

\bibitem[Black et~al.(2024)Black, Janner, Du, Kostrikov, and Levine]{black2024training}
Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine.
\newblock Training diffusion models with reinforcement learning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Bruna and Han(2024)]{bruna2024posterior}
Joan Bruna and Jiequn Han.
\newblock Posterior sampling with denoising oracles via tilted transport.
\newblock \emph{arXiv preprint arXiv:2407.00745}, 2024.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{chen2018neural}
Ricky T.~Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Chen et~al.(2021)Chen, Amos, and Nickel]{chen2021learning}
Ricky T.~Q. Chen, Brandon Amos, and Maximilian Nickel.
\newblock Learning neural event functions for ordinary differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Chung et~al.(2022)Chung, Kim, Mccann, Klasky, and Ye]{chung2022diffusion}
Hyungjin Chung, Jeongsol Kim, Michael~T Mccann, Marc~L Klasky, and Jong~Chul Ye.
\newblock Diffusion posterior sampling for general noisy inverse problems.
\newblock \emph{arXiv preprint arXiv:2209.14687}, 2022.

\bibitem[Clark et~al.(2024)Clark, Vicol, Swersky, and Fleet]{clark2024directly}
Kevin Clark, Paul Vicol, Kevin Swersky, and David~J. Fleet.
\newblock Directly fine-tuning diffusion models on differentiable rewards.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[De~Bortoli et~al.(2021)De~Bortoli, Thornton, Heng, and Doucet]{debortoli2021diffusion}
Valentin De~Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet.
\newblock Diffusion schr\"{o}dinger bridge with applications to score-based generative modeling.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~34, pages 17695--17709. Curran Associates, Inc., 2021.

\bibitem[Denker et~al.(2024)Denker, Vargas, Padhy, Didi, Mathis, Dutordoir, Barbano, Mathieu, Komorowska, and Lio]{denker2024deft}
Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula~Julia Komorowska, and Pietro Lio.
\newblock Deft: Efficient finetuning of conditional diffusion models by learning the generalised $h$-transform.
\newblock \emph{arXiv preprint arXiv:2406.01781}, 2024.

\bibitem[Domingo-Enrich(2024)]{domingoenrich2024taxonomy}
Carles Domingo-Enrich.
\newblock A taxonomy of loss functions for stochastic optimal control.
\newblock \emph{arXiv preprint arXiv:2410.00345}, 2024.

\bibitem[Domingo-Enrich et~al.(2023)Domingo-Enrich, Han, Amos, Bruna, and Chen]{domingoenrich2023stochastic}
Carles Domingo-Enrich, Jiequn Han, Brandon Amos, Joan Bruna, and Ricky T.~Q. Chen.
\newblock Stochastic optimal control matching.
\newblock \emph{arXiv preprint arXiv:2312.02027}, 2023.

\bibitem[Du et~al.(2024)Du, Plainer, Brekelmans, Duan, Noé, Gomes, Apsuru-Guzik, and Neklyudov]{du2024doobs}
Yuanqi Du, Michael Plainer, Rob Brekelmans, Chenru Duan, Frank Noé, Carla~P. Gomes, Alan Apsuru-Guzik, and Kirill Neklyudov.
\newblock Doob's lagrangian: A sample-efficient variational approach to transition path sampling.
\newblock \emph{arXiv preprint arXiv:2410.07974}, 2024.

\bibitem[Esser et~al.(2024)Esser, Kulal, Blattmann, Entezari, M{\"u}ller, Saini, Levi, Lorenz, Sauer, Boesel, et~al.]{esser2024scaling}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Fan and Lee(2023)]{fan2023optimizing}
Ying Fan and Kangwook Lee.
\newblock Optimizing ddpm sampling with shortcut fine-tuning.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Fan et~al.(2023)Fan, Watkins, Du, Liu, Ryu, Boutilier, Abbeel, Ghavamzadeh, Lee, and Lee]{fan2023dpok}
Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee.
\newblock Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models.
\newblock \emph{arXiv preprint arXiv:2305.16381}, 2023.

\bibitem[Fan et~al.(2024)Fan, Watkins, Du, Liu, Ryu, Boutilier, Abbeel, Ghavamzadeh, Lee, and Lee]{fan2024reinforcement}
Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee.
\newblock Reinforcement learning for fine-tuning text-to-image diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Fleming and Rishel(2012)]{fleming2012deterministic}
W.H. Fleming and R.W. Rishel.
\newblock \emph{Deterministic and Stochastic Optimal Control}.
\newblock Stochastic Modelling and Applied Probability. Springer New York, 2012.

\bibitem[Fu et~al.(2023)Fu, Tamir, Sundaram, Chai, Zhang, Dekel, and Isola]{fu2023learning}
Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola.
\newblock Dreamsim: Learning new dimensions of human visual similarity using synthetic data.
\newblock \emph{arXiv preprint arXiv:2306.09344}, 2023.

\bibitem[G{\'o}mez et~al.(2014)G{\'o}mez, Kappen, Peters, and Neumann]{gomez2014policy}
Vicen{\c{c}} G{\'o}mez, Hilbert~J Kappen, Jan Peters, and Gerhard Neumann.
\newblock Policy search for path integral control.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge Discovery in Databases}, pages 482--497. Springer, 2014.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Haber and Ruthotto(2017)]{haber2017stable}
Eldad Haber and Lars Ruthotto.
\newblock Stable architectures for deep neural networks.
\newblock \emph{Inverse problems}, 34\penalty0 (1):\penalty0 014004, 2017.

\bibitem[Han and E(2016)]{han2016deep}
Jiequn Han and Weinan E.
\newblock Deep learning approximation for stochastic control problems.
\newblock \emph{arXiv preprint arXiv:1611.07422}, 2016.

\bibitem[Hartmann and Sch{\"u}tte(2012)]{hartmann2012efficient}
Carsten Hartmann and Christof Sch{\"u}tte.
\newblock Efficient rare event simulation by optimal nonequilibrium forcing.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2012\penalty0 (11):\penalty0 P11004, 2012.

\bibitem[Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and Choi]{hessel2021clipscore}
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi.
\newblock Clipscore: A reference-free evaluation metric for image captioning.
\newblock \emph{arXiv preprint arXiv:2104.08718}, 2021.

\bibitem[Ho and Salimans(2022)]{ho2022classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock \emph{arXiv preprint arXiv:2207.12598}, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33. Curran Associates, Inc., 2020.

\bibitem[Huang et~al.(2024)Huang, Ghatare, Liu, Hu, Zhang, Sastry, Gururani, Oore, and Yue]{huang2024symbolic}
Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli~S Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue.
\newblock Symbolic music generation with non-differentiable rule guided diffusion.
\newblock \emph{arXiv preprint arXiv:2402.14285}, 2024.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Wightman, Gordon, Carlini, Taori, Dave, Shankar, Namkoong, Miller, Hajishirzi, Farhadi, and Schmidt]{ilharco2021openclip}
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
\newblock Openclip, July 2021.

\bibitem[Kappen(2005)]{kappen2005path}
H~J Kappen.
\newblock Path integrals and symmetry breaking for optimal control theory.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2005\penalty0 (11), nov 2005.

\bibitem[Kappen et~al.(2012)Kappen, G{\'o}mez, and Opper]{kappen2012optimal}
Hilbert~J Kappen, Vicen{\c{c}} G{\'o}mez, and Manfred Opper.
\newblock Optimal control as a graphical model inference problem.
\newblock \emph{Machine learning}, 87\penalty0 (2):\penalty0 159--182, 2012.

\bibitem[Kingma et~al.(2021)Kingma, Salimans, Poole, and Ho]{kingma2021ondensity}
Diederik~P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock On density estimation with diffusion models.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Kirstain et~al.(2023)Kirstain, Polyak, Singer, Matiana, Penna, and Levy]{kirstain2023pickapic}
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.
\newblock Pick-a-pic: An open dataset of user preferences for text-to-image generation.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Le et~al.(2024)Le, Vyas, Shi, Karrer, Sari, Moritz, Williamson, Manohar, Adi, Mahadeokar, et~al.]{le2024voicebox}
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et~al.
\newblock Voicebox: Text-guided multilingual universal speech generation at scale.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\bibitem[Li(2021)]{li2021differentiable}
Dongzhuo Li.
\newblock Differentiable gaussianization layers for inverse problems regularized by deep generative models.
\newblock \emph{arXiv preprint arXiv:2112.03860}, 2021.

\bibitem[Li et~al.(2020)Li, Wong, Chen, and Duvenaud]{li2020scalable}
Xuechen Li, Ting-Kam~Leonard Wong, Ricky T.~Q. Chen, and David Duvenaud.
\newblock Scalable gradients for stochastic differential equations.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3870--3882. PMLR, 2020.

\bibitem[Lipman et~al.(2023)Lipman, Chen, Ben-Hamu, Nickel, and Le]{lipman2023flow}
Yaron Lipman, Ricky T.~Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le.
\newblock Flow matching for generative modeling.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Liu et~al.(2024)Liu, Lipman, Nickel, Karrer, Theodorou, and Chen]{liu2023generalized}
Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos Theodorou, and Ricky T.~Q. Chen.
\newblock Generalized schr\"odinger bridge matching.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Liu(2022)]{liu2022rectified}
Qiang Liu.
\newblock Rectified flow: A marginal preserving approach to optimal transport.
\newblock \emph{arXiv preprint arXiv:2209.14577}, 2022.

\bibitem[Liu et~al.(2023)Liu, Gong, and qiang liu]{liu2023flow}
Xingchao Liu, Chengyue Gong, and qiang liu.
\newblock Flow straight and fast: Learning to generate and transfer data with rectified flow.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Long et~al.(2015)Long, Shelhamer, and Darrell]{long2015fully}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3431--3440, 2015.

\bibitem[Malkin et~al.(2023)Malkin, Jain, Bengio, Sun, and Bengio]{malkin2023trajectory}
Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio.
\newblock Trajectory balance: Improved credit assignment in gflownets.
\newblock \emph{arXiv preprint arXiv:2201.13259}, 2023.

\bibitem[Maoutsa et~al.(2020)Maoutsa, Reich, and Opper]{maoutsa2020interacting}
Dimitra Maoutsa, Sebastian Reich, and Manfred Opper.
\newblock Interacting particle solutions of fokker--planck equations through gradient--log--density estimation.
\newblock \emph{Entropy}, 22\penalty0 (8):\penalty0 802, 2020.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and Mnih]{mohamed2020monte}
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih.
\newblock Monte carlo gradient estimation in machine learning.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (132):\penalty0 1--62, 2020.

\bibitem[Mordvintsev et~al.(2015)Mordvintsev, Olah, and Tyka]{mordvintsev2015inceptionism}
Alexander Mordvintsev, Christopher Olah, and Mike Tyka.
\newblock Inceptionism: Going deeper into neural networks.
\newblock \emph{Google research blog}, 20\penalty0 (14):\penalty0 5, 2015.

\bibitem[N{\"u}sken and Richter(2021)]{nüsken2023solving}
Nikolas N{\"u}sken and Lorenz Richter.
\newblock Solving high-dimensional {H}amilton--{J}acobi--{B}ellman pdes using neural networks: perspectives from the theory of controlled diffusions and measures on path space.
\newblock \emph{Partial differential equations and applications}, 2:\penalty0 1--48, 2021.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul~F Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 27730--27744. Curran Associates, Inc., 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pokle et~al.(2023)Pokle, Muckley, Chen, and Karrer]{pokle2023training}
Ashwini Pokle, Matthew~J Muckley, Ricky T.~Q. Chen, and Brian Karrer.
\newblock Training-free linear image inversion via flows.
\newblock \emph{arXiv preprint arXiv:2310.04432}, 2023.

\bibitem[Pontryagin(1962)]{pontryagin1962mathematical}
L.S. Pontryagin.
\newblock \emph{The Mathematical Theory of Optimal Processes}.
\newblock Interscience Publishers, 1962.

\bibitem[Pooladian et~al.(2024)Pooladian, Domingo-Enrich, Chen, and Amos]{pooladian2024neural}
Aram-Alexandre Pooladian, Carles Domingo-Enrich, Ricky T.~Q. Chen, and Brandon Amos.
\newblock Neural optimal transport with lagrangian costs.
\newblock \emph{arXiv preprint arXiv:2406.00288}, 2024.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Rawlik et~al.(2013)Rawlik, Toussaint, and Vijayakumar]{rawlik2013stochastic}
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar.
\newblock On stochastic optimal control and reinforcement learning by approximate inference.
\newblock In \emph{Twenty-Third International Joint Conference on Artificial Intelligence}, 2013.

\bibitem[Richter and Berner(2024)]{richter2024improved}
Lorenz Richter and Julius Berner.
\newblock Improved sampling via learned diffusions.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Richter et~al.(2020)Richter, Boustati, N{\"u}sken, Ruiz, and Akyildiz]{richter2020vargrad}
Lorenz Richter, Ayman Boustati, Nikolas N{\"u}sken, Francisco Ruiz, and Omer~Deniz Akyildiz.
\newblock Var{G}rad: A low-variance gradient estimator for variational inference.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Rout et~al.(2024)Rout, Chen, Ruiz, Kumar, Caramanis, Shakkottai, and Chu]{rout2024rb}
Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu.
\newblock Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control.
\newblock \emph{arXiv preprint arXiv:2405.17401}, 2024.

\bibitem[Rubinstein and Kroese(2013)]{rubinstein2013cross}
Reuven~Y Rubinstein and Dirk~P Kroese.
\newblock \emph{The cross-entropy method: a unified approach to combinatorial optimization, {M}onte-{C}arlo simulation and machine learning}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Schuhmann and Beaumont(2022)]{schuhmann2022laion}
Christoph Schuhmann and Romain Beaumont.
\newblock Laion-aesthetics, 2022.

\bibitem[Sethi(2018)]{sethi2018optimal}
S.P. Sethi.
\newblock \emph{Optimal Control Theory: Applications to Management Science and Economics}.
\newblock Springer International Publishing, 2018.

\bibitem[Singer et~al.(2022)Singer, Polyak, Hayes, Yin, An, Zhang, Hu, Yang, Ashual, Gafni, et~al.]{singer2022make}
Uriel Singer, Adam Polyak, Thomas Hayes, Xi~Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock \emph{arXiv preprint arXiv:2209.14792}, 2022.

\bibitem[Song et~al.(2021{\natexlab{a}})Song, Meng, and Ermon]{song2021denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock In \emph{International Conference on Learning Representations}, 2021{\natexlab{a}}.

\bibitem[Song et~al.(2023)Song, Vahdat, Mardani, and Kautz]{song2023pseudoinverse}
Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz.
\newblock Pseudoinverse-guided diffusion models for inverse problems.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{arXiv preprint arXiv:1907.05600}, 2019.

\bibitem[Song et~al.(2021{\natexlab{b}})Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2021scorebased}
Yang Song, Jascha Sohl-Dickstein, Diederik~P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In \emph{International Conference on Learning Representations (ICLR 2021)}, 2021{\natexlab{b}}.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pages 3008--3021. Curran Associates, Inc., 2020.

\bibitem[Suh et~al.(2022)Suh, Simchowitz, Zhang, and Tedrake]{suh2022differentiable}
Hyung~Ju Suh, Max Simchowitz, Kaiqing Zhang, and Russ Tedrake.
\newblock Do differentiable simulators give better policy gradients?
\newblock In \emph{International Conference on Machine Learning}, pages 20668--20696. PMLR, 2022.

\bibitem[Tang(2024)]{tang2024finetuning}
Wenpin Tang.
\newblock Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond.
\newblock \emph{arXiv preprint arXiv:2403.06279}, 2024.

\bibitem[Todorov(2006)]{todorov2006linearly}
Emanuel Todorov.
\newblock Linearly-solvable markov decision problems.
\newblock \emph{Advances in neural information processing systems}, 19, 2006.

\bibitem[Tzen and Raginsky(2019)]{tzen2019theoretical}
Belinda Tzen and Maxim Raginsky.
\newblock Theoretical guarantees for sampling and inference in generative models with latent diffusions.
\newblock \emph{arXiv:1903.01608}, 2019.

\bibitem[Uehara et~al.(2024{\natexlab{a}})Uehara, Zhao, Biancalani, and Levine]{uehara2024understanding}
Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, and Sergey Levine.
\newblock Understanding reinforcement learning-based fine-tuning of diffusion models: A tutorial and review.
\newblock \emph{arXiv preprint arXiv:2407.13734}, 2024{\natexlab{a}}.

\bibitem[Uehara et~al.(2024{\natexlab{b}})Uehara, Zhao, Black, Hajiramezanali, Scalia, Diamant, Tseng, Biancalani, and Levine]{uehara2024finetuning}
Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel~Lee Diamant, Alex~M Tseng, Tommaso Biancalani, and Sergey Levine.
\newblock Fine-tuning of continuous-time diffusion models as entropy-regularized control.
\newblock \emph{arXiv preprint arXiv:2402.15194}, 2024{\natexlab{b}}.

\bibitem[Vargas et~al.(2022)Vargas, Ovsianas, Fernandes, Girolami, Lawrence, and N{\"u}sken]{vargas2022bayesian}
Francisco Vargas, Andrius Ovsianas, David~Lopes Fernandes, Mark Girolami, Neil~D Lawrence, and Nikolas N{\"u}sken.
\newblock Bayesian learning via neural schr\"odinger-f\"ollmer flows.
\newblock In \emph{Fourth Symposium on Advances in Approximate Bayesian Inference}, 2022.

\bibitem[Vargas et~al.(2023)Vargas, Grathwohl, and Doucet]{vargas2023denoising}
Francisco Vargas, Will~Sussman Grathwohl, and Arnaud Doucet.
\newblock Denoising diffusion samplers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674, 2011.

\bibitem[Vyas et~al.(2023)Vyas, Shi, Le, Tjandra, Wu, Guo, Zhang, Zhang, Adkins, Ngan, et~al.]{vyas2023audiobox}
Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang, Robert Adkins, William Ngan, et~al.
\newblock Audiobox: Unified audio generation with natural language prompts.
\newblock \emph{arXiv preprint arXiv:2312.15821}, 2023.

\bibitem[Wallace et~al.(2023{\natexlab{a}})Wallace, Dang, Rafailov, Zhou, Lou, Purushwalkam, Ermon, Xiong, Joty, and Naik]{wallace2023diffusion}
Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik.
\newblock Diffusion model alignment using direct preference optimization.
\newblock \emph{arXiv preprint arXiv:2311.12908}, 2023{\natexlab{a}}.

\bibitem[Wallace et~al.(2023{\natexlab{b}})Wallace, Gokul, Ermon, and Naik]{wallace2023endtoend}
Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik.
\newblock End-to-end diffusion latent optimization improves classifier guidance.
\newblock \emph{arXiv preprint arXiv:2303.13703}, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Trippe, Naesseth, Blei, and Cunningham]{wu2023practical}
Luhuan Wu, Brian Trippe, Christian Naesseth, David Blei, and John~P Cunningham.
\newblock Practical and asymptotically exact conditional sampling in diffusion models.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~36, pages 31372--31403. Curran Associates, Inc., 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Hao, Sun, Chen, Zhu, Zhao, and Li]{wu2023human}
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.
\newblock Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis.
\newblock \emph{arXiv preprint arXiv:2306.09341}, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{c}})Wu, Hao, Sun, Chen, Zhu, Zhao, and Li]{wu2023humanpreferencescorev2}
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.
\newblock Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis.
\newblock \emph{arXiv preprint arXiv:2306.09341}, 2023{\natexlab{c}}.

\bibitem[Xu et~al.(2023)Xu, Liu, Wu, Tong, Li, Ding, Tang, and Dong]{xu2023imagereward}
Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.
\newblock Imagereward: Learning and evaluating human preferences for text-to-image generation.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Zhang, Gu, Zhang, Susskind, Jaitly, and Zhai]{zhang2024improving}
Dinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang Zhang, Josh Susskind, Navdeep Jaitly, and Shuangfei Zhai.
\newblock Improving gflownets for text-to-image diffusion alignment.
\newblock \emph{arXiv preprint arXiv:2406.00633}, 2024.

\bibitem[Zhang and Chen(2022)]{zhang2022path}
Qinsheng Zhang and Yongxin Chen.
\newblock Path integral sampler: A stochastic control approach for sampling.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhang et~al.(2014)Zhang, Wang, Hartmann, Weber, and Schütte]{zhang2014applications}
Wei Zhang, Han Wang, Carsten Hartmann, Marcus Weber, and Christof Schütte.
\newblock Applications of the cross-entropy method to importance sampling and optimal control of diffusions.
\newblock \emph{SIAM Journal on Scientific Computing}, 36\penalty0 (6):\penalty0 A2654--A2672, 2014.

\bibitem[Zheng et~al.(2023)Zheng, Le, Shaul, Lipman, Grover, and Chen]{zheng2023guided}
Qinqing Zheng, Matt Le, Neta Shaul, Yaron Lipman, Aditya Grover, and Ricky T.~Q. Chen.
\newblock Guided flows for generative modeling and decision making.
\newblock \emph{arXiv preprint arXiv:2311.13443}, 2023.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey, et~al.]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, Anind~K Dey, et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA, 2008.

\bibitem[Ziegler et~al.(2020)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{ziegler2020finetuning}
Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{arXiv preprint arXiv:1909.08593}, 2020.

\end{thebibliography}
