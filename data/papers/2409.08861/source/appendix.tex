% \section{Preliminaries}

% \begin{theorem} [Adjoint method for SDEs, Lemma~8 of \cite{domingoenrich2023stochastic}, \cite{li2020scalable,kidger2021neural}]
% \label{lem:adjoint_method_sdes}
%     % Let $X$ be the solution of $dX_t = b(X_t,t) \, dt + \sigma(t) \, dB_t$. Then, if we let $a : \Omega \times [0,T] \to \R^d$ be the solution of 
%     Let $\bm{X} : \Omega \times [0,T] \to \R^d$ be a stochastic process that satisfies the SDE $\mathrm{d}X_t = b(X_t,t) \, \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t$, with initial condition $X_0 = x$.
%     We define the random process $a : \Omega \times [0,T] \to \R^{d}$ such that for all $\omega \in \Omega$, using the short-hand $a(t) := a(\omega,t)$,
%     \begin{talign}
%         da_t(\omega) &= \big( - \nabla_{x} b(X_t(\omega),t) a_t(\omega) - \nabla_x f(X_t(\omega),t) \big) \, \mathrm{d}t  - \nabla_x h(X_t(\omega),t) \, \mathrm{d}B_t, \\ a_T(\omega) &= \nabla_x g(X_T(\omega)),
%     \end{talign}
%     we have that 
%     \begin{talign}
%         &\nabla_{x} \mathbb{E} \big[ \int_0^T f(X_t(\omega),t) \, \mathrm{d}t + \int_0^T \langle h(X_t(\omega),t), \, \mathrm{d}B_t \rangle + g(X_T(\omega)) | X_0(\omega) = x \big] = \mathbb{E} \big[ a_0(\omega) \big], \\
%     \begin{split}
%         &\nabla_{x} \mathbb{E} \big[ \exp \big( - \int_0^T f(X_t(\omega),t) \, \mathrm{d}t - \int_0^T \langle h(X_t(\omega),t), \, \mathrm{d}B_t \rangle - g(X_T(\omega)) \big) | X_0(\omega) = x \big] \\ &= - \mathbb{E} \big[ a_0(\omega) \exp \big( - \int_0^T f(X_t(\omega),t) \, \mathrm{d}t - \int_0^T \langle h(X_t(\omega),t), \, \mathrm{d}B_t \rangle - g(X_T(\omega)) \big) | X_0(\omega) = x \big].
%     \end{split}
%     \end{talign}
% \end{theorem}

% \section{Proofs of \Cref{sec:background}}
\section{Results on DDIM and Flow Matching}
\label{sec:proofs_background}

\subsection{The continuous-time limit of DDIM} \label{subsec:continuous_DDIM}
% \ricky{TODO: switch notation to $\bar{\alpha}$}
The DDIM inference update \citep[Eq.~12]{song2021denoising} is 
\begin{talign} \label{eq:DDIM_original}
    x_{k+1} = \sqrt{\bar{\alpha}_{k+1}} \big( \frac{x_{k} - \sqrt{1-\bar{\alpha}_k} \epsilon(x_k,k)}{\sqrt{\bar{\alpha}_k}} \big) + \sqrt{1-\bar{\alpha}_{k+1} - \sigma_{k}^2} \epsilon(x_k,k) + \sigma_{k} \epsilon_{k}, \qquad x_K \sim N(0,I).
\end{talign}
If we let 
% $\Delta \bar{\alpha}_{k} = \bar{\alpha}_{k} - \bar{\alpha}_{k-1}$,
$\Delta \bar{\alpha}_{k} = \bar{\alpha}_{k+1} - \bar{\alpha}_{k}$,
we have that
\begin{talign}
    \sqrt{\frac{\bar{\alpha}_{k+1}}{\bar{\alpha}_k}} = \sqrt{\frac{\bar{\alpha}_k + \bar{\alpha}_{k+1} - \bar{\alpha}_k}{\bar{\alpha}_k}} = \sqrt{1 + \frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{\bar{\alpha}_k}} = \sqrt{1 + \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_k}} \approx 1 + \frac{\Delta \bar{\alpha}_k}{2\bar{\alpha}_k}, 
\end{talign}
where we used the first-order Taylor approximation of $\sqrt{1+x}$. And
\begin{talign}
\begin{split}
    &- \sqrt{\frac{\bar{\alpha}_{k+1}}{\bar{\alpha}_k} (1-\bar{\alpha}_k)} + \sqrt{1-\bar{\alpha}_{k+1} - \sigma_{k}^2} = - \sqrt{\big( 1 + \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_k} \big) (1-\bar{\alpha}_k)} + \sqrt{1-\bar{\alpha}_{k+1} - \sigma_{k}^2} \\ &= - \sqrt{1 + \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_k} -\bar{\alpha}_k - \Delta \bar{\alpha}_k} + \sqrt{1-\bar{\alpha}_{k+1} - \sigma_{k}^2} = - \sqrt{1 -\bar{\alpha}_{k+1} + \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_k}} + \sqrt{1-\bar{\alpha}_{k+1} - \sigma_{k}^2} \\ &= \sqrt{1- \bar{\alpha}_{k+1}} \big( - \sqrt{1 + \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_k(1-\bar{\alpha}_{k+1})}} + \sqrt{1 - \frac{\sigma_{k}^2}{1-\bar{\alpha}_{k+1}}} \big) \approx \sqrt{1- \bar{\alpha}_{k+1}} \big( - \big( 1+\frac{\Delta \bar{\alpha}_k}{2\bar{\alpha}_k(1-\bar{\alpha}_{k+1})} \big) + 1 - \frac{\sigma_{k}^2}{2(1-\bar{\alpha}_{k+1})} \big) \\ &= %\sqrt{1- \bar{\alpha}_{k-1}} \big( 
    % \frac{\Delta \bar{\alpha}_k}{2\bar{\alpha}_k\sqrt{1-\bar{\alpha}_{k-1}}} - \frac{\sigma_{k}^2}{2\sqrt{1-\bar{\alpha}_{k-1}}} 
    % \big)
    - \big( \frac{\Delta \bar{\alpha}_k}{2\bar{\alpha}_k} + \frac{\sigma_{k}^2}{2} \big) \frac{1}{\sqrt{1-\bar{\alpha}_{k+1}}},
\end{split}
\end{talign}
where we used the same first-order Taylor approximation. Thus, up to first-order approximations, \eqref{eq:DDIM_original} is equivalent to
\begin{talign} \label{eq:DDIM_approx}
    x_{k-1} = \big( 1 + \frac{\Delta \bar{\alpha}_k}{2\bar{\alpha}_k} \big) x_k - \big( \frac{\Delta \bar{\alpha}_k}{2\bar{\alpha}_k} + \frac{\sigma_{k}^2}{2} \big) \frac{\epsilon(x_k,k)}{\sqrt{1-\bar{\alpha}_{k+1}}} + \sigma_k \epsilon_k, \qquad x_K \sim N(0,I).
\end{talign}
If we modify our notation slightly, we can rewrite this as
\begin{talign} \label{eq:DDIM_approx_continuous}
    X_{(k+1)h} = \big( 1 - \frac{h \dot{\bar{\alpha}}_{kh}}{2\bar{\alpha}_{kh}} \big) X_{kh} + \big( \frac{h \dot{\bar{\alpha}}_{kh}}{2\bar{\alpha}_{kh}} - \frac{h \sigma(kh)^2}{2} \big) \frac{\epsilon(X_{kh},kh)}{\sqrt{1-\bar{\alpha}_{kh}}} + \sqrt{h} \sigma(kh) \epsilon_k, \qquad X_{0} \sim N(0,I).
\end{talign}
To go from \eqref{eq:DDIM_approx} to \eqref{eq:DDIM_approx_continuous}, we introduced a continuous time variable and a stepsize $h = 1/K$, and we regard the increment $h \bar{\alpha}_k$ as approximately equal to $h$ times the derivative of $\bar{\alpha}$. We also 
% flipped the time axis for the iterate variables, and
identified $\sigma_k$ with $\sqrt{h} \sigma(kh)$, where $\sigma(kh)$ plays the role of a diffusion coefficient. Note that equation \eqref{eq:DDIM_approx_continuous} can be reverse-engineered as the Euler-Maruyama discretization of the SDE
\begin{talign}
    \mathrm{d}X_t = \big( - \frac{\dot{\bar{\alpha}}_{t}}{2\bar{\alpha}_{t}} + \big( \frac{\dot{\bar{\alpha}}_{t}}{2\bar{\alpha}_{t}} - \frac{\sigma(t)^2}{2} \big) \frac{\epsilon(X_{t},t)}{\sqrt{1-\bar{\alpha}_{t}}} \big) \mathrm{d}t + \sigma(t) \mathrm{d}B_t, \qquad X_{0} \sim N(0,I).
\end{talign}
% Next, we prove equation \eqref{eq:DDPM_discrete}. When $\sigma_k = \sqrt{1-\frac{\bar{\alpha}_k}{\bar{\alpha}_{k+1}}}$, we have that
% \begin{talign}
% \begin{split}
%     &- \sqrt{\frac{\bar{\alpha}_{k+1}}{\bar{\alpha}_k} (1-\bar{\alpha}_k)} + \sqrt{1 - \bar{\alpha}_{k+1} - \sigma_k^2} = - \sqrt{\frac{\bar{\alpha}_{k+1}}{\bar{\alpha}_k} (1-\bar{\alpha}_k)} + \sqrt{1 - \bar{\alpha}_{k+1} - \big( 1 - \frac{\bar{\alpha}_k}{\bar{\alpha}_{k+1}} \big)} \\ &= - \sqrt{\frac{\bar{\alpha}_{k+1}}{\bar{\alpha}_k} - \bar{\alpha}_{k+1}} + \sqrt{\frac{\bar{\alpha}_k}{\bar{\alpha}_{k+1}} - \bar{\alpha}_{k+1}} = - \sqrt{1  - \bar{\alpha}_{k+1} + \frac{\Delta \bar{\alpha}_{k}}{\bar{\alpha}_k}} + \sqrt{1 - \bar{\alpha}_{k+1} - \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_{k+1}}} \\ &= \sqrt{1 - \bar{\alpha}_{k+1}} \big( -\sqrt{1 + \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_k (1-\bar{\alpha}_{k+1})}} + \sqrt{1 - \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_{k+1} (1-\bar{\alpha}_{k+1})}} \big) \\ &\approx \sqrt{1 - \bar{\alpha}_{k+1}} \big( -\big( 1 + \frac{\Delta \bar{\alpha}_k}{2\bar{\alpha}_k (1-\bar{\alpha}_{k+1})} \big) + \big( 1 - \frac{\Delta \bar{\alpha}_k}{2 \bar{\alpha}_{k-1} (1-\bar{\alpha}_{k-1})} \big) \big) = - \frac{\Delta \bar{\alpha}_k}{2 \sqrt{1-\bar{\alpha}_{k+1}}} \big( \frac{1}{\bar{\alpha}_{k}} + \frac{1}{\bar{\alpha}_{k+1}} \big) \\ &\approx - \frac{\Delta \bar{\alpha}_k}{\bar{\alpha}_{k+1} \sqrt{1-\bar{\alpha}_{k+1}}} = -\frac{1 - \frac{\bar{\alpha}_k}{\bar{\alpha}_{k+1}}}{\sqrt{1-\bar{\alpha}_{k+1}}}.
% \end{split}
% \end{talign}

\subsection{Forward and backward stochastic differential equations}

Let ${(\kappa_t)}_{t \in [0,1]}$ and ${(\eta_t)}_{t \in [0,1]}$ such that
\begin{talign} \label{eq:kappa_eta_conditions}
    \forall t \in [0,1], \quad \eta_t \geq 0, \qquad\qquad \int_0^1 \kappa_{1-s} \, \mathrm{d}s = +\infty, \qquad\qquad 2 \int_0^1 \eta_{1-t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t' = 1.
\end{talign}

As shown in \Cref{tab:coefficients}, DDIM corresponds to $\kappa_t = \frac{\dot{\bar{\alpha}}_{t}}{2\bar{\alpha}_{t}}$, $\eta_t = \frac{\dot{\bar{\alpha}}_{t}}{2\bar{\alpha}_{t}}$, and Flow Matching corresponds to $\kappa_t = \frac{\dot{\alpha}_{t}}{\alpha_{t}}$, $\eta_t = \beta_t \big( \frac{\dot{\alpha}_{t}}{\alpha_{t}} \beta_t - \dot{\beta}_t \big)$. 
\begin{lemma}[DDIM and Flow Matching fulfill the conditions \eqref{eq:kappa_eta_conditions}] \label{lem:DDIM_FM_conditions}
The choices of $(\kappa_t)_{t \in [0,1]}$ and $(\eta_t)_{t \in [0,1]}$ for DDIM and Flow Matching fulfill the conditions \eqref{eq:kappa_eta_conditions}. For DDIM, we have that
\begin{talign}
\begin{split}
    \int_0^t \kappa_{1-s} \, \mathrm{d}s = -\frac{1}{2} \log \bar{\alpha}_{1-t} &\implies \int_0^1 \kappa_{1-s} \, \mathrm{d}s = +\infty, \\
    2 \int_0^t \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t' = 1-\bar{\alpha}_{1-t} &\implies 2 \int_0^1 \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t' = 1.
\end{split}
\end{talign}
For Flow Matching, 
\begin{talign}
    \int_0^t \kappa_{1-s} \, \mathrm{d}s = - \log \alpha_{1-t} &\implies \int_0^1 \kappa_{1-s} \, \mathrm{d}s = +\infty, \\
    2 \int_0^t \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t' = \beta^2_{1-t} &\implies 2 \int_0^1 \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t' = 1.
\end{talign}
\end{lemma}

\paragraph{Forward and backward SDEs} Consider the forward and backward SDEs
\begin{talign} \label{eq:forward_generic}
    % \mathrm{d} \vec{X}_{t} &= - \frac{\dot{\alpha}_{1-t}}{2\alpha_{1-t}} \vec{X}_{t} \mathrm{d}t + \sqrt{\frac{\dot{\alpha}_{1-t}}{\alpha_{1-t}}} \mathrm{d}B_{t}, \qquad \vec{X}_0 \sim p_{\mathrm{data}}, \\ \mathrm{d} X_{t} &= \big( \frac{\dot{\alpha}_{t}}{2\alpha_{t}} X_{t} + \frac{\dot{\alpha}_{t}}{\alpha_{t}} \nabla \log \vec{p}_{1-t}(X_{t}) \big) \mathrm{d}t + \sqrt{\frac{\dot{\alpha}_{t}}{\alpha_{t}}} \mathrm{d}B_{t}, \qquad X_0 \sim N(0,I),
    \mathrm{d}\vec{X}_t &= - \kappa_{1-t} \vec{X}_t \, \mathrm{d}t + \sqrt{2 \eta_{1-t}} \, \mathrm{d}B_t, \qquad \vec{X}_0 \sim p_{\mathrm{data}}, \\
    \mathrm{d}X_t &= \big( \kappa_t X_t + 2 \eta_t \mathfrak{s}(X_t,t) \big) \mathrm{d}t + \sqrt{2 \eta_t} \, \mathrm{d}B_t, \qquad X_0 \sim N(0,I),
    \label{eq:backward_generic}
\end{talign}
where we let $\vec{p}_t$ be the density of $\vec{X}_t$, and we define the score function as $\mathfrak{s}(x,t) := \nabla \log \vec{p}_{1-t}(x)$. Similarly, we let $p_t$ be the density of $X_t$. $\vec{p}_t$ and $p_t$ solve the Fokker-Planck equations:
\begin{talign} \label{eq:FP_forward}
    \partial_t \vec{p}_t &= \nabla \cdot \big( \kappa_{1-t} x \vec{p}_t \big) + \eta_{1-t} \Delta \vec{p}_t, \qquad \vec{p}_0 = p_{\mathrm{data}}, \\
    \partial_t p_t &= \nabla \cdot \big( \big(- \kappa_t x - 2 \eta_t \nabla \log \vec{p}_{1-t}(X_{t}) \big) p_t \big) + \eta_t \Delta p_t, \qquad p_0 = N(0,I). \label{eq:FP_backward}
\end{talign}
\begin{lemma}[Solution of the forward SDE] \label{lem:OU_process}
    Let $(\kappa_t)_{t \geq 0}$, $(\eta_t)_{t \geq 0}$ with $\eta_t \geq 0$, and $(\xi_t)_{t \geq 0}$ be arbitrary. The solution $\vec{X}_t$ of the SDE 
    \begin{talign}
        \mathrm{d}\vec{X}_t &= \big( - \kappa_{1-t} \vec{X}_t + \xi_t \big) \, \mathrm{d}t + \sqrt{2 \eta_{1-t}} \, \mathrm{d}B_t, \qquad \vec{X}_0 \sim p_{\mathrm{data}}
    \end{talign}
    is
    \begin{talign}
        \vec{X}_t = \vec{X}_0 \exp \big( - \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) + \int_0^t \exp \big( - \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \xi_{1-t'} \, \mathrm{d}t' + \int_0^t \sqrt{2 \eta_{1-t'}} \exp \big( - \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}B_{t'},
    \end{talign}
    which has the same distribution as the random variable
    \begin{talign} 
    \begin{split} \label{eq:hat_X_t_OU_process}
        \hat{X}_t &= \vec{X}_0 \exp \big( - \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) + \int_0^t \exp \big( - \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \xi_{1-t'} \, \mathrm{d}t' + \sqrt{2 \int_0^t \eta_{1-t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t'} \epsilon, \\ \epsilon &\sim N(0,I).
    \end{split}
    \end{talign}
\end{lemma}
Applying \Cref{lem:OU_process} with $\xi_t \equiv 0$, we obtain that $\vec{p}_1$ is also the distribution of
\begin{talign} \label{eq:hat_X_1}
    \hat{X}_1 = \vec{X}_0 \exp \big( - \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) + \sqrt{2 \int_0^t \eta_{1-t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t'} \epsilon = \epsilon,
\end{talign}
where $\epsilon \sim N(0,I)$. The third equality in \eqref{eq:hat_X_1} holds by \eqref{eq:kappa_eta_conditions}. Hence we obtain that $\vec{p}_1 = N(0,I)$. Note also that
\begin{talign}
    \partial_t \vec{p}_{1-t} &= -\nabla \cdot \big( \kappa_{t} x \vec{p}_{1-t} \big) - \eta_{t} \Delta \vec{p}_{1-t} = -\nabla \cdot \big( \big( - \kappa_{t} x - 2 \eta_t \nabla \log \vec{p}_{1-t}(x) \big) \vec{p}_{1-t} \big) + \eta_{t} \Delta \vec{p}_{1-t}
\end{talign}
Thus, $\vec{p}_{1-t}$ is a solution of the backward Fokker-Planck equation \eqref{eq:FP_backward}, which proves the following: 
\begin{proposition}[Equality of marginal distributions] \label{prop:equality_marginals}
For any time $t \in [0,1]$, the densities of the solutions $\vec{X}_t$, $X_t$ of the forward and backward SDEs are equal up to a time flip: $p_t = \vec{p}_{1-t}$.
\end{proposition}

\paragraph{Forward and backward SDEs with arbitrary noise schedule} Next, we look at the following pair of forward-backward SDEs:
\begin{talign} \label{eq:forward_arbitrary}
    % \mathrm{d} \vec{X}_{t} &= - \frac{\dot{\alpha}_{1-t}}{2\alpha_{1-t}} \vec{X}_{t} \mathrm{d}t + \sqrt{\frac{\dot{\alpha}_{1-t}}{\alpha_{1-t}}} \mathrm{d}B_{t}, \qquad \vec{X}_0 \sim p_{\mathrm{data}}, \\ \mathrm{d} X_{t} &= \big( \frac{\dot{\alpha}_{t}}{2\alpha_{t}} X_{t} + \frac{\dot{\alpha}_{t}}{\alpha_{t}} \nabla \log \vec{p}_{1-t}(X_{t}) \big) \mathrm{d}t + \sqrt{\frac{\dot{\alpha}_{t}}{\alpha_{t}}} \mathrm{d}B_{t}, \qquad X_0 \sim N(0,I),
    \mathrm{d}\vec{X}_t &= \big( - \kappa_{1-t} \vec{X}_t + \big( \frac{\sigma(1-t)^2}{2} - \eta_{1-t} \big) \mathfrak{s}(\vec{X}_t,1-t) \big) \, \mathrm{d}t + \sigma(1-t) \, \mathrm{d}B_t, \qquad \vec{X}_0 \sim p_{\mathrm{data}}, \\
    \mathrm{d}X_t &= \big( \kappa_t X_t + \big( \frac{\sigma(t)^2}{2} + \eta_t \big) \mathfrak{s}(X_t,t) \big) \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad X_0 \sim N(0,I), \label{eq:backward_arbitrary}
\end{talign}
Here, the score function $\mathfrak{s}$ is the same vector field as in \eqref{eq:backward_arbitrary}.
Remark that equations \eqref{eq:forward_generic}-\eqref{eq:backward_generic} are a particular case of \eqref{eq:forward_arbitrary}-\eqref{eq:backward_arbitrary} for which $\sigma(t) = \sqrt{2 \eta_t}$. The Fokker-Planck equations for \eqref{eq:forward_arbitrary}-\eqref{eq:backward_arbitrary} are:
\begin{talign} \label{eq:FP_forward_arbitrary}
    \partial_t \vec{p}_t &= \nabla \cdot \big( \big( \kappa_{1-t} x + \big( - \frac{\sigma(1-t)^2}{2} + \eta_{1-t} \big) \mathfrak{s}(X_{t},t) \big) \vec{p}_t \big) + \eta_{1-t} \Delta \vec{p}_t, \qquad \vec{p}_0 = p_{\mathrm{data}}, \\
    \partial_t p_t &= \nabla \cdot \big( \big(- \kappa_t x - \big( \frac{\sigma(t)^2}{2} + \eta_t \big) \mathfrak{s}(X_{t},t) \big) p_t \big) + \frac{\sigma(t)^2}{2} \Delta p_t, \qquad p_0 = N(0,I). \label{eq:FP_backward_arbitrary}
\end{talign}
It is straight-forward to see that for any $\sigma$, the solutions $\vec{p}_t$ and $p_t$ of \eqref{eq:FP_forward_arbitrary}-\eqref{eq:FP_backward_arbitrary} are also solutions of \eqref{eq:FP_forward}-\eqref{eq:FP_backward}. Hence, the marginals $\vec{X}_t$ and $X_t$ are equally distributed for all noise schedules $\sigma$, and they are equal to each other up to a time flip.

\paragraph{Equality of distributions over trajectories} The result in \Cref{prop:equality_marginals} can be made even stronger:
\begin{proposition}[Equality of distributions over trajectories] \label{lem:equal_process_distributions}
    Let $\vec{\bm{X}}$, $\bm{X}$ be the solutions of the SDEs \eqref{eq:forward_arbitrary}-\eqref{eq:backward_arbitrary} with arbitrary noise schedule. For any sequence of times $(t_i)_{0 \leq i \leq I}$, the joint distribution of $(\vec{X}_{t_i})_{0 \leq i \leq I}$ is equal to the joint distribution of $(X_{1-t_i})_{0 \leq i \leq I}$, or equivalently, that the probability measures $\vec{\mathbb{P}}$, $\mathbb{P}$ of the forward and backward processes $\vec{\bm{X}}$, $\bm{X}$ are equal, up to a flip in the time direction.
\end{proposition}
This result states that sampling trajectories from the backward process is equivalent to sampling them from the forward process and then flipping their order.

% \paragraph{DDIM and Flow Matching fulfill the conditions \eqref{eq:kappa_eta_conditions}} 
\subsubsection{Proof of \Cref{lem:DDIM_FM_conditions}}
As shown in \Cref{tab:coefficients}, DDIM corresponds to $\kappa_t = \frac{\dot{\bar{\alpha}}_{t}}{2\bar{\alpha}_{t}}$, $\eta_t = \frac{\dot{\bar{\alpha}}_{t}}{2\bar{\alpha}_{t}}$. Thus, $\eta_t \geq 0$ because $\bar{\alpha}_t$ is increasing, and
\begin{talign}
    \begin{split}
        % &\exp \big( 
        &\int_0^t \kappa_{1-s} \, \mathrm{d}s 
        % \big)
         = 
        % \exp \big( 
        \int_0^t \frac{\dot{\bar{\alpha}}_{1-s}}{2\bar{\alpha}_{1-s}} \, \mathrm{d}s 
        % \big) 
         =  
        % \sqrt{\exp \big( 
        - \frac{1}{2} \int_0^t \partial_s \log \bar{\alpha}_{1-s} \, \mathrm{d}s 
        % \big)} 
         =  
        % \sqrt{\exp \big( 
        - \frac{1}{2} (\log \bar{\alpha}_{1-t} - \log \bar{\alpha}_{1}) 
        % \big)} 
         = - \frac{1}{2} \log \bar{\alpha}_{1-t}, \\
        % &\implies \exp \big( \! - \! \int_0^1 \kappa_s \, \mathrm{d}s \big) \! = \! \sqrt{\alpha_{0}} \! = \! 0, 
        &\implies \int_0^1 \kappa_{1-s} \, \mathrm{d}s = - \frac{1}{2} \log \bar{\alpha}_{0} = +\infty
    \end{split} \\
    \begin{split}
        & %\sqrt{
        2 \int_0^t \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t'
        % } 
        = %\sqrt{ 
        \int_0^t \frac{\dot{\bar{\alpha}}_{1-t'}}{\bar{\alpha}_{1-t'}} \exp \big( - \int_{t'}^{t} \frac{\dot{\bar{\alpha}}_{1-s}}{\bar{\alpha}_{1-s}} \, \mathrm{d}s \big) \, \mathrm{d}t'
        % } 
        \\ &= 
        % \sqrt{
        \int_0^t \frac{\dot{\bar{\alpha}}_{1-t'}}{\bar{\alpha}_{1-t'}} \frac{\bar{\alpha}_{1-t}}{\bar{\alpha}_{1-t'}} \, \mathrm{d}t'
        % } 
        = 
        % \sqrt{
        \bar{\alpha}_{1-t} \int_0^t \partial_{t'} \big( \frac{1}{\bar{\alpha}_{1-t'}} \big) \, \mathrm{d}t'
        % } 
        = 
        % \sqrt{
        \bar{\alpha}_{1-t} \big( \frac{1}{\bar{\alpha}_{1-t}} - \frac{1}{\bar{\alpha}_{1}} \big)
        % } 
        = 
        % \sqrt{
        1 - \bar{\alpha}_{1-t},
        % }, 
        \\
        &\implies %\sqrt{
        2 \int_0^1 \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t'
        % }
        = %\sqrt{
        1 - \bar{\alpha}_{0}
        % } 
        = 1.
    \end{split}
    \end{talign}
    where we used that $\bar{\alpha}_1 = 1$ and $\bar{\alpha}_0 = 0$. And Flow Matching corresponds to $\kappa_t = \frac{\dot{\alpha}_{t}}{\alpha_{t}}$, $\eta_t = \beta_t \big( \frac{\dot{\alpha}_{t}}{\alpha_{t}} \beta_t - \dot{\beta}_t \big)$. We have that $\eta_t \geq 0$ because $\alpha_t$ is increasing and $\beta_t$ is decreasing, and
    \begin{talign}
        \begin{split}
        &\int_0^t \kappa_{1-s} \, \mathrm{d}s = 
        \int_0^t \frac{\dot{\alpha}_{1-s}}{\alpha_{1-s}} \, \mathrm{d}s 
         =  
        - \int_0^t \partial_s \log \alpha_{1-s} \, \mathrm{d}s 
         =  
        - (\log \alpha_{1-t} - \log \alpha_{1}) 
         = - \log \alpha_{1-t}, \\
        &\implies \int_0^1 \kappa_{1-s} \, \mathrm{d}s = - \log \alpha_{0} = +\infty,
        \end{split}
    \end{talign}
    and
    \begin{talign}
    \begin{split} \label{eq:second_term_FM}
        &2 \int_0^t \eta_{1-t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t' = 2 \int_0^t  \beta_{1-t'} \big( \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \beta_{1-t'} - \dot{\beta}_{1-t'} \big) \exp \big( - 2 \int_{t'}^{t} \frac{\dot{\alpha}_{1-s}}{\alpha_{1-s}} \, \mathrm{d}s \big) \, \mathrm{d}t' \\ &= 2 \int_0^t  \beta_{1-t'} \big( \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \beta_{1-t'} - \dot{\beta}_{1-t'} \big) \big( \frac{\alpha_{1-t}}{\alpha_{1-t'}} \big)^2 \, \mathrm{d}t',
    \end{split}
    \end{talign}
    To develop the right-hand side, note that by integration by parts,
    \begin{talign}
    \begin{split}
        &\int_0^t \dot{\beta}_{1-t'} \beta_{1-t'} \big( \frac{\alpha_{1-t}}{\alpha_{1-t'}} \big)^2 \, \mathrm{d}t' = - \int_0^t \partial_{t'} \big( \frac{\beta_{1-t'}^2}{2} \big) \big( \frac{\alpha_{1-t}}{\alpha_{1-t'}} \big)^2 \, \mathrm{d}t' \\ &= - \big[ \frac{\beta_{1-t'}^2}{2} \big( \frac{\alpha_{1-t}}{\alpha_{1-t'}} \big)^2 \big]_{0}^{1} + \int_0^t \frac{\beta_{1-t'}^2}{2} \partial_{t'} \big( \frac{\alpha_{1-t}}{\alpha_{1-t'}} \big)^2 \, \mathrm{d}t' 
        = - \big[ \frac{\beta_{1-t'}^2}{2} \big( \frac{\alpha_{1-t}}{\alpha_{1-t'}} \big)^2 \big]_{0}^{t} + \int_0^t \beta_{1-t'}^2 \frac{\alpha_{1-t}^2 \dot{\alpha}_{1-t'}}{\alpha_{1-t'}^3} \, \mathrm{d}t'. 
    \end{split}
    \end{talign}
    And if we plug this into the right-hand side of \eqref{eq:second_term_FM}, we obtain
    \begin{talign}
        &2 \int_0^t \eta_{1-t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t' = \big[ \beta_{1-t'}^2 \big( \frac{\alpha_{1-t}}{\alpha_{1-t'}} \big)^2 \big]_{0}^{t} = \beta_{1-t}^2 - \beta_{1}^2 \big( \frac{\alpha_{1-t}}{\alpha_{1}} \big)^2 = \beta_{1-t}^2, \\
        &\implies 2 \int_0^1 \eta_{1-t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t' = \beta_{1}^2 = 1.
    \end{talign}
    where we used that $\beta_{1} = 0$, $\alpha_1 = 1$.
    % \begin{talign}
    % \begin{split}
    %     &2 \int_0^t (1-\alpha_{1-t'}) \big( \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} (1-\alpha_{1-t'}) + \dot{\alpha}_{1-t'} \big) \big( \frac{\alpha_{1-t}}{\alpha_{1-t'}} \big)^2 \, \mathrm{d}t' = - 2 \alpha_{1-t}^2 \int_0^t \partial_{t'} \big( \frac{1}{\alpha_{1-t'}} - \frac{1}{2\alpha_{1-t'}^2} \big)  \, \mathrm{d}t' \\ &= - 2 \alpha_{1-t}^2 \big( \frac{1}{\alpha_{1-t}} - \frac{1}{2\alpha_{1-t}^2} - \frac{1}{\alpha_{1}} + \frac{1}{2\alpha_{1}^2} \big) = - 2 \alpha_{1-t} + 1 + \alpha_{1-t}^2 = (1-\alpha_{1-t})^2
    % \end{split}
    % \end{talign}
    % \begin{talign}
    %     \int \frac{(1-x)^2}{x^3} \, \mathrm{d}x = \log x + \frac{2}{x} - \frac{1}{2x^2}, \qquad \int \frac{1-x}{x^2} \, \mathrm{d}x = - \log x - \frac{1}{x}, \qquad \int \big( \frac{(1-x)^2}{x^3} + \frac{1-x}{x^2} \big) \, \mathrm{d}x = \frac{1}{x} - \frac{1}{2x^2}
    % \end{talign}

\subsubsection{Proof of \Cref{lem:OU_process}}
    We can solve this equation by variation of parameters. To simplify the notation, we replace $\kappa_{1-s}$, $\eta_{1-s}$ and $\xi_{1-s}$ by $\kappa_{s}$, $\eta_{s}$ and $\xi_{s}$. Defining $f(\vec{X}_t,t) = \vec{X}_t \exp \big( \int_0^t \kappa_{1-s} \, \mathrm{d}s \big)$, we get that
    \begin{talign}
    \begin{split}
        df(\vec{X}_t,t) &= \kappa_{1-t} \vec{X}_t \exp \big( \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t + \exp \big( \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) \mathrm{d}\vec{X}_t \\ &= \kappa_{1-t} \vec{X}_t \exp \big( \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t + \exp \big( \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) \big( (- \kappa_{1-t} \vec{X}_t + \xi_{1-t}) \, \mathrm{d}t + \sqrt{2 \eta_{1-t}} \, \mathrm{d}B_t \big) \\ &= \exp \big( \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) \xi_{1-t} \, \mathrm{d}t + \sqrt{2 \eta_t} \exp \big( \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}B_t.
    \end{split}
    \end{talign}
    Integrating from 0 to $t$, we get that
    \begin{talign}
        &\vec{X}_t \exp \big( \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) = \vec{X}_0 + \int_0^t \exp \big( \int_0^{t'} \kappa_{1-s} \, \mathrm{d}s \big) \xi_{1-t'} \, \mathrm{d}t' + \int_0^t \sqrt{2 \eta_{1-t'}} \exp \big( \int_0^{t'} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}B_{t'}, \\
        &\iff \vec{X}_t = \vec{X}_0 \exp \big( - \int_0^t \kappa_{1-s} \, \mathrm{d}s \big) + \int_0^t \exp \big( - \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \xi_{1-t'} \, \mathrm{d}t' + \int_0^t \sqrt{2 \eta_{1-t'}} \exp \big( - \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}B_{t'}. \label{eq:alternative_forward_process}
    \end{talign}
    Since 
    \begin{talign}
    \mathbb{E} \big[ \big( \int_0^t \sqrt{2 \eta_{1-t'}} \exp \big( - \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}B_{t'} \big)^2 \big] = 2 \int_0^t \eta_{1-t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t',
    \end{talign}
    we obtain that $\int_0^t \sqrt{2 \eta_{1-t'}} \exp \big( - \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}B_{t'}$ has the same distribution as $\sqrt{2 \int_0^t \eta_{1-t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t'} \epsilon$, where $\epsilon \sim N(0,1)$. 
% \end{proof}

\subsubsection{Proof of \Cref{lem:equal_process_distributions}}

This is a result that has been used by previous works, e.g. \cite[Sec.~2.1]{debortoli2021diffusion}, but their derivation lacks rigor as it uses some unexplained approximations. While natural, the result is not common knowledge in the area. We provide a derivation which is still in discrete time, and hence not completely formal, but that corrects the gaps in the proof of \cite{debortoli2021diffusion}.

We introduce the short-hand
\begin{talign}
    \vec{b}(x,t) &= - \kappa_{1-t} x + \big( \frac{\sigma(1-t)^2}{2} - \eta_{1-t} \big) \mathfrak{s}(x,1-t), \\
    b(x,t) &= \kappa_t X_t + \big( \frac{\sigma(t)^2}{2} + \eta_t \big) \mathfrak{s}(X_t,t), \\
    \vec{\sigma}(t) &= \sigma(1-t).
\end{talign}
Remark that $b(x,t) = - \vec{b}(x,1-t) + \sigma(t)^2 \mathfrak{s}(X_t,t)$.

Suppose that we discretize the forward process $\vec{X}$ using $K+1$ equispaced timesteps:
\begin{talign} \label{eq:forward_discretized}
    x_{k+1} = x_{k} + h \vec{b}(x_k,kh) + \sqrt{h} \vec{\sigma}(kh) \epsilon_k, \qquad  \text{with} \ \epsilon_k \sim N(0,1).
\end{talign}
It is important to remark that $x_{k+1} - x_k = O(h^{1/2})$. Throughout the proof we will keep track of all terms up to linear order in $h$, while neglecting terms of order $O(h^{3/2})$ and higher.
The distribution of the discretized forward process is:
\begin{talign}
    \vec{p}(x_{0:K}) = \vec{p}_0(x_0) \prod_{k=0}^{K-1} \vec{p}_{k+1|k}(x_{k+1}|x_{k}), \qquad \text{where} \qquad
    \vec{p}_{k+1|k}(x_{k+1}|x_{k}) = \frac{\exp \big( - \frac{\|x_{k+1} - x_{k} - h \vec{b}(x_k,kh) \|^2}{2 h \vec{\sigma}(kh)^2} \big)}{(2\pi h \vec{\sigma}(kh)^2)^{d/2}}
\end{talign}
% \begin{talign}
%     \vec{p}_{k+1|k}(x_{k+1}|x_{k}) = \frac{\exp \big( - \frac{\|x^{(k+1)} - (1+h) x^{(k)} - 2h s^{(k)}(x^{(k)})\|^2}{4 h} \big)}{(4\pi h)^{d/2}}
% \end{talign}
Using telescoping products, we have that
\begin{talign}
\begin{split} \label{eq:telescoping}
    \vec{p}(x_{0:K}) &= \vec{p}_{K}(x_{K}) \prod_{k=0}^{K-1} \vec{p}_{k+1|k}(x_{k+1}|x_{k}) \frac{\vec{p}_{k}(x_{k})}{\vec{p}_{k+1}(x_{k+1})} \\ &= \vec{p}_{K}(x_{K}) \prod_{k=0}^{K-1} \vec{p}_{k+1|k}(x_{k+1}|x_{k}) \exp \big( \log(\vec{p}_{k}(x_{k})) - \log(\vec{p}_{k+1}(x_{k+1})) \big) 
    % \\ &= \vec{p}_{K}(x_{K}) \prod_{k=0}^{K-1} \vec{p}_{k+1|k}(x_{k+1}|x_{k}) \exp \big( \log(\vec{p}_{k}(x_{k})) - \log(\vec{p}_{k+1}(x_{k})) \\ &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad + \log(\vec{p}_{k+1}(x_{k})) - \log(\vec{p}_{k+1}(x_{k+1})) \big) 
\end{split}
\end{talign}
We can use a discrete time version of Ito's lemma:
\begin{talign} \label{eq:update_log_p_ito}
    \log \vec{p}(x_{k+1},(k+1)h) &\approx \log \vec{p}(x_{k},kh) + h \big( \partial_t \log \vec{p}(x_{k},kh) + %\langle \nabla \log p(X_k,kh), \mu_k \rangle + 
    \frac{\vec{\sigma}(kh)^2}{2} \Delta \log \vec{p}(x_k,kh) 
    \big) \\ &\qquad %+ \sqrt{2h} 
    + \langle \nabla \log \vec{p}(x_k,kh), x_{k+1} - x_k \rangle + O(h^{3/2}).
\end{talign}
Using equation \eqref{eq:forward_discretized} and a Taylor approximation, observe that
\begin{talign}
\begin{split} \label{eq:nabla_log_diff_x}
    &\langle \nabla \log p(x_k,kh), x_{k+1} - x_k \rangle \\ &= \langle \nabla \log p(x_{k+1},(k+1)h) -\nabla^2 \log p(x_{k+1},(k+1)h) (x_{k+1} - x_k), x_{k+1} - x_k \rangle + O(h^{3/2}) \\ &= \langle \nabla \log p(x_{k+1},(k+1)h), x_{k+1} - x_k \rangle %- 2 h \Delta \log p(x_{k+1},(k+1)h).
    \\ &\qquad - \langle h \vec{b}(x_k,kh) + \sqrt{h} \vec{\sigma}(kh) \epsilon_k, \nabla^2 \log p(x_{k+1},(k+1)h) \big( h \vec{b}(x_k,kh) + \sqrt{h} \vec{\sigma}(kh) \epsilon_k \big) \rangle  + O(h^{3/2}) \\ &= \langle \nabla \log p(x_{k+1},(k+1)h), x_{k+1} - x_k \rangle - h \vec{\sigma}(kh)^2 \Delta \log p(x_{k+1},(k+1)h)  + O(h^{3/2}).
    % \\ &\qquad - \langle h \vec{b}(x_k,kh) + \sqrt{h} \sigma(kh) \epsilon_k, \nabla^2 \log p(x_{k+1},(k+1)h) \big( h \vec{b}(x_k,kh) + \sqrt{h} \sigma(kh) \epsilon_k \big) \rangle.
\end{split}
\end{talign}
% We can use the mean value theorem:
% \begin{talign}
%     &\exp \big( \log(\vec{p}_{k+1}(x_{k})) - \log(\vec{p}_{k+1}(x_{k+1})) \big) = \exp \big( \big\langle \nabla \log(\vec{p}_{k+1}(\tilde{x}_{k+1})), x_{k} - x_{k+1} \big\rangle \big) \\ &\approx \exp \big( \big\langle \nabla \log(\vec{p}_{k+1}(x_{k+1})), x_{k} - x_{k+1} \big\rangle \big),
% \end{talign}
% where $\tilde{x}_{k+1}$ is a convex combination of $x_k$ and $x_{k+1}$. And
% \begin{talign}
% &\exp \big( \log(\vec{p}_{k}(x_{k})) - \log(\vec{p}_{k+1}(x_{k})) = \exp \big( \log(\vec{p}(x_{k},kh)) - \log(\vec{p}(x_{k},(k+1)h)) \big) \\ &= \exp \big( - h \partial_t \! \log(\vec{p}(x_{k},\tilde{t})) \big) \approx \exp \big( - h \partial_t \! \log(\vec{p}(x_{k},kh)) \big)
% \end{talign}
% We observe that
% \begin{talign}
% \begin{split}
%     &\vec{p}_{k+1|k}(x_{k+1}|x_{k}) \exp \big( \log(\vec{p}_{k+1}(x_{k})) - \log(\vec{p}_{k+1}(x_{k+1})) \\ &\approx \frac{\exp \big( - \frac{\|x_{k} - (1+h) x_{k+1}\|^2 - \langle 4 h \nabla \log(\vec{p}_{k+1}(x_{k+1})), x_{k} - x_{k+1} \rangle}{4 h} \big)}{(4\pi h)^{d/2}} \\ &= \frac{\exp \big( - \frac{\|x_{k} - (1+h) x_{k+1} - 2 h \nabla \log(\vec{p}_{k+1}(x_{k+1})) \|^2 + h^2 (\| x_{k+1} \|^2 - \| x_{k+1} + 2 \nabla \log(\vec{p}_{k+1}(x_{k+1})) \|^2)}{4 h} \big)}{(4\pi h)^{d/2}} \\ &= \frac{\exp \big( - \frac{\|x_{k} - (1+h) x_{k+1} - 2 h \nabla \log(\vec{p}_{k+1}(x_{k+1})) \|^2}{4 h} \big)}{(4\pi h)^{d/2}} \times \exp \big( - \frac{h (\| x_{k+1} \|^2 - \| x_{k+1} + 2 \nabla \log(\vec{p}_{k+1}(x_{k+1})) \|^2)}{4} \big)
% \end{split}
% \end{talign}
And since $\vec{p}$ satisfies the Fokker-Planck equation
\begin{talign}
    \partial_t \vec{p}_t = \nabla \cdot \big( (-\vec{b}(x,t) + \frac{\vec{\sigma}(t)^2}{2} \nabla \log \vec{p}_t(x) ) \vec{p}_t \big),
\end{talign}
we have that
\begin{talign}
\begin{split}
    \partial_t \log \vec{p}_t &= \frac{\partial_t \vec{p}_t}{\vec{p}_t} = \frac{\nabla \cdot \big( (-\vec{b}(x,t) + \frac{\vec{\sigma}(t)^2}{2} \nabla \log \vec{p}_t(x) ) \vec{p}_t \big)}{\vec{p}_t} \\ &= 
    % d + \Delta \log \vec{p}_t(x) + \langle x + \nabla \log \vec{p}_t(x), \nabla \log \vec{p}_t(x) \rangle
    - \nabla \cdot \vec{b}(x,t) + \frac{\vec{\sigma}(t)^2}{2} \Delta \log \vec{p}_t(x) + \langle -\vec{b}(x,t) + \frac{\vec{\sigma}(t)^2}{2} \nabla \log \vec{p}_t(x), \nabla \log \vec{p}_t(x) \rangle.
\end{split}
\end{talign}
Hence,
\begin{talign}
\begin{split} \label{eq:partial_t}
    &\partial_t \log p(x_{k},kh) = \partial_t \log p(x_{k+1},(k+1)h) + O(h^{1/2}) \\ &= - \nabla \cdot \vec{b}(x_{k+1},(k+1)h) + \frac{\vec{\sigma}((k+1)h)^2}{2} \Delta \log \vec{p}(x_{k+1},(k+1)h) \\ &\qquad + \langle -\vec{b}(x_{k+1},(k+1)h) + \frac{\vec{\sigma}((k+1)h)^2}{2} \nabla \log \vec{p}(x_{k+1},(k+1)h), \nabla \log \vec{p}(x_{k+1},(k+1)h) \rangle + O(h^{1/2}).
\end{split}
\end{talign}
If we plug \eqref{eq:nabla_log_diff_x} and \eqref{eq:partial_t} into \eqref{eq:update_log_p_ito}, we obtain
\begin{talign}
\begin{split} \label{eq:log_difference}
    &\log p(x_{k+1},(k+1)h) - \log p(x_{k},kh) \\ &= h \big( \! - \! \nabla \cdot \vec{b}(x_{k+1},(k+1)h) \! + \! \langle -\vec{b}(x_{k+1},(k+1)h) \! + \! \frac{\vec{\sigma}((k+1)h)^2}{2} \nabla \log \vec{p}(x_{k+1},(k+1)h), \nabla \log \vec{p}(x_{k+1},(k+1)h) \rangle \big) \\ &\qquad + \langle \nabla \log p(x_{k+1},(k+1)h), x_{k+1} - x_k \rangle + O(h^{3/2}) \\ &= \frac{\langle 2 h \vec{\sigma}(kh)^2 \nabla \log p(x_{k+1},(k+1)h), x_{k+1} - x_k - h \vec{b}(x_{k+1},(k+1)h) \rangle}{2 h \vec{\sigma}(kh)^2} \\ &\qquad + h \big( - \nabla \cdot \vec{b}(x_{k+1},(k+1)h) + \frac{\vec{\sigma}((k+1)h)^2}{2} \| \nabla \log \vec{p}(x_{k+1},(k+1)h) \|^2 \big) + O(h^{3/2}).
\end{split}
\end{talign}
Applying a discrete time version of Ito's lemma again, we have that
\begin{talign}
\begin{split}
    \vec{b}(x_k,kh) 
    % &= \vec{b}(x_{k+1},(k+1)h) 
    &= \vec{b}(x_{k+1},(k+1)h) - h \big( \partial_t \vec{b}(x_{k+1},(k+1)h) + %\langle \nabla \log p(X_k,kh), \mu_k \rangle + 
    \frac{\vec{\sigma}((k+1)h)^2}{2} \Delta \vec{b}(x_{k+1},(k+1)h) 
    \big)
    % \\ &\qquad 
    %+ \sqrt{2h} 
    \\ &\qquad + \nabla \vec{b}(x_{k+1},(k+1)h)^{\top} (x_{k} - x_{k+1}) + O(h^{3/2}) \\ &= \vec{b}(x_{k+1},(k+1)h) + \nabla \vec{b}(x_{k+1},(k+1)h)^{\top} (x_{k} - x_{k+1}) + O(h).
\end{split}
\end{talign}
where $\Delta \vec{b}$ denotes the component-wise Laplacian of $\vec{b}$. Thus,
\begin{talign}
\begin{split} \label{eq:log_transition}
    &\log \vec{p}_{k+1|k}(x_{k+1}|x_{k}) \\ &= - \frac{d}{2} \log \big( 2\pi h \vec{\sigma}(kh)^2 \big) - \frac{\|x_{k+1} - x_{k} - h \vec{b}(x_k,kh) \|^2}{2 h \vec{\sigma}(kh)^2} \\ &= - \frac{d}{2} \log \big( 2\pi h \vec{\sigma}(kh)^2 \big) - \frac{\|x_{k+1} - x_{k} - h (\vec{b}(x_{k+1},(k+1)h) + \nabla \vec{b}(x_{k+1},(k+1)h)^{\top} (x_{k} - x_{k+1})) \|^2}{2 h \vec{\sigma}(kh)^2} + O(h^{3/2}) \\ &= - \frac{d}{2} \log \big( 2\pi h \vec{\sigma}(kh)^2 \big) - \frac{\|x_{k+1} - x_{k} - h \vec{b}(x_{k+1},(k+1)h) \|^2}{2 h \vec{\sigma}(kh)^2} + \frac{\langle x_{k+1} - x_{k}, \nabla \vec{b}(x_{k+1},(k+1)h)^{\top} (x_{k} - x_{k+1}) \rangle}{\vec{\sigma}(kh)^2} + O(h^{3/2}) \\ &= - \frac{d}{2} \log \big( 2\pi h \vec{\sigma}(kh)^2 \big) - \frac{\|x_{k+1} - x_{k} - h \vec{b}(x_{k+1},(k+1)h) \|^2}{ h \vec{\sigma}(kh)^2} - \frac{h \vec{\sigma}(kh)^2 \langle \epsilon_k, \nabla \vec{b}(x_{k+1},(k+1)h)^{\top} \epsilon_k \rangle}{\vec{\sigma}(kh)^2} + O(h^{3/2}) \\
    &= - \frac{d}{2} \log \big( 2\pi h \vec{\sigma}(kh)^2 \big) - \frac{\|x_{k+1} - x_{k} - h \vec{b}(x_{k+1},(k+1)h) \|^2}{ h \vec{\sigma}(kh)^2} - h \Delta \vec{b}(x_{k+1},(k+1)h) + O(h^{3/2}) 
\end{split}
\end{talign}
Combining \eqref{eq:log_difference} and \eqref{eq:log_transition}, we obtain that
\begin{talign}
\begin{split}
    &\log \vec{p}_{k+1|k}(x_{k+1}|x_{k}) - \big( \log p(x_{k+1},(k+1)h) - \log p(x_{k},kh) \big) \\ &= - \frac{d}{2} \log \big( 2\pi h \vec{\sigma}(kh)^2 \big) - \frac{\|x_{k+1} - x_{k} - h \vec{b}(x_{k+1},(k+1)h) + h \vec{\sigma}(kh)^2 \nabla \log p(x_{k+1},(k+1)h) \|^2}{ h \vec{\sigma}(kh)^2} + O(h^{3/2})
    \\ &= - \frac{d}{2} \log \big( 2\pi h \vec{\sigma}((k+1)h)^2 \big) - \frac{\|x_{k+1} - x_{k} - h \vec{b}(x_{k+1},(k+1)h) + h \vec{\sigma}((k+1)h)^2 \nabla \log p(x_{k+1},(k+1)h) \|^2}{ h \vec{\sigma}((k+1)h)^2} + O(h^{3/2}). 
    % \\ &h \Delta \vec{b}(x_{k+1},(k+1)h) 
\end{split}    
\end{talign}
By Bayes rule, and taking the exponential of this equation, we obtain
\begin{talign}
\begin{split} \label{eq:bayes_application}
    \vec{p}_{k+1|k}(x_{k+1}|x_{k}) &:= \vec{p}_{k+1|k}(x_{k+1}|x_{k}) \frac{\vec{p}_{k}(x_{k})}{\vec{p}_{k+1}(x_{k+1})} \\ &= \frac{\exp \big( - \frac{\|x_{k} - x_{k+1} + h \vec{b}(x_{k+1},(k+1)h) - h \vec{\sigma}((k+1)h)^2 \nabla \log p(x_{k+1},(k+1)h) \|^2}{2 h \vec{\sigma}((k+1)h)^2} \big)}{(2\pi h \vec{\sigma}((k+1)h)^2)^{d/2}} + O(h^{3/2}).
\end{split}    
\end{talign}
Up to the $O(h^{3/2})$ term, the right-hand side is the conditional Gaussian corresponding to the update
\begin{talign} \label{eq:cond_gaussian_update}
    x_{k} = x_{k+1} + h \big( - \vec{b}(x_{k+1},(k+1)h) + \vec{\sigma}((k+1)h)^2 \nabla \log p(x_{k+1},(k+1)h) \big) + \sqrt{h} \vec{\sigma}((k+1)h) \epsilon_{k+1}, \ \epsilon_{k+1} \sim N(0,I).
\end{talign}
If we define $y_k = x_{K-k}$, and we use that $b(x,t) = - \vec{b}(x,1-t) + \vec{\sigma}(t)^2 \nabla \log p(x,1-t)$, we can rewrite \eqref{eq:cond_gaussian_update} as
\begin{talign}
\begin{split}
    y_{K-k} &= y_{K-k-1} + h \big( - \vec{b}(y_{K-k-1},(K-k-1)h) + \vec{\sigma}((K-k-1)h)^2 \nabla \log p(y_{K-k-1},(K-k-1)h) \big) \\ &\qquad\qquad\quad + \sqrt{h} \vec{\sigma}((K-k-1)h) \epsilon_{k} = y_{K-k-1} + h b(y_{K-k-1},kh) + \sqrt{h} \sigma(kh) \epsilon_{K-k-1}, \\
    &\implies y_{k+1} = y_{k} + h b(y_{k},kh) + \sqrt{h} \sigma(kh) \epsilon_{k}.
\end{split}
\end{talign}
And this is the Euler-Maruyama discretization of the backward process $\cev{X}$.
If we plug \eqref{eq:bayes_application} into \eqref{eq:telescoping}, we obtain that 
\begin{talign}
    \vec{p}(x_{0:K}) &\approx \vec{p}_{K}(x_{K}) \prod_{k=0}^{K-1} \vec{p}_{k+1|k}(x_{k+1}|x_{k}).
    % \vec{p}_{k+1|k}(x_{k+1}|x_{k}) \frac{\vec{p}_{k}(x_{k})}{\vec{p}_{k+1}(x_{k+1})}
\end{talign}
which concludes the proof, as $\vec{p}_{K}(x_{K})$ is the initial distribution of the backward process, and $\vec{p}_{k+1|k}(x_{k+1}|x_{k})$ are its transition kernels. 
% which means that
% \begin{talign}
% \begin{split}
%     &\vec{p}_{k+1|k}(x_{k+1}|x_{k}) \exp \big( \log(\vec{p}_{k}(x_{k})) - \log(\vec{p}_{k+1}(x_{k+1})) \big) \\ &\approx \frac{1}{(2\pi h \sigma(kh)^2)^{d/2}} \exp \big( - \frac{\langle x_{k+1} - x_{k} - h \vec{b}(x_k,kh), x_{k+1} - x_{k} - h \vec{b}(x_k,kh) \rangle}{2 h \sigma(kh)^2} \big)
% \end{split}
% \end{talign}
% Note that the joint density of the backward process can be written as $\cev{p}(x_{0:K}) = \cev{p}_K(x_K) \prod_{k=0}^{K-1} \vec{p}_{k|k+1}(x_{k}|x_{k+1})$
% \begin{talign}
%     \vec{p}_{k|k+1}(x_{k}|x_{k+1}) = \frac{\exp \big( - \frac{\|x_{k} - (1+h) x_{k} - 2h s_{k}(x_{k})\|^2}{4 h} \big)}{(4\pi h)^{d/2}}.
% \end{talign}
% Thus, we can write
% \begin{talign}
%     \vec{p}(x_{0:K}) \approx \cev{p}(x_{0:K}) Q(x_{0:K}),
% \end{talign}
% where
% \begin{talign}
%     Q(x_{0:K}) &= \prod_{k=0}^{K-1} \exp \big( - \frac{h (\| x_{k+1} \|^2 - \| x_{k+1} + 2 \nabla \log(\vec{p}_{k+1}(x_{k+1})) \|^2)}{4} \\ &\qquad\qquad - h \big( d + \Delta \log \vec{p}_{k+1}(x_{k+1}) + \langle x_{k+1} + \nabla \log \vec{p}_{k+1}(x_{k+1}), \nabla \log \vec{p}_{k+1}(x_{k+1}) \rangle \big) \big) \\ &= \prod_{k=0}^{K-1} \exp \big( - h \big( d + \Delta \log \vec{p}_{k+1}(x_{k+1}) \big) \big)
% \end{talign}

% \subsection{Proof of \Cref{lem:DDPM_DDIM_FM}}
\subsection{The relationship between the noise predictor $\epsilon$ and the score function} \label{subsec:hat_epsilon_score}
    % The connection between both objects is well-known; we cover it briefly. It is key to introduce the forward and backward SDEs:
    % \begin{talign} \label{eq:forward_backward}
    %     \mathrm{d} \vec{X}_{t} &= - \frac{\dot{\alpha}_{1-t}}{2\alpha_{1-t}} \vec{X}_{t} \mathrm{d}t + \sqrt{\frac{\dot{\alpha}_{1-t}}{\alpha_{1-t}}} \mathrm{d}B_{t}, \qquad \vec{X}_0 \sim p_{\mathrm{data}}, \\ \mathrm{d} X_{t} &= \big( \frac{\dot{\alpha}_{t}}{2\alpha_{t}} X_{t} + \frac{\dot{\alpha}_{t}}{\alpha_{t}} \nabla \log \vec{p}_{1-t}(X_{t}) \big) \mathrm{d}t + \sqrt{\frac{\dot{\alpha}_{t}}{\alpha_{t}}} \mathrm{d}B_{t}, \qquad X_0 \sim N(0,I),
    % \end{talign}
    % where $\vec{p}_t$ denotes the distribution of $\vec{X}_t$. If we let $p_t$ be the distribution of $X_t$, then $\vec{p}_t$ and $p_t$ solve the Fokker-Planck equations
    % \begin{talign} \label{eq:FP_forward}
    %     \partial_t \vec{p}_t &= \nabla \cdot \big( \frac{\dot{\alpha}_{1-t}}{2\alpha_{1-t}} x \vec{p}_t \big) + \frac{\dot{\alpha}_{1-t}}{\alpha_{1-t}} \Delta \vec{p}_t, \qquad \vec{p}_0 = p_{\mathrm{data}}, \\
    %     \partial_t p_t &= \nabla \cdot \big( \big(-\frac{\dot{\alpha}_{t}}{2\alpha_{t}} x - \frac{\dot{\alpha}_{t}}{\alpha_{t}} \nabla \log \vec{p}_{1-t}(X_{t}) \big) p_t \big) + \frac{\dot{\alpha}_{t}}{2\alpha_{t}} \Delta p_t, \qquad p_0 = N(0,I). \label{eq:FP_backward}
    % \end{talign}
    % First, we show that when we define
    % % Applying \Cref{lem:OU_process} with 
    % $\kappa_t = \frac{\dot{\alpha}_{1-t}}{2\alpha_{1-t}}$, $\eta_t = \frac{\dot{\alpha}_{1-t}}{2\alpha_{1-t}}$, the conditions \eqref{eq:kappa_eta_conditions} hold. Since $(\alpha_t)_{t \geq 0}$ is increasing by construction, $\eta_t \geq 0$.
    % And we have that
    % \begin{talign}
    % \begin{split}
    %     &\exp \big( \! - \! \int_0^t \kappa_s \, \mathrm{d}s \big) \! = \! \exp \big( \! - \! \int_0^t \frac{\dot{\alpha}_{1-s}}{2\alpha_{1-s}} \, \mathrm{d}s \big) \! = \! \sqrt{\exp \big( \int_0^t \partial_s \log \alpha_{1-s} \, \mathrm{d}s \big)} \! = \! \sqrt{\exp \big( \log \frac{\alpha_{1-t}}{\alpha_{1}} \big)} \! = \! \sqrt{\alpha_{1-t}}, \\
    %     &\implies \exp \big( \! - \! \int_0^1 \kappa_s \, \mathrm{d}s \big) \! = \! \sqrt{\alpha_{0}} \! = \! 0, 
    % \end{split} \\
    % \begin{split}
    %     &\sqrt{2 \int_0^t \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t'} = \sqrt{ \int_0^t \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \exp \big( - \int_{t'}^{t} \frac{\dot{\alpha}_{1-s}}{\alpha_{1-s}} \, \mathrm{d}s \big) \, \mathrm{d}t'} \\ &= \sqrt{\int_0^t \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \frac{\alpha_{1-t}}{\alpha_{1-t'}} \, \mathrm{d}t'} = \sqrt{\alpha_{1-t} \int_0^t \partial_{t'} \big( \frac{1}{\alpha_{1-t'}} \big) \, \mathrm{d}t'} = \sqrt{\alpha_{1-t} \big( \frac{1}{\alpha_{1-t}} - \frac{1}{\alpha_{1}} \big)} = \sqrt{1 - \alpha_{1-t}}, \\
    %     &\implies \sqrt{2 \int_0^1 \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t'} = \sqrt{1 - \alpha_{0}} = 1.
    % \end{split}
    % \end{talign}
    % where we used that $\alpha_1 = 1$ and $\alpha_0 = 0$.
    Applying \Cref{lem:OU_process} with the choices of $(\kappa_t)_{t \geq 0}$ and $(\eta_t)_{t \geq 0}$ for DDIM, we obtain that $\vec{X}_t$ has the same distribution as
    \begin{talign} \label{eq:hat_X_t_diffusion}
        \hat{X}_t = \sqrt{\bar{\alpha}_{1-t}} \vec{X}_0 + \sqrt{1 - \bar{\alpha}_{1-t}} \epsilon, \qquad \epsilon \sim N(0,1).
    \end{talign}
    % Since $\hat{X}_t \sim N(0,1)$, then $\vec{p}_1 = N(0,1)$. Thus, $\vec{p}_{1-t}$ is a solution of the Fokker-Planck equation \eqref{eq:FP_backward}, which means that the distribution $p_t$ of $X_t$ is equal to the distribution $\vec{p}_{1-t}$ of $\vec{X}_{1-t}$. 
    
    Since $\vec{X}_t$ and $\hat{X}_t$ have the same distribution, predicting the noise of $\vec{X}_t$ is equivalent to predicting the noise of $\hat{X}_t$. The noise predictor $\epsilon$ can be written as:
    \begin{talign} \label{eq:hat_epsilon_app}
        \epsilon(x,t) := \mathbb{E}[\epsilon|\hat{X}_{1-t} = x] = \mathbb{E} \big[\epsilon| \sqrt{\bar{\alpha}_{t}} \vec{X}_0 + \sqrt{1 - \bar{\alpha}_{t}} \epsilon = x \big] = \mathbb{E} \big[\frac{x - \sqrt{\bar{\alpha}_{t}} \vec{X}_0}{\sqrt{1 - \bar{\alpha}_{t}}} | \sqrt{\bar{\alpha}_{t}} \vec{X}_0 + \sqrt{1 - \bar{\alpha}_{t}} \epsilon = x \big]
    \end{talign}
    And the score function $\mathfrak{s}(x,t) := \nabla \log \vec{p}_{1-t}(x)$ admits the expression
    \begin{talign} \label{eq:score_app}
        \mathfrak{s}(x,t) &:= %\nabla \log p_{t}(x) = 
        \nabla \log \vec{p}_{1-t}(x) = \frac{\nabla \vec{p}_{1-t}(x)}{\vec{p}_{1-t}(x)} = \frac{\nabla \mathbb{E}[\vec{p}_{1-t|0}(x|\vec{X}_0)]}{\vec{p}_{1-t}(x)} = \frac{\mathbb{E}[\nabla \log \vec{p}_{1-t|0}(x|\vec{X}_0) \vec{p}_{1-t|0}(x|\vec{X}_0)]}{\vec{p}_{1-t}(x)}, 
        % \\ &= \mathbb{E}\big[ - \frac{x - \sqrt{\alpha_{t}} \vec{X}_0}{1 - \alpha_{t}} | \sqrt{\alpha_{t}} \vec{X}_0 + \sqrt{1 - \alpha_{t}} \epsilon = x \big].
    \end{talign}
    where
    \begin{talign}
        \vec{p}_{1-t|0}(x|\vec{X}_0) = \frac{\exp(-\|x - \sqrt{\bar{\alpha}_t} Y_1\|^2/(2(1-\bar{\alpha}_{t})))}{(2\pi (1-\bar{\alpha}_{t}))^{d/2}} \implies \nabla \log \vec{p}_{t|1}(x|Y_1) = - \frac{x - \sqrt{\bar{\alpha}_t} Y_1}{1-\bar{\alpha}_{t}}. 
    \end{talign} 
    Plugging this into the right-hand side of \eqref{eq:score_app} and using Bayes' rule, we get
    \begin{talign} \label{eq:score_app_2}
        \mathfrak{s}(x,t) = \mathbb{E}\big[ - \frac{x - \sqrt{\bar{\alpha}_{t}} \vec{X}_0}{1 - \bar{\alpha}_{t}} | \sqrt{\bar{\alpha}_{t}} \vec{X}_0 + \sqrt{1 - \bar{\alpha}_{t}} \epsilon = x \big].
    \end{talign}
    % Here, the third equality holds because $\vec{p}_{1-t}$ is also the distribution of $\hat{X}_{1-t}$. 
    Comparing the right-hand sides of \eqref{eq:hat_epsilon_app} and \eqref{eq:score_app_2}, we obtain that $\mathfrak{s}(x,t) = - \frac{\epsilon(x,t)}{\sqrt{1-\bar{\alpha}_t}}$.
    % We prove \textit{(i)} and \textit{(ii)} together. The forward process for DDIM \citep[Eq.~4]{song2022denoising} is such that
    % \begin{talign}
    %     \vec{X}_t = \sqrt{\alpha_t} \vec{X}_0 + \sqrt{1-\alpha_t} \epsilon, \qquad \vec{X}_0 \sim p_1, \qquad \epsilon \sim N(0,1).
    % \end{talign}
    % % where $\alpha_{\cdot} : [0,1] \to [0,1]$ is a decreasing function such that $\alpha_0 = 1$, $\alpha_1 = 0$.
    % % For simplicity, and without loss of generality, we set $\alpha_t = 1-t$:
    % % \begin{talign}
    % %     \vec{X}_t = \sqrt{1-t} \vec{X}_0 + \sqrt{t} \epsilon, \qquad \vec{X}_0 \sim p_1, \qquad \epsilon \sim N(0,1),
    % % \end{talign}
    % If we perform the change of variables %$\sqrt{1-t} = e^{-\tau}$, 
    % $\sqrt{\alpha_t} = e^{-\tau}$, 
    % we obtain:
    % \begin{talign}
    %     \vec{X}_{\tau} = e^{-\tau} \vec{X}_0 + \sqrt{1-e^{-2\tau}} \epsilon, \qquad \vec{X}_0 \sim p_1, \qquad \epsilon \sim N(0,1),
    % \end{talign}
    % This has the same marginals as the solution of the Ornstein-Uhlenbeck process
    % \begin{talign}
    %     \mathrm{d} \vec{X}_{\tau} = - \vec{X}_{\tau} \mathrm{d}\tau + \sqrt{2} \mathrm{d}B_{\tau}, \qquad \vec{X}_0 \sim p_1    
    % \end{talign}
    % % Since $\tau = - \log(\sqrt{1-t})$, we have that $\mathrm{d}\tau = \frac{1}{2(1-t)} \mathrm{d}t$, which means that $\mathrm{d}B_\tau = \frac{1}{\sqrt{2(1-t)}} \mathrm{d}B_t$. 
    % Since $\tau = - \log(\sqrt{\alpha_t})$, we have that $\mathrm{d}\tau = - \frac{\dot{\alpha}_t}{2\alpha_t} \mathrm{d}t$, which means that $\mathrm{d}B_\tau = \sqrt{-\frac{\dot{\alpha}_t}{2\alpha_t}} \mathrm{d}B_t$. 
    % Thus,
    % \begin{talign} \label{eq:DDPM_SDE}
    %     % \mathrm{d} \vec{X}_{t} = - \frac{\vec{X}_{t}}{2(1-t)} \mathrm{d}t + \frac{1}{\sqrt{1-t}} \mathrm{d}B_{t}, \qquad \vec{X}_0 \sim p_1.
    %     \mathrm{d} \vec{X}_{t} = \frac{\dot{\alpha}_t}{2\alpha_t} \vec{X}_{t} \mathrm{d}t + \sqrt{-\frac{\dot{\alpha}_t}{\alpha_t}} \mathrm{d}B_{t}, \qquad \vec{X}_0 \sim p_1.
    % \end{talign}
    % This SDE corresponds to DDPM \citep{ho2020denoising}. To double-check it, remark that the transitions of the forward DDPM process are of the form \citep[Eq.~3]{song2022denoising}:
    % \begin{talign}
    % \begin{split}
    %     \vec{p}(x_{t}|x_{t-h}) &= N\bigg( \sqrt{\frac{\alpha_t}{\alpha_{t-h}}} x_{t-h}, \big( 1 - \frac{\alpha_t}{\alpha_{t-h}} \big) \mathrm{I} \bigg) 
    %     %= N\bigg( \sqrt{\frac{1-t}{1-t+h}} x_{t-h}, \big( 1 - \frac{1-t}{1-t+h} \big) \mathrm{I} \bigg) \\ &\approx N\bigg( (1-\frac{h}{2(1-t)}) x_{t-h}, \frac{h}{1-t} \mathrm{I} \bigg),
    %      = N\bigg( \sqrt{1 + \frac{\alpha_t-\alpha_{t-h}}{\alpha_{t-h}}} x_{t-h}, \frac{\alpha_{t-h} - \alpha_t}{\alpha_{t-h}} \mathrm{I} \bigg)
    %      \\ &\approx N\bigg( \big( 1 + \frac{\dot{\alpha}_{t-h} h}{\alpha_{t-h}} \big) x_{t-h}, - \frac{\dot{\alpha}_{t-h} h}{\alpha_{t-h}} \mathrm{I} \bigg)
    % \end{split}
    % \end{talign}
    % which is the Euler-Maruyama discretization of the SDE \eqref{eq:DDPM_SDE}.
    
    % With respect to DDPM, DDIM allows for arbitrary diffusion coefficients. To change the diffusion coefficient to an arbitrary scalar coefficient $\tilde{\sigma}(t)$ while preserving the marginals of the SDE \eqref{eq:DDPM_SDE}, we need to add the term %$\frac{1}{2}(\tilde{\sigma}(t)^2 - \frac{1}{1-t})\mathfrak{s}(\vec{X}_{t},t)$,
    % $\frac{1}{2}(\tilde{\sigma}(t)^2 + \frac{\dot{\alpha}_t}{\alpha_t})\mathfrak{s}(\vec{X}_{t},t)$,
    % where $\mathfrak{s}(x,t) = \nabla \log \vec{p}(x,t)$:
    % \begin{talign}
    %     % \mathrm{d}\vec{X}_{t} = \big(- \frac{\vec{X}_{t}}{2(1-t)} + \frac{1}{2}(\tilde{\sigma}(t)^2 - \frac{1}{1-t})\mathfrak{s}(\vec{X}_{t},t) \big) \mathrm{d}t + \tilde{\sigma}(t) \mathrm{d}B_{t}, \qquad \vec{X}_0 \sim p_1.
    %     \mathrm{d}\vec{X}_{t} = \big(\frac{\dot{\alpha}_t}{2\alpha_t} \vec{X}_{t} + \frac{1}{2}(\tilde{\sigma}(t)^2 + \frac{\dot{\alpha}_t}{\alpha_t})\mathfrak{s}(\vec{X}_{t},t) \big) \mathrm{d}t + \tilde{\sigma}(t) \mathrm{d}B_{t}, \qquad \vec{X}_0 \sim p_1.
    % \end{talign}

\subsection{The relationship between the vector field $v$ and the score function} \label{subsec:v_score}
    % Consider the flow matching framework \citep{lipman2023flow,albergo2023building,albergo2023stochastic} with %(backward) 
    % interpolation path %$\cev{X}_t = \alpha_t \cev{X}_0 + \beta_t \cev{X}_1$, 
    % % $\cev{Y}_t = \beta_t \cev{Y}_0 + \alpha_t \cev{Y}_1$, 
    % $Y_t = \beta_t Y_0 + \alpha_t Y_1$, 
    % where $Y_0 \sim p_0 = N(0,1)$ and $Y_1 \sim p_1 = p_{\mathrm{data}}$. 
    % % The forward interpolation path reads $\vec{Y}_t = \alpha_t \vec{Y}_0 + \beta_t \vec{Y}_1$, where $\vec{Y}_0 \sim p_1$ and $\vec{Y}_1 \sim p_0$.
    % % $\alpha_{\cdot} : [0,1] \to [0,1]$ is a decreasing function such that $\alpha_0 = 1$, $\alpha_1 = 0$, and $\beta_{\cdot} : [0,1] \to [0,1]$ is an increasing function such that $\beta_0 = 0$, $\beta_1 = 1$.
    % In this framework, a vector field $v_t$ is learned, and the ODEs
    % \begin{talign} %\label{eq:FM_forward}
    %     % \frac{d\vec{X}_t}{dt} &= -v(\vec{X}_t,t), \qquad \vec{X}_0 \sim p_1, \\
    %     % \frac{d\cev{X}_t}{dt} &= v(\cev{X}_t,1-t), \qquad \cev{X}_0 \sim p_0. 
    %     % \frac{d\vec{X}_t}{dt} &= -v(\vec{X}_t,1-t), \qquad \vec{X}_0 \sim p_1, \\
    %     \frac{\mathrm{d}X_t}{dt} &= v(X_t,t), \qquad X_0 \sim p_0.
    %     \label{eq:FM_backward}
    % \end{talign}
    % have solutions such that $p_t := \mathrm{Law}(X_t) = \mathrm{Law}(Y_t)$.
    By construction \citep{lipman2023flow,albergo2023building,albergo2023stochastic}, we have that
    \begin{talign} 
    \begin{split} \label{eq:v}
        % v_t(x) &= \mathbb{E}[ \dot{\alpha}_t X_0 + \dot{\beta}_t X_1 | x = \alpha_t X_0 + \beta_t X_1] \\ &= \mathbb{E}[ \dot{\alpha}_t X_0 + \frac{\dot{\beta}_t(x - \alpha_t X_0)}{\beta_t} | x = \alpha_t X_0 + \beta_t X_1] \\ &= \frac{\dot{\beta}_t}{\beta_t} x + (\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t) \mathbb{E}[ X_0 | x = \alpha_t X_0 + \beta_t X_1 ],
        % v_t(x) &= \mathbb{E}[ \dot{\beta}_t \cev{Y}_0 + \dot{\alpha}_t \cev{Y}_1 | x = \beta_t \cev{Y}_0 + \alpha_t \cev{Y}_1] \\ &= \mathbb{E}[ \dot{\beta}_t \cev{Y}_0 + \frac{\dot{\alpha}_t(x - \beta_t \cev{Y}_0)}{\alpha_t} | x = \beta_t \cev{Y}_0 + \alpha_t \cev{Y}_1] \\ &= \frac{\dot{\alpha}_t}{\alpha_t} x + (\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \mathbb{E}[ \cev{Y}_0 | x = \beta_t \cev{Y}_0 + \alpha_t \cev{Y}_1 ],
        % v(x,t) &= - \mathbb{E}[ \dot{\alpha}_t \vec{Y}_0 + \dot{\beta}_t \vec{Y}_1 | x = \alpha_t \vec{Y}_0 + \beta_t \vec{Y}_1] \\ &= - \mathbb{E}[ \frac{\dot{\alpha}_t(x - \beta_t \vec{Y}_1)}{\alpha_t} + \dot{\beta}_t \vec{Y}_1 | x = \alpha_t \vec{Y}_0 + \beta_t \vec{Y}_1] \\ &= - \big( \frac{\dot{\alpha}_t}{\alpha_t} x + (\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \mathbb{E}[ \vec{Y}_1 | x = \alpha_t \vec{Y}_0 + \beta_t \vec{Y}_1 ] \big), \\
        % v(x,t) &= \mathbb{E}[ \dot{\beta}_t \vec{Y}_0 + \dot{\alpha}_t \vec{Y}_1 | x = \beta_t \vec{Y}_0 + \alpha_t \vec{Y}_1] \\ &= \mathbb{E}[ \frac{\dot{\beta}_t(x - \alpha_t \vec{Y}_1)}{\beta_t} + \dot{\alpha}_t \vec{Y}_1 | x = \beta_t \vec{Y}_0 + \alpha_t \vec{Y}_1] \\ &= \frac{\dot{\alpha}_t}{\alpha_t} x + (\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \mathbb{E}[ \vec{Y}_1 | x = \alpha_t \vec{Y}_0 + \beta_t \vec{Y}_1 ],
        v(x,t) &= \mathbb{E}[ \dot{\alpha}_t Y_1 + \dot{\beta}_t Y_0 | x = \alpha_t Y_1 + \beta_t Y_0] \\ &= \mathbb{E}[ \frac{\dot{\alpha}_t(x - \beta_t Y_0)}{\alpha_t} + \dot{\beta}_t Y_0 | x = \alpha_t Y_1 + \beta_t Y_0] \\ &= \frac{\dot{\alpha}_t}{\alpha_t} x + (\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \mathbb{E}[ Y_0 | x = \alpha_t Y_1 + \beta_t Y_0 ], 
    \end{split}
    \end{talign}
    where we used that 
    % $\vec{Y}_0 = (x - \beta_t \vec{Y}_1)/\alpha_t$.
    $Y_1 = (x - \beta_t Y_0)/\alpha_t$.
    Also, we can write the score as follows 
    \begin{talign} \label{eq:score}
        % \mathfrak{s}(x,t) \! := \! \nabla \log p_t(x) \! = \! \frac{\nabla p_t(x)}{p_t(x)} \! = \! \frac{\nabla \mathbb{E}[p_{t|1}(x|X_1)]}{p_t(x)} \! = \! \frac{\mathbb{E}[\nabla p_{t|1}(x|X_1)]}{p_t(x)} \! = \! \frac{\mathbb{E}[p_{t|1}(x|X_1) \nabla \log p_{t|1}(x|X_1)]}{p_t(x)},
        % \mathfrak{s}(x,t) \! := \! \nabla \log \vec{p}_t(x) \! = \! \frac{\nabla \vec{p}_t(x)}{\vec{p}_t(x)} \! = \! \frac{\nabla \mathbb{E}[\vec{p}_{t|0}(x|\vec{Y}_0)]}{\vec{p}_t(x)} \! = \! \frac{\mathbb{E}[\nabla \vec{p}_{t|0}(x|\vec{Y}_0)]}{\vec{p}_t(x)} \! = \! \frac{\mathbb{E}[\vec{p}_{t|0}(x|\vec{Y}_0) \nabla \log \vec{p}_{t|0}(x|\vec{X}_0)]}{\vec{p}_t(x)},
        \mathfrak{s}(x,t) \! := \! \nabla \log p_t(x) \! = \! \frac{\nabla p_t(x)}{p_t(x)} \! = \! \frac{\nabla \mathbb{E}[p_{t|1}(x|Y_1)]}{p_t(x)} \! = \! \frac{\mathbb{E}[\nabla p_{t|1}(x|Y_1)]}{p_t(x)} \! = \! \frac{\mathbb{E}[p_{t|1}(x|Y_1) \nabla \log p_{t|1}(x|Y_1)]}{p_t(x)},
    \end{talign}
    where
    \begin{talign}
        % p_t(x|X_1) = \frac{\exp(-\|x - \beta_t X_1\|^2/(2\alpha^2_t))}{(2\pi \alpha^2_t)^{d/2}} \implies \nabla \log p_t(x|X_1) = - \frac{x - \beta_t X_1}{\alpha^2_t} \\
        % \vec{p}_{t|0}(x|\vec{Y}_0) = \frac{\exp(-\|x - \alpha_t \vec{Y}_0\|^2/(2\beta^2_t))}{(2\pi \beta^2_t)^{d/2}} \implies \nabla \log \vec{p}_{t|0}(x|\vec{Y}_0) = - \frac{x - \alpha_t \vec{Y}_0}{\beta^2_t} 
        p_{t|1}(x|Y_1) = \frac{\exp(-\|x - \alpha_t Y_1\|^2/(2\beta^2_t))}{(2\pi \beta^2_t)^{d/2}} \implies \nabla \log \vec{p}_{t|1}(x|Y_1) = - \frac{x - \alpha_t Y_1}{\beta^2_t} 
    \end{talign} 
    Plugging this back into the right-hand side of \eqref{eq:score}, we obtain
    \begin{talign} 
    \begin{split} \label{eq:score2}
        % \mathfrak{s}(x,t) &= - \frac{\mathbb{E}[ \vec{p}_{t|0}(x|\vec{Y}_0) \frac{x - \alpha_t \vec{Y}_0}{\beta^2_t} ]}{\vec{p}_{t}(x)} = - \frac{\int \vec{p}_{t|0}(x|\vec{Y}_0) \vec{p}_{0}(\vec{Y}_0) \frac{x - \alpha_t \vec{Y}_0}{\beta^2_t} \, d\vec{Y}_0}{\vec{p}_{t}(x)} \\ &= - \int p_{0|t}(\vec{Y}_0|x) \frac{x - \alpha_t \vec{Y}_0}{\beta^2_t} \, d\vec{Y}_0 = - \mathbb{E}[ \frac{x - \alpha_t \vec{Y}_0}{\beta^2_t} | x = \alpha_t \vec{Y}_0 + \beta_t \vec{Y}_1 ] \\ &= - \frac{\mathbb{E}[ \vec{Y}_1 | x = \alpha_t \vec{Y}_0 + \beta_t \vec{Y}_1 ]}{\beta_t}
        \mathfrak{s}(x,t) &= - \frac{\mathbb{E}[ p_{t|1}(x|Y_1) \frac{x - \alpha_t Y_1}{\beta^2_t} ]}{p_{t}(x)} = - \frac{\int \vec{p}_{t|1}(x|Y_1) p_{1}(Y_1) \frac{x - \alpha_t Y_1}{\beta^2_t} \, dY_1}{\vec{p}_{t}(x)} \\ &= - \int p_{1|t}(Y_1|x) \frac{x - \alpha_t Y_1}{\beta^2_t} \, dY_1 = - \mathbb{E}[ \frac{x - \alpha_t Y_1}{\beta^2_t} | x = \alpha_t Y_1 + \beta_t Y_0 ] %\\ &
        = - \frac{\mathbb{E}[ Y_0 | x = \alpha_t Y_1 + \beta_t Y_0 ]}{\beta_t}
    \end{split}
    \end{talign}
    The last equality holds because $(x - \alpha_t Y_1)/\beta_t = Y_0$. Putting together \eqref{eq:v} and \eqref{eq:score2}, we obtain that
    \begin{talign} \label{eq:v_and_s}
        % \mathfrak{s}(x,t) = -\frac{1}{\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t)} \big( v_t(x) - \frac{\dot{\alpha}_t}{\alpha_t} x \big) \iff v_t(x) = -\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \mathfrak{s}(x,t) + \frac{\dot{\alpha}_t}{\alpha_t} x 
        % \iff 
        % v(x,t) = \beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \mathfrak{s}(x,t) - \frac{\dot{\alpha}_t}{\alpha_t} x \iff \mathfrak{s}(x,t) = \frac{1}{\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t)} \big( v(x,t) + \frac{\dot{\alpha}_t}{\alpha_t} x \big)
        v(x,t) = \frac{\dot{\alpha}_t}{\alpha_t} x + \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t) \mathfrak{s}(x,t) \iff \mathfrak{s}(x,t) = \frac{1}{\beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)} \big( v(x,t) - \frac{\dot{\alpha}_t}{\alpha_t} x \big)
    \end{talign}
    Thus, the %forward 
    %backward 
    ODE \eqref{eq:FM_ode} 
    can be rewritten like this:
    \begin{talign} \label{eq:FM_ode_rewritten}
        % \frac{\mathrm{d}\vec{X}_t}{\mathrm{d}t} &= - \frac{\dot{\beta}_t}{\beta_t} \vec{X}_t + \alpha_t(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t) \mathfrak{s}(\vec{X}_t,t), \qquad \vec{X}_0 \sim p_1.
        \frac{\mathrm{d}X_t}{\mathrm{d}t} &= \frac{\dot{\alpha}_t}{\alpha_t} X_t + \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t) \mathfrak{s}(X_t,t), \qquad X_0 \sim p_0.
    \end{talign}
    To allow for an arbitrary diffusion coefficient, we need to add a correction term to the drift:
    \begin{talign} \label{eq:SDE_FM_generic}
        % \mathrm{d}\vec{X}_t &= \big( - \frac{\dot{\beta}_t}{\beta_t} \vec{X}_t + (\alpha_t(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t) + \frac{\tilde{\sigma}(t)^2}{2}) \mathfrak{s}(\vec{X}_t,t) \big) \mathrm{d}t + \tilde{\sigma}(t) \mathrm{d}B_t, \qquad \vec{X}_0 \sim p_1.
        \mathrm{d}X_t &= \big(\frac{\dot{\alpha}_t}{\alpha_t} X_t + \big( \frac{\sigma(t)^2}{2} + \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t) \big) \mathfrak{s}(X_t,t) \big) \mathrm{d}t + \sigma(t) \mathrm{d}B_t, \qquad X_0 \sim p_0.
    \end{talign}
    This can be easily shown by writing down the Fokker-Planck equations for \eqref{eq:FM_ode_rewritten} and \eqref{eq:SDE_FM_generic}, and observing that they are the same up to a cancellation of terms.
    Finally, if we plug the right-hand side of \eqref{eq:v_and_s} into \eqref{eq:SDE_FM_generic}, we obtain the SDE for Flow Matching with arbitrary noise schedule (equation  \eqref{eq:FM_general_diffusion_coeff}).

\section{Stochastic optimal control as maximum entropy RL in continuous space and time}
\label{subsec:max_ent_RL}

In this section, we bridge KL-regularized (or MaxEnt) reinforcement learning and stochastic optimal control. We show that when the action space is Euclidean and the transition probabilities are conditional Gaussians, taking the limit in which the stepsize goes to zero on the KL-regularized RL problem gives rise to the SOC problem.  A consequence of this connection is that all algorithms for KL-regularized RL admit an analog for diffusion fine-tuning. This is not novel, but it may be useful for researchers that are familiar with RL fine-tuning formulations. 

\Cref{subsec:proof_eq_cond_kl} is providing a more direct, rigorous, continuous-time connection between SOC and MaxEnt RL, as it shows that the expected control cost is equal to the KL divergence between the distributions over trajectories, conditioned on the starting points (see equation \eqref{eq:cond_kl}). 

\subsection{Maximum entropy RL}
% \textcolor{red}{Add connection to existing diffusion methods based on maxent rl}
Several diffusion fine-tuning methods \citep{black2024training,uehara2024finetuning} are based on KL-regularized RL, also known as maximum entropy RL, which we review in the following.
In the classical reinforcement learning (RL) setting, we have an agent that, starting from state $s_0 \sim p_0$, iteratively observes a state $s_k$, takes an action $a_k$ according to a policy $\pi(a_k;s_k,k)$ which leads to a new state $s_{k+1}$ according to a fixed transition probability $p(s_{k+1}|a_k,s_k)$, and obtains rewards $r_k(s_k,a_k)$. 
% $r_{k+1}(s_{k+1})$. 
This can be summarized into a trajectory $\tau = ((s_k,a_k))_{k=0}^{K}$. 
% $\tau = (s_k)_{k=0}^{K}$.
The goal is to optimize the policy $\pi$ in order to maximize the expected total reward, i.e. $\max_{\pi} \mathbb{E}_{\tau \sim \pi,p} [\sum_{k=0}^{K} r_k(s_k,a_k)
% r_k(s_k)
]$. 

% Without loss of generality and to simplify the exposition, 
% To simplify the exposition, we can merge the policy $\pi$ and the transition probability $p$ into the transition kernel $\pi(s_{k+1};s_k,k) := \sum_{a_k} \pi(a_k;s_k,k) p(s_{k+1}|a_k,s_k)$ by integrating out the actions. The resulting problem is $\max_{\pi} \mathbb{E}_{\tau \sim \pi} [\sum_{k=0}^{K} r_k(s_k)]$, where the transition kernel $\pi$ may be subject to certain constraints, by construction.

Maximum entropy RL (MaxEnt RL; \cite{ziebart2008maximum}) amounts to adding the entropy $H(\pi)$ of the %transition kernel 
policy $\pi(\cdot;s_k,k)$ to the reward for each step $k$, in order to encourage exploration and improve robustness to changes in the environment: $\max_{\pi} \mathbb{E}_{\tau \sim \pi,p} [\sum_{k=0}^{K} r_k(s_k,a_k)
% r_k(s_k) 
+ \sum_{k=0}^{K-1} H(\pi(\cdot;s_k,k))]$ \footnote{The entropy terms are usually multiplied by a factor to tune their magnitude, but one can equivalently rescale the rewards, which is why we do not add any factor.}. As a generalization, one can regularize using the negative KL divergence between $\pi(\cdot;s_k,k)$ and a base %transition kernel 
policy $\pi_{\mathrm{base}}(\cdot;s_k,k)$: 
\begin{talign} \label{eq:maxent_rl}
\max_{\pi} \mathbb{E}_{\tau \sim \pi,p} [\sum_{k=0}^{K} r_k(s_k,a_k) 
% r_k(s_k)
- \sum_{k=0}^{K-1} \mathrm{KL}(\pi(\cdot;s_k,k)||\pi_{\mathrm{base}}(\cdot;s_k,k))],
\end{talign}
which prevents the learned policy to deviate too much from the base policy. Each %transition kernel 
policy $\pi$ induces a distribution $q(\tau)$ over trajectories $\tau$, and the MaxEnt RL problem \eqref{eq:maxent_rl} can be expressed solely in terms of such distributions (\Cref{lem:KL_equality} in \Cref{sec:proofs_max_ent_RL}):
\begin{talign} \label{eq:distribution_maxent_RL_first}
    \max_{q} \mathbb{E}_{\tau \sim q}[\sum_{k=0}^{K} r_k(s_k,a_k)
    % r_k(s_k)
    ] - \mathrm{KL}(q||q^{\mathrm{base}}),
\end{talign}
where $q^{\mathrm{base}}$ is the distribution induced by the base %transition kernel 
policy $\pi_{\mathrm{base}}$, and the maximization is over all distributions $q$ such that their marginal for $s_0$ is $p_0$. We can further recast this problem as (\Cref{lem:q_star_lemma} in \Cref{sec:proofs_max_ent_RL}):
\begin{talign} \label{eq:distribution_maxent_RL}
    &\min_{q} \mathrm{KL}(q||q^*), \qquad \text{where} \ 
     q^*(\tau) := 
     % \frac{q^{\mathrm{base}}(\tau) \exp\big( \sum_{k=0}^{K} r_k(s_k,a_k) \big)}{\frac{1}{p_0(s_0)} \sum_{\{\tau' | s'_0 = s_0 \}} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k) \big)}, 
    q^{\mathrm{base}}(\tau) \exp\big( \sum_{k=0}^{K} r_k(s_k,a_k) 
    % r_k(s_k) 
    - \mathcal{V}(s_0,0) \big), 
    % \\
    % \begin{split}
    % &\qquad\qquad \text{with} \ \mathcal{V}(s_0,0) = \log\big( \mathbb{E}_{\tau \sim \pi_{\mathrm{base}},p}[ \exp\big( \sum_{k=0}^{K} r_k(s_k,a_k) \big) | s_0] \big) \\ &\qquad\qquad\qquad\qquad\quad \ \ = \max_{\pi} \mathbb{E}_{\tau \sim \pi,p} \big[\sum_{k=0}^{K} r_{k}(s_{k},a_{k}) - \sum_{k=0}^{K-1} \mathrm{KL}(\pi(\cdot;s_{k},k)||\pi_{\mathrm{base}}(\cdot;s_{k},k)) | s_0 \big]
    % \end{split}
\end{talign}
where 
\begin{talign}
\begin{split} \label{eq:value_function_maxent_RL}
    \mathcal{V}(s_k,k) &:= \log\big( \mathbb{E}_{\tau \sim \pi_{\mathrm{base}},p}[ \exp\big( \sum_{k'=k}^{K} r_{k'}(s_{k'},a_{k'}) 
    % r_{k'}(s_{k'}) 
    \big) | s_k] \big) \\ &= \max_{\pi} \mathbb{E}_{\tau \sim \pi,p} \big[\sum_{k'=k}^{K} r_{k'}(s_{k'},a_{k'})
    % r_{k'}(s_{k'}) 
    - \sum_{k'=k}^{K-1} \mathrm{KL}(\pi(\cdot;s_{k'},k')||\pi_{\mathrm{base}}(\cdot;s_{k'},k')) | s_{k} \big]
\end{split}
\end{talign}
is the value function.
% which makes apparent 
Problem \eqref{eq:distribution_maxent_RL} directly implies that the distribution induced by the optimal policy $\pi^*$ is the tilted distribution $q^*$ (which has initial marginal $p_0$). 
% The value function $\mathcal{V}$ is the analog of the value function $V$ presented in \Cref{sec:soc_problem}, up to a sign flip.

% \subsection{The stochastic optimal control problem} \label{sec:soc_problem}

% Let $(\Omega, \mathcal{F}, (\mathcal{F}_t)_{t\geq 0}, \mathcal{P})$ be a fixed filtered probability space on which a Brownian motion $B=(B_t)_{t\geq 0}$ is defined. We consider the control-affine problem
% \begin{talign} \label{eq:control_problem_def_diffusion}
%     &\min\limits_{u \in \mathcal{U}} \mathbb{E} \big[ \int_0^T 
%     \big(\frac{1}{2} \|u(X^u_t,t)\|^2 \! + \! f(X^u_t,t) \big) \, \mathrm{d}t \! + \! 
%     g(X^u_T) \big], \\
%     \begin{split}
%     \text{s.t.}~ \mathrm{d}X^u_t \! = \! (b(X^u_t,t) \! + \! \sigma(t) u(X^u_t,t)) \, \mathrm{d}t \! + \! %\sqrt{\lambda} 
%     \sigma(t) \mathrm{d}B_t, \qquad X^u_0 \sim p_0
%     \end{split} 
%     \label{eq:controlled_SDE_diffusion}
% \end{talign}
% and where $X_t^u \in \R^d$ is the state, $u : \R^d \times [0,T] \to \R^d$ is the feedback control and belongs to the set of admissible controls $\mathcal{U}$, $f : \R^d \times [0,T] \to \R$ is the state cost, $g : \R^d \to \R$ is the terminal cost, $b : \R^d \times [0,T] \to \R^d$ is the base drift, and $\sigma : [0,T] \to \R^{d \times d}$ is the invertible diffusion coefficient. 
% %and $\lambda \in (0, +\infty)$ is the noise level. 
% For a formal treatment of the technical assumptions on these objects, and for connections between the problem \eqref{eq:control_problem_def}-\eqref{eq:controlled_SDE} and the Hamilton-Jacobi-Bellman equation, forward-backward SDEs, path-integral formulas, and importance sampling on diffusions, we refer the reader to \cite{domingoenrich2023stochastic} or \cite{nüsken2023solving}. Two relevant facts are \textit{(i)} that the optimal control $u^*$ for \eqref{eq:control_problem_def}-\eqref{eq:controlled_SDE} admits the expression 
% \begin{talign} \label{eq:optimal_control}
% u^*(x,t) = - \sigma(t)^{\top} \nabla V(x,t),
% \end{talign}
% where $V(x,t) := \min_{u\in \mathcal{U}} J(u;x,t)$ is the value function and 
% \begin{talign} \label{eq:cost_functional}
% J(u;x,t) := \mathbb{E} \big[ \int_t^T 
% \big(\frac{1}{2} \|u(X^u_t,t)\|^2 \! + \! f(X^u_t,t) \big) \, \mathrm{d}t \! + \! 
% g(X^u_T) | X^u_t = x \big]
% \end{talign}
% is the cost functional, and \textit{(ii)} that the value function can be expressed in terms of an expectation involving the uncontrolled process $X$, which is the process $X^u$ with $u \equiv 0$: 
% \begin{talign}
% V(x,t) \! = \! - %\lambda 
% \log \mathbb{E} \big[ \exp \big( \! - \! %\lambda^{-1} 
% \int_t^T f(X_s,s) \, \mathrm{d}s - \! 
% %\lambda^{-1} 
% g(X_T) \big) \big| X_t = x \big].
% \end{talign}

\subsection{From maximum entropy RL to stochastic optimal control}
The following well-known result, which we prove in \Cref{sec:proofs_max_ent_RL}, shows that in a natural sense, the continuous-time continuous-space version of MaxEnt RL is the SOC
% stochastic optimal control 
framework introduced in \Cref{sec:SOC_formulation}. In particular, when states and actions are vectors in $\R^d$, policies are specified by a vector field $u$ (the control), and transition probabilities are conditional Gaussians, the MaxEnt RL problem becomes an SOC problem when the number of timesteps grows to infinity. 

\begin{proposition} \label{prop:max_ent_stochastic_optimal_control}
    Suppose that 
    \begin{enumerate}[label=(\roman*)]
    % \textit{(i)} 
    \item The state space and the action space are 
    $\R^d$,
    \item Policies $\pi$ are specified as $\pi(a_k;s_k,k) = \delta(a_k - u(s_k,kh))$, where $u : \R^d \times [0,T] \to \R^d$ is a vector field, and $\delta$ denotes the Dirac delta, 
    \item 
    Transition probabilities are conditional Gaussian densities: $p(s_{k+1}|a_k,s_k) = N(s_k + h (b(s_k,kh) + \sigma(kh) a_k), h 
    \sigma(kh)\sigma(kh)^{\top})$, where $h  = T/K$ is the stepsize, and $b$ and $\sigma$ are defined as in \Cref{sec:SOC_formulation}. 
    % \item Transition kernels $\pi$ are restricted to be of the form 
    % \begin{talign}
    % \pi(s_{k+1}|s_k,k) = N(s_k + h (b(s_k,kh) + \sigma(kh) u(s_k,kh)), h \sigma(kh)\sigma(kh)^{\top}),
    % \end{talign}
    % where $h  = T/K$ is the stepsize, and $b$, $u$ and $\sigma$ are fixed and defined as in \Cref{sec:soc_problem}. That is, each vector field $u$ is associated to a transition kernel $\pi$. The base transition kernel is the one for which $u \equiv 0$.
    \end{enumerate}
    Then, in the limit in which the number of steps $K$ grows to infinity, the problem \eqref{eq:maxent_rl} is equivalent to the SOC
    % stochastic optimal control 
    problem \eqref{eq:control_problem_def}-\eqref{eq:controlled_SDE}, identifying
    \begin{itemize}
    \item the sequence of states $(s_k)_{k=0}^{k}$ with the trajectory $\bm{X}^u = (X^u_t)_{t \in [0,1]}$,
    \item the running reward $\sum_{k=0}^{K-1} r_k(s_k,a_k)$ with the negative running cost $- \int_0^T f(X^u_t,t) \, \mathrm{d}t$, 
    \item the terminal reward $r_K(s_K,a_K)$ with the negative terminal cost $- 
    g(X^u_T)$, 
    \item the KL regularization $\mathbb{E}_{\tau \sim \pi,p}[\sum_{k=0}^{K-1} \mathrm{KL}(\pi(\cdot;s_k,k)||\pi_{\mathrm{base}}(\cdot;s_k,k))]$ with $\frac{1}{2}$ times the expected $L^2$ norm of the control $\frac{1}{2} \mathbb{E} \big[ \int_0^T 
   \|u(X^u_t,t)\|^2 \, \mathrm{d}t \big]$,
   \item and the value function $\mathcal{V}(s_k,k)$ defined in \eqref{eq:value_function_maxent_RL} with the negative value function $-V(x,t)$ defined in \Cref{sec:SOC_formulation}. 
   \end{itemize}
\end{proposition}

A first consequence of this result is that every loss function designed for generic MaxEnt RL problems has a corresponding loss function for SOC
% stochastic optimal control 
problems. The geometric structure of the latter allows for additional losses that do not have an analog in the classical MaxEnt RL setting; in particular, we can differentiate the state and terminal costs. 
% We provide a brief overview of relevant existing losses for SOC
% % stochastic optimal control 
% problems in \Cref{subsec:existing_losses} and \Cref{sec:more_losses}. %(\textcolor{red}{check}).

%  If we translate this result to the SOC
% % stochastic optimal control 
% setting,
% \subsection{A first approach to diffusion fine-tuning}


A second consequence of \Cref{prop:max_ent_stochastic_optimal_control} is that the characterization \eqref{eq:distribution_maxent_RL} can be translated to the SOC setting. The analogs of the distributions $q^{*}$, $q^{\mathrm{base}}$ induced by the optimal policy $\pi^{*}$ and the base policy $\pi^{\mathrm{base}}$ are the distributions 
% $\mathbb{P}^{*}$, $\mathbb{P}^{\mathrm{base}}$
$p^*, p^{\mathrm{base}}$ induced by the optimal control $u^*$ and the null control.
% \footnote{
% \textcolor{red}{explain what $\mathbb{P}^{*}$, $\mathbb{P}^{\mathrm{base}}$ mean in plain English}
% $\mathbb{P}^{*}$, $\mathbb{P}^{\mathrm{base}}$ are the limiting objects of the joint distributions $q^{*}$, $q^{\mathrm{base}}$ when the stepsize $h$ goes to zero.
% The statement in 
% }. 
For an arbitrary trajectory $\bm{X} = (X_t)_{t\in[0,T]}$, the 
% ratio between $\mathbb{P}^{*}$ and $\mathbb{P}^{\mathrm{base}}$ is given by:
relation between $\mathbb{P}^{*}$ and $\mathbb{P}^{\mathrm{base}}$ is given by
% \footnote{Equation \eqref{eq:optimal_distribution_SOC} is informal because densities over continuous-time processes are ill-defined; the formal statement is $\frac{\mathrm{d}\mathbb{P}^{*}}{\mathrm{d}\mathbb{P}^{\mathrm{base}}}(\bm{X}) = \exp ( - \int_0^T f(X_t,t) \, \mathrm{d}t - g(X_T) + V(X_0,0))$, where $\frac{\mathrm{d}\mathbb{P}^{*}}{\mathrm{d}\mathbb{P}^{\mathrm{base}}}$ is the Radon-Nikodym derivative. This is the notation used in the proofs.}:
% We obtain that the distribution over trajectories $\mathbb{P}^{*}$ induced by the optimal control $u^*$ can be expressed in terms of the distribution $\mathbb{P}^{\mathrm{base}}$ of the uncontrolled process: %and the state and terminal costs:
\begin{talign}
%     % \frac{\mathrm{d}\mathbb{P}^{*}}{\mathrm{d}\mathbb{P}^{\mathrm{base}}}(X_{0:T})
%     % \mathbb{P}^{*}
%     p^{*}(\bm{X}) = 
%     % \mathbb{P}^{\mathrm{base}}
%     p^{\mathrm{base}}(\bm{X}) \exp \big( - \int_0^T f(X_t,t) \, \mathrm{d}t - g(X_T) + V(X_0,0) \big), 
    \frac{\mathrm{d}\mathbb{P}^{*}}{\mathrm{d}\mathbb{P}^{\mathrm{base}}}(\bm{X}) = \exp ( - \int_0^T f(X_t,t) \, \mathrm{d}t - g(X_T) + V(X_0,0))
\end{talign}
% \textcolor{red}{Add footnote that it is more formal in appendix.}
where $V$ is the value function as defined in \Cref{sec:SOC_formulation}. Note that this matches the statement in \eqref{eq:optimal_distribution_SOC}.
% As we show in \Cref{subsec:diffusion_SOC}, this characterization of $\mathbb{P}^*$ is critical to connect diffusion fine-tuning to stochastic optimal control.

\subsection{Proof of \Cref{prop:max_ent_stochastic_optimal_control}: from MaxEnt RL to SOC} \label{sec:proofs_max_ent_RL}

% \subsection{Proof of \Cref{prop:max_ent_stochastic_optimal_control}} \label{subsec:max_ent}

Since the transition $p(s_{k+1}|a_k,s_k)$ is fixed, for each $\pi$ we can define 
\begin{talign} \label{eq:tilde_pi_def}
\tilde{\pi}(a_k,s_{k+1};s_k,k) = \pi(a_k;s_k,k) p(s_{k+1}|a_k,s_k) \ \text{and} \ \tilde{\pi}_{\mathrm{base}}(a_k,s_{k+1};s_k,k) = \pi_{\mathrm{base}}(a_k;s_k,k) p(s_{k+1}|a_k,s_k), 
\end{talign}
and reexpress \eqref{eq:maxent_rl} as (see \Cref{lem:KL_equality})
\begin{talign} \label{eq:loss_maxent_RL}
    \min_{\tilde{\pi}} \mathbb{E}_{\tau \sim \tilde{\pi}} [\sum_{k=0}^{K} r_k(s_k,a_k) - \sum_{k=0}^{K-1} \mathrm{KL}(\tilde{\pi}(\cdot,\cdot;s_k,k)||\tilde{\pi}_{\mathrm{base}}(\cdot,\cdot;s_k,k))].
\end{talign}
% When the state space is the Euclidean space $\R^d$, it is natural to set actions to be vectors in $\R^d$, and transition probabilities to be conditional Gaussian densities. In other words, we can uniquely specify a policy $\pi(a_k;s_k,k) = \delta(a_k - u(s_k,k\eta))$ through a vector field $u : \R^d \times [0,T] \to \R^d$ ($\delta$ is a Dirac delta), and we can assume that $p(s_{k+1}|a_k,s_k) = N(s_k + \eta (b(s_k,k\eta) + \sigma(k\eta) a_k), \eta \lambda \sigma(k\eta)\sigma(k\eta)^{\top})$, where $\eta  = T/K$ is the stepsize, and $b$ and $\sigma$ are defined as in \Cref{sec:soc_problem}. Thus, 
Using the hypothesis of the proposition, we can write
\begin{talign}
\begin{split}
\tilde{\pi}(a_k,s_{k+1};s_k,k) &= \delta(a_k - u(s_k,k\eta)) N(s_k + \eta (b(s_k,k\eta) + \sigma(k\eta) a_k), \eta \sigma(k\eta)\sigma(k\eta)^{\top}) \\ &= \delta(a_k - u(s_k,k\eta)) \tilde{\pi}(s_{k+1};s_k,k), 
\end{split}
\end{talign}
where $\tilde{\pi}(s_{k+1};s_k,k) = N(s_k + \eta (b(s_k,k\eta) + \sigma(k\eta) u(s_k,k\eta)), \eta \sigma(k\eta)\sigma(k\eta)^{\top})$ is the state transition kernel. We set the base policy as $\pi_{\mathrm{base}}(a_k;s_k,k) = \delta(a_k)$, and we obtain analogously that $\tilde{\pi}(a_k,s_{k+1};s_k,k) = \delta(a_k) \tilde{\pi}_{\mathrm{base}}(s_{k+1};s_k,k)$ with $\tilde{\pi}_{\mathrm{base}}(s_{k+1};s_k,k) = N(s_k + \eta b(s_k,k\eta), \eta \sigma(k\eta)\sigma(k\eta)^{\top})$. Now, if we take $K$ large, the trajectory $(s_k)_{k=0}^{K}$ generated by $\tilde{\pi}$ can be regarded as the Euler-Maruyama discretization of a solution $X^u$ of the controlled SDE \eqref{eq:controlled_SDE}, while the trajectory generated by $\tilde{\pi}_{\mathrm{base}}$ is the discretization of the uncontrolled process $X^0$ obtained by setting $u = 0$. As a consequence
\begin{talign}
\begin{split}
    &\lim_{K \to \infty} \mathbb{E}_{\tau \sim \tilde{\pi}} [\sum_{k=0}^{K-1}\mathrm{KL}(\tilde{\pi}(\cdot,\cdot;s_k,k)||\tilde{\pi}_{\mathrm{base}}(\cdot,\cdot;s_k,k))] \\ &= \lim_{K \to \infty} \mathbb{E}_{\tau \sim \tilde{\pi}} [\sum_{k=0}^{K-1}\mathrm{KL}(\tilde{\pi}(\cdot;s_k,k)||\tilde{\pi}_{\mathrm{base}}(\cdot;s_k,k))] = \mathbb{E}_{X^u \sim \mathbb{P}^{u}} [ \log \frac{\mathrm{d}\mathbb{P}^{u}}{\mathrm{d}\mathbb{P}^{0}}(X^u)],
\end{split}
\end{talign}
where $\mathbb{P}^{u}$ and $\mathbb{P}^{0}$ are the measures of the processes $X^u$ and $X^0$, respectively. The Girsanov theorem (\Cref{cor:girsanov_sdes}) implies that $\log \frac{\mathrm{d}\mathbb{P}^{u}}{\mathrm{d}\mathbb{P}^{0}}(X^u) = - \int_0^T \langle u(X^u_t,t), \mathrm{d}B_t \rangle - \frac{1}{2} \int_0^T \|u(X^u_t,t)\|^2 \, \mathrm{d}t$, which implies that $\mathbb{E}_{X^u \sim \mathbb{P}^{u}}[\log \frac{\mathrm{d}\mathbb{P}^{u}}{\mathrm{d}\mathbb{P}^{0}}(X^u)] = - \frac{1}{2} \mathbb{E}_{X^u \sim \mathbb{P}^{u}}[\int_0^T \|u(X^u_t,t)\|^2 \, \mathrm{d}t]$. Setting the rewards $r_k(a_k,s_k) = \eta f(s_k, k\eta)$ for $k \in \{0,\dots,K-1\}$ and $r_{K}(a_K,s_K) = \eta g(s_k)$, where $f$ and $g$ are as in \Cref{sec:SOC_formulation}, yields the following limiting object: 
\begin{talign}
\lim_{K \to \infty} \mathbb{E}_{\tau \sim \tilde{\pi}} [\sum_{k=0}^{K} r_k(s_k,a_k)] = \mathbb{E}_{X^u \sim \mathbb{P}^{u}}[\int_0^T f(X^u_t,t) \, dt + g(X^u_T)].
\end{talign}
Hence, the limit of the MaxEnt RL loss \eqref{eq:loss_maxent_RL} is the %stochastic optimal control 
SOC loss \eqref{eq:control_problem_def}. 

\begin{lemma} \label{lem:KL_equality}
Let $\tilde{\pi}(a_k,s_{k+1};s_k,k)$ and $\tilde{\pi}_{\mathrm{base}}(a_k,s_{k+1};s_k,k)$ be as defined in \eqref{eq:tilde_pi_def}. $\mathrm{KL}(\tilde{\pi}(\cdot,\cdot;s_k,k)||\tilde{\pi}_{\mathrm{base}}(\cdot,\cdot;s_k,k))]$ and $\mathrm{KL}(\pi(\cdot;s_k,k)||\pi_{\mathrm{base}}(\cdot;s_k,k))]$ are equal. Moreover, if $q$, $q^{\mathrm{base}}$ denote the distributions over trajectories induced by $\pi$, $\pi_{\mathrm{base}}$, we have that
\begin{talign} \label{eq:equality_KL}
    \mathrm{KL}(q||q^{\mathrm{base}}) = \mathbb{E}[\sum_{k=0}^{K-1} \mathrm{KL}(\pi(\cdot;s_k,k)||\pi_{\mathrm{base}}(\cdot;s_k,k))].
\end{talign}
\end{lemma}
\begin{proof}
    We have that 
    \begin{talign}
    \begin{split}
        &\mathrm{KL}(\tilde{\pi}(\cdot,\cdot;s_k,k)||\tilde{\pi}_{\mathrm{base}}(\cdot,\cdot;s_k,k))] = \sum_{a_k,s_{k+1}} \tilde{\pi}(a_k,s_{k+1};s_k,k) \log \frac{\tilde{\pi}(a_k,s_{k+1};s_k,k)}{\tilde{\pi}_{\mathrm{base}}(a_k,s_{k+1};s_k,k)} \\ &= \sum_{a_k,s_{k+1}} \pi(a_k;s_k,k) p(s_{k+1}|a_k,s_k) \log \frac{\pi(a_k;s_k,k) p(s_{k+1}|a_k,s_k)}{\pi_{\mathrm{base}}(a_k;s_k,k) p(s_{k+1}|a_k,s_k)} \\ &= \sum_{a_k,s_{k+1}} \pi(a_k;s_k,k) p(s_{k+1}|a_k,s_k) \log \frac{\pi(a_k;s_k,k)}{\pi_{\mathrm{base}}(a_k;s_k,k)} \\ &= \sum_{a_k} \pi(a_k;s_k,k) \big( \sum_{s_{k+1}} p(s_{k+1}|a_k,s_k) \big) \log \frac{\pi(a_k;s_k,k)}{\pi_{\mathrm{base}}(a_k;s_k,k)} \\ &= \sum_{a_k} \pi(a_k;s_k,k) \log \frac{\pi(a_k;s_k,k)}{\pi_{\mathrm{base}}(a_k;s_k,k)} = \mathrm{KL}(\pi(\cdot;s_k,k)||\pi_{\mathrm{base}}(\cdot;s_k,k))].
    \end{split}
    \end{talign}
    To prove \eqref{eq:equality_KL}, by construction we can write
    \begin{talign}
        q(\tau) = p_0(s_0) \prod_{k=0}^{K-1} \tilde{\pi}(a_k,s_{k+1};s_k,k), \qquad\qquad q^{\mathrm{base}}(\tau) = p_0(s_0) \prod_{k=0}^{K-1} \tilde{\pi}_{\mathrm{base}}(a_k,s_{k+1};s_k,k),
    \end{talign}
    which means that 
    \begin{talign}
    \begin{split}
        \mathrm{KL}(q||q^{\mathrm{base}}) &= \mathbb{E}_{\tau \sim q}[\log \frac{q(\tau)}{q^{\mathrm{base}}(\tau)}] = \mathbb{E}_{\tau \sim q}[\sum_{k=0}^{K-1} \log \frac{\tilde{\pi}(a_k,s_{k+1};s_k,k)}{\tilde{\pi}_{\mathrm{base}}(a_k,s_{k+1};s_k,k)}]
        \\ &= \sum_{k=0}^{K-1} \mathbb{E}_{\tau \sim q^{0:(k+1)}}[\log \frac{\tilde{\pi}(a_k,s_{k+1};s_k,k)}{\tilde{\pi}_{\mathrm{base}}(a_k,s_{k+1};s_k,k)}] \\ &= \sum_{k=0}^{K-1} \mathbb{E}_{\tau \sim q^{0:k}}[ \sum_{a_k, s_{k+1}} \tilde{\pi}(a_k,s_{k+1};s_k,k) \log \frac{\tilde{\pi}(a_k,s_{k+1};s_k,k)}{\tilde{\pi}_{\mathrm{base}}(a_k,s_{k+1};s_k,k)}] \\ &= \sum_{k=0}^{K-1} \mathbb{E}_{\tau \sim q^{0:k}}[ \mathrm{KL}(\tilde{\pi}(\cdot,\cdot;s_k,k)||\tilde{\pi}_{\mathrm{base}}(\cdot,\cdot;s_k,k))]
        \\ &= \sum_{k=0}^{K-1} \mathbb{E}_{\tau \sim q^{0:k}}[ \mathrm{KL}(\pi(\cdot;s_k,k)||\pi_{\mathrm{base}}(\cdot;s_k,k))]
        \\ &= \mathbb{E}_{\tau \sim q^{0:k}}[\sum_{k=0}^{K-1} \mathrm{KL}(\pi(\cdot;s_k,k)||\pi_{\mathrm{base}}(\cdot;s_k,k))]
    \end{split}
    \end{talign}
    Here, the notation $q^{0:k}$ denotes the trajectory $q$ up to the state $s_{k}$.
\end{proof}

\begin{lemma} \label{lem:q_star_lemma}
    The distribution-based MaxEnt RL formulation in \eqref{eq:distribution_maxent_RL_first} is equivalent to the the following problem:
    \begin{talign} \label{eq:equivalent_max_ent}
        \min_{q} \mathrm{KL}(q||q^*), \qquad \text{where} \ q^*(\tau) := \frac{q^{\mathrm{base}}(\tau) \exp\big( \sum_{k=0}^{K} r_k(s_k,a_k) 
        % r_k(s_k) 
        \big)}{\frac{1}{p_0(s_0)} \sum_{\{\tau' | s'_0 = s_0 \}} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k) 
        % r_k(s'_k) 
        \big)},
    \end{talign}
    where the minimization is over $q$ with marginal $p_0$ at step zero. The optimum of the problem is $q^*$, which satisfies the marginal constraint. The following alternative characterization of $q^*$ holds:
    \begin{talign}
        q^*(\tau) &= q^{\mathrm{base}}(\tau) \exp\big( \sum_{k=0}^{K} r_k(s_k,a_k) 
        % r_k(s_k) 
        - \mathcal{V}(s_0,0) \big), \\
        \text{where} \ \mathcal{V}(x,k) &= \max_{\pi} \mathbb{E}_{\tau \sim \pi,p} \big[\sum_{k'=k}^{K} r_{k'}(s_{k'},a_{k'})
        % r_{k'}(s_{k'}) 
        - \sum_{k'=k}^{K-1} \mathrm{KL}(\pi(\cdot;s_{k'},k')||\pi_{\mathrm{base}}(\cdot;s_{k'},k')) | s_k = x \big].
    \end{talign}
\end{lemma}
\begin{proof}
    Let us expand $\mathrm{KL}(q||q^*)$:
    \begin{talign}
    \begin{split} \label{eq:KL_q_q_star}
        \mathrm{KL}(q||q^*) &= \mathbb{E}_{\tau \sim q} \big[ \log \frac{q(\tau)}{q^*(\tau)} \big] \\ &= \mathbb{E}_{\tau \sim q} \big[ \log q(\tau) - \log q^{\mathrm{base}}(\tau) - \sum_{k=0}^{K} 
        r_k(s_k,a_k)
        % r_k(s_k) 
        \\ &\qquad\qquad + \log \big( \frac{1}{p_0(s_0)} \sum_{\{\tau' | s'_0 = s_0 \}} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k)
        % r_k(s'_k) 
        \big) \big) \big] \\
        &= \mathrm{KL}(q || q^{\mathrm{base}}) - \mathbb{E}_{\tau \sim q} \big[ \sum_{k=0}^{K} r_k(s_k,a_k) 
        % r_k(s_k) 
        \big] \\ &\qquad + \mathbb{E}_{s_0 \sim p_0} \big[ \log \big( \frac{1}{p_0(s_0)} \sum_{\{\tau' | s'_0 = s_0 \}} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k)
        % r_k(s'_k) 
        \big) \big) \big],
    \end{split}
    \end{talign}
    where the third equality holds because the marginal of $q$ at step zero is $p_0$ by hypothesis. Since the third term in the right-hand side is independent of $q$, this proves the equivalence between \eqref{eq:distribution_maxent_RL_first} and \eqref{eq:equivalent_max_ent}.
    
    Next, we prove that the marginal of $q^*$ at step zero is $p_0$:
    \begin{talign}
        \sum_{\{\tau | s_0 = x \}} q^*(\tau) := \sum_{\{\tau | s_0 = x \}} \frac{q^{\mathrm{base}}(\tau) \exp\big( \sum_{k=0}^{K} r_k(s_k,a_k) 
        % r_k(s_k) 
        \big)}{ \frac{1}{p_0(x)} \sum_{\{\tau' | s'_0 = x \}} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k)
        % r_k(s'_k) 
        \big)} = p_0(x).
        % \sum_{\{\tau | s_0 = x \}} q^*(\tau) := \sum_{\{\tau | s_0 = x \}} \frac{q^{\mathrm{base}}(\tau) \exp\big( \sum_{k=0}^{K} r_k(s_k,a_k) \big)}{\sum_{\tau'} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k) \big)}
    \end{talign}

    Now, for an arbitrary $s_0$, let $q_{s_0}$, $q^*_{s_0}$ be the distributions $q$, $q^*$ conditioned on the initial state being $s_0$. We can write an analog to equation \eqref{eq:KL_q_q_star} for $q_{s_0}$, $q^*_{s_0}$:
    \begin{talign}
    \begin{split}
        \mathrm{KL}(q_{s_0}||q^*_{s_0}) &= \mathbb{E}_{\tau \sim q_{s_0}} \big[ \log \frac{q_{s_0}(\tau)}{q^*_{s_0}(\tau)} \big] \\ &= \mathbb{E}_{\tau \sim q_{s_0}} \big[ \log q_{s_0}(\tau) - \log q^{\mathrm{base}}_{s_0}(\tau) - \sum_{k=0}^{K} r_k(s_k,a_k)
        % r_k(s_k) 
        \\ &\qquad\qquad + \log \big( \frac{1}{p_0(s_0)} \sum_{\{\tau' | s'_0 = s_0 \}} q^{\mathrm{base}}_{s_0}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k)
        % r_k(s'_k) 
        \big) \big) \big] \\
        &= \mathrm{KL}(q_{s_0} || q^{\mathrm{base}}_{s_0}) - \mathbb{E}_{\tau \sim q_{s_0}} \big[ 
        \sum_{k=0}^{K} r_k(s_k,a_k)
        % r_k(s_k) 
        \big] \\ &\qquad + 
        % \mathbb{E}_{s_0 \sim p_0} \big[ 
        \log \big( \frac{1}{p_0(s_0)} \sum_{\{\tau' | s'_0 = s_0 \}} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k)
        % r_k(s'_k) 
        \big) \big), 
        % \big],
    \end{split}
    \end{talign}
    Hence,
    \begin{talign}
    \begin{split}
        0 = \min_{q_{s_0}} \mathrm{KL}(q_{s_0}||q^*_{s_0}) &= - \max_{q_{s_0}} \{ \mathbb{E}_{\tau \sim q_{s_0}} \big[ 
        \sum_{k=0}^{K} r_k(s_k,a_k) \big] - \mathrm{KL}(q_{s_0} || q^{\mathrm{base}}_{s_0}) \} \\ &+ \log \big( \frac{1}{p_0(s_0)} \sum_{\{\tau' | s'_0 = s_0 \}} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k) \big) \big).
    \end{split}
    \end{talign}
    And applying \eqref{eq:equality_KL} from \eqref{eq:equality_KL}, we obtain that
    \begin{talign}
    \begin{split}
        &\log \big( \frac{1}{p_0(s_0)} \sum_{\{\tau' | s'_0 = s_0 \}} q^{\mathrm{base}}(\tau') \exp\big( \sum_{k=0}^{K} r_k(s'_k,a'_k) \big) \big) \\ &= \max_{\pi} \mathbb{E}_{\tau \sim \pi,p} \big[\sum_{k=0}^{K} r_{k}(s_{k},a_{k}) - \sum_{k=0}^{K-1} \mathrm{KL}(\pi(\cdot;s_{k},k)||\pi_{\mathrm{base}}(\cdot;s_{k},k)) | s_0 \big] = \mathcal{V}(s_0,0),
    \end{split}
    \end{talign}
    which concludes the proof.
\end{proof}

\subsection{Proof of equation \eqref{eq:cond_kl}: the control cost is a KL regularizer}
\label{subsec:proof_eq_cond_kl}

\begin{theorem}[Girsanov theorem for SDEs] 
\label{cor:girsanov_sdes}
    If the two SDEs
    \begin{talign}
    \mathrm{d}X_{t} &= b_1 (X_{t},t) \, \mathrm{d}t + \sigma (X_{t},t) \, \mathrm{d}B_{t}, \qquad X_0 = x_{\mathrm{init}}  \\
    dY_{t} &= (b_1 (Y_{t},t) + b_2 (Y_{t},t)) \, \mathrm{d}t + \sigma (Y_{t},t) \, \mathrm{d}B_{t}, \qquad Y_0 = x_{\mathrm{init}}
    \end{talign}
    admit unique strong solutions on $[0,T]$, then for any bounded continuous functional $\Phi$ on $C([0,T])$, we have that
    \begin{talign} 
    \begin{split} \label{eq:X_to_Y}
        \mathbb{E}[\Phi(\bm{X})] &= \mathbb{E}\big[ \Phi(\bm{Y}) \exp \big( - \int_0^T \sigma(Y_{t},t)^{-1} b_2 (Y_{t},t) \, \mathrm{d}B_t - \frac{1}{2} \int_0^T \|\sigma(Y_{t},t)^{-1} b_2 (Y_{t},t)\|^2 \, \mathrm{d}t \big) \big] \\ &= \mathbb{E}\big[ \Phi(\bm{Y}) \exp \big( - \int_0^T \sigma(Y_{t},t)^{-1} b_2 (Y_{t},t) \, d\tilde{B}_t + \frac{1}{2} \int_0^T \|\sigma(Y_{t},t)^{-1} b_2 (Y_{t},t)\|^2 \, \mathrm{d}t \big) \big], %\label{eq:X_to_Y_2}
    \end{split}
    \end{talign}
    where $\tilde{B}_t = B_t + \int_0^t \sigma(Y_{s},s)^{-1} b_2 (Y_{s},s) \, \mathrm{d}s$. More generally, $b_1$ and $b_2$ can be random processes that are adapted to filtration of $\bm{B}$.
\end{theorem}

Consider the SDEs
\begin{talign} \label{eq:X_app}
    \mathrm{d}X_t &=  b(X_t,t) \, \mathrm{d}t + 
    \sigma(t) \mathrm{d}B_t, \qquad &X_0 = x_0, \\
    \mathrm{d}X^u_t &=  \left( b(X^u_t,t) + \sigma(t) u(X^u_t,t) \right) \, \mathrm{d}t + 
    \sigma(t) \mathrm{d}B_t, \qquad &X^u_0 = x_0.
    \label{eq:X_u_app}
\end{talign}
If we let $\mathbb{P} \rvert_{x_0}$, $\mathbb{P}^u \rvert_{x_0}$ be the probability measures of the solutions of \eqref{eq:X_app} and \eqref{eq:X_u_app}, \Cref{cor:girsanov_sdes} implies that
\begin{talign}
    \log \frac{\mathrm{d}\mathbb{P} \rvert_{x_0}}{\mathrm{d}\mathbb{P}^u \rvert_{x_0}} (\bm{X}^u) = - \int_0^1 u(X^u_t,t) \, \mathrm{d}B_t - \frac{1}{2} \int_0^1 \| u(X^u_t,t) \|^2 \, \mathrm{d}t.
\end{talign}
Hence, 
\begin{talign}
\begin{split}
    \infdiv*{\mathbb{P}^u \rvert_{x_0}}{\mathbb{P} \rvert_{x_0}} &= \mathbb{E} \big[\log \frac{\mathrm{d}\mathbb{P}^u \rvert_{x_0}}{\mathrm{d}\mathbb{P} \rvert_{x_0}}(\bm{X}^u) | X^u_0 = x_0 \big] = - \mathbb{E} \big[\log \frac{\mathrm{d}\mathbb{P} \rvert_{x_0}}{\mathrm{d}\mathbb{P}^u \rvert_{x_0}} (\bm{X}^u) | X^u_0 = x_0 \big] \\ &= \mathbb{E} \big[ \int_0^1 u(X^u_t,t) \, \mathrm{d}B_t + \frac{1}{2} \int_0^1 \| u(X^u_t,t) \|^2 \, \mathrm{d}t | X^u_0 = x_0 \big] = \mathbb{E} \big[ \frac{1}{2} \int_0^1 \| u(X^u_t,t) \|^2 \, \mathrm{d}t | X^u_0 = x_0 \big],
\end{split}
\end{talign}
where we used that stochastic integrals are martingales.

% \section{Proofs of \Cref{subsec:diffusion_SOC}}
\section{Proofs of \Cref{sec:memoryless_schedule}: memoryless noise schedule and fine-tuning recipe}

\subsection{Proof of %\Cref{prop:memorylessness_property}: 
\Cref{prop:memorylessness_noise_schedule}:
the memoryless noise schedule} \label{subsec:proof_memoryless}
% \begin{proposition}
%     Suppose that $b(x,t) = \kappa_t x + \big( \eta_t - \frac{\tilde{\sigma}(t)^2}{2} \big) \mathfrak{s}(x,t)$, and that $\sigma(t) = \sqrt{2 \eta_t}$. Then, the forward process $\vec{X}$ satisfies that $\vec{X}_0$ and $\vec{X}_1$ are independent. Similarly, the backward process $\cev{X}$ satisfies that $\cev{X}_0$ and $\cev{X}_1$ are independent.
% \end{proposition}
% \begin{proof}
    % Under this choice of $\sigma$, the forward process reads
    % The pair of forward and backward SDEs introduced in \Cref{subsec:hat_epsilon_score} can be generalized to arbitrary $\kappa_t$ and $\eta_t$:
    % Consider the pair of forward and backward SDEs:
    % \begin{talign}
    %     \mathrm{d}\vec{X}_t &= - \kappa_t \vec{X}_t \, \mathrm{d}t + \sqrt{2 \eta_t} \, \mathrm{d}B_t, \qquad \vec{X}_0 \sim p_{\mathrm{data}}, \\
    %     \mathrm{d}X_t &= \big( \kappa_t X_t + \big( \frac{\sigma(t)^2}{2} + \eta_t \big) \mathfrak{s}(X_t,t) \big) \mathrm{d}t + \sqrt{2 \eta_t} \, \mathrm{d}B_t, \qquad X_0 \sim N(0,I),
    % \end{talign}
    We consider the forward-backward SDEs \eqref{eq:forward_arbitrary}-\eqref{eq:backward_arbitrary} with arbitrary noise schedule. By \Cref{lem:equal_process_distributions}, the trajectories $\vec{\bm{X}}$, $\bm{X}$ of these two processes are equally distributed up to a time flip, which also means that their marginals satisfy $\vec{p}_{t} = p_{1-t}$, for all $t \in [0,1]$.
    First, we develop an explicit expression for the score function $s(x,t) = \nabla \log p_t(x)$. By the properties of flow matching, we know that $p_t$ is the distribution of the interpolation variable $\bar{X}_t = \beta_t \bar{X}_0 + \alpha_t \bar{X}_1$, where $\bar{X}_0  \sim N(0,I), \bar{X}_1 \sim p^{\mathrm{data}}$ are independent. Thus, $\frac{\bar{X}_t - \alpha_t \bar{X}_1}{\beta_t} \sim N(0,\mathrm{I})$, which means that we can express the density $p_t$ as
    \begin{talign}
        p_t(x) = \int_{\mathbb{R}^d} \frac{\exp\big(-\frac{\|x-\alpha_t y\|^2}{2\beta_t^2}\big)}{(2\pi \beta_t^2)^{d/2}} p^{\mathrm{data}}(y) \, \mathrm{d}y. 
    \end{talign}
    Thus,
    \begin{talign}
        s(x,t) = \nabla \log p_t(x) = - \frac{x}{\beta_t^2} + \frac{\alpha_t}{\beta_t^2} \frac{\int_{\mathbb{R}^d} y \exp\big( - \frac{\|x-\alpha_t y\|^2}{2\beta_t^2}\big) p^{\mathrm{data}}(y) \, \mathrm{d}y}{\int_{\mathbb{R}^d} \exp\big( - \frac{\|x-\alpha_t y\|^2}{2\beta_t^2} \big) p^{\mathrm{data}}(y) \, \mathrm{d}y} := - \frac{x - \alpha_t \xi_t(x)}{\beta_t^2},
    \end{talign}
    where we defined
    \begin{talign}
        \xi_t(x) = \frac{\int_{\mathbb{R}^d} y \exp\big( - \frac{\|x-\alpha_t y\|^2}{2\beta_t^2}\big) p^{\mathrm{data}}(y) \, \mathrm{d}y}{\int_{\mathbb{R}^d} \exp\big( - \frac{\|x-\alpha_t y\|^2}{2\beta_t^2} \big) p^{\mathrm{data}}(y) \, \mathrm{d}y}.
    \end{talign}
    Hence, we can rewrite the forward SDE \eqref{eq:forward_arbitrary} as
    \begin{talign}
        \mathrm{d}\vec{X}_t &= \big( - \kappa_{1-t} \vec{X}_t - \big( \frac{\sigma(1-t)^2}{2} - \eta_{1-t} \big) \frac{\vec{X}_t - \alpha_{1-t} \xi_{1-t}(\vec{X}_{t})}{\beta_{1-t}^2} \big) \, \mathrm{d}t + \sigma(1-t) \, \mathrm{d}B_t, \qquad \vec{X}_0 \sim p_{\mathrm{data}}
    \end{talign}
    Hence, if we substitute $\kappa_{1-t} \gets \kappa_{1-t} + \frac{\sigma(1-t)^2 - 2\eta_{1-t}}{2\beta_{1-t}^2}$, $\xi_{1-t} \gets \frac{\alpha_{1-t}(\sigma(1-t)^2 - 2\eta_{1-t})}{2\beta_{1-t}^2} \xi_{1-t}(\vec{X}_t)$ (where we ignore the dependency on $\vec{X}_t$), $\sqrt{2 \eta_{1-t}} \gets \sigma(1-t)$, we can apply \Cref{lem:OU_process}, which yields
    \begin{talign}
    \begin{split} \label{eq:memoryless_solution}
        \vec{X}_t &= \vec{X}_0 \exp \big( - \int_0^t \big( \kappa_{1-s} + \frac{\sigma(1-s)^2 - 2\eta_{1-s}}{2\beta_{1-s}^2} \big) \, \mathrm{d}s \big) \\ &\qquad + \int_0^t \exp \big( - \int_{t'}^{t} \big( \kappa_{1-s} + \frac{\sigma(1-s)^2 - 2\eta_{1-s}}{2\beta_{1-s}^2} \big) \, \mathrm{d}s \big) \frac{\alpha_{1-t'}(\sigma(1-t')^2 - 2\eta_{1-t'})}{2\beta_{1-t'}^2} \xi_{1-t'}(\vec{X}_{t'}) \, \mathrm{d}t' \\ &\qquad + \int_0^t \sigma(1-t') \exp \big( - \int_{t'}^{t} \big( \kappa_{1-s} + \frac{\sigma(1-s)^2 - 2\eta_{1-s}}{2\beta_{1-s}^2} \big) \, \mathrm{d}s \big) \, \mathrm{d}B_{t'}.
    \end{split}
    \end{talign}
    We simplify the recurring expression: 
    \begin{talign}
        \kappa_{1-s} + \frac{\sigma(1-s)^2 - 2\eta_{1-s}}{2\beta_{1-s}^2} = \frac{\dot{\alpha}_{1-s}}{\alpha_{1-s}} + \frac{\sigma(1-s)^2 - 2 \beta_{1-s} \big( \frac{\dot{\alpha}_{1-s}}{\alpha_{1-s}} \beta_{1-s} - \dot{\beta}_{1-s} \big)}{2 \beta_{1-s}^2} = \frac{\sigma(1-s)^2}{2 \beta_{1-s}^2} + \frac{\dot{\beta}_{1-s}}{\beta_{1-s}}
    \end{talign}
    Thus,
    \begin{talign}
        \int_{t'}^t \big( \kappa_{1-s} + \frac{\sigma(1-s)^2 - 2\eta_{1-s}}{2\beta_{1-s}^2} \big) \, \mathrm{d}s = \int_{t'}^t \big( \frac{\sigma(1-s)^2}{2 \beta_{1-s}^2} - \partial_s \log \beta_{1-s} \big) \, \mathrm{d}s = \int_{t'}^t \frac{\sigma(1-s)^2}{2 \beta_{1-s}^2} \, \mathrm{d}s - \big( \log \beta_{1-t} - \log \beta_{1-t'} \big),
    \end{talign}
    which means that
    \begin{talign} \label{eq:simplification_1a}
        \exp \big( - \int_{t'}^{t} \big( \kappa_{1-s} + \frac{\sigma(1-s)^2 - 2\eta_{1-s}}{2\beta_{1-s}^2} \big) \, \mathrm{d}s \big) &= \exp \big( - \int_{t'}^t \frac{\sigma(1-s)^2}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \frac{\beta_{1-t}}{\beta_{1-t'}}, \\
        \frac{\alpha_{1-t'}(\sigma(1-t')^2 - 2\eta_{1-t'})}{2\beta_{1-t'}^2} \xi_{1-t'}(\vec{X}_{t'}) &= \alpha_{1-t'} \big( \frac{\sigma(1-t')^2}{2 \beta_{1-t'}^2} + \frac{\dot{\beta}_{1-t'}}{\beta_{1-t'}} - \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \big) \xi_{1-t'}(\vec{X}_{t'}).
        \label{eq:simplification_1b}
    \end{talign}
    If we define $\chi(1-s)$ such that $\sigma^2(1-s) = 2 \beta_{1-s} \big( \frac{\dot{\alpha}_{1-s}}{\alpha_{1-s}} \beta_{1-s} - \dot{\beta}_{1-s} \big) + \chi(1-s)$, we obtain that
    \begin{talign} \label{eq:simplification_2a}
    \begin{split}
        &\exp \big( - \int_{t'}^t \frac{\sigma(1-s)^2}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \frac{\beta_{1-t}}{\beta_{1-t'}} = \exp \big( - \int_{t'}^t \big( \frac{\dot{\alpha}_{1-s}}{\alpha_{1-s}} - \frac{\dot{\beta}_{1-s}}{\beta_{1-s}} + \frac{\chi(1-s)}{2 \beta_{1-s}^2}\big) \, \mathrm{d}s \big) \frac{\beta_{1-t}}{\beta_{1-t'}} \\ &= \exp \big( \int_{t'}^t \big( \partial_s \log \alpha_{1-s} - \partial_s \log \beta_{1-s} - \frac{\chi(1-s)}{2 \beta_{1-s}^2}\big) \, \mathrm{d}s \big) \frac{\beta_{1-t}}{\beta_{1-t'}} = \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \frac{\alpha_{1-t}}{\alpha_{1-t'}},
    \end{split} \\
        &\alpha_{1-t'} \big( \frac{\sigma(1-t')^2}{2 \beta_{1-t'}^2} + \frac{\dot{\beta}_{1-t'}}{\beta_{1-t'}} - \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \big) \xi_{1-t'}(\vec{X}_{t'}) = \frac{\alpha_{1-t'} \chi(1-t')}{2 \beta_{1-t'}^2} \xi_{1-t'}(\vec{X}_{t'})
        \label{eq:simplification_2b}
    \end{talign}
    If we plug equations \eqref{eq:simplification_2a}-\eqref{eq:simplification_2b} into \eqref{eq:simplification_1a}-\eqref{eq:simplification_1b}, and then those into \eqref{eq:memoryless_solution}, we obtain that
    \begin{talign}
    \begin{split} \label{eq:memoryless_solution_2}
        \vec{X}_t &= \vec{X}_0 \exp \big( - \int_{0}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \frac{\alpha_{1-t}}{\alpha_{1}} + \alpha_{1-t} \int_0^t \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) 
        %\frac{\alpha_{1-t}}{\alpha_{1-t'}} 
        \frac{\chi(1-t')}{2 \beta_{1-t'}^2} \xi_{1-t'}(\vec{X}_{t'}) \, \mathrm{d}t' \\ &\qquad + \int_0^t \big( 2 \beta_{1-t'} \big( \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \beta_{1-t'} - \dot{\beta}_{1-t'} \big) + \chi(1-t') \big) \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \frac{\alpha_{1-t}}{\alpha_{1-t'}} \, \mathrm{d}B_{t'}.
    \end{split}
    \end{talign}
    and if we take the limit $t \to 1^-$ and use that $\alpha_1 = 1$,
    \begin{talign}
    \begin{split} \label{eq:memoryless_solution_3}
        \vec{X}_1 &= \vec{X}_0 \big( \lim_{t \to 1^-} \exp \big( - \int_{0}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \alpha_{1-t} \big) + \lim_{t \to 1^-} \alpha_{1-t} \int_0^t \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) 
        %\frac{\alpha_{1-t}}{\alpha_{1-t'}} 
        \frac{\chi(1-t')}{2 \beta_{1-t'}^2} \xi_{1-t'}(\vec{X}_{t'}) \, \mathrm{d}t' \\ &\qquad + \lim_{t \to 1^-} \int_0^t \big( 2 \beta_{1-t'} \big( \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \beta_{1-t'} - \dot{\beta}_{1-t'} \big) + \chi(1-t') \big) \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \frac{\alpha_{1-t}}{\alpha_{1-t'}} \, \mathrm{d}B_{t'}.
    \end{split}
    \end{talign}
    The assumption on $\chi$ in \eqref{eq:chi_condition} is equivalent, up to a rearrangement of the notation and a flip in the time variable, to the statement that for all $t' \in [0,1)$,
    \begin{talign} \label{eq:coefficient_1}
        \lim_{t \to 1^-} \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \alpha_{1-t} = 0.
    \end{talign}
    Hence, under assumption \eqref{eq:chi_condition}, the factor accompanying $\vec{X}_0$ in equation \eqref{eq:memoryless_solution_3} is zero. Moreover, this assumption also implies that
    \begin{talign}
    \begin{split} \label{eq:coefficient_2}
        &\lim_{t \to 1^-} \alpha_{1-t} \int_0^t \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) 
        %\frac{\alpha_{1-t}}{\alpha_{1-t'}} 
        \frac{\chi(1-t')}{2 \beta_{1-t'}^2} \xi_{1-t'}(\vec{X}_{t'}) \, \mathrm{d}t' \\ &= \int_0^1 \big( \lim_{t \to 1^-} \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \alpha_{1-t} \big) 
        %\frac{1}{\alpha_{1-t'}} 
        \frac{\chi(1-t')}{2 \beta_{1-t'}^2} \xi_{1-t'}(\vec{X}_{t'}) \, \mathrm{d}t' = 0.
    \end{split}
    \end{talign}
    If we plug \eqref{eq:coefficient_1} and \eqref{eq:coefficient_2} into \eqref{eq:memoryless_solution_3}, we obtain that
    \begin{talign}
        \vec{X}_1 = \lim_{t \to 1^-} \int_0^t \big( 2 \beta_{1-t'} \big( \frac{\dot{\alpha}_{1-t'}}{\alpha_{1-t'}} \beta_{1-t'} - \dot{\beta}_{1-t'} \big) + \chi(1-t') \big) \exp \big( - \int_{t'}^t \frac{\chi(1-s)}{2 \beta_{1-s}^2} \, \mathrm{d}s \big) \frac{\alpha_{1-t}}{\alpha_{1-t'}} \, \mathrm{d}B_{t'},
    \end{talign}
    which shows that $\vec{X}_1$ is independent of $\vec{X}_0$. Next, we leverage that $\vec{\bm{X}}$ and $\bm{X}$ have equal distributions over trajectories (\Cref{lem:equal_process_distributions}). In particular, the joint distribution of $(\vec{X}_0,\vec{X}_1)$ is equal to the joint distribution of $(X_1,X_0)$. We conclude that $X_1$ and $X_0$ are independent, which is the definition of the memorylessness property.
    Hence, the assumption \eqref{eq:chi_condition} is sufficient for memorylessness to hold.
    
    It remains to prove that the assumption \eqref{eq:chi_condition} is necessary. Looking at equation \eqref{eq:memoryless_solution_2} we deduce that generally, for any $t \in [0,1)$, $\vec{X}_0$ and $\vec{X}_t$ are not independent, because the first two terms in \eqref{eq:memoryless_solution_2} are different from zero. Thus, if there existed a $t' \in [0,1)$ such that the limit \eqref{eq:coefficient_1} is different from zero, then $\vec{X}_1$ would not be independent from $\vec{X}_{t'}$, which means that in general it would not be independent of $\vec{X}_0$ either.
    
    % \subsubsection{$\sigma(t) = \sqrt{2 \eta_t}$ is memoryless} 
    
    % Let $\vec{X}$ and $X$ be the solution of the forward-backward SDEs \eqref{eq:forward_generic}-\eqref{eq:backward_generic}.
    % A direct application of \Cref{lem:OU_process} yields that $\vec{X}_t$ is of the form
    % \begin{talign} \label{eq:vec_X_t}
    %     \vec{X}_1 = \vec{X}_0 \exp \big( - \int_0^1 \kappa_{1-s} \, \mathrm{d}s \big) - \int_0^1 \sqrt{2 \eta_{1-t'}} \exp \big( - \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}B_{t'}.
    % \end{talign}
    % and it has the same distribution as
    % \begin{talign} \label{eq:hat_X_t}
    %     \hat{X}_1 = \vec{X}_0 \exp \big( - \int_0^1 \kappa_{1-s} \, \mathrm{d}s \big) + \sqrt{2 \int_0^1 \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_{1-s} \, \mathrm{d}s \big) \, \mathrm{d}t'} \epsilon, \qquad \epsilon \sim N(0,1).
    % \end{talign}
    % Hence, the couplings $(\vec{X}_0,\vec{X}_1)$ and $(\vec{X}_0,\hat{X}_1)$ have the same distribution. 
    % By the conditions \eqref{eq:kappa_eta_conditions}, which are fulfilled by DDIM and Flow Matching, equation \eqref{eq:hat_X_t} implies that $\hat{X}_1 = \epsilon \sim N(0,1)$, and that it is independent of $\vec{X}_0$. Thus, $\vec{X}_0$ and $\vec{X}_1$ are also independent. Next, we leverage that $\vec{\bm{X}}$ and $\bm{X}$ have equal distributions over trajectories (\Cref{lem:equal_process_distributions}). In particular, the joint distribution of $(\vec{X}_0,\vec{X}_1)$ is equal to the joint distribution of $(X_1,X_0)$. We conclude that $X_1$ and $X_0$ are independent, which is the definition of the memorylessness property.
    
    % In particular, since $\vec{X}_1 \sim N(0,1)$ by construction, $\hat{X}_1$ is also distributed according to $N(0,1)$. But by \eqref{eq:hat_X_t}, $\hat{X}_1$ is a linear combination of two random variables $\vec{X}_0$ and $\epsilon$, which are linearly independent because $\vec{X}_0$ is distributed according to the data distribution, which is arbitrary. Hence, by linear independence, we have that
    % \begin{talign}
    %     \exp \big( - \int_0^t \kappa_s \, \mathrm{d}s \big) = 0, \qquad \sqrt{2 \int_0^t \eta_{t'} \exp \big( - 2 \int_{t'}^{t} \kappa_s \, \mathrm{d}s \big) \, \mathrm{d}t'} = 1.
    % \end{talign}
    % In other words, $\vec{X}_0$ and $\hat{X}_1$ are independent, and since the couplings $(\vec{X}_0,\vec{X}_1)$ and $(\vec{X}_0,\hat{X}_1)$ are equally distributed, $\vec{X}_0$ and $\vec{X}_1$ are also independent.

    % To prove that $\cev{X}_0$ and $\cev{X}_1$ are independent, it is sufficient to show that the couplings $(\vec{X}_0,\vec{X}_1)$ and $(\cev{X}_1,\cev{X}_0)$ are equally distributed. This follows from the more general \Cref{lem:equal_process_distributions}.
    % \end{proof}

    % \begin{lemma} \label{lem:equal_process_distributions}
    % For any sequence of times $(t_i)_{0 \leq i \leq I}$, the joint distribution of $(\vec{X}_{t_i})_{0 \leq i \leq I}$ is equal to the joint distribution of $(\cev{X}_{1-t_i})_{0 \leq i \leq I}$, or equivalently, that the distributions $\vec{\mathbb{P}}$, $\cev{\mathbb{P}}$ of the forward and backward processes $\vec{X}$, $\cev{X}$ are equal, up to a flip in the time direction.
    % \end{lemma}

%     \subsubsection{$\sigma(t) = \sqrt{2 \eta_t}$ is the only memoryless schedule}

%     To show that $\sigma(t) = \sqrt{2 \eta_t}$ is the only memoryless schedule, we will show that for a certain family of
%     %reward $r$ and 
%     data distributions $p^{\mathrm{data}}$, all other schedules are not memoryless. 
%     %We consider the reward $r(x) = - \langle \gamma, x\rangle$, where $\gamma \in \mathbb{R}^d$ is different from zero, and $p^{\mathrm{data}}(x) = N(0,I)$. 
%     We set $p^{\mathrm{data}}(x) = N(0,\gamma I)$, where $\gamma > 0$ is arbitrary.
%     By the properties of flow matching, the base process $p^{\mathrm{data}}$ has the same marginals as the interpolation variable $\bar{X}_t = \beta_t \bar{X}_0 + \alpha_t \bar{X}_1$, where $\bar{X}_0  \sim N(0,I), \bar{X}_1 \sim N(0,\gamma I)$ are independent. Equivalently, $p^{\mathrm{data}}_t = N(0,(\alpha_t^2 \gamma + \beta_t^2)\mathrm{I})$. Hence, the score reads $s(x,t) = - \frac{x}{\alpha_t^2 \gamma + \beta_t^2}$, and the 
%     % generative process \eqref{eq:gen_process_1}-\eqref{eq:gen_process_2} takes the form:
%     forward-backward SDEs \eqref{eq:forward_arbitrary}-\eqref{eq:backward_arbitrary} with arbitrary noise take the form:
%     \begin{talign}
%         \mathrm{d}X_t &= \big(\kappa_t - \frac{\sigma(t)^2 + 2\eta_t}{2(\alpha_t^2 \gamma + \beta_t^2)} \big) X_t \, \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad X_0 \sim \mathcal{N}(0,I).
%     \end{talign}
%     Applying \Cref{lem:OU_process}, we obtain that the solution of this SDE is 
%     \begin{talign}
%         X_t = X_0 \exp \big(  \int_0^t \big(\kappa_{s} - \frac{\sigma(s)^2 + 2\eta_s}{2(\alpha_s^2 + \beta_s^2 \gamma)} \big) \, \mathrm{d}s \big) + \int_0^t \sigma(t') \exp \big( \int_{t'}^{t} \big(\kappa_{s} - \frac{\sigma(s)^2 + 2\eta_s}{2(\alpha_s^2 + \beta_s^2 \gamma)} \big) \, \mathrm{d}s \big) \, \mathrm{d}B_{t'},
%     \end{talign}
%     which has the same distribution as the random variable
%     \begin{talign} \label{eq:hat_X_t_OU_process}
%         X_t = X_0 \exp \big(  \int_0^t \big(\kappa_{s} - \frac{\sigma(s)^2 + 2\eta_s}{2(\alpha_s^2 \gamma + \beta_s^2)} \big) \, \mathrm{d}s \big) + \sqrt{\int_0^t \sigma(t') \exp \big( \int_{t'}^{t} \big(\kappa_{s} - \frac{\sigma(s)^2 + 2\eta_s}{2(\alpha_s^2 \gamma + \beta_s^2)} \big) \, \mathrm{d}t'} \epsilon, \qquad \epsilon \sim N(0,I).
%     \end{talign}
%     Hence,
%     \begin{talign}
%     \begin{split}
%         \mathrm{Cov}(X_0,X_1) &= \mathbb{E}[X_0 X_1] - \mathbb{E}[X_0] \mathbb{E}[X_1] = \mathbb{E}[X_0 X_1] = \exp \big(  \int_0^1 \big(\kappa_{s} - \frac{\sigma(s)^2 + 2\eta_s}{2(\alpha_s^2 \gamma + \beta_s^2)} \big) \, \mathrm{d}s \big) \mathbb{E}[X_0^2] \\ &= \exp \big(  \int_0^1 \big(\kappa_{s} - \frac{\sigma(s)^2 + 2\eta_s}{2(\alpha_s^2 \gamma + \beta_s^2)} \big) \, \mathrm{d}s \big).
%     \end{split}
%     \end{talign}

% \begin{lemma}
%     Let $p^{\mathrm{data}}$ be of the form $p^{\mathrm{data}}(x) = (\tilde{p}^{\mathrm{data}} * N(0,\epsilon \mathrm{I}))(x)$, where the distribution $\tilde{p}^{\mathrm{data}}$ is arbitrary, $\epsilon > 0$, and $*$ denotes the convolution operator. Then, the score reads:
%     \begin{talign}
%         s(x,t) = %-\frac{\int_{\mathbb{R}^d} \frac{x-y}{\epsilon} \frac{\exp\big( - \frac{\|x-y\|^2}{2\epsilon}\big)}{(2\pi\epsilon)^{d/2}} \tilde{p}^{\mathrm{data}}(y) \, \mathrm{d}y}{\int_{\mathbb{R}^d} \frac{\exp\big( - \frac{\|x-y\|^2}{2\epsilon}\big)}{(2\pi\epsilon)^{d/2}} \tilde{p}^{\mathrm{data}}(y) \, \mathrm{d}y}
%         - \frac{x}{\epsilon} + \frac{1}{\epsilon} \frac{\int_{\mathbb{R}^d} y \exp\big( - \frac{\|x-y\|^2}{2(\epsilon + \int_0^t \sigma(s)^2 \, \mathrm{d}s)}\big) \tilde{p}^{\mathrm{data}}(y) \, \mathrm{d}y}{\int_{\mathbb{R}^d} \exp\big( - \frac{\|x-y\|^2}{2(\epsilon + \int_0^t \sigma(s)^2 \, \mathrm{d}s)}\big) \tilde{p}^{\mathrm{data}}(y) \, \mathrm{d}y}
%     \end{talign}
% \end{lemma}
% \begin{proof}
%     % We write the density $p^{\mathrm{data}}$ explicitly:
%     % \begin{talign}
%     %     p^{\mathrm{data}}(x) = \int_{\mathbb{R}^d} \frac{\exp\big( - \frac{\|x-y\|^2}{2\epsilon}\big)}{(2\pi\epsilon)^{d/2}} \tilde{p}^{\mathrm{data}}(y) \, \mathrm{d}y.
%     % \end{talign}
%     We have that $s(x,t) = \nabla \log $
%     Taking the gradient of the logarithm of this expression yields the desired result.
% \end{proof}

\subsection{Proof of \Cref{thm:general_fine-tuning}: fine-tuning recipe for general noise schedules} \label{subsec:proof_prop_diff_finetuning}

The proof of this result relies heavily on the properties of the Hamilton-Jacobi-Bellman equation:
\begin{theorem}[Hamilton-Jacobi-Bellman equation] \label{thm:HJB}
    If we define the infinitesimal generator
    \begin{talign}
    \mathcal{L} := \frac{1}{2} \sum_{i,j=1}^{d} (\sigma \sigma^{\top})_{ij} (t) \partial_{x_i} \partial_{x_j} + \sum_{i=1}^{d} b_i(x,t) \partial_{x_i},
    \end{talign}
    the value function $V$ for the SOC
    % stochastic optimal control 
    problem \eqref{eq:control_problem_def}-\eqref{eq:controlled_SDE} solves the following Hamilton-Jacobi-Bellman (HJB) partial differential equation:
    \begin{talign}
    \begin{split} \label{eq:HJB_setup}
        % &(\partial_t + L) V(x,t) - \frac{1}{2} \| (\sigma^{\top} \nabla V) (x,t) \|^2 + f(x,t) = 0, \\
        &\partial_t V(x,t) = - \mathcal{L} V(x,t) + \frac{1}{2} \| (\sigma^{\top} \nabla V) (x,t) \|^2 - f(x,t), \\
        &V(x,T) = g(x).
    \end{split}
    \end{talign}
\end{theorem}

Consider forward SDEs like \eqref{eq:forward_arbitrary}, starting from the distributions $p^{\mathrm{base}}$ and $p^*$, where $p^*(x) \propto p^{\mathrm{base}}(x) \exp(r(x))$.
\begin{talign}
\label{eq:forward_base}
    \mathrm{d}\vec{X}_t = \vec{b}(\vec{X}_t,t) \, \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad \vec{X}_0 \sim p^{\mathrm{base}}, \\
    \mathrm{d}\vec{X}^*_t = \vec{b}^*(\vec{X}^*_t,t) \, \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad \vec{X}_0 \sim p^*. \label{eq:forward_star}
\end{talign}
where the drifts are defined as
%By hypothesis we have that
\begin{talign}
\begin{split} \label{eq:b_b_star}
    \vec{b}(x,t) &= - \kappa_{1-t} x +
     \big( \frac{\sigma(1-t)^2}{2} - \eta_{1-t} \big) \mathfrak{s}(x,1-t) = - \kappa_{1-t} x +
    \big( \frac{\sigma(1-t)^2}{2} - \eta_{1-t} \big) \nabla \log \vec{p}_t(x), \\
    \vec{b}^*(x,t) &= - \kappa_{1-t} x +
    \big( \frac{\sigma(1-t)^2}{2} - \eta_{1-t} \big)
    \mathfrak{s}^*(x,1-t) = - \kappa_{1-t} x +
    \big( \frac{\sigma(1-t)^2}{2} - \eta_{1-t} \big)
    \nabla \log \vec{p}_t^*(x),
\end{split}
\end{talign}
and $\vec{p}_t$, $\vec{p}_t^*$ are the densities of $X_t$, $\vec{X}_t$, respectively. $\vec{p}_t$, $\vec{p}_t^*$ satisfy Fokker-Planck equations:
\begin{talign}
\begin{split} \label{eq:fokker_planck_1}
    \partial_t \vec{p}_t = \nabla \cdot (\vec{b}(x,t) \vec{p}_t) + \nabla \cdot ( %\frac{\sigma(t) \sigma(t)^{\top}}{2} 
    \frac{\sigma(1-t)^2}{2} \nabla \vec{p}_t), \qquad \vec{p}_0 = p^{\mathrm{base}}, \\
    \partial_t \vec{p}^*_t = \nabla \cdot (\vec{b}^*(x,t) \vec{p}^*_t) + \nabla \cdot ( %\frac{\sigma(t) \sigma(t)^{\top}}{2}
    \frac{\sigma(1-t)^2}{2}
    \nabla \vec{p}^*_t), \qquad \vec{p}_0 = p^*.
\end{split}
\end{talign}
Plugging \eqref{eq:b_b_star} into \eqref{eq:fokker_planck_1}, we obtain 
\begin{talign}
\begin{split}
    \partial_t \vec{p}_t = \nabla \cdot (\kappa_{1-t} x \vec{p}_t) + \nabla \cdot \big( 
    % \big( \frac{\sigma(t) \sigma(t)^{\top}}{2} + \eta_t \mathrm{I} \big) 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big)
    \eta_{1-t}
    \nabla \vec{p}_t \big), \qquad \vec{p}_0 = p^{\mathrm{base}}, \\
    \partial_t \vec{p}^*_t = \nabla \cdot (\kappa_{1-t} x \vec{p}^*_t) + \nabla \cdot \big( 
    % \big( \frac{\sigma(t) \sigma(t)^{\top}}{2} + \eta_t \mathrm{I} \big) 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big)
    \eta_{1-t}
    \nabla \vec{p}^*_t \big), \qquad \vec{p}_0 = p^*.
\end{split}
\end{talign}
We apply the Hopf-Cole transformation to obtain PDEs for $-\log \vec{p}_t$ (and $-\log \vec{p}^*_t$ analogously):
\begin{talign}
\begin{split}
    - \partial_t (- \log \vec{p}_t) &= \frac{\partial_t p_t}{p_t} = \frac{\nabla \cdot (\kappa_{1-t} x \vec{p}_t) + \nabla \cdot \big( 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big)
    \eta_{1-t}
    \nabla \vec{p}_t \big)}{p_t} \\ &= \kappa_{1-t} \nabla \cdot x + \kappa_{1-t} \langle x, \nabla \log \vec{p}_t \rangle +
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big)
    \eta_{1-t}
    \frac{\nabla \cdot (\nabla \log \vec{p}_t \exp(\log p_t))}{p_t} \\ &= \kappa_{1-t} d + \kappa_{1-t} \langle x, \nabla \log \vec{p}_t \rangle + 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big) 
    \eta_{1-t}
    \big( \Delta \log \vec{p}_t + \|\nabla \log \vec{p}_t \|^2 \big).
\end{split}
\end{talign}
Hence, if we define $\mathscr{V}(x,t) = - \log \vec{p}_t(x)$, $\mathscr{V}^*(x,t) = - \log \vec{p}^*_t(x)$, then $\mathscr{V}$ and $\mathscr{V}^*$ satisfy the following Hamilton-Jacobi-Bellman equations:
\begin{talign} \label{eq:V_HJB}
    -\partial_t \mathscr{V} &= \kappa_{1-t} d - \kappa_{1-t} \langle x, \nabla \mathscr{V} \rangle + 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big) 
    \eta_{1-t}
    \big( - \Delta \mathscr{V} + \|\nabla \mathscr{V} \|^2 \big), \qquad \mathscr{V}(x,0) = - \log p^{\mathrm{base}}(x), \\
    -\partial_t \mathscr{V}^* &= \kappa_{1-t} d - \kappa_{1-t} \langle x, \nabla \mathscr{V}^* \rangle + 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big)
    \eta_{1-t}
    \big( - \Delta \mathscr{V}^* + \|\nabla \mathscr{V}^* \|^2 \big), \qquad \mathscr{V}^*(x,0) = - \log p^*(x). \label{eq:V_star_HJB}
\end{talign}
% We can also apply the Hopf-Cole transformation directly on the original Fokker-Planck equations \eqref{eq:fokker_planck_1}, which yields:
% \begin{talign}
%     -\partial_t \mathscr{V} &= \nabla \cdot b(x,t) - \langle b(x,t), \nabla \mathscr{V} \rangle + \frac{\tilde{\sigma}(t)^2}{2} \big( - \Delta \mathscr{V} + \|\nabla \mathscr{V} \|^2 \big), \qquad \mathscr{V}(x,0) = - \log p_1(x), \\
%     -\partial_t \mathscr{V}^* &= \nabla \cdot b(x,t) - \langle b(x,t), \nabla \mathscr{V}^* \rangle + \frac{\tilde{\sigma}(t)^2}{2} \big( - \Delta \mathscr{V}^* + \|\nabla \mathscr{V}^* \|^2 \big), \qquad \mathscr{V}(x,0) = - \log p^*(x).
% \end{talign}
Now, define $\hat{\mathscr{V}}(x,t) = \mathscr{V}^*(x,t) - \mathscr{V}(x,t)$. Subtracting \eqref{eq:V_star_HJB} from \eqref{eq:V_HJB}, we obtain
\begin{talign}
\begin{split}
    -\partial_t \hat{\mathscr{V}} &= - \kappa_{1-t} \langle x, \nabla \hat{\mathscr{V}} \rangle + 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big)
    \eta_{1-t}
    \big( - \Delta \hat{\mathscr{V}} + \|\nabla \mathscr{V}^* \|^2 - \|\nabla \mathscr{V} \|^2 \big)
    \\ &= - \kappa_{1-t} \langle x, \nabla \hat{\mathscr{V}} \rangle + 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big)
    \eta_{1-t}
    \big( - \Delta \hat{\mathscr{V}} + \|\nabla (\hat{\mathscr{V}} + \mathscr{V}) \|^2 - \|\nabla \mathscr{V} \|^2 \big)
    \\ &= - \kappa_{1-t} \langle x, \nabla \hat{\mathscr{V}} \rangle + 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big) 
    \eta_{1-t}
    \big( - \Delta \hat{\mathscr{V}} + \|\nabla \hat{\mathscr{V}}\|^2 + 2\langle \nabla \mathscr{V}, \nabla \hat{\mathscr{V}} \rangle \big) \\ &= \langle - \kappa_{1-t} x + 
    % \big( \tilde{\sigma}(t)^2 + 2 \eta_t \big)
    2 \eta_{1-t}
    \nabla \mathscr{V}, \nabla \hat{\mathscr{V}} \rangle + 
    % \big( \frac{\tilde{\sigma}(t)^2}{2} + \eta_t \big) 
    \eta_{1-t}
    \big( - \Delta \hat{\mathscr{V}} + \|\nabla \hat{\mathscr{V}}\|^2 \big)
    \\ &= \langle - \kappa_{1-t} x - 
    2 \eta_{1-t}
    \mathfrak{s}(x,1-t), \nabla \hat{\mathscr{V}} \rangle + 
    \eta_{1-t}
    \big( - \Delta \hat{\mathscr{V}} + \|\nabla \hat{\mathscr{V}}\|^2 \big),
    \\
    \hat{\mathscr{V}}(x,0) &= - \log p^*(x) + \log p^{\mathrm{base}}(x) = - r(x) + \log \big( \int p^{\mathrm{base}}(y) \exp(r(y)) \, \mathrm{d}y \big). 
\end{split}
\end{talign}
Hence, $\hat{\mathscr{V}}$ also satisfies a Hamilton-Jacobi-Bellman equation. If we define $V$ such that $\hat{\mathscr{V}}(x,t) = V(x,1-t)$, we have that
\begin{talign}
    \partial_t V = \langle - \kappa_{t} x - 
    2 \eta_{t}
    \mathfrak{s}(x,t), \nabla V \rangle + 
    \eta_{t}
    \big( - \Delta V + \|\nabla V\|^2 \big), \qquad 
    V(x,1) = r(x) - \log \big( \int p^{\mathrm{base}}(y) \exp(r(y)) \, \mathrm{d}y \big). 
\end{talign}
Using \Cref{thm:HJB}, we can reverse-engineer 
% $\hat{\mathscr{V}}$ as $\hat{\mathscr{V}}(x,t) = V(x,1-t)$, where 
$V$ as the value function of the following SOC
%stochastic optimal control 
problem:
\begin{talign} \label{eq:control_problem_def_diff}
    &\min\limits_{u \in \mathcal{U}} \mathbb{E} \big[ \frac{1}{2} \int_0^1 \|u(X^u_t,t)\|^2 \, \mathrm{d}t \! - \! r(x) \! + \! \log \big( \int p^{\mathrm{base}}(y) \exp(r(y)) \, \mathrm{d}y \big) \big], \\
    \begin{split}
    \text{s.t.}~ \mathrm{d}X^u_t \! = \! \big(%\kappa_t X^u_t - \big( \tilde{\sigma}(t)^2 + 2 \eta_t \big) \nabla \mathscr{V}, \nabla \hat{\mathscr{V}} \rangle 
    % \hat{b}(X^u_t,1-t) 
    \kappa_t x + 2 \eta_t \mathfrak{s}(x,t)
    \! + \! %\hat{\sigma}(1-t) 
    \sqrt{2 \eta_{t}} 
    u(X^u_t,t) \big) \, \mathrm{d}t \! + \! 
    % \hat{\sigma}(1-t) 
    \sqrt{2 \eta_{t}}
    \mathrm{d}B_t, \qquad X^u_0 \sim p_0.
    \end{split} 
    \label{eq:controlled_SDE_diff}
\end{talign}
Note that this SOC problem is equal to the problem \eqref{eq:control_problem_def}-\eqref{eq:controlled_SDE} with the choices $f=0$, $g=-r$, and $\sigma(t) = \sqrt{2 \eta_t}$.
% where %$\frac{\hat{\sigma}(t)^2}{2} = \frac{\tilde{\sigma}(t)^2}{2} + \eta_t$
% \begin{talign} 
% % \label{eq:hat_sigma_hat_b_app}
% % \hat{\sigma}(t) = \sqrt{\tilde{\sigma}(t)^2 + 2\eta_t}, \qquad\quad 
% \hat{b}(x,t) = \kappa_t x - %\big( \tilde{\sigma}(t)^2 + 2 \eta_t \big) 
% 2 \eta_t
% \nabla \mathscr{V}(x,t) = \kappa_t x + 
% % \big( \tilde{\sigma}(t)^2 + 2 \eta_t \big) 
% 2 \eta_t
% \mathfrak{s}(x,t).
% \end{talign}
By equation \eqref{eq:optimal_control}, %and recalling that $\hat{\sigma}$ is scalar-valued, 
the optimal control of the problem \eqref{eq:control_problem_def_diff}-\eqref{eq:controlled_SDE_diff} is of the form:
\begin{talign} 
\begin{split} \label{eq:optimal_control_HJB}
u^*(x,t) &= - %\hat{\sigma}(1-t) 
\sqrt{2 \eta_{t}}
\nabla V(x,t) = - %\hat{\sigma}(1-t) 
\sqrt{2 \eta_{t}}
\nabla \hat{\mathscr{V}}(x, 1-t) = - 
% \hat{\sigma}(1-t)
\sqrt{2 \eta_{t}}
\big( \nabla \mathscr{V}^*(x, 1-t) - \nabla \mathscr{V}(x, 1-t) \big) \\ &= - \sqrt{2 \eta_{t}} \big( - \nabla \log \vec{p}^*_{1-t}(x) + \nabla \log \vec{p}_{1-t}(x) \big) = %\hat{\sigma}(1-t) 
\sqrt{2 \eta_{t}}
\big( \mathfrak{s}^*(x,t) - \mathfrak{s}(x,t) \big), 
\end{split} \\
&\iff \mathfrak{s}^*(x,t) = \mathfrak{s}(x,t) + u^*(x,t) / %\hat{\sigma}(1-t).
\sqrt{2 \eta_{t}}.
\label{eq:s_star_u_star}
\end{talign}
As in \eqref{eq:backward_arbitrary}, the backward SDEs corresponding to the forward SDEs \eqref{eq:forward_star} take the following form:
\begin{talign}
    % \mathrm{d}X_t &= \big( \kappa_t X_t + \big( \frac{\sigma(t)^2}{2} + \eta_t \big) \mathfrak{s}(X_t,t) \big) \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad X_0 \sim N(0,I), \\
    \mathrm{d}X^*_t &= \big( \kappa_t X^*_t + \big( \frac{\sigma(t)^2}{2} + \eta_t \big) \mathfrak{s}^*(X^*_t,t) \big) \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad X^*_0 \sim N(0,I).
\end{talign}
If we plug \eqref{eq:s_star_u_star} into this equation, we obtain
\begin{talign}
    \mathrm{d}X^*_t &= \big( \kappa_t X^*_t + \big( \frac{\sigma(t)^2}{2} + \eta_t \big) \big( \mathfrak{s}(X^*_t,t) + \frac{u^*(X^*_t,t)}{\sqrt{2 \eta_{t}}} \big) \big) \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad X^*_0 \sim N(0,I), \\
    \iff \mathrm{d}X^*_t &= \big( b(X^*_t,t) + \frac{\frac{\sigma(t)^2}{2} + \eta_t}{\sqrt{2 \eta_{t}}} u^*(X^*_t,t) \big) \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad X^*_0 \sim N(0,I).
\end{talign}
where we used that $b(x,t) = \kappa_t x + \big( \frac{\sigma(t)^2}{2} + \eta_t \big) \mathfrak{s}(x,t)$ by definition in equation \eqref{eq:gen_process_2}. 

% \subsection{Fine-tuned inference SDEs for DDIM and Flow Matching} \label{subsec:finetuned_inference_SDE}
\paragraph{The fine-tuned inference SDE for DDIM}
Now, for DDIM, we have that $u^*(x,t) = 
- \sqrt{\frac{\dot{\alpha}_t}{\alpha_t(1-\alpha_t)}} (\epsilon^*(x,t) - \epsilon^{\mathrm{base}}(x,t))$ by \eqref{eq:conversion_DDPM}. Hence,
\begin{talign}
    &\frac{\frac{\sigma(t)^2}{2} + \eta_t}{\sqrt{2 \eta_{t}}} u^*(x,t) = - \frac{\frac{\sigma(t)^2}{2} + \frac{\dot{\alpha}_t}{2\alpha_t}}{\sqrt{\frac{\dot{\alpha}_t}{\alpha_t}}} \sqrt{\frac{\dot{\alpha}_t}{\alpha_t(1-\alpha_t)}} (\epsilon^*(x,t) - \epsilon^{\mathrm{base}}(x,t)) = - \frac{\frac{\sigma(t)^2}{2} + \frac{\dot{\alpha}_t}{2\alpha_t}}{\sqrt{1-\alpha_t}} (\epsilon^*(x,t) - \epsilon^{\mathrm{base}}(x,t)), \\
    \begin{split}
    &\implies b(x,t) + \frac{\frac{\sigma(t)^2}{2} + \eta_t}{\sqrt{2 \eta_{t}}} u^*(x,t) = 
    % \frac{\dot{\alpha}_{t}}{2\alpha_{t}} x - \frac{\dot{\alpha}_{t}}{\alpha_{t}} \frac{\epsilon^{\mathrm{base}}(x,t)}{\sqrt{1-\alpha_{t}}} 
    \frac{\dot{\alpha}_{t}}{2\alpha_{t}} X_t - \big( \frac{\dot{\alpha}_{t}}{2\alpha_{t}} + \frac{\sigma(t)^2}{2} \big) \frac{\epsilon^{\mathrm{base}}(X_{t},t)}{\sqrt{1-\alpha_{t}}}
    - \frac{\frac{\sigma(t)^2}{2} + \frac{\dot{\alpha}_t}{2\alpha_t}}{\sqrt{1-\alpha_t}} (\epsilon^*(x,t) - \epsilon^{\mathrm{base}}(x,t)) \\ &\qquad\qquad\qquad\qquad\qquad = 
    \frac{\dot{\alpha}_{t}}{2\alpha_{t}} X_t - \big( \frac{\dot{\alpha}_{t}}{2\alpha_{t}} + \frac{\sigma(t)^2}{2} \big) \frac{\epsilon^{*}(X_{t},t)}{\sqrt{1-\alpha_{t}}}.
    % \frac{\dot{\alpha}_{t}}{2\alpha_{t}} X_t + \frac{\frac{\sigma(t)^2}{2} - \frac{\dot{\alpha}_t}{2\alpha_t}}{\sqrt{1-\alpha_t}} \epsilon^{\mathrm{base}}(x,t) - \frac{\frac{\sigma(t)^2}{2} + \frac{\dot{\alpha}_t}{2\alpha_t}}{\sqrt{1-\alpha_t}} \epsilon^*(x,t)
    \end{split}
\end{talign}
We obtain that the fine-tuned inference SDE for DDIM is 
\begin{talign}
    \mathrm{d}X^*_t &= \big( \frac{\dot{\alpha}_{t}}{2\alpha_{t}} X^*_t - \big( \frac{\dot{\alpha}_{t}}{2\alpha_{t}} + \frac{\sigma(t)^2}{2} \big) \frac{\epsilon^{*}(X^*_{t},t)}{\sqrt{1-\alpha_{t}}} \big) \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad X^*_0 \sim N(0,I),
\end{talign}
which is matches the SDE \eqref{eq:euler_maruyama_DDIM} with the choice $\epsilon = \epsilon^*$.

\paragraph{The fine-tuned inference SDE for Flow Matching}
For Flow Matching, we have that $u^*(x,t) = 
\sqrt{\frac{2}{\beta_{t}(\frac{\dot{\alpha}_{t}}{\alpha_{t}} \beta_{t} - \dot{\beta}_{t})}} (v^*(x,t) - v^{\mathrm{base}}(x,t))$ by \eqref{eq:conversion_MFM}. Hence,
\begin{talign}
    \begin{split}
    &\frac{\frac{\sigma(t)^2}{2} + \eta_t}{\sqrt{2 \eta_{t}}} u^*(x,t) = \frac{\frac{\sigma(t)^2}{2} + \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)}{\sqrt{2 \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)}} \sqrt{\frac{2}{\beta_{t}(\frac{\dot{\alpha}_{t}}{\alpha_{t}} \beta_{t} - \dot{\beta}_{t})}} (v^*(x,t) - v^{\mathrm{base}}(x,t)) \\ &\qquad\qquad\qquad \ = \big( 1 + \frac{\sigma(t)^2}{2 \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)} \big) (v^*(x,t) - v^{\mathrm{base}}(x,t)). 
    \end{split}
    \\
    \begin{split}
    &\implies b(x,t) + \frac{\frac{\sigma(t)^2}{2} + \eta_t}{\sqrt{2 \eta_{t}}} u^*(x,t) =  
    % - \frac{\dot{\alpha}_{t}}{\alpha_{t}} x + 2v^{\mathrm{base}}(x,t) 
    v^{\mathrm{base}}(x,t) + \frac{\sigma(t)^2}{2\beta_{t}(\frac{\dot{\alpha}_{t}}{\alpha_{t}} \beta_{t} -\dot{\beta}_{t})} \big( v^{\mathrm{base}}(x,t) - \frac{\dot{\alpha}_{t}}{\alpha_{t}} x \big) 
    \\ &\qquad\qquad\qquad\qquad\qquad\qquad\quad + \big( 1 + \frac{\sigma(t)^2}{2 \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)} \big) (v^*(x,t) - v^{\mathrm{base}}(x,t)) \\ &\qquad\qquad\qquad\qquad\qquad\qquad\quad = 
    % - \frac{\dot{\alpha}_{t}}{\alpha_{t}} x + \big( 1 -  \frac{\sigma(t)^2}{2 \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)} \big) v^{\mathrm{base}}(x,t) + \big( 1 + \frac{\sigma(t)^2}{2 \beta_t(\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)} \big) v^*(x,t).
    v^{*}(x,t) + \frac{\sigma(t)^2}{2\beta_{t}(\frac{\dot{\alpha}_{t}}{\alpha_{t}} \beta_{t} -\dot{\beta}_{t})} \big( v^{*}(x,t) - \frac{\dot{\alpha}_{t}}{\alpha_{t}} x \big).
    \end{split}
\end{talign}
We obtain that the fine-tuned inference SDE for Flow Matching is 
\begin{talign}
    \mathrm{d}X^*_t = \big( v(X^*_t,t) + \frac{\sigma(t)^2}{2\beta_{t}(\frac{\dot{\alpha}_{t}}{\alpha_{t}} \beta_{t} -\dot{\beta}_{t})} \big( v^*(X^*_t,t) - \frac{\dot{\alpha}_{t}}{\alpha_{t}} X^*_t \big) \big) \, \mathrm{d}t + \sigma(t) \, \mathrm{d}B_t, \qquad X^*_0 \sim N(0,I),
\end{talign}
which matches equation \eqref{eq:FM_general_diffusion_coeff} with the choice $v = v^*$.

% Recall under the assumption that $b(x,t) = \kappa_t x + (\eta_t - \frac{\tilde{\sigma}(t)^2}{2}) \mathfrak{s}(x,t)$, the forward and backward SDEs for $p^*$ read
% \begin{talign}
%     \mathrm{d}\vec{X}^*_t &= -
%     %b^*(\vec{X}^*_t,t) 
%     \big(\kappa_t \vec{X}^*_t + %\eta_t 
%     (\eta_t - \frac{\tilde{\sigma}(t)^2}{2})
%     \mathfrak{s}^*(\vec{X}^*_t,t) \big) \, \mathrm{d}t + \tilde{\sigma}(t) \, \mathrm{d}B_t, \qquad \vec{X}_0 \sim p^*. \\
%     \mathrm{d}\cev{X}^*_t &= \big(\kappa_{1-t} \vec{X}^*_{t} + 
%     % (\eta_{1-t} + \tilde{\sigma}(1-t)^2) 
%     (\eta_{1-t} + \frac{\tilde{\sigma}(1-t)^2}{2})
%     \mathfrak{s}^*(\vec{X}^*_t,1-t) \big) \, \mathrm{d}t + \tilde{\sigma}(1-t) \, \mathrm{d}B_t, \qquad \cev{X}^*_0 \sim p_0, 
%     \label{eq:backward_assumption}
% \end{talign}
% Plugging \eqref{eq:s_star_u_star} into \eqref{eq:backward_assumption}, we obtain 
% \begin{talign}
%     \mathrm{d}\cev{X}^*_t &= \big(\kappa_{1-t} \vec{X}^*_{t} + (\eta_{1-t} + \frac{\tilde{\sigma}(1-t)^2}{2}) \big( \mathfrak{s}(\vec{X}^*_{t},1-t) + u^*(\vec{X}^*_{t},t) / 
%     % \hat{\sigma}(1-t) 
%     \sqrt{2 \eta_{1-t}}
%     \big) \big) \, \mathrm{d}t + \tilde{\sigma}(1-t) \, \mathrm{d}B_t, \qquad \cev{X}^*_0 \sim p_0,
% \end{talign}
% which, defining the backward drift $\cev{b}(x,t) = b(x,t) + \tilde{\sigma}(t)^2 \mathfrak{s}(x,t)$,
% % and using the definition of $\hat{\sigma}$ in \eqref{eq:hat_sigma_hat_b_app}, 
% we can rewrite as
% \begin{talign}
%     \mathrm{d}\cev{X}^*_t &= \big( \cev{b}(\cev{X}^*_t,1-t) + \frac{\eta_{1-t} + \frac{\tilde{\sigma}(1-t)^2}{2}}{\sqrt{2\eta_{1-t}}} u^*(\vec{X}^*_{t},t) \big) \, \mathrm{d}t + \tilde{\sigma}(1-t) \, \mathrm{d}B_t, \qquad \cev{X}^*_0 \sim p_0,
% \end{talign}

\iffalse
\subsection{Proof of equations \eqref{eq:hat_sigma_hat_b_DDIM} and \eqref{eq:hat_sigma_hat_b_FM}} \label{eq:proof_eqs}
For DDIM (which includes DDPM as the case in which $\tilde{\sigma} \equiv 0$), we have that $\kappa_t = - \frac{\dot{\alpha}_t}{2\alpha_t}$ and $\eta_t = - \frac{1}{2}(\tilde{\sigma}(t)^2 + \frac{\dot{\alpha}_t}{\alpha_t})$, which means that
\begin{talign}
    \hat{\sigma}(t) = \sqrt{-\frac{\dot{\alpha}_t}{\alpha_t}}, \qquad\quad
    \hat{b}(x,t) = - \frac{\dot{\alpha}_t}{2\alpha_t} x -\frac{\dot{\alpha}_t}{\alpha_t} \mathfrak{s}(x,t),
\end{talign}
which is independent of $\tilde{\sigma}$.
If we denote by $\cev{b}(x,t) = b(x,t) + \tilde{\sigma}(t)^2 \mathfrak{s}(x,t)$ the drift of the backward SDE \eqref{eq:backward_sde}, for DDIM we obtain that
\begin{talign}
    \cev{b}(x,t) = 
    - \frac{\dot{\alpha}_t}{2\alpha_t} x - \frac{1}{2}(-\tilde{\sigma}(t)^2 + \frac{\dot{\alpha}_t}{\alpha_t})\mathfrak{s}(x,t).
\end{talign}
For DDPM, we have that $\hat{\sigma}(t) = \sqrt{-\frac{\dot{\alpha}_t}{\alpha_t}}$, which means that
\begin{talign}
    \cev{b}(x,t) = - \frac{\dot{\alpha}_t}{2\alpha_t} x -\frac{\dot{\alpha}_t}{\alpha_t} \mathfrak{s}(x,t) = \hat{b}(x,t).
\end{talign}
% In fact, plugging these choices into the SDE \eqref{eq:controlled_SDE_diff}, one observes that the resulting uncontrolled SDE is equal to the backward SDE for DDPM.
% $b(x,t) = - \frac{\dot{\alpha}_t}{\alpha_t}x - \big( \frac{\tilde{\sigma}(t)^2}{2} - \beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \big) \mathfrak{s}(x,t)$.

For Flow Matching, we have that $\kappa_t = 
% \frac{\dot{\beta}_t}{\beta_t}
- \frac{\dot{\alpha}_t}{\alpha_t}$, $\eta_t = 
% - \alpha_t(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t) - \frac{\tilde{\sigma}(t)^2}{2}$, 
 - \big( \frac{\tilde{\sigma}(t)^2}{2} - \beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \big)$,
which means that
\begin{talign}
    % \hat{\sigma}(t) = \sqrt{-2\alpha_t(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t)}, \qquad \hat{b}(x,t) = \frac{\dot{\beta}_t}{\beta_t} x - 2\alpha_t(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t) \mathfrak{s}(x,t).
    \hat{\sigma}(t) = \sqrt{2\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t)}, \qquad \hat{b}(x,t) = - \frac{\dot{\alpha}_t}{\alpha_t} x + 2\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \mathfrak{s}(x,t) = %- \frac{\dot{\alpha}_t}{\alpha_t} x + 2 \big( v(x,t) + \frac{\dot{\alpha}_t}{\alpha_t} x \big)
    2 v(x,t) + \frac{\dot{\alpha}_t}{\alpha_t} x.
\end{talign}
where we used \eqref{eq:v_and_s}.
For Flow Matching, the drift of the backward SDE \eqref{eq:backward_sde} reads
\begin{talign}
    \cev{b}(x,t) = 
    % \frac{\dot{\beta}_t}{\beta_t} x - \big(\alpha_t(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t) - \frac{\tilde{\sigma}(t)^2}{2} \big) \mathfrak{s}(x,t). 
    -\frac{\dot{\alpha}_t}{\alpha_t} x + \big(\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) + \frac{\tilde{\sigma}(t)^2}{2} \big) \mathfrak{s}(x,t) 
    = \frac{\dot{\alpha}_t}{\alpha_t} \frac{\tilde{\sigma}(t)^2}{2 \beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t)} x %+ \big(\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) 
    + \big( 1 + \frac{\tilde{\sigma}(t)^2}{2 \beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t)} \big) v(x,t)
\end{talign}
% \begin{talign}
%     \mathfrak{s}(x,t) = \frac{1}{\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t)} \big( v(x,t) + \frac{\dot{\alpha}_t}{\alpha_t} x \big)
% \end{talign}
where we used \eqref{eq:v_and_s}.
For Flow Matching with explicit forward process, we have that 
% $\tilde{\sigma}(t) = \sqrt{-2\alpha_t(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t)}$,
$\tilde{\sigma}(t) = \sqrt{2\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t)}$,
which means that
\begin{talign}
    \cev{b}(x,t) = 
    % \frac{\dot{\beta}_t}{\beta_t} x - 2\alpha_t(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t) \mathfrak{s}(x,t) 
    - \frac{\dot{\alpha}_t}{\alpha_t} x + 2\beta_t(\dot{\beta}_t - \frac{\dot{\alpha}_t}{\alpha_t} \beta_t) \mathfrak{s}(x,t)
    = \hat{b}(x,t).
\end{talign}

\subsection{Expressions for $\hat{\sigma}$, $\hat{b}$ and $\tilde{b}$ for particular choices of $\alpha_t$, $\beta_t$}

Suppose that $\alpha_t = 1-t$, $\beta_t = t$. 
% and $\tilde{\sigma}(t) = \sqrt{1-t}$. 
Then, for DDIM, we have that
\begin{talign}
    \hat{\sigma}(t) = \frac{1}{\sqrt{1-t}}, \qquad\quad
    \hat{b}(x,t) = \frac{1}{2(1-t)} x + \frac{1}{1-t} \mathfrak{s}(x,t), \qquad\quad \cev{b}(x,t) = 
    \frac{1}{2(1-t)} x + \frac{1}{2}(\tilde{\sigma}(t)^2 + \frac{1}{1-t})\mathfrak{s}(x,t).
\end{talign}
And for Flow Matching we have that
\begin{talign}
\begin{split}
    &\hat{\sigma}(t) 
    % \! = \! \sqrt{2t(1 \! + \! \frac{1}{1-t} t)} 
     = \sqrt{\frac{2t}{1-t}}, \qquad\qquad \hat{b}(x,t) 
    % \! = \! \frac{x}{t} \! + \! \frac{2(1-t)}{t} \mathfrak{s}(x,t), 
     =  \frac{x}{1-t}  + \frac{2t}{1-t} \mathfrak{s}(x,t)  =  2 v(x,t) - \frac{1}{1-t} x, \\ &\cev{b}(x,t)  =   
    % \frac{x}{t} \! + \! \big(\frac{2(1-t)}{t} \! + \! \frac{\tilde{\sigma}(t)^2}{2} \big) \mathfrak{s}(x,t). 
    \frac{x}{1-t}  +  \big(\frac{\tilde{\sigma}(t)^2}{2}  +  \frac{t}{1-t} \big) \mathfrak{s}(x,t)  = - \frac{\tilde{\sigma}(t)^2 x}{2t}  +  ( 1  + \frac{(1-t)\tilde{\sigma}(t)^2}{2t} ) v(x,t). 
\end{split}
\end{talign}
\fi

% \section{Proofs of \Cref{subsec:existing_losses}} 
\section{Loss function derivations}
\label{sec:proof_existing_losses}

\subsection{Derivation of the Continuous Adjoint method} \label{subsec:derivation_cont_adj_method}
\begin{proposition} \label{prop:cont_adjoint_method}
    The gradient $\frac{\mathrm{d} \mathcal{L}}{\mathrm{d} \theta}$ of the adjoint loss $\mathcal{L}(u ; \fX)$ defined in \eqref{eq:L_RE} with respect to the parameters $\theta$ of the control can be expressed as in \eqref{eq:continuous_adjoint_grads}.
\end{proposition}
\begin{proof}
    First, note that we can write 
    \begin{talign}
    \begin{split} \label{eq:first_eq_cont_adjoint}
        &\nabla_{\theta} \mathbb{E} \big[ \int_0^T \big(\frac{1}{2} \|u_{\theta}(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T) \big] \\ &= \mathbb{E} \big[ \int_0^T \nabla_{\theta} u_{\theta}(X^{u_{\theta}}_t,t) u_{\theta}(X^{u_{\theta}}_t,t) \, \mathrm{d}t \big] 
        % \\ &\qquad 
        + \nabla_{\theta} \mathbb{E} \big[ \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T)
        % \textcolor{blue}{+ \, \int_0^T v(X^{u_{\theta}}_t,t) \, \mathrm{d}B_t}
        \big] \rvert_{v = \mathrm{stopgrad}(u_{\theta})}.
    \end{split}
    \end{talign}
    To develop the second term, we apply \Cref{lem:adjoint_state_properties}. Namely, by the Leibniz rule and equation \eqref{eq:nabla_theta_cost}, we have that
    \begin{talign}
    \begin{split}
        &\nabla_{\theta} \mathbb{E} \big[ \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T) \big] \rvert_{v = \mathrm{stopgrad}(u_{\theta})} \\ &= \mathbb{E} \big[ \nabla_{\theta} \big( \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T) 
        \big) \rvert_{v = \mathrm{stopgrad}(u_{\theta})} \big] \\ &= \mathbb{E} \big[ \int_0^T (\nabla_{\theta} u_{\theta})(X^{u_{\theta}}_t(\omega),t)^{\top} \sigma(t)^{\top} a_t(\omega) \, \mathrm{d}t \big].
    \end{split}
    \end{talign}
    Plugging the right-hand side of this equation into \eqref{eq:first_eq_cont_adjoint} concludes the proof.
\end{proof}

\begin{lemma} \label{lem:adjoint_state_properties}
    Let $v$ be an arbitrary fixed vector field.
    The unique solution of the ODE 
    \begin{talign} 
    \begin{split} \label{eq:cont_adjoint_1_app}
        \frac{\mathrm{d}}{\mathrm{d}t} a(t;\fX^u,u)  &=  - \left[ \left(\nabla_{X^u_t} (b (X^u_t,t) + \sigma(t) u(X^u_t,t))\right)\tran{} a(t;\fX^u,u) 
        + \nabla_{X^u_t} \left( f(X^u_t,t) + \frac{1}{2}\|v(X^u_t,t)\|^2 \right) \right],
    \end{split}
        \\ a(1;\fX^u,u) &= \nabla g(X^u_1), \label{eq:cont_adjoint_2_app}
    \end{talign}
    satisfies:  
    \begin{talign} 
    \begin{split} \label{eq:adjoint_grad_x}
        &a(t ; \fX^u, u) := \nabla_{X^u_t} \big(\int_t^1 \big(\frac{1}{2} \|u(X_{t'}^u,t')\|^2 \! + \! f(X_{t'}^u,t') \big) \, \mathrm{d}t' \! + \! g(X^u_1) \big), \\
        &\text{where } \fX^u \text{ solves } \mathrm{d}X^u_t =  \left( b(X^u_t,t) + \sigma(t) u(X^u_t,t) \right) \, \mathrm{d}t + 
        \sigma(t) \mathrm{d}B_t.
    \end{split}
    \end{talign}
    Moreover, when $u = u_{\theta}$ is parameterized by $\theta$ we have that 
    \begin{talign}
    \begin{split} \label{eq:nabla_theta_cost}
        \nabla_{\theta} \big( \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T) 
        \big) = \int_0^T (\nabla_{\theta} u_{\theta})(X^{u_{\theta}}_t(\omega),t) \sigma(t)^{\top} a_t(\omega) \, \mathrm{d}t.
    \end{split}
    \end{talign}
\end{lemma}
\begin{proof}
    We use an approach based on Lagrange multipliers which mirrors and extends the derivation of the adjoint ODE \citep[Lemma~8]{domingoenrich2023stochastic}. For shortness, we use the notation $\tilde{b}_{\theta}(x,t) := b(x,t) + \sigma(t) u_{\theta}(x,t)$. Define a process $a : \Omega \times [0,T] \to \R^d$ such that for any $\omega \in \Omega$, $a(\omega,\cdot)$ is differentiable. For a given $\omega \in \Omega$, we can write
    \begin{talign}
    \begin{split}
        % &\int_0^T f(X_t(\omega),t) \, \mathrm{d}t + \int_0^T \langle h(X_t(\omega),t), \, \mathrm{d}B_t \rangle + g(X_T(\omega)) 
        &\int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T)
        % \textcolor{blue}{+ \, \int_0^T v(X^{u_{\theta}}_t,t) \, \mathrm{d}B_t}
        \\ &= 
        \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T)
        % \textcolor{blue}{+ \, \int_0^T v(X^{u_{\theta}}_t,t) \, \mathrm{d}B_t} 
        \\ &\qquad - \int_0^T \langle a_t(\omega), (dX^{u_{\theta}}_t(\omega) - \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t) \, \mathrm{d}t - \sigma(t) \, \mathrm{d}B_t) \rangle. 
    \end{split}
    \end{talign}
    By stochastic integration by parts \citep[Lemma~9]{domingoenrich2023stochastic}, we have that
    \begin{talign}
    \begin{split}
        \int_0^T \langle a_t(\omega), dX^{u_{\theta}}_t(\omega) \rangle = \langle a_T(\omega), X^{u_{\theta}}_T(\omega) \rangle - \langle a_0(\omega), X^{u_{\theta}}_0(\omega) \rangle - \int_0^T \langle X^{u_{\theta}}_t(\omega), \frac{da_t}{dt}(\omega) \rangle \, \mathrm{d}t.
    \end{split}
    \end{talign}
    Hence, if $X^{u_{\theta}}_0 = x_0$ is the initial condition, we have that\footnote{Unlike \citep[Lemma~8]{domingoenrich2023stochastic}, we use the convention that a Jacobian matrix $J = \nabla_x v(x)$ is defined as $J_{ij} = \frac{\partial v_i(x)}{\partial x_j}$. Their definition of $\nabla_x v$ is the transpose of ours.}
    \begin{talign}
    \begin{split}
        &\nabla_{x_0} \big( \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T) 
        % \textcolor{blue}{+ \, \int_0^T v(X^{u_{\theta}}_t,t) \, \mathrm{d}B_t} 
        \big) \\
        &= \nabla_{x_0} \big( 
        \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T)
        % \textcolor{blue}{+ \, \int_0^T v(X^{u_{\theta}}_t,t) \, \mathrm{d}B_t} 
        \\ &\qquad\quad - \langle a_T(\omega), X^{u_{\theta}}_T(\omega) \rangle + \langle a_0(\omega), X^{u_{\theta}}_0(\omega) \rangle + \int_0^T \big( \langle a_t(\omega), \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t) \rangle + \langle \frac{da_t}{dt}(\omega), X^{u_{\theta}}_t(\omega) \rangle \big) \, \mathrm{d}t \\ &\qquad\quad + \int_0^T \langle a_t(\omega), \sigma(t) \, \mathrm{d}B_t \rangle \big) \\
        &=  
        \int_0^T \nabla_{x_0} X^{u_{\theta}}_t(\omega)^{\top} \nabla_x \big( \frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t(\omega),t) \big) \, \mathrm{d}t + \nabla_{x_0} X^{u_{\theta}}_T(\omega)^{\top} \nabla_x g(X^{u_{\theta}}_T(\omega)) \\ &\qquad\quad 
        % \textcolor{blue}{+ \, \int_0^T \nabla_{\theta} X^{u_{\theta}}_t(\omega) \nabla_x v(X^{u_{\theta}}_t(\omega),t)) \, \mathrm{d}B_t} 
        - \nabla_{x_0} X^{u_{\theta}}_T(\omega)^{\top} a_T(\omega) + \nabla_{x_0} X^{u_{\theta}}_0(\omega)^{\top} a_0(\omega) \\ &\qquad\quad + \int_0^T \big( \nabla_{x_0} X^{u_{\theta}}_t(\omega)^{\top} \nabla_{x} \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t)^{\top} a_t(\omega)
        + \nabla_{x_0} X^{u_{\theta}}_t(\omega)^{\top} \frac{da_t}{dt}(\omega) \big) \, \mathrm{d}t
        \\ &= \int_0^T \nabla_{x_0} X^{u_{\theta}}_t(\omega)^{\top} \big( \nabla_x \big( \frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t(\omega),t) \big) + \nabla_{x} \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t)^{\top} a_t(\omega) + \frac{da_t}{dt}(\omega) \big) \, \mathrm{d}t \\ &\qquad\quad + \nabla_{x_0} X^{u_{\theta}}_T(\omega)^{\top} \big( \nabla_x g(X^{u_{\theta}}_T(\omega)) - a_T(\omega) \big) + a_0(\omega).
    \end{split}    
    \end{talign}
    In the last line we used that $\nabla_{x_0} X^{u_{\theta}}_0(\omega) = \nabla_{x_0} x_0 = \mathrm{I}$.
    If choose $a$ such that
    \begin{talign}
    \begin{split} \label{eq:a_rewritten}
        da_t(\omega) &= \big( - \nabla_{x} \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t)^{\top} a_t(\omega) - \nabla_x \big( \frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t(\omega),t) \big) \big) \, \mathrm{d}t, 
        % \, \textcolor{blue}{- \, \nabla_x v(X^{u_{\theta}}_t(\omega),t)) \, \mathrm{d}B_t}, 
        \\ a_T(\omega) &= \nabla_x g(X^{u_{\theta}}_T(\omega)),
    \end{split}
    \end{talign}
    which is the ODE \eqref{eq:cont_adjoint_1_app}-\eqref{eq:cont_adjoint_2_app}, then we obtain that
    \begin{talign}
        \nabla_{x_0} \big( \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T) 
        \big) = a_0(\omega)
    \end{talign}
    Without loss of generality, this argument can be extended from $t=0$ to an arbitrary $t \in [0,1]$, which proves the first statement of the lemma.

    To prove \eqref{eq:nabla_theta_cost}, we similarly write
    \begin{talign}
    \begin{split}
        &\nabla_{\theta} \big( \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T) 
        % \textcolor{blue}{+ \, \int_0^T v(X^{u_{\theta}}_t,t) \, \mathrm{d}B_t} 
        \big) 
        \\ &= \nabla_{\theta} \big( 
        \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T)
        % \textcolor{blue}{+ \, \int_0^T v(X^{u_{\theta}}_t,t) \, \mathrm{d}B_t} 
        \\ &\qquad\quad - \langle a_T(\omega), X^{u_{\theta}}_T(\omega) \rangle + \langle a_0(\omega), X^{u_{\theta}}_0(\omega) \rangle + \int_0^T \big( \langle a_t(\omega), \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t) \rangle + \langle \frac{da_t}{dt}(\omega), X^{u_{\theta}}_t(\omega) \rangle \big) \, \mathrm{d}t \\ &\qquad\quad + \int_0^T \langle a_t(\omega), \sigma(t) \, \mathrm{d}B_t \rangle \big)
        \\ &=  
        \int_0^T \nabla_{\theta} X^{u_{\theta}}_t(\omega)^{\top} \nabla_x \big( \frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t(\omega),t) \big) \, \mathrm{d}t + \nabla_{\theta} X^{u_{\theta}}_T(\omega)^{\top} \nabla_x g(X^{u_{\theta}}_T(\omega)) \\ &\qquad\quad 
        % \textcolor{blue}{+ \, \int_0^T \nabla_{\theta} X^{u_{\theta}}_t(\omega) \nabla_x v(X^{u_{\theta}}_t(\omega),t)) \, \mathrm{d}B_t} 
        - \nabla_{\theta} X^{u_{\theta}}_T(\omega)^{\top} a_T(\omega) + \nabla_{\theta} X^{u_{\theta}}_0(\omega)^{\top} a_0(\omega) \\ &\qquad\quad + \int_0^T \big( \nabla_{\theta} X^{u_{\theta}}_t(\omega)^{\top} \nabla_{x} \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t)^{\top} a_t(\omega) + \nabla_{\theta} \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t)^{\top} a_t(\omega) + \nabla_{\theta} X^{u_{\theta}}_t(\omega)^{\top} \frac{da_t}{dt}(\omega) \big) \, \mathrm{d}t 
        \\ &= \int_0^T \nabla_{\theta} X^{u_{\theta}}_t(\omega)^{\top} \big( \nabla_x \big( \frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t(\omega),t) \big) + \nabla_{x} \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t)^{\top} a_t(\omega) + \frac{da_t}{dt}(\omega) \big) \, \mathrm{d}t \\ &\qquad\quad + \nabla_{\theta} X^{u_{\theta}}_T(\omega)^{\top} \big( \nabla_x g(X^{u_{\theta}}_T(\omega)) - a_T(\omega) \big) 
        % \textcolor{blue}{+ \, \int_0^T \nabla_{\theta} X^{u_{\theta}}_t(\omega) \nabla_x v(X^{u_{\theta}}_t(\omega),t)) \, \mathrm{d}B_t} 
        % \\ &\qquad\quad 
        + \int_0^T (\nabla_{\theta} \tilde{b}_{\theta})(X^{u_{\theta}}_t(\omega),t)^{\top} a_t(\omega) \, \mathrm{d}t.
    \end{split}
    \end{talign}
    In the last line we used that $\nabla_{\theta} X^{u_{\theta}}_0(\omega) = \nabla_{\theta} x = 0$.
    % If choose $a$ such that
    % \begin{talign}
    % \begin{split}
    %     da_t(\omega) &= \big( - \nabla_{x} \tilde{b}_{\theta}(X^{u_{\theta}}_t(\omega),t) a_t(\omega) - \nabla_x \big( \frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t(\omega),t) \big) \big) \, \mathrm{d}t, 
    %     % \, \textcolor{blue}{- \, \nabla_x v(X^{u_{\theta}}_t(\omega),t)) \, \mathrm{d}B_t}, 
    %     \\ a_T(\omega) &= \nabla_x g(X^{u_{\theta}}_T(\omega)),
    % \end{split}
    % \end{talign}
    % which is the Continuous Adjoint ODE \eqref{eq:cont_adjoint_1}-\eqref{eq:cont_adjoint_2}, 
    When $a$ satisfies \eqref{eq:a_rewritten}, we obtain that
    \begin{talign}
    \begin{split}
        &\nabla_{\theta} \big( \int_0^T \big(\frac{1}{2} \|v(X^{u_{\theta}}_t,t)\|^2 \! + \! f(X^{u_{\theta}}_t,t) \big) \, \mathrm{d}t \! + \! g(X^{u_{\theta}}_T)
        % \textcolor{blue}{+ \, \int_0^T v(X^{u_{\theta}}_t,t) \, \mathrm{d}B_t} 
        \big) \\ &= \int_0^T (\nabla_{\theta} \tilde{b}_{\theta})(X^{u_{\theta}}_t(\omega),t) a_t(\omega) \, \mathrm{d}t = \int_0^T (\nabla_{\theta} u_{\theta})(X^{u_{\theta}}_t(\omega),t)^{\top} \sigma(t)^{\top} a_t(\omega) \, \mathrm{d}t.
    \end{split}
    \end{talign}
    The last equality holds because $\tilde{b}_{\theta}(x,t) := b(x,t) + \sigma(t) u_{\theta}(x,t)$. 
\end{proof}

\subsection{Proof of \Cref{prop:continuous_adjoint_loss_main}: Theoretical guarantees of the basic Adjoint Matching loss}
\label{subsec:derivation_continuous}

Let $\bar{u} = \texttt{stopgrad}(u_{\theta})$. We can rewrite equation \eqref{eq:continuous_adjoint_grads} as:
\begin{talign}\label{eq:continuous_adjoint_grads_app}
    \nabla_{\theta} \mathcal{L}(u_{\theta}; \fX^{\bar{u}}) &=  
    \frac{1}{2}\int_0^1 \nabla_{\theta} \norm{u_{\theta}(X^{\bar{u}}_t, t)}^2 \mathrm{d} t
    +\int_0^1 \nabla_{\theta} u(X^{\bar{u}}_t, t)\tran{} \sigma(t)\tran{} a(t; \fX^{\bar{u}}, \bar{u}) \mathrm{d} t \\ &= \frac{1}{2}\int_0^1 \nabla_{\theta} \norm{u_{\theta}(X^{\bar{u}}_t, t) + \sigma(t)\tran{} a(t; \fX^{\bar{u}}, \bar{u})}^2 \mathrm{d} t = \nabla_{\theta} \mathcal{L}_{\mathrm{Basic-Adj-Match}}(u_{\theta}; \fX^{\bar{u}})
\end{talign}
This proves the first statement of the proposition. To prove that the only critical point of the expected basic Adjoint Matching loss is the optimal control, we first compute the first variation of $\mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}]$. Letting $v : \mathbb{R}^d \times [0,T] \to \mathbb{R}^d$ be arbitrary, we have that
    \begin{talign}
    \begin{split}
        &\frac{\mathrm{d}}{\mathrm{d}\epsilon} \mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}} (u + \epsilon v; \fX^{\bar{u}})] = \frac{\mathrm{d}}{\mathrm{d}\epsilon} \mathbb{E} \big[ \frac{1}{2} \int_0^T \| (u+\epsilon v)(X^{\bar{u}}_t,t) + \sigma(t)^{\top} a(t,X^{\bar{u}},\bar{u}) \|^2 \, \mathrm{d}t \big] \\ &= \mathbb{E} \big[ \int_0^T \langle v(X^{\bar{u}}_t,t), u(X^{\bar{u}}_t,t) + \sigma(t)^{\top} a(t,X^{\bar{u}},\bar{u}) \rangle \, \mathrm{d}t \big] \\ &= \mathbb{E} \big[ \int_0^T \langle v(X^{\bar{u}}_t,t), u(X^{\bar{u}}_t,t) + \sigma(t)^{\top} \mathbb{E}\big[a(t,X^{\bar{u}},\bar{u}) | X^{\bar{u}}_t \big] \rangle \, \mathrm{d}t \big] \\
        &\implies \frac{\delta}{\delta u} \mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}(u)(x,t) = u(x,t) + \mathbb{E}\big[a(t,X^{\bar{u}},\bar{u}) | X^{\bar{u}}_t = x \big] 
    \end{split}
    \end{talign}
    Hence, critical points satisfy that
    \begin{talign} 
    \begin{split} \label{eq:critical_point_cont_adj}
        u(x,t) &= -\sigma(t)^{\top} \mathbb{E}[a(t,X^u,u)|X^u_t=x] = - \sigma(t)^{\top} \mathbb{E} \big[ \nabla_{X^v_t} \int_t^T \big(\frac{1}{2} \|v(X^v_t,t)\|^2 \! + \! f(X^v_t,t) \big) \, \mathrm{d}t \! + \! g(X^v_T) | X^v_0 = x \big] \\ &= - \sigma(t)^{\top} \nabla_{x} \mathbb{E} \big[ \int_t^T \big(\frac{1}{2} \|v(X^v_t,t)\|^2 \! + \! f(X^v_t,t) \big) \, \mathrm{d}t \! + \! g(X^v_T) | X^v_0 = x \big] = - \sigma(t)^{\top} \nabla J(u;x,t),
    \end{split}
    \end{talign}
    % where the last equality holds by equation \eqref{eq:expectation_a_cont_adjoint}. 
    In this equation, the second equality holds by equation \eqref{eq:adjoint_grad_x} from \Cref{lem:adjoint_state_properties}, and the third equality holds by the Leibniz rule.
    
    \Cref{eq:lemma_cost_functional} shows that any control $u$ that satisfies \eqref{eq:critical_point_cont_adj} is equal to the optimal control, which concludes the proof. 

\begin{lemma} \label{eq:lemma_cost_functional}
    Suppose that for any $x \in \mathbb{R}^d$, $t \in [0,T]$, $u(x,t) = - \sigma(t)^{\top} \nabla_x J(u;x,t)$. Then, $J(u;\cdot,\cdot)$ satisfies the Hamilton-Jacobi-Bellman equation \eqref{eq:HJB_setup}. By the uniqueness of the solution to the HJB equation, we have that $J(u;x,t) = V(x,t)$ for any $x \in \mathbb{R}^d$, $t \in [0,T]$. Hence, $u(x,t) = - \sigma(t)^{\top} \nabla_x V(x,t)$ is the optimal control.
\end{lemma}
\begin{proof}
    Since $J(u;x,t) = \mathbb{E} \big[ \int_t^T \big( \frac{1}{2}\|u(X^u_t,t)\|^2 + f(X^u_t,t) \big) \, ds + g(X^u_T) | X^u_t = x \big]$, we have that
    \begin{talign}
        J(u;x,t) = \mathbb{E} \big[J(u;X^u_{t+\Delta t},t+\Delta t) | X_t = x \big] + \mathbb{E} \big[ \int_t^{t+\Delta t}
        \big( \frac{1}{2}\|u(X^u_s,s)\|^2 + f(X^u_s,s) \big) \, ds | X_t = x \big],
    \end{talign}
    which means that
    \begin{align} \label{eq:pre_limit}
        0 = \frac{\mathbb{E} [J(u;X^u_{t+\Delta t},t+\Delta t) | X_t = x ] - J(u;x,t)}{\Delta t} + \frac{\mathbb{E} \big[ \int_t^{t+\Delta t}
        \big( \frac{1}{2}\|u(X^u_t,t)\|^2 + f(X^u_t,t) \big) \, ds | X_t = x \big]}{\Delta t}
    \end{align}
    Recall that the generator $\mathcal{T}^u$ of the controlled SDE \eqref{eq:controlled_SDE} takes the form:
    \begin{talign}
    \begin{split}
        \mathcal{T}^u f(x,t) &:= \lim_{\Delta t \to 0} \frac{\mathbb{E} \big[f(X^u_{t+\Delta t},t) | X_t = x \big] - f(x,t)}{\Delta t} \\ &= \partial_t f(x,t) + \langle \nabla f(x,t), b(x,t) + \sigma(t) u(x,t) \rangle + \mathrm{Tr}\big( \frac{\sigma(t) \sigma(t)^{\top}}{2} \nabla^2 f(x,t) \big)
    \end{split}
    \end{talign}
    Hence, if we take the limit $\Delta t \to 0$ on equation \eqref{eq:pre_limit}, we obtain that:
    \begin{talign} 
    \begin{split} \label{eq:HJB_discounted_1}
        0 &= \mathcal{T}^u J(u;x,t) + \frac{1}{2}\|u(x,t)\|^2 + f(x,t) \\ &= \partial_t J(u;x,t) + \langle \nabla J(u;x,t), b(x,t) + \sigma(t) u(x,t) \rangle + \mathrm{Tr}\big( \frac{\sigma(t) \sigma(t)^{\top}}{2} \nabla^2 J(u;x,t) \big) + \frac{1}{2}\|u(x,t)\|^2 + f(x,t).
    \end{split}
    \end{talign}
    % Setting $C(t,x,u) = - \nabla \cdot b_t(x) + \frac{1}{2}\|x\|^2$, equation \eqref{eq:HJB_discounted_1} becomes
    Now using that $u(x,t) = - \sigma(t)^{\top} \nabla_x J(u;x,t)$, we have that 
    \begin{talign}
    \begin{split}
        \langle \nabla J(u;x,t), \sigma(t) u(x,t) \rangle + \frac{1}{2}\|u(x,t)\|^2 &= - \|\sigma(t)^{\top} \nabla_x J(u;x,t) \|^2 + \frac{1}{2} \|\sigma(t)^{\top} \nabla_x J(u;x,t) \|^2 \\ &= - \frac{1}{2} \|\sigma(t)^{\top} \nabla_x J(u;x,t) \|^2.
    \end{split}
    \end{talign}
    Plugging this back into \eqref{eq:HJB_discounted_1}, we obtain that
    \begin{talign} 
    \begin{split} \label{eq:HJB_discounted_2}
        0 &= \partial_t J(u;x,t) + \langle \nabla J(u;x,t), b(x,t) \rangle + \mathrm{Tr}\big( \frac{\sigma(t) \sigma(t)^{\top}}{2} \nabla^2 J(u;x,t) \big) - \frac{1}{2} \|\sigma(t)^{\top} \nabla_x J(u;x,t) \|^2 + f(x,t).
    \end{split}
    \end{talign}
    And since $J(u;x, T) = g(x)$ by construction, we conclude that $J(u;x,t)$ satisfies the HJB equation \eqref{eq:HJB_setup}.
    \end{proof}

% \section{Proofs of \Cref{sec:new_losses}}

\subsection{Theoretical guarantees of the Adjoint Matching loss}
\label{subsec:proof_lean_adjoint}

\begin{proposition}[Theoretical guarantee of the Adjoint Matching loss] \label{prop:lean_adjoint}
    The only critical point of the loss $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$ is the optimal control $u^*$.
\end{proposition}

\begin{proof}
    Let $v$ be an arbitrary control. If $\tilde{a}(t; \mathbf{X}^v)
    % := \tilde{a}(\omega,t)
    $ is the solution of the Lean Adjoint ODE \eqref{eq:lean_adjoint_1}-\eqref{eq:lean_adjoint_2},
    it satisfies the integral equation 
    \begin{talign}
    \tilde{a}(t; \mathbf{X}^v) = \int_t^T \big( \nabla_x b(X^v_s, s)^\top \tilde{a}(s; \mathbf{X}^v) + \nabla_x f(X^v_s, s) \big) \, \mathrm{d}s + \nabla g(X^v_T).
    \end{talign}
    Hence,
    \begin{talign}
    \begin{split} \label{eq:expected_lean_integral}
    \mathbb{E}\big[\tilde{a}(t; \mathbf{X}^v) \big| X^v_t \big] &= \mathbb{E}\big[\int_t^T \big( \nabla_x b(X^v_s, s)^\top \tilde{a}(s; \mathbf{X}^v) + \nabla_x f(X^v_s, s) \big) \, \mathrm{d}s + \nabla g(X^v_T) \big| X^v_t \big] \\
    &= \mathbb{E}\big[\int_t^T \big( \nabla_x b(X^v_s, s)^{\top} \mathbb{E}\big[\tilde{a}(s; \mathbf{X}^v) \big| X^v_s \big] + \nabla_x f(X^v_s, s) \big) \, \mathrm{d}s + \nabla g(X^v_T) \big| X^v_t \big],
    \end{split}
    \end{talign}
    where we used the tower property of conditional expectation in the second equality.

    Similarly, if $a(t; \mathbf{X}^v, v)$ is the solution of the Adjoint ODE \eqref{eq:cont_adjoint_1}-\eqref{eq:cont_adjoint_2}, it satisfies the integral equation
    \begin{talign}
    a(t; \mathbf{X}^v, v) = \int_t^T \big( \nabla_x \big( b(X^v_s, s)^{\top} a(s; \mathbf{X}^v, v) + \sigma(s) v(X^v_s,s) \big) + \nabla_x \big( f(X^v_s, s) + \frac{1}{2} \|v(X^v_s,s)\|^2 \big) \big) \, \mathrm{d}s + \nabla g(X^v_T),
    \end{talign}
    and its expected value satisfies
    \begin{talign}
    \begin{split} \label{eq:expected_full_integral}
    &\mathbb{E}\big[a(t; \mathbf{X}^v, v)\big| X^v_t \big] \\ &= \mathbb{E}\big[\int_t^T \big( \nabla_x \big( b(X^v_s, s) + \sigma(s) v(X^v_s,s) \big)^{\top} a(s; \mathbf{X}^v, v) + \nabla_x \big( f(X^v_s, s) + \frac{1}{2} \|v(X^v_s,s)\|^2 \big) \big) \, \mathrm{d}s + \nabla g(X^v_T)\big| X^v_t \big] \\ &= \! \mathbb{E}\big[\int_t^T \big( \nabla_x \big( b(X^v_s, s) \! + \! \sigma(s) v(X^v_s,s) \big)^{\top} \mathbb{E}\big[ a(s; \mathbf{X}^v, v) \big| X^v_s \big] \! + \! \nabla_x \big( f(X^v_s, s) \! + \! \frac{1}{2} \|v(X^v_s,s)\|^2 \big) \big) \, \mathrm{d}s \! + \! \nabla g(X^v_T)\big| X^v_t \big].
    \end{split}
    \end{talign}
    
    Let us rewrite $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$ as follows:
    \begin{talign}
    \begin{split} \label{eq:lean_adjoint_rewritten}
        \mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}(u)] &:= \mathbb{E} \big[\int_0^{T} \big\| u(X^v_t,t)
        + \sigma(t)^{\top} \mathbb{E}\big[ \tilde{a}(t,\mathbf{X}^v) | X^v_{t} \big] \big\|^2 \, \mathrm{d}t \big] \rvert_{v = \mathrm{stopgrad}(u)} \\ &\qquad + \mathbb{E} \big[\int_0^{T} \big\| \sigma(t)^{\top} \big(\mathbb{E}\big[ \tilde{a}(t,\mathbf{X}^v) | X^v_{t} \big] - \tilde{a}(t,\mathbf{X}^v)  \big)\big\|^2 \, \mathrm{d}t \big] \rvert_{v = \mathrm{stopgrad}(u)},
    \end{split}
    \end{talign}
    Now, suppose that $\hat{u}$ is a critical point of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$. By definition, this implies that the first variation of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$ is zero. Using \eqref{eq:lean_adjoint_rewritten}, we can write this as follows:
    \begin{talign}
        &0 = \frac{\delta}{\delta u} \mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}(\hat{u})](x) = 2 \big(\hat{u}%(X^{\hat{u}}_t,t)
        (x,t)
        + \sigma(t)^{\top} \mathbb{E}[\tilde{a}(t,\mathbf{X}^{\hat{u}}) | X^{\hat{u}}_t = x] \big), \\ &\implies \hat{u}(x,t) = - \sigma(t)^{\top} \mathbb{E}[\tilde{a}(t,\mathbf{X}^{\hat{u}})| X^{\hat{u}}_t = x].
        \label{eq:AM_critical}
    \end{talign}
    Hence, we have
    \begin{talign}
        % - 
        &\nabla_x  \hat{u}(X^{\hat{u}}_t,t)^{\top} \sigma(t)^{\top} \mathbb{E}[\tilde{a}(t,\mathbf{X}^{\hat{u}}) |X^{\hat{u}}_t] 
        % - 
        + \nabla_x {\hat{u}}(X^{\hat{u}}_t,t)^{\top} {\hat{u}}(X^{\hat{u}}_t,t) = 0, \\
        &\implies \mathbb{E}\big[\int_t^T \big( \nabla_x \big( \sigma(s) \hat{u}(X^{\hat{u}}_s,s) \big)^{\top} \mathbb{E}\big[ \tilde{a}(s; \mathbf{X}^{\hat{u}}) \big| X^{\hat{u}}_s \big] \! + \! \nabla_x \big( \frac{1}{2} \|{\hat{u}}(X^{\hat{u}}_s,s)\|^2 \big) \big) \, \mathrm{d}s \big| X^{\hat{u}}_t \big] = 0.
        \label{eq:zero_equality}
    \end{talign}
    If we set $v = \hat{u}$ in equation \eqref{eq:expected_lean_integral}, and add \eqref{eq:zero_equality} to its right-hand side, we obtain that $\mathbb{E}[\tilde{a}(t,X^{\hat{u}})|X^{\hat{u}}_{t}]$ also solves the integral equation
    \begin{talign}
    \begin{split}
        &\mathbb{E}\big[\tilde{a}(t; \mathbf{X}^{\hat{u}})\big| X^{\hat{u}}_t \big] \\ &= \! \mathbb{E}\big[\int_t^T \big( \nabla_x \big( b(X^{\hat{u}}_s, s) \! + \! \sigma(s) {\hat{u}}(X^{\hat{u}}_s,s) \big)^{\top} \mathbb{E}\big[ \tilde{a}(s; \mathbf{X}^{\hat{u}}) \big| X^{\hat{u}}_s \big] \! + \! \nabla_x \big( f(X^{\hat{u}}_s, s) \! + \! \frac{1}{2} \|{\hat{u}}(X^{\hat{u}}_s,s)\|^2 \big) \big) \, \mathrm{d}s \! + \! \nabla g(X^{\hat{u}}_T)\big| X^{\hat{u}}_t \big].
    \end{split}
    \end{talign}
    Note that this integral equation is the same one as equation \eqref{eq:expected_full_integral} when we set $v = \hat{u}$ in the latter. \Cref{prop:uniqueness_integral} states that the solution of the integral equation is unique, which means that $\mathbb{E}\big[\tilde{a}(t; \mathbf{X}^{\hat{u}})\big| X^{\hat{u}}_t \big] = \mathbb{E}\big[a(t; \mathbf{X}^{\hat{u}},\hat{u})\big| X^{\hat{u}}_t \big]$ for all $t \in [0,T]$.
    % \begin{talign} 
    % \begin{split} \label{eq:equal_ODE_1_a}
    %     \frac{d\mathbb{E}[\tilde{a}(t,X^{\hat{u}})|X^{\hat{u}}_{t}]}{dt} &= - \nabla_x (b (X^{\hat{u}}_t,t) + \sigma(t) {\hat{u}}(X^{\hat{u}}_t,t)) \mathbb{E}[\tilde{a}(t,X^{\hat{u}})|X^{\hat{u}}_{t}] \\ &\quad - \nabla_x (f(X^{\hat{u}}_t,t) + \frac{1}{2}\|{\hat{u}}(X^{\hat{u}}_t,t)\|^2), 
    % \end{split}
    %     \\ \mathbb{E}[\tilde{a}(T,X^{\hat{u}})|X^{\hat{u}}_{T}] &= \nabla g(X^{\hat{u}}_T), \label{eq:equal_ODE_1_b}
    % \end{talign}
    % If $a(t,X^{\hat{u}}) := a(\omega,t)$ is the solution of the Adjoint ODE \eqref{eq:cont_adjoint_1}-\eqref{eq:cont_adjoint_2}, we obtain that $\mathbb{E}[a(t,X^{\hat{u}})|X^{\hat{u}}_{t}]$ is a solution of 
    % \begin{talign} 
    %     \begin{split} \label{eq:equal_ODE_2_a}
    %     \frac{d\mathbb{E}[a(t,X^{\hat{u}})|X^{\hat{u}}_{t}]}{dt} &= - \nabla_x (b (X^{\hat{u}}_t,t) + \sigma(t) {\hat{u}}(X^{\hat{u}}_t,t)) \mathbb{E}[a(t,X^{\hat{u}})|X^{\hat{u}}_{t}] \\ &\quad - \nabla_x (f(X^{\hat{u}}_t,t) + \frac{1}{2}\|{\hat{u}}(X^{\hat{u}}_t,t)\|^2),
    %     \end{split} \\ \mathbb{E}[a(T,X^{\hat{u}})|X^{\hat{u}}_{T}] &= \nabla g(X^{\hat{u}}_T), \label{eq:equal_ODE_2_b}
    % \end{talign}
    % Remark that \eqref{eq:equal_ODE_2_a}-\eqref{eq:equal_ODE_2_b} is the same ODE as \eqref{eq:equal_ODE_1_a}-\eqref{eq:equal_ODE_1_b}. By uniqueness of ODE solutions, we obtain that $\mathbb{E}[\tilde{a}(t,X^{\hat{u}})|X^{\hat{u}}_{t}] = \mathbb{E}[a(t,X^{\hat{u}})|X^{\hat{u}}_{t}]$ for all $t \in [0,T]$ when ${\hat{u}}$ is a critical point of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$. 
    % % \textcolor{red}{Use critical instead of stationary}.
    
    Since we can reexpress the basic Adjoint Matching loss as
    \begin{talign}
    \begin{split} \label{eq:cont_adjoint_rewritten}
        \mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}(u)] &:= \mathbb{E} \big[\int_0^{T} \big\| u(X^v_t,t)
        + \sigma(t)^{\top} \mathbb{E}\big[ a(t;\mathbf{X}^v,v) | X^v_{t} \big] \big\|^2 \, \mathrm{d}t \big] \rvert_{v = \mathrm{stopgrad}(u)} \\ &\qquad + \mathbb{E} \big[\int_0^{T} \big\| \sigma(t)^{\top} \big(\mathbb{E}\big[ a(t;\mathbf{X}^v,v) | X^v_{t} \big] - a(t;\mathbf{X}^v,v)  \big)\big\|^2 \, \mathrm{d}t \big] \rvert_{v = \mathrm{stopgrad}(u)},
    \end{split}
    \end{talign}
    we obtain that when ${\hat{u}}$ is a critical point of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$,
    \begin{talign}
    \begin{split}
        \frac{\mathrm{d}}{\mathrm{d}u} \mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}(\hat{u})](x) &= 2 \big(\hat{u}(x,t)
        + \sigma(t)^{\top} \mathbb{E}[a(t;\mathbf{X}^{\hat{u}},\hat{u}) | X^{\hat{u}}_t = x] \big) \\ &= 2 \big(\hat{u}(x,t)
        + \sigma(t)^{\top} \mathbb{E}[\tilde{a}(t;\mathbf{X}^{\hat{u}}) | X^{\hat{u}}_t = x] \big) = 0,
    \end{split}
    \end{talign}
    where the second equality holds because $\mathbb{E}\big[\tilde{a}(t; \mathbf{X}^{\hat{u}})\big| X^{\hat{u}}_t \big] = \mathbb{E}\big[a(t; \mathbf{X}^{\hat{u}},\hat{u})\big| X^{\hat{u}}_t \big]$, and the third equality holds by equation \eqref{eq:AM_critical}.
    Thus, we deduce that the critical points of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$ are critical points of $\mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}]$. By \Cref{prop:continuous_adjoint_loss_main}, $\mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}]$ has a single critical point, which is the optimal control $u^*$, which concludes the proof of the statement for $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$.
    \end{proof}
    
% \begin{proof}
%     If $\tilde{a}(t,X^v) := \tilde{a}(\omega,t)$ is the solution of the Lean Adjoint ODE \eqref{eq:lean_adjoint_1}-\eqref{eq:lean_adjoint_2},
%     we obtain that $\mathbb{E}[\tilde{a}(t,X^v)|X^v_{t}]$ is a solution of
%     \begin{talign} \label{eq:lean_adjoint_1_proof}
%         \frac{d\mathbb{E}[\tilde{a}(t,X^v)|X^v_{t}]}{dt} &= - \nabla_x b (X^v_t,t) \mathbb{E}[\tilde{a}(t,X^v)|X^v_{t}] - \nabla_x f(X^v_t,t), \\ \mathbb{E}[\tilde{a}(T,X^v)|X^v_{T}] &= \nabla g(X^v_T), \label{eq:lean_adjoint_2_proof}
%     \end{talign}
%     Let us rewrite $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$ as follows:
%     \begin{talign}
%     \begin{split} \label{eq:lean_adjoint_rewritten}
%         \mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}(u)] &:= \mathbb{E} \big[\int_0^{T} \big\| u(X^v_t,t)
%         + \sigma(t)^{\top} \mathbb{E}\big[ \tilde{a}(t,X^v) | X^v_{t} \big] \big\|^2 \, \mathrm{d}t \big] \rvert_{v = \mathrm{stopgrad}(u)} \\ &\qquad + \mathbb{E} \big[\int_0^{T} \big\| \sigma(t)^{\top} \big(\mathbb{E}\big[ \tilde{a}(t,X^v) | X^v_{t} \big] - \tilde{a}(t,X^v)  \big)\big\|^2 \, \mathrm{d}t \big] \rvert_{v = \mathrm{stopgrad}(u)},
%     \end{split}
%     \end{talign}
%     Now, suppose that $\hat{u}$ is a critical point of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$. By definition, this implies that the first variation of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$ is zero. Using \eqref{eq:lean_adjoint_rewritten}, we can write this as follows:
%     \begin{talign}
%         &0 = \frac{\delta}{\delta u} \mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}(\hat{u})](x) = 2 \big(\hat{u}%(X^{\hat{u}}_t,t)
%         (x,t)
%         + \sigma(t)^{\top} \mathbb{E}[\tilde{a}(t,X^{\hat{u}}) | X^{\hat{u}}_t = x] \big), \\ &\implies \hat{u}(x,t) = - \sigma(t)^{\top} \mathbb{E}[\tilde{a}(t,X^{\hat{u}})| X^{\hat{u}}_t = x].
%     \end{talign}
%     Hence, we have
%     \begin{talign}
%     \begin{split}
%         - \nabla_x  \hat{u}(X^{\hat{u}}_t,t) \sigma(t)^{\top} \mathbb{E}[\tilde{a}(t,X^{\hat{u}}) |X^{\hat{u}}_t] - \nabla_x {\hat{u}}(X^{\hat{u}}_t,t) {\hat{u}}(X^{\hat{u}}_t,t) = 0.
%     \end{split}
%     \end{talign}
%     Adding this to the right-hand side of \eqref{eq:lean_adjoint_1_proof}, we obtain that $\mathbb{E}[\tilde{a}(t,X^{\hat{u}})|X^{\hat{u}}_{t}]$ also solves the ODE
%     \begin{talign} 
%     \begin{split} \label{eq:equal_ODE_1_a}
%         \frac{d\mathbb{E}[\tilde{a}(t,X^{\hat{u}})|X^{\hat{u}}_{t}]}{dt} &= - \nabla_x (b (X^{\hat{u}}_t,t) + \sigma(t) {\hat{u}}(X^{\hat{u}}_t,t)) \mathbb{E}[\tilde{a}(t,X^{\hat{u}})|X^{\hat{u}}_{t}] \\ &\quad - \nabla_x (f(X^{\hat{u}}_t,t) + \frac{1}{2}\|{\hat{u}}(X^{\hat{u}}_t,t)\|^2), 
%     \end{split}
%         \\ \mathbb{E}[\tilde{a}(T,X^{\hat{u}})|X^{\hat{u}}_{T}] &= \nabla g(X^{\hat{u}}_T), \label{eq:equal_ODE_1_b}
%     \end{talign}
%     If $a(t,X^{\hat{u}}) := a(\omega,t)$ is the solution of the Adjoint ODE \eqref{eq:cont_adjoint_1}-\eqref{eq:cont_adjoint_2}, we obtain that $\mathbb{E}[a(t,X^{\hat{u}})|X^{\hat{u}}_{t}]$ is a solution of 
%     \begin{talign} 
%         \begin{split} \label{eq:equal_ODE_2_a}
%         \frac{d\mathbb{E}[a(t,X^{\hat{u}})|X^{\hat{u}}_{t}]}{dt} &= - \nabla_x (b (X^{\hat{u}}_t,t) + \sigma(t) {\hat{u}}(X^{\hat{u}}_t,t)) \mathbb{E}[a(t,X^{\hat{u}})|X^{\hat{u}}_{t}] \\ &\quad - \nabla_x (f(X^{\hat{u}}_t,t) + \frac{1}{2}\|{\hat{u}}(X^{\hat{u}}_t,t)\|^2),
%         \end{split} \\ \mathbb{E}[a(T,X^{\hat{u}})|X^{\hat{u}}_{T}] &= \nabla g(X^{\hat{u}}_T), \label{eq:equal_ODE_2_b}
%     \end{talign}
%     Remark that \eqref{eq:equal_ODE_2_a}-\eqref{eq:equal_ODE_2_b} is the same ODE as \eqref{eq:equal_ODE_1_a}-\eqref{eq:equal_ODE_1_b}. By uniqueness of ODE solutions, we obtain that $\mathbb{E}[\tilde{a}(t,X^{\hat{u}})|X^{\hat{u}}_{t}] = \mathbb{E}[a(t,X^{\hat{u}})|X^{\hat{u}}_{t}]$ for all $t \in [0,T]$ when ${\hat{u}}$ is a critical point of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$. 
%     % \textcolor{red}{Use critical instead of stationary}. 
%     Since we can reexpress the basic Adjoint Matching loss as
%     \begin{talign}
%     \begin{split} \label{eq:cont_adjoint_rewritten}
%         \mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}(u)] &:= \mathbb{E} \big[\int_0^{T} \big\| u(X^v_t,t)
%         + \sigma(t)^{\top} \mathbb{E}\big[ a(t,X^v) | X^v_{t} \big] \big\|^2 \, \mathrm{d}t \big] \rvert_{v = \mathrm{stopgrad}(u)} \\ &\qquad + \mathbb{E} \big[\int_0^{T} \big\| \sigma(t)^{\top} \big(\mathbb{E}\big[ a(t,X^v) | X^v_{t} \big] - a(t,X^v)  \big)\big\|^2 \, \mathrm{d}t \big] \rvert_{v = \mathrm{stopgrad}(u)},
%     \end{split}
%     \end{talign}
%     we obtain that when ${\hat{u}}$ is a critical point of $\mathcal{L}_{\mathrm{Adj-Match}}$,
%     \begin{talign}
%     \begin{split}
%         \frac{\mathrm{d}}{\mathrm{d}u} \mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}(\hat{u})](x) &= 2 \big(\hat{u}(x,t)
%         + \sigma(t)^{\top} \mathbb{E}[a(t,X^{\hat{u}}) | X^{\hat{u}}_t = x] \big) \\ &= 2 \big(\hat{u}(x,t)
%         + \sigma(t)^{\top} \mathbb{E}[\tilde{a}(t,X^{\hat{u}}) | X^{\hat{u}}_t = x] \big) = 0.
%     \end{split}
%     \end{talign}
%     Thus, we deduce that the critical points of $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$ are critical points of $\mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}]$. By \Cref{prop:continuous_adjoint_loss_main}, $\mathbb{E}[\mathcal{L}_{\mathrm{Basic-Adj-Match}}]$ has a single critical point, which is the optimal control $u^*$, which concludes the proof of the statement for $\mathbb{E}[\mathcal{L}_{\mathrm{Adj-Match}}]$.
%     \end{proof}

\begin{proposition} \label{prop:uniqueness_integral}
    Let $v$ be an arbitrary control. Consider the integral equation:
    \begin{talign}
         Y_t = 
         % \mathbb{E}\big[\int_t^T \big(Y_s^\top \nabla_x b(X_s^*, s) + \nabla_x f(X_s^*, s) \big) \mathrm{d}s  + \nabla g(X^*_T) \big| X^*_t\big],
         \mathbb{E}\big[\int_t^T \big( \nabla_x \big( b(X^v_s, s) \! + \! \sigma(s) v(X^v_s,s) \big)^{\top} Y_s \! + \! \nabla_x \big( f(X^v_s, s) \! + \! \frac{1}{2} \|v(X^v_s,s)\|^2 \big) \big) \, \mathrm{d}s \! + \! \nabla g(X^v_T)\big| X^v_t \big],
    \end{talign}
    where $t \in [0,T]$. This equation has a unique solution, i.e. if $Y^1$, $Y^2$ are two solutions then $Y_1 = Y_2$.
\end{proposition}
\begin{proof}
    Let $Y^1$, $Y^2$ be two solutions of the integral equation. We have that
    \begin{talign} 
        Y^1_t - Y^2_t = \mathbb{E}\big[\int_t^T \big((Y^1_s - Y^2_s)^\top \nabla_x b(X_s^*, s) \big) \mathrm{d}s \big| X^*_t\big].
    \end{talign}
    Thus,
    \begin{talign}
    \begin{split}
        &\|Y^1_t - Y^2_t\| \\ &\leq \mathbb{E}\big[\big\|\int_t^T \big((Y^1_s - Y^2_s)^\top \nabla_x b(X_s^*, s) \big) \mathrm{d}s \big\| \big| X^*_t\big] \leq \mathbb{E}\big[\int_t^T \big\|\big((Y^1_s - Y^2_s)^\top \nabla_x b(X_s^*, s) \big) \big\| \mathrm{d}s \big| X^*_t\big] \\ &\leq \mathbb{E}\big[\int_t^T \big\|Y^1_s - Y^2_s\big\| \cdot \big\| \nabla_x b(X_s^*, s) \big) \big\| \mathrm{d}s \big| X^*_t\big] = \int_t^T \mathbb{E}\big[\big\|Y^1_s - Y^2_s\big\| \cdot \big\| \nabla_x b(X_s^*, s) \big) \big\| \big| X^*_t\big] \mathrm{d}s \\ &\leq \int_t^T \big(\mathbb{E}\big[\big\|Y^1_s - Y^2_s\big\|^2 \big| X^*_t \big] \big)^{1/2} \cdot \big( \mathbb{E}\big[\big\| \nabla_x b(X_s^*, s) \big\|^2 \big| X^*_t\big] \big)^{1/2} \mathrm{d}s
    \end{split}
    \end{talign}
    And this implies that
    \begin{talign}
    \begin{split}
        &\sup_{t' \in [0,t]} \big(\mathbb{E}[\|Y^1_t - Y^2_t\|^2 | X^*_{t'} ] \big)^{1/2} \\ &\leq \int_t^T \big(\mathbb{E}\big[\big\|Y^1_s - Y^2_s\big\|^2 \big| X^*_t \big] \big)^{1/2} \cdot \big( \mathbb{E}\big[\big\| \nabla_x b(X_s^*, s) \big\|^2 \big| X^*_t\big] \big)^{1/2} \mathrm{d}s \\ &\leq \int_t^T \sup_{t' \in [0,s]} \big(\mathbb{E}\big[\big\|Y^1_s - Y^2_s\big\|^2 \big| X^*_{t'} \big] \big)^{1/2} \cdot \sup_{t' \in [0,s]} \big( \mathbb{E}\big[\big\| \nabla_x b(X_s^*, s) \big\|^2 \big| X^*_{t'}\big] \big)^{1/2} \mathrm{d}s.
    \end{split}
    \end{talign}
    Applying Grönwall's inequality on the function $f(t) = \sup_{t' \in [0,t]} \big(\mathbb{E}[\|Y^1_t - Y^2_t\|^2 | X^*_{t'} ] \big)^{1/2}$, we obtain that $\sup_{t' \in [0,t]} \big(\mathbb{E}[\|Y^1_t - Y^2_t\|^2 | X^*_{t'} ] \big)^{1/2} = 0$ for all $t \in [0,T]$, which means that $Y^1_t = Y^2_t$ almost surely. And since $\|Y^1_t - Y^2_t\| \leq \int_t^T \big(\mathbb{E}\big[\big\|Y^1_s - Y^2_s\big\|^2 | X^*_{t} \big] \big)^{1/2} \cdot \big( \mathbb{E}\big[\big\| \nabla_x b(X_s^*, s) \big\|^2 | X^*_{t} \big] \big)^{1/2} \mathrm{d}s = 0$, we obtain that $Y^1 = Y^2$.
\end{proof}

\subsection{Pseudo-code of Adjoint Matching for DDIM fine-tuning} \label{subsec:pseudocode_DDIM}

\begin{algorithm}[h]
\SetAlgoNoLine % Disable line numbering
\SetAlgoNlRelativeSize{0} %Set number line font to zero
\small{
\KwIn{Pre-trained denoiser $\epsilon^{\mathrm{base}}$, number of fine-tuning iterations $N$.}

Initialize fine-tuned denoiser: $\epsilon^{\mathrm{finetune}} = \epsilon^{\mathrm{base}}$ with parameters $\theta$.

  \For{$n \in \{0,\dots,N-1\}$}{
    Sample $m$ trajectories $\bm{X} = (X_t)_{t\in\{0, \dots, 1\}}$ 
    %with memoryless noise schedule $\sigma(t) = \sqrt{2 \beta_t (\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)}$, \eg :
    according to DDPM, \eg :
    \begin{talign} 
    \begin{split} \label{eq:EM_update_box_DDIM}
    % X_{k+1} = \sqrt{\bar{\alpha}_{k+1}}
    % \big( \frac{X_{k} - \sqrt{1-\bar{\alpha}_k} \epsilon(X_k,k)}{\sqrt{\bar{\alpha}_k}} \big)
    % + \sqrt{1-\bar{\alpha}_{k+1} - \sigma_{k}^2} \epsilon(X_k,k) +
    % % \frac{1}{\sqrt{K}} 
    % \sigma_{k} \varepsilon_{k},
    X_{k+1} &= \sqrt{\frac{\bar{\alpha}_{k+1}}{\bar{\alpha}_{k}}} \big( X_k - \frac{1-\bar{\alpha}_{k}/\bar{\alpha}_{k+1}}{\sqrt{1-\bar{\alpha}_k}} \epsilon^{\mathrm{finetune}}(X_k,k) \big) + \sqrt{\frac{1-\bar{\alpha}_{k+1}}{1-\bar{\alpha}_{k}} \big( 1 - \frac{\bar{\alpha}_k}{\bar{\alpha}_{k+1}} \big)} \varepsilon_k,
    \quad \varepsilon_k \sim \mathcal{N}(0,I), \ X_0 \sim \mathcal{N}(0,I),
    \end{split} \\
    \begin{split}
    \text{or } X_{k+1} &= X_{k} + \frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{2\bar{\alpha}_{k}} X_{k} - \frac{\bar{\alpha}_{k+1}-\bar{\alpha}_{k}}{\bar{\alpha}_{k}\sqrt{1-\bar{\alpha}_k}} \epsilon^{\mathrm{finetune}}(X_k,k) + \sqrt{\frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{\bar{\alpha}_{k}}} \varepsilon_k.
    \end{split} \label{eq:EM_update_box_DDIM_2}
    \end{talign}
    For each trajectory, solve the \textit{lean adjoint ODE} \eqref{eq:lean_adjoint_1}-\eqref{eq:lean_adjoint_2} backwards in time from $k=K$ to $0$, \eg:
    \begin{talign}
    \begin{split} \label{eq:Euler_lean_adjoint_DDIM}
        \tilde{a}_{k} 
        &= \tilde{a}_{k+1} + \tilde{a}_{k+1}\tran{} \nabla_{X_k} \left(
        \sqrt{\frac{\bar{\alpha}_{k+1}}{\bar{\alpha}_{k}}} \big( X_k - \frac{1-\bar{\alpha}_{k}/\bar{\alpha}_{k+1}}{\sqrt{1-\bar{\alpha}_k}} \epsilon^{\mathrm{base}}(X_k,k) \big) - X_k
        \right), \qquad 
        \tilde{a}_K = \nabla_{X_K} r(X_K),
    \end{split} \\
    \begin{split}
        \text{or } \tilde{a}_{k} 
        &= \tilde{a}_{k+1} + \tilde{a}_{k+1}\tran{} \nabla_{X_t} \left(
        % \frac{\dot{\alpha}_{t}}{2\alpha_{t}}
        \frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{2\bar{\alpha}_{k}}
        X_k - \frac{\bar{\alpha}_{k+1}-\bar{\alpha}_{k}}{\bar{\alpha}_{k}\sqrt{1-\bar{\alpha}_k}} \epsilon^{\mathrm{base}}(X_k,k)
        \right), \qquad 
        \tilde{a}_K = \nabla_{X_K} r(X_K).
    \end{split} \label{eq:Euler_lean_adjoint_DDIM_2}
    \end{talign}
    Note that $X_k$ and $\tilde{a}_k$ should be computed without gradients, \ie, $X_k = \texttt{stopgrad}(X_k)$, $\tilde{a}_k = \texttt{stopgrad}(\tilde{a}_k)$. \vspace{0.5em}

    For each trajectory, compute the Adjoint Matching objective \eqref{eq:lean_adjoint_matching}: 
    \begin{talign} \begin{split} \label{eq:adj_matching_algorithm_box_DDIM}
    \mathcal{L}_{\mathrm{Adj-Match}}(\theta) &=
        \sum_{k\in\{0, \dots, K - 1\}} \big\|
        % \frac{2}{\sigma(t)} \big(v^{\mathrm{finetune}}_{\theta}(X_t, t) - v^{\mathrm{base}}(X_t, t) \big)
        % \sqrt{\frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{\bar{\alpha}_k(1-\bar{\alpha}_k)}}
        \sqrt{\frac{\bar{\alpha}_{k+1}
        % (1-\bar{\alpha}_k)
        }{\bar{\alpha}_k(1-\bar{\alpha}_{k+1})} \big( 1 - \frac{\bar{\alpha}_k}{\bar{\alpha}_{k+1}} \big)}
        (\epsilon^{\mathrm{finetune}}(X_k,k) - \epsilon^{\mathrm{base}}(X_k,k))
        \\ &\qquad\qquad\qquad\qquad - 
        % \sqrt{\frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{\bar{\alpha}_{k}}}
        \sqrt{\frac{1-\bar{\alpha}_{k+1}}{1-\bar{\alpha}_{k}} \big( 1 - \frac{\bar{\alpha}_k}{\bar{\alpha}_{k+1}} \big)}
        \tilde{a}_k \big\|^2,
    \end{split} \\
    \begin{split} \label{eq:adj_matching_algorithm_box_DDIM_2}
    \text{or } \mathcal{L}_{\mathrm{Adj-Match}}(\theta) &=
        \sum_{k\in\{0, \dots, K - 1\}} \big\|
        % \frac{2}{\sigma(t)} \big(v^{\mathrm{finetune}}_{\theta}(X_t, t) - v^{\mathrm{base}}(X_t, t) \big)
        \sqrt{\frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{\bar{\alpha}_k(1-\bar{\alpha}_k)}} (\epsilon^{\mathrm{finetune}}(X_k,k) - \epsilon^{\mathrm{base}}(X_k,k))
        - \sqrt{\frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{\bar{\alpha}_{k}}} \tilde{a}_k \big\|^2.
    \end{split}
    \end{talign}
    
    Compute the gradient $\nabla_{\theta} \mathcal{L}(\theta)$ and update $\theta$ using favorite gradient descent algorithm.
  }
\KwOut{Fine-tuned vector field $v^{\mathrm{finetune}}$}}
\caption{Adjoint Matching for fine-tuning DDIM}
\label{alg:adjoint_matching_finetuning_DDIM}
\end{algorithm}

% \textcolor{red}{Finish}
% \begin{talign}\label{eq:euler_maruyama_DDPM_app}
%     \mathrm{d}X_t &= \big( \frac{\dot{\bar{\alpha}}_{t}}{2\bar{\alpha}_{t}} X_t - \frac{\dot{\bar{\alpha}}_{t}}{\bar{\alpha}_{t}} \frac{\epsilon^\text{base}(X_{t},t)}{\sqrt{1-\bar{\alpha}_{t}}} \big) \mathrm{d}t + \sqrt{\frac{\dot{\bar{\alpha}}_{t}}{\bar{\alpha}_{t}}} \mathrm{d}B_t, \qquad X_{0} \sim \mathcal{N}(0,I),
% \end{talign}
Note that for each pair of equations \eqref{eq:EM_update_box_DDIM}-\eqref{eq:EM_update_box_DDIM_2}, \eqref{eq:Euler_lean_adjoint_DDIM}-\eqref{eq:Euler_lean_adjoint_DDIM_2}, 
\eqref{eq:adj_matching_algorithm_box_DDIM}-\eqref{eq:adj_matching_algorithm_box_DDIM_2}, the first equation corresponds to the updates in the DDPM paper, while the second equation is an Euler-Maruyama / Euler discretization of the continuous-time object. 
To check that both discretizations are equal up to first order, remark that
\begin{talign}
    \sqrt{\frac{\bar{\alpha}_{k+1}}{\bar{\alpha}_{k}}} = \sqrt{1 + \frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{\bar{\alpha}_{k}}} \approx 1 + \frac{\bar{\alpha}_{k+1} - \bar{\alpha}_{k}}{2\bar{\alpha}_{k}} + O((\bar{\alpha}_{k+1} - \bar{\alpha}_{k})^2).
\end{talign}

\section{Adapting diffusion fine-tuning baselines to flow matching}

\subsection{Adapting ReFL \citep{xu2023imagereward} to flow matching}
Reward Feedback Learning (ReFL) is a diffusion fine-tuning algorithm introduced by \cite{xu2023imagereward} which tries to increase the reward on denoised samples. Namely, if $\bm{X} = (X_t)_{t \in [0,1]}$ is the solution of the DDPM SDE \eqref{eq:euler_maruyama_DDPM}, we can denoise $X_t$ as
\begin{talign}
    \hat{X}_1(X_t) = \frac{X_t - \sqrt{1-\bar{\alpha}_t} \epsilon(X_t,t)}{\sqrt{\bar{\alpha}_t}}.
\end{talign}
This equation follows from the stochastic interpolant equation \eqref{eq:reference_flow} if we replace $\bar{X}_0$ with the noise predictor $\epsilon(X_t,t)$. And then, the ReFL optimization update is based on the gradient:
\begin{talign}
    \nabla_{\theta} r(\hat{X}_1(X_t)) = \nabla_{\theta} r\big(\frac{X_t - \sqrt{1-\bar{\alpha}_t} \epsilon_{\theta}(X_t,t)}{\sqrt{\bar{\alpha}_t}} \big),
\end{talign}
where the trajectories have been detached. 

To adapt ReFL to Flow Matching, we need to express the denoiser map in terms of the vector field $v$. We have that
\begin{talign}
\begin{split}
    v(x,t) &= \mathbb{E} \big[\dot{\beta}_t \bar{X}_0 + \dot{\alpha}_t \bar{X}_1 \big| \beta_t \bar{X}_0 + \alpha_t \bar{X}_1 = x \big] = \mathbb{E} \big[ \frac{\dot{\beta}_t}{\beta_t} \big( \beta_t \bar{X}_0 + \alpha_t \bar{X}_1 \big) + \big( \dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t \big) \bar{X}_1 \big| \beta_t \bar{X}_0 + \alpha_t \bar{X}_1 = x \big] \\ &= \frac{\dot{\beta}_t}{\beta_t} x + \big( \dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t \big) \hat{X}_1(x,t).
\end{split}
\end{talign}
where we defined the denoiser map $\hat{X}_1(x,t) := \mathbb{E}\big[\bar{X}_1|\beta_t \bar{X}_0 + \alpha_t \bar{X}_1 = x\big]$.
Hence, 
\begin{talign} \label{eq:FM_denoiser}
\hat{X}_1(x,t) = \frac{v(x,t) - \frac{\dot{\beta}_t}{\beta_t} x}{\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t}.
\end{talign}
% \begin{talign}
% \end{talign}

\subsection{Adapting Diffusion-DPO \citep{wallace2023diffusion} to flow matching}
The Diffusion-DPO loss assumes access to ranked pairs of generated samples $x_1^w \succ x_1^l$, where $x^w$ and $x^l$ are the winning and losing samples. For DDPM, the loss implemented in practice reads \citep[Eq.~46]{wallace2023diffusion}:
\begin{talign}
\begin{split}
    L_{\mathrm{DPO}}(\theta) &= - \mathbb{E}_{(x_1^w, x_1^l) \sim \mathcal{D}, k \sim U[0,K], x_{kh}^w \sim q(x_{kh}^w|x_1^w), x_t^l \sim q(x_{kh}^l|x_1^l)} \big[ \\ &\qquad \log S \big( - \frac{\tilde{\beta}}{2} 
    % K \omega(\lambda_t)
    \big( \|\varepsilon^w - \epsilon_{\theta}(x_{kh}^w,kh) \|^2 - \|\varepsilon^w - \epsilon_{\mathrm{ref}}(x_{kh}^w,kh) \|^2 \\ &\qquad\qquad\qquad\quad - \big( \|\varepsilon^l - \epsilon_{\theta}(x_{kh}^l,kh) \|^2 - \|\varepsilon^l - \epsilon_{\mathrm{ref}}(x_{kh}^l,kh) \|^2 \big) \big) \big) \big],
\end{split}
\end{talign}
where $S(x) = \frac{1}{1+e^{-x}}$ denotes the sigmoid function, and $q(x_{kh}^{*}|x_1^{*})$ is the conditional distribution of the forward process, i.e. $x_{kh}^{*}$ is sampled as $x_{kh}^* = \sqrt{\gamma_{kh}} x_1^{*} + \sqrt{1-\gamma_{kh}} \epsilon$, $\epsilon \sim N(0,I)$. Following the derivation of the Diffusion-DPO loss in \cite[Sec.~S4]{wallace2023diffusion}, we observe that the term $- \frac{\tilde{\beta}}{2} \|\varepsilon^w - \epsilon_{\theta}(x_{kh}^w,kh) \|^2$ arises from 
\begin{talign}
    -\frac{\tilde{\beta}}{2\frac{1-\gamma_{kh}}{\gamma_{kh}}} \|\hat{x}_1(x_{kh}^w) - x_1^w\|^2, 
\end{talign}
up to a constant term in $\theta$. If we switch to the more general flow matching scheme, the analog of this term is
\begin{talign} \label{eq:denoiser_FM_1}
    -\frac{\tilde{\beta}}{2\frac{\beta^2_{kh}}{\alpha^2_{kh}}} \|\hat{x}_1(x_{kh}^w) - x_1^w\|^2.
\end{talign}
Using the expression of the denoiser map in terms of the vector field $v$ in equation \eqref{eq:FM_denoiser}, we can rewrite \eqref{eq:denoiser_FM_1} as:
\begin{talign} \label{eq:denoiser_FM_2}
    -\frac{\tilde{\beta}}{2\frac{\beta^2_{kh}}{\alpha^2_{kh}}} \big\|\frac{v(x^w_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^w_{kh}}{\dot{\alpha}_{kh} - \frac{\dot{\beta}_{kh}}{\beta_{kh}} \alpha_{kh}} - x_1^w \big\|^2 = -\frac{\tilde{\beta}}{2
    % \frac{\beta^2_{kh}}{\alpha^2_{kh}}
    } \big\|\frac{v(x^w_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^w_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^w \big\|^2.
\end{talign}
Thus, the Diffusion-DPO loss for Flow Matching reads
\begin{talign}
\begin{split} \label{eq:diff_dpo_2}
    L_{\mathrm{DPO}}(\theta) &= - \mathbb{E}_{(x_1^w, x_1^l) \sim \mathcal{D}, k \sim U[0,K], x_{kh}^w \sim q(x_{kh}^w|x_1^w), x_t^l \sim q(x_{kh}^l|x_1^l)} \big[ \\ &\qquad \log S \big( - \frac{\tilde{\beta}}{2} 
    \big( \big\|\frac{v_{\theta}(x^w_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^w_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^w \big\|^2 - \big\|\frac{v_{\mathrm{ref}}(x^w_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^w_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^w \big\|^2 \\ &\qquad\qquad\qquad\quad - \big( \big\|\frac{v_{\theta}(x^l_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^l_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^l \big\|^2 - \big\|\frac{v_{\mathrm{ref}}(x^l_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^l_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^l \big\|^2 \big) \big) \big) \big],
\end{split}
\end{talign}
\cite[Sec.~5.1]{wallace2023diffusion} claim that $\beta \in [2000,5000]$ yields good performance on Stable Diffusion 1.5 and Stable Diffusion XL-1.0, which if we translate to our notation corresponds to $\tilde{\beta} \in [4000,10000]$.

When we have access to the reward function $r$, instead of a winning sample $x^w_1$ and a losing sample $x^l_1$, we have a pair of samples $(x^a_1,x^b_1)$ with winning weights $S(r(x^a_1) - r(x^b_1)) = \frac{1}{1+\exp \big(r(x^b_1) - r(x^a_1) \big)}$, $S(-(r(x^a_1) - r(x^b_1))) = \frac{1}{1+\exp \big(-(r(x^b_1) - r(x^a_1)) \big)}$. Hence, the loss \eqref{eq:diff_dpo_2} becomes:
\begin{talign}
\begin{split} \label{eq:diff_dpo_3}
    L_{\mathrm{DPO}}(\theta) &= - \mathbb{E}_{(x_1^a, x_1^b) \sim \mathcal{D}, k \sim U[0,K], x_{kh}^a \sim q(x_{kh}^a|x_1^a), x_t^b \sim q(x_{kh}^b|x_1^b)} \bigg[ \sum_{s \in \{ \pm 1\}} S\big( s (r(x^a_1) - r(x^b_1)) \big) \times \\ &\qquad \log S \big( - \frac{s\tilde{\beta}}{2} 
    \big( \big\|\frac{v_{\theta}(x^a_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^a_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^a \big\|^2 - \big\|\frac{v_{\mathrm{ref}}(x^a_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^a_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^a \big\|^2 \\ &\qquad\qquad\qquad\quad - \big( \big\|\frac{v_{\theta}(x^b_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^b_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^b \big\|^2 - \big\|\frac{v_{\mathrm{ref}}(x^b_{kh},kh) - \frac{\dot{\beta}_{kh}}{\beta_{kh}} x^b_{kh}}{\frac{\dot{\alpha}_{kh}}{\alpha_{kh}} \beta_{kh} - \dot{\beta}_{kh}} - \frac{\alpha_{kh}}{\beta_{kh}} x_1^b \big\|^2 \big) \big) \big) \bigg].
\end{split}
\end{talign}
We want to emphasize that despite the similarities, even though the loss $L_{\mathrm{DPO}}$ that we use (equation \eqref{eq:diff_dpo_3}) is very similar to the one implemented by \cite{wallace2023diffusion}, the preference data pairs that we use are very different from theirs. We sample the preference data from the current model, which results in imperfect samples, while they consider off-policy, high-quality, curated preference samples. The reason for this discrepancy is that the starting point of our work is a reward model, not a set of preference data, and we only benchmark against approaches that leverage reward models for an apples-to-apples comparison. Our experimental results on DPO (\Cref{tab:evaluation_metrics}, \Cref{fig:training_figures}, \Cref{table:metrics_multiprompt_diversity}) show that the resulting model performs like the base model, or a bit worse according to some metrics. Hence, we conclude that DPO is not a competitive alternative for on-policy fine-tune when the base model is not already good.
% The Euler-Maruyama discretization of the SDE
% \eqref{eq:memoryless_FM_sde} is:
% \begin{talign}
%     x_{(k+1)h} \sim N\big( x_{kh} - \frac{\alpha_{(k+1)h} - \alpha_{kh}}{\alpha_{kh}} x_{k} + 2 h v(x_{kh},kh), 2 \beta_{kh} \big( \frac{\alpha_{(k+1)h} -\alpha_{kh}}{\alpha_{kh}} \beta_{kh} - (\beta_{(k+1)h}-\beta_{kh}) \big) \big)
% \end{talign}

\section{Experimental details}
\label{sec:experimental_details}

Unless otherwise specified, we used the same hyperparameters across all fine-tuning methods. Namely, we used:
\begin{itemize}
    \item $K=40$ timesteps.
    \item Adam optimizer with learning rate $\num{2e-5}$ and parameters $\beta_1 = 0.95$, $\beta_2 = 0.999$, $\epsilon=\num{1e-8}$, weight decay $\num{1e-2}$, gradient norm clipping value $1$. For Discrete Adjoint, these hyperparameters resulted in fine-tuning instability (see \Cref{table:alternative_hyperparameters}); the results that we report in all other tables for Discrete Adjoint were obtained with learning rate $\num{1e-5}$.
    \item Bfloat16 precision.
    \item Effective batch size 40; for each run we used two 80GB A100 GPUs with batch size 20 each.
    \item A set of 40k fine-tuning prompts taken from a licensed dataset consisting of text and image pairs (note that we disregarded the images). Thus, each epoch lasts 1000 iterations; see the total amount of fine-tuning iterations for each algorithm in \Cref{table:metrics_multiprompt_diversity}. For each of the three runs that we perform for each data point that we report, the set of 40k prompts is sampled independently among a total set of 100k prompts.
\end{itemize}

\subsection{Noise schedule details} \label{subsec:noise_schedule}
Since we use $K=40$ discretization steps, the timesteps are $t \in \{0, 0.025, 0.05, 0.075, 0.1, \dots, 0.95, 0.975\}$. To sample $X_{t+h}$ from $X_t$ we use equation \eqref{eq:EM_update_box}. We use the choices $\alpha_t = t$, $\beta_t = 1-t$, which means that $\sigma(t) = \sqrt{2 \beta_t (\frac{\dot{\alpha}_t}{\alpha_t} \beta_t - \dot{\beta}_t)} = \sqrt{2 (1-t) (\frac{1-t}{t} + 1)} = \sqrt{\frac{2 (1-t)}{t}}$. 

Note that if we plug $t=0$ into this expression, we obtain infinity, and if we plug $t \lessapprox 1$, we obtain $\sigma(t) \approx 0$. For obvious reasons, the former issue requires a fix: we simply add a small offset to the denominator of $\sigma(t)$, replacing $\sqrt{1/t}$ by $\sqrt{1/(t+h)}$ (note that $h:= 1/K = 0.025$). But the latter issue is also not completely satisfactory from a practical standpoint, because looking at the adjoint matching loss \eqref{eq:lean_adjoint_matching}, we observe that $u(X^{\bar{u}}_t,t)$ is trained to approximate the conditional expectation of  $\sigma(t)\tran{} \tilde{a}(t;\bm{X}^{\bar{u}})$. Thus, if we set $\sigma(t)$ very close to zero for $t \lessapprox 1$, we are forcing the control $u$ to be close to zero as well, or equivalently preventing $v^{\mathrm{finetune}}$ from deviating from $v^{\mathrm{base}}$. While this is the right thing to do from a theoretical perspective, we concluded experimentally that setting $\sigma(t)$ just slightly larger results in substantially faster fine-tuning, thanks to the additional leeway provided to $v^{\mathrm{finetune}}$ to deviate from $v^{\mathrm{base}}$. In particular, we added a small offset to the factor $1-t$ in the numerator $1-t$ of $\sigma(t)$: we replaced $1-t$ by $1-t+h$. Thus, the expression that we used to compute the diffusion coefficient in our experiments is
\begin{talign} \label{eq:sigma_empirical}
    \sigma(t) = \sqrt{\frac{2 (1-t + h)}{t + h}}.
\end{talign}
When solving the lean adjoint ODE \eqref{eq:lean_adjoint_1}-\eqref{eq:lean_adjoint_2} backwards in time via the Euler scheme \eqref{eq:Euler_lean_adjoint}, the timesteps we use are $t\in \{1, 0.975, 0.95, 0.925, 0.9, \dots, 0.05, 0.025\}$. We do not actually initialize the adjoint state as $\nabla_x g(X_1)$, but rather as $\nabla_x g(\hat{X}_1)$, where $\hat{X}_1 := X_{1-h} + h v^{\mathrm{base}}(X_{1-h},1-h)$. That is, $\hat{X}_1$ is obtained by performing a final noiseless update, instead of using noise $\sigma(1-h) = \sqrt{4h}$ given by equation \eqref{eq:sigma_empirical}. The reason for this is that the regular final iterate $X_1$ contains some noise that was added in the final step, and that can distort the gradient $\nabla_x g(X_1)$. By setting $\tilde{a}(1;\bm{X}) = \nabla_x g(X_1)$, we get rid of this bias. Note that in the continuous time limit $h \to 0$, $\hat{X}_1 = X_1$, which means that this small trick is consistent.

\subsection{Selection of gradient evaluation timesteps} \label{subsec:timestep_selection}
In \Cref{alg:adjoint_matching_finetuning_FM}, equation \eqref{eq:adj_matching_algorithm_box}, we state that the term $\big\|\frac{2}{\sigma(t)} \big(v^{\mathrm{finetune}}_{\theta}(X_t, t) - v^{\mathrm{base}}(X_t, t) \big) + \sigma(t) \tilde{a}_t \big\|^2$ must be computed for all $K$ steps in $\{0, \dots, 1-h\}$. However, the gradient signal provided by backpropagating through this expression for consecutive times $t$ and $t+h$ is quite similar. In the interest of computational efficiency, we sample a subset $\mathcal{K}$ of timesteps, and we only compute and backpropagate the terms $\big\|\frac{2}{\sigma(t)} \big(v^{\mathrm{finetune}}_{\theta}(X_t, t) - v^{\mathrm{base}}(X_t, t) \big) + \sigma(t) \tilde{a}_t \big\|^2$ for those timesteps. We construct $\mathcal{K}$ by sampling ten timesteps uniformly without repetition among $\{0, \dots, 0.725\}$, and always sampling the last ten timesteps $\{0.75, \dots, 0.975\}$. This is because fine-tuning the last ten steps (25\% of the total) well is critical for good empirical performance, while the initial steps are not as important.

\subsection{Loss function clipping: the $\mathrm{LCT}$ hyperparameter}
Note that the magnitude of $\sigma(t)\tran{} a(t;\bm{X}^{\bar{u}},\bar{u})$ is much larger for times $t \gtrapprox 0$ than for times $t \lessapprox 1$. The reason is two-fold: 
\begin{itemize}
    \item As discussed in \Cref{subsec:noise_schedule}, $\sigma(t)$ is much larger for $t \gtrapprox 0$ than for $t \lessapprox 1$.
    \item The magnitude of the lean adjoint state $\tilde{a}$ grows roughly exponentially as $t$ goes backward in time. In fact, if we assumed that $\nabla_x b(X_t,t)$ is constant in time, this statement would be exact.
\end{itemize}
Observe that when $\sigma(t)\tran{} a(t;\bm{X}^{\bar{u}},\bar{u})$ is large, the gradient $\nabla_{\theta} \big\|\frac{2}{\sigma(t)} \big(v^{\mathrm{finetune}}_{\theta}(X_t, t) - v^{\mathrm{base}}(X_t, t) \big) + \sigma(t) \tilde{a}_t \big\|^2$ also has a high magnitude. Including such terms in our gradient computation decreases the signal to noise ratio of the gradient. Even more so, as discussed in \Cref{subsec:timestep_selection} for good practical performance it is critical to get a good gradient signal from the last 25\% steps. Hence, including the high-magnitude terms for $t \lessapprox 0$ in our gradients can muffle these other important, low-magnitude terms.

To fix this issue, we clip the terms such that $\big\|\frac{2}{\sigma(t)} \big(v^{\mathrm{finetune}}_{\theta}(X_t, t) - v^{\mathrm{base}}(X_t, t) \big) + \sigma(t) \tilde{a}_t \big\|^2 > \mathrm{LCT}$, where $\mathrm{LCT}$ stands for the loss clipping threshold. That is, the adjoint matching loss that we use in our experiments is of the form:
\begin{talign}
    \hat{\mathcal{L}}_{\mathrm{Adj-Match}}(\theta) =
        \sum_{t\in \mathcal{K}} \min\big\{\mathrm{LCT}, \big\|\frac{2}{\sigma(t)} \big(v^{\mathrm{finetune}}_{\theta}(X_t, t) - v^{\mathrm{base}}(X_t, t) \big) + \sigma(t) \tilde{a}_t \big\|^2 \big\},
\end{talign}
where $\mathcal{K}$ is the random timestep subset described in \Cref{subsec:timestep_selection}. 

For adjoint matching, we set $\mathrm{LCT} = 1.6 \times \lambda^2$. Remark that $\mathrm{LCT}$ needs to grow quadratically with $\lambda$, because the magnitude of the lean adjoint $\tilde{a}$ grows quadratically with $\lambda$. We set the constant 1.6 through experimentation; all or almost all of the terms for the last ten timesteps fall below $\mathrm{LCT}$, but only a fraction of the terms ($\approx 25 \%$) for the first ten steps fall below $\mathrm{LCT}$. The constant for $\mathrm{LCT}$ is a relevant hyperparameter that needs to be tuned to obtain a similar behavior.

We also used loss function clipping on the continuous adjoint loss. For that loss we set $\mathrm{LCT} = 1600 \times \lambda^2$. The reason is that the magnitude of the regular adjoint states is significantly larger than the magnitude of the lean adjoint states (which is a big reason why adjoint matching outperforms the continuous adjoint).

\subsection{Computation of evaluation metrics} \label{subsec:evaluation_metrics}
We used the \url{open_clip} library \citep{ilharco2021openclip} to compute ClipScores. We computed ClipScore diversity as the variance of Clip embeddings of 40 generations for a given prompt, averaged across 25 prompts. Namely,
\begin{talign}
    \mathrm{ClipScore\_Diversity} = \frac{1}{40} \sum_{k=1}^{40} \frac{2}{25 \cdot 24} \sum_{1 \leq i < j \leq 25} \|\mathrm{Clip}(g^k_i) - \mathrm{Clip}(g^k_j)\|^2, 
\end{talign}
where $g^k_i$ denotes the $i$-th generation for the $k$-th prompt.

We used the \url{transformers} library to compute the PickScore processor and model \citep{kirstain2023pickapic}. PickScore diversity is computed in analogy with ClipScore diversity.

We used the \url{hps} library to compute values of Human Preference Score v2 \citep{wu2023human}.

To compute Dreamsim diversity we use the \url{dreamsim} library \citep{fu2023learning}. Dreamsim diversity is computed in analogy with ClipScore diversity.

\subsection{Remarks on computational costs} \label{subsec:remarks_computational_cost}
Observe from the figures reported in \Cref{table:metrics_multiprompt_diversity} that the per iteration wall-clock time of Adjoint Matching (156 seconds) is very similar to that of the Discrete Adjoint loss (152 seconds). The reason is that both algorithms perform a similar amount of forward and backward passes on the flow matching model and the reward model. Namely, for each sample in the batch, both algorithms perform $K$ forward passes on the flow model to obtain the trajectories. In order to compute the gradient of the training loss, the Discrete Adjoint loss does $K$ additional forward passes to evaluate the base flow model, one forward and backward pass on the reward model, and $K$ backward passes on the current flow model, which typically use gradient checkpointing to avoid memory overflow. In the case of Adjoint Matching, solving the lean adjoint ODE requires one forward and backward pass on the reward model, and $K$ backward passes on the base flow model. Finally, computing the gradient of the loss takes $K/2$ additional backward passes if we evaluate at only half of the timesteps as we do, although this computation is much quicker because it can be fully parallelized.

Meanwhile, computing the gradient of the Continuous Adjoint loss takes 204 seconds per iteration. With respect to Adjoint Matching, Continuous Adjoint performs additional backward passes to compute the gradients $\nabla_{X_t} \|u(X_t,t)\|^2$ when solving the adjoint ODE. Finally, we observe that models that directly fine-tune the reward are quicker, but that comes with its own set of issues that we discuss throughout the paper.

\subsection{Remarks on number of sampling timesteps} \label{subsec:remarks_sampling_timesteps}

In our experiments and all baselines, we used 40 timesteps in the fine-tuning procedure ($h=1/40$ in \Cref{alg:adjoint_matching_finetuning_FM}). The experiments reported in all tables and figures except for \Cref{table:metrics_multiprompt_sampling_steps} were performed at 40 inference timesteps. In \Cref{table:metrics_multiprompt_sampling_steps} (\Cref{sec:additional_figures_tables}), we show experimental results at 10, 20, 40, 100, and 200 inference timesteps, for the base model and the models fine-tuned with adjoint matching and DRaFT-1. We make the following observations about the results: 
\begin{itemize}
\item The metrics for Adjoint Matching at 100 and 200 timesteps are statistically equal to the ones for 40 timesteps, with slight increases in Dreamsim diversity. This suggests that fine-tuning at large numbers of timesteps is a good idea if we want to perform inference at a large number of timesteps, as otherwise the capabilities of the model are limited by the number of fine-tuning timesteps instead of the inference compute. Also, at 100 and 200 timesteps the difference in performance of Adjoint Matching relative to DRaFT-1 increases.
\item The metrics for Adjoint Matching at 10 and 20 timesteps are worse than at 40 timesteps, especially for 10. The difference in performance between Adjoint Matching and DRaFT-1 vanishes at 10 timesteps for all metrics except for diversity, for which Adjoint Matching is still clearly better.
\end{itemize}

