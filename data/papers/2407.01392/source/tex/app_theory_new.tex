
\section{Theoretical Justification}
\label{appendix:theory}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\unifsim}{\overset{\mathrm{unif}}{\sim}}
\newcommand{\Eforward}{\Exp_{\mathrm{forward}}}
\newcommand{\EforwardD}{\Exp_{\mathrm{forward},\cD}}

\newcommand{\Epz}{\Exp_{p,\mathbf{z}_{1:T}}}
\newcommand{\I}{\mathbf{1}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\Dkl}{\mathrm{D}_{\mathbb{KL}}}

\newcommand{\veck}{\mathbf{k}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\ptheta}{p_{\bm{\theta}}}

In this section, we provide theoretical justification for the train of \algo. The main contributions can be summarized as follows:
\begin{itemize}
    \item We show that our training methods optimize a reweighting of the Evidence Lower Bound (ELBO) on the average log-likelihood of our data. We first establish this in full generality (\Cref{thm:main_elbo}), and then specialize to the form of Gaussian diffusion (\Cref{cor:elbo}). We show that the resulting terms decouple in such a fashion that, in the limit of a fully expressive latent and model, makes the reweighting terms immaterial. 
    \item We show that the expected likelihood over \emph{any} distribution over sequences of noise levels can be lower bounded by a sum over nonnegative terms which, when reweighted, correspond to the terms optimized in the \algo{} training objective maximizes.  Thus, for a fully expressive network that can drive all terms to their minimal value, \algo{} optimizes a valid surrogate of the likelihood of \emph{all sequences of noise levels simultaneously.}
\end{itemize}

We begin by stating an ELBO for general Markov forward processes $q(\cdot)$, and generative models $\ptheta(\cdot)$, and then specialize to Gaussian diffusion, thereby recovering our loss.  We denote our Markov forward process $q(\cdot)$ as 
\begin{align}
q(\bx^{1:K} \mid \bx^0) = \prod_{k=1}^K q(\bx^k \mid \bx^{k-1}), \label{eq:qforward}
\end{align}
and a parameterized probability model 
\begin{align}
\ptheta(((\bx^{k}_t)_{1 \le k \le K},\bz_t)_{t \ge 1})
\end{align}
We assume that $\ptheta$ satisfies the \emph{Markov property} that 
\begin{align}
&\ptheta(\bz_t, \bx_t^{k_t}\mid \bz_{1:t-1},(\bx_{s}^{k_s})_{1 \le s < t}) = \ptheta(\bz_t, \bx^{k_t}\mid \bz_{t-1})
\end{align} 
that is, the latent codes $\bz_{t-1}$ is a sufficient statistic for $\bx^{k_t}$ given the history. We say that $\ptheta$ has \emph{deterministic latents} if $\ptheta(\bz_t\mid \bz_{1:t-1},(\bx_{s}^{k_s})_{1 \le s < t},\bx_t^{k_t})$ is a Dirac delta. 
\begin{remark} In order for $\ptheta$ to have deterministic latents and correspond to a valid probability distribution, we need to view the latents $\bz_t$ not as individual variables, but as a collection of variables $\bz_t(k_{1:t})$  indexed by $t \in [T]$ and the \emph{history} of noise levels $k_{1:t} \in \{0,1,\dots,K\}^t$. In this case, simply setting $\bz_t(k_{1:t}) = (k_{1:t},(\bx_{s}^{k_s})_{1 \le s \le t}$ tautologically produces deterministic latents. The reason for indexing $\bz_t(k_{1:t})$ with $k_{1:t}$ then arises because, otherwise, $\ptheta(\bz_t \mid ((\bx_{s}^{k_s})_{1 \le s \le t},(\bx_{s}^{k_s'})_{1 \le s \le t})$ would be ill-defined unless $k_s = k_s'$ for all $1 \le s \le t$, and thus, $\ptheta$ would not correspond to a joint probability measure. The exposition and theorem that follows allow $\bz_t(k_{1:t})$ to be indexed on past noise levels $k_{1:t}$ but suppresses dependence on $k_{1:t}$ to avoid notational confusion.
\end{remark}




\subsection{Main Results}
We can now state our main theorem, which provides an evidence lower bound (ELBO) on the expected log-likelihood of partially-noised sequences $(\bx_{t}^{k_t})_{1 \le t \le T}$, under uniformly sampled levels $k_t$ and $\bx_t^{k_t}$ obtained by noising according to $q(\cdot)$ as in \eqref{eq:qforward}. Notice that this formulation does not require an explicit for of $q(\cdot)$ or $\ptheta$, but we will specialize to Gaussian diffusion in the following section. 
\newcommand{\mutheta}{\mu_{\bm{\theta}}}
\begin{theorem}\label{thm:main_elbo}  Fix $\bx_{1:T}^{0}$. Define the expectation over the forward process with random noise level $k_{1:T}$ as 
\begin{align}
\Eforward[\cdot] := \Exp_{k_{1},\dots,k_T \unifsim [K]}\Exp_{\bx_{s}^{k_s}\sim q(\bx_{s}^{k_s} \mid \bx_s^0),1 \le s \le T}[\cdot],
\end{align}
and the expectation over the latents under $\ptheta(\cdot)$ conditioned on $k_{1:T},(\bx_{s}^{k_t})_{1 \le t \le T}$ as 
\begin{align}
\Epz[\cdot] := \Exp_{\bz_{s} \sim p (\bz_{s} \mid \bz_{s-1},\bx_s^{k_s}), s\le T}\left[\cdot \mid k_{1:T}, (\bx_t^{k_t})_{1 \le t \le T}\right]
\end{align}
 Then, as long as $\ptheta$ satisfies the Markov property, 
\begin{align*}
&\Eforward[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})] \ge C(\bx_{1:T}^0)  \\
&+ \Eforward\Epz \left[\sum_{t=1}^T \left(\frac{1}{K+1}\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=2}^{K}\frac{j}{K+1}\Dkl\left(q(\bx_{t}^{j-1} \mid \bx_t^{j},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j-1}, \bz_{t-1})\right)\right)\right], 
\end{align*}
 where $C(\bx_{1:T}^0)$ is a constant depending only on $\bx_{1:T}^0$ (the unnoised data). Moreover, if the latents are deterministic (i.e. $\ptheta(\bz_t \mid \bz_{t-1},\bx_t^{k_t})$ is a Dirac distribution),  then the inequality holds with inequality if and only if $q(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t}) \equiv \ptheta(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t},\bz_{t-1})$, i.e. the variational approximation is exact. 
\end{theorem}
The proof of the above theorem is given in \Cref{sec:proof:thm:main_elbo}. Remarkably, it involves \emph{only two} inequalities! The first holds with equality under deterministic latents and the second holds if and only if variational approximation is exact: $q(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t}) \equiv \ptheta(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t},\bz_{t-1})$. This tightness of the ELBO suggests that the expression in \Cref{thm:main_elbo} is a relatively strong surrogate objective for optimizing the likelihoods. 

\subsubsection{Specializing to Gaussian diffusion}
We now special \Cref{thm:main_elbo} to Gaussian diffusion. For now, we focus on the ``$\bx$-prediction'' formulation of diffusion, which is the one used in our implementation. The ``$\beps$-prediction'' formalism, used throughout the main body of the text, can be derived similarly (see Section 2 of \cite{chan2024tutorial} for a clean exposition). The following theorem follows directly by apply standard likelihood and KL-divergence computations for the DDPM \cite{ho2020denoising,chan2024tutorial} to \Cref{thm:main_elbo}.  
\newcommand{\xthet}{\hat{\bx}_{\bm\theta}}
\begin{corollary}\label{cor:elbo} Let 
\begin{align}
q(\bx^{k+1} \mid \bx_t^k) = \mathcal{N}(\bx^k; \sqrt{1-\beta_k}\bx^{k-1}, \beta_k \mathbf{I}),
\end{align}
and define $\alpha_k = (1-\beta_k)$, $\bar{\alpha}_k = \prod_{j=1}^k \alpha_j$.  Suppose that we parameterize $\ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1},\bz_{t-1}) = \cN(\mutheta(\bx_{t}^{j+1},\bz_{t-1},j),\sigma_j^2)$, where further, 
\begin{align*}
\mutheta(\bx_{t}^{j},\bz_{t-1},j) = \frac{(1 - \bar{\alpha}_{j-1})\sqrt{{\alpha}_j}}{1-\bar{\alpha}_j} \bx_t^j +  \frac{(1 - {\alpha}_{j})\sqrt{\bar{\alpha}_{j-1}}}{1-\bar{\alpha}_j}\xthet(\bx_{t}^{j},\bz_{t-1},j), \quad \sigma_j^2 := \frac{(1 - \alpha_{j})(1-\sqrt{\bar{\alpha}_{j-1}})}{1-\bar{\alpha}_j}.
\end{align*}
 Then, as long as $\ptheta$ satisfies the Markov property, we obtained
\begin{align*}
\Eforward[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})] + C(\bx_{1:T}^0)  &\ge \Eforward\Epz \left[\sum_{t=1}^T  \frac{j}{K+1}\sum_{j=1}^{K}c_j \| \hat{\bx}^0_{\bm{\theta}}(\bx_{t}^{j},\bz_{t-1},j) - \bx_t^0\|^2 \right]\\
&= \Eforward\Epz \left[\sum_{t=1}^T  \I\{k_t \ge 1\}\cdot k_tc_{k_t} \| \hat{\bx}^0_{\bm{\theta}}(\bx_{t}^{k_t},\bz_{t-1},{k_t}) - \bx_t^0\|^2 \right],
\end{align*}
where above, we define $c_j = \frac{(1 - \alpha_{j})^2\bar{\alpha}_{j-1}}{2\sigma^2(1 - \bar\alpha_{j})^2}$.
\end{corollary}
\begin{proof} The first inequality follows from the standard computations for the ``$\bx$-prediction'' formulation of Diffusion (see Section 2.7 of  \cite{chan2024tutorial} and references therein). The second follows by replacing the sum over $j$ with an expectation over $k_t \unifsim \{0,1,\dots,K\}$. 
\end{proof}
We make a couple of remarks:
\begin{itemize}
    \item As noted above, \Cref{cor:elbo} can also be stated for $\beps$-prediction, or the so-called ``$\mathbf{v}$-prediction'' formalism, as all are affinely related. 
    \item Define an idealized latent $\tilde{\bz}_{t-1}$ consisting of all past tokens $(\bx_t^{k_t})$ as well as of their noise levels $k_t$. This is a sufficient statistic for $\bz_{t-1}$, and thus we can always view $\hat{\bx}^0_{\bm{\theta}}(\bx_{t}^{k_t},\bz_{t-1},{k_t}) = \hat{\bx}^0_{\bm{\theta}}(\bx_{t}^{k_t},\bar\bz_{t-1},{k_t}) $, where $\bz_{t-1}$ is just compressing $\bar \bz_{t-1}$.  When applying the expectation of $\bx_{1:T} \sim q$ to both sides of the bound in \Cref{cor:elbo}, and taking an infimum over possible function approximator $\hat{\bx}^0_{\theta}$, we obtain
    \begin{align*}
        \inf_{p_\theta} \Exp_{q}\Eforward \Epz \| \hat{\bx}^0_{\bm{\theta}}(\bx_{t}^{k_t},\bz_{t-1},{k_t}) - \bx_t^0\|^2 &= \inf_{p_\theta} \Exp_{q}\Eforward \Epz \| \hat{\bx}^0_{\bm{\theta}}(\bx_{t}^{k_t},\bar{\bz}_{t-1}) - \bx_t^0\|^2 \\
        &= \mathbf{Var}_{q}[\bx_t^0 \mid (\bx_s^{k_s})_{1 \le s \le t}, k_1,\dots,k_t ]. 
    \end{align*}
    This leads to a striking finding: with expressive enough latents and $p_{\theta}$, we can view the maximization of each term in \Cref{cor:elbo} separately across time steps. The absence of this coupling means that the weighting terms are immaterial to the optimization, and thus can be ignored. 
    \item Given the above remarks, we can optimize the ELBO by taking gradients through the objective specified by \Cref{cor:elbo}, and are free to drop any weighting terms (or rescale them) as desired. Backpropagation through $\Epz$ is straightforward due to deterministic latents. This justifies the correctness of our training objective  \eqref{eq:train} and protocol \Cref{alg:diffusion_forcing_training}.
\end{itemize}



\subsubsection{Capturing all subsequences}
\Cref{thm:main_elbo} stipulates that, up to reweighting, the \algo{} objective optimizes a valid ELBO on the expected log-likelihoods over uniformly sampled noise levels.  The following theorem can be obtained by a straightforward modification of the proof of \Cref{thm:main_elbo} generalizes this to arbitrary (possibly temporally correlated) sequences of noise. 




\begin{theorem}\label{thm:main_elbo_general} Let $\cD$ be an arbitrary distribution over $[K]^T$, and define $P_t(j \mid k_{1:t-1}) := \Pr_{\cD}[k_t = j \mid k_{1:t-1}]$.
Fix $\bx_{1:T}^{0}$. Define the expectation over the forward process with random noise level $k_{1:T}$ as 
\begin{align}
\EforwardD[\cdot] := \Exp_{k_{1},\dots,k_T \sim \cD}\Exp_{\bx_{s}^{k_s}\sim q(\bx_{s}^{k_s} \mid \bx_s^0),1 \le s \le T}[\cdot],
\end{align}
and the expectation over the latent under $\ptheta(\cdot)$ conditioned on $k_{1:T},(\bx_{s}^{k_t})_{1 \le t \le T}$ as 
\begin{align}
\Epz[\cdot] := \Exp_{\bz_{s} \sim p (\bz_{s} \mid \bz_{s-1},\bx_s^{k_s}), s\le T}\left[\cdot \mid k_{1:T}, (\bx_t^{k_t})_{1 \le t \le T}\right]
\end{align}
 Then, as long as $\ptheta$ satisfies the Markov property, 
\begin{align*}
&\EforwardD[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})] \ge C(\bx_{1:T}^0) +  \EforwardD\Epz \left[\sum_{t=1}^T \Xi_t\right], \text{where } \\
&\Xi_t :=  \left(P_t(1 \mid k_{1:t-1})\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=2}^{K}j P_t(j \mid k_{1:t-1}) \Dkl\left(q(\bx_{t}^{j-1} \mid \bx_t^{j},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j-1}, \bz_{t-1})\right)\right), 
\end{align*}
 where $C(\bx_{1:T}^0)$ is a constant depending only on $\bx_{1:T}^0$ (the noise-free data), and where the inequality is an \emph{equality} under the conditions that (a) $\ptheta(\bz_t \mid \bz_{t-1},\bx_t^{k_t})$ is a Dirac distribution (deterministic latents), and (b) $q(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t}) \equiv \ptheta(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t},\bz_{t-1})$, i.e. the variational approximation is sharp. 

 In particular, in the Gaussian case of \Cref{cor:elbo}, we have
     \begin{align*}
\EforwardD[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})] + C(\bx_{1:T}^0)  
&\ge \EforwardD\Epz \left[\sum_{t=1}^T  \I\{k_t \ge 1\} k_t c_{k_t} \| \hat{\bx}^0_{\bm{\theta}}(\bx_{t}^{k_t},\bz_{t-1},{k_t}) - \bx_t^0\|^2 \right],
\end{align*}

\end{theorem}
The most salient case for us is the restriction of $\cD$ to fixed sequences of noise $k_1,\dots,k_T$ (i.e. Dirac distributions on $[K]^T$). In this case, $P_t(j \mid k_{1:t-1}) = 0$ for all but $j = k_t$, and thus our training objective need not be a lower bound on $\EforwardD[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})]$. However, the terms in the lower bound are, up to reweighting, an \emph{subset} of those terms optimized in the training objective. Thus, in light of the remarks following \Cref{cor:elbo}, a fully expressive network can optimize all the terms in the loss simultaneously. We conclude that, for a fully expressive neural network, optimizing the training objective \eqref{eq:train} is a valid surrogate for maximizing the likelihood of all possible noise sequences. 


\subsection{Proof of \Cref{thm:main_elbo}}\label{sec:proof:thm:main_elbo}

Define $\Exp_{< t}[\cdot]$ as shorthand for $\Exp_{k_{1:s}\unifsim [K]}\Exp_{\bx_{s}^{k_s}\sim q(\bx_{s}^{k_s} \mid \bx_s^0),1 \le s \le t-1}\Exp_{\bz_{s} \sim p (\bz_{s} \mid \bz_{s-1},\bx_s^{k_s}), s\le t} [\cdot]$. We begin with the following claim
\begin{claim}[Expanding the latents]\label{claim:first_claim} The following lower bound holds:
\begin{align}
\Eforward[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})] &\ge \sum_{t=1}^T \Exp_{<t}\Exp_{k_t \unifsim\{0,1,\dots,K\}}\Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)}\left[\ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right],
\end{align}
Moreover, this lower bound holds with equality if $\bz_{s} \sim p (\bz_{s} \mid \bz_{s-1},\bx_s^{k_s})$ is a Dirac distribution (i.e., deterministic latents).
\end{claim}
\begin{proof}
Let's fix a sequence $k_{1:T}$. It holds that
\begin{align}
\ptheta((\bx_{t}^{k_t})_{1 \le t \le T}) &= \int_{\bz_{1:T} } \prod_{t=1}^T p (\bx_{t}^{k_t}, \bz_{t}\mid (\bx_s^{k_s},\bz_{s})_{s < t}) \nonumber\\
    &= \int_{\bz_{1:T}} \prod_{t=1}^T p (\bx_{t}^{k_t}, \bz_{t}\mid \bz_{t-1}) \tag{Markov Property}\\
    &= \int_{\bz_{1:T}(\veck)} \prod_{t=1}^Tp (\bz_{t} \mid \bz_{t-1},\bx_t^{k_t}) \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1}) \nonumber\\
    &= \Exp_{\bz_{s} \sim p (\bz_{s} \mid \bz_{s-1},\bx_s^{k_s}), s\le T} \prod_{t=1}^T \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1}). \tag{Importance Sampling}
\end{align}
Thus, by Jensen's inequality, 
\begin{align*}
\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T}) &\ge \Exp_{\bz_{s} \sim p (\bz_{s} \mid \bz_{s-1},\bx_s^{k_s}), s\le T} \sum_{t=1}^T \ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1}) = \Epz\left[\sum_{t=1}^T \ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right],
\end{align*}
where the inequality is and equality when $\ptheta(\bz_s \mid \bz_{s-1},\bx_s^{k_s})$ is a Dirac distribution. By applying $\Eforward$ to both sides of the above display, and invoking the Markov property of the latents, we conclude that
\begin{align*}
\Eforward[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})] &\ge \Eforward\Epz\left[\sum_{t=1}^T \ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right] \\
&\quad=  \sum_{t=1}^T \Exp_{<t}\Exp_{k_t \unifsim\{0,1,\dots,K\}}\Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)}\left[\ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right].
\end{align*}
\end{proof}
We now unpack the terms obtained from the preceding claim.
\begin{claim}[ELBO w.r.t. $q$]  It holds that
\begin{align*}
\Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)}\left[\ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right] \ge C_1(\bx_0,k_t) + \left[\Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)}  \ln\frac{\ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx_t^{0})}\right].
\end{align*}
where $ C_1(\bx_0,k_t) $ is a constant depending only on $\bx_0$ and $k_t$, and where the inequality holds with equality if and only if $q(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t}) \equiv \ptheta(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t},\bz_{t-1})$. 
\end{claim}
\begin{proof}
We have that 
\begin{align*}
&\Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)}\left[\ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right]\\
 &= \Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)} \left[\ln \int  \ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})\rmd \bx_{t}^{k_t+1:K}\right]\\
&= \Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)}\left[\ln\left(\Exp_{\bx_t^{k_t+1:K} \sim q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}  \left[\frac{\ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\right]\right)\right] \\
&\ge \Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)}\left[\Exp_{\bx_t^{k_t+1:K} \sim q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\left[  \ln\frac{\ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\right]\right] \tag{(Jensen's inequality)}\\
&= \Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)}  \left[\ln\frac{\ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\right] \tag{Markov property of $q(\cdot)$} \\
&= C_1(\bx_0,k_t) + \left[\Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)}  \ln\frac{\ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx_t^{0})}\right],
\end{align*}
where the constant $ C_1(\bx_0,k_t) = \Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)}\left[ \ln\frac{q(\bx_t^{k_t+1:K} \mid \bx_t^{0})}{q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\right]$ depends only on $\bx_0$ and $k_t$. To check the conditions for equality, note that if $q(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t}) \equiv \ptheta(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t},\bz_{t-1})$, then 
\begin{align*}
\Exp_{\bx_t^{k_t+1:K} \sim q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\left[  \ln\frac{\ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\right] &=   \ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1}) + \Exp_{\bx_t^{k_t+1:K} \sim q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\left[  \ln \ptheta(\bx_{t}^{k_t+1:K} \mid \bz_{t-1}, \bx_t^{k_t})\right]
\end{align*}
Since $\ln(\cdot)$ is strictly concave, $\Exp_{\bx_t^{k_t+1:K} \sim q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})}\left[  \ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right] = 0$ if and only if $\ptheta(\bx_{t}^{k_t+1:K} \mid \bz_{t-1}, \bx_t^{k_t}) = q(\bx_t^{k_t+1:K} \mid \bx_t^{k_t})$. 
\end{proof}
\begin{claim}[Computing the expected ELBO]
\begin{align*}
&\Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)} \ln \frac{ \ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx^{0}_t)} \\
&=C_3(\bx_0,k_t) + \I\{k_t = 0\}\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=1}^{K-1}\I\{j \ge k_t\}\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right)\nonumber,
\end{align*}
where $C_2(\bx_0,k_t)$ is some other constant depending on $\bx_0$ and $k_t$.
\end{claim}
\begin{proof}
The proof invokes similar manipulations to the standard ELBO derivation for diffusion, but with a few careful modifications to handle the fact that we only noise to level $k_t$. As is standard, we  require the identity 
\begin{align}
q(\bx_{t}^{j} \mid \bx_t^{j-1},\bx_t^0) = q(\bx_{t}^{j-1} \mid \bx_t^{j},\bx_t^0) \cdot \frac{q(\bx_{t}^{j} \mid \bx_t^0)}{q(\bx_{t}^{j-1} \mid  \bx_t^0)}. \label{eq:time_reversal}
\end{align}
\paragraph{Part 1: Expanding the likelihood ratios}.  Using the above identity, we obtain
\begin{align*}
&\ln \frac{ \ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx^{0}_t)} \\
&= \ln  p (\bx_t^{K} \mid \bz_{t-1}) + \ln \frac{\ptheta(\bx_t^{k_t} \mid \bx_{t}^{k_t+1},\bz_{t-1})}{q(\bx_{t}^{k_t+1} \mid \bx_t^{0})} + \sum_{j=k_{t}+2}^{K}\ln \frac{\ptheta(\bx_{t}^{j-1} \mid \bx_{t}^{j},\bz_{t-1})}{q(\bx_{t}^{j} \mid \bx_t^{j-1},\bx_t^{0})}\\
&\overset{(i)}{=} \ln  p (\bx_t^{K}\mid \bz_{t-1}) + \ln \frac{\ptheta(\bx_t^{k_t} \mid \bx_{t}^{k_t+1},\bz_{t-1})}{q(\bx_{t}^{k_t+1} \mid  \bx_t^{0})} + \sum_{j=k_{t}+2}^{K}\left(\ln \frac{\ptheta(\bx_{t}^{j-1} \mid \bx_{t}^{j},\bz_{t-1})}{q(\bx_{t}^{j-1} \mid \bx_t^{j},\bx_t^{k_t})} + \ln \frac{q(\bx_{t}^{j-1} \mid \bx_t^{0})}{q(\bx_{t}^{j} \mid \bx_t^{0})}\right)\\
&\overset{(ii)}{=} \ln  p (\bx_t^{K}\mid \bz_{t-1}) + \ln \frac{\ptheta(\bx_t^{k_t} \mid \bx_{t}^{k_t+1},  \bz_{t-1})}{q(\bx_{t}^{k_t+1} \mid  \bx_t^{0})} + \ln \frac{q(\bx_{t}^{k_t+1} \mid \bx_t^{k_t})}{q(\bx_{t}^{K} \mid \bx_t^{k_t})} + \sum_{j=k_{t}+1}^{K-1}\ln \frac{\ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})}{q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0)} \\
&= \frac{\ln  p (\bx_t^{K}\mid \bz_{t-1})}{q(\bx_{t}^{K} \mid \bx_t^{k_t})} + \ln \ptheta(\bx_t^{k_t} \mid \bx_{t}^{k_t+1},  \bz_{t-1})  + \sum_{j=k_{t}+1}^{K-1}\ln \frac{\ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})}{q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0)} \\
&= \ln (q(\bx_t^{k_t} \mid \bx_{t}^{k_t+1})^{\I\{k_t \ge 1\}})  + 
\ln\frac{  p (\bx_t^{K}\mid \bz_{t-1})}{q(\bx_{t}^{K} \mid \bx_t^{k_t})} + \ln \frac{\ptheta(\bx_t^{k_t} \mid \bx_{t}^{k_t+1},  \bz_{t-1})}{q(\bx_t^{k_t} \mid \bx_{t}^{k_t+1})^{\I\{k_t \ge 1\}}}  + \sum_{j=k_{t}+1}^{K-1}\ln \frac{\ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})}{q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0)},
\end{align*}
where $(i)$ uses \ref{eq:time_reversal}, $(ii)$ invokes a cancellation in the telescoping sum, and the final display follows from the computation 
\begin{align}
q(\bx_t^{k_t} \mid \bx_{t}^{k_t+1})^{\I\{k_t \ge 1\}} = \begin{cases} 1 & k_t = 0 \\
q(\bx_t^{k_t} \mid \bx_{t}^{k_t+1}) & k_t \ge 1
\end{cases}.
\end{align}
Observe that, because we don't parameterize $p (\bx_t^{K}\mid \bz_{t-1})$,  $\ln (q(\bx_t^{k_t} \mid \bx_{t}^{k_t+1})^{\I\{k_t \ge 1\}})  + 
\frac{\ln  p (\bx_t^{K}\mid \bz_{t-1})}{q(\bx_{t}^{K} \mid \bx_t^{k_t})}$ can be regarded as some constant $C'(\bx_t^{k_t},\bx_t^{k_t+1},\bx_t^K)$. Thus, 
\begin{align}
&\ln \frac{ \ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx^{0}_t)} = C'(\bx_t^{k_t},\bx_t^{k_t+1},\bx_t^K) + \ln \frac{\ptheta(\bx_t^{k_t} \mid \bx_{t}^{k_t+1},  \bz_{t-1})}{q(\bx_t^{k_t} \mid \bx_{t}^{k_t+1})^{\I\{k_t \ge 1\}}}  + \sum_{j=k_{t}+1}^{K-1}\ln \frac{\ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})}{q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0)} \label{eq:thing2}
\end{align}


\paragraph{Part 2: Taking expecations.} We can now simplify to taking expectations. Observe that 
\begin{align*}
\Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)}\ln \frac{\ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})}{q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0)} = \Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right),
\end{align*}
and similarly,
\begin{align*}
\Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)} \ln \frac{\ptheta(\bx_t^{k_t} \mid \bx_{t}^{k_t+1},  \bz_{t-1})}{q(\bx_t^{k_t} \mid \bx_{t}^{k_t+1})^{\I\{k_t \ge 1\}}}  = \begin{cases} \ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) & k_t = 0 \\
\Dkl\left(q(\bx_{t}^{k_t} \mid \bx_t^{k_t+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{k_t} \mid \bx_{t}^{j+1}, \bz_{t-1})\right) & k_t \ge 1.
\end{cases}
\end{align*}
Finally, $\Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)}  C'(\bx_t^{k_t},\bx_t^{k_t+1},\bx_t^K)$ is a constant $C_2(k_t,\bx_0)$ depending only on $k_t,\bx_0$. 
Thus, from \eqref{eq:thing2}
\begin{align}
&\Exp_{\bx_t^{k_t:K} \sim q(\bx_t^{k_t:K} \mid \bx_t^0)} \ln \frac{ \ptheta(\bx_{t}^{k_t:K} \mid \bz_{t-1})}{q(\bx_t^{k_t+1:K} \mid \bx^{0}_t)}\nonumber\\
&= C_2(k_t,\bx_0) + \I\{k_t = 0\}\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=\max\{1,k_t\}}^{K-1}\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right)\nonumber\\
&= C_2(k_t,\bx_0) + \I\{k_t = 0\}\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=1}^{K-1}\I\{j \ge k_t\}\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right)\nonumber.
\end{align}
\end{proof}


\paragraph{Completing the proof of the ELBO.} We are now ready to complete the proof. By combining the previous two claims, we have
\begin{align*}
&\Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)}\left[\ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right] \\
&\quad\ge C_3(\bx_0,k_t) + \I\{k_t = 0\}\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=1}^{K-1}\I\{j \ge k_t\}\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right),
\end{align*}
where $C_3(\bx_0,k_t) = C_1(\bx_0,k_t)+C_2(\bx_0,k_t)$ and where again, the above is an equality when $q(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t}) \equiv \ptheta(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t},\bz_{t-1})$. Taking an expectation over $k_t\unifsim\{0,1,\dots,K\}$, we have
\begin{align}
\Exp_{k_t \unifsim\{0,1,\dots,K\}}[\I\{k_t = 0\}] = \frac{1}{K+1}, \quad \Exp_{k_t \unifsim\{0,1,\dots,K\}}\I\{j \ge k_t\} = \frac{j+1}{K+1}.
\end{align}
and consequently,
\begin{align*}
&\Exp_{k_t \unifsim\{0,1,\dots,K\}}\Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0), 1 \le t \le T} \ln \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})\\
&\quad\ge C_4(\bx_t^0) +\frac{1}{K+1}\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=1}^{K-1}\frac{j+1}{K+1}\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right)
\end{align*}
Invoking \Cref{claim:first_claim},
\begin{align*} 
&\Eforward[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})] \\
&\ge \sum_{t=1}^T \Exp_{<t}\Exp_{k_t \unifsim\{0,1,\dots,K\}}\Exp_{\bx_t^{k_t} \sim q(\bx_t^{k_t} \mid \bx_t^0)}\left[\ln \ptheta(\bx_{t}^{k_t} \mid \bz_{t-1})\right]\\
&=\sum_{t=1}^T \Exp_{<t} \left[C_4(\bx_t^0) +\frac{1}{K+1}\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=1}^{K-1}\frac{j+1}{K+1}\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right)\right]
\end{align*}
We conclude by observing that $\sum_{t=1}^T \Exp_{<t} \left[C_4(\bx_t^0)\right]$ is a constant $C(\bx_{1:T}^0)$, and that 
\begin{align*}
&\Exp_{<t}\left[\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1})\right] = \Eforward \Epz\left[\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1})\right]\\
&\Exp_{<t}\left[\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right)\right] \\
&= \Eforward \Epz\left[\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right)\right],
\end{align*}
since both terms only depend on $k_{1:t-1},(\bx_s^{k_s})_{1 \le s \le t-1}$ and $\bz_{1:t-1}$. We conclude then that 
\begin{align*}
&\Eforward[\ln  \ptheta((\bx_{t}^{k_t})_{1 \le t \le T})] \ge C(\bx_{1:T}^0) \\
&+ \Eforward\Epz \left[\sum_{t=1}^T \left(\frac{1}{K+1}\ln \ptheta(\bx_t^{0} \mid \bx_{t}^{1},  \bz_{t-1}) +  \sum_{j=1}^{K-1}\frac{j+1}{K+1}\Dkl\left(q(\bx_{t}^{j} \mid \bx_t^{j+1},\bx_t^0) \parallel \ptheta(\bx_{t}^{j} \mid \bx_{t}^{j+1}, \bz_{t-1})\right)\right)\right], 
\end{align*}
as needed. Lastly, we recall that the above is an \emph{equality} under the conditions that \newline (a) $\ptheta(\bz_t \mid \bz_{t-1},\bx_t^{k_t})$ is a Dirac distribution, and (b) $q(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t}) \equiv \ptheta(\bx_{t}^{k_t+1:T} \mid \bx_t^{k_t},\bz_{t-1})$, and we reindex $j \gets j+1$ to ensure consistency with indexing in standard expositions of the diffusion ELBO. 

\qed






 















