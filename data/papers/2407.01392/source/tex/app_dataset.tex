\label{app:dataset_detail}
\section{Additional details about datasets}
\subsection{Dataset for video diffusion}
We adopt the video prediction dataset Minecraft and DMlab used by TECO\cite{yan2023temporally}.

\paragraph{Minecraft Navigation}
The Minecraft navigation dataset consists of first-person-view videos of random walks in the Minecraft `swamp` biome. The agent walks via a technique called `sprint jump` which allows it to jump across blocks without getting stuck at 1 block obstacles. The agent walks straight most of the time, with small chances of turning left or right. The height and width of the video is $128$ pixels and we trim long videos to subsequences of $72$ frames. The dataset comes with paired action data but we discard them to bring more stochasticity to the prediction task. Due to limited compute, we only train on about $10\%$ of the total subsequences.

One problem we noticed about the dataset is when the agent runs into obstacles with a height of 2 blocks or more. In this case, the agent will get stuck and the entire video sequence will consist of grey granite patterns or brown dirty patterns. This leads to a huge amount of frames with these patterns, making video models predict meaningless frames. Yet, we deem this as a problem of this dataset itself.  

\paragraph{DMLab Navigation}
Deepmind Lab navigation dataset consists of random walks in a 3D maze environment. For DMLab, the resolution is $64$ pixels and we use subsequences of $48$ frames. We also disregard the provided actions due to training.

We note that the VQ-VAE latent that stable video diffusion~\cite{blattmann2023stable} diffuses is also only $128\times128\times 3$, indicating \algo{} has the potential to scale up to higher resolution images with pre-trained image encoder and decoders. Due to the sheer size of the datasets, we only use about $10\%$ of the total data sequences for training due to limited computing, as we observe that doing so already allows us to make good generations from initial frames from the test set.

\subsection{Dataset for planning}
\label{app:dataset_planning}
D4RL~\cite{d4rl} is a standard offline RL benchmark featuring a wide range of reinforcement learning environments. Each environment is associated with a provided dataset of offline interactions with the environment featuring state, action, and reward trajectories. 

Like Diffuer~\cite{janner2022planning}, we choose the 3 maze environments as they are challenging long-horizon, multi-modal, sparse reward problems uniquely suited for visualization and evaluating planning algorithms. The IDs for the 3 used environments are ``maze2d-medium-v1'', ``maze2d-large-v1'', ``maze2d-umaze-v1''. In each environment, one controls the acceleration of a robot to walk it towards a goal. The observation space is $4$ dimensional, featuring 2D location and velocity. The action space is 2D acceleration. The agent always receives a random start location and the goal is to reach a fixed goal position for each maze. The agent receives a reward of 1 if it is within a circle of radius 0.5 centered at the goal state, and 0 otherwise. 

The offline RL dataset for the maze environments consists of random walks in the maze. Specifically, the authors first designate all intersections and turn in the maze as waypoints and code an agent to navigate between waypoints with some randomization. As a result, the random walks are generated in a way that the path is collision-free with the walls. The random walks introduce stochasticity to the dataset, as trajectories in the dataset are never towards a specific goal. 

There are a few choices adopted from our main baseline Diffuser~\cite{janner2022planning}: we disregard the reward in the dataset and plan with goals only. We also evaluate a multi-goal variant of each environment (labeled as ``multi'' in Table~\ref{fig:planning}), where the goal is randomized just like the starting position. 

\subsection{Dataset for robot learning}
We choose a long horizon robotic manipulation task as described in Section~\ref{sec:exp_robot}: Consider a tabletop with three slots where we can place objects. One places an apple at slot A or slot B randomly, and then places an orange at the other slot between A and B. A robot is challenged to swap the position of two fruits using the third slot C. That is, it can only move a fruit to an empty slot at a time. For example, when the apple is at slot A and the orange is at slot B, it may move the apple to slot C, leaving slot A empty. Then move the orange to slot A and finally move the apple from slot C to slot B. In figure~\ref{fig:robot}, we illustrate the non-markovian property of the task: When the apple is at slot B and the orange is at slot C, one cannot tell what the immediate action is without knowing the initial positions of objects.

We put stickers on the table indicating a circular region occupied by any slot. Each circular region is designed to be about double the diameter of a fruit. To make sure the task requires visual feedback, we also randomize the location of a fruit inside the slot. We collected $150$ expert demonstrations of a Franka robot performing the task using VR teleoperation and impedance control. Among them, each initial slot configuration makes up half of the dataset. We record videos from two camera views, one from a hand camera and one in the front capturing all three slots. Each demonstration also comes with $6$ dof actions of the robot hand. During the data collection, since one successful demonstration will swap the position of two objects, its end configuration will naturally serve as the starting configuration of the other randomized location, which we leverage to save time. 

Each demonstration comprises $500-600$ frames and actions. We train \algo{} on the entire sequence. However, since adjacent frames are visually close, we pad and downsample the videos to $40$ frames where each frame is bundled with $15$ actions. 


\subsection{Dataset for time series }
\label{app:dataset_timeseries}
\input{tables/ts_data}
We use a set of time series datasets accessible via GluonTS~\cite{gluonts}, which are adopted from prior works like \cite{DBLP:journals/corr/YuRD15,DBLP:journals/corr/LaiCYL17,SALINAS20201181}. These datasets capture real-world data of high-dimensional dynamics like monetary exchange rates or the electricity grid. In Table~\ref{tab:ts_data}, we provide a summary of the features of these datasets, such as the dimensionality, the domains, the sampling frequency, the length of the multivariate sequence in the training set, and the prediction length. We access the datasets in Table~\ref{tab:ts_data} via GluonTS and wrap the data processing functions implemented in GluonTS in our own dataloaders. Each dataset consists of one long multivariate sequence, which is the training split, and a set of short sequences that make up the test split. We construct a validation set of the same cardinality as the held-out test set as a randomly sampled subset of subsequences from the training set. All splits are normalized by the mean and the standard deviation of the features in the training split.

\paragraph{Covariates}
Often, statistical models that approximate~\eqref{eq:prediction_pdf} benefit from manually curated features as additional input to the observations. A sequence of covariates $\boldsymbol{C} = \left\{ \mathbf{c}_t \right\}_{t=1}^T$ can be constructed to help the model recognize seasonal patterns and other temporal dependencies. We follow the implementation in~\cite{rasul2021multivariate} to construct the covariate sequence as a function of the frequency of each dataset in Table~\ref{tab:ts_data}. As such, our covariates are composed of lagged inputs, as well as learned embeddings and handcrafted temporal features that encode information such as the hour of the day or the day of the month, depending on the sampling rate of the particular time series that is being modeled. Therefore, covariates are known for the entire interval~$\left[1, T\right]$, even at inference. We can easily incorporate covariates into the probabilistic framework as
\begin{equation} \label{eq:prediction_pdf_covariates}
    q{\left( \mathbf{x}_{t_0:T} \mid \mathbf{x}_{1:t_0-1}, \mathbf{c}_{1:T} \right)} := \prod_{t=t_0}^T{q{\left( \mathbf{x}_t \mid \mathbf{x}_{1:t_0-1}, \mathbf{c}_{1:T} \right)}} \text{.}
\end{equation}
The benefit obtained from covariates is highly dependent on the characteristics of both the dataset and the model used, as well as the feature engineering practices followed.


\paragraph{Metric}
The Continuous Ranked Probability Score (CRPS)~\cite{james1976scoring} is a scoring function that measures how well the forecast distribution matches the ground truth distribution:
\begin{equation*}
    \operatorname{CRPS}(F, x) = \int_\mathbb{R} \left(F(z) - \mathbb{I}\left\{ x \leq z \right\} \right)^2 \dd{z} \text{,}
\end{equation*}
where $F(z)$ is the univariate cumulative distribution function (CDF) over the predicted value, $x$ is a ground truth observation, and $\mathbb{I}\left\{ x \leq z \right\}$ is the indicator function that is one if $x \leq z$ and zero otherwise. By summing the $D$-dimensional time series along the feature dimension for simulated samples (resulting in $\hat{F}_\text{sum}(t)$) and ground truth data (as $\sum_i x^0_{i,t}$), we can report the $\operatorname{CRPS}_\text{sum}$
\begin{equation*}
    \operatorname{CRPS}_\text{sum} = \mathbb{E}_{t \sim \mathcal{U}\left(t_0, T\right)} \left[\operatorname{CRPS}{\left(\hat{F}_\text{sum}(t), \sum_i x^0_{i,t} \right)}\right]
\end{equation*}
as the average over the prediction window. The lower the $\operatorname{CRPS}_\text{sum}$ value, the better the predicted distribution match the data distribution.

First, we manually sum the time series along the feature dimension and estimate the CDF $\hat{F}_\text{sum}(t)$ via 19 quantile levels at each time step $t$ from 100 sampled trajectories. We then use the implementation in GluonTs~\cite{gluonts} to compute the $\operatorname{CRPS}$, which we report as $\operatorname{CRPS}_\text{sum}$ in Table~\ref{tab:results_ts}. While we aggregate the data manually, we verify that the numerical error relative to the GluonTS implementation remains orders of magnitude below the precision threshold of the reported metric.
