\section{Extended Related Work}\label{app:extended_related}


\paragraph{Reconstructing masked tokens.} Masked Autoencoders for images~\cite{he2022masked} and videos~\cite{feichtenhofer2022masked} are a popular method for representation learning in pixel space. They have been extended to perform diffusion to generate masked patches conditioned on unmasked ones~\cite{wei2023diffusion,gao2023masked}.

\paragraph{Casting Image Generation as Sequence Generation.} \cite{van2016conditional,chen2020generative} show that even generative modeling of non-sequential data, such as images, can be fruitfully cast as sequence generative modeling.

\paragraph{Non-Diffusion Probabilistic Sequence Models.}
\cite{chung2015recurrent} parameterize token-to-token transitions via a variational auto-encoder. This makes them probabilistic, but does not directly maximize the joint probability of sequences, but rather, enables sampling from the distribution of single-step transitions.

\paragraph{Sequence Diffusion with Varying Noise Levels.} Most similar to our work is AR-Diffusion~\cite{wu2023ardiffusion} which similarly aims to train next-token prediction models for sequence diffusion. Key differences are that AR-Diffusion proposes a noise level that is \emph{linearly} dependent on the position of each word in the sequence, while our critical contribution is to have each noise level be \emph{independent}, as this uniquely enables our proposed sampling schemes, such as stabilizing auto-regressive generation and conditioning on corrupted observations. Further, AR-Diffusion only explores language modeling and does not explore guidance, while we investigate Diffusion Forcing as a broadly applicable sequence generative model with particular applications to sequential decision-making. In particular, we introduce Monte-Carlo Guidance as a novel guidance mechanism. Another closely related work is Rolling Diffusion \cite{ruhe2024rolling}, which proposes to diffuse a sequence with near future more certain and far future more uncertain, resembling the causally uncertain sampling scheme of \algo. Like AR-Diffussion, Rolling Diffusion's training noise levels are linearly dependent on the positions of tokens and must use the exact same noise level scheme at sampling time. It, therefore, shares the aforementioned limitations of AR-Diffusion as well.
